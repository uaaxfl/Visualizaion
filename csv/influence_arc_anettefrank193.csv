1999.mtsummit-1.20,P98-1022,0,0.0257569,"Missing"
1999.mtsummit-1.20,P98-1056,1,0.881336,"Missing"
1999.mtsummit-1.20,P98-1060,0,0.256161,"ic representation, which further abstracts from structural differences between languages (Kaplan and Wedekind 1993). In this approach structural misalignment is dealt with in the syntax-semantics interface. While in correspondence-based transfer a piecewise correspondence function maps a source structure node to a corresponding single target node, in transfer models based on term rewriting it is possible to relate a single source structure node to distinct target nodes. Transfer based on term rewriting has been proposed for syntactic and semantics-based transfer (Dorna and Emele 1996a; 1996b; Emele and Dorna 1998 and references therein).3 The transfer component of Dorna and Emele is used in the VerbMobil MT project for transfer on underspecified semantic structures. The term rewriting algorithm operates on sets of terms of the semantic representation language. Transfer on (underspecified) semantic structures has the obvious advantage that structural syntactic differences are neutralized in the more abstract se3 For a comparison between f-structure- and semanticsbased transfer in a term rewriting system see Dorna et al (1998). Closer comparison with correspondence-based fstructure transfer shows that t"
1999.mtsummit-1.20,W97-1508,0,0.0154476,"ce order. 3 Computational technology for LFGbased NLP applications XLE as a grammar development platform The grammar development platform XLE (Xerox Linguistic Environment) is an efficient reimplementation of its precursor system, the Xerox LFG Grammar Writer&apos;s Workbench (Kaplan and Maxwell 1996).1 1 The Grammar Writer&apos;s Workbench, written in Medley Lisp, provides a complete implementation of the LFG formalism, and is particularly useful for teaching pur- 135- XLE as a grammar development platform comes with an interface to finite-state transducers for tokenization and morphological analysis (Kaplan and Newman 1997). A cascade of tokenizers and normalizers segments the input string into tokens, which are then &quot;looked up&quot; in finite-state morphological transducers. The integration of morphological analysis allows to automatically generate large LFG lexica for open class categories like nouns, adjectives, adverbs, etc. They are created by generic LFG lexicon entries which specify f-structure annotations for morphological and lexical information provided by the morphology. While each grammar comes with handcoded core LFG lexica for closed class &quot;syntactic&quot; lexical items, XLE supports integration and processi"
1999.mtsummit-1.20,W89-0203,0,0.71391,". While each grammar comes with handcoded core LFG lexica for closed class &quot;syntactic&quot; lexical items, XLE supports integration and processing of large-size subcategorization lexica, which are extracted and converted from machine-readable dictionaries (Brazil 1997), or obtained by use of corpus analysis tools (Kuhn et al 1998). Algorithms and architectures for unificationbased grammar processing The parsing and generation algorithms realized in XLE are based on insights from research into efficient processing algorithms for parsing and generation with unification-based grammars, in particular (Maxwell and Kaplan 1989, 1993, 1996) and (Shemtov 1997). While context-free phrase structure grammars allow for parsing in polynomial time, grammar formalisms that in addition specify feature constraints can be NP-complete or undecidable, and parse in worst-case exponential or infinite time. Maxwell and Kaplan (1993) investigate the computational properties of standard hybrid parsing architectures for unification-based grammars. They propose alternative non-interleaved processing architectures, which exploit various computational interface properties that support sub-exponential parsing. The unification algorithm de"
1999.mtsummit-1.20,J93-4001,0,0.669401,"s analysis tools (Kuhn et al 1998). Algorithms and architectures for unificationbased grammar processing The parsing and generation algorithms realized in XLE are based on insights from research into efficient processing algorithms for parsing and generation with unification-based grammars, in particular (Maxwell and Kaplan 1989, 1993, 1996) and (Shemtov 1997). While context-free phrase structure grammars allow for parsing in polynomial time, grammar formalisms that in addition specify feature constraints can be NP-complete or undecidable, and parse in worst-case exponential or infinite time. Maxwell and Kaplan (1993) investigate the computational properties of standard hybrid parsing architectures for unification-based grammars. They propose alternative non-interleaved processing architectures, which exploit various computational interface properties that support sub-exponential parsing. The unification algorithm described in (Maxwell and Kaplan 1996) automatically takes advantage of poses and small experimental grammars. The system can be freely downloaded from http://www.parc.xerox.com/ istl/groups/nltt/medley/ . The XLE system is implemented in C and Tcl/Tk, operating under Unix, with plans to port to"
1999.mtsummit-1.20,P98-2212,0,0.0266819,"inspected by the user, to select fstructures for further processing. Ranked alternatives can be selected by reference to local ambiguities, indexed by their context variables (see Fig.4). This interactive model can be extended in various ways, e.g. to trigger user-intervention for predefined decision problems (which may presented in terms of stochastic ranking), and by integration of learning and propagation techniques for human-approved ambiguity resolution. Acquisition of Transfer Knowledge: Techniques for automatic acquisition of transfer lexica from bilingual corpora were proposed e.g. by Turcato (1998). His approach can be generalized to packed f-structure processing, and seamlessly integrated within the XTE translation architecture. Extensions towards alignment models as proposed by Grishman (1994) can be exploited for automatic acquisition of transfer rules. 12 Obvious complications like translation cycles are dealt with in a straightforward way, by source and target language marking of lexical predicates. MT Summit VII __________________________________________________________________ Sept. 1999 Statistical Methods and Robustness: Further extensions are required for transforming an MT pr"
2016.lilt-14.3,baker-etal-2010-modality,0,0.0173017,"s between 68.7 and 93.5, depending on the verb. Closer investigation of their data set, however, reveals a strong bias in sense distributions. As a consequence, the majority sense baselines are hard to beat: none of their classification models is able to beat the 3 Krippendorff’s α, see Krippendorff (2004) Modal Sense Classification At Large / 7 majority baseline with uniform settings across all modal verb types. Prabhakaran et al. (2012) present annotation and classification experiments on manually annotated data from different genres. Their annotation scheme is based on five categories from Baker et al. (2010): Ability, Effort, Intention, Success, and Want. They performed preselection based on an existing modality tagger and performed annotation using Amazon Mechanical Turk. Classification was performed with an SVM multiclass classification model based on lexical and shallow (no syntactic) features. They report 41.9 F1 score on heterogeneous gold data consisting of newswire, letters, blog, and email genres. By contrast, 71.9 F1 score was achieved for 4-fold cross-validation (CV) on a subset of homogeneous genre data consisting of email only. Using confidence values from annotator agreement in train"
2016.lilt-14.3,W12-3802,0,0.0295668,"rocessing. Recent work in Marasović and Frank (2016) show that convolutional neural networks are able to improve on manually crafted feature-based approaches and are easily portable to novel languages, while preserving semantic factors that were found to be relevant in the present work. Currently, we build lexically specific local classifier models. A single classifier model for all modal verbs could also be explored. Further extensions will include source and target role assignment. Possible application areas for our work are argumentation mining (Becker et al., 2016) and sentiment analysis (Benamara et al., 2012) or detection of speculative language. Lexical modal sense classification also has immediate applications in Machine Translation (Baker et al., 2012). Acknowledgments This work was partially funded by the Leibniz Science Campus Empirical Linguistics and Computational Language Modeling, funded by the Leibniz Association under grant no. SAS-2015-IDS-LWC and by the Ministry of Science, Research, and Art (MWK) of the state of BadenWürttemberg, as well as the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) und"
2016.lilt-14.3,N09-2035,0,0.126008,"neous data (cf. Section 2).41 A natural next step from our current findings is to take further advantage of the manually labeled portions of the multi-genre corpus MASC to perform systematic domain or genre adaptation. This could be done by using sampling 41 Though we cannot compare directly to their work given that their annotation scheme and experimental data differs from ours. Modal Sense Classification At Large / 47 techniques that re-proportion training data to approximate the distribution of out-of-domain target genres. Another option is to instead adapt model parameters, as proposed in Bloodgood and Vijay-Shanker (2009), who adapt the cost factor of SVM classifier models to the estimated distribution of out-of-domain evaluation data. 9 Conclusions Modality is an important aspect of discourse analysis relating to argumentation, planning, and reasoning in intensional contexts. In this work, we focused on the classification of modal verb senses that carry different types of intensional meaning: epistemic, deontic and dynamic modality. Prior work in Ruppenhofer and Rehbein (2012) established a wellfounded annotation scheme for modal senses, provided manually annotated resources and induced lexical modal sense cl"
2016.lilt-14.3,W13-0304,0,0.0481662,"Missing"
2016.lilt-14.3,W08-1301,0,0.112187,"Missing"
2016.lilt-14.3,J12-2003,0,0.0627065,"Missing"
2016.lilt-14.3,P02-1033,0,0.251902,"See Section 5.3 for a more detailed discussion of MASC. 8 / LiLT volume 14, issue 3 August 2016 semantic role labeling (Padó and Lapata, 2009). In contrast to this line of work, we do not exploit existing and possibly noisy automatic classification models to project annotations to a novel target language. Our aim is to induce high-quality modal sense annotations from scratch, by exploiting unambiguous paraphrases of specific modal senses in a source language and projecting these senses to the target language. A similar technique has been applied for the induction of word sense annotations by Diab and Resnik (2002). 3 Research Questions and Overview Our leading hypothesis is that modal sense classification can be improved by semantically enriched feature sets. To explore this hypothesis, we aim to find automated methods for extending the size of the currently available data sets, to reduce sparsity and distributional bias. An open question is also whether modal sense classification models are able to generalize across different genres. We explore these questions in the following order: Q1: How can we resolve the manual annotation bottleneck for modal sense classification, in view of acquiring large-size"
2016.lilt-14.3,P14-2085,1,0.783597,"rporate semantic factors at various levels. In the following we motivate and describe our semantic feature set, which we organize in seven groups. An overview of the proposed features is given in Table 7.19 VB: Lexical features of the embedded verb. The embedded verb in the scope of the modal plays an important role in determining modal sense. For instance, with the embedded verb fly in (10.a), we prefer a dynamic reading of can, whereas with play in (10.b) we find a deontic reading. 19 In Zhou et al. (2015) our feature set included an additional feature group “Lexical aspect” (LA), following Friedrich and Palmer (2014). Significance tests showed that this group was not effective and thus can be omitted for overall simplicity. 18 / LiLT volume 14, issue 3 August 2016 (10) a. The children can fly (if they just believe, says Peter Pan)! b. The children can play outside. Building on the dependency parser20 output, we access the lemma of the embedded verb and its part-of-speech tag in the sentence. We also extract whether the verb has a particle (e.g. the plane could take off ), and if yes, which. In most cases, the relation between a modal verb and the embedded verb is given through the auxiliary (aux) dependen"
2016.lilt-14.3,N13-1092,0,0.0240711,"(2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. (2005). They build on the well-established inventory of modal sense categories of Kratzer (1991) in formal semantics: epistemic, deontic/bouletic and circumstantial/dynamic modality, illustrated in (3) above. R&R add three further categories: concessive, conditional and optative. Their annotation scheme proves reliable, with an inter-annotator agreement that ranges from K=0.6 to 0.84 for the different modal verbs. 6 / LiLT volume 14, issue 3 August 2016 Rubinstein et al. (2013)’s modality annotation scheme is equally grounded in the categories of Kratzer (1981, 1991), but assumes various subdivisions, resulting in a fine-grained scheme of 8 categories. They also investigate various groupings of these 8 classes, up to a binary distinction of epistemic vs. non-epistemic modality. They measured very poor levels of inter-annotator reliability for the fine-grained classes, and substantial agreement for the binary distinction, at α=0.89.3 Similar experience is reported by Cui and Chi (2013) for modality annotation on the Chinese Treebank. Nissim et al. (2013) propose a fi"
2016.lilt-14.3,hendrickx-etal-2012-modality,0,0.176785,"Missing"
2016.lilt-14.3,ide-etal-2008-masc,0,0.0168562,"Missing"
2016.lilt-14.3,2005.mtsummit-papers.11,0,0.00613649,"the classifiers are able to generalize to novel textual genres or domains in case their sense distributions differ from those seen in training. 5.2 EPOS: the Europarl and OpenSubtitles heuristically labeled data sets To our knowledge, R&R’s modal sense annotation on MPQA is the largest existing data set so labeled that covers the full range of modal verbs in English.13 We aim to build a larger modal sense-annotated corpus without extensive manual annotation, using cross-lingual sense projection (cf. Section 4). For this approach we selected the parallel German-English datasets from Europarl (Koehn, 2005) and from the OPUS OpenSubtitles corpus (Tiedemann, 2012). The German-English Europarl corpus contains roughly 1.5 million aligned sentences, and the German-English section of OpenSubtitles contains more than 5 million aligned sentences. By applying our projection method, we obtained 7,693 instances of heuristically sense-annotated modal verbs (Table 3). We refer to this corpus henceforth as EPOS. Note that the extracted data set, because it is derived by alignment with a selected set of paraphrases, cannot be considered to represent a natural distribution. In fact, while this data set is simi"
2016.lilt-14.3,D15-1189,0,0.0236564,"adjectives, as well as various types of attitude verbs. With FactBank, Saurí and Pustejovsky (2009) proposed an annotation scheme and an annotated resource that distinguishes 8 degrees of (non)factuality of events. Saurí (2008) and Saurí and Pustejovsky (2012) developed a rule-based system for factuality recognition trained on FactBank, including recognition of sources. de Marneffe et al. (2012) refined the annotations on FactBank and developed a machine learning classifier for event factuality using lexical and structural features, as well as approximations of world knowledge. Recent work in Lee et al. (2015) builds on and further improves upon this work. FactBank’s focus is on notions of (degrees of) factuality or veridicality of events, and considers primarily epistemic uses of modal verbs. Related work in the biomedical domain (i.a. Light et al., 2004, Thompson et al., 2008, Morante and Daelemans, 2011, Szarvas et al., 2012) similarly focuses on the detection of factuality, hedging, and expressions marking evidence. In contrast, our work is concerned with the task of sense disambiguation of modal verbs, which imply non-factuality of their embedded verbs. Annotating and classifying modal senses."
2016.lilt-14.3,W04-3103,0,0.0163406,"vsky (2012) developed a rule-based system for factuality recognition trained on FactBank, including recognition of sources. de Marneffe et al. (2012) refined the annotations on FactBank and developed a machine learning classifier for event factuality using lexical and structural features, as well as approximations of world knowledge. Recent work in Lee et al. (2015) builds on and further improves upon this work. FactBank’s focus is on notions of (degrees of) factuality or veridicality of events, and considers primarily epistemic uses of modal verbs. Related work in the biomedical domain (i.a. Light et al., 2004, Thompson et al., 2008, Morante and Daelemans, 2011, Szarvas et al., 2012) similarly focuses on the detection of factuality, hedging, and expressions marking evidence. In contrast, our work is concerned with the task of sense disambiguation of modal verbs, which imply non-factuality of their embedded verbs. Annotating and classifying modal senses. Most relevant to our work is the current state of the art in automatic modal sense classification in Ruppenhofer and Rehbein (2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. ("
2016.lilt-14.3,P14-5010,0,0.00295742,"type (common, proper, pronoun). person is identified via personal pronoun features; the remaining features are extracted from POS tags. The countability of the subject head is obtained from the Celex database (Baayen et al., 1996). We make use of the following columns from the database: lemma (word), countability (C_N) and uncountability (Unc_N) of a noun.21 Since nouns can appear more than once with the same or different column values, we convert the output, so that every noun appears only once with a single countability value from 20 Feature extraction is performed using Stanford’s CoreNLP (Manning et al., 2014) and the Stanford dependencies (De Marneffe and Manning, 2008). 21 Available at http://celex.mpi.nl/scripts/colstart.pl. Modal Sense Classification At Large / 19 Embedded verb VB lemma part-of-speech particle lemma of head POS of head up, off, on,... TVA passive progressive perfect voice true / false true / false true / false active / passive NEG negation true / false WNV WN sense [0 − 2] WN senseTop WN senses (head+hypernyms) top sense in hypernym hierarchy Subject noun phrase SBJ number person countability noun type WN sense [0 − 2] WN senseTop WN lexname active WN lexname passive sg, pl 1,"
2016.lilt-14.3,W16-1613,1,0.835133,"tion experiment. In response to these questions, we have made various contributions and obtained a number of insights. We applied a paraphrase-driven projection method for the automatic induction of sense-labeled training data using parallel corpora. The senses of modal verbs bear an ideal level of granularity for sense projection using a small set of paraphrases. Using carefully selected paraphrase seeds, the induced annotations yield high accuracy of 0.92. This method can be applied on a broad scale and for many languages, given that bitexts are available in large quantities. Recent work in Marasović and Frank (2016) applied the same method to automatically acquire a large dataset for modal sense classification for German. We designed a linguistically motivated semantic feature space for modal sense classification that is effective in reducing misclassifications. 48 / LiLT volume 14, issue 3 August 2016 We obtain high performance gains especially for modal verbs with difficult sense distinctions and variable sense distributions (most prominently can and could). The high generalization capacity of these models is confirmed in balanced and unbalanced training settings. Even in isolation, the novel semantic"
2016.lilt-14.3,J12-2001,0,0.0792226,"d models have high generalization capacity, especially in unstable distributional settings. 1 Introduction Factuality recognition (Saurí, 2008, de Marneffe et al., 2011, 2012, Saurí and Pustejovsky, 2012) is an important subtask in information extraction. Beyond the bare filtering aspects of veridicality recognition, classification of modal senses plays an important role in text understanding, plan recognition, and the emerging field of argumentation mining. Communication often revolves about hypothetical, planned, apprehended or desired states of affairs. Such “extra-propositional meanings” (Morante and Sporleder, 2012), or intensional contexts are often linguistically marked using modal verbs, adverbs, or attitude verbs,2 as in (1) for hypothetical situations, or (2) for expression of desires or requests. (1) a. He must’ve hurt himself. b. He has certainly found the place by now. c. We anticipate that no one will leave. (2) a. We must solve this problem. b. It is mandatory to resolve this situation. c. I want you to solve this problem. In the present work, we focus on modal verbs and their epistemic and non-epistemic meaning distinctions. Following Kratzer (1991)’s seminal work in formal semantics, recent c"
2016.lilt-14.3,W13-0501,0,0.434247,"Missing"
2016.lilt-14.3,C14-1054,0,0.168897,"ire, letters, blog, and email genres. By contrast, 71.9 F1 score was achieved for 4-fold cross-validation (CV) on a subset of homogeneous genre data consisting of email only. Using confidence values from annotator agreement in training, numbers rose to 44.0 F1 score on gold data, and to 91.1 F1 on within-domain training on email. We thus observe clear overfitting (R&R) and cross-genre effects (Prabhakaran et al.) in state-of-the-art modal sense classification that call for careful analysis of data composition as well as the generalization capacities of specific classification models. Finally, Passonneau et al. (2014) use a variety of stylistic features to model genre variation in the multi-genre MASC corpus.4 Using principal component analysis with a set of 37 lexical, word class, and grammatical features (e.g. past tense), four components are identified as relevant for genre classification. These primarily involve typing over nouns and noun phrases (including named entities) but also reflect various attributes of the verb or the clause (e.g. adverbs or past participles). These results support our choice of MASC for studying cross-genre variation for modal senses. Cross-lingual annotation projection. Cros"
2016.lilt-14.3,W12-3807,0,0.0732284,"ese features are able to capture very diverse contextual factors, but it is difficult to interpret their impact on distinguishing modal senses. The lexical classifiers yield accuracies between 68.7 and 93.5, depending on the verb. Closer investigation of their data set, however, reveals a strong bias in sense distributions. As a consequence, the majority sense baselines are hard to beat: none of their classification models is able to beat the 3 Krippendorff’s α, see Krippendorff (2004) Modal Sense Classification At Large / 7 majority baseline with uniform settings across all modal verb types. Prabhakaran et al. (2012) present annotation and classification experiments on manually annotated data from different genres. Their annotation scheme is based on five categories from Baker et al. (2010): Ability, Effort, Intention, Success, and Want. They performed preselection based on an existing modality tagger and performed annotation using Amazon Mechanical Turk. Classification was performed with an SVM multiclass classification model based on lexical and shallow (no syntactic) features. They report 41.9 F1 score on heterogeneous gold data consisting of newswire, letters, blog, and email genres. By contrast, 71.9"
2016.lilt-14.3,P10-1005,1,0.720779,"Individual features and feature groups. 20 / LiLT volume 14, issue 3 August 2016 {countable, uncountable, ambiguous, none}. If the surface form of the subject head can be found in the above described countability database, countability of the surface form is used. Otherwise, the subject head is lemmatized and the countability of the lemma is assigned to it. Since entries in the Celex database are case-sensitive, the subject is lower cased if it is the first word in the sentence. Lexical semantic features for the subject head are extracted from WordNet (Fellbaum, 1999) using NLTK.22 Following Reiter and Frank (2010), we take the most frequent sense of the common or proper noun in WordNet (subject_sense0), add the direct hypernym of this sense, the direct hypernym of that hypernym, etc., resulting in features subject_sense[1, 2, 3]. We also extract the topmost hypernym of subject_sense0 in WordNet as subject_sense_top (e.g. entity). Finally, the name of the lexicographer file in WordNet, containing subject_sense0, is retrieved using two features, lexname_active and lexname_passive. If the subject appears in an active construction, the WordNet lexical filename is assigned to lexname_active and lexname_pass"
2016.lilt-14.3,W13-0306,0,0.170583,"nhofer and Rehbein (2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. (2005). They build on the well-established inventory of modal sense categories of Kratzer (1991) in formal semantics: epistemic, deontic/bouletic and circumstantial/dynamic modality, illustrated in (3) above. R&R add three further categories: concessive, conditional and optative. Their annotation scheme proves reliable, with an inter-annotator agreement that ranges from K=0.6 to 0.84 for the different modal verbs. 6 / LiLT volume 14, issue 3 August 2016 Rubinstein et al. (2013)’s modality annotation scheme is equally grounded in the categories of Kratzer (1981, 1991), but assumes various subdivisions, resulting in a fine-grained scheme of 8 categories. They also investigate various groupings of these 8 classes, up to a binary distinction of epistemic vs. non-epistemic modality. They measured very poor levels of inter-annotator reliability for the fine-grained classes, and substantial agreement for the binary distinction, at α=0.89.3 Similar experience is reported by Cui and Chi (2013) for modality annotation on the Chinese Treebank. Nissim et al. (2013) propose a fi"
2016.lilt-14.3,ruppenhofer-rehbein-2012-yes,0,0.483321,"ten linguistically marked using modal verbs, adverbs, or attitude verbs,2 as in (1) for hypothetical situations, or (2) for expression of desires or requests. (1) a. He must’ve hurt himself. b. He has certainly found the place by now. c. We anticipate that no one will leave. (2) a. We must solve this problem. b. It is mandatory to resolve this situation. c. I want you to solve this problem. In the present work, we focus on modal verbs and their epistemic and non-epistemic meaning distinctions. Following Kratzer (1991)’s seminal work in formal semantics, recent computational approaches such as Ruppenhofer and Rehbein (2012) distinguish different modal ‘senses’, 2 These are common strategies for encoding extra-propositional meaning in English and many other European languages; other grammatical mechanisms beyond these constructions are employed in the world’s languages. Modal Sense Classification At Large / 3 most prominently, epistemic (3.a), deontic/bouletic (3.b) and circumstantial/dynamic (3.c) modality. (3) a. Geez, Buddha must be so annoyed! (epistemic – possibility) b. We must have clear European standards. (deontic – permission/request) c. She can’t even read them. (dynamic – ability) Modal sense tagging"
2016.lilt-14.3,J12-2002,0,0.0757924,"affect classification performance. Our investigations uncover the difficulty of specific sense distinctions and how they are affected by training set size and distributional bias. Our large-scale experiments confirm that semantically enriched models outperform models built on shallow feature sets. Cross-genre experiments shed light on differences in sense distributions across genres and confirm that semantically enriched models have high generalization capacity, especially in unstable distributional settings. 1 Introduction Factuality recognition (Saurí, 2008, de Marneffe et al., 2011, 2012, Saurí and Pustejovsky, 2012) is an important subtask in information extraction. Beyond the bare filtering aspects of veridicality recognition, classification of modal senses plays an important role in text understanding, plan recognition, and the emerging field of argumentation mining. Communication often revolves about hypothetical, planned, apprehended or desired states of affairs. Such “extra-propositional meanings” (Morante and Sporleder, 2012), or intensional contexts are often linguistically marked using modal verbs, adverbs, or attitude verbs,2 as in (1) for hypothetical situations, or (2) for expression of desire"
2016.lilt-14.3,I08-1064,1,0.804533,"ed training instances for an under-resourced language (the target language) from automatically labeled instances provided by an existing labeling system in another language (the source language) (Resnik, 2004). The annotations on the source language are projected via word alignment from the source to the target language sentences, building on the hypothesis that crucial, especially semantic, properties are shared between translated sentences in a parallel corpus. This technique has been successfully applied for part-of-speech and named entity tagging (Yarowsky et al., 2001), temporal tagging (Spreyer and Frank, 2008), dependency parsing (Hwa et al., 2005), and 4 See Section 5.3 for a more detailed discussion of MASC. 8 / LiLT volume 14, issue 3 August 2016 semantic role labeling (Padó and Lapata, 2009). In contrast to this line of work, we do not exploit existing and possibly noisy automatic classification models to project annotations to a novel target language. Our aim is to induce high-quality modal sense annotations from scratch, by exploiting unambiguous paraphrases of specific modal senses in a source language and projecting these senses to the target language. A similar technique has been applied f"
2016.lilt-14.3,J12-2004,0,0.0157701,"ined on FactBank, including recognition of sources. de Marneffe et al. (2012) refined the annotations on FactBank and developed a machine learning classifier for event factuality using lexical and structural features, as well as approximations of world knowledge. Recent work in Lee et al. (2015) builds on and further improves upon this work. FactBank’s focus is on notions of (degrees of) factuality or veridicality of events, and considers primarily epistemic uses of modal verbs. Related work in the biomedical domain (i.a. Light et al., 2004, Thompson et al., 2008, Morante and Daelemans, 2011, Szarvas et al., 2012) similarly focuses on the detection of factuality, hedging, and expressions marking evidence. In contrast, our work is concerned with the task of sense disambiguation of modal verbs, which imply non-factuality of their embedded verbs. Annotating and classifying modal senses. Most relevant to our work is the current state of the art in automatic modal sense classification in Ruppenhofer and Rehbein (2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. (2005). They build on the well-established inventory of modal sense categori"
2016.lilt-14.3,tiedemann-2012-parallel,0,0.14773,"fentlich (hopefully – deontic), adjectives like erforderlich (necessary – deontic), verbs like gelingen (succeed – dynamic), erlauben (admit – deontic) or affixes such as -bar (-able) as in (lesbar (readable) – dynamic). The seed paraphrases are given in Appendix A.1, including the number of extracted instances per paraphrase and their estimated reliability. Reliability of seeds was evaluated in terms of accuracy, based on manual evaluation of 20 randomly-extracted instances for each seed. For projection we employed the word-aligned Europarl and OpenSubtitles parallel corpus provided by OPUS (Tiedemann, 2012). We used PostCAT (Graca et al., 2007) for word alignment.7 6 Both examples are drawn from the MASC data set. made use of PostCAT because its model optimizes the agreement between source-target and target-source alignments and hence size and quality of the intersective alignment, which is particularly important for our task. Graca et al. found that AER on the Hansard corpus benefits quite significantly from this method. 7 We 12 / LiLT volume 14, issue 3 August 2016 EPOS data (subset) avg. MPQA data (subset) κ_class κ_majclass_experts κ_experts 0.62 0.83 0.87 κ(exp1, gold) κ(exp2, gold) κ(exp1,"
2016.lilt-14.3,P13-4001,0,0.0395971,"Missing"
2016.lilt-14.3,W15-2705,1,0.74635,"Missing"
2020.emnlp-main.321,P15-1039,0,0.105786,"le for different languages: the benchmark datasets CoNLL-09 (Hajiˇc et al., 2009) and CoNLL-12 (Pradhan et al., 2012) are well-established, however, a direct cross-lingual comparison of SRL performance across the covered languages is not possible. The reason being that the language-specific datasets come from different sources and were not conceived for such a comparison. On the other hand, van der Plas et al. (2010) attested the validity of English PropBank labels for French and directly applied them on French data. This motivated SRL projection methods such as van der Plas et al. (2011) and Akbik et al. (2015), which aim to generate a common label set across languages. A known issue with this approach is the need for good quality parallel and sentence-level filtered data. For this reason they used existing parallel corpora, Europarl (Koehn, 2005) and UN (Ziemski et al., 2016), automatically labeled the English side with SRL annotations and transferred them to the corresponding translations. The major issue with this is that evaluation against groundtruth and detailed error analysis on the target languages are not possible, since all annotations are 3905 automatic and come from noisy sources. Likewi"
2020.emnlp-main.321,P16-4001,0,0.0172101,"es. A known issue with this approach is the need for good quality parallel and sentence-level filtered data. For this reason they used existing parallel corpora, Europarl (Koehn, 2005) and UN (Ziemski et al., 2016), automatically labeled the English side with SRL annotations and transferred them to the corresponding translations. The major issue with this is that evaluation against groundtruth and detailed error analysis on the target languages are not possible, since all annotations are 3905 automatic and come from noisy sources. Likewise, the Universal Proposition Bank5 (Akbik et al., 2015; Akbik and Li, 2016), adds an SRL layer on top of the Universal Dependencies (de Marneffe et al., 2014) corpus, which covers eight different languages. However, i) the original corpora come from independent sources and are not parallel, ii) the source sentences were automatically labeled containing noise even before the alignment step, and iii) the test sets also contain automatic projections without human validation of the labels. In contrast, we present a corpus that transfers English high-quality labels to the target side, thus projecting the same labeling style to the other languages; more importantly, we con"
2020.emnlp-main.321,L18-1344,0,0.0186934,"marking for SRL systems, and additionally allows us to assess the validity of the proposed automatic projection for the rest of the data. Our projection method works as follows (see also Figure 1): We obtain automatic translations of the English CoNLL-09 corpus into each of our target languages; then, we automatically label them without applying any language-pair specific label projection model, but use mBERT with additional filters as a means for alignment. We show that by following this approach we obtain a more densely annotated dataset compared to an existing SOTA label projection method (Akbik and Vollgraf, 2018). In short, our contributions are: • The first fully parallel SRL dataset with 2 E.g., German F1 score on the CoNLL-09 dataset lags 10 points behind English, but with currently available datasets we cannot be sure if this is due to differences in the available training data or because of language-specific characteristics. 3 It is not straightforward to use the CoNLL-09 data in a multilingual model: for example, annotations for German use a role inventory with roles A0-A9, and a one-to-one mapping to all English labels is not available. 4 https://www.deepl.com/translator dense, homogeneous anno"
2020.emnlp-main.321,W19-0417,0,0.0114978,"respectively, were considered to be translation shifts and thus were not considered further. 3.4 Automatic Projection The next step is to find an efficient method to automatically transfer the labels in the train/dev portions of the data to the target languages without loosing too many gold labels. Contrary to the test set, we cannot perform human validation due to the size of the data; here we are mostly interested in getting automatically good enough labels to train models. Usually, label projection methods (Pado, 2007; Pad´o and Lapata, 2009; van der Plas et al., 2011; Akbik et al., 2015; Aminian et al., 2019) rely on the intersection of source-to-target and targetto-source word alignments to transfer the labels in the least noisy manner, and this way prefer to have higher precision at the expense of lower recall. Instead, we take a novel approach and rely on the shared space of mBERT embeddings (Devlin et al., 2019). Specifically, we compute pair-wise cosine similarity between source and target tokens and emulate word-alignments according to this measure16 . We show that using mBERT instead of typical word alignments dramatically improves the recall of the projected annotations, and enhanced with"
2020.emnlp-main.321,P98-1013,0,0.470009,"Missing"
2020.emnlp-main.321,C18-1233,0,0.0457574,"Missing"
2020.emnlp-main.321,W05-0620,0,0.129505,"Missing"
2020.emnlp-main.321,D19-1056,1,0.90578,"for MT (Sennrich et al., 2016) and Argumentation Mining (Eger et al., 2018). We similarly create a parallel corpus using automatic translation, however, to our knowledge, we are the first to create a directly comparable multilingual SRL dataset using automatic translation with a manually validated test set, minimizing human labour. Another attempt to close the gap between languages is by training multilingual SRL systems. He et al. (2019) propose a biaffine scorer with syntax rules to prune of candidates, achieving SOTA independently in all languages from CoNLL-09. Mulcaire et al. (2018) and Daza and Frank (2019) train a single model using input data from different languages and obtain modest improvements, especially for languages where less monolingual training data is available. In this sense, our X-SRL corpus contributes with more compatible training data across languages, and aims to improve the performance of jointly trained multilingual models. 3 Building X-SRL In this section we first explain our method for translating the English CoNLL-09 SRL dataset (§3.1) into three target languages (DE, ES, FR) 6 . In §3.2 5 https://github.com/System-T/ UniversalPropositions 6 We chose these languages given"
2020.emnlp-main.321,N19-1423,0,0.220622,"tly on multiple languages annotated with a homogeneous labeling style will be able to better generalize across languages3 . Moreover, by minimizing the need of specialized human annotators, our parallel corpus construction method is lightweight and portable, since it is built on three main components: i) A high-quality annotated dataset in the source language, in this case the English CoNLL-09 corpus (Hajiˇc et al., 2009), ii) a high-quality SOTA Machine Translation system, we are using DeepL Pro4 ; and iii) multilingual contextual word representations, in this case multilingual BERT (mBERT) (Devlin et al., 2019). The situation for these multilingual resources is improving with each day, and thus our method, in perspective, could be followed for producing training data for more lower-resource languages. Importantly, although we automatically project labels from English to the newly available corpora in the different languages for train and dev sections, we also provide test sets on which humans assess the quality of the automatic translations and select the valid predicates as well as the appropriate target head words for the target sentences. Having a humanvalidated test set ensures solid benchmarkin"
2020.emnlp-main.321,C18-1071,0,0.129908,"ls. In contrast, we present a corpus that transfers English high-quality labels to the target side, thus projecting the same labeling style to the other languages; more importantly, we conceive of this corpus, from the very beginning, as a parallel resource with translation equivalence with the source and target languages at the sentence-level. In addition, we create a human-validated test set to allow for proper benchmarking in each language. The use of synthetic data generated by automatic translation has proven to improve performance for MT (Sennrich et al., 2016) and Argumentation Mining (Eger et al., 2018). We similarly create a parallel corpus using automatic translation, however, to our knowledge, we are the first to create a directly comparable multilingual SRL dataset using automatic translation with a manually validated test set, minimizing human labour. Another attempt to close the gap between languages is by training multilingual SRL systems. He et al. (2019) propose a biaffine scorer with syntax rules to prune of candidates, achieving SOTA independently in all languages from CoNLL-09. Mulcaire et al. (2018) and Daza and Frank (2019) train a single model using input data from different l"
2020.emnlp-main.321,P17-1044,0,0.017772,"ling schemes for this task is based on PropBank (Palmer et al., 2005). It comes in two variants: span-based labeling, where arguments are characterized as word-spans (Carreras and M`arquez, 2005; Pradhan et al., 2012), and head-based labeling, which only labels the syntactic head (Hajiˇc et al., 2009). In this work we focus on head-based labeling, as it is applied in the multilingual CoNLL09 shared task dataset, comprising 7 languages. The performance of English SRL has considerably improved in recent years through continuous refinements of Deep Neural Network (DNN) models (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017; Cai et al., 2018); however, although the CoNLL-09 SRL dataset already covers 7 languages, other languages have not received the same level of attention. This situation may be due to factors such as i) the lack of sufficient training data to successfully apply a languageagnostic DNN model; ii) the fact that creating new SRL datasets is resource-consuming; iii) current label projection methods suffering from low recall; finally, iv) even in cases where annotated resources are available in other languages, often they were automatically converted from independent pre-e"
2020.emnlp-main.321,D19-1538,0,0.0897273,"a human-validated test set to allow for proper benchmarking in each language. The use of synthetic data generated by automatic translation has proven to improve performance for MT (Sennrich et al., 2016) and Argumentation Mining (Eger et al., 2018). We similarly create a parallel corpus using automatic translation, however, to our knowledge, we are the first to create a directly comparable multilingual SRL dataset using automatic translation with a manually validated test set, minimizing human labour. Another attempt to close the gap between languages is by training multilingual SRL systems. He et al. (2019) propose a biaffine scorer with syntax rules to prune of candidates, achieving SOTA independently in all languages from CoNLL-09. Mulcaire et al. (2018) and Daza and Frank (2019) train a single model using input data from different languages and obtain modest improvements, especially for languages where less monolingual training data is available. In this sense, our X-SRL corpus contributes with more compatible training data across languages, and aims to improve the performance of jointly trained multilingual models. 3 Building X-SRL In this section we first explain our method for translating"
2020.emnlp-main.321,D19-1355,0,0.0152408,"fically, we compute pair-wise cosine similarity between source and target tokens and emulate word-alignments according to this measure16 . We show that using mBERT instead of typical word alignments dramatically improves the recall of the projected annotations, and enhanced with filters, it also achieves high enough precision, resulting in a more densely labeled target side and therefore better quality training data is expected. Additionally, previous works show that BERT contextualized representations are useful for monolingual Word Sense Disambiguation (WSD) tasks (Loureiro and Jorge, 2019; Huang et al., 2019) which lets us assume that we can rely on mBERT to find good word-level alignments across languages. 16 This is similar to what is done as a first step in BERTScore (Zhang et al., 2020) towards computing a metric for (semantic) sentence similarity, but here we use the token-wise similarity as a guide for cross-lingual word alignments. Figure 2: We compute a pair-wise cosine similarity matrix to simulate word alignments. For each column, we look only at source word-pieces with an associated label and keep the top-k (k=2) most similar target-side word piece candidates (red squares). The black ci"
2020.emnlp-main.321,2005.mtsummit-papers.11,0,0.0370674,"The reason being that the language-specific datasets come from different sources and were not conceived for such a comparison. On the other hand, van der Plas et al. (2010) attested the validity of English PropBank labels for French and directly applied them on French data. This motivated SRL projection methods such as van der Plas et al. (2011) and Akbik et al. (2015), which aim to generate a common label set across languages. A known issue with this approach is the need for good quality parallel and sentence-level filtered data. For this reason they used existing parallel corpora, Europarl (Koehn, 2005) and UN (Ziemski et al., 2016), automatically labeled the English side with SRL annotations and transferred them to the corresponding translations. The major issue with this is that evaluation against groundtruth and detailed error analysis on the target languages are not possible, since all annotations are 3905 automatic and come from noisy sources. Likewise, the Universal Proposition Bank5 (Akbik et al., 2015; Akbik and Li, 2016), adds an SRL layer on top of the Universal Dependencies (de Marneffe et al., 2014) corpus, which covers eight different languages. However, i) the original corpora"
2020.emnlp-main.321,P19-1569,0,0.0156426,"evlin et al., 2019). Specifically, we compute pair-wise cosine similarity between source and target tokens and emulate word-alignments according to this measure16 . We show that using mBERT instead of typical word alignments dramatically improves the recall of the projected annotations, and enhanced with filters, it also achieves high enough precision, resulting in a more densely labeled target side and therefore better quality training data is expected. Additionally, previous works show that BERT contextualized representations are useful for monolingual Word Sense Disambiguation (WSD) tasks (Loureiro and Jorge, 2019; Huang et al., 2019) which lets us assume that we can rely on mBERT to find good word-level alignments across languages. 16 This is similar to what is done as a first step in BERTScore (Zhang et al., 2020) towards computing a metric for (semantic) sentence similarity, but here we use the token-wise similarity as a guide for cross-lingual word alignments. Figure 2: We compute a pair-wise cosine similarity matrix to simulate word alignments. For each column, we look only at source word-pieces with an associated label and keep the top-k (k=2) most similar target-side word piece candidates (red s"
2020.emnlp-main.321,W12-4501,0,0.452463,"eate X-SRL. We automatically translate the English CoNLL-09 corpus, use a fast label projection method for train-dev and get human annotators to select the appropriate head words on the target sentences to obtain gold annotations for the test sets. Introduction Semantic Role Labeling (SRL) is the task of extracting semantic predicate-argument structures from sentences. One of the most widely used labeling schemes for this task is based on PropBank (Palmer et al., 2005). It comes in two variants: span-based labeling, where arguments are characterized as word-spans (Carreras and M`arquez, 2005; Pradhan et al., 2012), and head-based labeling, which only labels the syntactic head (Hajiˇc et al., 2009). In this work we focus on head-based labeling, as it is applied in the multilingual CoNLL09 shared task dataset, comprising 7 languages. The performance of English SRL has considerably improved in recent years through continuous refinements of Deep Neural Network (DNN) models (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017; Cai et al., 2018); however, although the CoNLL-09 SRL dataset already covers 7 languages, other languages have not received the same level of attention. This situation may b"
2020.emnlp-main.321,K17-1041,0,0.0138166,"this task is based on PropBank (Palmer et al., 2005). It comes in two variants: span-based labeling, where arguments are characterized as word-spans (Carreras and M`arquez, 2005; Pradhan et al., 2012), and head-based labeling, which only labels the syntactic head (Hajiˇc et al., 2009). In this work we focus on head-based labeling, as it is applied in the multilingual CoNLL09 shared task dataset, comprising 7 languages. The performance of English SRL has considerably improved in recent years through continuous refinements of Deep Neural Network (DNN) models (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017; Cai et al., 2018); however, although the CoNLL-09 SRL dataset already covers 7 languages, other languages have not received the same level of attention. This situation may be due to factors such as i) the lack of sufficient training data to successfully apply a languageagnostic DNN model; ii) the fact that creating new SRL datasets is resource-consuming; iii) current label projection methods suffering from low recall; finally, iv) even in cases where annotated resources are available in other languages, often they were automatically converted from independent pre-existing annotation schemes"
2020.emnlp-main.321,P16-1009,0,0.0133553,"projections without human validation of the labels. In contrast, we present a corpus that transfers English high-quality labels to the target side, thus projecting the same labeling style to the other languages; more importantly, we conceive of this corpus, from the very beginning, as a parallel resource with translation equivalence with the source and target languages at the sentence-level. In addition, we create a human-validated test set to allow for proper benchmarking in each language. The use of synthetic data generated by automatic translation has proven to improve performance for MT (Sennrich et al., 2016) and Argumentation Mining (Eger et al., 2018). We similarly create a parallel corpus using automatic translation, however, to our knowledge, we are the first to create a directly comparable multilingual SRL dataset using automatic translation with a manually validated test set, minimizing human labour. Another attempt to close the gap between languages is by training multilingual SRL systems. He et al. (2019) propose a biaffine scorer with syntax rules to prune of candidates, achieving SOTA independently in all languages from CoNLL-09. Mulcaire et al. (2018) and Daza and Frank (2019) train a s"
2020.emnlp-main.321,de-marneffe-etal-2014-universal,0,0.179368,"Missing"
2020.emnlp-main.321,W04-2705,0,0.176537,"labels (only for the test sets) were obtained in an efficient way, and report annotator agreement statistics in §3.3. The details of how we perform (automatic) label projection enhanced with filtering for train/dev are given in §3.4. With this we achieve big annotated SRL datasets for three new languages (cf. Table 3). When building the X-SRL dataset, in line with the current PropBank SRL data available in different languages, we focus on verbal predicates only. Note that the English CoNLL-09 data includes both verbal and nominal predicate annotations, yet this is due to the NomBank project (Meyers et al., 2004) being available for that language. By contrast, the remaining languages with PropBank SRL training data (including the CoNLL-09 non-English data) only provide annotations for verbal predicates. While we could attempt projecting the English nominal predicate annotations and create an X-SRL dataset that includes nominal SRL for all target languages – which would mean a big advance over the current situation – admitting nominal and verbal SRL annotations in a multilingual setting would confront us with many translation shifts. We could try to capture these for the manually curated test set, howe"
2020.emnlp-main.321,P18-2106,0,0.0345783,"Missing"
2020.emnlp-main.321,J05-1004,0,0.451019,"noand multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages. 1 Figure 1: Method to create X-SRL. We automatically translate the English CoNLL-09 corpus, use a fast label projection method for train-dev and get human annotators to select the appropriate head words on the target sentences to obtain gold annotations for the test sets. Introduction Semantic Role Labeling (SRL) is the task of extracting semantic predicate-argument structures from sentences. One of the most widely used labeling schemes for this task is based on PropBank (Palmer et al., 2005). It comes in two variants: span-based labeling, where arguments are characterized as word-spans (Carreras and M`arquez, 2005; Pradhan et al., 2012), and head-based labeling, which only labels the syntactic head (Hajiˇc et al., 2009). In this work we focus on head-based labeling, as it is applied in the multilingual CoNLL09 shared task dataset, comprising 7 languages. The performance of English SRL has considerably improved in recent years through continuous refinements of Deep Neural Network (DNN) models (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017; Cai et al., 2018); howeve"
2020.emnlp-main.321,W18-6017,0,0.0207274,"n a multilingual setting would confront us with many translation shifts. We could try to capture these for the manually curated test set, however we would run a risk of generating noisy or scarce target annotations when projecting them for the train/dev sections.7 3.1 Dataset Translation We aim to produce high-quality labeled corpora while reducing as much as possible the amount of time, cost and human intervention needed to fulfill this task. We use Machine Translation to perform dataset translation, obviating the need of human translator services. As previous work (Tiedemann and Agic, 2016; Tyers et al., 2018) has shown, automatic translations are useful as supervision for syntactic dependency labeling tasks since they are quite close to the source languages; likewise, in Argumentation Mining, Eger et al. (2018) achieve comparable results to using human-translated data. One could argue that by automatically translating the English source, we could run into a problem of 7 The reasons are complex: First, by including nominal SRL, we would be confronted with translation shifts in both directions, e.g. N-to-V or V-to-N translations. For these, we’d have to verify whether they correspond to valid verbal"
2020.emnlp-main.321,P15-1109,0,0.014269,"st widely used labeling schemes for this task is based on PropBank (Palmer et al., 2005). It comes in two variants: span-based labeling, where arguments are characterized as word-spans (Carreras and M`arquez, 2005; Pradhan et al., 2012), and head-based labeling, which only labels the syntactic head (Hajiˇc et al., 2009). In this work we focus on head-based labeling, as it is applied in the multilingual CoNLL09 shared task dataset, comprising 7 languages. The performance of English SRL has considerably improved in recent years through continuous refinements of Deep Neural Network (DNN) models (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017; Cai et al., 2018); however, although the CoNLL-09 SRL dataset already covers 7 languages, other languages have not received the same level of attention. This situation may be due to factors such as i) the lack of sufficient training data to successfully apply a languageagnostic DNN model; ii) the fact that creating new SRL datasets is resource-consuming; iii) current label projection methods suffering from low recall; finally, iv) even in cases where annotated resources are available in other languages, often they were automatically converted from"
2020.emnlp-main.321,L16-1561,0,0.0310785,"the language-specific datasets come from different sources and were not conceived for such a comparison. On the other hand, van der Plas et al. (2010) attested the validity of English PropBank labels for French and directly applied them on French data. This motivated SRL projection methods such as van der Plas et al. (2011) and Akbik et al. (2015), which aim to generate a common label set across languages. A known issue with this approach is the need for good quality parallel and sentence-level filtered data. For this reason they used existing parallel corpora, Europarl (Koehn, 2005) and UN (Ziemski et al., 2016), automatically labeled the English side with SRL annotations and transferred them to the corresponding translations. The major issue with this is that evaluation against groundtruth and detailed error analysis on the target languages are not possible, since all annotations are 3905 automatic and come from noisy sources. Likewise, the Universal Proposition Bank5 (Akbik et al., 2015; Akbik and Li, 2016), adds an SRL layer on top of the Universal Dependencies (de Marneffe et al., 2014) corpus, which covers eight different languages. However, i) the original corpora come from independent sources"
2020.emnlp-main.321,P11-2052,0,0.0813997,"Missing"
2020.findings-emnlp.267,P19-1134,0,0.0561932,"Missing"
2020.findings-emnlp.267,D18-1454,0,0.0391683,"Missing"
2020.findings-emnlp.267,A83-1012,0,0.369888,"Missing"
2020.findings-emnlp.267,P19-1470,0,0.440573,"nt commonsense knowledge and (ii) effectively incorporating it into state-of-the-art neural models to improve their reasoning capabilities. In current research, the standard approach to address the first bottleneck is to extract knowledge tuples or paths from large structured knowledge graphs (KGs) (e.g. ConceptNet, Speer et al. (2017)) using graph-based methods (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). However, in this work, instead of retrieving and selecting knowledge from a static KG, we dynamically generate contextually relevant knowledge using COMET (based on GPT-2) (Bosselut et al., 2019). To address the second bottleneck, we build on the hypothesis that models performing such reasoning tasks need to consider multiple knowledge rules jointly (see Fig. 1). Hence, we introduce a novel multi-head knowledge attention model which learns to focus on multiple pieces of knowledge at the same time, and is able to refine the input representation in a recursive manner, to improve the reasoning capabilities. An important aspect of using specified knowledge rules is a gain in interpretability. In this work, we perturb the pieces of knowledge available to the model to demonstrate its robust"
2020.findings-emnlp.267,D15-1075,0,0.350715,"ervations: Dotty was being very grumpy and She felt much better afterwards – we can come up with a plausible explanation about what could have provoked the change in Dotty’s emotion. We can also construct alternative hypotheses that will not change Dotty’s emotion. In order to judge the plausibility of such explanations, we need to have information about mental states and social norms, i.e., a form of commonsense knowledge. Such information includes that calling a close friend, in general, makes people feel happy. This kind of inference goes beyond the broadly studied textual entailment task (Bowman et al., 2015) in that i) it requires a specific form of knowledge, namely knowledge about mental states (intent, motivation), social norms (cause or effect of an event) and behaviour (emotional reactions), and ii) the awareness that inferences we can draw on their basis must often be viewed as plausible explanations, and hence can be defeasible, rather than being strict inferences. In this paper, we investigate social commonsense reasoning in narrative contexts. Specifically, we address two different reasoning tasks: language-based abductive reasoning, and counterfactual invariance prediction. We introduce"
2020.findings-emnlp.267,P08-1090,0,0.210491,"Missing"
2020.findings-emnlp.267,W18-2501,0,0.0151512,"ted to buy a toy plane, but he didn’t have any money; O2 : He bought his toy plane, and kept working so he could buy another; correct Hi : He opened a lemonade stand. Here, the model needs to link O2 back to O1 using social inference knowledge relating to the Hi that best supports one of the sequences: O1 , Hi , O2 . In this case, the model 2972 Task Train Dev Test ↵NLI CIP 169654 12700 1532 1008 3059 1184 We extract, for each event in each input sentence, social commonsense reasoning knowledge from COMET2.0, as detailed in §3. For the extraction process we use SRL as implemented in AllenNLP (Gardner et al., 2018). Table 3: Dataset Statistics: nb. of instances. obtains the (encoded) input: O1 , Hi , O2 , and is tasked to predict the correct Hi , using available knowledge rules.5 (b) For Counterfactual Invariance Prediction, CIP, the model needs to decide whether for given a context Cs1 ,s2 ,s3 , under the assumption of a counterfactual s02 , the given s3 remains invariant or not. I.e., given: Dotty was grumpy. Dotty called close friends to chat. She felt better afterwards. and the counterfactual s02 : Dotty ate something bad – can it still be true that Dolly felt better afterwards? Here our model gets"
2020.findings-emnlp.267,P19-1478,0,0.0194581,"et al., 2020). We discuss the ones most related to our work. Earlier works sought to utilize rule-based reasoning or hand-crafted features (Sun, 1995; Gupta and Hennacy, 2005). With the increase in size of commonsense knowledge bases (Suchanek et al., 2007; Speer et al., 2017) researchers started utilizing them to help models perform commonsense reasoning (Sch¨uller, 2014; Liu et al., 2017). Recently, there have been attempts to leverage pre-trained language models to learn and perform commonsense inference, and they achieved state-of-the-art results (Radford et al., 2018; Trinh and Le, 2018; Kocijan et al., 2019; Radford et al., 2019). Our model takes advantage of both pre-trained LMs and structured knowledge, which allows us to inspect the reasoning process. We also demonstrate that our model shows strong performance for different, and finely structured tasks in abductive and counterfactual reasoning. Structured Commonsense Knowledge in Neural Systems: Different approaches have been proposed to extract and integrate external knowledge into neural models for various NLU tasks such as reading comprehension (RC) (Xu et al., 2017; Mihaylov and Frank, 2018; Weissenborn et al., 2018), question answering ("
2020.findings-emnlp.267,P16-1137,0,0.0295777,"sentations (Lenat, 1995; Espinosa and Lieberman, 2005; Speer et al., 2017; Tandon et al., 2017). In this work, we address social commonsense reasoning, where knowledge about events and its implications is crucial. Rashkin et al. (2018) (Event2Mind) proposed a knowledge resource for commonsense inference about people’s intentions and reactions in everyday events. Later, Sap et al. (2019) (ATOMIC) extended the Event2Mind resource with substantially more events, and with nine dimensions (If-then relation types) per event. There has also been work on automatically acquiring commonsense knowledge (Li et al., 2016; Bosselut et al., 2019; Malaviya et al., 2020). Recently, Nasrin Mostafazadeh (2020) introduced a large-scale dataset (GLUCOSE) capturing ten dimensions of causal explanation (implicit commonsense knowledge) in a narrative context. However, learning to reason over such event-based semistructured knowledge is still a challenging task. In this work, we propose a model which learns to imitate reasoning using such structured knowledge. Commonsense Reasoning (CR): There is a large body of research on commonsense reasoning over natural language text (Levesque et al., 2012; Bowman et al., 2015; Zell"
2020.findings-emnlp.267,D19-1282,0,0.28642,"re is still a performance gap between machines and humans, especially when the task involves implicit knowledge (Talmor et al., 2018). There are two important bottlenecks: (i) obtaining relevant commonsense knowledge and (ii) effectively incorporating it into state-of-the-art neural models to improve their reasoning capabilities. In current research, the standard approach to address the first bottleneck is to extract knowledge tuples or paths from large structured knowledge graphs (KGs) (e.g. ConceptNet, Speer et al. (2017)) using graph-based methods (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). However, in this work, instead of retrieving and selecting knowledge from a static KG, we dynamically generate contextually relevant knowledge using COMET (based on GPT-2) (Bosselut et al., 2019). To address the second bottleneck, we build on the hypothesis that models performing such reasoning tasks need to consider multiple knowledge rules jointly (see Fig. 1). Hence, we introduce a novel multi-head knowledge attention model which learns to focus on multiple pieces of knowledge at the same time, and is able to refine the input representation in a recursive manner, to improve the reasoning"
2020.findings-emnlp.267,N19-1423,0,0.58748,"tart. s1 : Bill and Teddy were at the bar together. s2 : Bill noticed a pretty girl. s3 : He went up to her to flirt. s02 : Bill noticed his mom was there. s3 : He went up to her to flirt. [Yes] or [No] [Yes] or [No] Table 1: Examples from each dataset used in this work. The correct choice in each example is given in bold text. sis), “She felt much better afterwards.”(conclusion) – will a counterfactual assumption (alternative hypothesis), e.g., “Dotty ate something bad”, still lead to same conclusion? While there has been positive impact of transformer-based pretrained language models (LMs) (Devlin et al., 2019; Liu et al., 2019) on several downstream NLU tasks including commonsense reasoning, there is still a performance gap between machines and humans, especially when the task involves implicit knowledge (Talmor et al., 2018). There are two important bottlenecks: (i) obtaining relevant commonsense knowledge and (ii) effectively incorporating it into state-of-the-art neural models to improve their reasoning capabilities. In current research, the standard approach to address the first bottleneck is to extract knowledge tuples or paths from large structured knowledge graphs (KGs) (e.g. ConceptNet, Sp"
2020.findings-emnlp.267,2021.ccl-1.108,0,0.179554,"Missing"
2020.findings-emnlp.267,D19-1509,0,0.261179,"Missing"
2020.findings-emnlp.267,P18-1076,1,0.753975,"t results (Radford et al., 2018; Trinh and Le, 2018; Kocijan et al., 2019; Radford et al., 2019). Our model takes advantage of both pre-trained LMs and structured knowledge, which allows us to inspect the reasoning process. We also demonstrate that our model shows strong performance for different, and finely structured tasks in abductive and counterfactual reasoning. Structured Commonsense Knowledge in Neural Systems: Different approaches have been proposed to extract and integrate external knowledge into neural models for various NLU tasks such as reading comprehension (RC) (Xu et al., 2017; Mihaylov and Frank, 2018; Weissenborn et al., 2018), question answering (QA) (Xu et al., 2016; Tandon et al., 2018; Wang et al., 2019), etc. Recently, many works proposed different ways to extract knowledge from static knowledge graphs (KGs). Most notable are ones that extract subgraphs from KGs using either heuristic methods (Bauer et al., 2018) or graph-based ranking methods (Paul and Frank, 2019; Paul et al., 2020), or else utilize knowledge graph embeddings (Lin et al., 2019) to rank and select relevant knowledge triples or paths. Similar to Bosselut and Choi (2019) and Shwartz et al. (2020), in this work we gene"
2020.findings-emnlp.267,N16-1098,0,0.14386,"Missing"
2020.findings-emnlp.267,2020.emnlp-main.370,0,0.708413,"ctual reasoning in a narrative context. v) To analyze the reasoning capabilities of our model we investigate a) how it performs without fine-tuning on a pre-trained model, b) how robustly it behaves when confronted with perturbations and noise in the knowledge and c) offer qualitative analysis of the reasoning module. Our code is made publicly available.1 2 Social Commonsense Reasoning Tasks We address two social commonsense reasoning tasks that require different reasoning skills. They are exemplified in Table 1 and detailed below. Abdutive Natural Language Inference (↵NLI) Bhagavatula et al. (2020) created a dataset that tests a model’s ability to choose the best explanation for an incomplete set of observations. Abduction is a backward reasoning task. Given a pair of observations O1 and O2 , the ↵NLI task is to select the most plausible explanation (hypothesis) H1 or H2 . Counterfactual Invariance Prediction (CIP) Counterfactual Reasoning (CR) is the mental abil1 https://github.com/Heidelberg-NLP/ MHKA 2970 Priya bought and broke a new laptop [ARG0: Priya] [V: bought] and broke [ARG1: a new laptop] SRL [ARG0: Priya] bought and [V: broke] [ARG1: a new laptop] Priya broke a new laptop Pr"
2020.findings-emnlp.267,N19-1368,1,0.918993,"onsense reasoning, there is still a performance gap between machines and humans, especially when the task involves implicit knowledge (Talmor et al., 2018). There are two important bottlenecks: (i) obtaining relevant commonsense knowledge and (ii) effectively incorporating it into state-of-the-art neural models to improve their reasoning capabilities. In current research, the standard approach to address the first bottleneck is to extract knowledge tuples or paths from large structured knowledge graphs (KGs) (e.g. ConceptNet, Speer et al. (2017)) using graph-based methods (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). However, in this work, instead of retrieving and selecting knowledge from a static KG, we dynamically generate contextually relevant knowledge using COMET (based on GPT-2) (Bosselut et al., 2019). To address the second bottleneck, we build on the hypothesis that models performing such reasoning tasks need to consider multiple knowledge rules jointly (see Fig. 1). Hence, we introduce a novel multi-head knowledge attention model which learns to focus on multiple pieces of knowledge at the same time, and is able to refine the input representation in a recursive manner, to imp"
2020.findings-emnlp.267,P18-1043,0,0.081634,"tanding (McCarthy, 1959; Davis and Marcus, 2015; Storks et al., 2019). Given the growth of interest among researchers in commonsense reasoning, a large body of work has been focused on learning commonsense knowl9 Note that we do not consider the attention maps as explanations. We assume that attention exhibits an intuitive interpretation of the model’s inner workings. 2976 edge representations (Lenat, 1995; Espinosa and Lieberman, 2005; Speer et al., 2017; Tandon et al., 2017). In this work, we address social commonsense reasoning, where knowledge about events and its implications is crucial. Rashkin et al. (2018) (Event2Mind) proposed a knowledge resource for commonsense inference about people’s intentions and reactions in everyday events. Later, Sap et al. (2019) (ATOMIC) extended the Event2Mind resource with substantially more events, and with nine dimensions (If-then relation types) per event. There has also been work on automatically acquiring commonsense knowledge (Li et al., 2016; Bosselut et al., 2019; Malaviya et al., 2020). Recently, Nasrin Mostafazadeh (2020) introduced a large-scale dataset (GLUCOSE) capturing ten dimensions of causal explanation (implicit commonsense knowledge) in a narrat"
2020.findings-emnlp.267,2020.emnlp-main.373,0,0.0775246,"Missing"
2020.findings-emnlp.267,D18-1006,0,0.0274177,"). Our model takes advantage of both pre-trained LMs and structured knowledge, which allows us to inspect the reasoning process. We also demonstrate that our model shows strong performance for different, and finely structured tasks in abductive and counterfactual reasoning. Structured Commonsense Knowledge in Neural Systems: Different approaches have been proposed to extract and integrate external knowledge into neural models for various NLU tasks such as reading comprehension (RC) (Xu et al., 2017; Mihaylov and Frank, 2018; Weissenborn et al., 2018), question answering (QA) (Xu et al., 2016; Tandon et al., 2018; Wang et al., 2019), etc. Recently, many works proposed different ways to extract knowledge from static knowledge graphs (KGs). Most notable are ones that extract subgraphs from KGs using either heuristic methods (Bauer et al., 2018) or graph-based ranking methods (Paul and Frank, 2019; Paul et al., 2020), or else utilize knowledge graph embeddings (Lin et al., 2019) to rank and select relevant knowledge triples or paths. Similar to Bosselut and Choi (2019) and Shwartz et al. (2020), in this work we generate contextually relevant knowledge using language models trained on KGs. With the increa"
2020.findings-emnlp.267,P17-4020,0,0.0605605,"Missing"
2020.findings-emnlp.267,P16-1220,0,0.0154585,"ford et al., 2019). Our model takes advantage of both pre-trained LMs and structured knowledge, which allows us to inspect the reasoning process. We also demonstrate that our model shows strong performance for different, and finely structured tasks in abductive and counterfactual reasoning. Structured Commonsense Knowledge in Neural Systems: Different approaches have been proposed to extract and integrate external knowledge into neural models for various NLU tasks such as reading comprehension (RC) (Xu et al., 2017; Mihaylov and Frank, 2018; Weissenborn et al., 2018), question answering (QA) (Xu et al., 2016; Tandon et al., 2018; Wang et al., 2019), etc. Recently, many works proposed different ways to extract knowledge from static knowledge graphs (KGs). Most notable are ones that extract subgraphs from KGs using either heuristic methods (Bauer et al., 2018) or graph-based ranking methods (Paul and Frank, 2019; Paul et al., 2020), or else utilize knowledge graph embeddings (Lin et al., 2019) to rank and select relevant knowledge triples or paths. Similar to Bosselut and Choi (2019) and Shwartz et al. (2020), in this work we generate contextually relevant knowledge using language models trained on"
2020.findings-emnlp.267,P19-1472,0,0.0197625,"2016; Bosselut et al., 2019; Malaviya et al., 2020). Recently, Nasrin Mostafazadeh (2020) introduced a large-scale dataset (GLUCOSE) capturing ten dimensions of causal explanation (implicit commonsense knowledge) in a narrative context. However, learning to reason over such event-based semistructured knowledge is still a challenging task. In this work, we propose a model which learns to imitate reasoning using such structured knowledge. Commonsense Reasoning (CR): There is a large body of research on commonsense reasoning over natural language text (Levesque et al., 2012; Bowman et al., 2015; Zellers et al., 2019; Trichelair et al., 2019; Becker et al., 2020). We discuss the ones most related to our work. Earlier works sought to utilize rule-based reasoning or hand-crafted features (Sun, 1995; Gupta and Hennacy, 2005). With the increase in size of commonsense knowledge bases (Suchanek et al., 2007; Speer et al., 2017) researchers started utilizing them to help models perform commonsense reasoning (Sch¨uller, 2014; Liu et al., 2017). Recently, there have been attempts to leverage pre-trained language models to learn and perform commonsense inference, and they achieved state-of-the-art results (Radford"
2020.findings-emnlp.267,D19-1335,0,0.0207822,"2019; Malaviya et al., 2020). Recently, Nasrin Mostafazadeh (2020) introduced a large-scale dataset (GLUCOSE) capturing ten dimensions of causal explanation (implicit commonsense knowledge) in a narrative context. However, learning to reason over such event-based semistructured knowledge is still a challenging task. In this work, we propose a model which learns to imitate reasoning using such structured knowledge. Commonsense Reasoning (CR): There is a large body of research on commonsense reasoning over natural language text (Levesque et al., 2012; Bowman et al., 2015; Zellers et al., 2019; Trichelair et al., 2019; Becker et al., 2020). We discuss the ones most related to our work. Earlier works sought to utilize rule-based reasoning or hand-crafted features (Sun, 1995; Gupta and Hennacy, 2005). With the increase in size of commonsense knowledge bases (Suchanek et al., 2007; Speer et al., 2017) researchers started utilizing them to help models perform commonsense reasoning (Sch¨uller, 2014; Liu et al., 2017). Recently, there have been attempts to leverage pre-trained language models to learn and perform commonsense inference, and they achieved state-of-the-art results (Radford et al., 2018; Trinh and L"
2020.lrec-1.282,S17-1027,1,0.261416,"owledge relations, ConceptNet 1. Introduction In everyday communication as well as in written texts people omit information that seems clear and evident, such that only part of the message needs to be expressed in words (Grice, 1975). While this information can easily be filled in by the hearer, a computational system typically does not possess commonsense or domain-specific knowledge that is needed to reconstruct the implied information. Especially in argumentative texts it is very common that (important) parts of the argument such as warrants are implied and omitted (Rajendran et al., 2016; Becker et al., 2017b; Hulpus et al., 2019). This leads us to the assumption that the logic of an argument is in general not fully recoverable from what is explicitly said, and that for argument analysis it will be beneficial to reconstruct such implied information. We aim to fill such gaps by identifying and inserting knowledge that connects given statements. To perform this, we want to learn from human-generated data of missing and implied information. This motivates the current work, in which we gather high-quality annotations of implied knowledge in the form of simple natural language sentences in English. Th"
2020.lrec-1.282,W16-2815,0,0.185424,"al enthymemes. Using these as trigger words, they reconstruct enthymemes from the local context, while Rajendran et al. (2016) retrieve and fill missing propositions in arguments from similar or related arguments. Becker et al. (2016a, 2016b) show that argumentative texts are rich in generic and generalizing sentences, which are semantic clause types (Friedrich and Palmer, 2014) that often express commonsense knowledge. We will show that large portions of implied knowledge in argumentative texts are naturally stated using these clause types. In their attempt to reconstruct implicit knowledge, Boltuzic and Snajder (2016) find that the claims that users make in 1 2316 The data is available at maria-becker/IKAT-EN. https://github.com/ online debate platforms often build on implicit knowledge. They show that the amount of implicitness is dependent on genre and register and point out that the reconstruction of implicit premises can be helpful for claim detection. Recently, Hulpus et al. (2019) point out the relevance of reconstructing implicit knowledge for understanding arguments in a computational setting, by proposing the task of argument explicitation, which they define as a task that makes explicit both (i)"
2020.lrec-1.282,P14-2085,0,0.166797,"s from the Microtexts Corpus (Peldszus and Stede, 2015), a very concise and focused argumentation dataset which is already annotated with argumentative components and relations such as support, rebuttal or undercut. For all unit pairs they are presented with, annotators are asked to add the information that makes the connection between the units explicit, using short and simple sentences. To learn more about the nature and characteristics of both the argumentative texts and the added information, we further annotate the data with two specific semantic information types: semantic clause types (Friedrich and Palmer, 2014) and ConceptNet knowledge relations (Speer and Havasi, 2012; Havasi et al., 2009), which were both found to be characteristic for argumentative texts (Becker et al., 2016a; Becker et al., 2017b). The outcome of our work is a carefully designed and richly annotated dataset,1 for which we provide an in-depth analysis by investigating characteristic distributions and correlations between the assigned labels. The contributions of this work are: (i) high-quality annotations of implicit knowledge on the argumentative Microtext corpus, (ii) characterization of the argumentative units from the Microte"
2020.lrec-1.282,N18-1175,0,0.0724859,"plicit knowledge annotations we present in this paper are also based on argumentative microtexts (Peldszus and Stede, 2015), thus our corpus can be seen as an extension of the corpus published by Becker et al. (2017b). The main differences are that (i) our data is in English (as opposed to German), (ii) the semantic clause types and commonsense knowledge relations are not only annotated for the inserted sentences, but also for the argumentative texts themselves, (iii) our corpus includes more annotated unit pairs, and that (iv) in our corpus all annotations are conducted by expert annotators. Habernal et al. (2018) present the argument reasoning comprehension task, where given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. They provide a dataset where Amazon Turkers added warrants for 2k arguments from news comments. As opposed to our dataset, the annotators were only supposed to fill in the gap between a pair of claim and premise, while we consider larger arguments consisting of a claim and several premises. Furthermore, we annotate implicit information not on"
2020.lrec-1.282,W16-2804,0,0.455231,"clauses, commonsense knowledge relations, ConceptNet 1. Introduction In everyday communication as well as in written texts people omit information that seems clear and evident, such that only part of the message needs to be expressed in words (Grice, 1975). While this information can easily be filled in by the hearer, a computational system typically does not possess commonsense or domain-specific knowledge that is needed to reconstruct the implied information. Especially in argumentative texts it is very common that (important) parts of the argument such as warrants are implied and omitted (Rajendran et al., 2016; Becker et al., 2017b; Hulpus et al., 2019). This leads us to the assumption that the logic of an argument is in general not fully recoverable from what is explicitly said, and that for argument analysis it will be beneficial to reconstruct such implied information. We aim to fill such gaps by identifying and inserting knowledge that connects given statements. To perform this, we want to learn from human-generated data of missing and implied information. This motivates the current work, in which we gather high-quality annotations of implied knowledge in the form of simple natural language sen"
2020.lrec-1.282,speer-havasi-2012-representing,0,0.0104862,"concise and focused argumentation dataset which is already annotated with argumentative components and relations such as support, rebuttal or undercut. For all unit pairs they are presented with, annotators are asked to add the information that makes the connection between the units explicit, using short and simple sentences. To learn more about the nature and characteristics of both the argumentative texts and the added information, we further annotate the data with two specific semantic information types: semantic clause types (Friedrich and Palmer, 2014) and ConceptNet knowledge relations (Speer and Havasi, 2012; Havasi et al., 2009), which were both found to be characteristic for argumentative texts (Becker et al., 2016a; Becker et al., 2017b). The outcome of our work is a carefully designed and richly annotated dataset,1 for which we provide an in-depth analysis by investigating characteristic distributions and correlations between the assigned labels. The contributions of this work are: (i) high-quality annotations of implicit knowledge on the argumentative Microtext corpus, (ii) characterization of the argumentative units from the Microtext corpus and the inserted sentences in terms of semantic c"
2020.tacl-1.34,P13-2131,0,0.460095,"arsing task, metrics do not need to address this issue because input tokens are typically projected to lexical concepts by lemmatization, hence two graphs for the same sentence tend not to disagree on the concepts projected from the input. This is different in semantic parsing where the projected concepts are often more abstract. This article is structured as follows: We first establish seven principles that one may expect a metric for comparing meaning representations to Different metrics have been proposed to compare Abstract Meaning Representation (AMR) graphs. The canonical SMATCH metric (Cai and Knight, 2013) aligns the variables of two graphs and assesses triple matches. The recent SEMBLEU metric (Song and Gildea, 2019) is based on the machine-translation metric BLEU (Papineni et al., 2002) and increases computational efficiency by ablating the variablealignment. In this paper, i) we establish criteria that enable researchers to perform a principled assessment of metrics comparing meaning representations like AMR; ii) we undertake a thorough analysis of SMATCH and SEMBLEU where we show that the latter exhibits some undesirable properties. For example, it does not conform to the identity of indisc"
2020.tacl-1.34,S17-2001,0,0.0775004,"Missing"
2020.tacl-1.34,W19-1201,0,0.0993383,"Missing"
2020.tacl-1.34,W13-2322,0,0.765135,"SMATCH and SEMBLEU where we show that the latter exhibits some undesirable properties. For example, it does not conform to the identity of indiscernibles rule and introduces biases that are hard to control; and iii) we propose a novel metric S2 MATCH that is more benevolent to only very slight meaning deviations and targets the fulfilment of all established criteria. We assess its suitability and show its advantages over SMATCH and SEMBLEU. 1 Introduction Proposed in 2013, the aim of Abstract Meaning Representation (AMR) is to represent a sentence’s meaning in a machine-readable graph format (Banarescu et al., 2013). AMR graphs are rooted, acyclic, directed, and edge-labeled. Entities, events, properties, and states are represented as variables that are linked to corresponding concepts (encoded as leaf nodes) via is-instance relations (cf. Figure 1, left). This structure allows us to capture complex linguistic phenomena such as coreference, semantic roles, or polarity. When measuring the similarity between two AMR graphs A and B , for instance for the purpose of AMR parse quality evaluation, the metric of choice is usually SMATCH (Cai and Knight, 2013). Its backbone is an alignment-search be1 Most resear"
2020.tacl-1.34,W14-3346,0,0.0257131,"(i) SEMBLEU performs k-gram extraction from Avf and B vf in a breadth-first traversal (path extraction). It then (ii) adopts the BLEU score from MT (Papineni et al., 2002) to calculate an overlap score based on the extracted bags of k-grams: ! n X SEMBLEU = BP · exp wk log pk (2) k =1   |B vf | min 1− vf ,0 BP = e |A | (3) where pk is BLEU’s modified k-gram precision that measures k-gram overlap of a candidate against a Avf )∩kgram(B vf )| . wk is the reference: pk = |kgram|(kgram (Avf )| (typically uniform) weight over chosen k-gram sizes. SEMBLEU uses NIST geometric probability smoothing (Chen and Cherry, 2014). The recallfocused ‘‘brevity penalty’’ BP returns a value smaller than 1 when the candidate length |Avf |is smaller than the reference length |B vf |. The graph traversal performed in SEMBLEU starts at the root node. During this traversal it simplifies the graph by replacing variables with their corresponding concepts (see Figure 1: the node c becomes DRINK-01) and collects visited nodes and edges in uni-, bi- and tri grams (k = 3 is recommended). Here, a source node together with a relation and its target node counts as a bi-gram. For the graph in Figure 1, the extracted unigrams are {cat, w"
2020.tacl-1.34,N19-1366,0,0.0399522,"Missing"
2020.tacl-1.34,E17-1051,0,0.388219,"hat the asymmetry (Principle III) measured for BLEU in MT is amplified by SEMBLEU in AMR, mainly due to the biases it incurs (Principle V). Although asymmetry can be tolerated in parser evaluation if outputs are compared against gold graphs in a standardized manner, it is difficult to extension S2 MATCH may also prove beneficial for DRS. Other research into AMR metrics aims at making the comparison fairer by normalizing graphs (Goodman, 2019). Anchiˆeta et al. (2019) argue that one should not, for example, insert an extra is-root node when comparing AMR graphs (as done in SEMBLEU and SMATCH). Damonte et al. (2017) extend SMATCH to analyze individual AMR facets (co-reference, WSD, etc.). Cai and Lam (2019) adapt SMATCH to analyze their parser’s performance in predicting triples that are in close proximity to the root. Our metric S2 MATCH allows for straightforward integration of these approaches. apply an asymmetric metric to measure IAA or to compare parses for detecting paraphrases, or in tri-parsing, where no reference is available. If the asymmetry is amplified by a bias, it becomes harder to judge the scores. Finally, considering that SEMBLEU does not match AMR graphs on the graph-level but matches"
2020.tacl-1.34,J16-3006,0,0.0618184,"the fulfilment of all seven principles and propose S2 MATCH, a metric that accounts for graded similarity of concepts as atomic graph components. In future work, we aim for a metric that accounts for graded compositional similarity of subgraphs. 7 Related Work Developing similarity metrics for meaning representations (MRs) is important, as it, inter alia, affects semantic parser evaluation and computation of IAA statistics for sembanking. MRs are designed to represent the meaning of text in a well-defined, interpretable form that is able to identify meaning differences and support inference. Bos (2016, 2019) has shown how AMR can be translated to FOL, a well-established MR formalism. Discourse Representation Theory (DRT; Kamp, 1981; Kamp and Reyle, 1993) is based on and extends FOL to discourse representation. A recent shared task on DRS parsing used the COUNTER metric (Abzianidze et al., 2019; Evang, 2019), an adaption of SMATCH, underlining SMATCH’s general applicability. Its 535 Acknowledgments Deng Cai and Wai Lam. 2019. Core semantic first: A top-down approach for AMR parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna"
2020.tacl-1.34,J02-2001,0,0.17166,"properties of different AMR metrics that would help informing researchers to answer questions such as: Which metric should I use to assess the similarity of two AMR graphs, e.g., in AMR parser evaluation? What are the trade-offs when choosing one metric over the other? Besides providing criteria for such a principled comparison, we discuss a property that none of the existing AMR metrics currently satisfies: They do not measure graded meaning differences. Such differences may emerge because of near-synonyms such as ruin – annihilate; skinny – thin – slim; enemy – foe (Inkpen and Hirst, 2006; Edmonds and Hirst, 2002) or paraphrases such as be able to – can; unclear – not clear. In a classical syntactic parsing task, metrics do not need to address this issue because input tokens are typically projected to lexical concepts by lemmatization, hence two graphs for the same sentence tend not to disagree on the concepts projected from the input. This is different in semantic parsing where the projected concepts are often more abstract. This article is structured as follows: We first establish seven principles that one may expect a metric for comparing meaning representations to Different metrics have been propos"
2020.tacl-1.34,J06-1003,0,0.239029,"(d / develop-01:mod ( u / usual :polarity -)) :ARG0 (d1 / develop-02:mod ( u0 / unusual )) 0.60 14.0 dissimilar Table 6: Examples where S2 MATCH assigns a higher score, accounting for the similarity of aligned concepts . two sentences to be semantically similar in their meaning, the higher the metric should rate the corresponding AMR graphs (meaning similarity). Second, the more a human rates two sentences to be related in their meaning (maximum: equivalence), the higher the score of our metric of the corresponding AMR graphs should tend to be (meaning relatedness). Albeit not the exact same (Budanitsky and Hirst, 2006), the tasks are closely related and both in conjunction should allow us to better assess the performance of our AMR metrics. As ground truth for the meaning similarity rating task we use test data of the Semantic Textual Similarity (STS) shared task (Cer et al., 2017), with 1,379 sentence pairs annotated for meaning similarity. For the meaning-relatedness task we use SICK (Marelli et al., 2014) with 9,840 sentence pairs that have been additionally annotated for semantic relatedness.14 We proceed as follows: We normalize the human ratings to [0,1]. Then we apply GPLA to parse the sentence tuple"
2020.tacl-1.34,W19-1202,0,0.0643758,"sentations (MRs) is important, as it, inter alia, affects semantic parser evaluation and computation of IAA statistics for sembanking. MRs are designed to represent the meaning of text in a well-defined, interpretable form that is able to identify meaning differences and support inference. Bos (2016, 2019) has shown how AMR can be translated to FOL, a well-established MR formalism. Discourse Representation Theory (DRT; Kamp, 1981; Kamp and Reyle, 1993) is based on and extends FOL to discourse representation. A recent shared task on DRS parsing used the COUNTER metric (Abzianidze et al., 2019; Evang, 2019), an adaption of SMATCH, underlining SMATCH’s general applicability. Its 535 Acknowledgments Deng Cai and Wai Lam. 2019. Core semantic first: A top-down approach for AMR parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3797–3807, Hong Kong, China. We are grateful to the anonymous reviewers and the action editors for their valuable time and comments. This work has been partially funded by DFG within the project ExpLAIN. Between the Lines – Knowledge-b"
2020.tacl-1.34,P14-1134,0,0.227066,"Missing"
2020.tacl-1.34,W18-6450,0,0.0362839,"Missing"
2020.tacl-1.34,S14-2001,0,0.0302982,"tences to be related in their meaning (maximum: equivalence), the higher the score of our metric of the corresponding AMR graphs should tend to be (meaning relatedness). Albeit not the exact same (Budanitsky and Hirst, 2006), the tasks are closely related and both in conjunction should allow us to better assess the performance of our AMR metrics. As ground truth for the meaning similarity rating task we use test data of the Semantic Textual Similarity (STS) shared task (Cer et al., 2017), with 1,379 sentence pairs annotated for meaning similarity. For the meaning-relatedness task we use SICK (Marelli et al., 2014) with 9,840 sentence pairs that have been additionally annotated for semantic relatedness.14 We proceed as follows: We normalize the human ratings to [0,1]. Then we apply GPLA to parse the sentence tuples (si , s′i ), obtaining tuples (parse(si ), parse(s′i)) and score the graph pairs with the metrics: SMATCH(i), S2 MATCH(i), SEMBLEU(i), and H(i), where H(i) is the human score. For both tasks SMATCH and S2 MATCH yield better or equal correlations with human raters than SEMBLEU (Table 7). When considering the RMS error p P n−1 ni=1 (H (i) − metric(i))2 . the difference is even more pronounced."
2020.tacl-1.34,W17-4730,0,0.0232378,"Missing"
2020.tacl-1.34,P19-1451,0,0.0147133,"st to S(2)MATCH (∆: +2). 6 Summary of Our Metric Analyses Table 10 summarizes our analyses’ integral results. Principle I is fulfilled by all metrics as they exhibit continuity, non-negativity and an upper bound. Principle II, however, is not satisfied by SEMBLEU because it can mistake two graphs of different meaning as equivalent. This is because it ablates a variable-alignment and therefore cannot capture facets of coreference. Yet, a positive outcome of this is that it is fast to compute. This could make it first choice in some recent AMR parsing approaches that use reinforcement learning (Naseem et al., 2019), where rapid feedback is needed. It also marks 534 a point by fully satisfying Principle IV, yielding fully deterministic results. SMATCH, by contrast, either needs to resort to a costly ILP solution or (in practice) uses hill-climbing with multiple restarts to reduce divergence to a negligible amount. A central insight brought out by our analysis is that SEMBLEU exhibits biases that are hard to control. This is caused by two (interacting) factors: (i) The extraction of k-grams is applied on the graph top to bottom and visits some nodes more frequently than others. (ii) It raises some (but no"
2020.tacl-1.34,J06-2003,0,0.0246017,"-depth comparison of the properties of different AMR metrics that would help informing researchers to answer questions such as: Which metric should I use to assess the similarity of two AMR graphs, e.g., in AMR parser evaluation? What are the trade-offs when choosing one metric over the other? Besides providing criteria for such a principled comparison, we discuss a property that none of the existing AMR metrics currently satisfies: They do not measure graded meaning differences. Such differences may emerge because of near-synonyms such as ruin – annihilate; skinny – thin – slim; enemy – foe (Inkpen and Hirst, 2006; Edmonds and Hirst, 2002) or paraphrases such as be able to – can; unclear – not clear. In a classical syntactic parsing task, metrics do not need to address this issue because input tokens are typically projected to lexical concepts by lemmatization, hence two graphs for the same sentence tend not to disagree on the concepts projected from the input. This is different in semantic parsing where the projected concepts are often more abstract. This article is structured as follows: We first establish seven principles that one may expect a metric for comparing meaning representations to Differen"
2020.tacl-1.34,S19-1024,1,0.858056,"an SVM that classifies AMRs to determine whether two sentences are equivalent in meaning. In such a case, S2 MATCH is bound to detect meaningsimilarities that cannot be captured by SMATCH or SEMBLEU, for example, due to paraphrases being projected into the parses. Computational AMR tasks Since the introduction of AMR, many AMR-related tasks have emerged. Most prominent is AMR parsing (Wang et al., 2015, 2016; Damonte et al., 2017; Konstas et al., 2017; Lyu and Titov, 2018; Zhang et al., 2019). The inverse task generates text from AMR graphs (Song et al., 2017, 2018; Damonte and Cohen, 2019). Opitz and Frank (2019) rate the quality of automatic AMR parses without costly gold data. 8 Conclusion We motivated seven principles for metrics measuring the similarity of graph-based (Abstract) Meaning Representations, from mathematical, linguistic and engineering perspectives. A metric that fulfills all principles is applicable to a wide spectrum of cases, ranging from parser evaluation to sound IAA calculation. Hence (i) our principles can inform (A)MR researchers who desire to compare and select among metrics, and (ii) they ease and guide the development of new metrics. We provided examples for both scenarios."
2020.tacl-1.34,P02-1040,0,0.110722,"to disagree on the concepts projected from the input. This is different in semantic parsing where the projected concepts are often more abstract. This article is structured as follows: We first establish seven principles that one may expect a metric for comparing meaning representations to Different metrics have been proposed to compare Abstract Meaning Representation (AMR) graphs. The canonical SMATCH metric (Cai and Knight, 2013) aligns the variables of two graphs and assesses triple matches. The recent SEMBLEU metric (Song and Gildea, 2019) is based on the machine-translation metric BLEU (Papineni et al., 2002) and increases computational efficiency by ablating the variablealignment. In this paper, i) we establish criteria that enable researchers to perform a principled assessment of metrics comparing meaning representations like AMR; ii) we undertake a thorough analysis of SMATCH and SEMBLEU where we show that the latter exhibits some undesirable properties. For example, it does not conform to the identity of indiscernibles rule and introduces biases that are hard to control; and iii) we propose a novel metric S2 MATCH that is more benevolent to only very slight meaning deviations and targets the f"
2020.tacl-1.34,P17-1014,0,0.0930406,"Missing"
2020.tacl-1.34,D14-1162,0,0.0820966,"Missing"
2020.tacl-1.34,P18-1037,0,0.379604,"represented as variables that are linked to corresponding concepts (encoded as leaf nodes) via is-instance relations (cf. Figure 1, left). This structure allows us to capture complex linguistic phenomena such as coreference, semantic roles, or polarity. When measuring the similarity between two AMR graphs A and B , for instance for the purpose of AMR parse quality evaluation, the metric of choice is usually SMATCH (Cai and Knight, 2013). Its backbone is an alignment-search be1 Most research papers on AMR display the graphs in this ‘‘shallow’’ form. This increases simplicity and readability. (Lyu and Titov, 2018; Konstas et al., 2017; Zhang et al., 2019; Damonte and Cohen, 2019; Song et al., 2016). 522 Transactions of the Association for Computational Linguistics, vol. 8, pp. 522–538, 2020. https://doi.org/10.1162/tacl a 00329 Action Editor: Adam Lopez. Submission batch: 11/2019; Revision batch: 3/2020; Published 9/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. the following constraint on metric: D × D → [0, 1].2 II. identity of indiscernibles This focal principle is formalized by metric(A, B ) = 1 ⇔ A = B . It is violated if a metric assigns a value in"
2020.tacl-1.34,P18-1150,0,0.09268,"Missing"
2020.tacl-1.34,S16-1181,0,0.0939778,"Missing"
2020.tacl-1.34,P19-1446,0,0.301148,"ncepts by lemmatization, hence two graphs for the same sentence tend not to disagree on the concepts projected from the input. This is different in semantic parsing where the projected concepts are often more abstract. This article is structured as follows: We first establish seven principles that one may expect a metric for comparing meaning representations to Different metrics have been proposed to compare Abstract Meaning Representation (AMR) graphs. The canonical SMATCH metric (Cai and Knight, 2013) aligns the variables of two graphs and assesses triple matches. The recent SEMBLEU metric (Song and Gildea, 2019) is based on the machine-translation metric BLEU (Papineni et al., 2002) and increases computational efficiency by ablating the variablealignment. In this paper, i) we establish criteria that enable researchers to perform a principled assessment of metrics comparing meaning representations like AMR; ii) we undertake a thorough analysis of SMATCH and SEMBLEU where we show that the latter exhibits some undesirable properties. For example, it does not conform to the identity of indiscernibles rule and introduces biases that are hard to control; and iii) we propose a novel metric S2 MATCH that is"
2020.tacl-1.34,P15-2141,0,0.0209567,"operties of SMATCH but is benevolent to slight lexical meaning deviations. Besides parser evaluation, this property makes the metric also more suitable for other tasks, for example, it can be used as a kernel in an SVM that classifies AMRs to determine whether two sentences are equivalent in meaning. In such a case, S2 MATCH is bound to detect meaningsimilarities that cannot be captured by SMATCH or SEMBLEU, for example, due to paraphrases being projected into the parses. Computational AMR tasks Since the introduction of AMR, many AMR-related tasks have emerged. Most prominent is AMR parsing (Wang et al., 2015, 2016; Damonte et al., 2017; Konstas et al., 2017; Lyu and Titov, 2018; Zhang et al., 2019). The inverse task generates text from AMR graphs (Song et al., 2017, 2018; Damonte and Cohen, 2019). Opitz and Frank (2019) rate the quality of automatic AMR parses without costly gold data. 8 Conclusion We motivated seven principles for metrics measuring the similarity of graph-based (Abstract) Meaning Representations, from mathematical, linguistic and engineering perspectives. A metric that fulfills all principles is applicable to a wide spectrum of cases, ranging from parser evaluation to sound IAA"
2020.tacl-1.34,P17-2002,0,0.0156303,"r tasks, for example, it can be used as a kernel in an SVM that classifies AMRs to determine whether two sentences are equivalent in meaning. In such a case, S2 MATCH is bound to detect meaningsimilarities that cannot be captured by SMATCH or SEMBLEU, for example, due to paraphrases being projected into the parses. Computational AMR tasks Since the introduction of AMR, many AMR-related tasks have emerged. Most prominent is AMR parsing (Wang et al., 2015, 2016; Damonte et al., 2017; Konstas et al., 2017; Lyu and Titov, 2018; Zhang et al., 2019). The inverse task generates text from AMR graphs (Song et al., 2017, 2018; Damonte and Cohen, 2019). Opitz and Frank (2019) rate the quality of automatic AMR parses without costly gold data. 8 Conclusion We motivated seven principles for metrics measuring the similarity of graph-based (Abstract) Meaning Representations, from mathematical, linguistic and engineering perspectives. A metric that fulfills all principles is applicable to a wide spectrum of cases, ranging from parser evaluation to sound IAA calculation. Hence (i) our principles can inform (A)MR researchers who desire to compare and select among metrics, and (ii) they ease and guide the development"
2020.tacl-1.34,D16-1224,0,0.0176045,"es) via is-instance relations (cf. Figure 1, left). This structure allows us to capture complex linguistic phenomena such as coreference, semantic roles, or polarity. When measuring the similarity between two AMR graphs A and B , for instance for the purpose of AMR parse quality evaluation, the metric of choice is usually SMATCH (Cai and Knight, 2013). Its backbone is an alignment-search be1 Most research papers on AMR display the graphs in this ‘‘shallow’’ form. This increases simplicity and readability. (Lyu and Titov, 2018; Konstas et al., 2017; Zhang et al., 2019; Damonte and Cohen, 2019; Song et al., 2016). 522 Transactions of the Association for Computational Linguistics, vol. 8, pp. 522–538, 2020. https://doi.org/10.1162/tacl a 00329 Action Editor: Adam Lopez. Submission batch: 11/2019; Revision batch: 3/2020; Published 9/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. the following constraint on metric: D × D → [0, 1].2 II. identity of indiscernibles This focal principle is formalized by metric(A, B ) = 1 ⇔ A = B . It is violated if a metric assigns a value indicating equivalence to inputs that are not equivalent or if it considers equivalent in"
2020.tacl-1.34,P19-1009,0,0.051171,"Missing"
2020.tacl-1.34,N15-1100,0,\N,Missing
2020.tacl-1.34,D19-1393,0,\N,Missing
2021.acl-long.395,D13-1178,0,0.0850904,"• SomeoneA wants to go to SomewhereB (to theatre) • SomeoneA possess(es) a phone. • SomeoneB wants SomeoneA to work. Missing Sentence: Cause • SomeoneA wasn’t able to go SomewhereB (to see the play) Effect and Cause S3: Janie’s boss gave her new work. S4: Janie couldn’t attend her sisters’ play Figure 1: An example of the Narrative Story Completion Task. Top and bottom boxes show the context (top) and missing sentences (bottom). The chain of implicit inference rules explains the connection between beginning and end, and allows to infer the missing sentences. tics (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Nguyen et al., 2015). Recently, large pretrained language models (LMs) such as GPT-2 have shown remarkable performance on various generation tasks. While these pretrained LMs learn probabilistic associations between words and sentences, they still have difficulties in modeling causality (Mostafazadeh et al., 2020). Also, in narrative story generation, models need to be consistent with everyday commonsense norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reasoning methods, and ideally iii"
2021.acl-long.395,P19-1470,0,0.0286765,"ode is made publicly available.4 2 Related Work Sentence-level Commonsense Inference and Beyond. Recent research in this area has focused on commonsense knowledge acquisition (Sap et al., 2019; Zhang et al., 2020; Speer et al., 2017; Malaviya et al., 2020) and commonsense reasoning (Zellers et al., 2019; Talmor et al., 2018). In our work, we focus on inferential knowledge about events, and entities participating in such events. Rashkin et al. (2018b) introduced a knowledge resource of commonsense inferences regarding people’s intents and reactions towards a diverse set of events. With C OMET, Bosselut et al. (2019) have shown that pre-trained neural language models can be fine-tuned using large knowledge bases (such as ATOMIC, Sap et al. (2019)) to generate inferences for a given event or sentence. However, the generated knowledge from C OMET is noncontextualized and hence, can be inconsistent. Recently, Mostafazadeh et al. (2020) proposed G LU COSE , a new resource and dataset that offers semistructured commonsense inference rules that are grounded in sentences of specific stories. They show that fine-tuning a pre-trained LM on the G LUCOSE dataset helps the model to better generate inferrable commonse"
2021.acl-long.395,P08-1090,0,0.0984157,"es for an incomplete story. Learning to predict iterative inference steps for successive events in a narration using semi-structured knowledge rules is still a difficult and underexplored task. We propose a model that learns to iteratively generate a coherent completion of an incomplete narrative story utilizing semi-structured knowledge as offered by the G LUCOSE framework. Commonsense Reasoning in Narrative Stories. Early work on narrative events focused on script learning, by defining stereotypical event sequences together with their participants (Schank and Abelson, 1977). In later works, Chambers and Jurafsky (2008, 2009); Balasubramanian et al. (2013); Nguyen et al. (2015); Pichotta and Mooney (2014) proposed methods to learn narrative event chains using a simpler event representation that allows for efficient learning and inference. Chambers and Jurafsky (2009) acquired Narrative Event Schemata from corpora and established the Narrative Cloze Task (Chambers and Jurafsky, 2008) that evaluates script knowledge by predicting a missing event (verb and its arguments) in a sequence of observed events. More recently, Mostafazadeh et al. (2016) proposed the story cloze task that selects a plausible (right) ov"
2021.acl-long.395,P09-1068,0,0.302614,"ect Implicit Inference Rules • SomeoneA wants to go to SomewhereB (to theatre) • SomeoneA possess(es) a phone. • SomeoneB wants SomeoneA to work. Missing Sentence: Cause • SomeoneA wasn’t able to go SomewhereB (to see the play) Effect and Cause S3: Janie’s boss gave her new work. S4: Janie couldn’t attend her sisters’ play Figure 1: An example of the Narrative Story Completion Task. Top and bottom boxes show the context (top) and missing sentences (bottom). The chain of implicit inference rules explains the connection between beginning and end, and allows to infer the missing sentences. tics (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Nguyen et al., 2015). Recently, large pretrained language models (LMs) such as GPT-2 have shown remarkable performance on various generation tasks. While these pretrained LMs learn probabilistic associations between words and sentences, they still have difficulties in modeling causality (Mostafazadeh et al., 2020). Also, in narrative story generation, models need to be consistent with everyday commonsense norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reas"
2021.acl-long.395,2020.emnlp-main.54,0,0.728889,"rk on story generation relied on symbolic plan´ ning methods (Lebowitz, 1987; PErez and Sharples, 2001; J´ozefowicz et al., 2016). With the advances of Seq2Seq models, several works applied them in automatic story generation tasks (Roemmele, 2016; Jain et al., 2017). Fan et al. (2018) proposed a hierarchical approach to generate short stories from initial prompts. Recently, many works have focused on integrating external commonsense knowledge from large static knowledge bases like ATOMIC (Sap et al., 2019) or ConceptNet (Speer et al., 2017) for different tasks such as story ending generation (Ji et al., 2020; Guan et al., 2019) or story generation (Guan et al., 2020; Xu et al., 2020). In concurrent work, Ammanabrolu et al. (2021) look into causality for a commonsense plot generation task. In our work, we model the assumption that contextualized inference rules provide inferred information that can guide a system in generating both contextually grounded and coherent follow-up sentences in a story generation task. 3 Task Definition We formulate the Narrative Story Completion task (NSC) as follows: given an incomplete story (S= s1 , s2 , sn ) as a sequence of tokens t = {t1 , t2 , ..., tSEP , ..., t"
2021.acl-long.395,N16-1014,0,0.0846618,"Missing"
2021.acl-long.395,W04-1013,0,0.0481441,"Missing"
2021.acl-long.395,D15-1166,0,0.0764169,"Missing"
2021.acl-long.395,1985.tmi-1.17,0,0.102479,"se norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reasoning methods, and ideally iii) we want to be able to make the effectiveness of these methods transparent. Introduction Narrative story understanding, and similarly story generation, requires the ability to construe meaning that is not explicitly stated through commonsense reasoning over events in the story (Rashkin et al., 2018a). Previous work in modeling narrative stories has focused on learning scripts1 (Schank and Abelson, 1977; Mooney and DeJong, 1985) and learning narrative schemas using corpus statis1 Scripts are structured knowledge about stereotypical event sequences together with their participants. In this work we focus on the aspects i) to iii), by investigating new methods that build on pretrained LMs to generate missing sentences from an incomplete narrative story. Specifically, we focus on Narrative Story Completion (NSC), a new task setting for story generation. Given an incomplete story, specified only through its beginning and ending, the task is to generate the missing sentences to complete the story (see Figure 1). Our hypoth"
2021.acl-long.395,N16-1098,0,0.0980574,"h their participants (Schank and Abelson, 1977). In later works, Chambers and Jurafsky (2008, 2009); Balasubramanian et al. (2013); Nguyen et al. (2015); Pichotta and Mooney (2014) proposed methods to learn narrative event chains using a simpler event representation that allows for efficient learning and inference. Chambers and Jurafsky (2009) acquired Narrative Event Schemata from corpora and established the Narrative Cloze Task (Chambers and Jurafsky, 2008) that evaluates script knowledge by predicting a missing event (verb and its arguments) in a sequence of observed events. More recently, Mostafazadeh et al. (2016) proposed the story cloze task that selects a plausible (right) over an implausible (wrong) story ending. Bhagavatula et al. (2020) proposed an abductive reasoning task to test a model’s ability to generate plausible explanations for an incomplete set of observations. Paul and Frank (2020) proposed a multi-head knowledge attention method to dynamically incorporate non-contextualized inferential knowledge to address the abductive reasoning task. Qin et al. (2020) proposed an unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf"
2021.acl-long.395,2020.emnlp-main.370,0,0.439822,"the Narrative Story Completion Task. Top and bottom boxes show the context (top) and missing sentences (bottom). The chain of implicit inference rules explains the connection between beginning and end, and allows to infer the missing sentences. tics (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Nguyen et al., 2015). Recently, large pretrained language models (LMs) such as GPT-2 have shown remarkable performance on various generation tasks. While these pretrained LMs learn probabilistic associations between words and sentences, they still have difficulties in modeling causality (Mostafazadeh et al., 2020). Also, in narrative story generation, models need to be consistent with everyday commonsense norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reasoning methods, and ideally iii) we want to be able to make the effectiveness of these methods transparent. Introduction Narrative story understanding, and similarly story generation, requires the ability to construe meaning that is not explicitly stated through commonsense reasoning over events in the story (Rashkin et al., 2018a). Previous work"
2021.acl-long.395,P15-1019,0,0.0517421,"Missing"
2021.acl-long.395,P02-1040,0,0.108845,"Missing"
2021.acl-long.395,2020.findings-emnlp.267,1,0.732627,"arning and inference. Chambers and Jurafsky (2009) acquired Narrative Event Schemata from corpora and established the Narrative Cloze Task (Chambers and Jurafsky, 2008) that evaluates script knowledge by predicting a missing event (verb and its arguments) in a sequence of observed events. More recently, Mostafazadeh et al. (2016) proposed the story cloze task that selects a plausible (right) over an implausible (wrong) story ending. Bhagavatula et al. (2020) proposed an abductive reasoning task to test a model’s ability to generate plausible explanations for an incomplete set of observations. Paul and Frank (2020) proposed a multi-head knowledge attention method to dynamically incorporate non-contextualized inferential knowledge to address the abductive reasoning task. Qin et al. (2020) proposed an unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf language models to generate plausible explanations. Concurrent to our work, Paul and Frank (2021) presented a method for addressing the abductive reasoning task by explicitly learning what events could follow other events in a hypothetical scenario. In our work, we make use of the ROCStori"
2021.acl-long.395,2021.starsem-1.6,1,0.759204,"ausible (wrong) story ending. Bhagavatula et al. (2020) proposed an abductive reasoning task to test a model’s ability to generate plausible explanations for an incomplete set of observations. Paul and Frank (2020) proposed a multi-head knowledge attention method to dynamically incorporate non-contextualized inferential knowledge to address the abductive reasoning task. Qin et al. (2020) proposed an unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf language models to generate plausible explanations. Concurrent to our work, Paul and Frank (2021) presented a method for addressing the abductive reasoning task by explicitly learning what events could follow other events in a hypothetical scenario. In our work, we make use of the ROCStories dataset (Mostafazadeh et al., 2016) to build a Narrative Story Completion task that tests a model’s ability of generating missing sentences in a story. We propose a model that aims to produce coherent narrative stories by performing iterative commonsense inference steps. Narrative Story Generation. Much existing work on story generation relied on symbolic plan´ ning methods (Lebowitz, 1987; PErez and"
2021.acl-long.395,P18-1213,0,0.140685,"causality (Mostafazadeh et al., 2020). Also, in narrative story generation, models need to be consistent with everyday commonsense norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reasoning methods, and ideally iii) we want to be able to make the effectiveness of these methods transparent. Introduction Narrative story understanding, and similarly story generation, requires the ability to construe meaning that is not explicitly stated through commonsense reasoning over events in the story (Rashkin et al., 2018a). Previous work in modeling narrative stories has focused on learning scripts1 (Schank and Abelson, 1977; Mooney and DeJong, 1985) and learning narrative schemas using corpus statis1 Scripts are structured knowledge about stereotypical event sequences together with their participants. In this work we focus on the aspects i) to iii), by investigating new methods that build on pretrained LMs to generate missing sentences from an incomplete narrative story. Specifically, we focus on Narrative Story Completion (NSC), a new task setting for story generation. Given an incomplete story, specified o"
2021.acl-long.395,P18-1043,0,0.350349,"causality (Mostafazadeh et al., 2020). Also, in narrative story generation, models need to be consistent with everyday commonsense norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reasoning methods, and ideally iii) we want to be able to make the effectiveness of these methods transparent. Introduction Narrative story understanding, and similarly story generation, requires the ability to construe meaning that is not explicitly stated through commonsense reasoning over events in the story (Rashkin et al., 2018a). Previous work in modeling narrative stories has focused on learning scripts1 (Schank and Abelson, 1977; Mooney and DeJong, 1985) and learning narrative schemas using corpus statis1 Scripts are structured knowledge about stereotypical event sequences together with their participants. In this work we focus on the aspects i) to iii), by investigating new methods that build on pretrained LMs to generate missing sentences from an incomplete narrative story. Specifically, we focus on Narrative Story Completion (NSC), a new task setting for story generation. Given an incomplete story, specified o"
2021.acl-long.395,E14-1024,0,0.0223467,"events in a narration using semi-structured knowledge rules is still a difficult and underexplored task. We propose a model that learns to iteratively generate a coherent completion of an incomplete narrative story utilizing semi-structured knowledge as offered by the G LUCOSE framework. Commonsense Reasoning in Narrative Stories. Early work on narrative events focused on script learning, by defining stereotypical event sequences together with their participants (Schank and Abelson, 1977). In later works, Chambers and Jurafsky (2008, 2009); Balasubramanian et al. (2013); Nguyen et al. (2015); Pichotta and Mooney (2014) proposed methods to learn narrative event chains using a simpler event representation that allows for efficient learning and inference. Chambers and Jurafsky (2009) acquired Narrative Event Schemata from corpora and established the Narrative Cloze Task (Chambers and Jurafsky, 2008) that evaluates script knowledge by predicting a missing event (verb and its arguments) in a sequence of observed events. More recently, Mostafazadeh et al. (2016) proposed the story cloze task that selects a plausible (right) over an implausible (wrong) story ending. Bhagavatula et al. (2020) proposed an abductive"
2021.acl-long.395,2020.acl-tutorials.7,0,0.0143185,"ayer, 768-hidden, 12-heads), trained with an objective to predict the next word. The input to the GPT-2 model is the concatenation of the source and the target story sequence. We follow the standard procedure to fine-tune GPT-2 on the NSC task during training and minimize the loss function: −log(s3 , s4 |[SOS]s1 , s2 , [SEP ], s5 [EOS]) (4) (b) Knowledge-Enhanced GPT-2 (KE) (Guan et al., 2020) is the current SOTA for ROCStories generation. It first fine-tunes a pre-trained GPT-2 (small) model with knowledge triples from commonsense datasets (ConceptNet [CN] Speer et al. (2017) and ATOMIC [AT] Sap et al. (2020)). The knowledge triples were converted to sentences using templates. A multitask learning framework further fine-tunes this model on both the Story Ending Generation task and classifying corrupted stories from real ones. As our baseline we choose the version without multi-tasking, since the corrupted story setting is not applicable for the NSC task. (c) GRF (Ji et al., 2020) is the current SOTA for the Abductive Reasoning and the Story Ending Generation tasks. GRF enables pre-trained models (GPT-2 small) with dynamic multi-hop reasoning on multi-relational paths extracted from the external Co"
2021.acl-long.395,2020.emnlp-main.58,0,0.0439167,"Missing"
2021.acl-long.395,2020.emnlp-main.226,0,0.0350062,"; PErez and Sharples, 2001; J´ozefowicz et al., 2016). With the advances of Seq2Seq models, several works applied them in automatic story generation tasks (Roemmele, 2016; Jain et al., 2017). Fan et al. (2018) proposed a hierarchical approach to generate short stories from initial prompts. Recently, many works have focused on integrating external commonsense knowledge from large static knowledge bases like ATOMIC (Sap et al., 2019) or ConceptNet (Speer et al., 2017) for different tasks such as story ending generation (Ji et al., 2020; Guan et al., 2019) or story generation (Guan et al., 2020; Xu et al., 2020). In concurrent work, Ammanabrolu et al. (2021) look into causality for a commonsense plot generation task. In our work, we model the assumption that contextualized inference rules provide inferred information that can guide a system in generating both contextually grounded and coherent follow-up sentences in a story generation task. 3 Task Definition We formulate the Narrative Story Completion task (NSC) as follows: given an incomplete story (S= s1 , s2 , sn ) as a sequence of tokens t = {t1 , t2 , ..., tSEP , ..., tm } (with tSEP a mask token delimiting s2 and sn ), the goal is to generate t"
2021.acl-long.395,P19-1472,0,0.0623432,"Missing"
2021.alvr-1.4,W19-8621,0,0.0114278,"enchmarks for training and evaluating PG systems (Everingham et al., 2010; Lin et al., 2014; Kazemzadeh et al., 2014; Plummer et al., 2015; Krishna et al., 2017) generally provide rectangular bounding boxes as ground truth (GT). Therefore a PG ground truth is represented as a phrase linking to a (gold) bounding box enclosing the image patch referred to by the phrase. Some datasets provide pixel segmentation masks (Lin et al., 2014), which enable more precise evaluations but are more difficult and costly to produce. Thus, the trend towards annotating bounding boxes persists in recent datasets (Ilinykh et al., 2019). Plural phrases describe multiple entities in an image, either through a collective term (e.g. crowd) or a plural form (e.g. two children). Depending on the annotation, the gold box consists either of a single box enclosing all entities or several component boxes representing the individual entities. By convention1 , component boxes are merged into one union box spanning all individual boxes, functioning as a single gold box. Figure (2.a) gives an example of a union box for a plural phrase with two components. This reduction of multiple boxes to a single union box is widely established in PG"
2021.alvr-1.4,D14-1086,0,0.0386428,"ction and union. Introduction Phrase grounding (PG) describes the multimodal task of identifying objects in images and connecting them to free-form phrases in a textual description (caption). A phrase usually describes one, or sometimes several, specific objects. Grounding phrases in image regions provides an essential link between texts and images and serves as a foundation for multimodal understanding tasks, including sentence-to-image alignment, Visual QA, Visual Common-sense Reasoning (VCR), etc. Benchmarks for training and evaluating PG systems (Everingham et al., 2010; Lin et al., 2014; Kazemzadeh et al., 2014; Plummer et al., 2015; Krishna et al., 2017) generally provide rectangular bounding boxes as ground truth (GT). Therefore a PG ground truth is represented as a phrase linking to a (gold) bounding box enclosing the image patch referred to by the phrase. Some datasets provide pixel segmentation masks (Lin et al., 2014), which enable more precise evaluations but are more difficult and costly to produce. Thus, the trend towards annotating bounding boxes persists in recent datasets (Ilinykh et al., 2019). Plural phrases describe multiple entities in an image, either through a collective term (e.g."
2021.alvr-1.4,2020.splu-1.4,0,0.0107619,"that the detected hands constitute an incorrect prediction for juggling pins (blue tick). i) We detect, describe and quantify an evaluation bias in the grounding of plural phrases when applying standard practice of measuring IoU over union boxes, using an unsupervised PG system on the PG dataset Flickr30k. ii) We propose a novel evaluation metric based on component boxes rather than union boxes. iii) We show that the new metric alleviates this bias and reduces the evaluation failures. quality mappings for all phrase types. However, the annotation of plural phrases is challenging, as shown in Testoni et al. (2020); Marín et al. (2020) who investigate how phrases can refer to groups of objects or several entities within a group. (Semi-)supervised PG systems generally do not differentiate singular and plural phrases and always predict a single box (Li et al., 2019; Lu et al., 2020; Plummer et al., 2015). For multiple boxes with the same predicted label, either the largest box or the union box is returned. Thus, components are not individually evaluated, and the same metric, Intersection over Union (IoU), can be uniformly applied to a prediction box for any phrase type. IoU computes the ratio of the area"
2021.argmining-1.3,W17-5112,0,0.0260673,"s conclusions and argument similarity ratings: we i) assess predictors of human argument similarity ratings to investigate the criteria that correlate with human ratings of argument similarity; ii) discuss potential advantages of using AMR for graph-based argumentation tasks in a concrete example, and iii) investigate how interpretable argument similarity computation can help assess the quality and usefulness of conclusions drawn from arguments in a reference-less conclusion evaluation setup.1 2 the analysis of linguistic aspects (Lauscher et al., 2021), e.g., by extracting selected features (Aker et al., 2017; Lugini and Litman, 2018) or leveraging discourse knowledge in language models (Opitz, 2019), others exploit large background knowledge graphs (Kobbe et al., 2019; Paul et al., 2020; Yuan et al., 2021) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2017) or DBpedia (Mendes et al., 2012). An advantage of our approach is the explicit graph alignment between two arguments’ meaning graphs that better marks related structures, and that can help explain argument similarity judgements. Related work Argument similarity and search Assessing argument similarity is a key task in argument mining"
2021.argmining-1.3,D17-1070,0,0.0255921,"2 MATCHStruct . The first metric variant focuses on conceptual overlap (Fig. 1, middle), i.e. the more concrete semantic aspects, by putting a triple weight on concept matches. The second variant focuses on structural matches (Fig. 1, bottom), i.e., the more abstract semantic aspects, by putting triple weight on relation matches. Baselines We compare to previously established unsupervised baselines (Reimers et al., 2019): i) Tfidf calculates cosine similarity between Tfidf-weighted bag-of-word vectors; i) InferSent(FastText|Glove) leverages sentence embeddings produced by the InferSent model (Conneau et al., 2017) based on either FastText (Bojanowski et al., 2016) or GloVe (Pennington et al., 2014) vectors, which are compared with cosine similarity; iii) (GloVe|ELMo|BERT) Embedding uses averaged GloVe embeddings or averaged contextualized embeddings from ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) language models. 5.2 Results Best system Table 1 shows our main results. The AMR-based approach that is based on conceptfocused S2 MATCH scores, taking both the argument and its inferred conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT ba"
2021.argmining-1.3,E17-1051,0,0.208134,"alcohol leads to depression vs. depression leads to consumption of alcohol are clearly distinct, while sharing the same concepts. Other AMR facets may also be useful. E.g., AMR captures coreferences and resolving them in different ways can induce significant meaning differences, Finally, AMR includes key semantic relations (location, cause, possession, etc.) that are often implicit or underspecified in language, hence their explicit representation in AMR provides a rich basis for assessing arguments. Arguments represented with AMR can be compared with AMR graph metrics (Cai and Knight, 2013; Damonte et al., 2017; Opitz et al., 2020) that also induce an explicit alignment between two argument graphs. As a complementary example, the similarity of two arguments may be reinforced by the similarity of their inferred conclusions, as shown below: i) Fracking can contaminate water and water wells and suck towns dry. ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems. Arguments i) and ii) are rated as similar, presumably because they point at detrimental ramifications of fracking related to water issues. This similarity is likely to be refl"
2021.argmining-1.3,2021.eacl-main.17,0,0.0947016,"Missing"
2021.argmining-1.3,N19-1423,0,0.0048565,"weight on relation matches. Baselines We compare to previously established unsupervised baselines (Reimers et al., 2019): i) Tfidf calculates cosine similarity between Tfidf-weighted bag-of-word vectors; i) InferSent(FastText|Glove) leverages sentence embeddings produced by the InferSent model (Conneau et al., 2017) based on either FastText (Bojanowski et al., 2016) or GloVe (Pennington et al., 2014) vectors, which are compared with cosine similarity; iii) (GloVe|ELMo|BERT) Embedding uses averaged GloVe embeddings or averaged contextualized embeddings from ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) language models. 5.2 Results Best system Table 1 shows our main results. The AMR-based approach that is based on conceptfocused S2 MATCH scores, taking both the argument and its inferred conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT baseline. The difference is significant with p &lt; 0.005 (Student t-test). This system is closely followed by other AMR-based systems, e.g., using concept-focused S2 MATCH that sees only the argument (68.17 macro Conclusion generator We generate conclusions from arguments using the T5 model (Raffel et al.,"
2021.argmining-1.3,2020.acl-main.399,0,0.0892293,"as mechanisms for explaining functions, strengths and weaknesses of arguments. Other research aims at studying the computational and formal aspects of argumentation, e.g. abstract argumentation (Dung, 1995) and Bayesian argumentation (Zenker, 2013). Research in empirical argument mining led researchers to investigate practical methods for explanations (Lawrence, 2021; Becker et al., 2021; Gunning et al., 2019; Rago et al., 2021; Vassiliades et al., 2021). While most approaches focus on Generation of argumentative conclusions The task of conclusion generation has been recently investigated by Alshomary et al. (2020, 2021), and allows us to infer conclusions from given premises. Conclusion generation can be seen as the inverse of argument generation (Sato et al., 2015; Schiller et al., 2020). In this work, we show that by considering conclusions inferred from pairs of arguments, we can improve our argument similarity ratings. 3 Hypotheses We base our models for explanatory argument similarity assessment on two hypotheses. Hypothesis I: Abstract Meaning Representation (Banarescu et al., 2013) of arguments supports explainable argument similarity assessment AMRs are directed, rooted and acyclic 1 https://g"
2021.argmining-1.3,W13-2322,0,0.283129,"ers et al., 2019; Lenz et al., 2019) and can enhance argument search (Maturana, 1988; Rissland et al., 1993; Wachsmuth et al., 2017; Ajjour et al., 2019; Chesnevar and Maguitman, 2004). Yet, while delivering solid performance on benchmarks, current methods fail to provide any deeper rationale for their predictions. It is thus not clear whether and to what extent spurious clues or other artifacts may influence the similarity decision (Opitz and Frank, 2019; Niven and Kao, 2019). In this paper, we aim at alleviating these issues by i) representing arguments with Abstract Meaning Representation (Banarescu et al., 2013) and conducting similarity assessment using well-defined graph metrics that provide explanatory AMR structure alignments; and ii) by investigating to what extent argument similarity can be projected to inferred conclusions. Argument mining with graphs There is growing interest in extracting graph structures from natural language arguments. Lenz et al. (2020), e.g., propose a pipeline for detecting and linking argumentative discourse units (ADUs). Al-Khatib et al. (2020) detect textual phrases and link them with POS/NEG relations, where POS indicates a positive influence and NEG a negative infl"
2021.argmining-1.3,2021.deelio-1.2,1,0.65493,".g., offers a theory of what is needed to make an argument complete (Toulmin, 2003). Argumentation schemes, which develop taxonomies of argument types and argumentation fallacies (Walton, 2005; Walton et al., 2008) can be viewed as mechanisms for explaining functions, strengths and weaknesses of arguments. Other research aims at studying the computational and formal aspects of argumentation, e.g. abstract argumentation (Dung, 1995) and Bayesian argumentation (Zenker, 2013). Research in empirical argument mining led researchers to investigate practical methods for explanations (Lawrence, 2021; Becker et al., 2021; Gunning et al., 2019; Rago et al., 2021; Vassiliades et al., 2021). While most approaches focus on Generation of argumentative conclusions The task of conclusion generation has been recently investigated by Alshomary et al. (2020, 2021), and allows us to infer conclusions from given premises. Conclusion generation can be seen as the inverse of argument generation (Sato et al., 2015; Schiller et al., 2020). In this work, we show that by considering conclusions inferred from pairs of arguments, we can improve our argument similarity ratings. 3 Hypotheses We base our models for explanatory argu"
2021.argmining-1.3,P13-2131,0,0.224136,"claims: consumption of alcohol leads to depression vs. depression leads to consumption of alcohol are clearly distinct, while sharing the same concepts. Other AMR facets may also be useful. E.g., AMR captures coreferences and resolving them in different ways can induce significant meaning differences, Finally, AMR includes key semantic relations (location, cause, possession, etc.) that are often implicit or underspecified in language, hence their explicit representation in AMR provides a rich basis for assessing arguments. Arguments represented with AMR can be compared with AMR graph metrics (Cai and Knight, 2013; Damonte et al., 2017; Opitz et al., 2020) that also induce an explicit alignment between two argument graphs. As a complementary example, the similarity of two arguments may be reinforced by the similarity of their inferred conclusions, as shown below: i) Fracking can contaminate water and water wells and suck towns dry. ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems. Arguments i) and ii) are rated as similar, presumably because they point at detrimental ramifications of fracking related to water issues. This similarit"
2021.argmining-1.3,W19-4503,1,0.940331,"inisch2 1 Anette Frank1 lastname@cl.uni-heidelberg.de, {pheinisch,cimiano}@techfak.uni-bielefeld.de Abstract language models such as BERT (Devlin et al., 2019) or InferSent (Conneau et al., 2017). Two key advantages of such approaches are due to their unsupervised setup: First, unsupervised methods do not rely on human annotations, which are expensive and can be subject to noise and biases. Second, it has been shown for previous supervised methods that they have learned less about argumentation tasks than had been assumed, by exploiting spurious clues and artifacts from manually created data (Opitz and Frank, 2019; Niven and Kao, 2019). This has led to a recent interest in solving argumentation tasks in an unsupervised manner, e.g., by logical reasoning (Jo et al., 2021). In this paper we will highlight that previous methods for rating argument similarity suffer from a common flaw: beyond shallow statistics (word matches in bag-of-word models, or word similarities in distributional space), they do not provide any rationale for their predictions, and the prediction process is in general not transparent. Therefore, we know only little about the following question: When assessing the similarity of argumen"
2021.argmining-1.3,2021.eacl-main.129,1,0.841402,"Missing"
2021.argmining-1.3,2020.tacl-1.34,1,0.924772,"ession vs. depression leads to consumption of alcohol are clearly distinct, while sharing the same concepts. Other AMR facets may also be useful. E.g., AMR captures coreferences and resolving them in different ways can induce significant meaning differences, Finally, AMR includes key semantic relations (location, cause, possession, etc.) that are often implicit or underspecified in language, hence their explicit representation in AMR provides a rich basis for assessing arguments. Arguments represented with AMR can be compared with AMR graph metrics (Cai and Knight, 2013; Damonte et al., 2017; Opitz et al., 2020) that also induce an explicit alignment between two argument graphs. As a complementary example, the similarity of two arguments may be reinforced by the similarity of their inferred conclusions, as shown below: i) Fracking can contaminate water and water wells and suck towns dry. ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems. Arguments i) and ii) are rated as similar, presumably because they point at detrimental ramifications of fracking related to water issues. This similarity is likely to be reflected in conclusions"
2021.argmining-1.3,W18-5208,0,0.0205579,"rgument similarity ratings: we i) assess predictors of human argument similarity ratings to investigate the criteria that correlate with human ratings of argument similarity; ii) discuss potential advantages of using AMR for graph-based argumentation tasks in a concrete example, and iii) investigate how interpretable argument similarity computation can help assess the quality and usefulness of conclusions drawn from arguments in a reference-less conclusion evaluation setup.1 2 the analysis of linguistic aspects (Lauscher et al., 2021), e.g., by extracting selected features (Aker et al., 2017; Lugini and Litman, 2018) or leveraging discourse knowledge in language models (Opitz, 2019), others exploit large background knowledge graphs (Kobbe et al., 2019; Paul et al., 2020; Yuan et al., 2021) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2017) or DBpedia (Mendes et al., 2012). An advantage of our approach is the explicit graph alignment between two arguments’ meaning graphs that better marks related structures, and that can help explain argument similarity judgements. Related work Argument similarity and search Assessing argument similarity is a key task in argument mining (Reimers et al., 2019; Len"
2021.argmining-1.3,D14-1162,0,0.0855337,"ith 4 folds. In every iteration, 7 topics serve as testing data, while the other 21 topics serve to tune a decision threshold of the metric score.6 As in Reimers et al. (2019), we evaluate the F1 score for each of the two labels and the arithmetic F1 mean (macro F1). ARG1 ARG1 water endanger-01 ARG1 ARG1 ARG0 fracking ecosystem Figure 1: Standard, concept-focus and structure focus. AMR metric We use S2 MATCH (Opitz et al., 2020), which is based on the AMR graph matching metric S MATCH (Cai and Knight, 2013), but admits graded concept similarity by matching concept nodes with GloVe embeddings (Pennington et al., 2014) and cosine similarity4 . To find an optimal graph mapping, exactly like S MATCH, it leverages a hill-climber to approximate the NPhard problem of aligning AMR graphs. Following the alignment step, the (soft) matching of propositions (triples) are scored with an F1 score. Since, so-far, little is known about the trade-off and interface between concrete and abstract semantics in human mental representations (Mkrtychian et al., 2019), we introduce two more variants that assess similarity from complementary perspectives: S2 MATCHConcept and S2 MATCHStruct . The first metric variant focuses on con"
2021.argmining-1.3,N18-1202,0,0.00760878,"tic aspects, by putting triple weight on relation matches. Baselines We compare to previously established unsupervised baselines (Reimers et al., 2019): i) Tfidf calculates cosine similarity between Tfidf-weighted bag-of-word vectors; i) InferSent(FastText|Glove) leverages sentence embeddings produced by the InferSent model (Conneau et al., 2017) based on either FastText (Bojanowski et al., 2016) or GloVe (Pennington et al., 2014) vectors, which are compared with cosine similarity; iii) (GloVe|ELMo|BERT) Embedding uses averaged GloVe embeddings or averaged contextualized embeddings from ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) language models. 5.2 Results Best system Table 1 shows our main results. The AMR-based approach that is based on conceptfocused S2 MATCH scores, taking both the argument and its inferred conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT baseline. The difference is significant with p &lt; 0.005 (Student t-test). This system is closely followed by other AMR-based systems, e.g., using concept-focused S2 MATCH that sees only the argument (68.17 macro Conclusion generator We generate conclusions from arguments usi"
2021.argmining-1.3,mendes-etal-2012-dbpedia,0,0.0171408,"Missing"
2021.argmining-1.3,P19-1459,0,0.306852,"lastname@cl.uni-heidelberg.de, {pheinisch,cimiano}@techfak.uni-bielefeld.de Abstract language models such as BERT (Devlin et al., 2019) or InferSent (Conneau et al., 2017). Two key advantages of such approaches are due to their unsupervised setup: First, unsupervised methods do not rely on human annotations, which are expensive and can be subject to noise and biases. Second, it has been shown for previous supervised methods that they have learned less about argumentation tasks than had been assumed, by exploiting spurious clues and artifacts from manually created data (Opitz and Frank, 2019; Niven and Kao, 2019). This has led to a recent interest in solving argumentation tasks in an unsupervised manner, e.g., by logical reasoning (Jo et al., 2021). In this paper we will highlight that previous methods for rating argument similarity suffer from a common flaw: beyond shallow statistics (word matches in bag-of-word models, or word similarities in distributional space), they do not provide any rationale for their predictions, and the prediction process is in general not transparent. Therefore, we know only little about the following question: When assessing the similarity of arguments, researchers typica"
2021.argmining-1.3,P19-1054,0,0.19062,"ements. Our approach provides significant performance improvements over strong baselines in a fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations. 1 Philipp Cimiano2 Dept. of Computational Linguistics, Heidelberg University 2 CITEC, Bielefeld University 1 2 Philipp Wiesenbach1 • Which argument features correlate with human argument similarity decisions? In this work, we undertake a first attempt at answering this question, by testing two hypotheses: Introduction Rating the similarity of arguments (Reimers et al., 2019) is a core task in argument mining and argument search (Maturana, 1988; Wachsmuth et al., 2017; Ajjour et al., 2019). Argument similarity ratings are also needed for (case-based) argument retrieval (Rissland et al., 1993; Chesnevar and Maguitman, 2004), data exploration via argument clustering, and even automated debaters (Slonim et al., 2021): to counter an opponent’s argument, one may retrieve an argument similar to theirs, but of opposite stance to the topic (Wachsmuth et al., 2018). Typically, argument similarity ratings are computed over ‘bag-of-word’ argument representations, or else ove"
2021.argmining-1.3,2021.findings-acl.203,0,0.0298737,"cuss potential advantages of using AMR for graph-based argumentation tasks in a concrete example, and iii) investigate how interpretable argument similarity computation can help assess the quality and usefulness of conclusions drawn from arguments in a reference-less conclusion evaluation setup.1 2 the analysis of linguistic aspects (Lauscher et al., 2021), e.g., by extracting selected features (Aker et al., 2017; Lugini and Litman, 2018) or leveraging discourse knowledge in language models (Opitz, 2019), others exploit large background knowledge graphs (Kobbe et al., 2019; Paul et al., 2020; Yuan et al., 2021) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2017) or DBpedia (Mendes et al., 2012). An advantage of our approach is the explicit graph alignment between two arguments’ meaning graphs that better marks related structures, and that can help explain argument similarity judgements. Related work Argument similarity and search Assessing argument similarity is a key task in argument mining (Reimers et al., 2019; Lenz et al., 2019) and can enhance argument search (Maturana, 1988; Rissland et al., 1993; Wachsmuth et al., 2017; Ajjour et al., 2019; Chesnevar and Maguitman, 2004). Yet, while"
2021.argmining-1.3,J17-3005,0,0.0271019,"conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT baseline. The difference is significant with p &lt; 0.005 (Student t-test). This system is closely followed by other AMR-based systems, e.g., using concept-focused S2 MATCH that sees only the argument (68.17 macro Conclusion generator We generate conclusions from arguments using the T5 model (Raffel et al., 2020) pre-trained on summarization tasks. To encourage the model to generate informative conclusions (as opposed to summaries), we further finetune it on premise-conclusion samples from Stab and Gurevych (2017), which contain intelligible 4 Setup 5 For further detail on this fine-tuning step see Appendix. Strictly speaking, this is not a fully unsupervised setup, however, we stick to this framing of the task to facilitate comparison to the previous work (Reimers et al., 2019). 6 If the cosine similarity exceeds τ = 0.95. 27 Baselines Ours metric model type macro F1 score sim not sim rank human ? 78.34 74.74 81.94 0 random Tf-Idf InfSnt-fText InfSnt-GloVe GloVe Emb. ELMo Emb. BERT Embe. f (a, a0 ) f (a, a0 ) f (a, a0 ) f (a, a0 ) f (a, a0 ) f (a, a0 ) 48.01 61.18 66.21 64.94 64.68 64.47 65.39 34.31 5"
2021.argmining-1.3,W17-5106,0,0.174565,"fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations. 1 Philipp Cimiano2 Dept. of Computational Linguistics, Heidelberg University 2 CITEC, Bielefeld University 1 2 Philipp Wiesenbach1 • Which argument features correlate with human argument similarity decisions? In this work, we undertake a first attempt at answering this question, by testing two hypotheses: Introduction Rating the similarity of arguments (Reimers et al., 2019) is a core task in argument mining and argument search (Maturana, 1988; Wachsmuth et al., 2017; Ajjour et al., 2019). Argument similarity ratings are also needed for (case-based) argument retrieval (Rissland et al., 1993; Chesnevar and Maguitman, 2004), data exploration via argument clustering, and even automated debaters (Slonim et al., 2021): to counter an opponent’s argument, one may retrieve an argument similar to theirs, but of opposite stance to the topic (Wachsmuth et al., 2018). Typically, argument similarity ratings are computed over ‘bag-of-word’ argument representations, or else over argument representations inferred with i) Representing arguments with Abstract Meaning Repre"
2021.argmining-1.3,P18-1023,0,0.0230075,"attempt at answering this question, by testing two hypotheses: Introduction Rating the similarity of arguments (Reimers et al., 2019) is a core task in argument mining and argument search (Maturana, 1988; Wachsmuth et al., 2017; Ajjour et al., 2019). Argument similarity ratings are also needed for (case-based) argument retrieval (Rissland et al., 1993; Chesnevar and Maguitman, 2004), data exploration via argument clustering, and even automated debaters (Slonim et al., 2021): to counter an opponent’s argument, one may retrieve an argument similar to theirs, but of opposite stance to the topic (Wachsmuth et al., 2018). Typically, argument similarity ratings are computed over ‘bag-of-word’ argument representations, or else over argument representations inferred with i) Representing arguments with Abstract Meaning Representations (AMRs) and using AMR graph metrics improves argument similarity rating and provides explanatory information. ii) Extending arguments with inferred conclusions can improve argument similarity rating. In the following §2 we discuss related work. §3 introduces our two key hypotheses, and §4 presents our argument similarity rating model and its implementation. In §5 we compare our model"
2021.deelio-1.2,2020.deelio-1.9,0,0.0135775,"NLP tasks. It has also been shown that knowledge acquired during pre-training can be leveraged by fine-tuning these models to advanced semantic inference or NL generation tasks (Wang et al. 2018). Recently, pre-trained LMs have been augmented with external knowledge from commonsense knowledge bases such as ConceptNet, which provides more explicit knowledge grounding and improves their performance on downstream tasks that require reasoning abilities. Wang et al. (2020b), for example, retrieve multi-hop knowledge paths from ConceptNet for fine-tuning LMs for multiple choice question answering. Chang et al. (2020) and Bosselut et al. (2021) incorporate knowledge paths from ConceptNet into pre-trained LMs for solving the SocialIQA task (Sap et al., 2019). However, all these approaches evaluate the effectiveness of integrating commonsense knowledge indirectly on downstream tasks, and do not explicitly evaluate the impact and relevance of knowledge for a specific system prediction. We address this shortcoming by generating and carefully evaluating statements that connect pairs of sentences as explanations of their underlying, implicit knowledge link. Closest to this aim is the task of explanation generati"
2021.deelio-1.2,2020.lrec-1.282,1,0.88528,"nces in a text. To this end we create corpora with sentence pairs enriched with implicit information based on on Generics-KB (Bhakthavatsalam et al., 2020) and e-SNLI (Camburu et al., 2018), which we use for LM fine-tuning. For improved performance we explore methods of constrained language generation, guiding the model by way of relevant concepts and connecting commonsense knowledge paths. We aim to build a system that is not limited to specific text genres or knowledge domains, and thus evaluate our models in-domain – on testsets from our fine-tuning corpora; and out-of-domain – using IKAT (Becker et al., 2020), an argumentative corpus which offers sentence pairs annotated with implicit knowledge that connects them. A central contribution of this work is an indepth evaluation of the quality of generations delivered by different model variants, and their ability of expressing implicitly conveyed knowledge. We propose a manual evaluation setup covering four dimensions – grammaticality, coherence, content, and comparison to gold references – , and compare these to various automatic evaluation metrics. Our experiments show that with our proposed approach we can generate coherent sentences that explicate"
2021.deelio-1.2,2020.aacl-main.28,0,0.196803,"jecting concepts or commonsense knowledge paths as language modeling constraints, and show that key concepts, and even more, knowledge paths improve the quality of generations. (iv) We carefully evaluate the quality of the generated implicit knowledge sentences, both manually and automatically, and discuss strengths and limitations of automatic similarity metrics.1 2 Pre-trained LMs enhanced with commonsense knowledge have also been the models of choice for other text generation tasks, e.g. dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2020), or abductive NLI (Ji et al., 2020b). While these models aim at generating explanations for a single statement, or completing a given sequence of sentences, we investigate how to make use of LMs to generate a sentence that fills in implicit knowledge between two sentences. Related Work Recent progress in pretraining LMs on large text corpora led to improvements for various downstream NLP tasks. It has also been shown that knowledge acquired during pre-training can be leveraged by fine-tuning these models to advanced semantic inference or NL generation tasks (Wang et al. 2018). Recently, pre-trained LMs have been augmented with"
2021.deelio-1.2,D15-1075,0,0.0153615,"generic knowledge. Each generic sentence occurs in its surrounding context (1-5 sents before/after), hence each instance forms a triple consisting of the context before (Cb ), the generic sentence (GS) and the context after (Ca ). We collect all instances where a phrase p1 (NP, VP, ADJP or ADVP) from GS also occurs in Cb , and another phrase p2 from GS occurs in Ca . For each instance we extract the sentence containing p1 and the one containing p2 as our source sentences S1 , S2 ; GS as our target sentence T ; and p1 and p2 as key concepts c1 , c2 . e-SNLI is an extension of the SNLI dataset (Bowman et al., 2015), additionally annotated with explanations: Given a premise-hypothesis pair and the relation between them (entailment, contradiction, or neutral), annotators added natural language sentences that explain why the pair is in the relation. Annotators had to mark essential key phrases for the relation in premise and hypothesis, and had to formulate explanations that employ these key phrases. For fine-tuning and testing our models, we consider all instances labelled with entailment and contradiction relations (but do not include the labels in fine-tuning). We interpret premise and hypothesis as our"
2021.deelio-1.2,2020.emnlp-main.54,0,0.194488,"jecting concepts or commonsense knowledge paths as language modeling constraints, and show that key concepts, and even more, knowledge paths improve the quality of generations. (iv) We carefully evaluate the quality of the generated implicit knowledge sentences, both manually and automatically, and discuss strengths and limitations of automatic similarity metrics.1 2 Pre-trained LMs enhanced with commonsense knowledge have also been the models of choice for other text generation tasks, e.g. dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2020), or abductive NLI (Ji et al., 2020b). While these models aim at generating explanations for a single statement, or completing a given sequence of sentences, we investigate how to make use of LMs to generate a sentence that fills in implicit knowledge between two sentences. Related Work Recent progress in pretraining LMs on large text corpora led to improvements for various downstream NLP tasks. It has also been shown that knowledge acquired during pre-training can be leveraged by fine-tuning these models to advanced semantic inference or NL generation tasks (Wang et al. 2018). Recently, pre-trained LMs have been augmented with"
2021.deelio-1.2,2020.acl-main.119,0,0.0232429,"nce representations. BERT-Score uses BERT’s contextualized word embeddings to calculate a cross similarity score for each token in the generation with each token in the reference, while Sentence-BERT is fine-tuned on NLI and STS to predict the similarity of two sequences. For BERT-Score we report F1 scores; for Sentence-BERT we average the similarity scores obtained for the generated vs. reference sentences. (iii) S2Match (Opitz et al., 2020) is an AMR graph matching metric, which measures the overlap of the AMR semantic graphs that we construct from the reference and generated sentence using Cai and Lam (2020)’s parser, and reports accuracy; (iv) Distinct-N (Li et al., 2015) and GRUEN (Zhu and Bhat, 2020) are reference-free metrics that only consider properties of the generated sentence. Distinct-N measures the diversity of a sentence by focusing on the number of distinct unigrams (Distinct-1) and bigrams (Distinct-2); GRUEN evaluates the linguistic quality of a sentence in terms of grammaticality, non-redundancy, and structure. In a preliminary experiment based on the complete test sets of Generics-KB, e-SNLI and IKAT (cf. Table 3) we first investigate which model generates sentences that are most"
2021.deelio-1.2,2020.semeval-1.46,0,0.0179744,"sentences in-between contiguous sentences, which explicate their logical connection, utilizing pre-trained language models (LMs) that we refine as follows: i) we inject ’explanatory’ knowledge by fine-tuning LMs on specifically prepared corpora, and (ii) condition text generation through constraints in form of relevant concepts and knowledge paths. Our work is 11 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 11–24 Online, June 10, 2021. ©2021 Association for Computational Linguistics mal et al./Jon et al. 2020) shows that pre-trained LMs play a central role in the success of the topperforming systems, demonstrating that they contain commonsense information to a good extent. The success of models enriched with knowledge from external sources such as ConceptNet furthermore shows that additional knowledge supports the generation of commonsense explanations. However, there is still a large gap between systems and human performance. tences. (ii) We create datasets that include implicit information holding between sentence pairs, which we use for fine-tuning our LMs, and which can be used for general comm"
2021.deelio-1.2,E06-1032,0,0.0436131,"Missing"
2021.deelio-1.2,2020.findings-emnlp.165,0,0.0282213,"tements that connect pairs of sentences as explanations of their underlying, implicit knowledge link. Closest to this aim is the task of explanation generation, which has received attention very recently. Wang et al. (2020a) propose the SemEval2020 Task 4 (Subtask C), which is to generate an explanation for why a statement does not make sense, by way of a natural language statement. A comparison of the participating systems (cf. PeruConstraining LMs. Recent work addresses how to control content in LM text generation, while maintaining fluency, coherence and plausibility of the generated text. Lin et al. (2020) explore how to generate a coherent and plausible situation description given an unordered set of concepts as input, and find that even pre-trained LMs (BART, T5) fine-tuned to this task cannot solve it: the generated sentences are grammatical, but highly implausible, lacking commonsense. This suggests that either the underlying LMs, or input constraints for generation need to incorporate commonsense knowledge. Orbach and Goldberg (2020) attempt to control the content when generating longer stories by specifying facts the story needs to include. They propose a plan-and-cloze model that first c"
2021.deelio-1.2,W04-1013,0,0.0144301,"fferent evaluation metrics commonly applied in text generation tasks, which either measure the similarity to a reference sentence (in our case, the 17 Figure 2: Results of 2nd manual evaluation: comparing models constrained with concepts (+c) or paths (+p) against a baseline S2M BERT S-BERT dist1 dist2 GRUEN e-SNLI e-SNLI+c e-SNLI+p G-KB G-KB+c G-KB+p IKAT IKAT+c IKAT+p ROU-1 generic sentences in GenericsKB, inference explanations in e-SNLI, or implicit knowledge statements in IKAT); or the linguistic quality and diversity of the generated sentence. (i) BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) measure token overlap using ngrams. We apply BLEU-1 to measure precision and ROUGE-1 to measure recall based on unigrams; (ii) BERT-Score (Zhang* et al., 2020) and Sentence-BERT (Reimers and Gurevych, 2019) compute semantic similarity scores for text sequences based on word or sentence representations. BERT-Score uses BERT’s contextualized word embeddings to calculate a cross similarity score for each token in the generation with each token in the reference, while Sentence-BERT is fine-tuned on NLI and STS to predict the similarity of two sequences. For BERT-Score we report F1 scores; for Sen"
2021.deelio-1.2,C04-1020,0,0.0630396,"ning Data For the first annotation round we sample 10 source sentence pairs from each testset, hence 30 pairs overall, and the sentences generated by GPT-2, XLNet and BART for each instance, using concepts as 5 The difference between dimension 2 and 3 is that with dimension 2 (coherence), we want to explore if the generated sentence semantically fits to the two given source sentences. We understand coherence together with Hobbs (1979) as the existence of specific knowledge relations that hold between concepts in a text (or discourse), such as Cause-Effect, Condition, or Temporal Sequence, cf. Wolf and Gibson (2004). These relations make the texts interpretable and informative and are motivated ultimately by the speaker’s or writer’s need to be understood (Hobbs, 1979). In contrast, when evaluating the content of the generated sentence in dimension 3, we want to discover if the sentence really explains the connection between the two source sentences. 6 The reference sentence is only provided for Question 4. 4 The annotation manual together with example annotations can be found here: https://github.com/Heidelber g-NLP/LMs4Implicit-Knowledge-Generation/ blob/main/manual.pdf 15 Figure 1: Example generations"
2021.deelio-1.2,2020.tacl-1.34,1,0.649928,"n unigrams; (ii) BERT-Score (Zhang* et al., 2020) and Sentence-BERT (Reimers and Gurevych, 2019) compute semantic similarity scores for text sequences based on word or sentence representations. BERT-Score uses BERT’s contextualized word embeddings to calculate a cross similarity score for each token in the generation with each token in the reference, while Sentence-BERT is fine-tuned on NLI and STS to predict the similarity of two sequences. For BERT-Score we report F1 scores; for Sentence-BERT we average the similarity scores obtained for the generated vs. reference sentences. (iii) S2Match (Opitz et al., 2020) is an AMR graph matching metric, which measures the overlap of the AMR semantic graphs that we construct from the reference and generated sentence using Cai and Lam (2020)’s parser, and reports accuracy; (iv) Distinct-N (Li et al., 2015) and GRUEN (Zhu and Bhat, 2020) are reference-free metrics that only consider properties of the generated sentence. Distinct-N measures the diversity of a sentence by focusing on the number of distinct unigrams (Distinct-1) and bigrams (Distinct-2); GRUEN evaluates the linguistic quality of a sentence in terms of grammaticality, non-redundancy, and structure."
2021.deelio-1.2,2020.coling-main.211,0,0.0343813,"PeruConstraining LMs. Recent work addresses how to control content in LM text generation, while maintaining fluency, coherence and plausibility of the generated text. Lin et al. (2020) explore how to generate a coherent and plausible situation description given an unordered set of concepts as input, and find that even pre-trained LMs (BART, T5) fine-tuned to this task cannot solve it: the generated sentences are grammatical, but highly implausible, lacking commonsense. This suggests that either the underlying LMs, or input constraints for generation need to incorporate commonsense knowledge. Orbach and Goldberg (2020) attempt to control the content when generating longer stories by specifying facts the story needs to include. They propose a plan-and-cloze model that first creates a cloze template, placing input facts at fixed positions in the output. In the cloze step, the system expands the fact tokens into complex sentences that complete the story. While uni-directional LMs such as GPT-2 or BART generate fluent text but do not well adhere to the desired content, the fine-tuned multi-directional XLNet outputs coherent text and adheres to the facts. While none of the above works incorporate external knowle"
2021.deelio-1.2,P02-1040,0,0.111656,"Missing"
2021.eacl-demos.15,W19-0801,1,0.856587,"Missing"
2021.eacl-demos.15,P19-1470,0,0.0242058,"Missing"
2021.eacl-demos.15,W04-2209,0,0.0104388,"er than overspecific or uninformative concepts, and allows to assess more relational information stored in the knowledge graph. 1 1 Introduction ConceptNet (Speer et al., 2017) is a semantic network which contains general commonsense facts about the world, e.g. Birds can fly or Computers are used for sending e-mails (Liebermann, 2008). It originates from the crowdsourcing project Open Mind Common Sense (Speer et al., 2008) that acquired commonsense knowledge from contributions over the web. The current version also includes expert-created resources such as WordNet (Fellbaum, 1998) and JMDict (Breen, 2004), other crowdsourced resources such as Wiktionary, 1 We provide a demo video (https://www. youtube.com/watch?v=bgqVhE2vR9A&feature= youtu.be) and the code (https://github.com/ Heidelberg-NLP/CoCo-Ex) for C O C O -E X. knowledge obtained through games with a purpose such as Verbosity, and automatically extracted knowledge (cf. Speer et al. (2008)). Knowledge facts in ConceptNet are represented as triples, e.g. [dog,I S A,domestic animal]. The current version, ConceptNet 5, comprises 37 relations, such as U SED F OR, I S A, PART O F, or L OCATEDAT. ConceptNet has been proven a useful resource of"
2021.eacl-demos.15,2020.lrec-1.283,0,0.0316619,"Missing"
2021.eacl-demos.15,P16-1137,0,0.0558225,"Missing"
2021.eacl-demos.15,D19-1282,0,0.028375,"Missing"
2021.eacl-demos.15,N15-1108,0,0.0132439,"ated (that is, referring to a similar or the same entity or event), but represented as distinct nodes. In this dictionary we then look up the preprocessed candidate phrases and get all ConceptNet nodes which contain them. In order to avoid obtaining conceptually unrelated nodes, in Step 4 we establish a method that allows us to filter out nodes that are not similar enough to the candidate phrase using similarity metrics and vector space representations. Step 1: Extracting Candidate Phrase Types. We start by extracting candidate phrases from a given text using the Stanford Constituency parser (Mi and Huang, 2015). We extract noun phrases, verb phrases and adjective phrases.2 We find that some verb phrases are very long and specific and therefore unlikely to find exact matches in ConceptNet (e.g., “be sorted into different wheelie bins”). Yet, ConceptNet concepts often consist of general verb-object phrases, such as walk the dog; cook dinner; bake a cake. To accommodate for this, we create, for every verbal phrase we extract from the text, additional versions (i.e., chunks) that exclude subordinated prepositional phrases and/or noun phrases (e.g., for “be sorted into different wheelie bins” we addition"
2021.eacl-demos.15,D18-1260,0,0.0138808,"simple string matching vs. using C O C O -E X. CommonsenseQA contains 12,247 questions with 5 answer choices each, and OpenBookQA provides 6,000 4-way multiple-choice questions. 3 Applications Recent approaches that map natural language text to nodes in ConceptNet apply simple string matching. Wang et al. (2020) for example use ConceptNet in order to retrieve multi-hop knowledge paths as background information for improving the task of question answering. They map concepts that appear in questions and answers from the two benchmark datasets, CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018), to ConceptNet using plain string matching. Irrespective of the question answering task, we want to evaluate the two methods of linking concepts from texts to ConceptNet (plain string matching vs. C O C O -E X) by comparing the number of concepts that could be retrieved from ConceptNet by both methods, respectively; and by evaluating the quality of the retrieved concepts, with regard to their coverage and informativity, as well as the amount of utilized relational knowledge from the ConceptNet knowledge graph. We reimplement the string matching method and make it comparable to C O C O -E X by"
2021.eacl-demos.15,P18-1076,1,0.842856,"VhE2vR9A&feature= youtu.be) and the code (https://github.com/ Heidelberg-NLP/CoCo-Ex) for C O C O -E X. knowledge obtained through games with a purpose such as Verbosity, and automatically extracted knowledge (cf. Speer et al. (2008)). Knowledge facts in ConceptNet are represented as triples, e.g. [dog,I S A,domestic animal]. The current version, ConceptNet 5, comprises 37 relations, such as U SED F OR, I S A, PART O F, or L OCATEDAT. ConceptNet has been proven a useful resource of background knowledge for various NLP downstream tasks, and is thus widely used, e.g., for reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang et al., 2018; Gonz´alez et al., 2018), dialog modelling (Young et al., 2018), argument classification (Paul et al., 2020), textual entailment (Weissenborn et al., 2018), question answering (Ostermann et al., 2018) or for explaining sentiment (Paul and Frank, 2019). As opposed to conventional knowledge bases such as NELL (Carlson et al., 2010), Freebase (Bollacker et al., 2008), or YAGO (Nickel et al., 2012), the nodes in ConceptNet are represented as non-canonicalized, free-form text. This means that (I) concept nodes are not normalized: e.g. bake cake, bake cakes"
2021.eacl-demos.15,S18-1119,0,0.0193916,"facts in ConceptNet are represented as triples, e.g. [dog,I S A,domestic animal]. The current version, ConceptNet 5, comprises 37 relations, such as U SED F OR, I S A, PART O F, or L OCATEDAT. ConceptNet has been proven a useful resource of background knowledge for various NLP downstream tasks, and is thus widely used, e.g., for reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang et al., 2018; Gonz´alez et al., 2018), dialog modelling (Young et al., 2018), argument classification (Paul et al., 2020), textual entailment (Weissenborn et al., 2018), question answering (Ostermann et al., 2018) or for explaining sentiment (Paul and Frank, 2019). As opposed to conventional knowledge bases such as NELL (Carlson et al., 2010), Freebase (Bollacker et al., 2008), or YAGO (Nickel et al., 2012), the nodes in ConceptNet are represented as non-canonicalized, free-form text. This means that (I) concept nodes are not normalized: e.g. bake cake, bake cakes, baking cake, and baking cakes are represented as distinct nodes; likewise bin bag, binbag, bin bags, and bin-bag are separate nodes in ConceptNet. (II) concept nodes often consist of multi-word expressions, which can be very long and complex"
2021.eacl-demos.15,N19-1368,1,0.929601,"[dog,I S A,domestic animal]. The current version, ConceptNet 5, comprises 37 relations, such as U SED F OR, I S A, PART O F, or L OCATEDAT. ConceptNet has been proven a useful resource of background knowledge for various NLP downstream tasks, and is thus widely used, e.g., for reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang et al., 2018; Gonz´alez et al., 2018), dialog modelling (Young et al., 2018), argument classification (Paul et al., 2020), textual entailment (Weissenborn et al., 2018), question answering (Ostermann et al., 2018) or for explaining sentiment (Paul and Frank, 2019). As opposed to conventional knowledge bases such as NELL (Carlson et al., 2010), Freebase (Bollacker et al., 2008), or YAGO (Nickel et al., 2012), the nodes in ConceptNet are represented as non-canonicalized, free-form text. This means that (I) concept nodes are not normalized: e.g. bake cake, bake cakes, baking cake, and baking cakes are represented as distinct nodes; likewise bin bag, binbag, bin bags, and bin-bag are separate nodes in ConceptNet. (II) concept nodes often consist of multi-word expressions, which can be very long and complex. Often they consist of several nested phrase types"
2021.eacl-demos.15,D14-1162,0,0.083801,"Missing"
2021.eacl-demos.15,K18-1014,0,0.0292043,"Missing"
2021.eacl-demos.15,P15-1061,0,0.0403694,"Missing"
2021.eacl-demos.15,N19-1421,0,0.0206158,"of concepts linked to ConceptNet by simple string matching vs. using C O C O -E X. CommonsenseQA contains 12,247 questions with 5 answer choices each, and OpenBookQA provides 6,000 4-way multiple-choice questions. 3 Applications Recent approaches that map natural language text to nodes in ConceptNet apply simple string matching. Wang et al. (2020) for example use ConceptNet in order to retrieve multi-hop knowledge paths as background information for improving the task of question answering. They map concepts that appear in questions and answers from the two benchmark datasets, CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018), to ConceptNet using plain string matching. Irrespective of the question answering task, we want to evaluate the two methods of linking concepts from texts to ConceptNet (plain string matching vs. C O C O -E X) by comparing the number of concepts that could be retrieved from ConceptNet by both methods, respectively; and by evaluating the quality of the retrieved concepts, with regard to their coverage and informativity, as well as the amount of utilized relational knowledge from the ConceptNet knowledge graph. We reimplement the string matching method an"
2021.eacl-demos.15,P19-1023,0,0.0609397,"Missing"
2021.eacl-demos.15,S18-1120,0,0.0191402,"github.com/ Heidelberg-NLP/CoCo-Ex) for C O C O -E X. knowledge obtained through games with a purpose such as Verbosity, and automatically extracted knowledge (cf. Speer et al. (2008)). Knowledge facts in ConceptNet are represented as triples, e.g. [dog,I S A,domestic animal]. The current version, ConceptNet 5, comprises 37 relations, such as U SED F OR, I S A, PART O F, or L OCATEDAT. ConceptNet has been proven a useful resource of background knowledge for various NLP downstream tasks, and is thus widely used, e.g., for reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang et al., 2018; Gonz´alez et al., 2018), dialog modelling (Young et al., 2018), argument classification (Paul et al., 2020), textual entailment (Weissenborn et al., 2018), question answering (Ostermann et al., 2018) or for explaining sentiment (Paul and Frank, 2019). As opposed to conventional knowledge bases such as NELL (Carlson et al., 2010), Freebase (Bollacker et al., 2008), or YAGO (Nickel et al., 2012), the nodes in ConceptNet are represented as non-canonicalized, free-form text. This means that (I) concept nodes are not normalized: e.g. bake cake, bake cakes, baking cake, and baking cakes are repres"
2021.eacl-demos.15,2020.findings-emnlp.369,0,0.0118544,"tions are implemented as well (as command line parameters), so users can experiment with different settings easily. Str-Match CommonsenseQA Questions Answers OpenBookQA Questions Answers CoCo-Ex 99,217 106,681 88,631 116,941 38,415 53,748 38,485 61,313 Table 1: Number of concepts linked to ConceptNet by simple string matching vs. using C O C O -E X. CommonsenseQA contains 12,247 questions with 5 answer choices each, and OpenBookQA provides 6,000 4-way multiple-choice questions. 3 Applications Recent approaches that map natural language text to nodes in ConceptNet apply simple string matching. Wang et al. (2020) for example use ConceptNet in order to retrieve multi-hop knowledge paths as background information for improving the task of question answering. They map concepts that appear in questions and answers from the two benchmark datasets, CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018), to ConceptNet using plain string matching. Irrespective of the question answering task, we want to evaluate the two methods of linking concepts from texts to ConceptNet (plain string matching vs. C O C O -E X) by comparing the number of concepts that could be retrieved from ConceptNet by"
2021.eacl-main.129,2020.acl-main.640,0,0.108123,"ing Representation (AMR, Banarescu et al. (2013)) aims at capturing the meaning of a sentence in a machine-readable graph format. AMR captures, i.a., word senses, semantic roles and coreference. The AMR in Fig. 1 represents the sentence Perhaps, the parrot is telling itself a story. In this graph, tell-01 links to a PropBank (Palmer et al., 2005) frame, and argn labels indicate participant roles: parrot is both speaker (arg0) and hearer (arg2), story is the utterance (arg1). The task of AMR-to-text generation has recently garnered much attention (Song et al., 2017, 2018; Konstas et al., 2017; Cai and Lam, 2020b; Ribeiro et al., 2019). The output of AMR-to-text systems is typically evaluated against the sentence from which the AMR was created, using standard surface string matching metrics such as B LEU (Papineni et al., 2002) or CHR F(++) (Stanojevi´c et al., 2015; Popovi´c, 2015, 2016; Popov, 2017), as is standard in many NLG tasks. These metrics suffer from several issues, for example, they penalize paraphrases, are highly sensitive to outliers (Mathur et al., 2020), and lack interpretability (Sai et al., 2020). Some of these issues get compounded when evaluating AMR-to-text. The core of the prob"
2021.eacl-main.129,P13-2131,0,0.0829855,"ce s0 and source AMR m, we match parse(s0 ) against m by computing amrM etric(parse(s0 ), m). This means that we have to decide upon parse and amrM etric. We propose two potential settings. AMR reconstruction To reconstruct the AMR with parse, we use the latest state-of-the-art AMR parser by Cai and Lam (2020a). With 80.3 Smatch F1, this parser is almost on-par with human agreement (estimated at 0.71–0.83 Smatch F1 in Banarescu et al. (2013)). We henceforth call it GSII. Assessing M with AMR metrics To obtain a score for M we propose to use S2 match (Opitz et al., 2020) – a variant of Smatch (Cai and Knight, 2013) that performs a graded match for concept nodes. This offers the potential to compensate for noise in automatically generated text or minor lexical deviations from the original sentence. Discussion Comparing to references by matching their meaning graphs has the prospect of offering interpretability and explanations, by detecting redundant or missing meaning components in the generations. In our studies, we will see that this assessment can be conducted by computing a single graph overlap score (e.g., S2 match F1), or along multiple dimensions of meaning, such as SRL, coreference or WSD (Damon"
2021.eacl-main.129,W13-0801,0,0.021308,"over to the evaluation of AMR-to-text generation: May and Priyadarshi (2017) find that B LEU does not well correspond to human ratings of generations from AMR, and Manning et al. (2020) show through human analysis that none of the existing automatic metrics can provide nuanced views on generation quality. Our proposal MFβ takes a first step to address these issues by aiming at a clear separation of form and meaning, as called for by Bender and Koller (2020). First attempts of assessing semantic generation quality have been examined in MT using semantic role labeling (Lo, 2017) or WSD and NLI (Carpuat, 2013; Poliak et al., 2018), in-between lies SPICE that evaluates caption generation via inferred semantic propositions (Anderson et al., 2016). Just like MFβ , SPICE relies on automatic parses (a dependency parse of the caption and a scene graph predicted for the image) to evaluate content overlap of image and caption. Thus, SPICE is a direct precursor of an NLG metric in V&L that relies on automatically produced structured representations. Our work extends this previous work by showing ways of probing potentially harmful effects of incorporating automatic parsing components. 6 Conclusion We propo"
2021.eacl-main.129,E17-1051,0,0.311999,"2013) that performs a graded match for concept nodes. This offers the potential to compensate for noise in automatically generated text or minor lexical deviations from the original sentence. Discussion Comparing to references by matching their meaning graphs has the prospect of offering interpretability and explanations, by detecting redundant or missing meaning components in the generations. In our studies, we will see that this assessment can be conducted by computing a single graph overlap score (e.g., S2 match F1), or along multiple dimensions of meaning, such as SRL, coreference or WSD (Damonte et al., 2017). Generally, MFβ gives researchers the flexibility of choosing a parser or amrM etric to their liking. In this work, we choose the best current parser that achieves high IAA with humans. Yet, we would also like to know whether the parser is vulnerable to specific peculiarities of generated sentences, or how using another parser affects the scores. We will investigate these issues more closely in §4.1. 2.3 Parameterizing form with LMs Assessing sentence grammaticality and fluency is not an easy task (Heilman et al., 2014; Katinskaia and Ivanova, 2019). Recently, Lau et al. (2020); Zhu and Bhat"
2021.eacl-main.129,N19-1423,0,0.147584,"cision of whether the F orm of a generated sentence s0 is acceptable is then calculated as ( 1, if pref Score ≥ 0.5 − tol accept = 0, otherwise, where tol is a tolerance parameter. Less formally, a sentence is considered to have an acceptable surface form in relation to its reference if its form is estimated to be at least as good as the reference minus a tolerance, which we fix at 0.05. I.e., the corpus-level F orm score reflects the ratio of generated sentences that are of acceptable form.5 Predictor selection We consider GPT-2 (Radford et al., 2019), distil GPT-2 (Sanh et al., 2019), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) as a basis for assessing F orm. We conduct experiments on WebNLG (Gardent et al., 2017; Shimorina et al., 2017), which contains human fluency and grammaticality judgements for machine-generated sentences. We find that GPT-2 performs best: it discriminates sentences of poor and perfect fluency and grammaticality with an F1 score of approximately 0.8, and shows marginally better performance compared to the other LMs (see Appendix A.2 for the experiment details). We thus select GPT-2 as our LM for F orm assessment. Discussion While the reconstruction of meaning doe"
2021.eacl-main.129,W18-4912,0,0.027953,"neration against the parse (distant) source sentence (GSII♦ ). stract meaning representation. This would in turn offer means for conducting fine-grained meaning analysis of generation tasks where the reference is a natural language sentence (e.g., in MT). Note, however, that AMR, as of now, does not capture some facets of meaning that may be of interest in some generation tasks. For instance, it does not capture tense or aspect. However, what we have investigated as a potential weakness of MFβ , namely the necessity to select a meaning parser, can also be viewed as a potential strength. E.g., Donatelli et al. (2018) show how tense and aspect can be captured with AMR. This indicates that MFβ can indeed be used for a tense and aspect analysis of generated text – if we parameterize it with a dedicated parser. Finally, if output and reference do not consist of single sentences, it may be apt to use a parser that constructs MRs for discourse (e.g., DRS (Kamp, 1981)). In summary, we conclude that MFβ , our proposed metric that aims to assess text generation quality by decomposing it into form and meaning aspects, is broadly applicable. However, different parser parametrizations may have to be considered in lig"
2021.eacl-main.129,W17-3518,0,0.0525356,"tol accept = 0, otherwise, where tol is a tolerance parameter. Less formally, a sentence is considered to have an acceptable surface form in relation to its reference if its form is estimated to be at least as good as the reference minus a tolerance, which we fix at 0.05. I.e., the corpus-level F orm score reflects the ratio of generated sentences that are of acceptable form.5 Predictor selection We consider GPT-2 (Radford et al., 2019), distil GPT-2 (Sanh et al., 2019), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) as a basis for assessing F orm. We conduct experiments on WebNLG (Gardent et al., 2017; Shimorina et al., 2017), which contains human fluency and grammaticality judgements for machine-generated sentences. We find that GPT-2 performs best: it discriminates sentences of poor and perfect fluency and grammaticality with an F1 score of approximately 0.8, and shows marginally better performance compared to the other LMs (see Appendix A.2 for the experiment details). We thus select GPT-2 as our LM for F orm assessment. Discussion While the reconstruction of meaning does not depend on the reference sentence, we do make use of it, in pref Score, for better assessment 4 We use the mean ("
2021.eacl-main.129,P18-1170,0,0.047093,"Missing"
2021.eacl-main.129,Q19-1019,0,0.116546,"Missing"
2021.eacl-main.129,P14-2029,0,0.0307706,"or along multiple dimensions of meaning, such as SRL, coreference or WSD (Damonte et al., 2017). Generally, MFβ gives researchers the flexibility of choosing a parser or amrM etric to their liking. In this work, we choose the best current parser that achieves high IAA with humans. Yet, we would also like to know whether the parser is vulnerable to specific peculiarities of generated sentences, or how using another parser affects the scores. We will investigate these issues more closely in §4.1. 2.3 Parameterizing form with LMs Assessing sentence grammaticality and fluency is not an easy task (Heilman et al., 2014; Katinskaia and Ivanova, 2019). Recently, Lau et al. (2020); Zhu and Bhat (2020) show that probability estimates based on language models can be used as an indicator for measuring complex notions of form and for measuring acceptability in context. For 1506 our MFβ score we desire an interpretable ratio as input, which we base on LM predictions as follows. Binary form assessment Given a specific candidate generation s0 , we use a binary variable to assess whether s0 is of satisfactory form. For this, we first calculate the mean token probability:4 n 1X mtp(·) = P (tokj |ctxj ), n (2) j=1 where"
2021.eacl-main.129,W19-3702,0,0.0133476,"nsions of meaning, such as SRL, coreference or WSD (Damonte et al., 2017). Generally, MFβ gives researchers the flexibility of choosing a parser or amrM etric to their liking. In this work, we choose the best current parser that achieves high IAA with humans. Yet, we would also like to know whether the parser is vulnerable to specific peculiarities of generated sentences, or how using another parser affects the scores. We will investigate these issues more closely in §4.1. 2.3 Parameterizing form with LMs Assessing sentence grammaticality and fluency is not an easy task (Heilman et al., 2014; Katinskaia and Ivanova, 2019). Recently, Lau et al. (2020); Zhu and Bhat (2020) show that probability estimates based on language models can be used as an indicator for measuring complex notions of form and for measuring acceptability in context. For 1506 our MFβ score we desire an interpretable ratio as input, which we base on LM predictions as follows. Binary form assessment Given a specific candidate generation s0 , we use a binary variable to assess whether s0 is of satisfactory form. For this, we first calculate the mean token probability:4 n 1X mtp(·) = P (tokj |ctxj ), n (2) j=1 where ctxj is different for uni-dire"
2021.eacl-main.129,2020.aacl-main.27,1,0.842434,"and from GPLA to GSII. An unfair treatment could arise, e.g., if GSII generates bad AMR reconstructions for specific NLG systems but not so for others. However, we do not observe such tendencies. Hence we assume that GSII’s score increments 11 We observe one switch of ranks for TTSA-GPLA and GPLA-GSII and 2 rank switches for TTSA–GSII in R E SMATCH , and no rank switch for TTSA-GSII and one switch for TTSA-GPLA and GPLA-GSII, for MF 0.5 . stem from the fact that GSII yields better reconstructions for all systems. In future work, we plan to explore parse quality control (Opitz and Frank, 2019; Opitz, 2020) or ensemble parsing (van Noord and Bos, 2017), to gain more detailed information on the quality of the meaning reconstructions. Ablating the gold graph? Yes, we can. In lack of a gold standard for the automatic reconstructions, we elicit some indirect answers and insight about the parser’s quality, by considering the following question: What is the effect on system rankings when we replace the input gold graphs with automatic parses of the distant source sentence? If this effect is large, this will give us reasons to worry, as it would indicate that the parser is less reliable than expected g"
2021.eacl-main.129,S19-1024,1,0.796042,"ents from TTSA to GPLA and from GPLA to GSII. An unfair treatment could arise, e.g., if GSII generates bad AMR reconstructions for specific NLG systems but not so for others. However, we do not observe such tendencies. Hence we assume that GSII’s score increments 11 We observe one switch of ranks for TTSA-GPLA and GPLA-GSII and 2 rank switches for TTSA–GSII in R E SMATCH , and no rank switch for TTSA-GSII and one switch for TTSA-GPLA and GPLA-GSII, for MF 0.5 . stem from the fact that GSII yields better reconstructions for all systems. In future work, we plan to explore parse quality control (Opitz and Frank, 2019; Opitz, 2020) or ensemble parsing (van Noord and Bos, 2017), to gain more detailed information on the quality of the meaning reconstructions. Ablating the gold graph? Yes, we can. In lack of a gold standard for the automatic reconstructions, we elicit some indirect answers and insight about the parser’s quality, by considering the following question: What is the effect on system rankings when we replace the input gold graphs with automatic parses of the distant source sentence? If this effect is large, this will give us reasons to worry, as it would indicate that the parser is less reliable t"
2021.eacl-main.129,2020.acl-main.704,0,0.0298234,", but it cannot be carved out by conventional metrics, since these do not disentangle F orm and M eaning. 5 Related work Traditionally, the performance of NLG systems has been evaluated with word n-gram matching metrics such as the popular B LEU metric in MT (Papineni et al., 2002) or Rouge (Lin, 2004) in document summarization. Yet, such metrics suffer from several well-known issues (Novikova et al., 2017; Nema and Khapra, 2018; Sai et al., 2020). E.g., due to their symbolic matching strategy they cannot account for paraphrases. Recently, unsupervised (Zhang et al., 2020) or learned metrics (Sellam et al., 2020; Zhou and Xu, 2020) based on contextual language models have been proposed. For example, BERTscore (Zhang et al., 2020) uses BERT (Devlin et al., 2019) to encode candidate and reference and computes a score based on a cross-sentence word-similarity alignment. Compared with B LEU, it is computationally more expensive but tends to show higher agreement with humans. However, all of the aforementioned metrics return scores that are hardly interpretable and we cannot tell what exactly they have measured. These problems carry over to the evaluation of AMR-to-text generation: May and Priyadarshi (20"
2021.eacl-main.129,P17-2002,0,0.0188701,"ing itself a story”. Introduction Abstract Meaning Representation (AMR, Banarescu et al. (2013)) aims at capturing the meaning of a sentence in a machine-readable graph format. AMR captures, i.a., word senses, semantic roles and coreference. The AMR in Fig. 1 represents the sentence Perhaps, the parrot is telling itself a story. In this graph, tell-01 links to a PropBank (Palmer et al., 2005) frame, and argn labels indicate participant roles: parrot is both speaker (arg0) and hearer (arg2), story is the utterance (arg1). The task of AMR-to-text generation has recently garnered much attention (Song et al., 2017, 2018; Konstas et al., 2017; Cai and Lam, 2020b; Ribeiro et al., 2019). The output of AMR-to-text systems is typically evaluated against the sentence from which the AMR was created, using standard surface string matching metrics such as B LEU (Papineni et al., 2002) or CHR F(++) (Stanojevi´c et al., 2015; Popovi´c, 2015, 2016; Popov, 2017), as is standard in many NLG tasks. These metrics suffer from several issues, for example, they penalize paraphrases, are highly sensitive to outliers (Mathur et al., 2020), and lack interpretability (Sai et al., 2020). Some of these issues get compounded wh"
2021.eacl-main.129,P18-1150,0,0.0926507,"Missing"
2021.eacl-main.129,W15-3031,0,0.0729311,"Missing"
2021.eacl-main.129,2020.acl-main.326,0,0.0187005,"rs (Mathur et al., 2020), and lack interpretability (Sai et al., 2020). Some of these issues get compounded when evaluating AMR-to-text. The core of the problem is that there are many ways to realize a sentence from a meaning representation. Fig. 2 shows four candidate sentences (i-iv) for a given AMR (left). One system generates (i): Maybe the cat is playing. while another generates (iii): Perhaps, the cat plays the flute. Clearly, (i) captures the meaning of the gold graph better than (iii), which contains ‘hallucinated’ content – a well-known issue in neural generation (Logan et al., 2019; Wang and Sennrich, 2020). Yet, when using a canonical metric such as B LEU to evaluate sentences (i) and (iii) against the reference, the system that produces hallucinations (iii) is greatly rewarded (54 B LEU points) to the disadvantage of systems that yield meaning preserving 1504 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1504–1518 April 19 - 23, 2021. ©2021 Association for Computational Linguistics play-01 arg0 cat sen t sco re MF in g amr For m amr vs can o sen t Generated candidates: Mea n n vs s ical ent arg0 i: Maybe the cat is playing. 2"
2021.eacl-main.129,2020.tacl-1.2,0,0.0485639,"Missing"
2021.eacl-main.129,2020.findings-emnlp.9,0,0.0362702,"et al., 2017). Generally, MFβ gives researchers the flexibility of choosing a parser or amrM etric to their liking. In this work, we choose the best current parser that achieves high IAA with humans. Yet, we would also like to know whether the parser is vulnerable to specific peculiarities of generated sentences, or how using another parser affects the scores. We will investigate these issues more closely in §4.1. 2.3 Parameterizing form with LMs Assessing sentence grammaticality and fluency is not an easy task (Heilman et al., 2014; Katinskaia and Ivanova, 2019). Recently, Lau et al. (2020); Zhu and Bhat (2020) show that probability estimates based on language models can be used as an indicator for measuring complex notions of form and for measuring acceptability in context. For 1506 our MFβ score we desire an interpretable ratio as input, which we base on LM predictions as follows. Binary form assessment Given a specific candidate generation s0 , we use a binary variable to assess whether s0 is of satisfactory form. For this, we first calculate the mean token probability:4 n 1X mtp(·) = P (tokj |ctxj ), n (2) j=1 where ctxj is different for uni-directional LMs (ctxj = tok1...j−1 ) and bi-directiona"
2021.iwpt-1.6,P15-1039,0,0.0690959,"Missing"
2021.iwpt-1.6,P13-2131,0,0.0696484,"and their AMRs. 4 Experiments Data We employ the cross-lingual AMR parsing benchmark LDC2020T07. It was built from the test split of the English mono-lingual LDC2017T10 data by translating its sentences to four languages: German, Spanish, Italian and Mandarin Chinese. This amounts to a total of 5,484 AMR-sentence pairs, or 1,371 AMR-sentence pairs per language. Baselines For all languages (German, Spanish, Italian and Mandarin Chinese), we compare against i) AMREAGER (Damonte and Cohen, 2018), and ii) XL-AMR (Blloshmi et al., 2020). Evaluation metrics Our main evaluation metric is Smatch F1 (Cai and Knight, 2013). The Smatch metric aligns the predicted graph with the gold graph and computes an F1 score that measures normalized triple overlap. Additionally, we calculate F1 scores for finer-grained core semantic sub-tasks Damonte et al. (2017).5 In our analyses (§4.2), we also study results with S2MATCH (Opitz et al., 2020), that offers a potentially fairer evaluation in cross-lingual AMR parsing, since it does not penalize allowed paraphrases that may emerge, e.g., due (Mono-lingual) AMR parsing Mono-lingual AMR parsing equally made big strides in recent years, so that today AMR parsers deliver benchma"
2021.iwpt-1.6,W13-2322,0,0.806417,"id progress of both NMT and AMR parsing models for English, our hypothesis is that this baseline has become more effective and thus more realistic. Moreover, we argue that it could be beneficial to disentangle two key latent representations involved in the process of cross-lingual AMR parsing: i) one that translates between two natural languages and ii) one that translates between a natural language and a meaning representation. This way, the cross-lingual AMR construction process is more transparent and can be better analyzed. Introduction Abstract Meaning Representation (AMR), introduced by Banarescu et al. (2013), aims at representing the meaning of a sentence in a semantic graph format. Nodes represent entities, events and concepts, while (typed) edges express their relations. AMR itself, as of now, is English-focused, e.g., predicate frames are linked to English PropBank (Kingsbury and Palmer, 2002). However, the abstract nature of AMR, and the fact that they are not explicitly linked to syntactic structure, make it appealing for extracting semantic structure of sentences in various languages. This insight led to the recent interest in a new task: cross-lingual AMR parsing (Damonte and Cohen, 2018)."
2021.iwpt-1.6,N18-1104,0,0.713192,"mple baseline tends to be overlooked: translating the sentences to English and projecting their AMR with a monolingual AMR parser (translate+parse, T+P). In this paper, we revisit this simple two-step baseline, and enhance it with a strong NMT system and a strong AMR parser. Our experiments show that T+P outperforms a recent stateof-the-art system across all tested languages: German, Italian, Spanish and Mandarin with +14.6, +12.6, +14.3 and +16.0 Smatch points. 1 Let’s go! Andiamo! Lasst uns gehen! xamr 我们走吧 recommend-01 arg1 go-01 we arg0 Figure 1: Cross-lingual AMR parsing as introduced by Damonte and Cohen (2018). els that have recently been proposed are typically trained on large-scale silver data and learn to directly project the non-English sentences onto their AMR graphs (see Figure 1) (Damonte and Cohen, 2018; Blloshmi et al., 2020). However, there is an intuitive baseline that we argue has so-far received too little attention: translate+parse, T+P. It first translates a sentence to a pivot language and applies a mono-lingual parser for that language. In light of the rapid progress of both NMT and AMR parsing models for English, our hypothesis is that this baseline has become more effective and t"
2021.iwpt-1.6,E17-1051,0,0.146764,"via an iterative BFS writing traversal (Cai and Lam, 2019, 2020). Related work 3 Cross-lingual AMR parsing Cross-lingual AMR parsing was introduced by Damonte and Cohen (2018). They trained an alignmentbased AMR parser model that leverages large amounts of parallel silver AMR data obtained through annotation projection from a curated parallel corpus. The authors also discussed translate+parse (T+P) as a baseline using either the NMT systems Google translate and Nematus (Sennrich et al., 2017), or the SMT system Moses (Koehn et al., 2007), together with a mono-lingual transition-based parser (Damonte et al., 2017). However, their best T+P approach was Google Translate (GT) – which cannot be fully replicated by other researchers since both training data and model structure are hidden. Given the recent advances in NMT (Barrault et al., 2019, 2020) and mono-lingual AMR parsing (Xu et al., 2020), where parsers now achieve scores on par with human IAA assessments (c.f. Banarescu et al. (2013)), we show that time is ripe to put more spotlight on T+P. Blloshmi et al. (2020) address the problem from complementary perspectives: i) they train a system that projects AMR graphs from parsed English sentences to tar"
2021.iwpt-1.6,duran-aluisio-2012-propbank,0,0.0449137,"Missing"
2021.iwpt-1.6,W19-5301,0,0.0442908,"Missing"
2021.iwpt-1.6,2020.emnlp-main.231,0,0.0193349,"h sentences into corresponding AMRs, would not solve, but only displace the problem of being tied to a specific language’s lexical semantic inventory.7 On the other hand, AMR does contain abstract meaning components that represent language phenomena we may consider as universals: negation, occurrence of named entities, semantic events and their related participants, as well as semantic relations such as Possession, Purpose or Instrument.8 We argue that this abstract structure again pushes AMR more towards an interlingua. Hence, the emergent interest in cross-lingual (A)MR (Oepen et al., 2020; Fan and Gardent, 2020; Sheth et al., 2021; Sherborne and Lapata, 2021) is well justified. However, even if AMR’s inventory may favor an interlingual representation, we cannot, in general, expect a homomorphism of AMRs constructed from semantically equivalent sentences in various languages, given wide-spread phenomena that can preclude a uniform AMR representation, such as constructions involving head-switching phenomena or differences in lexical meaning. Such a middle-ground is indicated by our results: (Too) much divergence may be involved when mapping non-English sentences to original ENAMRs directly, which is p"
2021.iwpt-1.6,kingsbury-palmer-2002-treebank,0,0.747824,"ing: i) one that translates between two natural languages and ii) one that translates between a natural language and a meaning representation. This way, the cross-lingual AMR construction process is more transparent and can be better analyzed. Introduction Abstract Meaning Representation (AMR), introduced by Banarescu et al. (2013), aims at representing the meaning of a sentence in a semantic graph format. Nodes represent entities, events and concepts, while (typed) edges express their relations. AMR itself, as of now, is English-focused, e.g., predicate frames are linked to English PropBank (Kingsbury and Palmer, 2002). However, the abstract nature of AMR, and the fact that they are not explicitly linked to syntactic structure, make it appealing for extracting semantic structure of sentences in various languages. This insight led to the recent interest in a new task: cross-lingual AMR parsing (Damonte and Cohen, 2018). Here, researchers develop models to project sentences from different languages onto AMR graphs. ModIn our work we test these hypotheses by translating the source language sentences into English with a strong NMT system, and parse the resulting English sentences using a strong AMR parser. We s"
2021.iwpt-1.6,2020.emnlp-main.195,0,0.79945,"a strong NMT system and a strong AMR parser. Our experiments show that T+P outperforms a recent stateof-the-art system across all tested languages: German, Italian, Spanish and Mandarin with +14.6, +12.6, +14.3 and +16.0 Smatch points. 1 Let’s go! Andiamo! Lasst uns gehen! xamr 我们走吧 recommend-01 arg1 go-01 we arg0 Figure 1: Cross-lingual AMR parsing as introduced by Damonte and Cohen (2018). els that have recently been proposed are typically trained on large-scale silver data and learn to directly project the non-English sentences onto their AMR graphs (see Figure 1) (Damonte and Cohen, 2018; Blloshmi et al., 2020). However, there is an intuitive baseline that we argue has so-far received too little attention: translate+parse, T+P. It first translates a sentence to a pivot language and applies a mono-lingual parser for that language. In light of the rapid progress of both NMT and AMR parsing models for English, our hypothesis is that this baseline has become more effective and thus more realistic. Moreover, we argue that it could be beneficial to disentangle two key latent representations involved in the process of cross-lingual AMR parsing: i) one that translates between two natural languages and ii) o"
2021.iwpt-1.6,P07-2045,0,0.0087738,"Computational Linguistics We will release all code under public license.1 2 via an iterative BFS writing traversal (Cai and Lam, 2019, 2020). Related work 3 Cross-lingual AMR parsing Cross-lingual AMR parsing was introduced by Damonte and Cohen (2018). They trained an alignmentbased AMR parser model that leverages large amounts of parallel silver AMR data obtained through annotation projection from a curated parallel corpus. The authors also discussed translate+parse (T+P) as a baseline using either the NMT systems Google translate and Nematus (Sennrich et al., 2017), or the SMT system Moses (Koehn et al., 2007), together with a mono-lingual transition-based parser (Damonte et al., 2017). However, their best T+P approach was Google Translate (GT) – which cannot be fully replicated by other researchers since both training data and model structure are hidden. Given the recent advances in NMT (Barrault et al., 2019, 2020) and mono-lingual AMR parsing (Xu et al., 2020), where parsers now achieve scores on par with human IAA assessments (c.f. Banarescu et al. (2013)), we show that time is ripe to put more spotlight on T+P. Blloshmi et al. (2020) address the problem from complementary perspectives: i) they"
2021.iwpt-1.6,D19-1393,0,0.236397,"rong performance in cross-lingual AMR parsing across all considered languages, outperforming task-focused state-of-the-art models in all settings. We also discuss fairer evaluation of cross-lingual AMR parsing and relevant implications of this work for research into cross-lingual AMR parsing. ∗ *Equal contribution. 58 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 58–64 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics We will release all code under public license.1 2 via an iterative BFS writing traversal (Cai and Lam, 2019, 2020). Related work 3 Cross-lingual AMR parsing Cross-lingual AMR parsing was introduced by Damonte and Cohen (2018). They trained an alignmentbased AMR parser model that leverages large amounts of parallel silver AMR data obtained through annotation projection from a curated parallel corpus. The authors also discussed translate+parse (T+P) as a baseline using either the NMT systems Google translate and Nematus (Sennrich et al., 2017), or the SMT system Moses (Koehn et al., 2007), together with a mono-lingual transition-based parser (Damonte et al., 2017). However, their best T+P approach wa"
2021.iwpt-1.6,2021.eacl-main.30,0,0.570115,"Missing"
2021.iwpt-1.6,2020.acl-main.703,0,0.123157,"Missing"
2021.iwpt-1.6,P18-1037,0,0.0217924,"g., due (Mono-lingual) AMR parsing Mono-lingual AMR parsing equally made big strides in recent years, so that today AMR parsers deliver benchmark scores that are on-par with measured human IAA. The latest step forward was achieved with neural sequence-to-sequence models pre-trained on large-scale MT benchmark data (Roberts et al., 2020; Xu et al., 2020) or are fine-tuning selfsupervised seq-to-seq language models such as T5 or BART (Lewis et al., 2019; Bevilacqua et al., 2021). Previou models perform parsing based on different techniques, e.g., predicting latent alignments jointly with nodes (Lyu and Titov, 2018), or 2 They are implemented in EasyNMT, a SOTA NMT package: https://github.com/UKPLab/EasyNMT 3 See https://huggingface.co/ Helsinki-NLP for scores on benchmarks. 4 https://github.com/bjascob/amrlib 5 i) Unlabeled: score without node labels, ii) No WSD: score w/o predicate sense disambiguation; iii) Reentrancies: score on re-entrant nodes (coreference); iv) Concepts: score on concept nodes; v) Named Ent.: indicating NER performance; vi) negation: polarity detection performance; vii) SRL: semantic role labeling performance. 1 https://github.com/Heidelberg-NLP/ simple-xamr 59 Original Sentence a"
2021.iwpt-1.6,2020.conll-shared.1,0,0.045269,"Missing"
2021.iwpt-1.6,2020.emnlp-main.196,0,0.59934,"ned through annotation projection from a curated parallel corpus. The authors also discussed translate+parse (T+P) as a baseline using either the NMT systems Google translate and Nematus (Sennrich et al., 2017), or the SMT system Moses (Koehn et al., 2007), together with a mono-lingual transition-based parser (Damonte et al., 2017). However, their best T+P approach was Google Translate (GT) – which cannot be fully replicated by other researchers since both training data and model structure are hidden. Given the recent advances in NMT (Barrault et al., 2019, 2020) and mono-lingual AMR parsing (Xu et al., 2020), where parsers now achieve scores on par with human IAA assessments (c.f. Banarescu et al. (2013)), we show that time is ripe to put more spotlight on T+P. Blloshmi et al. (2020) address the problem from complementary perspectives: i) they train a system that projects AMR graphs from parsed English sentences to target sentences via a parallel corpus, yielding gold non-English sentences and silver AMRs. Conversely, ii) they train a system that employs an NMT system to translate English sentences from a human-annotated AMR dataset to another language, yielding pairs of silver nonEnglish sentenc"
2021.iwpt-1.6,2020.tacl-1.34,1,0.897153,"airs, or 1,371 AMR-sentence pairs per language. Baselines For all languages (German, Spanish, Italian and Mandarin Chinese), we compare against i) AMREAGER (Damonte and Cohen, 2018), and ii) XL-AMR (Blloshmi et al., 2020). Evaluation metrics Our main evaluation metric is Smatch F1 (Cai and Knight, 2013). The Smatch metric aligns the predicted graph with the gold graph and computes an F1 score that measures normalized triple overlap. Additionally, we calculate F1 scores for finer-grained core semantic sub-tasks Damonte et al. (2017).5 In our analyses (§4.2), we also study results with S2MATCH (Opitz et al., 2020), that offers a potentially fairer evaluation in cross-lingual AMR parsing, since it does not penalize allowed paraphrases that may emerge, e.g., due (Mono-lingual) AMR parsing Mono-lingual AMR parsing equally made big strides in recent years, so that today AMR parsers deliver benchmark scores that are on-par with measured human IAA. The latest step forward was achieved with neural sequence-to-sequence models pre-trained on large-scale MT benchmark data (Roberts et al., 2020; Xu et al., 2020) or are fine-tuning selfsupervised seq-to-seq language models such as T5 or BART (Lewis et al., 2019; B"
2021.iwpt-1.6,xue-etal-2014-interlingua,0,0.436916,"Missing"
2021.iwpt-1.6,palmer-etal-2008-pilot,0,0.0085582,"s that can affect the cross-lingual mapping of sentences into a uniform interlingual AMR.9 We believe that the surprising effectiveness of translate+parse touches upon a key question: to what degree can AMR be considered an interlingua? On one hand, Banarescu et al. (2013) explicitly state that AMR ‘is not designed as an interlingua’. Indeed, AMRs created for English sentences do have a flavour of English, since they are partially grounded in English PropBank (Kingsbury and Palmer, 2002). But linking AMRs to a PropBank of another language, e.g., Brazilian (Duran and Alu´ısio, 2012) or Arabic (Palmer et al., 2008), and parsing non-English sentences into corresponding AMRs, would not solve, but only displace the problem of being tied to a specific language’s lexical semantic inventory.7 On the other hand, AMR does contain abstract meaning components that represent language phenomena we may consider as universals: negation, occurrence of named entities, semantic events and their related participants, as well as semantic relations such as Possession, Purpose or Instrument.8 We argue that this abstract structure again pushes AMR more towards an interlingua. Hence, the emergent interest in cross-lingual (A)"
2021.iwpt-1.6,P02-1040,0,0.110295,"ise (DE: ‘versprechen’); write-compose (DE: ‘verfasst’) strong-resolute (DE: ‘deutlich’); spiritghost (DE: ‘Geist’), etc. In all these cases the crosslingual AMR system predicted the correct concept, but was penalized by SMATCH. A concrete example case, with lexical (see colored nodes) and structural (see dotted nodes) meaning-preserving divergences, is shown in Fig. 2. For future work that applies cross-lingual AMR parsing evaluation, we recommend additional evaluation assessment with S2MATCH. NMT quality The quality of our automatic translations is evaluated with two metrics: i) BLEU score (Papineni et al., 2002) and ii) S(entence)BERT (Reimers and Gurevych, 2019), in order to assess surface-oriented as well as semantic similarity. For SBERT, we create sentence embeddings for both our translations and the English reference sentences and compute pair-wise cosine similarity. Looking at the quality of our MT outputs (Table 4), we see that translation quality is generally quite high. The moderate BLEU scores seem to result more from variation in surface form than from incorrect translations, which is backed by the high cosine similarity scores across languages (and also 60 AMREAGER XL-AMR translate+parse"
2021.iwpt-1.6,D19-1410,0,0.0144295,"rfasst’) strong-resolute (DE: ‘deutlich’); spiritghost (DE: ‘Geist’), etc. In all these cases the crosslingual AMR system predicted the correct concept, but was penalized by SMATCH. A concrete example case, with lexical (see colored nodes) and structural (see dotted nodes) meaning-preserving divergences, is shown in Fig. 2. For future work that applies cross-lingual AMR parsing evaluation, we recommend additional evaluation assessment with S2MATCH. NMT quality The quality of our automatic translations is evaluated with two metrics: i) BLEU score (Papineni et al., 2002) and ii) S(entence)BERT (Reimers and Gurevych, 2019), in order to assess surface-oriented as well as semantic similarity. For SBERT, we create sentence embeddings for both our translations and the English reference sentences and compute pair-wise cosine similarity. Looking at the quality of our MT outputs (Table 4), we see that translation quality is generally quite high. The moderate BLEU scores seem to result more from variation in surface form than from incorrect translations, which is backed by the high cosine similarity scores across languages (and also 60 AMREAGER XL-AMR translate+parse Metric DE ES IT ZH DE ES IT ZH DE ES IT ZH SMATCH 39"
2021.iwpt-1.6,2020.emnlp-main.437,0,0.0262567,"Missing"
2021.iwpt-1.6,E17-3017,0,0.0412803,"Missing"
2021.starsem-1.6,2020.acl-main.656,0,0.0199505,"y scores do not necessarily reflect understanding (Min et al., 2019), large pretrained models may exploit superficial clues and annotation artifacts (Gururangan et al., 2018; Kavumba et al., 2019). Therefore, the ability of models to generate explanations has become desirable, as this enhances interpretability. Recently, there has been substantial effort to build datasets with natural language explanations (Camburu et al., 2018; Park et al., 2018; Thayaparan et al., 2020). There have also been numerous research works proposing models that are interpretable or explainable (Rajani et al., 2019; Atanasova et al., 2020; Latcinnik and Berant, 2020; Wiegreffe and Marasovi´c, 2021). Our work sheds light in this direction, as our MT L model not only predicts the plausible hypothesis Hj but also generH ates possible next events O2 j and chooses the one that is closer to the given context, thereby making our model more expressive. Commonsense Reasoning. There is growing interest in this research field, which led to the creation of several new resources on commonsense reasoning, in form of both datasets, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2019), CosmosQA (Huang et al., 2019) and k"
2021.starsem-1.6,P17-1152,0,0.026837,"out rate = 0.1. We experimented on GPU size of 11GB and 24GB. Training is performed using cross-entropy loss. The loss function is LαN LI + w ∗ Lsimilarity , where w is a trainable parameter. During our experiment we initialize w = 1. The input format is depicted in Table 3. We report performance by averaging results along with the variance obtained for 5 different seeds. Baselines. We compare to the following baseline models that we apply to the αNLI task, training them on the training portion of the ART dataset (cf. Table 2). • ESIM + ELMo is based on the ESIM model previously used for NLI (Chen et al., 2017). We use (a) ELMo to encode the observations and hypothesis, followed by (b) an attention O2 then passed through two linear layers, one for the αNLI (main task) and another for predicting the 70 Model Majority (from dev set) LMI + BERTScore Infersent  ESIM + ELMo  BERTLarge  GPT-2 +MT L COMET +MT L LMI + MT L Human Performance ESIM+ELMo), as well as BERTLarge . Bhagavatula et al. (2020) re-train the ESIM+ELMo and Infersent models on the ART dataset and fine-tuned the BERT model on the αNLI task and report the results. We find that our unsupervised model with BERTScore (LMI + BERTScore) out"
2021.starsem-1.6,D17-1070,0,0.0174334,"Missing"
2021.starsem-1.6,N19-1423,0,0.0105965,"b) learning which possible next event is more similar supports the model in the αNLI task (inductive transfer). The architecture of LMI + MT L model is shown in Figure 4. The model marked (a) in Figure 4 depicts the LMI model as described in §3. The outputs of the LMI model, which we get from Eq. (2) for both hypotheses are incorporated as an input to the MT L model. Concretely, we feed the MT L classifier a sequence of tokens as stated in part (b) of Figure 4, and aim to compute their contextualized representations using pre-trained BERT. The input format is described in Table 3. Similar to (Devlin et al., 2019), two additional tokens are added [CLS] at the start of each sequence input and [SEP] at the end of each sentence. In the shared layers (see Fig 4(b)), the model first transform the input sequence to a sequence of embedding vectors. Then it applies an attention mechanism that learns contextual relations between words (or sub-words) in the input sequence. For each instance we get four [CLS] embeddings (CLSHj , CLS Hj ; j ∈ [1, 2]) which are Experimental Setup Data. We conduct experiments on the ART (Bhagavatula et al., 2020) dataset. Data statistics are given in Table 2. For evaluation, we meas"
2021.starsem-1.6,N18-2017,0,0.0254514,"ative situations. We found that making language models learn how different hypothetical events can evolve from a premise and result in similar or different future events forming from a premise and how these events can result in similar or different future events helps models to perform better in abduction. Explainability. Despite the success of large pretrained language models, recent studies have raised some critical points such as: high accuracy scores do not necessarily reflect understanding (Min et al., 2019), large pretrained models may exploit superficial clues and annotation artifacts (Gururangan et al., 2018; Kavumba et al., 2019). Therefore, the ability of models to generate explanations has become desirable, as this enhances interpretability. Recently, there has been substantial effort to build datasets with natural language explanations (Camburu et al., 2018; Park et al., 2018; Thayaparan et al., 2020). There have also been numerous research works proposing models that are interpretable or explainable (Rajani et al., 2019; Atanasova et al., 2020; Latcinnik and Berant, 2020; Wiegreffe and Marasovi´c, 2021). Our work sheds light in this direction, as our MT L model not only predicts the plausibl"
2021.starsem-1.6,D19-1243,0,0.0320674,"Missing"
2021.starsem-1.6,2020.emnlp-main.54,0,0.210749,"ameworks (Pearl, 1988), and others on Markov Logics (Singla and Mooney, 2011). Recently, moving away from logic-based abductive reasoning, Bhagavatula et al. (2020) proposed to study languagebased abductive reasoning. They introduced two tasks: Abductive Natural Language Inference (αNLI) and Generation (αNLG). They establish baseline performance based on state-of-the-art language models and make use of inferential structured knowledge from ATOMIC (Sap et al., 2019a) as background knowledge. Zhu et al. (2020) proposed to use a learning-to-rank framework to address the abductive reasoning task. Ji et al. (2020) 13 Their dataset, ROCStories, was later extended in Qin et al. (2019) and Bhagavatula et al. (2020). 74 References proposed a model GRF that enables pre-trained models (GPT-2) with dynamic multi-hop reasoning on multi-relational paths extracted from the external ConceptNet commonsense knowledge graph for the αNLG task. Paul and Frank (2020) have proposed a multi-head knowledge attention method to incorporate commonsense knowledge to tackle the αNLI task. Unlike our previous work in Paul and Frank (2020), which focused on leveraging structured knowledge, in this work, we focus on learning abou"
2021.starsem-1.6,N19-1368,1,0.831282,"led to the creation of several new resources on commonsense reasoning, in form of both datasets, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2019), CosmosQA (Huang et al., 2019) and knowledge bases, e.g. ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019a), or Event2Mind (Rashkin et al., 2018). Recently, many works proposed to utilize external static knowledge graphs (KGs) to address the bottleneck of obtaining relevant commonsense knowledge. Lin et al. (2019) proposed to utilize knowledge graph embeddings to rank and select relevant knowledge triples or paths. Paul and Frank (2019) proposed to extract subgraphs from KGs using graph-based ranking methods and further Paul et al. (2020) adopted the graph-based ranking method and proposed to dynamically extend the KG to combat sparsity. In concurrent work, Paul and Frank (2021) introduced a method to dynamically generate contextually relevant knowledge that guides a model while performing the narrative story completion task. Both hypothetical reasoning and abductive reasoning are understudied problems in NLP. Recently, Tandon et al. (2019) proposed a first large-scale dataset of “What if...” questions over procedural text."
2021.starsem-1.6,2020.findings-emnlp.267,1,0.75789,"Missing"
2021.starsem-1.6,D19-6004,0,0.013258,"d that making language models learn how different hypothetical events can evolve from a premise and result in similar or different future events forming from a premise and how these events can result in similar or different future events helps models to perform better in abduction. Explainability. Despite the success of large pretrained language models, recent studies have raised some critical points such as: high accuracy scores do not necessarily reflect understanding (Min et al., 2019), large pretrained models may exploit superficial clues and annotation artifacts (Gururangan et al., 2018; Kavumba et al., 2019). Therefore, the ability of models to generate explanations has become desirable, as this enhances interpretability. Recently, there has been substantial effort to build datasets with natural language explanations (Camburu et al., 2018; Park et al., 2018; Thayaparan et al., 2020). There have also been numerous research works proposing models that are interpretable or explainable (Rajani et al., 2019; Atanasova et al., 2020; Latcinnik and Berant, 2020; Wiegreffe and Marasovi´c, 2021). Our work sheds light in this direction, as our MT L model not only predicts the plausible hypothesis Hj but als"
2021.starsem-1.6,2021.acl-long.395,1,0.759204,"et al., 2017), ATOMIC (Sap et al., 2019a), or Event2Mind (Rashkin et al., 2018). Recently, many works proposed to utilize external static knowledge graphs (KGs) to address the bottleneck of obtaining relevant commonsense knowledge. Lin et al. (2019) proposed to utilize knowledge graph embeddings to rank and select relevant knowledge triples or paths. Paul and Frank (2019) proposed to extract subgraphs from KGs using graph-based ranking methods and further Paul et al. (2020) adopted the graph-based ranking method and proposed to dynamically extend the KG to combat sparsity. In concurrent work, Paul and Frank (2021) introduced a method to dynamically generate contextually relevant knowledge that guides a model while performing the narrative story completion task. Both hypothetical reasoning and abductive reasoning are understudied problems in NLP. Recently, Tandon et al. (2019) proposed a first large-scale dataset of “What if...” questions over procedural text. They introduced the dataset to study the effect of perturbations in procedural text. Related to our work, Qin et al. (2019) investigated the capabilities of state-of-the-art LMs to rewrite stories with counterfactual reasoning. In our work we util"
2021.starsem-1.6,D19-1282,0,0.013905,"thereby making our model more expressive. Commonsense Reasoning. There is growing interest in this research field, which led to the creation of several new resources on commonsense reasoning, in form of both datasets, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2019), CosmosQA (Huang et al., 2019) and knowledge bases, e.g. ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019a), or Event2Mind (Rashkin et al., 2018). Recently, many works proposed to utilize external static knowledge graphs (KGs) to address the bottleneck of obtaining relevant commonsense knowledge. Lin et al. (2019) proposed to utilize knowledge graph embeddings to rank and select relevant knowledge triples or paths. Paul and Frank (2019) proposed to extract subgraphs from KGs using graph-based ranking methods and further Paul et al. (2020) adopted the graph-based ranking method and proposed to dynamically extend the KG to combat sparsity. In concurrent work, Paul and Frank (2021) introduced a method to dynamically generate contextually relevant knowledge that guides a model while performing the narrative story completion task. Both hypothetical reasoning and abductive reasoning are understudied problems"
2021.starsem-1.6,P19-1416,0,0.0158795,"s for narrative omissions. Our research touches on the issue of hypothetical reasoning about alternative situations. We found that making language models learn how different hypothetical events can evolve from a premise and result in similar or different future events forming from a premise and how these events can result in similar or different future events helps models to perform better in abduction. Explainability. Despite the success of large pretrained language models, recent studies have raised some critical points such as: high accuracy scores do not necessarily reflect understanding (Min et al., 2019), large pretrained models may exploit superficial clues and annotation artifacts (Gururangan et al., 2018; Kavumba et al., 2019). Therefore, the ability of models to generate explanations has become desirable, as this enhances interpretability. Recently, there has been substantial effort to build datasets with natural language explanations (Camburu et al., 2018; Park et al., 2018; Thayaparan et al., 2020). There have also been numerous research works proposing models that are interpretable or explainable (Rajani et al., 2019; Atanasova et al., 2020; Latcinnik and Berant, 2020; Wiegreffe and Ma"
2021.starsem-1.6,D19-1509,0,0.0497667,"Missing"
2021.starsem-1.6,N19-1421,0,0.0161771,"explainable (Rajani et al., 2019; Atanasova et al., 2020; Latcinnik and Berant, 2020; Wiegreffe and Marasovi´c, 2021). Our work sheds light in this direction, as our MT L model not only predicts the plausible hypothesis Hj but also generH ates possible next events O2 j and chooses the one that is closer to the given context, thereby making our model more expressive. Commonsense Reasoning. There is growing interest in this research field, which led to the creation of several new resources on commonsense reasoning, in form of both datasets, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2019), CosmosQA (Huang et al., 2019) and knowledge bases, e.g. ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019a), or Event2Mind (Rashkin et al., 2018). Recently, many works proposed to utilize external static knowledge graphs (KGs) to address the bottleneck of obtaining relevant commonsense knowledge. Lin et al. (2019) proposed to utilize knowledge graph embeddings to rank and select relevant knowledge triples or paths. Paul and Frank (2019) proposed to extract subgraphs from KGs using graph-based ranking methods and further Paul et al. (2020) adopted the graph-based ranking method and pr"
2021.starsem-1.6,P19-1487,0,0.0145623,"such as: high accuracy scores do not necessarily reflect understanding (Min et al., 2019), large pretrained models may exploit superficial clues and annotation artifacts (Gururangan et al., 2018; Kavumba et al., 2019). Therefore, the ability of models to generate explanations has become desirable, as this enhances interpretability. Recently, there has been substantial effort to build datasets with natural language explanations (Camburu et al., 2018; Park et al., 2018; Thayaparan et al., 2020). There have also been numerous research works proposing models that are interpretable or explainable (Rajani et al., 2019; Atanasova et al., 2020; Latcinnik and Berant, 2020; Wiegreffe and Marasovi´c, 2021). Our work sheds light in this direction, as our MT L model not only predicts the plausible hypothesis Hj but also generH ates possible next events O2 j and chooses the one that is closer to the given context, thereby making our model more expressive. Commonsense Reasoning. There is growing interest in this research field, which led to the creation of several new resources on commonsense reasoning, in form of both datasets, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2019), CosmosQA (H"
2021.starsem-1.6,D19-1629,0,0.028073,"e knowledge graph embeddings to rank and select relevant knowledge triples or paths. Paul and Frank (2019) proposed to extract subgraphs from KGs using graph-based ranking methods and further Paul et al. (2020) adopted the graph-based ranking method and proposed to dynamically extend the KG to combat sparsity. In concurrent work, Paul and Frank (2021) introduced a method to dynamically generate contextually relevant knowledge that guides a model while performing the narrative story completion task. Both hypothetical reasoning and abductive reasoning are understudied problems in NLP. Recently, Tandon et al. (2019) proposed a first large-scale dataset of “What if...” questions over procedural text. They introduced the dataset to study the effect of perturbations in procedural text. Related to our work, Qin et al. (2019) investigated the capabilities of state-of-the-art LMs to rewrite stories with counterfactual reasoning. In our work we utilize this dataset to model how to generate possible next events emerging from different hypothetical and counterfactual events. Mostafazadeh et al. (2016) designed the narrative cloze task, a task to choose the correct ending of a story.13 Conversely, Bhagavatula et a"
2021.starsem-1.6,P18-1043,0,0.102899,"tion, as our MT L model not only predicts the plausible hypothesis Hj but also generH ates possible next events O2 j and chooses the one that is closer to the given context, thereby making our model more expressive. Commonsense Reasoning. There is growing interest in this research field, which led to the creation of several new resources on commonsense reasoning, in form of both datasets, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2019), CosmosQA (Huang et al., 2019) and knowledge bases, e.g. ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019a), or Event2Mind (Rashkin et al., 2018). Recently, many works proposed to utilize external static knowledge graphs (KGs) to address the bottleneck of obtaining relevant commonsense knowledge. Lin et al. (2019) proposed to utilize knowledge graph embeddings to rank and select relevant knowledge triples or paths. Paul and Frank (2019) proposed to extract subgraphs from KGs using graph-based ranking methods and further Paul et al. (2020) adopted the graph-based ranking method and proposed to dynamically extend the KG to combat sparsity. In concurrent work, Paul and Frank (2021) introduced a method to dynamically generate contextually"
2021.starsem-1.6,N19-5004,0,0.0149879,"o generate different possible next events, (b) the MT L model learns to predict the best explanaH tion (Hj ) and possible next events (O2 j ) at the same time to perform the αNLI task. H similarity (auxiliary task) between O2 j and O2 . We compute the joint loss function L = LαN LI + w ∗ Lsimilarity ; where w is a trainable parameter, LαN LI and Lsimilarity are the loss function for the αN LI task and auxiliary task, respectively. 4 to the observation (O2 ). Multi-task learning aims to improve the performance of a model for a task by utilizing the knowledge acquired by learning related tasks (Ruder, 2019). We hypothesize that a) H the possible next event O2 j of the more plausible hypothesis Hj should be most similar to observation O2 , and that b) learning which possible next event is more similar supports the model in the αNLI task (inductive transfer). The architecture of LMI + MT L model is shown in Figure 4. The model marked (a) in Figure 4 depicts the LMI model as described in §3. The outputs of the LMI model, which we get from Eq. (2) for both hypotheses are incorporated as an input to the MT L model. Concretely, we feed the MT L classifier a sequence of tokens as stated in part (b) of"
2021.starsem-1.6,D19-1454,0,0.0391897,"Missing"
burchardt-etal-2006-salsa,erk-pado-2006-shalmaneser,1,\N,Missing
burchardt-etal-2006-salsa,burchardt-etal-2006-salto,1,\N,Missing
burchardt-etal-2006-salsa,fliedner-2006-towards,0,\N,Missing
burchardt-etal-2006-salsa,W04-2703,0,\N,Missing
burchardt-etal-2006-salsa,H05-1047,0,\N,Missing
burchardt-etal-2006-salsa,P98-1013,0,\N,Missing
burchardt-etal-2006-salsa,C98-1013,0,\N,Missing
burchardt-etal-2006-salsa,J02-3001,0,\N,Missing
burchardt-etal-2006-salsa,J05-1004,0,\N,Missing
burchardt-etal-2006-salsa,erk-pado-2004-powerful,1,\N,Missing
burchardt-etal-2006-salsa,W04-1906,1,\N,Missing
burchardt-etal-2006-salto,brants-plaehn-2000-interactive,0,\N,Missing
burchardt-etal-2006-salto,P97-1003,0,\N,Missing
burchardt-etal-2006-salto,P98-1013,0,\N,Missing
burchardt-etal-2006-salto,C98-1013,0,\N,Missing
burchardt-etal-2006-salto,P03-1068,1,\N,Missing
burchardt-etal-2006-salto,erk-pado-2004-powerful,1,\N,Missing
C02-1093,W97-0307,0,\N,Missing
C02-1093,P97-1003,0,\N,Missing
C02-1093,A97-1012,0,\N,Missing
C02-1093,A00-1033,0,\N,Missing
C02-1093,P02-1056,1,\N,Missing
C02-1093,A00-1031,0,\N,Missing
C04-1185,P01-1019,0,0.0612952,"Missing"
C04-1185,P03-1014,1,0.909951,"Missing"
C04-1185,callmeier-etal-2004-deepthought,0,\N,Missing
C04-1185,E03-1052,0,\N,Missing
C04-1185,C04-1039,0,\N,Missing
C04-1185,C02-1013,0,\N,Missing
C10-1049,P08-1028,0,0.634627,"by adjectives; (ii) we complement this novel representation of adjective meaning with structured vectors for noun meanings similarly built on attributes as meaning dimensions; (iii) we propose a composition of these representations that mirrors principles of compositional semantics in mapping adjective-noun phrases to their corresponding ontological representation; (iv) we propose and evaluate several metrics for the selection of meaningful components from vector representations. 2 Related Work Adjective-noun meaning composition has not been addressed in a distributional framework before (cf. Mitchell and Lapata (2008)). Our approach leans on related work on attribute learning for ontology induction and recent work in distributional semantics. Attribute learning. Early approaches to attribute learning include Hatzivassiloglou and McKeown (1993), who cluster adjectives that denote values of the same attribute. A weakness of their work is that the type of the attribute cannot be made explicit. More recent attempts to attribute learning from adjectives are Cimiano (2006) and Almuhareb (2006). Cimiano uses attributes as features to arrange sets of concepts in a lattice. His approach to attribute acquisition har"
C10-1049,D09-1045,0,0.382551,"sentative for (ii). Baroni et al. (2010) use lexico-syntactic patterns to represent concepts in a structured VSM whose dimensions are interpretable as empirical manifestations of properties. We rely on similar techniques for the acquisition of structured vectors, whereas our work focusses on exposing the hidden meaning dimensions involved in compositional processes underlying concept modification. The commonly adopted method for modelling compositionality in VSM is vector composition (Mitchell and Lapata, 2008; Widdows, 2008). Showing the benefits of vector composition for language modelling, Mitchell and Lapata (2009) emphasize its potential to become a standard method in NLP. The approach pursued in this paper builds on both lines of research sketched in (i) and (ii) in that we model a specific meaning layer in the semantics of adjectives and nouns in a structured VSM. Vector composition is used to expose their hidden meaning dimensions on the phrase level. 3 3.1 Structured Vector Representations for Adjective-Noun Meaning Motivation Contrary to prior work, we model attribute selection as involving triples of nouns, attributes and 431 COLOR DIRECTION DURATION SHAPE SIZE SMELL SPEED TASTE TEMPERATURE WEIGH"
C10-1049,J07-2002,0,0.0552185,"Missing"
C10-1049,D08-1094,0,0.0622052,"Missing"
C10-1049,hartung-frank-2010-semi,1,0.540323,"f possibly several roles or attributes1 from the semantics of the noun. (1) a. a blue car b. COLOR(car)=blue In this paper, we define a distributional framework that models the compositional process underlying the modification of nouns by adjectives. 1 In the original statement of the theory, adjectives select qualia roles that can be considered as collections of attributes. We focus on property-denoting adjectives as they are valuable for acquiring concept representations for, e.g., ontology learning. An approach for automatic subclassification of property-denoting adjectives is presented in Hartung and Frank (2010). Our goal is to expose, for adjective-noun phrases as in (1a), the attribute in the semantics of the noun that is selected by the adjective, while not being overtly realized on the syntactic level. The semantic information we intend to capture for (1a) is formalized in (1b). Ideally, this kind of knowledge could be extracted from corpora by searching for patterns that paraphrase (1a), e.g. the color of the car is blue. However, linguistic patterns that explicitly relate nouns, adjectives and attributes are very rare. We avoid these sparsity issues by reducing the triple r=hnoun, attribute, ad"
C10-1049,P93-1023,0,0.368901,"sentations that mirrors principles of compositional semantics in mapping adjective-noun phrases to their corresponding ontological representation; (iv) we propose and evaluate several metrics for the selection of meaningful components from vector representations. 2 Related Work Adjective-noun meaning composition has not been addressed in a distributional framework before (cf. Mitchell and Lapata (2008)). Our approach leans on related work on attribute learning for ontology induction and recent work in distributional semantics. Attribute learning. Early approaches to attribute learning include Hatzivassiloglou and McKeown (1993), who cluster adjectives that denote values of the same attribute. A weakness of their work is that the type of the attribute cannot be made explicit. More recent attempts to attribute learning from adjectives are Cimiano (2006) and Almuhareb (2006). Cimiano uses attributes as features to arrange sets of concepts in a lattice. His approach to attribute acquisition harnesses adjectives that occur frequently as concept modifiers in corpora. The association of adjectives with their potential attributes is performed by dictionary look-up in WordNet (Fellbaum, 1998). Similarly, Almuhareb (2006) use"
C10-1049,P06-1015,0,0.151129,"tions. 2 Related Work Adjective-noun meaning composition has not been addressed in a distributional framework before (cf. Mitchell and Lapata (2008)). Our approach leans on related work on attribute learning for ontology induction and recent work in distributional semantics. Attribute learning. Early approaches to attribute learning include Hatzivassiloglou and McKeown (1993), who cluster adjectives that denote values of the same attribute. A weakness of their work is that the type of the attribute cannot be made explicit. More recent attempts to attribute learning from adjectives are Cimiano (2006) and Almuhareb (2006). Cimiano uses attributes as features to arrange sets of concepts in a lattice. His approach to attribute acquisition harnesses adjectives that occur frequently as concept modifiers in corpora. The association of adjectives with their potential attributes is performed by dictionary look-up in WordNet (Fellbaum, 1998). Similarly, Almuhareb (2006) uses adjectives and attributes as (independent) features for the purpose of concept learning. He acquires adjectiveattribute pairs using a pattern-based approach. As a major limitation, these approaches are confined to adjective-at"
C10-1049,W09-0203,0,0.0218266,"Missing"
C10-1049,J10-4006,0,\N,Missing
C10-1049,W05-1003,0,\N,Missing
C10-1108,H05-1042,0,0.0321602,"s automatically from corpus data. Previous work by Dale et al. (2005) and Roth and Frank (2009) on generating NL directions used hand-crafted heuristics. Duboue and McKeown (2003) were the first to model content selection as a machine learning task, in which selection rules are induced from pairs of human-written text and associated sets of database entries. They induce baseline selection rules from exact matches of NL expressions with database entries; in addition, classbased rules are computed by matching database entry types against NL expressions, using statistical co-occurrence clusters. Barzilay and Lapata (2005) incorporate the interplay between multiple events and entities when learning content selection rules using a special link function. Recent work by Liang et al. (2009) focuses on modeling grounded language, by aligning realworld representations with NL text that references corresponding world states. They show how a generative model can be used to segment text into utterances and to identify relevant facts with minimal supervision. Both tasks are handled jointly in a unified framework by training a hierarchical semi-Markov model on pairs of text and world states, thereby modeling sequencing ef"
C10-1108,W02-1022,0,0.0749619,"Missing"
C10-1108,J93-2003,0,0.0128356,"each component and each attribute: p(f, w|a) = p(f |a)λ(dist(f, a)) p(w|a) (1) The individual models are described in more detail in the following subsections. 4.1 Frame Alignment Model This basic frame alignment model specifies the probabilities p(f |a) for aligning an attribute a of type at (i.e., one of the types listed in Section 3.1) to a frame element f labeled as type ft . This alignment model is initialized as a uniform distribution over f and trained using a straight-forward implementation of the EM algorithm, following the well-known IBM Model 1 for alignment in machine translation (Brown et al., 1993). The expectation step (E-step) computes expected counts given occurrences of ft and at under the assumption that all alignments are independent 1:1 correspondences: P 0 0 {hf 0 ,a0 i|ft0 =ft ∧a0t =at } p(f |a ) P count(ft , at ) = 0 {hf 0 ,yi|ft0 =ft } p(f |y) (2) The probabilities are re-estimated to maximize the overall alignment probability by normalizing 962 the estimated counts (M-step): 4.2 count(ft , at ) p(f |a) = P x count(xt , at ) (3) Distance Model We hypothesize that the order of route directions tends to be consistent with the order of maneuvers encoded by the route representati"
C10-1108,burchardt-etal-2006-salto,1,0.836714,"cation that guided them along a route by means of a 2D animation. Subsequently they had to write NL route directions in German for the shown routes. The subjects were allowed to use all information displayed by the web application: named places, buildings, bridges and street names, etc. The resulting directions were POS-tagged with TreeTagger (Schmid, 1997), dependency-parsed with XLE (Maxwell and Kaplan, 1993), and manually revised. Additionally, we annotated framesemantic markup (Fillmore et al., 2003) and gold standard alignments to the route representation using the SALTO annotation tool (Burchardt et al., 2006). Frame semantic markup. The texts are annotated with an inventory of 4 frames relevant for directions (S ELF MOTION, P ERCEPTION, B E ING LOCATED , L OCATIVE RELATION ), with semantic roles (frame elements) such as DIREC TION , GOAL , PATH , LOCATION . Figure 2 illustrates a typical example for the use of the S ELF M OTION frame, once with the elements SOURCE and DIRECTION , and once with the elements DIRECTION and GOAL. Our alignment model uses the frame semantic annotation as structuring information. Gold standard alignments. For evaluation we constructed gold alignments. We asked two annot"
C10-1108,P09-1011,0,0.02701,"Missing"
C10-1108,J93-4001,0,0.0294045,"Directions The corpus of route directions used in this work is a subset of the data collected by Schuldes et al. (2009) in a desk-based experiment. To elicit NL route directions, subjects were shown a web application that guided them along a route by means of a 2D animation. Subsequently they had to write NL route directions in German for the shown routes. The subjects were allowed to use all information displayed by the web application: named places, buildings, bridges and street names, etc. The resulting directions were POS-tagged with TreeTagger (Schmid, 1997), dependency-parsed with XLE (Maxwell and Kaplan, 1993), and manually revised. Additionally, we annotated framesemantic markup (Fillmore et al., 2003) and gold standard alignments to the route representation using the SALTO annotation tool (Burchardt et al., 2006). Frame semantic markup. The texts are annotated with an inventory of 4 frames relevant for directions (S ELF MOTION, P ERCEPTION, B E ING LOCATED , L OCATIVE RELATION ), with semantic roles (frame elements) such as DIREC TION , GOAL , PATH , LOCATION . Figure 2 illustrates a typical example for the use of the S ELF M OTION frame, once with the elements SOURCE and DIRECTION , and once wit"
C10-1108,P09-4010,1,0.926595,"hat specify, for a given navigational task and an automatically computed route representation, a sequence of actions to be followed by the user to reach his or her goal. A corpusbased approach to generate route directions involves (i) the selection of elements along the route that need to be mentioned, and (ii) the induction of a mapping from route elements to linguistic structures that can be used as a basis for NL generation. Previously developed natural language generation (NLG) systems make use of simple heuristics for the task of content selection for route directions (Dale et al., 2005; Roth and Frank, 2009). In our work, we aim for a corpus-based approach that can be flexibly modeled after natural, humanproduced directions for varying subtasks (e.g., indoor vs. outdoor navigation), and that facilitates multilingual extensions. By employing salient landmarks and allowing for variation in NL realization, such a system is expected to generate natural sounding directions that are easier to memorize and easier to follow than directions given by a classical route planner or navigation system. This paper presents an Expectation-Maximization (EM) based algorithm that aligns geographical route representa"
C10-1108,W09-2814,1,0.921155,"t al., 1977) to learn correspondences between (segments of) the geographical representation of a route and linguistic instructions of how to follow this route in order to arrive at a designated goal. We are specifically interested in identifying which parts of a route are realized in natural language and which kinds of semantic constructions are used to express them. As a data source for inducing such correspondences we use a parallel corpus of route representations and corresponding route directions that were collected in a controlled experiment for navigation in an urban street network (cf. Schuldes et al. (2009)). For the alignment task, the routes were compiled to a specification format that has been realized in an internal version of an online route planner. Figure 1 displays the route rep959 Figure 1: A (partial) route representation of the route segment displayed on the right. resentation for a small route segment (a junction connecting ’Hauptstraße’ and ’Leyergasse’). The corresponding part of a NL route direction is displayed in Figure 2. The route representation and the NL direction share some common concepts: For example, both contain references to a landmark called “Sudpfanne” (marked as [1]"
C10-1108,E03-1026,0,0.0256458,"nt. As route directions are typically presented in a linear order with respect to the route, we incorporate an additional distance model λ in our alignment. We further account for word choice within a frame element as an additional factor. The word choice model p(w|a) will exploit attribute type and value information in the route representations that are reflected in word choice in the linguistic instructions. Both extensions are inspired by and share similarities with models that have been successfully applied in work on text alignment for the task of machine translation (Vogel et al., 1996; Tiedemann, 2003). Our full model is a distribution over frame elements f and words w that factorizes the three above mentioned parts under the assumption of independence between each component and each attribute: p(f, w|a) = p(f |a)λ(dist(f, a)) p(w|a) (1) The individual models are described in more detail in the following subsections. 4.1 Frame Alignment Model This basic frame alignment model specifies the probabilities p(f |a) for aligning an attribute a of type at (i.e., one of the types listed in Section 3.1) to a frame element f labeled as type ft . This alignment model is initialized as a uniform distri"
C10-1108,P10-1083,0,0.0273985,"ed as follows: In Section 2 we discuss related work. Section 3 introduces the task, and the representation formats and resources we use. Section 4 introduces a basic Expectation-Maximization model and two extensions for the alignment task. Section 5 outlines the experiments and presents the evaluation results. In Section 6 we conclude and discuss future work. 2 Related Work Various aspects of route directions have been subject of research in computational linguistics, ranging from instructional dialogues in MapTask (Anderson et al., 1991) to recent work on learning to follow route directions (Vogel and Jurafsky, 2010). However, little work has been done on generating NL directions based on data from Geographical Information Systems (Dale et al., 2005; Roth and Frank, 2009). NLG systems are typically realized as pipeline architectures (Reiter and Dale, 2000). As a first step, they compute a set of messages that represent the information to be conveyed to a user, given a specific communicative task (Content Selection). Selecting appropriate content for a task can be defined heuristically, by manually crafted rules or by learning content selection rules automatically from corpus data. Previous work by Dale et"
C10-1108,C96-2141,0,0.166103,"of direction alignment. As route directions are typically presented in a linear order with respect to the route, we incorporate an additional distance model λ in our alignment. We further account for word choice within a frame element as an additional factor. The word choice model p(w|a) will exploit attribute type and value information in the route representations that are reflected in word choice in the linguistic instructions. Both extensions are inspired by and share similarities with models that have been successfully applied in work on text alignment for the task of machine translation (Vogel et al., 1996; Tiedemann, 2003). Our full model is a distribution over frame elements f and words w that factorizes the three above mentioned parts under the assumption of independence between each component and each attribute: p(f, w|a) = p(f |a)λ(dist(f, a)) p(w|a) (1) The individual models are described in more detail in the following subsections. 4.1 Frame Alignment Model This basic frame alignment model specifies the probabilities p(f |a) for aligning an attribute a of type at (i.e., one of the types listed in Section 3.1) to a frame element f labeled as type ft . This alignment model is initialized a"
C10-1108,W03-1016,0,\N,Missing
C10-1108,W01-0100,0,\N,Missing
C98-1054,P92-1005,0,0.0272462,"igned an underspecified truth-conditional interpretation (Genabith and Crouch, 1997). a Appendix B gives a relational formulation of the correspondence between f-structures and UDRSs. The UDRS representations are processed by semantic-based transfer. The resulting system is bi-directional. Consider again the simple head switching case discussed in (1) and (3) above. (4) shows the corresponding UDRSs. The structural nfismatch between the two fstructures has disappeared on the level of UDRS representations and transfer is facilitated. 4 3A similar coresl)oi, dence between f-structures and QLFs (Alshawi and Crouch, 1992) lt~ been shown in (Genabith and Crouch, 1996), 4In the implementation, a Neo-Davidsonian style en343 4.1 Embedded Head-Switching The syntactic transfer rules (2) are supplemented by (5). The complex rule for gcrne in (5) overrides 5 (2d) and the COMP rule in (5). For each additional level of embedding triggered by head switching adjuncts a special rule is needed. (5) { { { { { { vermuten(E) } &lt;-> { suspect(E) }. Ede(X) } &lt;-> { Ede(X) }. COMP(E,X) } &lt;-> { COMP(E,X) }. gerne(X),ADJN(E,X),COMP(EI,E) } # SUBJ(E,Y) } &lt;-> like(X) ,XCOMP(X,E) ,SUBJ(X,Y) ,COMP(EI,X) }. By contrast, on the level of UD"
C98-1054,C96-1045,1,0.874848,"Missing"
C98-1054,P97-1052,1,0.868231,"Missing"
C98-1054,J91-2001,0,0.466264,"ransfer This section presents a simple bidirectional translation between LFG f-structures and term representations which serve as input to and output of a transfer component developed within the Verbmobil project (Dorna and Emele, 1996a). The term representation is inspired by earlier work (Kay et al., 1994; Caspari and Sehmid, 1994) which uses terms as a quasisemantic representation for transfer and generation. The translation between f-structures and terms is based on the correspondence between directed graphs representing f-structures and the functional interpretation of these graphs (cf. (Johnson, 1991)). Given an arc labeled f which connects two nodes nl and n2 in a graph, the same can be expressed by a function f ( n l ) = n2. An f-structure is the set of such feature equations describing the associated graph. Instead of feature equations f ( n t ) = n2 we use the relational notation f ( n l , n2). Using this idea f-structures can be converted into sets of terms and vice versa. 1 F-structure 1For motivation why we prefer term representations 342 P R E D features and their ""semantic form"" values are given special treatment. Instead of introducing PRED terms we build unary relations with the"
C98-1054,E93-1024,0,0.0534882,"d semantic representations, respectively. Both support ambiguity preserving transfer to differing degrees (NP scope, operators, adjuncts). F-structure based syntactic represenlr : (11) l~:[ ~ oiten(l~12I l[~:~ Zm: I 345 tations may come up against structural mismatches in transfer. The original co-description based approach in (Kaplan et al., 1989) faced problems when it came to examples involving embedded head-switching and multiple adjuncts (Sadler and Thompson, 1991), which led to the introduction of a restriction operator, to enable transfer on partial f-structures or semantic structures (Kaplan and Wedekind, 1993). One might suppose that the need to refer to partial structures is an artifact of the correspondencebased approach, which doesn&apos;t allow the mapping from a single node of the source f-structure to distinct nodes in the target f-structure without violation of the functional property of the T correspondence. On closer inspection, though, the rewriting approach to syntactic f-structureterm translations presented above suffers from the very same problems that were met by the correspondence-based approach in (Kaplan et al., 1989). By contrast, transfer on the semantic U D R S representations does n"
C98-1054,E89-1037,0,0.432715,"eed to be sensible representations for both parsing and generation. LFG Lstructures are abstract, ""high-lever&apos; syntactic representations which go some way towards meeting these often irreconcilable requirements. * We would like to thank H. Kamp, M. Schiehlen and the anonymous reviewers for helpful comments on earlier versions of this article. Part of this work was flmded by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the Verbinobil project under grant 01 IV 701 Na. 341 Correspondence-based transfer on f-structures has been proposed in (Kaplan et al., 1989). A closer look at translation problems involving structural mismatches between languages in particular head switching phenomena (Sadler and Thompson, 1991) led to the contention that transfer is facilitated at the level of semantic representation, where structm&apos;al differences between languages are often neutralized. Structural misalignment is treated in semantics construction involving a restriction operator (Kaplan and Wcdekind, 1993) where f-structures are related to (possibly sets of) disambiguated semantic representations. Given the high potential of semantic ambiguities, the advantage of"
C98-1054,J93-4001,0,0.243101,"Missing"
C98-1054,C96-1054,1,\N,Missing
C98-1054,C96-1024,0,\N,Missing
C98-1054,P98-1060,1,\N,Missing
C98-1054,C98-1058,1,\N,Missing
D11-1050,J10-4006,0,0.0475271,"attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline. 1 Introduction Corpus-based statistical modeling of semantics is gaining increased attention in computational linguistics. This field of research includes distributional vector space models (VSMs), i.e., models that represent the semantics of words or phrases as vectors over high-dimensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the (1) a. a hotvalue summerconcept b. TEMPERATURE(summer) = hot It is by way of the composition of adjective and noun that specific attributes are selected from the adjective’s space of possible attribute meanings, and typically lead to a disambiguation of the adjective and possibly the noun. Hartung and Frank (2010) were the first to model this insight in a VSM by representing the meaning of adjectives and nouns in semantic vec"
D11-1050,D10-1115,0,0.589681,"ses Matthias Hartung and Anette Frank Computational Linguistics Department Heidelberg University {hartung,frank}@cl.uni-heidelberg.de Abstract data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et ´ S´eaghdha, 2010), or word sense disamal., 2010; O biguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE , TASTE or EMO TIONALITY . These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective-noun phrase, as illustrated in (1). This"
D11-1050,W10-2805,0,0.46094,"ctive-Noun Phrases Matthias Hartung and Anette Frank Computational Linguistics Department Heidelberg University {hartung,frank}@cl.uni-heidelberg.de Abstract data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et ´ S´eaghdha, 2010), or word sense disamal., 2010; O biguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE , TASTE or EMO TIONALITY . These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective-noun phrase"
D11-1050,C10-1049,1,0.170083,"igning Attributes to Adjective-Noun Phrases Matthias Hartung and Anette Frank Computational Linguistics Department Heidelberg University {hartung,frank}@cl.uni-heidelberg.de Abstract data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et ´ S´eaghdha, 2010), or word sense disamal., 2010; O biguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE , TASTE or EMO TIONALITY . These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adject"
D11-1050,W11-2506,1,0.812311,"Missing"
D11-1050,P10-1116,0,0.0322438,"Missing"
D11-1050,P08-1028,0,0.0899661,"-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a process combining vectors (or matrices, resp.) to yield a new, contextualized vector representation within the same semantic space. Latent Dirichlet Allocation, aka. Topic Models (TMs). LDA is a generative probabilistic model for document collections. Each document is represented as a mixture over latent topics, where each topic is a probability distribution over words (Blei et al., 2003). These topics can be used as dense features for, e.g., document clustering. Depending on the number of topics, which has to"
D11-1050,D09-1045,0,0.0409131,"be pre-specified, the dimensionality of the document representation can be considerably reduced in comparison to simple bag-of-words models. The remainder of this paper will assume some familiarity with LDA and the LDA terminology as introduced in Blei et al. (2003). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008). We will discuss L-LDA in Section 3. Distributional VSMs and TMs. The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model with dimensions set to topics over bag-of-words features. In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. Mitchell and Lapata (2010) compare VSMs defined over bags of context words vs. latent topics in a similarity judgement task. Their results indicate that a multiplicative setting works best for vector composition in word-based models, while vector addition is"
D11-1050,P10-1045,0,0.188819,"Missing"
D11-1050,C00-2094,0,0.0415483,"topics cannot be related to previously defined target categories. For attribute selection, the LDA-inferred topics need to be linked to semantic attributes. Therefore, we apply two extensions of standard LDA that are capable of taking supervised category information into account, either implicitly or directly, by including an additional observable variable into the generative process. In general, LVMs can be expected to overcome sparsity issues that are frequently encountered in distributional models. This positive smoothing effect is achieved by marginalization over the latent variables (cf. Prescher et al. (2000)). For instance, it is unlikely to observe a dependency path linking the adjective mature to the attribute MATURITY. Such a relation is more likely for young, for example. If young co-occurs with mature in a different pseudodocument (AGE might be a candidate), this results in a situation where (i) young and mature share one or more latent topics and (ii) the topic proportions for the attributes MATURITY and AGE will become similar to the extent of common words in their pseudodocuments. Consequently, the final attribute model is expected to assign a (small) positive probability to the relation"
D11-1050,D09-1026,0,0.726465,"006) and Hartung and Frank (2010), which are both embedded in a purely distributional setting. Specifically, we use Latent Dirichlet Allocation (LDA; Blei et al. (2003)) to train an attribute model that captures semantic information encoded in adjectives and nouns independently of one another. Following Hartung and Frank (2010), this model is embedded into a VSM that employs vector composition to combine the meaning of adjectives and nouns. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al. (2009)). Both will be presented in detail in Section 3. Our aims in this paper are two-fold: (i) We investigate LDA as a modeling framework in the attribute selection task, as its use of topics as latent variables may alleviate inherent sparsity problems faced by prior work using pattern-based (Almuhareb, 2006) or vector space models (Hartung and Frank, 2010). (ii) While these prior approaches were restricted to a confined set of 10 attributes, we will we apply our 1 The figure is adopted from the distributional setting of Hartung and Frank (2010), with component values defined by pattern frequency"
D11-1050,P10-1044,0,0.147156,"Missing"
D11-1050,P10-1093,0,0.0167966,"be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a process combining vectors (or matrices, resp.) to yield a new, contextualized vector representation within the same semantic space. Latent Dirichlet Allocation, aka. Topic Models (TMs). LDA is a generative probabilistic model for document collections. Each document is represented as a mixture over latent topics, where each topic is a probability distribution over words (Blei et al., 2003). These topics can be used as dense features for, e.g., document clustering. Depending on the number of topics, which has to be pre-specified, the dimensio"
D11-1050,C00-2137,0,0.0222074,"s. We evaluate the performance of the VSMs based on C-LDA and L-LDA in two experimental settings, contrasting the problem of attribute selection on semantic spaces of radically different dimensionality, using sets of 10 vs. 206 attributes. Evaluation measures. We evaluate against two gold standards consisting of adjective-noun phrases (or adjective-noun pairs) and their associated attribute meanings. We report precision, recall and f1 -score. Where appropriate, we test differences in the performance of various model configurations for statistical significance in a randomized permutation test (Yeh, 2000), using the sigf tool (Pad´o, 2006). Baselines. We compare our models against two baselines, PATT VSM and D EP VSM. PATT SVM is reconstructed from Hartung and Frank (2010). It is grounded in a selection of lexical patterns that identify the target elements (adjectives and nouns) for the vector basis elements (i.e., the attribute nouns) in a local context window. The component values are defined using raw frequency counts over the extracted patterns. D EP VSM is similar to PATT VSM; however, it relies on dependency paths that connect the target elements and attributes in local contexts. The pat"
D12-1016,S12-1051,0,0.039481,"Missing"
D12-1016,J08-1001,0,0.0596153,"rning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior"
D12-1016,N04-1015,0,0.0367931,"sed clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1 -score. 1 The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sente"
D12-1016,W09-2816,0,0.0210245,"coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior work in NLG (Belz et al., 2009, inter alia). The specific set-up we examine, however, allows us to further investi171 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 171–182, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics gate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. Example (1), extracted from our corpus of aligned texts,illustrates this point: Both texts report on the same event of locating victims i"
D12-1016,C10-3009,0,0.121121,"Missing"
D12-1016,C10-1011,0,0.131038,"Missing"
D12-1016,J93-2003,0,0.0355543,"Missing"
D12-1016,J08-4005,0,0.580087,"or the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of aligning words in general has been studied extensively in previous work, for example as part of research in statistical machine translation (SMT). Typically, alignment models in SMT are trained by observing and (re-)estimating co-occurrence counts of word pairs in parallel sentences (Brown et al., 1993). The same methods have also been applied in monolingual settings, for example to align words in paraphrases (Cohn et al., 2008). In contrast to traditional word alignment tasks, our focus is not on pairs of isolated sentences but on aligning predicates within the discourse contexts in which they are situated. Furthermore, text pairs for our task should not be strictly parallel as we are specifically interested in the impact of different discourse contexts. In Section 5, we will show that this particular setting indeed constitutes a more challenging task compared to traditional word alignment in parallel or paraphrasing sentences. Another set of related tasks is found in the area of textual inference. Since 2006, there"
D12-1016,I05-5002,0,0.0397043,"ncebased word alignment setting and our novel alignment task that operates on full texts. 5.1.1 Sentence-level Alignment Setting For sentence-based predicate alignment we make use of the following three corpora that are wordaligned subsets of the paraphrase collections described in (Cohn et al., 2008): MTC consists of 100 177 sentence pairs from the Multiple-Translation Chinese Corpus (Huang et al., 2002), Leagues contains 100 sentential paraphrases from two translations of Jules Verne’s “Twenty Thousand Leagues Under the Sea”, and MSR is a sub-set of the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 130 sentence pairs. All three paraphrase collections are in English. Results for these experiments are reported in Section 5.3.1. Note that in order to determine alignment candidates, we apply the same pre-processing steps as used for the annotation of our corpus. The semantic parser identified an average number of 3.8, 5.1 and 4.7 predicates per text (i.e., per paraphrase sentence) in MTC, Leagues and MSR, respectively. All models are evaluated against the subset of gold standard alignments (cf. Cohn et al. (2008)) between pairs of words marked as predicates. 5.1.2 Text-level"
D12-1016,J95-2003,0,0.422835,"predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investi"
D12-1016,D11-1051,0,0.0596534,"larity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1 , p2 depending on their relatedness within the VerbNet class hierarchy:  1.0     0.8 simVN (p1 , p2 ) =     0.0 if ∃C : p1 , p2 ∈ C if ∃C, Cs : Cs ∈ sub(C) ∧ p1 , p2 ∈ C ∪ Cs (4) else Distributional similarity. As some predicates may not be covered by the WordNet and VerbNet hierarchies, we additionally calculate similarity based on distributional meaning in a semantic space (Landauer and Dumais, 1997). Following the traditional bag-of-words approach that has been applied in related tasks (Guo and Diab, 2011; Mitchell and Lapata, 2010), we consider the 2,000 most frequent context words c1 , . . . , c2000 ∈ C as dimensions of a vector space and define predicates as vectors using their Pointwise Mutual Information (PMI): p~ = (PMI(p, c1 ), . . . , PMI(p, c2000 ) (5) freq(x, y) freq(x) ∗ freq(y) Given the vector representations of two predicates, we calculate their similarity as the cosine of the angle between the two vectors: with PMI(x, y) = simDist (p1 , p2 ) = p~1 · p~2 |p~1 |∗ |p~2 | (6) Argument similarity. While the previous similarity measures are purely type-based, argument similarity integ"
D12-1016,N06-1014,0,0.0847026,"tead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8 Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 178 5.3 Results We measure precision as the number of pred"
D12-1016,D08-1084,0,0.350652,"Missing"
D12-1016,C10-1087,0,0.0314354,"Missing"
D12-1016,P10-1123,0,0.0347533,"Missing"
D12-1016,J03-1002,0,0.00933824,"settings. In order to assess the benefits of the clustering step, we propose a second baseline that uses the same similarity measures and thresholds as our Full model, but omits the clustering step described in Section 4.3. Instead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8 Note that the performance of this system lies slightly below the state-of-the"
D12-1016,J05-1004,0,0.118418,"Missing"
D12-1016,N04-3012,0,0.0569774,"= args(p2 ), we define the following measures. WordNet similarity. Given all pairs of synsets s1 , s2 that contain the predicates p1 , p2 , respectively, we compute the maximal similarity using the information theoretic measure described in Lin (1998). Our implementation exploits the WordNet hierarchy 6 All token-based frequency counts (i.e., f req() and idf ()) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition. 175 (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al. (2004) to compute Lin’s measure: simWN (p1 , p2 ) = IC(lcs(s1 , s2 )) IC(s1 ) ∗ IC(s2 ) (3) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy ("
D12-1016,C08-1092,0,0.0642078,"compute Lin’s measure: simWN (p1 , p2 ) = IC(lcs(s1 , s2 )) IC(s1 ) ∗ IC(s2 ) (3) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy (cf. Richens (2008)), we further compute similarity between verbal predicates using VerbNet (Kipper et al., 2008). Verbs in VerbNet are categorized into semantic classes according to their syntactic behavior. A class C can recursively embed sub-classes Cs ∈ sub(C) that represent finer semantic and syntactic distinctions. We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1 , p2 depending on their relatedness within the VerbNet class hierarchy:  1.0     0.8 simVN (p1 , p2 ) =     0.0 if ∃C : p1 , p2 ∈ C if ∃C, Cs : Cs ∈ sub(C) ∧ p1 , p2 ∈"
D12-1016,S12-1030,1,0.855971,"le corpus of paired texts. The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse. We investigate a graph-based clustering method for induc1 See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010). 2 Note that we provide details regarding the construction of a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012). 172 ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph. The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of al"
D12-1016,S10-1008,0,0.0877217,"dications. This paper focuses on the first of these tasks, henceforth called predicate alignment.2 In line with data-driven approaches in NLP, we automatically align predicates in a suitable corpus of paired texts. The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse. We investigate a graph-based clustering method for induc1 See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010). 2 Note that we provide details regarding the construction of a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012). 172 ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph. The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for t"
D12-1016,P10-1098,0,0.0295023,"rms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1 -score. 1 The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. For our purposes"
D12-1016,U06-1019,0,0.10726,"tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8 Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 178 5.3 Results We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments divided by the total n"
D12-1016,W09-0621,0,0.0512409,"Missing"
D12-1016,W07-1401,0,\N,Missing
D17-1021,P16-1061,0,0.16936,"ional hardware are really being obsoleted by microprocessor-based machines]”, said Mr. Benton. [AnaphS As a result of this trendAA , longtime powerhouses HP, IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.] A major obstacle for solving this task is the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Ane"
D17-1021,P16-1036,0,0.0158228,"nking neural coreference model proposed in Clark and Manning (2015), and their improved model in Clark and Manning (2016a), which integrates a loss function (Wiseman et al., 2015) which learns distinct feature representations for anaphoricity detection and antecedent ranking. Siamese Nets distinguish between similar and dissimilar pairs of samples by optimizing a loss over the metric induced by the representations. It is widely used in vision (Chopra et al., 2005), and in NLP for semantic similarity, entailment, query normalization and QA (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016; Das et al., 2016). (2) Congress has focused almost solely on the fact that [special education is expensive - and that it takes away money from regular education.] (3) Environmental Defense [...] notes that [Antec Mowing the lawn with a gas mower produces as much pollution [...] as driving a car 172 miles.] [AnaphS This fact may [...] explain the recent surge in the sales of [...] old-fashioned push mowers [...]]. KZH13 presented an approach for resolving six typical shell nouns following the observation that CSNs are easy to resolve based on their syntactic structure alone, and the assumption that ASNs share l"
D17-1021,D16-1131,0,0.0583456,"lution of events in biomedical text that refer to a single or multiple clauses. However, instead of selecting the correct antecedent clause(s) (our task) for a given event, their model is restricted to classifying the event into six abstract categories: this these changes, responses, analysis, context, finding, observation, based on its surrounding context. While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types. More related to our work is Anand and Hardt (2016) who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset. They employ features modeling distance, containment, discourse structure, and – less effectively – content and lexical correlates.4 Closest to our work is Kolhatkar et al. (2013b) Related and prior work Abstract anaphora has been extensively studied in linguistics and shown to exhibit specific properties in terms of semantic antecedent types, their degrees of abstractness, and general dis2 3 4 Abadi et al. (2015) 222 We thank the authors for making their data available. T"
D17-1021,P16-1139,0,0.013068,"croprocessor-based machines]”, said Mr. Benton. [AnaphS As a result of this trendAA , longtime powerhouses HP, IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.] A major obstacle for solving this task is the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work."
D17-1021,S15-1035,0,0.184916,"ew smaller-scale corpora have been constructed. We evaluate our models on a subset of the ARRAU corpus (Uryupina et al., 2016) that contains abstract anaphors and the shell noun corpus used in Kolhatkar et al. (2013b).3 We are not aware of other freely available abstract anaphora datasets. Little work exists for the automatic resolution of abstract anaphora. Early work (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008) has focused on spoken language, which exhibits specific properties. Recently, event coreference has been addressed using feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016). Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase – acquire) with no special focus on (pro)nominal anaphora. Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1). Rajagopal et al. (2016) proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses. However, instead of selecting the correct antecedent clause("
D17-1021,P15-1136,0,0.16758,"is task is the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. 1 221 Example drawn from ARRAU (Uryupina et al., 2016). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics learning"
D17-1021,D16-1245,0,0.049568,"Missing"
D17-1021,D14-1162,0,0.0803502,"e on the devset. As devsets we employ the ARRAUAA corpus for shell noun resolution and the ASN corpus for unrestricted abstract anaphora resolution. For each trial we record performance on the test set. We report the best test s@1 score in 10 trials if it is better than the scores from default HPs. The default HPs and prior distributions for HPs used by TPE are given below. The (exact) HPs we used can be found in Supplementary Materials. Input representation. To construct word vectors wi as defined in Section 3, we used 100-dim. GloVe word embeddings pre-trained on the Gigaword and Wikipedia (Pennington et al., 2014), and did not fine-tune them. Vocabulary was built from the words in the training data with frequency in {3, U(1, 10)}, and OOV words were replaced with an UNK token. Embeddings for tags are initialized with values drawn  from the uniform distri1 1 bution U − √d+t , √d+t , where t is the number of Candidates extraction. Following KZH13, for every anaphor we create a list of candidates by extracting all syntactic constituents from sentences which contain antecedents. Candidates that differ from antecedents in only one word, or one word and punctuation, were as well considered as antecedents. C"
D17-1021,P03-1054,0,0.0449401,"Missing"
D17-1021,poesio-artstein-2008-anaphoric,0,0.414535,"ouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016). It produces large amounts of instances and is easily adaptable to other languages. This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages. We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results. Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016). To our knowledge this provides the first state-of-the-art benchmark on this data subset. Our TensorFlow2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora. 2 course properties (Asher, 1993; Webber, 1991). In contrast to nominal anaphora, abstract anaphora is difficult to resolve, given that agreement and lexical match features are not applicable. Annotation of abstract anaphora is also difficult for humans (Dipper and Zinsmeister, 2012), and thus, only few smaller-scale corpora have be"
D17-1021,D14-1056,0,0.34164,"Model for Abstract Anaphora Resolution Ana Marasovi´c, Leo Born†, Juri Opitz†, and Anette Frank† Research Training Group AIPHES Department of Computational Linguistics Heidelberg University 69120 Heidelberg, Germany {marasovic,born,opitz,frank}@cl.uni-heidelberg.de Abstract world, which is arguably the most frequently occurring type. Distinct from these are diverse types of abstract anaphora (AA) (Asher, 1993) where reference is made to propositions, facts, events or properties. An example is given in (1) below.1 While recent approaches address the resolution of selected abstract shell nouns (Kolhatkar and Hirst, 2014), we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it). Henceforth, we refer to a sentence that contains an abstract anaphor as the anaphoric sentence (AnaphS), and to a constituent that the anaphor refers to as the antecedent (Antec) (cf. (1)). Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the ana"
D17-1021,W16-6005,0,0.0290208,"2004; M¨uller, 2008) has focused on spoken language, which exhibits specific properties. Recently, event coreference has been addressed using feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016). Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase – acquire) with no special focus on (pro)nominal anaphora. Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1). Rajagopal et al. (2016) proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses. However, instead of selecting the correct antecedent clause(s) (our task) for a given event, their model is restricted to classifying the event into six abstract categories: this these changes, responses, analysis, context, finding, observation, based on its surrounding context. While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types. Mo"
D17-1021,W13-2314,0,0.234393,"relation between the anaphor in the anaphoric sentence and its antecedent. Fig. 1 displays our architecture. In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016). It produces large amounts of instances and is easily adaptable to other languages. This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages. We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results. Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016). To our knowledge this provides the first state-of-the-art benchmark on this data subset. Our TensorFlow2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora. 2 course properties (Asher, 1993; Webber, 1991). In contrast to nominal anaphora, abstract ana"
D17-1021,E17-1027,0,0.0222221,"quipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.] A major obstacle for solving this task is the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. 1 221 Example drawn from ARRAU (Uryupina et al., 2016). Proceedings of the 2017 Conference on Empirical Methods in Natural Language"
D17-1021,D13-1030,0,0.320768,"relation between the anaphor in the anaphoric sentence and its antecedent. Fig. 1 displays our architecture. In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016). It produces large amounts of instances and is easily adaptable to other languages. This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages. We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results. Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016). To our knowledge this provides the first state-of-the-art benchmark on this data subset. Our TensorFlow2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora. 2 course properties (Asher, 1993; Webber, 1991). In contrast to nominal anaphora, abstract ana"
D17-1021,L16-1631,0,0.0292572,"ora have been constructed. We evaluate our models on a subset of the ARRAU corpus (Uryupina et al., 2016) that contains abstract anaphors and the shell noun corpus used in Kolhatkar et al. (2013b).3 We are not aware of other freely available abstract anaphora datasets. Little work exists for the automatic resolution of abstract anaphora. Early work (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008) has focused on spoken language, which exhibits specific properties. Recently, event coreference has been addressed using feature-based classifiers (Jauhar et al., 2015; Lu and Ng, 2016). Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase – acquire) with no special focus on (pro)nominal anaphora. Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1). Rajagopal et al. (2016) proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses. However, instead of selecting the correct antecedent clause(s) (our task) for"
D17-1021,W16-0706,0,0.141878,"f the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphor – similar to the encoding proposed by Zhou and Xu (2015) for individuating multiply occurring predicates in SRL. With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent. Fig. 1 displays our architecture. In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016). It produces large amounts of instances and is easily adaptable to other languages. This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages. We evaluate our model on the shell noun resolution dataset of Kolhatkar et al. (2013b) and show that it outperforms their state-of-the-art results. Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (Poesio and Artstein, 2008; Uryupina et al., 2016). To our knowledge this provides th"
D17-1021,P03-1022,0,0.602913,"Missing"
D17-1021,J93-2004,0,0.0598939,"of extraction patterns, a downside of our method is that our artificially created antecedents are uniformly of type S. However, the majority of abstract anaphora antecedents found in the existing datasets are of type S. Also, our models are intended to induce semantic representations, and so we expect syntactic form to be less critical, compared to a feature-based model.8 Finally, the general extraction pattern in Fig. 2, covers a much wider range of anaphoric types. Using this method we generated a dataset of artificial anaphoric sentence–antecedent pairs from the WSJ part of the PTB Corpus (Marcus et al., 1993), automatically parsed using the Stanford Parser (Klein and Manning, 2003). 5 Experimental setup 5.1 b. Abstract anaphora resolution data set. We use the automatically constructed data from the WSJ corpus (Section 4) for training.12 Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus (Uryupina et al., 2016). We extracted all abstract anaphoric instances from the WSJ part of ARRAU that are marked with the category abstract or plan,13 and call the subcorpus ARRAU-AA. Datasets We evaluate our model on two types of anaphora: (a) shell noun anaphora and (b)"
D17-1021,P15-1150,0,0.144231,"the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. 1 221 Example drawn from ARRAU (Uryupina et al., 2016). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics learning"
D17-1021,L16-1326,0,0.318155,"ueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. 1 221 Example drawn from ARRAU (Uryupina et al., 2016). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics learning similarity between sentences. Given an anaphoric sentence (AntecS in (1)) and a candidate antecedent (any constituent in a given context, e.g. being obsoleted by microprocessor-based machines in (1)), the LSTM-Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space. These representations are combined into a joint representation used to calculate a sco"
D17-1021,W16-1617,0,0.0664538,"arge amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. 1 221 Example drawn from ARRAU (Uryupina et al., 2016). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics learning similarity between sentences. Given an anaphoric sentence (AntecS in (1)) and a candidate antecedent"
D17-1021,P15-1137,0,0.364529,"bstacle for solving this task is the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution (Clark and Manning, 2016a), textual entailment (Bowman et al., 2016), learning textual similarity (Mueller and Thyagarajan, 2016), and discourse relation sense classification (Rutherford et al., 2017). Our model is inspired by the mention-ranking model for coreference resolution (Wiseman et al., 2015; Clark and Manning, 2015, 2016a,b) and combines it with a Siamese Net (Mueller and Thyagarajan, 2016), (Neculoiu et al., 2016) for Introduction Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. 1 221 Example drawn from ARRAU (Uryupina et al., 2016). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 221–232 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computati"
D17-1021,P15-1109,0,0.0237304,"amese Net learns representations for the candidate and the anaphoric sentence in a shared space. These representations are combined into a joint representation used to calculate a score that characterizes the relation between them. The learned score is used to select the highest-scoring antecedent candidate for the given anaphoric sentence and hence its anaphor. We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphor – similar to the encoding proposed by Zhou and Xu (2015) for individuating multiply occurring predicates in SRL. With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent. Fig. 1 displays our architecture. In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns (Kolhatkar and Hirst, 2014) or anaphoric connectives (Stede and Grishina, 2016). It produces large amounts of instances and is easily adaptable to other languages. This enables us to build a robust, knowledge-lean model for abstract anaphora reso"
D17-1021,P02-1011,0,\N,Missing
D19-1056,C18-1139,0,0.0129019,"the source predicate is preserved in the target sentence. Since the role labels are projected from automatically PropBankparsed English sentences, all languages share the same label set. The underlying corpus for this dataset is composed of Machine Translation (MT) parallel corpora: Europarl (Koehn, 2005) for ENDE (about 63K sents), and UN (Ziemski et al., 2016) for EN-FR (about 40K sents). Since we only had access to the labeled sentences (target-side), we constructed our parallel training pairs EN to FR-SRL and EN to DE-SRL by finding the original source English counterparts. We use Flair (Akbik et al., 2018) to predict PropBank frames on the English source sentences and find the alignment to the labeled predicate on the target side using fast-align (Dyer et al., 2013). In addition to the parallel SRL-labeled data, we choose a subset of 100K parallel (non-labeled) sentences for each language pair from the mentioned MT datasets (Europarl and UN corpora) to 4.3 Multilingual Experiments and Results We train a single multilingual model with the concatenation of the training data for the three languages EN, DE and FR that we previously used on the monolingual experiments. We use a common vocabulary for"
D19-1056,P16-1004,0,0.0273702,"on the Encoder-Decoder approach (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). More recent architectures (Zoph and Knight, 2016; Firat et al., 2016a) show that training with multiple languages performs better than one-to-one NMT. Multilingual models have also been trained to perform Zero-shot translation (Johnson et al., 2017; Firat et al., 2016b). The Enc-Dec approach has been tested in many tasks that can be formulated as a sequence transduction problem: syntactic parsing (Vinyals et al., 2015), AMR and Semantic Parsing (Konstas et al., 2017; Dong and Lapata, 2016) and SRL (Daza and Frank, 2018). The most similar approach to ours is Zhang et al. (2017), who propose a cross-lingual Enc-Dec that produces OpenIE-annotated English given a Chinese sentence. However, their setup is easier than ours since they have a reliable labeler on the target side, facilitating the generation of more training data unlike us who are interested in labeling the resource-poor language. Extrinsic Task: Data Augmentation Finally, we augment the training sets of our two resource-poor languages DE and FR, in portions of 10K until we cover the complete generated data. We compare t"
D19-1056,P15-1039,0,0.590925,"assification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is significant prior work on SRL data augmentation (Hartmann et al., 2017), annotation projection for monolingual (F¨urstenau and Lapata, 2012; Hartmann et al., 2016), and cross-lingual SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Akbik et al., 2015, 2016). A drawback of cross-lingual projection is that even at prediction time it requires parallel sentences, a semantic role labeler on the source side, as well as syntactic information for both language sides. Thus, it is desirable to design an architecture that can make use of existing annotations in more than one lan603 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 603–615, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics guage an"
D19-1056,N13-1073,0,0.0902228,"are the same label set. The underlying corpus for this dataset is composed of Machine Translation (MT) parallel corpora: Europarl (Koehn, 2005) for ENDE (about 63K sents), and UN (Ziemski et al., 2016) for EN-FR (about 40K sents). Since we only had access to the labeled sentences (target-side), we constructed our parallel training pairs EN to FR-SRL and EN to DE-SRL by finding the original source English counterparts. We use Flair (Akbik et al., 2018) to predict PropBank frames on the English source sentences and find the alignment to the labeled predicate on the target side using fast-align (Dyer et al., 2013). In addition to the parallel SRL-labeled data, we choose a subset of 100K parallel (non-labeled) sentences for each language pair from the mentioned MT datasets (Europarl and UN corpora) to 4.3 Multilingual Experiments and Results We train a single multilingual model with the concatenation of the training data for the three languages EN, DE and FR that we previously used on the monolingual experiments. We use a common vocabulary for the three languages and keep all tokens that occur more than 5 times in the combined dataset. We train the model with batches containing instances randomly chosen"
D19-1056,D16-1102,0,0.265226,"Missing"
D19-1056,L18-1344,0,0.0967265,".33 70.52 70.39 Table 6: We retrain the monolingual systems DE, FR using the original training sets (BL: Original) shown in Table 1 and inject our generated data in different sizes. We also compare to the stronger baseline LabelProj where we add data created by label projection (Akbik et al., 2015). SRL Performance on Gold Standard. We use our human-annotated sentences to measure the automatic labeling performance of our cross-lingual SRL model which we call XL-BERT). We obtain 73.21 F1 score (73.33 precision, 73.1 recall). We also measure the performance of the ZAP label projection system of Akbik and Vollgraf (2018) on this data (we only consider arguments of the predicates that were annotated). ZAP obtains a low F1 score of 56.03 (42.65 precision, 81.7 recall). Thus, XL-BERT shows much better, and more precise results compared to this baseline and achieves overall very acceptable and stable labeling quality. This shows that the joint translation-labeling task is successful. ZAP, by contrast, shows more unstable results, which might be due to word alignment noise. Although we train on such data, our model can also loose some of this noise, given that the same model is trained to produce more than one lab"
D19-1056,N16-1101,0,0.154644,"ural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 603–615, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics guage and that learns to translate input sentences to another language while transferring semantic role annotations from the source to the target. Techniques for low-resource Neural Machine Translation (NMT) show the positive impact on target predictions by adding more than one language during training, such as Multi-source NMT (Zoph and Knight, 2016) and Multilingual NMT (Johnson et al., 2017; Firat et al., 2016a), whereas Mulcaire et al. (2018) show the advantages of training a single polyglot SRL system that improves over monolingual baselines in lowerresource settings. In this work, we propose a general Encoder-Decoder (Enc-Dec) architecture for SRL (see Figure 1). We extend our previous EncDec approach for SRL (Daza and Frank, 2018) to a cross-lingual model that translates sentences from a source language to a (lower-resource) target language, and during decoding jointly labels it with SRL annotations.1 Our contributions are as follows: Figure 2: Monolingual Enc-Dec model for SRL with copying (Da"
D19-1056,D16-1026,0,0.0513838,"Missing"
D19-1056,P98-1013,0,0.267357,"Missing"
D19-1056,J12-1005,0,0.0684762,"Missing"
D19-1056,W18-2501,0,0.0359527,"Missing"
D19-1056,C18-1233,0,0.0855369,"Missing"
D19-1056,W05-0620,0,0.663292,"Missing"
D19-1056,L18-1550,0,0.0565674,"Missing"
D19-1056,P16-1154,0,0.162698,"e apply a similar joint multilingual learning method to produce structured output sequences in the form of translations enriched with SRL annotations on the (lower-resource) target language (cf. Figure 3). We will apply this universal structure-inducing Enc-Dec model to the Semantic Role Labeling task, and show that it can be deployed in three different settings: i) monolingual: encode a sentence in a given language and learn to decode a labeled sequence by reproducing the source words and inserting the appropriate structure-indicating labels in the output (cf. Figure 2). A copying mechanism (Gu et al., 2016) allows this model to reproduce the input sentence as faithfully as possible. ii) one-to-one multilingual: train a single, joint model to generate n different structure-enriched target languages given inputs in the same language. For example: Labeled English (EN-SRL) given an EN sentence or Labeled German (DESRL) given a DE sentence. This multilingual model still relies on copying to relate each labeled output sentence to its corresponding input counterpart. However, unlike (i), it has the advantage of sharing parameters among languages. iii) cross-lingual: generate outputs in n different targ"
D19-1056,W18-3027,1,0.837263,"ource to the target. Techniques for low-resource Neural Machine Translation (NMT) show the positive impact on target predictions by adding more than one language during training, such as Multi-source NMT (Zoph and Knight, 2016) and Multilingual NMT (Johnson et al., 2017; Firat et al., 2016a), whereas Mulcaire et al. (2018) show the advantages of training a single polyglot SRL system that improves over monolingual baselines in lowerresource settings. In this work, we propose a general Encoder-Decoder (Enc-Dec) architecture for SRL (see Figure 1). We extend our previous EncDec approach for SRL (Daza and Frank, 2018) to a cross-lingual model that translates sentences from a source language to a (lower-resource) target language, and during decoding jointly labels it with SRL annotations.1 Our contributions are as follows: Figure 2: Monolingual Enc-Dec model for SRL with copying (Daza and Frank, 2018). We generalize this architecture to multilingual and cross-lingual SRL. contain word tokens of different languages if desired. This means that we could train an Enc-Dec model that learns not only to label a sentence, but to jointly translate it while applying SRL annotations directly to the target language. Mo"
D19-1056,N19-1423,0,0.0451157,"Missing"
D19-1056,W09-1201,0,0.0395648,"Missing"
D19-1056,2005.mtsummit-papers.11,0,0.0660412,"it benefits more from ELMo, achieving SOTA results for spanbased and dependency-based SRL. We use the dependency-based labeled German and French SRL corpus from Akbik et al. (2015) which was produced via annotation projection. These sentences are already pre-filtered to ensure that the predicate sense of the source predicate is preserved in the target sentence. Since the role labels are projected from automatically PropBankparsed English sentences, all languages share the same label set. The underlying corpus for this dataset is composed of Machine Translation (MT) parallel corpora: Europarl (Koehn, 2005) for ENDE (about 63K sents), and UN (Ziemski et al., 2016) for EN-FR (about 40K sents). Since we only had access to the labeled sentences (target-side), we constructed our parallel training pairs EN to FR-SRL and EN to DE-SRL by finding the original source English counterparts. We use Flair (Akbik et al., 2018) to predict PropBank frames on the English source sentences and find the alignment to the labeled predicate on the target side using fast-align (Dyer et al., 2013). In addition to the parallel SRL-labeled data, we choose a subset of 100K parallel (non-labeled) sentences for each language"
D19-1056,Q16-1015,0,0.0185422,"SRL consists of three steps: i) predicate detection, ii) argument identification and iii) role classification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is significant prior work on SRL data augmentation (Hartmann et al., 2017), annotation projection for monolingual (F¨urstenau and Lapata, 2012; Hartmann et al., 2016), and cross-lingual SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Akbik et al., 2015, 2016). A drawback of cross-lingual projection is that even at prediction time it requires parallel sentences, a semantic role labeler on the source side, as well as syntactic information for both language sides. Thus, it is desirable to design an architecture that can make use of existing annotations in more than one lan603 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 603–615, c"
D19-1056,P17-1014,0,0.0148523,"f NMT models are based on the Encoder-Decoder approach (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). More recent architectures (Zoph and Knight, 2016; Firat et al., 2016a) show that training with multiple languages performs better than one-to-one NMT. Multilingual models have also been trained to perform Zero-shot translation (Johnson et al., 2017; Firat et al., 2016b). The Enc-Dec approach has been tested in many tasks that can be formulated as a sequence transduction problem: syntactic parsing (Vinyals et al., 2015), AMR and Semantic Parsing (Konstas et al., 2017; Dong and Lapata, 2016) and SRL (Daza and Frank, 2018). The most similar approach to ours is Zhang et al. (2017), who propose a cross-lingual Enc-Dec that produces OpenIE-annotated English given a Chinese sentence. However, their setup is easier than ours since they have a reliable labeler on the target side, facilitating the generation of more training data unlike us who are interested in labeling the resource-poor language. Extrinsic Task: Data Augmentation Finally, we augment the training sets of our two resource-poor languages DE and FR, in portions of 10K until we cover the complete gene"
D19-1056,W17-0814,1,0.88539,"Missing"
D19-1056,P13-1117,0,0.0733215,"might be due to word alignment noise. Although we train on such data, our model can also loose some of this noise, given that the same model is trained to produce more than one labeled language, namely FR-SRL and DE-SRL. 4.5.2 Data Size 39K 83K 49K 59K 69K 83K 73K 105K 83K 93K 105K The label distribution is given in the Supplement, A.3. 610 6 Cross-lingual Annotation Projection. A common approach to address the lack of annotations is projecting labels from English to a lower-resource language of interest. This has shown good results in the transfer of semantic information to target languages. Kozhevnikov and Titov (2013) propose an unsupervised method to transfer SRL labels to another language by training on the source side and using shared feature representations for predicting on the target side. Pad´o and Lapata (2009) project FrameNet (Baker et al., 1998) SRL labels by searching for the best alignment in source and target constituent trees, defining label transfer as an optimization problem in a bipartite graph. van der Plas et al. (2011) use intersective word alignments between English and French with additional filtering heuristics to determine whether a PropBank label should be transferred and then use"
D19-1056,P18-2058,0,0.0671142,"r resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages. 1 Figure 1: We propose an Encoder-Decoder model that translates a sentence into a target language and applies SRL labeling to the translated words. In this example we translate from English to German and label roles for the predicate have. predicate (Marcheggiani et al., 2017; He et al., 2017), and even jointly predict predicates inside sentences (He et al., 2018; Cai et al., 2018). While these approaches alleviate the need for pipeline models, they require sufficient amounts of training data to perform adequately. To date, such models have been tested primarily for English, which offers a considerable amount of high-quality training data compared to other languages. The lack of sufficiently large SRL datasets makes it hard to straightforwardly apply the same architectures to other languages and calls for methods to augment the training data in lower-resource languages. Introduction Semantic Role Labeling (SRL) extracts semantic predicate-argument str"
D19-1056,P17-1044,0,0.177015,"our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages. 1 Figure 1: We propose an Encoder-Decoder model that translates a sentence into a target language and applies SRL labeling to the translated words. In this example we translate from English to German and label roles for the predicate have. predicate (Marcheggiani et al., 2017; He et al., 2017), and even jointly predict predicates inside sentences (He et al., 2018; Cai et al., 2018). While these approaches alleviate the need for pipeline models, they require sufficient amounts of training data to perform adequately. To date, such models have been tested primarily for English, which offers a considerable amount of high-quality training data compared to other languages. The lack of sufficiently large SRL datasets makes it hard to straightforwardly apply the same architectures to other languages and calls for methods to augment the training data in lower-resource languages. Introductio"
D19-1056,P18-1040,0,0.0266179,"ion context vector ct . In addition, a copying score ψc is calculated. The decoder learns from these scores when to generate a new token and when to copy from the encoded hidden states H. Formally we compute the scores as: Encoder-Decoder Architecture We reimplement and extend the Enc-Dec model with attention (Bahdanau et al., 2015) and copying (Gu et al., 2016) mechanisms for SRL proposed by Daza and Frank (2018). This model encodes the source sentence and decodes the input sequence of words (in the same language) interleaved with SRL labels. Data Representation. Similar to other prior work (Liu et al., 2018) and our own (Daza and Frank, 2018), we linearize the SRL structure in order to process it as a sequence of symbols suitable for the Enc-Dec architecture. We restrict ourselves to argument identification and labeling of one predicate at a time. We feed the gold predicate in training and inference, and process each sentence as many times as it has predicates. An opening bracket (# indicates the start of a labeledargument region; a closing labeled bracket, e.g. A0), indicates the ending and the tag of the labeled region (see Figure 2). Vocabulary. We define a shared vocabulary consisting of all"
D19-1056,D15-1076,0,0.0248482,"vide an in-depth quality assessment of the generated sentences, we create a small-scale gold standard consisting of 226 sentences. To select a representative sample from our newly generated labeled sentences,11 we analyze the distribution of labels in the data and apply stratified sampling to cover as many predicates as possible and as many role label variants as possible. We judge these sentences on the quality of the generated language and annotate them with PropBank roles. SRL Gold Standard. As we are lacking trained PropBank annotators, we mimic the questionbased role annotation method of He et al. (2015), who constructed QA pairs in order to label the predicate-argument structure of verbs. The annotation involves several subtasks: The first is to generate questions targeting a specific verb in a sentence and to mark as answers a subset of words from the same sentence. The next subtask is to choose the head word of each selected subset and to assign a PropBank label to this head according to a table that correlates WH-phrases with the most likely label.12 We ask two linguistically trained annotators to perform the whole task independently and compute Krippendorff’s Alpha (Krippendorff, 1980) o"
D19-1056,D15-1166,0,0.0186689,"ld have been written by a native speaker’). We obtain a high average score of 4.4 for Quality and 4.2 for Naturalness. bel scheme has arguments not seen in our training data (namely A5-A9). Presumably we see this improvement because the frequency of the major roles is more prominent. In the case of French, we don’t see significant improvement, however also here the addition of projected data shows a similar trend. 5 Related Work Encoder-Decoder Models. A wide range of NMT models are based on the Encoder-Decoder approach (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). More recent architectures (Zoph and Knight, 2016; Firat et al., 2016a) show that training with multiple languages performs better than one-to-one NMT. Multilingual models have also been trained to perform Zero-shot translation (Johnson et al., 2017; Firat et al., 2016b). The Enc-Dec approach has been tested in many tasks that can be formulated as a sequence transduction problem: syntactic parsing (Vinyals et al., 2015), AMR and Semantic Parsing (Konstas et al., 2017; Dong and Lapata, 2016) and SRL (Daza and Frank, 2018). The most similar approach to ours is Zhang et al. (2017), who propose a"
D19-1056,N18-2078,0,0.0426696,"require sufficient amounts of training data to perform adequately. To date, such models have been tested primarily for English, which offers a considerable amount of high-quality training data compared to other languages. The lack of sufficiently large SRL datasets makes it hard to straightforwardly apply the same architectures to other languages and calls for methods to augment the training data in lower-resource languages. Introduction Semantic Role Labeling (SRL) extracts semantic predicate-argument structure from sentences. This has proven to be useful in Neural Machine Translation (NMT) (Marcheggiani et al., 2018), Multidocument-summarization (Khan et al., 2015), AMR parsing (Wang et al., 2015) and Reading Comprehension (Mihaylov and Frank, 2019). SRL consists of three steps: i) predicate detection, ii) argument identification and iii) role classification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is signi"
D19-1056,K17-1041,0,0.101595,"easure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages. 1 Figure 1: We propose an Encoder-Decoder model that translates a sentence into a target language and applies SRL labeling to the translated words. In this example we translate from English to German and label roles for the predicate have. predicate (Marcheggiani et al., 2017; He et al., 2017), and even jointly predict predicates inside sentences (He et al., 2018; Cai et al., 2018). While these approaches alleviate the need for pipeline models, they require sufficient amounts of training data to perform adequately. To date, such models have been tested primarily for English, which offers a considerable amount of high-quality training data compared to other languages. The lack of sufficiently large SRL datasets makes it hard to straightforwardly apply the same architectures to other languages and calls for methods to augment the training data in lower-resource lang"
D19-1056,P17-4012,0,0.0227302,"our cross-lingual model as a labeled data generator by applying it on EN sentences from Europarl (100K) and UN corpora (100K)9 and let the model predict DE-SRL and FR-SRL as target languages. This results in unseen German and French labeled sentences. Since we cannot guarantee that the generated sentences preserve the source predicate meaning, we filter all outputs by keeping only those that come close to the original sentence meaning. We approximate this by back-translating the generated outputs (stripping the labels and keeping only the words) using the pre-trained DE-EN model from OpenNMT (Klein et al., 2017). We compare the back-translations to the sentences that we originally presented to the system and, using the previously described filtering heuristic, we keep only those whose BLEU score is equal or greater than 10. The logic behind this is that if the back-translation is close enough to the source, the generated sentence preserves a fair amount of the original sentence meaning10 . With this strategy, after applying the BLEU filter, we end up with a parallel dataset of 44K generated sentences for (EN, DE-SRL) and 32K for (EN, FRSRL). In the next section we show more detailed evaluation measur"
D19-1056,D17-1159,0,0.0268078,", Minard et al. (2016) generated a multilingual event and time parallel corpus including SRL annotations. Their corpus was manually annotated on the English side and automatically projected to Italian, Spanish, and Dutch based on the manual alignment of the annotated elements. Unfortunately, the authors do not report the performance of the SRL task, making it difficult for us to use their data for benchmarking. Semantic Role Labeling. Span-based SRL only exists on English data (Zhou and Xu, 2015; He et al., 2018; Strubell et al., 2018; Ouchi et al., 2018). Dependency-based SRL models such as (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019) are the state-of-the-art for English. For French, we compare against van der Plas et al. (2014) since we did not find more recent work for that language. Roth and Lapata (2016) show a model based on dependency path embeddings that achieved SOTA in English and German. The Polyglot SRL model of Mulcaire et al. (2018) shows some improvement over monolingual baselines when aggregating all multilingual data available from CoNLL-09, while more refined integration did not show further improvement. Their system does not perform better than our multilingual models f"
D19-1056,D19-1257,1,0.843273,"fers a considerable amount of high-quality training data compared to other languages. The lack of sufficiently large SRL datasets makes it hard to straightforwardly apply the same architectures to other languages and calls for methods to augment the training data in lower-resource languages. Introduction Semantic Role Labeling (SRL) extracts semantic predicate-argument structure from sentences. This has proven to be useful in Neural Machine Translation (NMT) (Marcheggiani et al., 2018), Multidocument-summarization (Khan et al., 2015), AMR parsing (Wang et al., 2015) and Reading Comprehension (Mihaylov and Frank, 2019). SRL consists of three steps: i) predicate detection, ii) argument identification and iii) role classification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is significant prior work on SRL data augmentation (Hartmann et al., 2017), annotation projection for monolingual (F¨urstenau and Lapata, 2012;"
D19-1056,W10-1814,0,0.392641,"Missing"
D19-1056,L16-1699,0,0.059618,"Missing"
D19-1056,J08-2005,0,0.057116,"(SRL) extracts semantic predicate-argument structure from sentences. This has proven to be useful in Neural Machine Translation (NMT) (Marcheggiani et al., 2018), Multidocument-summarization (Khan et al., 2015), AMR parsing (Wang et al., 2015) and Reading Comprehension (Mihaylov and Frank, 2019). SRL consists of three steps: i) predicate detection, ii) argument identification and iii) role classification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is significant prior work on SRL data augmentation (Hartmann et al., 2017), annotation projection for monolingual (F¨urstenau and Lapata, 2012; Hartmann et al., 2016), and cross-lingual SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Akbik et al., 2015, 2016). A drawback of cross-lingual projection is that even at prediction time it requires parallel sentences, a semantic role labeler on the source side, as well as syntactic information for both language si"
D19-1056,P16-1113,0,0.207751,"+5 in French on the full sequences. This is very important given that we have a small training set compared to classic NMT scenarios. The bottom part of Table 5 shows the scores when restricting the evaluation to sentences with score ≥ 10. We observed that this threshold8 is a good trade-off in both the amount of kept sentences (above the threshold) and average BLEU FR-Test 73 70.3 70.5 70.7 72.4 Table 4: F1 scores for role labeling on dependencybased SRL data. EN and DE Tests: CoNLL-09; FRTest: van der Plas et al. (2011). State of the art (SOTA) models∗ are: Cai et al. (2018) [GloVe] for EN, Roth and Lapata (2016) [Dependency-path Embeddings] for DE and van der Plas et al. (2014) [Non-neural] for FR, respectively. Multilingual training yields improvement on the three languages studied in this paper when compared to our monolingual baselines, particularly for German, which shows more than 6 points (F1) of improvement. In addition, we compare with the polyglot SRL system of Mulcaire et al. (2018) (which also leverages data from multiple languages during training), obtaining better results for English using GloVe. We then show that adding contextual representations to our model results in bigger improveme"
D19-1056,P18-2106,0,0.188295,"Missing"
D19-1056,D18-1548,0,0.0245499,"Missing"
D19-1056,D18-1191,0,0.0183135,"labeling for more than one predicate at a time. Separately, Minard et al. (2016) generated a multilingual event and time parallel corpus including SRL annotations. Their corpus was manually annotated on the English side and automatically projected to Italian, Spanish, and Dutch based on the manual alignment of the annotated elements. Unfortunately, the authors do not report the performance of the SRL task, making it difficult for us to use their data for benchmarking. Semantic Role Labeling. Span-based SRL only exists on English data (Zhou and Xu, 2015; He et al., 2018; Strubell et al., 2018; Ouchi et al., 2018). Dependency-based SRL models such as (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019) are the state-of-the-art for English. For French, we compare against van der Plas et al. (2014) since we did not find more recent work for that language. Roth and Lapata (2016) show a model based on dependency path embeddings that achieved SOTA in English and German. The Polyglot SRL model of Mulcaire et al. (2018) shows some improvement over monolingual baselines when aggregating all multilingual data available from CoNLL-09, while more refined integration did not show further improvement."
D19-1056,J05-1004,0,0.524988,"me architectures to other languages and calls for methods to augment the training data in lower-resource languages. Introduction Semantic Role Labeling (SRL) extracts semantic predicate-argument structure from sentences. This has proven to be useful in Neural Machine Translation (NMT) (Marcheggiani et al., 2018), Multidocument-summarization (Khan et al., 2015), AMR parsing (Wang et al., 2015) and Reading Comprehension (Mihaylov and Frank, 2019). SRL consists of three steps: i) predicate detection, ii) argument identification and iii) role classification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is significant prior work on SRL data augmentation (Hartmann et al., 2017), annotation projection for monolingual (F¨urstenau and Lapata, 2012; Hartmann et al., 2016), and cross-lingual SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Akbik et al., 2015, 2016). A drawback of cross-lingual projection is that"
D19-1056,Q15-1003,0,0.049013,"Missing"
D19-1056,P02-1040,0,0.104442,"+ GloVe] Ours-DE [Mono + GloVe] Ours-FR [Mono + GloVe] Mulcaire 2018 [Multi + GloVe] Ours [Multi + GloVe] Ours [Multi + ELMo] Ours [Multi + BERT] EN-Test 90.4 85.5 86.5 87 91.1 89.7 DE-Test 80.1 61.9 69.9 68.2 75.7 77.2 Evaluating Cross-lingual SRL. As in classical MT, evaluation is difficult, since the system outputs will approximate a target reference but will never be guaranteed to match it. Hence in this setting we do not have a proper gold standard to evaluate the labeled outputs, since we are generating labeled target sentences from scratch. Similar to MT research, we apply BLEU score (Papineni et al., 2002) to measure the closeness of the outputs against our Dev Set. The upper part of Table 5 compares the scores of two versions of the Enc-Dec model trained on the cross-lingual data from Table 2 systems, one using GloVe embeddings and the second using BERT, respectively. To better distinguish translation vs. labeling quality, we compute BLEU scores for the system outputs against labeled reference sentences in three different ways: on words only, labels only, and on full labeled sequences (both word and label outputs). We see that the prediction of words is similar in the two languages, but labeli"
D19-1056,D14-1162,0,0.0933885,"Missing"
D19-1056,N18-1202,0,0.0274023,"Missing"
D19-1056,N15-1040,0,0.0268987,"ve been tested primarily for English, which offers a considerable amount of high-quality training data compared to other languages. The lack of sufficiently large SRL datasets makes it hard to straightforwardly apply the same architectures to other languages and calls for methods to augment the training data in lower-resource languages. Introduction Semantic Role Labeling (SRL) extracts semantic predicate-argument structure from sentences. This has proven to be useful in Neural Machine Translation (NMT) (Marcheggiani et al., 2018), Multidocument-summarization (Khan et al., 2015), AMR parsing (Wang et al., 2015) and Reading Comprehension (Mihaylov and Frank, 2019). SRL consists of three steps: i) predicate detection, ii) argument identification and iii) role classification. In this work we focus on PropBank SRL (Palmer et al., 2005), which has proven its validity across languages (van der Plas et al., 2010). While former SRL systems rely on syntactic features (Punyakanok et al., 2008; T¨ackstr¨om et al., 2015), recent neural approaches learn to model both argument detection and role classification given a There is significant prior work on SRL data augmentation (Hartmann et al., 2017), annotation pro"
D19-1056,C14-1121,0,0.0508803,"Missing"
D19-1056,E17-2011,0,0.0271714,"Missing"
D19-1056,P15-1109,0,0.0845235,"architecture are to extend it to joint predicate and role labeling for more than one predicate at a time. Separately, Minard et al. (2016) generated a multilingual event and time parallel corpus including SRL annotations. Their corpus was manually annotated on the English side and automatically projected to Italian, Spanish, and Dutch based on the manual alignment of the annotated elements. Unfortunately, the authors do not report the performance of the SRL task, making it difficult for us to use their data for benchmarking. Semantic Role Labeling. Span-based SRL only exists on English data (Zhou and Xu, 2015; He et al., 2018; Strubell et al., 2018; Ouchi et al., 2018). Dependency-based SRL models such as (Marcheggiani and Titov, 2017; Cai et al., 2018; Li et al., 2019) are the state-of-the-art for English. For French, we compare against van der Plas et al. (2014) since we did not find more recent work for that language. Roth and Lapata (2016) show a model based on dependency path embeddings that achieved SOTA in English and German. The Polyglot SRL model of Mulcaire et al. (2018) shows some improvement over monolingual baselines when aggregating all multilingual data available from CoNLL-09, whil"
D19-1056,L16-1561,0,0.0506858,"s for spanbased and dependency-based SRL. We use the dependency-based labeled German and French SRL corpus from Akbik et al. (2015) which was produced via annotation projection. These sentences are already pre-filtered to ensure that the predicate sense of the source predicate is preserved in the target sentence. Since the role labels are projected from automatically PropBankparsed English sentences, all languages share the same label set. The underlying corpus for this dataset is composed of Machine Translation (MT) parallel corpora: Europarl (Koehn, 2005) for ENDE (about 63K sents), and UN (Ziemski et al., 2016) for EN-FR (about 40K sents). Since we only had access to the labeled sentences (target-side), we constructed our parallel training pairs EN to FR-SRL and EN to DE-SRL by finding the original source English counterparts. We use Flair (Akbik et al., 2018) to predict PropBank frames on the English source sentences and find the alignment to the labeled predicate on the target side using fast-align (Dyer et al., 2013). In addition to the parallel SRL-labeled data, we choose a subset of 100K parallel (non-labeled) sentences for each language pair from the mentioned MT datasets (Europarl and UN corp"
D19-1056,N16-1004,0,0.099543,"n603 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 603–615, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics guage and that learns to translate input sentences to another language while transferring semantic role annotations from the source to the target. Techniques for low-resource Neural Machine Translation (NMT) show the positive impact on target predictions by adding more than one language during training, such as Multi-source NMT (Zoph and Knight, 2016) and Multilingual NMT (Johnson et al., 2017; Firat et al., 2016a), whereas Mulcaire et al. (2018) show the advantages of training a single polyglot SRL system that improves over monolingual baselines in lowerresource settings. In this work, we propose a general Encoder-Decoder (Enc-Dec) architecture for SRL (see Figure 1). We extend our previous EncDec approach for SRL (Daza and Frank, 2018) to a cross-lingual model that translates sentences from a source language to a (lower-resource) target language, and during decoding jointly labels it with SRL annotations.1 Our contributions are as follow"
D19-1056,P11-2052,0,\N,Missing
D19-1056,C98-1013,0,\N,Missing
D19-1257,D18-1454,0,0.100557,"ourse-semantic annotations: DiscRel (Dicourse Relations) (NE - Non-Explicit), SRL (Semantic Role Labeling), Coref (Co-reference resolution). The distinct horizontal lines show the interaction between the tokens: Coref - full context, SRL - single sentence, Non-Explicit DR - two neighbouring sentences. ing has focused on integrating external knowledge (linguistic and/or knowledge-based) into recurrent neural network models using Graph Neural Networks (Song et al., 2018), Graph Convolutional Networks (Sun et al., 2018; De Cao et al., 2019), attention (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018) or pointers to coreferent mentions (Dhingra et al., 2017). In contrast, in this work we examine the impact of discourse-semantic annotations (Figure 1) in a self-attention architecture. We build on the QANet (Yu et al., 2018) model by modifying the encoder of its self-attention modeling layer. In particular, we specialize self-attention heads to focus on specific discourse-semantic annotations, such as, e.g., an ARG 1 relation in SRL, a CAUSA TION relation holding between clauses in shallow discourse parsing, or coreference relations holding between entity mentions. Our contributions are the"
D19-1257,P16-1223,0,0.0232523,"Missing"
D19-1257,P17-1171,0,0.0142996,"atural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answer questions about them. In terms of model architecture, previous work in reading comprehension and question answer2541 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2541–2552, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics DR (NE) SRL Coref Tokens S-ARG"
D19-1257,P19-1285,0,0.0437479,"Missing"
D19-1257,P17-2057,0,0.0424797,"cement . Figure 2: Example on different discourse-semantic annotations: DiscRel (Dicourse Relations) (NE - Non-Explicit), SRL (Semantic Role Labeling), Coref (Co-reference resolution). The distinct horizontal lines show the interaction between the tokens: Coref - full context, SRL - single sentence, Non-Explicit DR - two neighbouring sentences. ing has focused on integrating external knowledge (linguistic and/or knowledge-based) into recurrent neural network models using Graph Neural Networks (Song et al., 2018), Graph Convolutional Networks (Sun et al., 2018; De Cao et al., 2019), attention (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018) or pointers to coreferent mentions (Dhingra et al., 2017). In contrast, in this work we examine the impact of discourse-semantic annotations (Figure 1) in a self-attention architecture. We build on the QANet (Yu et al., 2018) model by modifying the encoder of its self-attention modeling layer. In particular, we specialize self-attention heads to focus on specific discourse-semantic annotations, such as, e.g., an ARG 1 relation in SRL, a CAUSA TION relation holding between clauses in shallow discourse parsing, or coreference relations holding betw"
D19-1257,N19-1240,0,0.0306277,"Missing"
D19-1257,N19-1423,0,0.179053,"rfleet]A0 [assigned]V [Jellico]A1 [as Picard‘s replacement]A2. Figure 1: Motivational example: context and questions with required discourse and semantic annotations. we can use discourse and semantic information to extend self-attention-based neural models for a higher-level task such as Reading Comprehension. Introduction Transformer-based self-attention models (Vaswani et al., 2017) have been shown to work well on many natural language tasks that require largescale training data, such as Machine Translation (Vaswani et al., 2017; Dai et al., 2019), Language Modeling (Radford et al., 2018a; Devlin et al., 2019; Dai et al., 2019; Radford et al., 2019) or Reading Comprehension (Yu et al., 2018), and can even be trained to perform surprisingly well in several multi-modal tasks (Kaiser et al., 2017b). Recent work (Strubell et al., 2018) has shown that for downstream semantic tasks with much smaller datasets, such as Semantic Role Labeling (SRL) (Palmer et al., 2005), self-attention models greatly benefit from the use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural"
D19-1257,W18-2501,0,0.127615,"h masks Input Embedding Inputs h4 S0 S1 S2 &'() Annotation embedding Previous (ex. DiscRel_Cause_Arg1) layer input &'() Previous layer input h5 DiscRel (Non-Explicit) – succ sents S0 S1 Concat *+ h3 h2 S0 S1 Mask Add & Norm Add & Norm h1 S2 SoftMax N h0 h7 h6 S2 DiscRel (Exp) Coref – full text Figure 3: A) Base Multi-Head Self-Attention Encoder Block, B) Discourse-Aware Semantic Self-Attention (DASSA) Encoder Block , C) Single Attention Head with disource/semantic Information, D) Example of attention scope masks for different attention heads and different information. implemented in AllenNLP (Gardner et al., 2018). The system splits paragraphs into sentences and tokens, performs POS (part of speech tagging) and for each verb token V it predicts semantic tags such as ARG0, ARG1 (Argument Role 0, 1 of verb V), etc. When several argument-taking predicates are realized in a sentence, we obtain more than single semantic argument structure, and each token in the sentence can be involved in the argument structure of more than one verb. We refer to these annotations as different semantic views (Khashabi et al., 2018a), e.g., ‘semantic view for verb 1‘. Different self-attention heads will be able to attend to i"
D19-1257,P17-1044,0,0.10528,"Missing"
D19-1257,D18-1232,0,0.0495782,"Missing"
D19-1257,P17-1147,0,0.0154912,"atly benefit from the use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answer questions about them. In terms of model architecture, previous work in reading comprehension and question answer2541 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and"
D19-1257,P16-1086,0,0.0192022,"eline model (i) by BiDAF and (ii) configurations of QANet with linguistic information. Question types with * have less than 100 instances in the Test set. has the highest Rouge-L. In contrast, using Oracle (Ours), described in Section 4, we report a +11 Rouge-L score improvement (Table 1: This work). The Oracle performance in this setting is important since the produced annotations are used for training of the span-prediction systems, and is considered upper-bound.3 Seq2Seq (no context) is an encoder-decoder RNN model trained only on the question. ASR is a version of the Attention Sum Reader (Kadlec et al., 2016) implemented as a pointer-generator that reads the question and points to words in the context that are contained in the answer. BiDAF is Bi-Directional Attention Flow (Seo et al., 2017) trained either with the Oracle (original) or Oracle (ours). The models from Previous Work are described in Section 5. In the last section of Table 1 we present the results of our experiments (This work). Here, BiDAF and QANet are implementations available in the AllenNLP framework (Gardner et al., 2018). In the last two rows we give the results of QANet extended with the proposed Discourse-Aware Se3 The previo"
D19-1257,J05-1004,0,0.217346,"models (Vaswani et al., 2017) have been shown to work well on many natural language tasks that require largescale training data, such as Machine Translation (Vaswani et al., 2017; Dai et al., 2019), Language Modeling (Radford et al., 2018a; Devlin et al., 2019; Dai et al., 2019; Radford et al., 2019) or Reading Comprehension (Yu et al., 2018), and can even be trained to perform surprisingly well in several multi-modal tasks (Kaiser et al., 2017b). Recent work (Strubell et al., 2018) has shown that for downstream semantic tasks with much smaller datasets, such as Semantic Role Labeling (SRL) (Palmer et al., 2005), self-attention models greatly benefit from the use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016)"
D19-1257,P17-2049,0,0.0178463,"aylov and Frank, 2018; Bauer et al., 2018; Wang et al., 2018b) and QA (Das et al., 2017; Sun et al., 2018; Tandon et al., 2018). These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b). Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candidate mentions (De Cao et al., 2019) and OpenIE triples (Khot et al., 2017) into RNN-based encoders. Recently, several pretrained language models (Peters et al., 2018; Radford et al., 2018b; Devlin et al., 2019) have been shown to incrementally boost the performance of well-performing models for several short paragraph reading comprehension tasks (Peters et al., 2018; Devlin et al., 2019) and question answering (Sun et al., 2019), as well as many tasks from the GLUE benchmark (Wang et al., 2018a). Approaches based on BERT (Devlin et al., 2019) usually perform best when the weights are fine-tuned for the specific training task. Earlier, many papers that do not use sel"
D19-1257,N18-1202,0,0.233732,"un et al., 2018; Tandon et al., 2018). These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b). Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candidate mentions (De Cao et al., 2019) and OpenIE triples (Khot et al., 2017) into RNN-based encoders. Recently, several pretrained language models (Peters et al., 2018; Radford et al., 2018b; Devlin et al., 2019) have been shown to incrementally boost the performance of well-performing models for several short paragraph reading comprehension tasks (Peters et al., 2018; Devlin et al., 2019) and question answering (Sun et al., 2019), as well as many tasks from the GLUE benchmark (Wang et al., 2018a). Approaches based on BERT (Devlin et al., 2019) usually perform best when the weights are fine-tuned for the specific training task. Earlier, many papers that do not use self-attention models or even neural methods have also tried to use semantic parse labels (Yih"
D19-1257,Q18-1023,0,0.145924,"use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answer questions about them. In terms of model architecture, previous work in reading comprehension and question answer2541 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International"
D19-1257,prasad-etal-2008-penn,0,0.223658,"medium size model from the neuralcoref spaCy extension available at https://github.com/huggingface/neuralcoref. For each token we give as information the label of the corresponding coreference cluster (see Figure 2) that it belongs to. Therefore, tokens from the same coreference cluster get the same label as input. Discourse Relations In narrative texts, events are connected by discourse relations such as causation, temporal succession, etc. (Mani, 2012). In this work we adopt the 15 fine-grained discourse relation sense types from the annotation scheme of the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008). For producing discourse relation annotations we use the discourse relation sense disambiguation system from Mihaylov and Frank (2016) which is trained on the data provided by the CoNLL Shared Task on Shallow Discourse Parsing (Xue et al., 2016). In this annotation scheme discourse relations are divided into two main types: Explicit and Non-Explicit. Explicit relations are usually connected with an explicit discourse connective, such as because, but, if. Non-Explicit1 relations are not explicitly marked with a discourse connective and the arguments are usually contained in two consecutive sen"
D19-1257,N19-1238,0,0.0478097,"Missing"
D19-1257,W04-1013,0,0.0181356,"summary as document or context. The dataset contains 1572 documents in total, devided into Train (1102 docs, 32.7k questions), Dev (115 documents, 3.5k questions) and Test (355 documents, 10.5k questions) sets. Generative QA as Span Prediction An interesting aspect of the NarrativeQA dataset is that in contrast to most other RC datasets, the two answers provided for each question are written by human annotators. Therefore, answers typically differ in form from the context passages that license them. To map the human-generated answers to answer candidate spans from the context, we use Rouge-L (Lin, 2004) to calculate a similarity score between token n-grams from the provided answer and token n-grams from candidate answers selected from the context (we select candidate spans of the same length as the given answer). If two answer candidates have the same Rouge-L score, we calculate the score between the candidates’ surrounding tokens (window size: 15 tokens to the left and right) and the question tokens, and choose the candidate with the higher score. We retrieve the best candidate answer span for each answer and use the candidate with the higher Rouge-L score as supervision for training. We re"
D19-1257,K16-2014,1,0.859604,"give as information the label of the corresponding coreference cluster (see Figure 2) that it belongs to. Therefore, tokens from the same coreference cluster get the same label as input. Discourse Relations In narrative texts, events are connected by discourse relations such as causation, temporal succession, etc. (Mani, 2012). In this work we adopt the 15 fine-grained discourse relation sense types from the annotation scheme of the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008). For producing discourse relation annotations we use the discourse relation sense disambiguation system from Mihaylov and Frank (2016) which is trained on the data provided by the CoNLL Shared Task on Shallow Discourse Parsing (Xue et al., 2016). In this annotation scheme discourse relations are divided into two main types: Explicit and Non-Explicit. Explicit relations are usually connected with an explicit discourse connective, such as because, but, if. Non-Explicit1 relations are not explicitly marked with a discourse connective and the arguments are usually contained in two consecutive sentences (see Figure 2). To extract explicit discourse relations we take into account only arguments that are in the same sentence. We co"
D19-1257,P18-1076,1,0.919711,"Example on different discourse-semantic annotations: DiscRel (Dicourse Relations) (NE - Non-Explicit), SRL (Semantic Role Labeling), Coref (Co-reference resolution). The distinct horizontal lines show the interaction between the tokens: Coref - full context, SRL - single sentence, Non-Explicit DR - two neighbouring sentences. ing has focused on integrating external knowledge (linguistic and/or knowledge-based) into recurrent neural network models using Graph Neural Networks (Song et al., 2018), Graph Convolutional Networks (Sun et al., 2018; De Cao et al., 2019), attention (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018) or pointers to coreferent mentions (Dhingra et al., 2017). In contrast, in this work we examine the impact of discourse-semantic annotations (Figure 1) in a self-attention architecture. We build on the QANet (Yu et al., 2018) model by modifying the encoder of its self-attention modeling layer. In particular, we specialize self-attention heads to focus on specific discourse-semantic annotations, such as, e.g., an ARG 1 relation in SRL, a CAUSA TION relation holding between clauses in shallow discourse parsing, or coreference relations holding between entity mentions. Our c"
D19-1257,D16-1241,0,0.0159183,"Palmer et al., 2005), self-attention models greatly benefit from the use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answer questions about them. In terms of model architecture, previous work in reading comprehension and question answer2541 Proceedings of the 2019 Conference on Empiri"
D19-1257,L18-1564,0,0.0133905,"candidate with the higher score. We retrieve the best candidate answer span for each answer and use the candidate with the higher Rouge-L score as supervision for training. We refer to this method for answer retrieval as Oracle (Ours). 5 Related Work Reading Comprehension with Knowledge Recent work has proposed different approaches for integrating external knowledge into neural models for the high-level downstream tasks reading comprehension (RC) and question answering (QA). One line of work leverages external knowledge from knowledge bases for RC (Xu et al., 2016; Weissenborn et al., 2017a; Ostermann et al., 2018; Mihaylov and Frank, 2018; Bauer et al., 2018; Wang et al., 2018b) and QA (Das et al., 2017; Sun et al., 2018; Tandon et al., 2018). These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b). Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candidate mentions (De Cao et al., 2019) and OpenIE trip"
D19-1257,D16-1264,0,0.0256209,"self-attention models greatly benefit from the use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answer questions about them. In terms of model architecture, previous work in reading comprehension and question answer2541 Proceedings of the 2019 Conference on Empirical Methods in Natural La"
D19-1257,D13-1020,0,0.0168859,"ser et al., 2017b). Recent work (Strubell et al., 2018) has shown that for downstream semantic tasks with much smaller datasets, such as Semantic Role Labeling (SRL) (Palmer et al., 2005), self-attention models greatly benefit from the use of linguistic information such as dependency parsing annotations. Motivated by this work, we examine to what extent Reading Comprehension is a task that requires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answe"
D19-1257,N18-2074,0,0.0408517,"lso tried to use semantic parse labels (Yih et al., 2016), or annotations from upstream tasks (Khashabi et al., 2018b). Self-Attention Models in NLP Vanilla selfattention models (Vaswani et al., 2017) use positional encoding, sometimes combined with local convolutions (Yu et al., 2018) to model the token order in text. Although they are scalable due to their recurrence-free nature, most self-attention models do not well work when trained with fixedlength context, due to the fact that they often learn global token positions observed during training, rather than relative. To address this issue, Shaw et al. (2018) proposes relative position encoding to model the distance between tokens in the context. Dai et al. (2019) address the problem of moving beyond fixed-length context by adding recurrence to the self-attention model. Dai et al. (2019) argue that the fixed-length segments used for language modeling hurt the performance due to the fact that they do not respect sentence or any other semantic boundaries. In this work we also support the claim that the lack of semantic, and also discourse boundaries is an issue, and therefore we aim to introduce structured linguistic information into the self-attent"
D19-1257,D18-1548,0,0.0916734,"Missing"
D19-1257,D18-1455,0,0.0550266,". Starleet assigned Jellico O C6 as Picard‘s replacement . Figure 2: Example on different discourse-semantic annotations: DiscRel (Dicourse Relations) (NE - Non-Explicit), SRL (Semantic Role Labeling), Coref (Co-reference resolution). The distinct horizontal lines show the interaction between the tokens: Coref - full context, SRL - single sentence, Non-Explicit DR - two neighbouring sentences. ing has focused on integrating external knowledge (linguistic and/or knowledge-based) into recurrent neural network models using Graph Neural Networks (Song et al., 2018), Graph Convolutional Networks (Sun et al., 2018; De Cao et al., 2019), attention (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018) or pointers to coreferent mentions (Dhingra et al., 2017). In contrast, in this work we examine the impact of discourse-semantic annotations (Figure 1) in a self-attention architecture. We build on the QANet (Yu et al., 2018) model by modifying the encoder of its self-attention modeling layer. In particular, we specialize self-attention heads to focus on specific discourse-semantic annotations, such as, e.g., an ARG 1 relation in SRL, a CAUSA TION relation holding between clauses in shallow disco"
D19-1257,N19-1270,0,0.0215173,"Missing"
D19-1257,D18-1006,0,0.0317467,"ge-L score as supervision for training. We refer to this method for answer retrieval as Oracle (Ours). 5 Related Work Reading Comprehension with Knowledge Recent work has proposed different approaches for integrating external knowledge into neural models for the high-level downstream tasks reading comprehension (RC) and question answering (QA). One line of work leverages external knowledge from knowledge bases for RC (Xu et al., 2016; Weissenborn et al., 2017a; Ostermann et al., 2018; Mihaylov and Frank, 2018; Bauer et al., 2018; Wang et al., 2018b) and QA (Das et al., 2017; Sun et al., 2018; Tandon et al., 2018). These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b). Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candidate mentions (De Cao et al., 2019) and OpenIE triples (Khot et al., 2017) into RNN-based encoders. Recently, several pretrained language models (Peters et al., 2018; Radford et al.,"
D19-1257,D18-1238,0,0.0244209,"Missing"
D19-1257,H89-1033,0,0.766057,"Missing"
D19-1257,S18-1120,0,0.0169473,"r span for each answer and use the candidate with the higher Rouge-L score as supervision for training. We refer to this method for answer retrieval as Oracle (Ours). 5 Related Work Reading Comprehension with Knowledge Recent work has proposed different approaches for integrating external knowledge into neural models for the high-level downstream tasks reading comprehension (RC) and question answering (QA). One line of work leverages external knowledge from knowledge bases for RC (Xu et al., 2016; Weissenborn et al., 2017a; Ostermann et al., 2018; Mihaylov and Frank, 2018; Bauer et al., 2018; Wang et al., 2018b) and QA (Das et al., 2017; Sun et al., 2018; Tandon et al., 2018). These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b). Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candidate mentions (De Cao et al., 2019) and OpenIE triples (Khot et al., 2017) into RNN-based encoders. Recently, severa"
D19-1257,K17-1028,0,0.0762792,"equires a model to answer natural language questions, given a text as context: a paragraph or even full documents. Many datasets have been proposed for the task, starting with a small multi-choice dataset (Richardson et al., 2013), large-scale automatically created cloze-style datasets (Hermann et al., 2015; Hill et al., 2016) and big manually annotated datasets such as Onishi et al. (2016); Rajpurkar et al. (2016); Joshi et al. (2017); Kocisky et al. (2018). Previous research has shown that some datasets are not challenging enough, as simple heuristics work well with them (Chen et al., 2016; Weissenborn et al., 2017b; Chen et al., 2017). In this work we focus on the recent NarrativeQA (Kocisky et al., 2018) dataset that was designed not to be easy to answer and that requires a model to read narrative stories and answer questions about them. In terms of model architecture, previous work in reading comprehension and question answer2541 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2541–2552, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics DR (NE) S"
D19-1257,P16-1220,0,0.0295626,"ght) and the question tokens, and choose the candidate with the higher score. We retrieve the best candidate answer span for each answer and use the candidate with the higher Rouge-L score as supervision for training. We refer to this method for answer retrieval as Oracle (Ours). 5 Related Work Reading Comprehension with Knowledge Recent work has proposed different approaches for integrating external knowledge into neural models for the high-level downstream tasks reading comprehension (RC) and question answering (QA). One line of work leverages external knowledge from knowledge bases for RC (Xu et al., 2016; Weissenborn et al., 2017a; Ostermann et al., 2018; Mihaylov and Frank, 2018; Bauer et al., 2018; Wang et al., 2018b) and QA (Das et al., 2017; Sun et al., 2018; Tandon et al., 2018). These approaches make use of implicit (Weissenborn et al., 2017a) or explicit (Mihaylov and Frank, 2018; Sun et al., 2018; Bauer et al., 2018) attention-based knowledge aggregation or leverage features from knowledge base relations (Wang et al., 2018b). Another line of work builds on linguistic knowledge from downstream tasks, such as coreference resolution (Dhingra et al., 2017) or notions of co-occurring candi"
D19-1257,K16-2001,0,0.120327,"se events are related in discourse (e.g., by causation, contrast, or temporal sequence) (Mani, 2012). Our aim is to extract structured knowledge about these phenomena from long texts and to integrate this information in a neural self-attention model, in order to examine to what extent such knowledge can enhance the efficiency of a strong reading comprehension model applied to NarrativeQA. Specifically, we enhance self-attention with knowledge about entity coreference (Coref), their participation in events (SRL) and the relation between events in narrative discourse (Shallow Discourse Parsing (Xue et al., 2016), DR). All these linguistic information types are relational in nature. For integrating relational knowledge into the self-attention mechanism, we follow a two-step approach: i) we extract such relations from a multi-sentence paragraph and project them down to the token level, specifically to the tokens of the text fragments that they involve; ii) we design a neural self-attention model that uses the interaction information between these tokens in a multi-head self-attention module. To be able to map the extracted linguistic knowledge to paragraph tokens, we need annotations that are easy to m"
D19-1257,P16-2033,0,0.0322,"018; Radford et al., 2018b; Devlin et al., 2019) have been shown to incrementally boost the performance of well-performing models for several short paragraph reading comprehension tasks (Peters et al., 2018; Devlin et al., 2019) and question answering (Sun et al., 2019), as well as many tasks from the GLUE benchmark (Wang et al., 2018a). Approaches based on BERT (Devlin et al., 2019) usually perform best when the weights are fine-tuned for the specific training task. Earlier, many papers that do not use self-attention models or even neural methods have also tried to use semantic parse labels (Yih et al., 2016), or annotations from upstream tasks (Khashabi et al., 2018b). Self-Attention Models in NLP Vanilla selfattention models (Vaswani et al., 2017) use positional encoding, sometimes combined with local convolutions (Yu et al., 2018) to model the token order in text. Although they are scalable due to their recurrence-free nature, most self-attention models do not well work when trained with fixedlength context, due to the fact that they often learn global token positions observed during training, rather than relative. To address this issue, Shaw et al. (2018) proposes relative position encoding to"
E95-1002,P92-1005,0,\N,Missing
hartung-frank-2010-semi,D09-1138,0,\N,Missing
hartung-frank-2010-semi,W04-3221,0,\N,Missing
hartung-frank-2010-semi,amoia-gardent-2008-test,0,\N,Missing
I05-6001,C02-1013,0,0.0245299,"truction that converts dependency structures to RMRSs as they are output by HPSG grammars. By applying this method to the TIGER Dependency Bank, we construct an RMRS Bank that allows cross-framework parser evaluation for German. Our method for RMRS construction can be transposed to dependency banks for other languages, such as the PARC 700 Dependency Bank for English (King et al., 2003). The choice of RMRS also ensures that the semantic bank can be used for comparative evaluation of HPSG grammars with low-level parsers that output partial semantics in terms of RMRS, such as the RASP parser of Carroll and Briscoe (2002). While the formalism of (R)MRS has its origins in HPSG, we have shown that RMRS semantics construction can be carried over to dependency-based frameworks like LFG. In future research, we will investigate how the semantic algebra of Copestake et al. (2001) Error analysis. A large portion of the errors did not concern the RMRS as such, but 10 This evaluation did not perform correction of partof-speech tags (cf. below, error analysis). 9 References compares to Glue Semantics (Dalrymple, 1999). Our construction rules may in fact be modified and extended to yield semantics construction along the l"
I05-6001,P01-1019,0,0.183393,"r to bridge this gap, we construct an RMRS “treebank” from a subset of the TIGER Dependency Bank (Forst et al., 2004), which can serve as a gold standard for HPSG parsing for evaluation, and for training of stochastic HPSG grammar models. In contrast to treebanks constructed from analyses of hand-crafted grammars, our treebank conWe present a treebank conversion method by which we construct an RMRS bank for HPSG parser evaluation from the TIGER Dependency Bank. Our method effectively performs automatic RMRS semantics construction from functional dependencies, following the semantic algebra of Copestake et al. (2001). We present the semantics construction mechanism, and focus on some special phenomena. Automatic conversion is followed by manual validation. First evaluation results yield high precision of the automatic semantics construction rules. 1 Anette Frank Language Technology Lab DFKI GmbH 66123 Saarbr¨ ucken, Germany frank@dfki.de Introduction Treebanks are under development for many languages. They are successfully exploited for the induction of treebank grammars, training of stochastic parsers, and for evaluating and benchmarking competitive parsing and grammar models. While parser evaluation aga"
I05-6001,W04-1905,0,0.0879287,"parsing is difficult to match against such structures. HPSG analyses do not come with an explicit representation of functional structure, but directly encode semantic structures, in terms of (Robust) Minimal Recursion Semantics (henceforth (R)MRS.1 This leaves a gap to be bridged in terms of the encoding of arguments vs. adjuncts, the representation of special constructions like relative clauses, and not least, the representation of quantifiers and their (underspecified) scoping relations. In order to bridge this gap, we construct an RMRS “treebank” from a subset of the TIGER Dependency Bank (Forst et al., 2004), which can serve as a gold standard for HPSG parsing for evaluation, and for training of stochastic HPSG grammar models. In contrast to treebanks constructed from analyses of hand-crafted grammars, our treebank conWe present a treebank conversion method by which we construct an RMRS bank for HPSG parser evaluation from the TIGER Dependency Bank. Our method effectively performs automatic RMRS semantics construction from functional dependencies, following the semantic algebra of Copestake et al. (2001). We present the semantics construction mechanism, and focus on some special phenomena. Automa"
I05-6001,W03-2401,0,\N,Missing
I05-6001,W03-2404,0,\N,Missing
I08-1051,burchardt-etal-2006-salsa,1,0.931521,"ularity offered by a given annotation layer may diverge considerably from the granularity that is needed for the integration of corpus-derived data in large symbolic processing architectures or general lexical resources. This problem is multiplied when more than one layer of annotation is considered, for example in the characterisation of interface phenomena. While it may be possible to obtain coarser-grained representations procedurally by collapsing categories, such procedures are not flexibly configurable. Figure 1 illustrates these difficulties with a sentence from the SALSA/TIGER corpus (Burchardt et al., 2006), a manually annotated German newspaper corpus which contains role-semantic analyses in the FrameNet paradigm (Fillmore et al., 2003) on top of syntactic structure (Brants et al., 2002).1 The se1 While FrameNet was originally developed for English, the majority of frames has been found to generalise well to other 389 which the official Croatia but in significant international-law difficulties bring would Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (‘which would bring official Croatia into significant difficulties with international law’) mantic structure"
I08-1051,E03-1068,0,0.0163518,"namely 4 missing metaphor flags and 4 omitted underspecification links. On the semantic level, we extracted annotation instances (in context) for metaphorical vs. nonmetaphorical readings, or frames that are involved in underspecification in certain sentences, but not in others. While the result sets thus obtained still require manual inspection, they clearly illustrate how the detection of inconsistencies can be enhanced by a declarative formalisation of the annotation scheme. Another strategy could be to concentrate on frames or lemmas exhibiting proportionally high variation in annotation (Dickinson and Meurers, 2003). 6 Conclusion In this paper, we have constructed a Description Logics-based lexicon model directly from multi-layer linguistic corpus annotations. We have shown how such a model allows for explicit data modelling, and for flexible and fine-grained definition of various degrees of abstractions over corpus annotations. Furthermore, we have demonstrated that a powerful logical formalisation which integrates an underlying annotation scheme can be used to directly control consistency of the annotations using general KR techniques. It can also overcome limitations of current XML-based search tools"
I08-1051,J02-3001,0,0.00609142,"roblematic are intersecting hierarchies, i.e., tree-shaped analyses on multiple linguistic levels. Introduction Over the years, much effort has gone into the creation of large corpora with multiple layers of linguistic annotation, such as morphology, syntax, semantics, and discourse structure. Such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis. Currently, the most common use of such corpora is the acquisition of statistical models that make use of the “more shallow” levels to predict the “deeper” levels of annotation (Gildea and Jurafsky, 2002; Miltsakaki et al., 2005). While these models fill an important need for practical applications, they fall short of the general task of lexicon modelling, i.e., creating an abstracted and compact representation of the corpus information that lends itself to ’linguistically informed’ usages such as human interpretation or integration with other knowledge sources (e.g., deep grammar resources or ontologies). In practice, this task faces three major problems: ∗ At the time of writing, Sebastian Padó and Dennis Spohr were affiliated with Saarland University, and Anette Frank with DFKI Saarbrücken"
I08-1051,U04-1019,0,0.0261719,"heme. We present a general approach to formally modelling corpora with multi-layered annotation, thereby inducing a lexicon model in a typed logical representation language, OWL DL. This model can be interpreted as a graph structure that offers flexible querying functionality beyond current XML-based query languages and powerful methods for consistency control. We illustrate our approach by applying it to the syntactically and semantically annotated SALSA/TIGER corpus. 1 4 Dept. of Linguistics Stanford University Stanford, CA Querying multiple layers of linguistic annotation. A recent survey (Lai and Bird, 2004) found that currently available XML-based corpus query tools support queries operating on multiple linguistic levels only in very restricted ways. Particularly problematic are intersecting hierarchies, i.e., tree-shaped analyses on multiple linguistic levels. Introduction Over the years, much effort has gone into the creation of large corpora with multiple layers of linguistic annotation, such as morphology, syntax, semantics, and discourse structure. Such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis. Currently, the m"
I08-1051,W06-1007,0,0.0219647,"mantics of the model makes it possible to use general and efficient knowledge representation techniques for consistency control. Finally, we can extract specific subsets from a corpus by defining task-specific views on the graph. After a short discussion of related approaches in languages (Burchardt et al., 2006; Boas, 2005). Section 2, Section 3 provides details on our methodology. Sections 4 and 5 demonstrate the benefits of our strategy on a model of the SALSA/TIGER data. Section 6 concludes. 2 Related Work One recent approach to lexical resource modelling is the Lexical Systems framework (Polguère, 2006), which aims at providing a highly general representation for arbitrary kinds of lexica. While this is desirable from a representational point of view, the resulting models are arguably too generic to support strong consistency checks on the encoded data. A further proposal is the currently evolving Lexical Markup Framework (LMF; Francopoulo et al. (2006)), an ISO standard for lexical resource modelling, and an LMF version of FrameNet exists. However, we believe that our usage of a typed formalism takes advantage of a strong logical foundation and the notions of inheritance and entailment (cf."
I08-1051,W06-0609,0,\N,Missing
I08-1064,J03-4003,0,0.0344317,"Missing"
I08-1064,J94-4004,0,0.0605483,"ons as well as in the word alignment. Moreover, successful projection relies on the direct correspondence assumption (DCA, Hwa et al. (2002)) which demands that the annotations in the source text be homomorphous with those in its (literal) translation. The DCA has been found to hold, to a substantial degree, for the above mentioned domains. The results we report here show that it can also be confirmed for temporal annotations in English and German. Yet, we cannot preclude divergence from translational correspondence; on the contrary, it occurs routinely and to a certain extent systematically (Dorr, 1994). We employ two different techniques to filter noise. Firstly, the projection process is equipped with (partly language-specific) knowledge for a principled account of typical alignment errors and cross-language discrepancies in the realisation of events and timexes (section 3.2). Secondly, we apply aggressive data engineering techniques to the noisy projections and use them to train statistical classifiers which generalise beyond the noise (section 5). The paper is structured as follows. Section 2 gives an overview of the TimeML specification language and compatible annotation tools. Section"
I08-1064,P02-1050,0,0.0220892,"approaches to multilingual annotation have proven adequate in various domains, including part-of-speech tagging (Yarowsky and Ngai, 2001), NP-bracketing (Yarowsky et al., 2001), dependency analysis (Hwa et al., 2005), and role semantic analysis (Pad´o and Lapata, 2006). To our knowledge, the present proposal is the first to apply projection algorithms to temporal annotations. 489 Cross-lingually projected information is typically noisy, due to errors in the source annotations as well as in the word alignment. Moreover, successful projection relies on the direct correspondence assumption (DCA, Hwa et al. (2002)) which demands that the annotations in the source text be homomorphous with those in its (literal) translation. The DCA has been found to hold, to a substantial degree, for the above mentioned domains. The results we report here show that it can also be confirmed for temporal annotations in English and German. Yet, we cannot preclude divergence from translational correspondence; on the contrary, it occurs routinely and to a certain extent systematically (Dorr, 1994). We employ two different techniques to filter noise. Firstly, the projection process is equipped with (partly language-specific)"
I08-1064,N03-1017,0,0.00711347,"Missing"
I08-1064,2005.mtsummit-papers.11,0,0.186085,"Missing"
I08-1064,P00-1010,0,0.0478194,"multi-word. a. of the parallel corpus. However, given that the projected annotations are to provide enough data for training a target language labeller (section 5), manual annotation is not an option. Instead, we use the TARSQI tools for automatic TimeML annotation of English text (Verhagen et al., 2005). They have been modelled and evaluated on the basis of the TimeBank (Pustejovsky et al., 2003b), yet for the most part rely on hand-crafted rules. To obtain a full temporal annotation, the modules are combined in a cascade. We are using the components for timex recognition and normalisation (Mani and Wilson, 2000), event extraction (Saur´ı et al., 2005a), and identification of modal contexts (Saur´ı et al., 2006).3 3 Informed Projection 3.1 . . . [ ws ]e . . . b. The Core Algorithm Recall that TimeML represents temporal entities with EVENT and TIMEX3 tags which are anchored to words in the text. Slinks, on the other hand, are not anchored in the text directly, but rather relate temporal entities. The projection of links is therefore entirely determined by the projection of the entities they are defined on (see Table 1 for the notation used throughout this paper): a link l = (e, e′ ) in the source annot"
I08-1064,N03-2019,0,0.025111,"re anchored to words in the text. Slinks, on the other hand, are not anchored in the text directly, but rather relate temporal entities. The projection of links is therefore entirely determined by the projection of the entities they are defined on (see Table 1 for the notation used throughout this paper): a link l = (e, e′ ) in the source annotation as projects to the target annotation at iff both e and e′ project to non-empty sequences of words. The projection of the entities e, e′ themselves, however, is a non-trivial task. 3 TARSQI also comprises a component that introduces temporal links (Mani et al., 2003); we are not using it here because the output includes the entire tlink closure. Although Mani et al. (2006) use the links introduced by closure to boost the amount of training data for a tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations. [ . . . ]e wtj−2 wtj−1 wtj b. [ . . . ]e wtj+1 . . .[ . . . ]e′ wt Figure 3: Problematic projection scenarios: (a) noncontiguous aligned span, (b) rivalling tags. Given a temporal entity e covering a sequence as (e) of tokens in the source annotation, the project"
I08-1064,sauri-etal-2006-slinket,0,0.0299941,"Missing"
I08-1064,P06-1095,0,0.285917,"ellers for German from the projected annotations; section 6 concludes. 2 Temporal Annotation 2.1 The TimeML Specification Language The TimeML specification language (Pustejovsky et al., 2003a)1 and annotation framework emerged from the TERQAS workshop2 in the context of the ARDA AQUAINT programme. The goal of the programme is the development of question answering (QA) systems which index content rather than plain keywords. Semantic indexing based on the identification of named entities in free text is an established 1 A standardised version ISO-TimeML is in preparation, cf. Schiffrin and Bunt (2006). 2 See http://www.timeml.org/site/terqas/in dex.html method in QA and related applications. Recent years have also seen advances in relation extraction, a variant of event identification, albeit restricted in terms of coverage: the majority of systems addressing the task use a pre-defined set of—typically domainspecific—templates. In contrast, TimeML models events in a domain-independent manner and provides principled definitions for various event classes. Besides the identification of events, it addresses their relative ordering and anchoring in time by integrating timexes in the annotation."
I08-1064,J03-1002,0,0.00713781,"Missing"
I08-1064,H05-1108,0,0.0404707,"Missing"
I08-1064,P06-1146,0,0.25969,"contains an event and a timex annotation. The event-denoting verb met is aligned with the German traf, hence the latter also receives the event tag. Likewise, the components of the multi-word timex last night align with German gestern and abend, respectively, and the timex tag is transferred to the expression gestern abend. Projection-based approaches to multilingual annotation have proven adequate in various domains, including part-of-speech tagging (Yarowsky and Ngai, 2001), NP-bracketing (Yarowsky et al., 2001), dependency analysis (Hwa et al., 2005), and role semantic analysis (Pad´o and Lapata, 2006). To our knowledge, the present proposal is the first to apply projection algorithms to temporal annotations. 489 Cross-lingually projected information is typically noisy, due to errors in the source annotations as well as in the word alignment. Moreover, successful projection relies on the direct correspondence assumption (DCA, Hwa et al. (2002)) which demands that the annotations in the source text be homomorphous with those in its (literal) translation. The DCA has been found to hold, to a substantial degree, for the above mentioned domains. The results we report here show that it can also"
I08-1064,rohrer-forst-2006-improving,0,0.0501503,"Missing"
I08-1064,W03-1001,0,0.0522928,"Missing"
I08-1064,P05-3021,0,0.0913177,"Missing"
I08-1064,N01-1026,0,0.342279,"el corpus, and automatically project the annotations to the wordaligned German translation. Fig. 1 shows a simple example. The English sentence contains an event and a timex annotation. The event-denoting verb met is aligned with the German traf, hence the latter also receives the event tag. Likewise, the components of the multi-word timex last night align with German gestern and abend, respectively, and the timex tag is transferred to the expression gestern abend. Projection-based approaches to multilingual annotation have proven adequate in various domains, including part-of-speech tagging (Yarowsky and Ngai, 2001), NP-bracketing (Yarowsky et al., 2001), dependency analysis (Hwa et al., 2005), and role semantic analysis (Pad´o and Lapata, 2006). To our knowledge, the present proposal is the first to apply projection algorithms to temporal annotations. 489 Cross-lingually projected information is typically noisy, due to errors in the source annotations as well as in the word alignment. Moreover, successful projection relies on the direct correspondence assumption (DCA, Hwa et al. (2002)) which demands that the annotations in the source text be homomorphous with those in its (literal) translation. The DCA"
I08-1064,H01-1035,0,0.105752,"annotations to the wordaligned German translation. Fig. 1 shows a simple example. The English sentence contains an event and a timex annotation. The event-denoting verb met is aligned with the German traf, hence the latter also receives the event tag. Likewise, the components of the multi-word timex last night align with German gestern and abend, respectively, and the timex tag is transferred to the expression gestern abend. Projection-based approaches to multilingual annotation have proven adequate in various domains, including part-of-speech tagging (Yarowsky and Ngai, 2001), NP-bracketing (Yarowsky et al., 2001), dependency analysis (Hwa et al., 2005), and role semantic analysis (Pad´o and Lapata, 2006). To our knowledge, the present proposal is the first to apply projection algorithms to temporal annotations. 489 Cross-lingually projected information is typically noisy, due to errors in the source annotations as well as in the word alignment. Moreover, successful projection relies on the direct correspondence assumption (DCA, Hwa et al. (2002)) which demands that the annotations in the source text be homomorphous with those in its (literal) translation. The DCA has been found to hold, to a substanti"
I08-1064,W06-2001,0,0.0484665,"Missing"
I08-1064,H05-1088,0,0.0527953,"Missing"
J15-4003,J10-4006,0,0.0256897,"the similarity of contexts of two given predicates over all their instances in a corpus. To compute this measure, we first calculate the Pointwise Mutual Information (PMI) for each predicate p ∈ {p1 , p2 } and the n most frequent context words c ∈ C following Equation (11). pmi(p, c) = freq(p, c) freq(p) ∗ freq(c) (11) As we are dealing with predicates of different parts-of-speech, we calculate joint frequencies in terms of context windows instead of relying on syntactic dependencies as proposed in more recent approaches to distributional semantics (Pado´ and Lapata 2007; Erk and Pado´ 2008; Baroni and Lenci 2010). More precisely, we extract context windows of five words to the left and to the right from the Gigaword corpus (Parker et al. 2011), and compute the PMI for the 2,000 most frequent context words c1 . . . c2,000 ∈ C. The same setting has been successfully applied in related tasks, including word sense disambiguation (Guo and Diab 2011) and measuring phrase similarity (Mitchell and Lapata 2010). Vector representations are computed following Equation (12), and similarities are calculated as the cosine function of the angle between two vectors, as defined in Equation (13). p~ = (pmi(p, c1 ), pmi"
J15-4003,P05-1018,0,0.224995,"ved in related work on classifying discourse relations, information learned from artificial training data might not always generalize well to naturally occurring examples (Sporleder and Lascarides 2008). To automatically create data that is linguistically more similar to manually labeled implicit arguments, we introduce an alternative method that induces instances of implicit arguments from a raw corpus of comparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-b"
J15-4003,J08-1001,0,0.0279457,"d Lascarides 2008). To automatically create data that is linguistically more similar to manually labeled implicit arguments, we introduce an alternative method that induces instances of implicit arguments from a raw corpus of comparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are,"
J15-4003,P11-2040,0,0.0138763,"th identical predicate form and constituent order. We found that this restriction constrains affected arguments to be modifiers, prepositional phrases, and direct objects. We argue that this is actually a desirable property because more complicated alternations could affect coherence by themselves. In other words, resulting interplays would make it difficult to distinguish between the isolated effect of argument realization itself and other effects, triggered for example by sentence order (Gordon, Grosz, and Gilliom 1993). Annotation. We set up a Web experiment using the evaluation toolkit by Belz and Kow (2011) to collect ratings of local coherence for implicit and explicit arguments. For this experiment, we compiled a data set of 150 document pairs. Each text in such a pair consists of the same content, with the only difference being one argument realization. We presented all 150 pairs to two annotators12 and asked them to indicate their preference for one alternative over the other using a continuous slider scale. The annotators got to see the full texts, with the alternatives presented next to each other. To make texts easier to read and differences easier to spot, we collapsed all identical sent"
J15-4003,C10-3009,0,0.19257,"et of comparable texts used in this work contains 167,728 pairs of articles that were extracted by matching the headlines of texts published within the same time frame (Roth and Frank 2012a). A set of such document headlines is given in Example 4: Example 4 India fires tested anti-ship cruise missile (Xinhua News Agency, 29 October 2003) India tests supersonic cruise anti-ship missile (Agence France Presse, 29 October 2003) URGENT: India tests anti-ship cruise missile (Associated Press Worldstream, 29 October 2003) ¨ We preprocess each article in the set of 167,728 pairs using the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including a state-of-the-art semantic role labeler that identifies PropBank/NomBank-style predicate–argument structures (Palmer, Gildea, and Kingsbury 2005; Meyers, Reeves, and Macleod 2008). Based on the acquired PAS, we perform manual alignments. In Section 4.1.1, we summarize the annotation guidelines for this step. An overview of the resulting development and evaluation data set is provided in Section 4.1.2. 4.1.1 Manual Annotation. We selected 70 pairs of comparable texts and asked two annotators to manually align predicate–argument structures obtained from preprocessing."
J15-4003,C10-1011,0,0.146645,"sed in this work contains 167,728 pairs of articles that were extracted by matching the headlines of texts published within the same time frame (Roth and Frank 2012a). A set of such document headlines is given in Example 4: Example 4 India fires tested anti-ship cruise missile (Xinhua News Agency, 29 October 2003) India tests supersonic cruise anti-ship missile (Agence France Presse, 29 October 2003) URGENT: India tests anti-ship cruise missile (Associated Press Worldstream, 29 October 2003) ¨ We preprocess each article in the set of 167,728 pairs using the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including a state-of-the-art semantic role labeler that identifies PropBank/NomBank-style predicate–argument structures (Palmer, Gildea, and Kingsbury 2005; Meyers, Reeves, and Macleod 2008). Based on the acquired PAS, we perform manual alignments. In Section 4.1.1, we summarize the annotation guidelines for this step. An overview of the resulting development and evaluation data set is provided in Section 4.1.2. 4.1.1 Manual Annotation. We selected 70 pairs of comparable texts and asked two annotators to manually align predicate–argument structures obtained from preprocessing. Both annotator"
J15-4003,C10-1017,0,0.0228962,"airs of comparable texts as graphs and aim to find those edges in a graph that represent connections between predicates that need to be aligned. Although our aim is to find edges between nodes, we note that the majority of predicates (nodes) in our data set are not aligned and hence a crucial prerequisite to generate precise alignments is to filter out those nodes that are unlikely to be good alignment candidates. To achieve the filtering and alignment goals at the same time, we rely on graph clustering techniques that have successfully been applied in the NLP literature (Su and Markert 2009; Cai and Strube 2010; Chen and Ji 2010, inter alia) and that can be used to partition a graph into singleton nodes and smaller subgraphs. The clustering method applied in our model relies on so-called minimum cuts (henceforth also called mincuts) in order to partition a bipartite graph, representing pairs of texts, into clusters of alignable predicate–argument structures. A mincut operation divides a given graph into two disjoint subgraphs. Each cut is performed between some source node s and some target node t, such that (1) each of the two nodes will be in a different subgraph and (2) the sum of weights of all"
J15-4003,E09-1018,0,0.0456256,"Missing"
J15-4003,S10-1059,0,0.0603408,"annotations for NomBank (Meyers, Reeves, and Macleod 2008), and Moor, Roth, and Frank (2013) provided annotations for parts of the OntoNotes corpus (Weischedel et al. 2011). All three resources are, however, severely limited: Annotations in the latter two studies are restricted to 10 and 5 predicate types, respectively; the training set of the SemEval task, in contrast, consists of full-text annotations for all occurring predicates but contains only 245 instances of resolved implicit arguments in total. All groups working on the shared task identified data sparsity as one of the main issues (Chen et al. 2010; Ruppenhofer et al. 2012; Laparra and Rigau 2013). Silberer and Frank (2012) point out that additional training data can be heuristically created by treating anaphoric pronoun mentions as implicit arguments. Their experimental results confirmed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. As observed in related work on classifying discourse relations, information learned from artificial training data might not always generalize well to naturally occurring"
J15-4003,W10-2301,0,0.0129356,"xts as graphs and aim to find those edges in a graph that represent connections between predicates that need to be aligned. Although our aim is to find edges between nodes, we note that the majority of predicates (nodes) in our data set are not aligned and hence a crucial prerequisite to generate precise alignments is to filter out those nodes that are unlikely to be good alignment candidates. To achieve the filtering and alignment goals at the same time, we rely on graph clustering techniques that have successfully been applied in the NLP literature (Su and Markert 2009; Cai and Strube 2010; Chen and Ji 2010, inter alia) and that can be used to partition a graph into singleton nodes and smaller subgraphs. The clustering method applied in our model relies on so-called minimum cuts (henceforth also called mincuts) in order to partition a bipartite graph, representing pairs of texts, into clusters of alignable predicate–argument structures. A mincut operation divides a given graph into two disjoint subgraphs. Each cut is performed between some source node s and some target node t, such that (1) each of the two nodes will be in a different subgraph and (2) the sum of weights of all removed edges will"
J15-4003,J08-4005,0,0.06079,"Missing"
J15-4003,C90-2047,0,0.147538,"Missing"
J15-4003,P11-1118,0,0.0428116,"Missing"
J15-4003,P11-2022,0,0.0743087,"ng. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, solely based on explicit entity mentions, resulting in insufficient representations when dealing with inferable references. Example 3 illustrates this shortcoming. Example 3 (a) 27 tons of cigarettes were picked up in Le Havre. (b) The containers had a"
J15-4003,D08-1094,0,0.10509,"Missing"
J15-4003,W07-2321,0,0.0332981,"mparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, solely based on explicit entity mentions, resulting in insufficient representations when dealing with inferable references. Example 3 illustrates this shortcoming. Example 3 (a) 27 tons of cigarettes were picked up in Le H"
J15-4003,J12-1005,0,0.0255035,"nomena such as coreference (Postolache, Cristea, and Orasan 2006). All of the aforementioned instances of the projection approach make use of the same underlying technique: Firstly, words are 630 Roth and Frank Inducing Implicit Arguments from Comparable Texts aligned in a parallel corpus using statistical word alignment; secondly, annotations on a single word or between multiple words in one text are transferred to the corresponding aligned word(s) in the parallel text. This procedure typically assumes that two parallel ¨ sentences express the same meaning. A notable exception is the work by Furstenau and Lapata (2012), which utilizes alignments between syntactic structures to “project” semantic role information from a role-annotated corpus to unseen sentences that are selected from a corpus in the same language. In our work, we apply annotation projection to monolingual comparable texts. In comparison to parallel texts, we have to account for potential differences in perspective and detail that make our task—in particular, the alignment sub-task—considerably ¨ more difficult. In contrast to Furstenau and Lapata’s setting, which involves incomparable texts, we assume that text pairs in our setting still con"
J15-4003,J12-4003,0,0.341067,"withdrew its troops last month.1 Applying a semantic role labeling system on sentence (1b) produces a representation that consists of the predicate withdraw, a temporal modifier (last month) and two associated arguments: the entity withdrawing (Nicaragua) and the thing being withdrawn (its troops). From the previous sentence (1a), we can additionally infer a third argument: namely, the source from which Nicaragua withdrew its troops (Iraq). By leaving this piece of information implicit, the text fragment in sentence (1b) illustrates a typical case of non-local, or implicit, role realization (Gerber and Chai 2012). In this article, we view implicit arguments as a discourse-level phenomenon and treat corresponding instances as implicit references to discourse entities. Taking this perspective, we build upon previous work on discourse analysis. Following Sidner (1979) and Joshi and Kuhn (1979), utterances in discourse typically focus on a set of salient entities, which are also called the foci or centers. Using the notion of centers, Grosz, Joshi, and Weinstein (1995) defined the Centering framework, which relates the salience of an entity in discourse to linguistic factors such as choice of referring ex"
J15-4003,W13-0111,0,0.713011,"Missing"
J15-4003,J95-2003,0,0.809679,"Missing"
J15-4003,D11-1051,0,0.019409,"th predicates of different parts-of-speech, we calculate joint frequencies in terms of context windows instead of relying on syntactic dependencies as proposed in more recent approaches to distributional semantics (Pado´ and Lapata 2007; Erk and Pado´ 2008; Baroni and Lenci 2010). More precisely, we extract context windows of five words to the left and to the right from the Gigaword corpus (Parker et al. 2011), and compute the PMI for the 2,000 most frequent context words c1 . . . c2,000 ∈ C. The same setting has been successfully applied in related tasks, including word sense disambiguation (Guo and Diab 2011) and measuring phrase similarity (Mitchell and Lapata 2010). Vector representations are computed following Equation (12), and similarities are calculated as the cosine function of the angle between two vectors, as defined in Equation (13). p~ = (pmi(p, c1 ), pmi(p, c2 ), . . . , pmi(p, c2,000 )) simDist (p1 , p2 ) = p~1 · p~2 ||p~1 ||∗ ||p~2 || (12) (13) 6 Note that the weight of 0.8 was set in an ad hoc manner (instead of being optimized) in order to avoid overfitting on our small development corpus. 636 Roth and Frank Inducing Implicit Arguments from Comparable Texts Bag-of-Words Similarity."
J15-4003,N13-1111,0,0.0440643,"Missing"
J15-4003,J09-1003,0,0.0195639,"lience, that is, contexts of referential continuity and irrelevance, can also be reflected by the non-realization of an entity (Brown 1983). Specific instances of this phenomenon, so-called zero anaphora, have been well-studied in pro-drop languages such as Japanese (Kameyama 1985), Turkish (Turan 1995), and Italian (Di Eugenio 1990). For English, only a few studies exist that explicitly investigated the effect of non-realizations on coherence. Existing work suggests, however, that indirect references and non-realizations are important for modeling and measuring coherence (Poesio et al. 2004; Karamanis et al. 2009), respectively, and that such phenomena need to be taken into consideration to explain local coherence where adjacent sentences are neither connected by discourse relations nor in terms of coreference (Louis and Nenkova 2010). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example 1 illustrated a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. That is, mentioning Iraq in the second sentence is not necessary (for a human being) to understan"
J15-4003,J93-1006,0,0.698565,"Missing"
J15-4003,W13-0114,0,0.0321226,"Missing"
J15-4003,J13-4004,0,0.0229695,"Missing"
J15-4003,D12-1045,0,0.173169,"st to work on paraphrasing, we are specifically interested in pairs of text fragments that involve implicit arguments, which can only be resolved in context. In line with our goal of inducing implicit arguments, we define the units or expressions to be aligned in our task as the predicate–argument structures that can (automatically) be identified in text. This task definition further makes our task distinct from event coreference, where coreference is established based on a pre-specified set of events, reference types, or definitions of event identity (Walker et al. 2006; Pradhan et al. 2007; Lee et al. 2012, inter alia). Although corresponding annotations can certainly overlap with those in our task, we emphasize that the focus of our work is not to find all occurrences of co-referring events. Instead, our goal is to align predicate–argument structures that have a common meaning in context across different discourses. Hence, we neither consider intra-document coreference nor pronominal event references here. As alignable units in our work are not restricted to a pre-specified definition of event or event identity, the task addressed here involves any kind of event, state, or entity that is lingu"
J15-4003,N06-1014,0,0.0822724,"Missing"
J15-4003,P14-5010,0,0.00753894,"uments implicit in one structure can straightforwardly be induced based on this information by looking for co-referring mentions of the argument explicit in the aligned structure. In practice, we make use of additional checks and filters to ensure that only reliable information is being used. We describe the preprocessing steps in the following paragraphs and provide additional details on our implementation of the induction procedure in Section 5.2. Single Document Preprocessing. We apply several preprocessing steps to each document in our data set. First, we use the Stanford CoreNLP package (Manning et al. 2014) ¨ for tokenization and sentence splitting. We then apply the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including the integrated PropBank/NomBank-based semantic parser, to determine local PAS. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012), which placed second for English in the CoNLL-2012 Shared Task (Pradhan et al. 2012). High Precision Alignments. Once all single documents are preprocessed, we align PAS across pairs of comparable texts. We want to induce reliable instances of implicit arguments based on aligned PAS"
J15-4003,W12-4511,0,0.0246843,"Missing"
J15-4003,P10-1158,0,0.0128194,"y more similar to manually labeled implicit arguments, we introduce an alternative method that induces instances of implicit arguments from a raw corpus of comparable texts. Coherence Modeling. In the context of coherence modeling, much previous work has focused on entity-based approaches, with the most prominent model being the entity grid by Barzilay and Lapata (2005). This model has originally been proposed for automatic sentence ordering but has since also been applied in coherence evaluation and readability assessment (Barzilay and Lapata 2008; Pitler and Nenkova 2008), story generation (McIntyre and Lapata 2010), and authorship attribution (Feng and Hirst 2014). Based on the original model, several extensions have been proposed: For example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, solely based on explicit entity mentions, resulting in insuffi"
J15-4003,W13-0211,1,0.915473,"Missing"
J15-4003,J07-2002,0,0.153484,"Missing"
J15-4003,J05-1004,0,0.0421712,"Missing"
J15-4003,P86-1004,0,0.212552,"in paraphrasing and event coreference. 3.1 Implicit Arguments and Coherence Modeling The goal of this work is to induce instances of implicit arguments, together with their discourse antecedents, and to utilize them in semantic processing and coherence modeling. This section summarizes previous work on implicit arguments and coherence modeling, and provides an outlook on how instances of implicit arguments can be of use in a novel entity-based model of local coherence. Implicit Arguments. The role of implicit arguments was studied early on in the context of semantic processing (Fillmore 1986; Palmer et al. 1986), although most semantic 628 Roth and Frank Inducing Implicit Arguments from Comparable Texts role labeling systems nowadays operate solely within local syntactic structures and do not perform any additional inference regarding missing information. First data sets that focus on implicit arguments have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse,” Gerber and Chai (2012) made available implicit argument annotations for NomBank (Meyers, Reeves, and Macleod 2008), and Moor, Roth, and Frank (2013) provide"
J15-4003,N10-1047,0,0.0257012,"k by considering discourse-level information and an additional argument-specific measure that takes into account argument labels. We demonstrate the benefits of these measures in practice in Section 4.3. Similarity in WordNet. Given all synsets that contain the two predicates p1 , p2 , we compute their similarity in WordNet (Fellbaum 1998) as the maximal pairwise score calculated using the information content based measure proposed by Lin (1998). We rely on the WordNet hierarchy to find the least common subsumer (lcs) of two synsets and use the pre-computed Information Content (IC) files from Pedersen (2010) to compute this measure as defined in Equation (9). simWN (p1 , p2 ) = max hs1 ,s2 i:si ∈synsets(pi ) IC(lcs(s1 , s2 )) IC(s1 ) ∗ IC(s2 ) (9) 635 Computational Linguistics Volume 41, Number 4 Similarity in VerbNet. We additionally make use of VerbNet (Kipper et al. 2008) to compute similarities between verb pairs that cannot be captured by WordNet relations. Verbs in VerbNet are categorized into classes according to their meaning as well as syntactic behavior. A verb class C can recursively embed sub-classes Cs ∈ sub(C) that represent finer semantic and syntactic distinctions. In Equation (10"
J15-4003,D08-1020,0,0.0699801,"Missing"
J15-4003,J04-3003,0,0.0474076,"Missing"
J15-4003,postolache-etal-2006-transferring,0,0.0745251,"Missing"
J15-4003,W12-4501,0,0.0280163,"mentation of the induction procedure in Section 5.2. Single Document Preprocessing. We apply several preprocessing steps to each document in our data set. First, we use the Stanford CoreNLP package (Manning et al. 2014) ¨ for tokenization and sentence splitting. We then apply the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including the integrated PropBank/NomBank-based semantic parser, to determine local PAS. Finally, we resolve pronouns that occur in a PAS using the coreference resolution system by Martschat et al. (2012), which placed second for English in the CoNLL-2012 Shared Task (Pradhan et al. 2012). High Precision Alignments. Once all single documents are preprocessed, we align PAS across pairs of comparable texts. We want to induce reliable instances of implicit arguments based on aligned PASs pairs and hence apply our graph-based clustering technique using the high-precision tuning step described in Section 4.3. We run Figure 2 Illustration of the induction approach: Texts consist of PAS (represented by overlapping rounded rectangles); we exploit alignments between corresponding predicates across texts (solid lines) and co-referring entity mentions (dashed lines) to infer implicit arg"
J15-4003,J10-4004,0,0.0174609,"ument). Because our task is based in a monolingual setting, we can make use of the same preprocessing tools across texts. Paraphrasing and Event Coreference. We overcome the difficulty of inducing word alignments across comparable texts by computing alignments on the basis of predicate– argument structures. Using predicate–argument structures as targets makes our setting related to previous work on paraphrase detection and coreference resolution of event mentions. Each of these tasks focuses, however, on a different level of linguistic analysis from ours: Following the definitions embraced by Recasens and Vila (2010), “paraphrasing” is a relation between two lexical units that have the same meaning, whereas “coreference” indicates that two referential expressions point to the same referent in discourse.3 In contrast to work on paraphrasing, we are specifically interested in pairs of text fragments that involve implicit arguments, which can only be resolved in context. In line with our goal of inducing implicit arguments, we define the units or expressions to be aligned in our task as the predicate–argument structures that can (automatically) be identified in text. This task definition further makes our ta"
J15-4003,S12-1030,1,0.823849,"Section 4.3. 4.1 Corpus and Annotation As a basis for aligning predicate–argument structures across texts, we make use of a data set of comparable texts extracted from the English Gigaword corpus (Parker et al. 2011). The Gigaword corpus is one of the largest English corpora available in the news domain and contains over 9.8 million articles from seven newswire agencies that report on (the same) real-world incidents. The data set of comparable texts used in this work contains 167,728 pairs of articles that were extracted by matching the headlines of texts published within the same time frame (Roth and Frank 2012a). A set of such document headlines is given in Example 4: Example 4 India fires tested anti-ship cruise missile (Xinhua News Agency, 29 October 2003) India tests supersonic cruise anti-ship missile (Agence France Presse, 29 October 2003) URGENT: India tests anti-ship cruise missile (Associated Press Worldstream, 29 October 2003) ¨ We preprocess each article in the set of 167,728 pairs using the MATE tools (Bjorkelund et al. 2010; Bohnet 2010), including a state-of-the-art semantic role labeler that identifies PropBank/NomBank-style predicate–argument structures (Palmer, Gildea, and Kingsbury"
J15-4003,D12-1016,1,0.892284,"Missing"
J15-4003,S10-1008,0,0.477606,"vides an outlook on how instances of implicit arguments can be of use in a novel entity-based model of local coherence. Implicit Arguments. The role of implicit arguments was studied early on in the context of semantic processing (Fillmore 1986; Palmer et al. 1986), although most semantic 628 Roth and Frank Inducing Implicit Arguments from Comparable Texts role labeling systems nowadays operate solely within local syntactic structures and do not perform any additional inference regarding missing information. First data sets that focus on implicit arguments have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared task on “linking events and participants in discourse,” Gerber and Chai (2012) made available implicit argument annotations for NomBank (Meyers, Reeves, and Macleod 2008), and Moor, Roth, and Frank (2013) provided annotations for parts of the OntoNotes corpus (Weischedel et al. 2011). All three resources are, however, severely limited: Annotations in the latter two studies are restricted to 10 and 5 predicate types, respectively; the training set of the SemEval task, in contrast, consists of full-text annotations for all occurring predicates but contains only 245 in"
J15-4003,S12-1001,1,0.928839,"Missing"
J15-4003,I08-1064,1,0.816176,"rationale of this approach is to induce annotated data in one language, given already-annotated instances in another language. As an example, semantic role annotations of a text in English can be transferred to a parallel text in order to induce annotated instances for a lexicon in another language (Pado´ and Lapata 2009). In previous work, this method has been applied on various levels of linguistic analysis: from syntactic information in the form of part-of-speech tags and dependencies (Yarowsky and Ngai 2001; Hwa et al. 2005), through annotation of temporal expressions and semantic roles (Spreyer and Frank 2008; van der Plas, Merlo, and Henderson 2011), to discourse-level phenomena such as coreference (Postolache, Cristea, and Orasan 2006). All of the aforementioned instances of the projection approach make use of the same underlying technique: Firstly, words are 630 Roth and Frank Inducing Implicit Arguments from Comparable Texts aligned in a parallel corpus using statistical word alignment; secondly, annotations on a single word or between multiple words in one text are transferred to the corresponding aligned word(s) in the parallel text. This procedure typically assumes that two parallel ¨ sente"
J15-4003,N09-1001,0,0.0344601,"Missing"
J15-4003,S10-1065,0,0.0943488,"Missing"
J15-4003,W11-0908,0,0.0403508,"Missing"
J15-4003,P11-2052,0,0.0564057,"Missing"
J15-4003,U06-1019,0,0.012047,"ing described in Section 4.2.3. Instead, it greedily merges as many 1-to-1 alignments as possible, starting with the highest similarity (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation. We train our own word alignment model using the state-of-the-art word alignment tool Berkeley Aligner (Liang, Taskar, and Klein 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases using a re-implementation of a previously proposed paraphrase detection system based on lemma and n-gram overlap (Wan et al. 2006). In the following section, we abbreviate the alignment based model as WordAlign. 4.3.2 Results. The results for the alignment task are presented in Table 2. From all approaches, Greedy and WordAlign yield the lowest performance. For WordAlign, we observe two main reasons. On the one hand, sentence paraphrase detection does not perform perfectly. Hence, the extracted sentence pairs do not always contain gold 8 We also performed an evaluation on sentence-level predicate alignment, but skipped the discussion here as this task is not relevant for our induction framework. As the additional discour"
J15-4003,P13-2012,0,0.0505633,"Missing"
J15-4003,N01-1026,0,0.0744396,"n to ensure that necessary references are explicit and that redundant repetitions are avoided. 3.2 Semantic Resource Induction The methods applied in this article are based on ideas from previous work on inducing semantic resources from parallel and comparable texts. Most work in this direction has been done in the context of cross-lingual settings, including the learning of transla¨ tions of words and phrases using statistical word alignments (Kay and Roscheisen ˆ e, and Klein 2008, inter alia) and approaches to pro1993; DeNero, Bouchard-Cot´ jecting annotations from one language to another (Yarowsky and Ngai 2001; Kozhevnikov and Titov 2013, inter alia). In the following, we discuss previous approaches to annotation projection as well as related work in paraphrasing and event coreference. Annotation Projection. A widespread method for the induction of semantic resources is the so-called annotation projection approach. The rationale of this approach is to induce annotated data in one language, given already-annotated instances in another language. As an example, semantic role annotations of a text in English can be transferred to a parallel text in order to induce annotated instances for a lexicon in a"
J15-4003,C00-2137,0,0.0403076,"Missing"
J15-4003,N10-1043,0,\N,Missing
J15-4003,N10-1138,0,\N,Missing
J15-4003,H86-1011,0,\N,Missing
J15-4003,D08-1033,0,\N,Missing
J15-4003,P08-2011,0,\N,Missing
K16-2014,W15-3037,0,0.0175955,"using among others Recurrent Neural Networks and CNNs for matching sentences, as well as other neural network models that incorporate correlation between the input arguments, such as the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b). Since we observe that the neural network approaches improve on the LR Embeddings-only models for most of the senses, in future work we could combine these models with our wellperforming similarity features. Combining the output of a deep learning system with additional features has been shown to achieve state of the art performance in other tasks (Kreutzer et al., 2015). Acknowledgments. This work is supported by the German Research Foundation as part of the Research Training Group “Adaptive Preparation of Information from Heterogeneous Sources” (AIPHES) under grant No. GRK 1994/1. We thank Ana Marasovi´c for her advice in the implementation of CNN models. Conclusion and Future work In this paper we describe our system for the participation in the CoNLL Shared Task on Discourse Relation Sense Classification. We compare different approaches including Logistic Regression classifiers using features based on word embeddings and cross-argument similarity and two"
K16-2014,K15-2007,0,0.0121047,"hierarchically 1 https://github.com/tbmihailov/ conll16st-hd-sdp - Source code for our Discourse Relation Sense Classification system 100 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 100–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have a connective that can help to determine their sense. In last year’s task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional"
K16-2014,D15-1262,0,0.0256724,"an, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional Neural Networks (Zhang et al., 2015). Braud and Denis (2015) compare word representations for implicit discourse relation classification and find that denser representations systematically outperform sparser ones. 3 Figure 1: System architecture: Training and evaluating models for Explicit and Non-Explicit discourse relation sense classification into one of the given fifteen classes of relation senses. For detecting Non-Explicit discourse relations we also make use of a feature-based approach, but in addition we experiment with two models based on Convolutional Neural Networks. Method 3.1 We divide the task into two subtasks, and develop separate class"
K16-2014,P14-2050,0,0.0132285,"on the Train and evaluated on the Dev set. Data In our experiments we use the official data (English) provided from the task organizers: Train (15500 Explicit + 18115 Non-Explicit), Dev (740 Explicit + 782 Non-Explicit), Test (990 Explicit + 1026 Non-Explicit), Blind (608 Explicit + 661 Non-Explicit). All models are trained on Train set. 4.2 Embeddings only experiments. The first three columns show the results obtained with three approaches that use only features based on word embeddings. We use word2vec word embeddings. We also experimented with pre-trained dependency-based word embeddings (Levy and Goldberg, 2014), but this yielded slightly worse results on the Dev set. Classifier settings For our feature-based approach we concatenate the extracted features in a feature vector, scale their values to the 0 to 1 range, and feed the vectors to a classifier. We train and evaluate a L2-regularized Logistic Regression classifier with the LIBLINEAR (Fan et al., 2008) solver as implemented in scikit-learn (Pedregosa et al., 2011). For most of our experiments, we tuned the classifier with different values of the C (cost) parameter, and chose C=0.1 as it yielded the best accuracy on 5-fold cross-validation on th"
K16-2014,K15-2006,0,0.0162966,"tLex or EntRel. A further attribute to be detected is the relation Sense, which can be one of 15 classes organized hierarchically 1 https://github.com/tbmihailov/ conll16st-hd-sdp - Source code for our Discourse Relation Sense Classification system 100 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 100–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have a connective that can help to determine their sense. In last year’s task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network a"
K16-2014,W16-1613,1,0.889026,"Missing"
K16-2014,P02-1047,0,0.0861986,"have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional Neural Networks (Zhang et al., 2015). Braud and Denis (2015) compare word representations for implicit discourse relation classification and find that denser representations systematically outperform sparser ones. 3 Figure 1: System architecture: Training and evaluating models for Explicit and Non-Explicit discourse relation sense classification into one of the given fifteen classes of relation senses. For detecting Non-Explicit"
K16-2014,P16-2075,0,0.0241401,"Missing"
K16-2014,S16-1137,0,0.0248516,"Missing"
K16-2014,S16-1136,1,0.289669,"onnective, as well as on parts of the arguments: results in adding 103 similarity features for every relation. We use these features for implicit discourse relations sense classification only. We assume that knowledge about the relation sense can be inferred by calculating the similarity between the semantic information of the relation arguments and specific discourse connectives. Our feature-based approach yields very good results on Explicit relations sense classification with an F-score of 0.912 on the Dev set. Combining features based on word embeddings and similarity between arguments in Mihaylov and Nakov (2016) yielded state-of-the art performance in a similar task setup in Community Question Answering (Nakov et al., 2016), where two text arguments (question and answer) are to be ranked. Arg1 to Arg2 similarity. We assume that for given arguments Arg1 and Arg2 that stand in a specific discourse relation sense, their centroid vectors should stand in a specific similarity relation to each other. We thus use their cosine similarity as a feature. Maximized similarity. Here we rank each word in Arg2’s text according to its similarity with the centroid vector of Arg1, and we compute the average similarity"
K16-2014,N13-1090,0,0.0522411,"nk}@cl.uni-heidelberg.de Abstract in 4 parent classes. With this work we participate in the Supplementary Task on Discourse Relation Sense Classification in English. The task is to predict the discourse relation sense when the arguments Arg1, Arg2 are given, as well as the Discourse Connective in case of explicit marking. In our contribution we compare different approaches including a Logistic Regression classifier using similarity features based on word embeddings, and two Convolutional Neural Network architectures. We show that an approach using only word embeddings retrieved from word2vec (Mikolov et al., 2013) and cross-argument similarity features is simple and fast, and yields results that rank first in the Overall, second in the Explicit and forth in the Non-Explicit sense classification task. Our system’s code is publicly accessible1 . This paper describes our system for the CoNLL 2016 Shared Task’s supplementary task on Discourse Relation Sense Classification. Our official submission employs a Logistic Regression classifier with several cross-argument similarity features based on word embeddings and performs with overall F-scores of 64.13 for the Dev set, 63.31 for the Test set and 54.69 for t"
K16-2014,D14-1181,0,0.0889087,". Here we rank each word in Arg2’s text according to its similarity with the centroid vector of Arg1, and we compute the average similarity for the top-ranked N words. We chose the similarity scores of the top 1,2,3 and 5 words as features. The assumption is that the average similarity between the first argument (Arg1) and the top N most similar words in the second argument (Arg2) might imply a specific sense. 3.2 CNNs for sentence classification We also experiment with Convolutional Neural Network architectures to detect Implicit relation senses. We have implemented the CNN model proposed in Kim (2014) as it proved successful in tasks like sentence classification and modal sense classification (Marasovi´c and Frank, 2016). This model (Figure 2) defines one convolutional layer that uses pre-trained Word2Vec vectors trained on the Google News dataset. As shown in Kim (2014), this architecture yields very good results for various single sentence classification tasks. For our relation classification task we input the concatenated tokens of Arg1 and Arg2. Aligned similarity. For each word in Arg1, we choose the most similar word from the yield of Arg2 and we take the average of all best word pai"
K16-2014,S16-1083,0,0.0105824,"Missing"
K16-2014,S15-2038,0,0.01615,"s like sentence classification and modal sense classification (Marasovi´c and Frank, 2016). This model (Figure 2) defines one convolutional layer that uses pre-trained Word2Vec vectors trained on the Google News dataset. As shown in Kim (2014), this architecture yields very good results for various single sentence classification tasks. For our relation classification task we input the concatenated tokens of Arg1 and Arg2. Aligned similarity. For each word in Arg1, we choose the most similar word from the yield of Arg2 and we take the average of all best word pair similarities, as suggested in Tran et al. (2015). Part of speech (POS) based word vector similarities. We used part of speech tags from the parsed input data provided by the organizers, and computed similarities between centroid vectors of words with a specific tag from Arg1 and the centroid vector of Arg2. Extracted features for POS similarities are symmetric: for example we calculate the similarity between Nouns from Arg1 with Pronouns from Arg2 and the opposite. The assumption is that some parts of speech between Arg1 and Arg2 might be closer than other parts of speech depending on the relation sense. Explicit discourse connectives simil"
K16-2014,K15-2002,0,0.40748,"ttribute to be detected is the relation Sense, which can be one of 15 classes organized hierarchically 1 https://github.com/tbmihailov/ conll16st-hd-sdp - Source code for our Discourse Relation Sense Classification system 100 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 100–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have a connective that can help to determine their sense. In last year’s task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been"
K16-2014,C08-2022,0,0.0285995,"ared task on Shallow Discourse Parsing (Xue et al., 2015). The difference to last year’s task is that there is a new Supplementary Task on Discourse Relation Sense classification, where participants are not required to build an end-to-end discourse relation parser but can participate with a sense classification system only. Discourse relations in the task are divided in two major types: Explicit and Non-Explicit (Implicit, EntRel and AltLex). Detecting the sense of Explicit relations is an easy task: given the discourse connective, the relation sense can be determined with very high accuracy (Pitler et al., 2008). A challenging task is to detect the sense of NonExplicit discourse relations, as they usually don’t Introduction The CoNLL 2016 Shared Task on Shallow Discourse Parsing (Xue et al., 2016) focuses on identifying individual discourse relations presented in text. This year the shared task has a main track that requires end-to-end discourse relation parsing and a supplementary task that is restricted to discourse relation sense classification. For the main task, systems are required to build a system that given a raw text as input can identify arguments Arg1 and Arg2 that are related in the disc"
K16-2014,K15-2001,0,0.17743,"Regression classifier to different Convolutional Neural Network architectures. After the official submission we enriched our model for Non-Explicit relations by including similarities of explicit connectives with the relation arguments, and part of speech similarities based on modal verbs. This improved our Non-Explicit result by 1.46 points on the Dev set and by 0.36 points on the Blind set. 1 2 Related Work This year’s CoNLL 2016 Shared Task on Shallow Discourse Parsing (Xue et al., 2016) is the second edition of the shared task after the CoNLL 2015 Shared task on Shallow Discourse Parsing (Xue et al., 2015). The difference to last year’s task is that there is a new Supplementary Task on Discourse Relation Sense classification, where participants are not required to build an end-to-end discourse relation parser but can participate with a sense classification system only. Discourse relations in the task are divided in two major types: Explicit and Non-Explicit (Implicit, EntRel and AltLex). Detecting the sense of Explicit relations is an easy task: given the discourse connective, the relation sense can be determined with very high accuracy (Pitler et al., 2008). A challenging task is to detect the"
K16-2014,K16-2001,0,0.159291,"set and 54.69 for the Blind set, ranking first in the Overall ranking for the task. We compare the feature-based Logistic Regression classifier to different Convolutional Neural Network architectures. After the official submission we enriched our model for Non-Explicit relations by including similarities of explicit connectives with the relation arguments, and part of speech similarities based on modal verbs. This improved our Non-Explicit result by 1.46 points on the Dev set and by 0.36 points on the Blind set. 1 2 Related Work This year’s CoNLL 2016 Shared Task on Shallow Discourse Parsing (Xue et al., 2016) is the second edition of the shared task after the CoNLL 2015 Shared task on Shallow Discourse Parsing (Xue et al., 2015). The difference to last year’s task is that there is a new Supplementary Task on Discourse Relation Sense classification, where participants are not required to build an end-to-end discourse relation parser but can participate with a sense classification system only. Discourse relations in the task are divided in two major types: Explicit and Non-Explicit (Implicit, EntRel and AltLex). Detecting the sense of Explicit relations is an easy task: given the discourse connectiv"
K16-2014,prasad-etal-2008-penn,0,0.282123,"he parsed input data provided by the organizers, and computed similarities between centroid vectors of words with a specific tag from Arg1 and the centroid vector of Arg2. Extracted features for POS similarities are symmetric: for example we calculate the similarity between Nouns from Arg1 with Pronouns from Arg2 and the opposite. The assumption is that some parts of speech between Arg1 and Arg2 might be closer than other parts of speech depending on the relation sense. Explicit discourse connectives similarity. We collected 103 explicit discourse connectives from the Penn Discourse Treebank (Prasad et al., 2008) annotation manual4 and for all of them construct vector representations according to (2), where for multi-token connectives we calculate a centroid vector from all tokens in the connective. For every discourse connective vector representation we calculate the similarity with the centroid vector representations from all Arg1 and Arg2 tokens. This Figure 2: CNN architecture by Kim (2014). 3.3 Modified ARC-1 CNN for sentence matching An alternative model we try for Implicit discourse relation sense classification is a modification of the ARC-1 architecture proposed for sentence matching by Hu et"
K16-2014,D15-1266,0,0.0451358,"y lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional Neural Networks (Zhang et al., 2015). Braud and Denis (2015) compare word representations for implicit discourse relation classification and find that denser representations systematically outperform sparser ones. 3 Figure 1: System architecture: Training and evaluating models for Explicit and Non-Explicit discourse relation sense classification into one of the given fifteen classes of relation senses. For detecting Non-Explicit discourse relations we also make use of a feature-based approach, but in addition we experiment with two models based on Convolutional Neural Networks. Method 3.1 We divide the task into two subtasks, an"
K16-2014,E14-1068,0,0.0808682,"nse Classification system 100 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 100–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have a connective that can help to determine their sense. In last year’s task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional Neural Networks (Zhang et al., 2015). Braud and Denis (2015) compare word representations for implicit discourse re"
K16-2014,N15-1081,0,0.0319344,"16. 2016 Association for Computational Linguistics have a connective that can help to determine their sense. In last year’s task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-task Neural Networks (Liu et al., 2016) and Shallow-Convolutional Neural Networks (Zhang et al., 2015). Braud and Denis (2015) compare word representations for implicit discourse relation classification and find that denser representations systematically outperform sparser ones. 3 Figure 1: System architecture: Training and evaluating models for E"
K16-2014,K15-2003,0,0.0185274,"ted is the relation Sense, which can be one of 15 classes organized hierarchically 1 https://github.com/tbmihailov/ conll16st-hd-sdp - Source code for our Discourse Relation Sense Classification system 100 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 100–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have a connective that can help to determine their sense. In last year’s task Non-Explicit relations have been tackled with features based on Brown clusters (Chiarcos and Schenk, 2015; Wang and Lan, 2015; Stepanov et al., 2015), VerbNet classes (Kong et al., 2015; Lalitha Devi et al., 2015) and MPQA polarity lexicon (Wang and Lan, 2015; Lalitha Devi et al., 2015). Earlier work (Rutherford and Xue, 2014) employed Brown cluster and coreference patterns to identify senses of implicit discourse relations in naturally occurring text. More recently Rutherford and Xue (2015) improved inference of implicit discourse relations via classifying explicit discourse connectives, extending prior research (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008). Several neural network approaches have been proposed, e.g., Multi-ta"
K16-2014,K15-2004,0,\N,Missing
L16-1484,P12-1090,0,0.020379,"SemAF-SR based on LIRICS (ISO, 2014) proposed a semantic role inventory in close consideration of VerbNet roles. Bonial et al. (2011) and Hwang (2014) present revised VerbNet role hierarchies. They serve as a basis for proposed revisions to the VerbNet role inventory, as described in Sec. 3.2. and 3.6. VerbNet is based on a set of semantic verb classes. Assigning a verb to its class is similar to a sense disambiguation act, with senses defined by the syntactic alternation behaviour of verbs (Levin, 1993). Prior work has investigated the multilingual applicability of VerbNet semantic classes (Falk et al., 2012). While Levin-style verb classes are well established, the classes are relatively coarse-grained and not all of them are semantically homogeneous. German sense- and role-annotated corpora. There are only few sense- and role-annotated corpora for German. Corpora with word sense annotations according to GermaNet have been created by Broscheit et al. (2010), Henrich et al. (2012), and Henrich and Hinrichs (2014). However, none of these provide semantic role annotation. Schulte im Walde (2006) clustered German verbs to semantic verb classes inspired by Levin (1993), trying to match semantic verb ’"
L16-1484,W97-0802,0,0.691546,"ation scheme and establish detailed annotation guidelines. Finally, combining word sense and semantic role annotation raises several questions that we will address in our study: To what extent are word sense and role annotation dependent on each other? What kinds of preannotation for predicates and arguments are helpful to support role annotation (e.g., presenting dependency heads or full argument spans as role targets)? To answer these questions, we design contrastive annotation setups and evaluate their impact on annotation processes and results. In contrast to VerbNet, we use GermaNet 9.0 (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) – the German counterpart of Princeton WordNet (Fellbaum, 1998) – as a fine-grained sense inventory for predicate labeling, and investigate its suitability for combined sense and VerbNet-style role labeling. Our contributions We provide a novel adaptation of a VerbNet-style semantic role set for manual annotation on a German corpus. We identify best annotation practice for joint word sense and VerbNet-style role labeling, including analysis of IAA, and investigate systematic dependencies between predicate and role annotation. Using the combined annotation scheme, w"
L16-1484,henrich-hinrichs-2010-gernedit,0,0.027528,"sh detailed annotation guidelines. Finally, combining word sense and semantic role annotation raises several questions that we will address in our study: To what extent are word sense and role annotation dependent on each other? What kinds of preannotation for predicates and arguments are helpful to support role annotation (e.g., presenting dependency heads or full argument spans as role targets)? To answer these questions, we design contrastive annotation setups and evaluate their impact on annotation processes and results. In contrast to VerbNet, we use GermaNet 9.0 (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) – the German counterpart of Princeton WordNet (Fellbaum, 1998) – as a fine-grained sense inventory for predicate labeling, and investigate its suitability for combined sense and VerbNet-style role labeling. Our contributions We provide a novel adaptation of a VerbNet-style semantic role set for manual annotation on a German corpus. We identify best annotation practice for joint word sense and VerbNet-style role labeling, including analysis of IAA, and investigate systematic dependencies between predicate and role annotation. Using the combined annotation scheme, we conduct a large-scale annot"
L16-1484,E12-1039,0,0.0245846,"its class is similar to a sense disambiguation act, with senses defined by the syntactic alternation behaviour of verbs (Levin, 1993). Prior work has investigated the multilingual applicability of VerbNet semantic classes (Falk et al., 2012). While Levin-style verb classes are well established, the classes are relatively coarse-grained and not all of them are semantically homogeneous. German sense- and role-annotated corpora. There are only few sense- and role-annotated corpora for German. Corpora with word sense annotations according to GermaNet have been created by Broscheit et al. (2010), Henrich et al. (2012), and Henrich and Hinrichs (2014). However, none of these provide semantic role annotation. Schulte im Walde (2006) clustered German verbs to semantic verb classes inspired by Levin (1993), trying to match semantic verb ’fields’ as defined by Schumacher (1986). These classes have not been employed for manual corpus labeling or automatic sense tagging. For semantic role annotation, SALSA (Burchardt et al., 2009) is the only available corpus. It is annotated with FrameNet 1.3 frames and roles, and extended with protoframes for predicates unknown to FrameNet 1.3. For the CoNLL 2009 SRL task (Haji"
L16-1484,P09-1033,0,0.756801,"Missing"
L16-1484,C14-2023,1,0.897458,"Missing"
L16-1484,J05-1004,0,0.562795,"A corpus. We publish the annotated corpus and detailed guidelines for the new role annotation scheme. Keywords: word sense annotation, semantic role annotation, GermaNet, VerbNet, SALSA corpus, guidelines, German 1. Introduction Semantic annotation of predicate-argument structure is an important task in NLP. During decades, different frameworks for representing semantic predicate-argument structure have been established, notably FrameNet, VerbNet and PropBank, with accompanying sense and role inventories and annotated resources, primarily for English (Baker et al., 1998; Kipper-Schuler, 2005; Palmer et al., 2005). The resources are interoperable via SemLink and the Unified Verb Index (Loper et al., 2007). Combining GermaNet sense and VerbNet role annotation Within these established frameworks, the characterization of predicates and roles differ in important ways: FrameNet defines semantic roles for verbs, nouns and adjectives evoking a frame. Due to its large frame inventory, and since roles are specific to a frame, there is a vast amount of roles to distinguish, and roles do not generalize across frames. PropBank defines six major roles (A0-A5); except for A0 and A1, their interpretation is not consi"
L16-1484,petukhova-bunt-2008-lirics,0,0.162875,"ed semantic criteria, with roles gouped into five semantic groups Actor, Undergoer, Place, Time, and Circumstance. 3.3. Guidelines for Semantic Annotation We created annotation guidelines for both considered semantic annotation tasks that fulfill general desiderata: a) clear definition of annotation targets, b) instructions about the procedure, and c) clear definition of the annotation labels, including guides for deciding difficult and irregular cases, using guiding questions and examples. We created new role annotation guidelines for German that are assumed to generalize to other languages (Petukhova and Bunt, 2008). The original VerbNet guidelines2 do not provide detailed instructions. The UVI3 web interface with VerbNet roles gives insight into how roles are applied for specific verbs or verb classes, but does not serve as a general guideline. SemAF-SR provides more specific definitions which we refer to in some of the role descriptions. Beyond technical instructions for usage of the annotation tool, the annotation targets, ’predicate’ and ’argument’, have to been defined. Even though we provide preannotated targets, the annotators must be able to verify – and if needed, correct – the proposed targets."
L16-1484,Q15-1034,0,0.0777467,"Missing"
L16-1484,J06-2001,0,0.057914,"Missing"
L16-1484,S12-1001,1,0.837012,"Patient-like A1 roles generalize over syntactic alternations. Their definition is based on Dowty’s proto-roles (Dowty, 1991), capturing salient Agent- and Patient-like properties. Reisinger et al. (2015) attempt a characterization of ProtoRoles as distributions over properties, collected by crowdsourcing. In our study, we aim at a concise role inventory that covers the full range of arguments to a predicate. VerbNet is located between FrameNet and PropBank on a continous scale between a fine-grained interpretable role inventory on one side and a compact, coarse-grained inventory on the other. Silberer and Frank (2012) identified a stronger generalization capacity of VerbNet roles compared to FrameNet roles in the task of non-local role binding. Merlo and van der Plas (2009) found that PropBank roles, being closer to syntax, are easier to assign than VerbNet roles, while the latter provide better semantic generalization. VerbNet defines a set of up to 35 roles, which are defined independently from specific verb senses. Next to VerbNet, the ISO-standard SemAF-SR based on LIRICS (ISO, 2014) proposed a semantic role inventory in close consideration of VerbNet roles. Bonial et al. (2011) and Hwang (2014) presen"
L16-1484,P14-5016,1,0.891987,"Missing"
L16-1484,P98-1013,0,\N,Missing
L16-1484,C98-1013,0,\N,Missing
L18-1217,W07-1430,0,0.0581955,"Missing"
L18-1217,N16-1098,0,0.0475776,"Missing"
L18-1217,P16-1119,0,0.150291,"lasses influence the omissibility of adjectives under truth-conditional aspects. Amoia and Gardent (2008) published a data set where these and other syntactic and semantic properties of adjectives are tested in an RTE (recognizing textual entailment) setting. In this work, the context in which the semantic effects of adjectives are tested is the sentence, and the relevant criterion is preservation of truth when, e.g., deleting the adjective or the head noun as in the following inference pairs: Daisy is a big mouse → Daisy is a mouse or Daisy is a big mouse → Daisy is big. Along similar lines, Stanovsky and Dagan (2016) describe the construction process and resulting dataset of non-restrictive noun phrase modification. Nonrestrictive modifiers – e.g. The speaker thanked president Obama who just came into the room – can be removed to shorten sentences. The dataset gathered and annotated trough crowd sourcing has no restrictions on the length of the modifiers, which often span phrases. The context provided for annotation is a sentence for each modifier. From a conceptual point of view, modifiers were studied with respect to the distortion effect they have on the concept denoted by the head noun (Murphy, 2002)."
L18-1217,P12-1107,0,0.0605082,"Missing"
L18-1217,D17-1062,0,0.016999,"– extracting a sentence from a document, deleting a phrase from a sentence or a sub-phrase from a larger chunk. Vanderwende et al. (2007), Zajic et al. (2007) propose syntax-based trimming, where branches of a syntactic tree are scored using a combination of features that 1 http://www.cl.uni-heidelberg.de/english/ research/downloads/resource_pages/deModify/ deModify_data.shtml 1357 marks them for potential deletion. Wubben et al. (2012) approach the problem of text simplification as a machine translation problem trained on pairs of texts from Wikipedia and SimpleWikipedia. Wang et al. (2016), Zhang and Lapata (2017) reformulate this approach in the form of neural encoder-decoder models. Focusing on the modifiers, modification can be viewed from many different perspectives. From a linguistic point of view, several typologies of modifiers have been proposed (McNally, 2013). Of these, the semantic impact of modifiers is taken into account in: (a) the entailmentbased typology, in which modifiers are grouped into three broad categories based on the inferences they license, which stem from potential interpretations of the extension of modifiers, head nouns and the compounds as sets: intersective modifiers (mal"
N18-1054,E17-2026,0,0.0237471,"rged (He et al., 2017; Yang and Mitchell, 2017; Marcheggiani and Titov, 2017) since we started this work. In future work we can improve our models with such new proposals. Auxiliary tasks for MTL. Other work investigates under which conditions MTL is effective. Mart´ınez Alonso and Plank (2017) show that the best auxiliary tasks have low kurtosis of labels (usually a small label set) and high entropy (labels occur uniformly). We show that the best MTL model for ORL is the model which uses shared layers only. Thus it seems reasonable to consider only a small and uniform SRL label set {A0, A1}. Bingel and Søgaard (2017) show that MTL works when the main task has a flattening learning curve, but the auxiliary task curve is still steep. We notice such behavior in our learning curves. ficulties in labeling targets originate from similar reasons as for holders. Examples 1–3 demonstrate complex syntactic constructions, examples 4–6 MPQA-specific annotations that require inference and example 7 exemplifies a missing target. How does MTL help? There are 18/1055 instances in the dev set for which the FS model predicts the holder correctly and the Z&X-STL model does not, and 19/1055 for targets. For holders, for 9 ou"
N18-1054,P17-1044,0,0.0713983,". However, the gold holder is always the entity from the coreference cluster that is the closest to the opinion.8 The evaluation scripts needs to be extended such that predicting any entity from the coreference cluster is considered to be correct. To conclude, to better evaluate future developments, it would be worth curating MPQA instances with missing roles and extending evaluation to account for coreferent holders and discontinuous roles. The examples in Table 10 demonstrate that dif8 590 We followed the prior work (Katiyar and Cardie, 2016). Neural SRL. New neural SRL models have emerged (He et al., 2017; Yang and Mitchell, 2017; Marcheggiani and Titov, 2017) since we started this work. In future work we can improve our models with such new proposals. Auxiliary tasks for MTL. Other work investigates under which conditions MTL is effective. Mart´ınez Alonso and Plank (2017) show that the best auxiliary tasks have low kurtosis of labels (usually a small label set) and high entropy (labels occur uniformly). We show that the best MTL model for ORL is the model which uses shared layers only. Thus it seems reasonable to consider only a small and uniform SRL label set {A0, A1}. Bingel and Søgaard (2"
N18-1054,C10-3009,0,0.0241927,"Missing"
N18-1054,D14-1080,0,0.0702815,"ns first, to ensure a reproducible evaluation setup on a fixed set of gold opinion expressions. The MTL models we develop in this work will, however, be the basis for the full task in a later stage. Because of these differences, direct comparison to Y&C and K&C is not possible. However, if we compare our results we notice a big gap that demonstrates that opinion expression extraction is the import step in FGOA. Similar to K&C, Liu et al. (2015) jointly labels opinion expressions and their targets in reviews. Some work focuses entirely on labeling of opinion expressions (Yang and Cardie, 2014; Irsoy and Cardie, 2014). Other work looks into specific subcategories of ORL: opinion role induction for verbal predicates (Wiegand and Ruppenhofer, 2015), categorization of opinion words into actor and speaker view (Wiegand et al., 2016b), opinion roles extraction on opinion compounds (Wiegand et al., 2016a). Wiegand and Ruppenhofer (2015) report 72.54 binary F1 score for labeling of holders in MPQA (results for targets are not reported). Acknowledgments This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Source"
N18-1054,W05-0620,0,0.273305,"Missing"
N18-1054,J13-3002,0,0.406436,"Labeling Using Multi-Task Learning With Semantic Role Labeling Ana Marasovi´c and Anette Frank Research Training Group AIPHES Department of Computational Linguistics Heidelberg University {marasovic,frank}@cl.uni-heidelberg.de Abstract holders and targets), the task is usually approached with sequence labeling techniques and the BIO encoding scheme (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). Initially pipeline models were proposed which first predict opinion expressions and then, given an opinion, label its opinion roles, i.e. holders and targets (Kim and Hovy, 2006; Johansson and Moschitti, 2013). Pipeline models have been substituted with so-called joint models that simultaneously identify all opinion entities, and predict which opinion role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). Recently an LSTM-based joint model was proposed (Katiyar and Cardie, 2016) that unlike the prior work (Choi et al., 2006; Yang and Cardie, 2013) does not depend on external resources (such as syntactic parsers or named entity recognizers). The neural variant does not outperform the feature-based CRF model (Yang and Cardie, 2013) in Opinion Role Label"
N18-1054,P17-1110,0,0.0304018,"(Fig. 3, marked red) predicts to which task the current batch of data belongs, based on the representation produced by the shared LSTMs. If the shared LSTMs are taskinvariant, the discriminator should perform badly. Thus, we update the shared parameters to maximize the discriminator’s cross-entropy loss. At the same time we want the discriminator to challenge the shared LSTMs, so we update the discriminator’s parameters to minimize its cross-entropy loss. This minmax optimization is known as adversarial training and recently it gained a lot of attention for NLP applications (Liu et al., 2017; Chen et al., 2017; Kim et al., 2017; Qin et al., 2017; Wu et al., 2017; Gui et al., 2017; Li et al., 2017; Zhang et al., 2017; Joty et al., 2017). 3 3.1 3.3 We evaluate our models using two evaluation settings. First, we follow Katiyar and Cardie (2016) which set aside 132 documents for development and used the remaining 350 documents for 10-fold CV. However, in the 10-fold CV setting, the size of the tests sets is 3 times smaller than the dev set size (Table 2, row 3), and, consequently, results in high-variance estimates on the test sets. Therefore we additionally evaluate our models with 4-fold CV. We set a"
N18-1054,P16-1087,0,0.476404,"repressive and grave human rights violators, and aggressively seeking weapons [...]. In examples (4–5), the same opinion expression concerned realizes different scopes for the target. A model which exploits SRL knowledge could be biased to always label targets as complete SRL role constituents, as in example (5). (4) Rice told us [the administration]H was [concerned]Oneg that [Iraq]T would take advantage of the 9/11 attacks. (5) [The Chinese government]H is deeply [concerned]Oneg about [the sudden deterioration in the Middle East situation]T , Tang said. Regarding incompleteness, prior work (Katiyar and Cardie, 2016) has shown that their model makes reasonable predictions in sentences which do not have annotations at all, e.g. [mothers]H [care]O for [their young]T , in: From the fact that mothers care for their young, we can not deduce that they ought to do so, Hume argued. The examples above show that incorporating SRL knowledge via multi-task learning is a reasonable way to improve ORL, but at the same time they alert us that given the specificities of MPQA and ORL annotations in general, it is not obvious whether MTL can overcome divergences in the annotation schemes of opinion and semantic role labeli"
N18-1054,W06-1651,0,0.171251,"hought the election had been stolen . A0 - A1 A1 A1 A1 A1 A1 A1 A1 A1 A1 A1 - A0 A1 AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV A0 A1 A1 A1 A1 A1 A1 A1 - Table 1: Output of the SRL demo. Table 1 illustrates the output of the SRL demo2 for example (1), following the PropBank SRL scheme (Palmer et al., 2005)3 . SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA. For this reason, the output of SRL systems has been commonly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Additionally, a considerable amount of training data is available for training SRL models (Table 2 in Sec. 3), which made neural SRL models successful (Zhou and Xu, 2015; Yang and Mitchell, 2017). Obstacles. Although SRL is similar in nature to ORL, it cannot solve ORL for all cases (Ruppenhofer et al., 2008). In example (2) holder and target of the predicate please correspond to A1, A0 semantic roles respectively, wheres for the predicate fear in (1) holder and target correspond to A0, A1 respectively. We took into account this observation when deciding on an appropr"
N18-1054,W06-0301,0,0.761878,"roving Opinion Role Labeling Using Multi-Task Learning With Semantic Role Labeling Ana Marasovi´c and Anette Frank Research Training Group AIPHES Department of Computational Linguistics Heidelberg University {marasovic,frank}@cl.uni-heidelberg.de Abstract holders and targets), the task is usually approached with sequence labeling techniques and the BIO encoding scheme (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). Initially pipeline models were proposed which first predict opinion expressions and then, given an opinion, label its opinion roles, i.e. holders and targets (Kim and Hovy, 2006; Johansson and Moschitti, 2013). Pipeline models have been substituted with so-called joint models that simultaneously identify all opinion entities, and predict which opinion role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). Recently an LSTM-based joint model was proposed (Katiyar and Cardie, 2016) that unlike the prior work (Choi et al., 2006; Yang and Cardie, 2013) does not depend on external resources (such as syntactic parsers or named entity recognizers). The neural variant does not outperform the feature-based CRF model (Yang and Car"
N18-1054,P17-1119,0,0.0274769,") predicts to which task the current batch of data belongs, based on the representation produced by the shared LSTMs. If the shared LSTMs are taskinvariant, the discriminator should perform badly. Thus, we update the shared parameters to maximize the discriminator’s cross-entropy loss. At the same time we want the discriminator to challenge the shared LSTMs, so we update the discriminator’s parameters to minimize its cross-entropy loss. This minmax optimization is known as adversarial training and recently it gained a lot of attention for NLP applications (Liu et al., 2017; Chen et al., 2017; Kim et al., 2017; Qin et al., 2017; Wu et al., 2017; Gui et al., 2017; Li et al., 2017; Zhang et al., 2017; Joty et al., 2017). 3 3.1 3.3 We evaluate our models using two evaluation settings. First, we follow Katiyar and Cardie (2016) which set aside 132 documents for development and used the remaining 350 documents for 10-fold CV. However, in the 10-fold CV setting, the size of the tests sets is 3 times smaller than the dev set size (Table 2, row 3), and, consequently, results in high-variance estimates on the test sets. Therefore we additionally evaluate our models with 4-fold CV. We set aside 100 documents"
N18-1054,D17-1230,0,0.0884464,"Missing"
N18-1054,P16-2038,0,0.0707504,"Figure 1: Fully-shared (FS) MTL. Figure 2: HierarchicalMTL (H-MTL). Figure 3: (Adversarial) state-private ((A)SP) MTL. 2 Neural MTL for SRL and ORL other tasks are not updated. This model should be effective for constructions with a clear mapping between opinion and semantic roles such as {H 7→ A0, T 7→ A1} as in example (1) (Sec. 1). Hierarchical MTL (H-MTL) model. For NLP applications, often some given (high-level) task is supposed to benefit from another (low-level) task more than the other way around, e.g. parsing from POS tagging. This intuition lead to designing hierarchical MTL models (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) in which predictions for low-level tasks are not made on the basis of the representation produced at the final LSTM, but on the representation produced by a lower-layer LSTM (Fig. 2). Task-specific layers atop shared layers could potentially give the model more power to distinguish or ignore certain semantic roles. If so, this MTL model is more suitable for examples like (2) and (3) (Sec. 1). Shared-private (SP) MTL model. In the stateprivate model, in addition to the stack of shared LSTMs, each task has a stack of task-specific LSTMs (Liu et al., 2017) (Fig. 3). Repr"
N18-1054,P17-1001,0,0.048228,"Missing"
N18-1054,N16-1094,0,0.231865,"Missing"
N18-1054,D17-1159,0,0.0717766,"Missing"
N18-1054,K15-1022,0,0.0446038,"Missing"
N18-1054,E17-1005,0,0.0228276,"tter evaluate future developments, it would be worth curating MPQA instances with missing roles and extending evaluation to account for coreferent holders and discontinuous roles. The examples in Table 10 demonstrate that dif8 590 We followed the prior work (Katiyar and Cardie, 2016). Neural SRL. New neural SRL models have emerged (He et al., 2017; Yang and Mitchell, 2017; Marcheggiani and Titov, 2017) since we started this work. In future work we can improve our models with such new proposals. Auxiliary tasks for MTL. Other work investigates under which conditions MTL is effective. Mart´ınez Alonso and Plank (2017) show that the best auxiliary tasks have low kurtosis of labels (usually a small label set) and high entropy (labels occur uniformly). We show that the best MTL model for ORL is the model which uses shared layers only. Thus it seems reasonable to consider only a small and uniform SRL label set {A0, A1}. Bingel and Søgaard (2017) show that MTL works when the main task has a flattening learning curve, but the auxiliary task curve is still steep. We notice such behavior in our learning curves. ficulties in labeling targets originate from similar reasons as for holders. Examples 1–3 demonstrate co"
N18-1054,N16-1092,0,0.0451408,"Missing"
N18-1054,J05-1004,0,0.0389011,"t opinion entities (opinions, 1 Examples are drawn from MPQA (Wiebe et al., 2005). 583 Proceedings of NAACL-HLT 2018, pages 583–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics say.01 fear.01 think.01 steal.01 Australia said it feared violence if voters thought the election had been stolen . A0 - A1 A1 A1 A1 A1 A1 A1 A1 A1 A1 A1 - A0 A1 AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV A0 A1 A1 A1 A1 A1 A1 A1 - Table 1: Output of the SRL demo. Table 1 illustrates the output of the SRL demo2 for example (1), following the PropBank SRL scheme (Palmer et al., 2005)3 . SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA. For this reason, the output of SRL systems has been commonly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Additionally, a considerable amount of training data is available for training SRL models (Table 2 in Sec. 3), which made neural SRL models successful (Zhou and Xu, 2015; Yang and Mitchell, 2017). Obstacles. Although SRL is similar in nature to ORL, it cannot solve ORL for al"
N18-1054,P17-1093,0,0.0647233,"Missing"
N18-1054,D17-1187,0,0.068912,"Missing"
N18-1054,P13-1161,0,0.685314,"had been stolen . A0 - A1 A1 A1 A1 A1 A1 A1 A1 A1 A1 A1 - A0 A1 AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV AM-ADV A0 A1 A1 A1 A1 A1 A1 A1 - Table 1: Output of the SRL demo. Table 1 illustrates the output of the SRL demo2 for example (1), following the PropBank SRL scheme (Palmer et al., 2005)3 . SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA. For this reason, the output of SRL systems has been commonly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Additionally, a considerable amount of training data is available for training SRL models (Table 2 in Sec. 3), which made neural SRL models successful (Zhou and Xu, 2015; Yang and Mitchell, 2017). Obstacles. Although SRL is similar in nature to ORL, it cannot solve ORL for all cases (Ruppenhofer et al., 2008). In example (2) holder and target of the predicate please correspond to A1, A0 semantic roles respectively, wheres for the predicate fear in (1) holder and target correspond to A0, A1 respectively. We took into account this observation when deciding on an appropriate MTL model by splitt"
N18-1054,ruppenhofer-etal-2008-finding,0,0.246349,"RL. The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA. For this reason, the output of SRL systems has been commonly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Additionally, a considerable amount of training data is available for training SRL models (Table 2 in Sec. 3), which made neural SRL models successful (Zhou and Xu, 2015; Yang and Mitchell, 2017). Obstacles. Although SRL is similar in nature to ORL, it cannot solve ORL for all cases (Ruppenhofer et al., 2008). In example (2) holder and target of the predicate please correspond to A1, A0 semantic roles respectively, wheres for the predicate fear in (1) holder and target correspond to A0, A1 respectively. We took into account this observation when deciding on an appropriate MTL model by splitting its parameters into shared and task-specific ones (i.e. hard-parameter sharing). repressive and grave human rights violators, and aggressively seeking weapons [...]. In examples (4–5), the same opinion expression concerned realizes different scopes for the target. A model which exploits SRL knowledge could"
N18-1054,Q14-1039,0,0.143467,"cting opinion expressions first, to ensure a reproducible evaluation setup on a fixed set of gold opinion expressions. The MTL models we develop in this work will, however, be the basis for the full task in a later stage. Because of these differences, direct comparison to Y&C and K&C is not possible. However, if we compare our results we notice a big gap that demonstrates that opinion expression extraction is the import step in FGOA. Similar to K&C, Liu et al. (2015) jointly labels opinion expressions and their targets in reviews. Some work focuses entirely on labeling of opinion expressions (Yang and Cardie, 2014; Irsoy and Cardie, 2014). Other work looks into specific subcategories of ORL: opinion role induction for verbal predicates (Wiegand and Ruppenhofer, 2015), categorization of opinion words into actor and speaker view (Wiegand et al., 2016b), opinion roles extraction on opinion compounds (Wiegand et al., 2016a). Wiegand and Ruppenhofer (2015) report 72.54 binary F1 score for labeling of holders in MPQA (results for targets are not reported). Acknowledgments This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information"
N18-1054,D17-1128,0,0.0633456,"ates the output of the SRL demo2 for example (1), following the PropBank SRL scheme (Palmer et al., 2005)3 . SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA. For this reason, the output of SRL systems has been commonly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Additionally, a considerable amount of training data is available for training SRL models (Table 2 in Sec. 3), which made neural SRL models successful (Zhou and Xu, 2015; Yang and Mitchell, 2017). Obstacles. Although SRL is similar in nature to ORL, it cannot solve ORL for all cases (Ruppenhofer et al., 2008). In example (2) holder and target of the predicate please correspond to A1, A0 semantic roles respectively, wheres for the predicate fear in (1) holder and target correspond to A0, A1 respectively. We took into account this observation when deciding on an appropriate MTL model by splitting its parameters into shared and task-specific ones (i.e. hard-parameter sharing). repressive and grave human rights violators, and aggressively seeking weapons [...]. In examples (4–5), the same"
N18-1054,P17-1179,0,0.0704993,"Missing"
N18-1054,P15-1109,0,0.600332,"mo. Table 1 illustrates the output of the SRL demo2 for example (1), following the PropBank SRL scheme (Palmer et al., 2005)3 . SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opinion roles H and T, according to MPQA. For this reason, the output of SRL systems has been commonly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Additionally, a considerable amount of training data is available for training SRL models (Table 2 in Sec. 3), which made neural SRL models successful (Zhou and Xu, 2015; Yang and Mitchell, 2017). Obstacles. Although SRL is similar in nature to ORL, it cannot solve ORL for all cases (Ruppenhofer et al., 2008). In example (2) holder and target of the predicate please correspond to A1, A0 semantic roles respectively, wheres for the predicate fear in (1) holder and target correspond to A0, A1 respectively. We took into account this observation when deciding on an appropriate MTL model by splitting its parameters into shared and task-specific ones (i.e. hard-parameter sharing). repressive and grave human rights violators, and aggressively seeking weapons [...]. I"
N19-1368,D18-1454,0,0.18447,"Missing"
N19-1368,D14-1067,0,0.0263455,"encode selected triples from ConceptNet using attention mechanisms, the latter enriches question and context embeddings by encoding triples as mapped statements extracted from ConceptNet. Concurrently to our work, Bauer et al. (2018) proposed a heuristic method to extract multi-hop paths from ConceptNet for a reading comprehension task. They construct paths starting from concepts appearing in the question to concepts appearing in the context, aiming to emulate multi-hop reasoning. Tamilselvam et al. (2017) use ConceptNet relations for aspect-based sentiment analysis. Similar to our approach, Bordes et al. (2014) make use of knowledge bases to obtain longer paths connecting entities appearing in questions to answers in a QA task. They also provide a richer representation of answers by building subgraphs of entities appearing in answers. In contrast, our work aims to provide information about missing links 3672 Attention Weights cr2k .… …. .… …. crmk Encoding layer Self-attention layer Dense layer Concatenate Layer(o) xcxt Input representation Encoding Encoding Encoding :::: wmcxt xs BILSTMs Encoding w1cxt w2cxt w3cxt xk Sentence char w1s w2s w3s ::: wms Sigmoid layer cr3k ::: ::: ::: Context Common Se"
N19-1368,P16-1032,0,0.0227004,"y available.1 2 Related Work Sentiment Analysis and Beyond. Starting with Pang et al. (2002), sentiment analysis and emotion detection has grown to a wide research field. Researchers have investigated polarity classifica1 https://github.com/debjitpaul/ Multi-Hop-Knowledge-Paths-Human-Needs tion, sentiment and emotion detection and classification (Tang et al., 2015; Yin et al., 2017; Li et al., 2017) on various levels (tokens, phrases, sentences or documents), as well as structured prediction tasks such as the identification of holders and targets (Deng and Wiebe, 2015) or sentiment inference (Choi et al., 2016). Our work goes beyond the analysis of overtly expressed sentiment and aims at identifying goals, desires or needs underlying the expression of sentiment. Li and Hovy (2017) argued that the goals of an opinion holder can be categorized by human needs. There has been work related to goals, desires, wish detection (Goldberg et al., 2009; Rahimtoroghi et al., 2017). Most recently, Ding and Riloff (2018) propose to categorize affective events into physiological needs to explain people’s motivations and desires. Rashkin et al. (2018) published a dataset for tracking emotional reactions and motivati"
N19-1368,D15-1018,0,0.016808,"are indeed relevant. Our code is made publicly available.1 2 Related Work Sentiment Analysis and Beyond. Starting with Pang et al. (2002), sentiment analysis and emotion detection has grown to a wide research field. Researchers have investigated polarity classifica1 https://github.com/debjitpaul/ Multi-Hop-Knowledge-Paths-Human-Needs tion, sentiment and emotion detection and classification (Tang et al., 2015; Yin et al., 2017; Li et al., 2017) on various levels (tokens, phrases, sentences or documents), as well as structured prediction tasks such as the identification of holders and targets (Deng and Wiebe, 2015) or sentiment inference (Choi et al., 2016). Our work goes beyond the analysis of overtly expressed sentiment and aims at identifying goals, desires or needs underlying the expression of sentiment. Li and Hovy (2017) argued that the goals of an opinion holder can be categorized by human needs. There has been work related to goals, desires, wish detection (Goldberg et al., 2009; Rahimtoroghi et al., 2017). Most recently, Ding and Riloff (2018) propose to categorize affective events into physiological needs to explain people’s motivations and desires. Rashkin et al. (2018) published a dataset fo"
N19-1368,N18-1174,0,0.342679,"re has been limited work towards explaining the reasons for the expression of sentiment and emotions in texts (Li and Hovy, 2017). In our work, we aim to go beyond the detection of sentiment, toward explaining sentiments. Such explanations can range from detecting overtly expressed explanations or reasons for sentiments towards specific aspects of, e.g., products or films, as in user reviews to the explanation of the underlying reasons for emotional reactions of characters in a narrative story. The latter requires understanding of stories and modeling the mental state of characters. Recently, Ding and Riloff (2018) proposed to categorize affective events with categories based on human needs, to provide explanations of people’s attitudes towards such events. Given an expression such as I broke my leg, they categorize the reason for the expressed negative sentiment as being related to a need concerning ‘health’. In this paper we focus on the Modelling Naive Psychology of Characters in Simple Commonsense Stories dataset of Rashkin et al. (2018), which contains annotations of a fully-specified chain of motivations and emotional reactions of characters for a collection of narrative stories. The stories are a"
N19-1368,N09-1030,0,0.0442476,"classification (Tang et al., 2015; Yin et al., 2017; Li et al., 2017) on various levels (tokens, phrases, sentences or documents), as well as structured prediction tasks such as the identification of holders and targets (Deng and Wiebe, 2015) or sentiment inference (Choi et al., 2016). Our work goes beyond the analysis of overtly expressed sentiment and aims at identifying goals, desires or needs underlying the expression of sentiment. Li and Hovy (2017) argued that the goals of an opinion holder can be categorized by human needs. There has been work related to goals, desires, wish detection (Goldberg et al., 2009; Rahimtoroghi et al., 2017). Most recently, Ding and Riloff (2018) propose to categorize affective events into physiological needs to explain people’s motivations and desires. Rashkin et al. (2018) published a dataset for tracking emotional reactions and motivations of characters in stories. In this work, we use this dataset to develop a knowledge-enhanced system that ‘explains’ sentiment in terms of human needs. Integrating Structured Knowledge into Neural NLU Systems. Neural models aimed at solving NLU tasks have been shown to profit from the integration of knowledge, using different method"
N19-1368,D16-1057,0,0.0221711,"human needs. Selectively integrating knowledge paths boosts performance and establishes a new state-of-the-art. Our model offers interpretability through the learned attention map over commonsense knowledge paths. Human evaluation highlights the relevance of the encoded knowledge. 1 Introduction Sentiment analysis and emotion detection are essential tasks in human-computer interaction. Due to its broad practical applications, there has been rapid growth in the field of sentiment analysis (Zhang et al., 2018). Although state-of-the-art sentiment analysis can detect the polarity of text units (Hamilton et al., 2016; Socher et al., 2013), there has been limited work towards explaining the reasons for the expression of sentiment and emotions in texts (Li and Hovy, 2017). In our work, we aim to go beyond the detection of sentiment, toward explaining sentiments. Such explanations can range from detecting overtly expressed explanations or reasons for sentiments towards specific aspects of, e.g., products or films, as in user reviews to the explanation of the underlying reasons for emotional reactions of characters in a narrative story. The latter requires understanding of stories and modeling the mental stat"
N19-1368,P82-1020,0,0.670439,"Missing"
N19-1368,P18-1076,1,0.808523,"ain people’s motivations and desires. Rashkin et al. (2018) published a dataset for tracking emotional reactions and motivations of characters in stories. In this work, we use this dataset to develop a knowledge-enhanced system that ‘explains’ sentiment in terms of human needs. Integrating Structured Knowledge into Neural NLU Systems. Neural models aimed at solving NLU tasks have been shown to profit from the integration of knowledge, using different methods: Xu et al. (2017) show that injecting loosely structured knowledge with a recall-gate mechanism is beneficial for conversation modeling; Mihaylov and Frank (2018) and Weissenborn et al. (2017) propose integration of commonsense knowledge for reading comprehension: the former explicitly encode selected triples from ConceptNet using attention mechanisms, the latter enriches question and context embeddings by encoding triples as mapped statements extracted from ConceptNet. Concurrently to our work, Bauer et al. (2018) proposed a heuristic method to extract multi-hop paths from ConceptNet for a reading comprehension task. They construct paths starting from concepts appearing in the question to concepts appearing in the context, aiming to emulate multi-hop"
N19-1368,W02-1011,0,0.0222144,"demonstrate the effectiveness of the extracted knowledge paths and show significant performance improvements over the prior state-of-the-art. (iv) Our model provides interpretability in two ways: by selecting relevant words from the input text and by choosing relevant knowledge paths from the imported knowledge. In both cases, the degree of relevance is indicated via an attention map. (v) A small-scale human evaluation demonstrates that the extracted multi-hop knowledge paths are indeed relevant. Our code is made publicly available.1 2 Related Work Sentiment Analysis and Beyond. Starting with Pang et al. (2002), sentiment analysis and emotion detection has grown to a wide research field. Researchers have investigated polarity classifica1 https://github.com/debjitpaul/ Multi-Hop-Knowledge-Paths-Human-Needs tion, sentiment and emotion detection and classification (Tang et al., 2015; Yin et al., 2017; Li et al., 2017) on various levels (tokens, phrases, sentences or documents), as well as structured prediction tasks such as the identification of holders and targets (Deng and Wiebe, 2015) or sentiment inference (Choi et al., 2016). Our work goes beyond the analysis of overtly expressed sentiment and aim"
N19-1368,D14-1162,0,0.0821548,"otions or goals expressed in the text in view of a human needs category. We thus integrate these knowledge paths into our model, (i) to help the model making correct predictions and (ii) to provide explanations of emotions expressed in the text in view of different human needs categories. For each input, we represent the extracted ranked list of n commonsense knowledge paths p as a list k,i crk,1 , crk,2 , ...., crk,n , where each cr1:l represents a path consisting of concepts and relations, with l the length of the path. We embed all concepts and k,i relations in cr1:l with pretrained GloVe (Pennington et al., 2014) embeddings. Encoding Layer: We use a single-layer BiLSTM to obtain encodings (hk,i ) for each knowledge path h k,i = BiLST M (ek,i 1:n ) (14) where hk represents the output of the BiLSTM for the knowledge path and i its the ranking index. Attention Layer: We use an attention layer, where each encoded commonsense knowledge path interacts with the sentence representation xs ˆ k,i ): to receive attention weights (h e hk,i = (xs hk,i ), e k,i ˆ k,i = P h h N e k,i i=1 h (15) In Eq. 15, we use sigmoid to calculate the attention weights, similar to Eq. 6. However, this time we compute attention to"
N19-1368,N18-1202,0,0.176386,"ach word in context vicxt and sentence vis (cf. (2-5)). A Bi-LSTM Encoder with Attention to Predict Human Needs Our Bi-LSTM encoder takes as input a sentence S consisting of a sequence of tokens, denoted as s and its preceding context w1s , w2s , ...., wns , or w1:n cxt cxt , or w cxt . As Cxt, denoted as w1 , w2cxt , ...., wm 1:m further input we read the name of a story character, which is concatenated to the input sentence. For 1 Embedding Layer: We embed each word from the sentence and the context with a contextualized word representation using character-based word representations (ELMo) (Peters et al., 2018). The embedding of each word wi in the sentence and context is represented as esi and ecxt i , respectively. Encoding Layer: We use a single-layer BiLSTM (Hochreiter and Schmidhuber, 1997) to obtain sentence and context representations hs and hcxt , which we form by concatenating the final states of the forward and backward encoders. asi = ReLU (Wis hsi + bsi ), (2) cxt acxt = ReLU (Wicxt hcxt i i + bi ) (3) vis = Wvs i asi + bsv i (4) cxt vicxt = Wvcxt i acxt i + bv i (5) where, W s , bs , W cxt , bcxt , Wvs , Wvcxt are trainable parameters. We calculate the soft attention weights for both se"
N19-1368,W17-5543,0,0.0701331,"al., 2015; Yin et al., 2017; Li et al., 2017) on various levels (tokens, phrases, sentences or documents), as well as structured prediction tasks such as the identification of holders and targets (Deng and Wiebe, 2015) or sentiment inference (Choi et al., 2016). Our work goes beyond the analysis of overtly expressed sentiment and aims at identifying goals, desires or needs underlying the expression of sentiment. Li and Hovy (2017) argued that the goals of an opinion holder can be categorized by human needs. There has been work related to goals, desires, wish detection (Goldberg et al., 2009; Rahimtoroghi et al., 2017). Most recently, Ding and Riloff (2018) propose to categorize affective events into physiological needs to explain people’s motivations and desires. Rashkin et al. (2018) published a dataset for tracking emotional reactions and motivations of characters in stories. In this work, we use this dataset to develop a knowledge-enhanced system that ‘explains’ sentiment in terms of human needs. Integrating Structured Knowledge into Neural NLU Systems. Neural models aimed at solving NLU tasks have been shown to profit from the integration of knowledge, using different methods: Xu et al. (2017) show tha"
N19-1368,P18-1213,0,0.152364,"ons for emotional reactions of characters in a narrative story. The latter requires understanding of stories and modeling the mental state of characters. Recently, Ding and Riloff (2018) proposed to categorize affective events with categories based on human needs, to provide explanations of people’s attitudes towards such events. Given an expression such as I broke my leg, they categorize the reason for the expressed negative sentiment as being related to a need concerning ‘health’. In this paper we focus on the Modelling Naive Psychology of Characters in Simple Commonsense Stories dataset of Rashkin et al. (2018), which contains annotations of a fully-specified chain of motivations and emotional reactions of characters for a collection of narrative stories. The stories are annotated with labels from multiple theories of psychology (Reiss, 2004; Maslow, 1943; Plutchik, 1980) to provide explanations for the emotional reactions of characters. Similar to Ding and Riloff (2018), we hypothesize that emotional reactions (joy, trust, fear, etc.) of characters can be explained by (dis)satisfaction of their psychological needs. However, predicting categories of human needs that underlie the expression of sentim"
N19-1368,N18-1027,0,0.0215646,"knowledge paths from ConceptNet that connect textual expressions with human need categories. Finally, we extend our model with a gated knowledge integration mechanism to incorporate relevant multi-hop commonsense knowledge paths for predicting human needs. An overview of the model is given in Figure 2. We now describe each component in detail. 3.1 hs = BiLST M (es1:n ); hcxt = BiLST M (ecxt 1:m ) (1) A Self-Attention Layer allows the model to dynamically control how much each token contributes to the sentence and context representation. We use a modified version of self-attention proposed by Rei and Søgaard (2018), where both input representations are passed through a feedforward layer to generate scalar values for each word in context vicxt and sentence vis (cf. (2-5)). A Bi-LSTM Encoder with Attention to Predict Human Needs Our Bi-LSTM encoder takes as input a sentence S consisting of a sequence of tokens, denoted as s and its preceding context w1s , w2s , ...., wns , or w1:n cxt cxt , or w cxt . As Cxt, denoted as w1 , w2cxt , ...., wm 1:m further input we read the name of a story character, which is concatenated to the input sentence. For 1 Embedding Layer: We embed each word from the sentence and"
N19-1368,D13-1170,0,0.00394295,"ly integrating knowledge paths boosts performance and establishes a new state-of-the-art. Our model offers interpretability through the learned attention map over commonsense knowledge paths. Human evaluation highlights the relevance of the encoded knowledge. 1 Introduction Sentiment analysis and emotion detection are essential tasks in human-computer interaction. Due to its broad practical applications, there has been rapid growth in the field of sentiment analysis (Zhang et al., 2018). Although state-of-the-art sentiment analysis can detect the polarity of text units (Hamilton et al., 2016; Socher et al., 2013), there has been limited work towards explaining the reasons for the expression of sentiment and emotions in texts (Li and Hovy, 2017). In our work, we aim to go beyond the detection of sentiment, toward explaining sentiments. Such explanations can range from detecting overtly expressed explanations or reasons for sentiments towards specific aspects of, e.g., products or films, as in user reviews to the explanation of the underlying reasons for emotional reactions of characters in a narrative story. The latter requires understanding of stories and modeling the mental state of characters. Recen"
N19-1368,speer-havasi-2012-representing,0,0.483905,"d will typically be implicit in the text. In contrast, human readers can make use of relevant information from the story and associate it with their knowledge about human interaction, desires and human needs, and thus will be able 3671 Proceedings of NAACL-HLT 2019, pages 3671–3681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics to infer underlying reasons for emotions indicated in the text. In this work, we propose a computational model that aims to categorize human needs of story characters by integrating commonsense knowledge from ConceptNet (Speer and Havasi, 2012). Our model aims to imitate human understanding of a story, by (i) learning to select relevant words from the text, (ii) extracting pieces of knowledge from the commonsense inventory and (iii) associating them with human need categories put forth by psychological theories. Our assumption is that by integrating commonsense knowledge in our model we will be able to overcome the lack of textual evidence in establishing relations between expressed emotions in specific situations and the inferable human needs of story characters. In order to provide such missing associations, we leverage the graph"
N19-1368,I17-1053,0,0.025224,"nborn et al. (2017) propose integration of commonsense knowledge for reading comprehension: the former explicitly encode selected triples from ConceptNet using attention mechanisms, the latter enriches question and context embeddings by encoding triples as mapped statements extracted from ConceptNet. Concurrently to our work, Bauer et al. (2018) proposed a heuristic method to extract multi-hop paths from ConceptNet for a reading comprehension task. They construct paths starting from concepts appearing in the question to concepts appearing in the context, aiming to emulate multi-hop reasoning. Tamilselvam et al. (2017) use ConceptNet relations for aspect-based sentiment analysis. Similar to our approach, Bordes et al. (2014) make use of knowledge bases to obtain longer paths connecting entities appearing in questions to answers in a QA task. They also provide a richer representation of answers by building subgraphs of entities appearing in answers. In contrast, our work aims to provide information about missing links 3672 Attention Weights cr2k .… …. .… …. crmk Encoding layer Self-attention layer Dense layer Concatenate Layer(o) xcxt Input representation Encoding Encoding Encoding :::: wmcxt xs BILSTMs Enco"
N19-1368,D15-1167,0,0.0355656,"dge paths from the imported knowledge. In both cases, the degree of relevance is indicated via an attention map. (v) A small-scale human evaluation demonstrates that the extracted multi-hop knowledge paths are indeed relevant. Our code is made publicly available.1 2 Related Work Sentiment Analysis and Beyond. Starting with Pang et al. (2002), sentiment analysis and emotion detection has grown to a wide research field. Researchers have investigated polarity classifica1 https://github.com/debjitpaul/ Multi-Hop-Knowledge-Paths-Human-Needs tion, sentiment and emotion detection and classification (Tang et al., 2015; Yin et al., 2017; Li et al., 2017) on various levels (tokens, phrases, sentences or documents), as well as structured prediction tasks such as the identification of holders and targets (Deng and Wiebe, 2015) or sentiment inference (Choi et al., 2016). Our work goes beyond the analysis of overtly expressed sentiment and aims at identifying goals, desires or needs underlying the expression of sentiment. Li and Hovy (2017) argued that the goals of an opinion holder can be categorized by human needs. There has been work related to goals, desires, wish detection (Goldberg et al., 2009; Rahimtorog"
N19-1368,D17-1217,0,0.0578036,"Missing"
P02-1056,P01-1019,0,0.0162156,"Missing"
P02-1056,A97-1035,0,0.0548928,"SG feature structures) is available in XML format with hyperlinks to full feature structure representations externally stored in corresponding data files. Fig. 1 gives an overview of the architecture of the WHITEBOARD Annotation Machine (WHAM). Applications feed the WHAM with input texts and a specification describing the components and configuration options requested. The core WHAM engine has an XML markup storage (external “offline” representation), and an internal “online” multi-level annotation chart (index-sequential access). Following the trichotomy of NLP data representation models in (Cunningham et al., 1997), the XML markup contains additive information, while the multi-level chart contains positional and abstraction-based information, e.g., feature structures representing NLP entities in a uniform, linguistically motivated form. Applications and the integrated components access the WHAM results through an object-oriented programming (OOP) interface which is designed as general as possible in order to abstract from component-specific details (but preserving shallow and deep paradigms). The interfaces of the actually integrated components form subclasses of the generic interface. New components ca"
P02-1056,P01-1034,0,0.116035,"Missing"
P02-1056,W97-0802,0,0.0694652,"Missing"
P02-1056,C02-1093,1,\N,Missing
P03-1014,C02-1093,1,0.865241,"of DNLP parsers is the high degree of ambiguity found in large-scale grammars, which can often only be resolved within a larger syntactic domain. Within a hybrid shallow-deep platform one can take advantage of partial knowledge provided by shallow parsers to pre-structure the search space of the deep parser. In this paper, we will thus complement the efforts made on the lexical side by integration at the phrasal level. We will show that this may lead to considerable performance increase for the DNLP component. More specifically, we combine a probabilistic topological field parser for German (Becker and Frank, 2002) with the HPSG parser of (Callmeier, 2000). The HPSG grammar used is the one originally developed by (M¨uller and Kasper, 2000), with significant performance enhancements by B. Crysmann. In Section 2 we discuss the mapping problem involved with syntactic integration of shallow and deep analyses and motivate our choice to combine the HPSG system with a topological parser. Section 3 outlines our basic approach towards syntactic shallow-deep integration. Section 4 introduces various confidence measures, to be used for fine-tuning of phrasal integration. Sections 5 and 6 report on experiments and"
P03-1014,A00-1031,0,0.0371373,"Missing"
P03-1014,C02-1013,0,0.0556235,"Missing"
P03-1014,E03-1052,0,0.0836085,"ng Topological to HPSG Structures While structurally similar, topological trees are not fully isomorphic to HPSG structures. In Figure 1, e.g., the span from the verb ‘h¨atte’ to the end of the sentence forms a constituent in the HPSG analysis, while in the topological tree the same span is dominated by a sequence of categories: LK , MF, RK , NF. Yet, due to its linguistic underpinning, the topological tree can be used to systematically predict key constituents in the corresponding ‘target’ HPSG 2 See Section 6 for comparison to recent work on integrated chunk-based and dependency parsing in (Daum et al., 2003). 3 As, for example, in (Duchier and Debusmann, 2001). analysis. We know, for example, that the span from the fronted verb (LK - VFIN) till the end of its clause CL - V 2 corresponds to an HPSG phrase. Also, the first position that follows this verb, here the leftmost daughter of MF, demarcates the left edge of the traditional VP. Spans of the vorfeld VF and clause categories CL exactly match HPSG constituents. Category CL - V 2 tells us that we need to reckon with a fronted verb in position of its LK daughter, here 3, while in CL - SUBCL we expect a complementiser in the position of LK, and a"
P03-1014,P01-1024,0,0.0200026,"ucturally similar, topological trees are not fully isomorphic to HPSG structures. In Figure 1, e.g., the span from the verb ‘h¨atte’ to the end of the sentence forms a constituent in the HPSG analysis, while in the topological tree the same span is dominated by a sequence of categories: LK , MF, RK , NF. Yet, due to its linguistic underpinning, the topological tree can be used to systematically predict key constituents in the corresponding ‘target’ HPSG 2 See Section 6 for comparison to recent work on integrated chunk-based and dependency parsing in (Daum et al., 2003). 3 As, for example, in (Duchier and Debusmann, 2001). analysis. We know, for example, that the span from the fronted verb (LK - VFIN) till the end of its clause CL - V 2 corresponds to an HPSG phrase. Also, the first position that follows this verb, here the leftmost daughter of MF, demarcates the left edge of the traditional VP. Spans of the vorfeld VF and clause categories CL exactly match HPSG constituents. Category CL - V 2 tells us that we need to reckon with a fronted verb in position of its LK daughter, here 3, while in CL - SUBCL we expect a complementiser in the position of LK, and a finite verb within the right verbal complex RK, whic"
P03-1014,P01-1034,0,0.0213369,"ntroduction One of the strong points of deep processing (DNLP) technology such as HPSG or LFG parsers certainly lies with the high degree of precision as well as detailed linguistic analysis these systems are able to deliver. Although considerable progress has been made in the area of processing speed, DNLP systems still cannot rival shallow and medium depth technologies in terms of throughput and robustness. As a net effect, the impact of deep parsing technology on application-oriented NLP is still fairly limited. With the advent of XML-based hybrid shallowdeep architectures as presented in (Grover and Lascarides, 2001; Crysmann et al., 2002; Uszkoreit, 2002) it has become possible to integrate the added value of deep processing with the performance and robustness of shallow processing. So far, integration has largely focused on the lexical level, to improve upon the most urgent needs in increasing the robustness and coverage of deep parsing systems, namely 1 This work was in part supported by a BMBF grant to the DFKI project WHITEBOARD (FKZ 01 IW 002). lexical coverage. While integration in (Grover and Lascarides, 2001) was still restricted to morphological and PoS information, (Crysmann et al., 2002) exte"
P03-1014,W00-1306,0,0.0415869,"Missing"
P03-1014,W01-1815,0,0.0904078,"Missing"
P03-1014,W03-0802,1,0.800722,"Missing"
P03-1014,P02-1056,1,\N,Missing
P09-4010,W09-2814,1,\N,Missing
P09-4010,A00-2026,0,\N,Missing
P10-1005,J93-2004,0,\N,Missing
P10-1005,W02-1503,0,\N,Missing
P10-1005,C92-2082,0,\N,Missing
P10-1005,P03-1054,0,\N,Missing
P15-2061,P08-1030,1,0.909629,"Introduction Event trigger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-3"
P15-2061,P13-1008,1,0.784185,"s are encoded as a small set of event-independent classification features, based on lexical matches and external resources like WordNet. Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types. Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system. We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system (Li et al., 2013). When evaluated on the ACE-2005 dataset,1 our system outperforms the fully-supervised one (Section 4), even though we don’t utilize any annotated triggers of the test events during the labeling phase, and only The task of event trigger labeling is typically addressed in the standard supervised setting: triggers for each target event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classifier for trigger labeli"
P15-2061,C10-1077,0,0.363517,"ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-2061,P10-1081,0,0.783761,"ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-2061,N07-4013,0,0.0609519,"Missing"
P15-2061,P09-2015,0,0.0541745,"Missing"
P15-2061,P08-1004,0,0.0706965,"Missing"
P15-2061,P07-2005,0,0.0331584,"Missing"
P15-2061,P04-1015,0,0.0269755,"ers mentioned in each event description into a seed list for the event type, which is provided as input to our trigger labeling method. Triggers from the above quoted sentences are hence included in the Meet seed list, shown in Figure 1. As mentioned in the Introduction, our method (Section 3) is based on event-independent features 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi ∈ L ∪ {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi , yi ) is calculated as w · f , where f is the binary feature vector calculated for (xi , yi ), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additiona"
P15-2061,P06-2094,0,0.0914989,"Missing"
P15-2061,N06-1039,0,0.0182505,"Missing"
P15-2061,P05-1047,0,0.0254773,"ing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009)"
P15-2061,C00-2136,0,0.127914,"Missing"
P15-2061,N13-1092,0,0.0152668,"Missing"
P15-2061,W06-0204,0,0.0207608,"rk contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and language"
P15-2061,P11-1113,0,0.0616286,"f identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association for Computational L"
P15-2061,E12-1029,0,0.0449883,"extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009). They addressed a differ"
P15-2061,N12-1015,0,0.0101718,"nt description into a seed list for the event type, which is provided as input to our trigger labeling method. Triggers from the above quoted sentences are hence included in the Meet seed list, shown in Figure 1. As mentioned in the Introduction, our method (Section 3) is based on event-independent features 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi ∈ L ∪ {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi , yi ) is calculated as w · f , where f is the binary feature vector calculated for (xi , yi ), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additionally, each feature is"
P18-1076,D16-1006,0,0.0157558,"but where additional information is needed to predict the answer, which can be retrieved from a knowledge base and added to the context representations explicitly.1 An illustration is given in Figure 1. Such knowledge may be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet (Speer et al., 2017), BabelNet (Navigli and Ponzetto, 2012), Freebase (Tanon et al., 2016) or domain-specific KBs collected with Information Extraction approaches (Fader et al., 2011; Mausam et al., 2012; Bhutani et al., 2016). Thus, we aim to define a neural model that encodes preselected knowledge in a memory, and that learns to include the available knowledge as an enrichment to the context representation. The main difference of our model to prior state-of-the-art is that instead of relying only on document-to-question interaction or discrete features while performing multiple hops over the document, our model (i) attends to relevant selected Introduction Reading comprehension (RC) is a language understanding task similar to question answering, where a system is expected to read a given passage of text and answe"
P18-1076,P17-1147,0,0.141424,"x models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) perform multi-turn reading of the story context and the question, before inferring the correct answer or use features (GA Reader, Dhingra et al. (2017). Performing multiple hops and modeling a deeper relation between question and document was further developed by several models (Seo et al., 2017; Xiong et al., 2016; Wang et al., 2016, 2017; Shen et al., 2016) on another generation of RC datasets, e.g. SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) or TriviaQA (Joshi et al., 2017). Table 1: Characteristics of Children Book Test datasets. CN: Common Nouns, NE: Named Entities. Cells for Train, Dev, Test show overall numbers of examples and average story size in tokens. 3 Related Work Data and Task Description We experiment with knowledge-enhanced clozestyle reading comprehension using the Common Nouns and Named Entities partitions of the Children’s Book Test (CBT) dataset (Hill et al., 2015). In the CBT cloze-style task a system is asked to read a children story context of 20 sentences. The following 21st sentence involves a placeholder token that the system needs to pre"
P18-1076,P16-1223,0,0.0558932,"Missing"
P18-1076,P16-1086,0,0.422249,"ational Linguistics 2.1 external knowledge and (ii) combines this knowledge with the context representation before inferring the answer, in a single hop. This allows the model to explicitly imply knowledge that is not stated in the text, but is relevant for inferring the answer, and that can be found in an external knowledge source. Moreover, by including knowledge explicitly, our model provides evidence and insight about the used knowledge in the RC. Our main contributions are: (i) We develop a method for integrating knowledge in a simple but effective reading comprehension model (AS Reader, Kadlec et al. (2016)) and improve its results significantly whereas other models employ features or multiple hops. (ii) We examine two sources of common knowledge: WordNet (Miller et al., 1990) and ConceptNet (Speer et al., 2017) and show that this type of knowledge is important for answering common nouns questions and also improves slightly the performance for named entities. (iii) We show that knowledge facts can be added directly to the text-only representation, enriching the neural context encoding. (iv) We demonstrate the effectiveness of the injected knowledge by case studies and data statistics in a qualit"
P18-1076,D16-1032,0,0.0725289,"Missing"
P18-1076,P17-1055,0,0.252956,"cument-to-question interaction or discrete features while performing multiple hops over the document, our model (i) attends to relevant selected Introduction Reading comprehension (RC) is a language understanding task similar to question answering, where a system is expected to read a given passage of text and answer questions about it. Cloze-style reading comprehension is a task setting where the question is formed by replacing a token in a sentence of the story with a placeholder (left part of Figure 1). In contrast to many previous complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) that perform multi-turn reading of a story and a question before inferring the correct answer, we aim to tackle the cloze-style RC task in a way that resembles how humans solve it: using, in addition, background knowledge. We develop 1 ‘Context representation’ refers to a vector representation computed from textual information only (i.e., document (story) or question). 821 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 821–832 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associati"
P18-1076,D17-1086,0,0.0254641,"s in the document and the question with the words from the supporting knowledge that share the same lemma. In contrast to the implicit knowledge transfer of Weissenborn et al. (2017), our explicit attention over external knowledge facts can deliver insights about the used knowledge and how it interacts with specific context tokens (see Section 6). rectly into the LSTM cell state to improve event and entity extraction. They used knowledge embeddings trained on WordNet (Miller et al., 1990) and NELL (Mitchell et al., 2015) using the BILINEAR (Yang et al., 2014) model. Work similar to ours is by Long et al. (2017), who have introduced a new task of Rare Entity Prediction. The task is to read a paragraph from WikiLinks (Singh et al., 2012) and to fill a blank field in place of a missing entity. Each missing entity is characterized with a short description derived from Freebase, and the system needs to choose one from a set of pre-selected candidates to fill the field. While the task is superficially similar to cloze-style reading comprehension, it differs considerably: first, when considering the text without the externally provided entity information, it is clearly ambiguous. In fact, the task is more"
P18-1076,P17-1168,0,0.273767,"of relying only on document-to-question interaction or discrete features while performing multiple hops over the document, our model (i) attends to relevant selected Introduction Reading comprehension (RC) is a language understanding task similar to question answering, where a system is expected to read a given passage of text and answer questions about it. Cloze-style reading comprehension is a task setting where the question is formed by replacing a token in a sentence of the story with a placeholder (left part of Figure 1). In contrast to many previous complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) that perform multi-turn reading of a story and a question before inferring the correct answer, we aim to tackle the cloze-style RC task in a way that resembles how humans solve it: using, in addition, background knowledge. We develop 1 ‘Context representation’ refers to a vector representation computed from textual information only (i.e., document (story) or question). 821 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 821–832 c Melbourne, Australia, July 15 - 20, 20"
P18-1076,D12-1048,0,0.0418312,"the document (story), but where additional information is needed to predict the answer, which can be retrieved from a knowledge base and added to the context representations explicitly.1 An illustration is given in Figure 1. Such knowledge may be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet (Speer et al., 2017), BabelNet (Navigli and Ponzetto, 2012), Freebase (Tanon et al., 2016) or domain-specific KBs collected with Information Extraction approaches (Fader et al., 2011; Mausam et al., 2012; Bhutani et al., 2016). Thus, we aim to define a neural model that encodes preselected knowledge in a memory, and that learns to include the available knowledge as an enrichment to the context representation. The main difference of our model to prior state-of-the-art is that instead of relying only on document-to-question interaction or discrete features while performing multiple hops over the document, our model (i) attends to relevant selected Introduction Reading comprehension (RC) is a language understanding task similar to question answering, where a system is expected to read a given pa"
P18-1076,D11-1142,0,0.0337761,"rs from is given in the document (story), but where additional information is needed to predict the answer, which can be retrieved from a knowledge base and added to the context representations explicitly.1 An illustration is given in Figure 1. Such knowledge may be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet (Speer et al., 2017), BabelNet (Navigli and Ponzetto, 2012), Freebase (Tanon et al., 2016) or domain-specific KBs collected with Information Extraction approaches (Fader et al., 2011; Mausam et al., 2012; Bhutani et al., 2016). Thus, we aim to define a neural model that encodes preselected knowledge in a memory, and that learns to include the available knowledge as an enrichment to the context representation. The main difference of our model to prior state-of-the-art is that instead of relying only on document-to-question interaction or discrete features while performing multiple hops over the document, our model (i) attends to relevant selected Introduction Reading comprehension (RC) is a language understanding task similar to question answering, where a system is expect"
P18-1076,D16-1147,0,0.0465496,"the plain tokens; (ii) we preserve the triple directionality; (iii) we use the relation type as a way of filtering the subject information to initialize the object. P (ai |q, d) = sof tmax( X αiensemble ) (12) j W1 Att(rqctx , cctx dj ) αiensemble j = +W2 Att(rqctx , cctx+kn ) dj +W3 Att(rqctx+kn , cctx dj ) (13) +W4 Att(rqctx+kn , cctx+kn ) dj Querying the Knowledge Memory. To enrich the context representation of the document and question tokens with the facts collected in the knowledge memory, we select a single sum of weighted fact representations for each token using Key-Value retrieval (Miller et al., 2016). In our k(ey) subj obj model the key Mi can be either flast or flast v(alue) obj and the value Mi is flast . For each context-encoded token cctx si (s = d, q; i the token index) we attend over all knowledge , where j is an index pointer from the list of indices that point to the candidate ai token occurrences in the document context representation ctx(+kn) cd . W1..4 are scalar weights initialized with 1.0 and optimized during training.4 We propose the combination of ctx and ctx + kn attentions because our task does not provide supervision whether the knowledge is needed or not. 3 The 0 in w0"
P18-1076,P17-1019,0,0.0466819,"Missing"
P18-1076,D16-1241,0,0.0552299,"ation type word. Ex. /r/IsUsedFor. 4 An example for learned W1..4 is (2.13, 1.41, 1.49, 1.84) in setting (CBT CN, CN5Sel, Subj-Obj as k-v, 50 facts). 824 Train Dev Test Vocab CN 120,769 / 470 2,000 / 448 2,500 / 461 53,185 NE 108,719 / 433 2,000 / 412 2,500 / 424 53,063 4 Cloze-Style Reading Comprehension. Following the original MCTest (Richardson et al., 2013) dataset multiple-choice version of cloze-style RC) recently several large-scale, automatically generated datasets for cloze-style reading comprehension gained a lot of attention, among others the ‘CNN/Daily Mail’ (Hermann et al., 2015; Onishi et al., 2016) and the Children’s Book Test (CBTest) data set (Hill et al., 2015). Early work introduced simple but good single turn models (Hermann et al., 2015; Kadlec et al., 2016; Chen et al., 2016), that read the document once with the question representation ‘in mind’ and select an answer from a given set of candidates. More complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) perform multi-turn reading of the story context and the question, before inferring the correct answer or use features (GA Reader, Dhingra et al. (2017). Perf"
P18-1076,D14-1162,0,0.083215,"s with different knowledge sources, for CBT-CN (Full model, 50 facts). # facts Dev Test 50 71.85 67.64 100 71.35 67.44 200 71.40 68.12 500 71.20 67.24 CN Dev Test 68.20 64.80 70.85 66.32 70.80 66.32 71.20 67.96 71.85 67.64 70.65 67.12 70.75 67.00 71.75 66.32 70.80 66.80 Table 4: Results for different combinations of interactions between document (D) and question (Q) context (ctx) and context + knowledge (ctx+kn) representations. (CN5Sel, 50 facts) Table 3: Results for CBT (CN) with different numbers of facts. (Full model, CN5Sel) Hyper-parameters. For our experiments we use pre-trained Glove (Pennington et al., 2014) embeddings, BiGRU with hidden size 256, batch size of 64 and learning reate of 0.001 as they were shown (Kadlec et al., 2016) to perform good on the AS Reader. 5.2 NE Dev Test 75.50 70.30 76.45 69.68 77.10 69.72 75.65 70.88 76.80 70.24 75.95 70.24 76.20 69.80 76.55 70.52 76.05 70.84 Key/Value Subj/Obj Obj/Obj NE Dev Test 76.65 71.52 76.70 71.28 CN Dev Test 71.85 67.64 71.25 67.48 Table 5: Results for key-value knowledge retrieval and integration. (CN5Sel, 50 facts). Subj/Obj means: we attend over the fact subject (Key) and take the weighted fact object as value (Value). Empirical Results We p"
P18-1076,W17-2623,0,0.029547,"given set of candidates. More complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) perform multi-turn reading of the story context and the question, before inferring the correct answer or use features (GA Reader, Dhingra et al. (2017). Performing multiple hops and modeling a deeper relation between question and document was further developed by several models (Seo et al., 2017; Xiong et al., 2016; Wang et al., 2016, 2017; Shen et al., 2016) on another generation of RC datasets, e.g. SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) or TriviaQA (Joshi et al., 2017). Table 1: Characteristics of Children Book Test datasets. CN: Common Nouns, NE: Named Entities. Cells for Train, Dev, Test show overall numbers of examples and average story size in tokens. 3 Related Work Data and Task Description We experiment with knowledge-enhanced clozestyle reading comprehension using the Common Nouns and Named Entities partitions of the Children’s Book Test (CBT) dataset (Hill et al., 2015). In the CBT cloze-style task a system is asked to read a children story context of 20 sentences. The following 21st sentence involves a placeholder t"
P18-1076,D16-1264,0,0.506237,"mind’ and select an answer from a given set of candidates. More complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) perform multi-turn reading of the story context and the question, before inferring the correct answer or use features (GA Reader, Dhingra et al. (2017). Performing multiple hops and modeling a deeper relation between question and document was further developed by several models (Seo et al., 2017; Xiong et al., 2016; Wang et al., 2016, 2017; Shen et al., 2016) on another generation of RC datasets, e.g. SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) or TriviaQA (Joshi et al., 2017). Table 1: Characteristics of Children Book Test datasets. CN: Common Nouns, NE: Named Entities. Cells for Train, Dev, Test show overall numbers of examples and average story size in tokens. 3 Related Work Data and Task Description We experiment with knowledge-enhanced clozestyle reading comprehension using the Common Nouns and Named Entities partitions of the Children’s Book Test (CBT) dataset (Hill et al., 2015). In the CBT cloze-style task a system is asked to read a children story context of 20 sentences. The following 21st"
P18-1076,D16-1013,0,0.033075,"Missing"
P18-1076,D13-1020,0,0.0845984,"lized with 1.0 and optimized during training.4 We propose the combination of ctx and ctx + kn attentions because our task does not provide supervision whether the knowledge is needed or not. 3 The 0 in w0rel indicates that we encode the relation as a single relation type word. Ex. /r/IsUsedFor. 4 An example for learned W1..4 is (2.13, 1.41, 1.49, 1.84) in setting (CBT CN, CN5Sel, Subj-Obj as k-v, 50 facts). 824 Train Dev Test Vocab CN 120,769 / 470 2,000 / 448 2,500 / 461 53,185 NE 108,719 / 433 2,000 / 412 2,500 / 424 53,063 4 Cloze-Style Reading Comprehension. Following the original MCTest (Richardson et al., 2013) dataset multiple-choice version of cloze-style RC) recently several large-scale, automatically generated datasets for cloze-style reading comprehension gained a lot of attention, among others the ‘CNN/Daily Mail’ (Hermann et al., 2015; Onishi et al., 2016) and the Children’s Book Test (CBTest) data set (Hill et al., 2015). Early work introduced simple but good single turn models (Hermann et al., 2015; Kadlec et al., 2016; Chen et al., 2016), that read the document once with the question representation ‘in mind’ and select an answer from a given set of candidates. More complex models (Weston e"
P18-1076,P17-1018,0,0.0736666,"Missing"
P18-1076,P17-1132,0,0.0954926,"Missing"
P18-1076,D17-1197,0,0.0636793,"Missing"
P98-1056,P92-1005,0,0.0295005,"assigned an underspecified truth-conditional interpretation (Genabith and Crouch, 1997). 3 Appendix B gives a relational formulation of the correspondence between f-structures and UDRSs. The UDRS representations are processed by semantic-based transfer. The resulting system is bi-directional. Consider again the simple head switching case discussed in (1) and (3) above. (4) shows the corresponding UDRSs. The structural mismatch between the two fstructures has disappeared on the level of UDRS representations and transfer is facilitated. 4 3A similar corespondence between f-structures and QLFs (Alshawi and Crouch, 1992) has been shown in (Genabith and Crouch, 1996). 4In the implementation, a Neo-Davidsonian style en343 4.1 Embedded Head-Switching The syntactic transfer rules (2) are supplemented by (5). The complex rule for gerne in (5) overrides 5 (2d) and the COMP rule in (5). For each additional level of embedding triggered by head switching adjuncts a special rule is needed. (5) { vermuten(E) } &lt;-> { suspect(E) }. Ede(X) } &lt;-> (Ede(X) }. •[ COMP(E,X) } &lt;-> { COMP(E,X) }. { gerne(X),ADJN(E,X),COMP(E1,E) } # (SUBJ(E,Y) } &lt;-> { like(X),XCOMP(X,E),SUBJ(X,Y),COMP(EI,X) }. By contrast, on the level of UDRSs he"
P98-1056,C96-1045,1,0.879182,"Missing"
P98-1056,P97-1052,1,0.871464,"Missing"
P98-1056,J91-2001,0,0.427559,"ransfer This section presents a simple bidirectional translation between LFG f-structures and term representations which serve as input to and output of a transfer component developed within the Verbmobil project (Dorna and Emele, 1996a). The term representation is inspired by earlier work (Kay et al., 1994; Caspari and Schmid, 1994) which uses terms as a quasisemantic representation for transfer and generation. The translation between f-structures and terms is based on the correspondence between directed graphs representing f-structures and the functional interpretation of these graphs (cf. (Johnson, 1991)). Given an arc labeled f which connects two nodes nl and n2 in a graph, the same can be expressed by a function f ( n l ) = n2. An f-structure is the set of such feature equations describing the associated graph. Instead of feature equations f ( n l ) -- n2 we use the relational notation f ( n l , n2). Using this idea f-structures can be converted into sets of terms and vice versa} F-structure 1For motivation why we prefer term representations 342 PRED features and their ""semantic form"" values are given special treatment. Instead of introducing PRED terms we build unary relations with the sem"
P98-1056,E93-1024,0,0.0510858,"h and Technology (BMBF) in the framework of the Verbmobil project under grant 01 IV 701 N3. 341 Correspondence-based transfer on f-structures has been proposed in (Kaplan et al., 1989). A closer look at translation problems involving structural mismatches between languages in particular head switching phenomena (Sadler and Thompson, 1991) - led to the contention that transfer is facilitated at the level of semantic representation, where structural differences between languages are often neutralized. Structural misalignment is treated in semantics construction involving a restriction operator (Kaplan and Wedekind, 1993) where f-structures are related to (possibly sets of) disambiguated semantic representations. Given the high potential of semantic ambiguities, the advantage of defining transfer on semantic representations could well be counterbalanced by the overhead generated by multiple disambiguated structures as input to transfer. This and the observation that many semantic (and syntactic) ambiguities can be preserved when translating into a target language that is ambiguous in similar ways, sheds light on the issue of the properties of representations for the task of defining transfer. In principle, the"
P98-1056,E89-1037,0,0.177318,"eed to be sensible representations for both parsing and generation. LFG f-structures are abstract, ""high-level"" syntactic representations which go some way towards meeting these often irreconcilable requirements. "" We would like to thank H. Kamp, M. Schiehlen and the anonymous reviewers for helpful comments on earlier versions of this article. Part of this work was funded by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the Verbmobil project under grant 01 IV 701 N3. 341 Correspondence-based transfer on f-structures has been proposed in (Kaplan et al., 1989). A closer look at translation problems involving structural mismatches between languages in particular head switching phenomena (Sadler and Thompson, 1991) - led to the contention that transfer is facilitated at the level of semantic representation, where structural differences between languages are often neutralized. Structural misalignment is treated in semantics construction involving a restriction operator (Kaplan and Wedekind, 1993) where f-structures are related to (possibly sets of) disambiguated semantic representations. Given the high potential of semantic ambiguities, the advantage"
P98-1056,J93-4001,0,0.268167,"Missing"
P98-1056,C96-1054,1,\N,Missing
P98-1056,C96-1024,0,\N,Missing
P98-1056,P98-1060,1,\N,Missing
P98-1056,C98-1058,1,\N,Missing
reiter-etal-2010-using,N03-1033,0,\N,Missing
reiter-etal-2010-using,P03-1054,0,\N,Missing
reiter-etal-2010-using,P09-1068,0,\N,Missing
reiter-etal-2010-using,P09-2054,0,\N,Missing
reiter-etal-2010-using,W09-2417,0,\N,Missing
reiter-etal-2010-using,P05-1045,0,\N,Missing
S12-1001,C10-1017,0,0.0192096,"ssical SRL vs. CR features to clarify the nature of this special phenomenon. iii. We automatically acquire heuristically labeled data to address the sparse data problem. i. An entity-mention model for anaphoric role resolution. In our model implicit roles that are discourse-bound (i.e. classified as DNI) are treated as anaphoric, similar to zero anaphora: the implicit role will be bound to a discourse antecedent. In line with recent research in CR, we adopt an entity-mention model, where an entity is represented by all mentions pertaining to a coreference chain (see i.a. Rahman and Ng (2011), Cai and Strube (2010)). Our model is based on binary classifier decisions that take as input the anaphoric role and an entity candidate from the preceding discourse. The final classification of a role linking to an entity is obtained by discriminative ranking of the binary classifiers’ probability estimates. Details on the system architecture are given in Section 3.2. ii. SRL vs. CR: Analysis of feature sets. The linking of implicit semantic roles represents an interesting mixture of SRL and CR that displays exceptional characteristics of both types of phenomena. In contrast to classical SRL, the relation between"
S12-1001,W11-1907,0,0.0432846,"Missing"
S12-1001,S10-1059,0,0.610183,"Missing"
S12-1001,P11-1144,0,0.0214276,"ictions, e.g., a valid candidate must not already fill another role of the active frame. #ent avg avg #frames #frame#DNI #DNI #ent/doc size types types SemEval 141 141 9 1,370 317 245 155 ONotes 7899 23 3 12,770 258 2,220 270 ACE-2 3564 11 4 58,204 757 4,265 578 MUC-6 1841 15 3 20,140 654 997 310 corpus coref semantic roles ONotes manual manual PB CoNLL05, ported to FN ACE-2 manual automatic FN (Semafor) MUC-6 manual automatic FN (Semafor) Table 1: SemEval vs. heuristically acquired data mapped to their FrameNet (FN) counterparts, if existent. For the ACE-2 and MUC-6 corpora, we used Semafor (Das and Smith, 2011) for automatic annotation with FN semantic roles. From these data sets we acquired heuristically annotated instances of role linking using the strategy explained in 3.1. Table 1 summarizes the resulting training data. The heuristically labeled data extends the manually labeled DNI instances by an order of magnitude. 4.3 Model parameters Entity sets Edni . For definition of the set of candidate entities to consider for DNI linking, Edni , we determined different parameter settings with restrictions on the types, distances and prominence of candidate antecedents. For instance, unlike in noun phr"
S12-1001,P10-1160,0,0.527349,"Missing"
S12-1001,N06-2015,0,0.039915,"systems of the SemEval NI-only task. The SemEval task is based on fiction stories by A. C. Doyle, one story as training data and another two chapters as test set, enriched with coreference and FrameNet-style frame annotations. Information about the training section is found in Table 1. The test data comprise 710 NIs (349 DNIs, 361 INIs), of which 259 DNIs are linked. 4.2 Heuristic data acquisition Since the training data has a critically small amount of linked DNIs, we heuristically labeled training data on the basis of data sets with manually annotated coreference information: OntoNotes 3.0 (Hovy et al., 2006), as well as ACE-2 (Mitchell et al., 2003) and MUC-6 (Chinchor and Sundheim, 2003). OntoNotes 3.0 was merged with gold SRL annotations from the CoNLL-2005 shared task. By means of SemLink-1.1 (Loper et al., 2007) and a mapping included in the SemEval data, these PropBank (PB, Palmer et al. (2005)) annotations were 1 We additionally impose several restrictions, e.g., a valid candidate must not already fill another role of the active frame. #ent avg avg #frames #frame#DNI #DNI #ent/doc size types types SemEval 141 141 9 1,370 317 245 155 ONotes 7899 23 3 12,770 258 2,220 270 ACE-2 3564 11 4 58,2"
S12-1001,P11-1081,0,0.00915565,".3 (baseline: 28.9). 3 3.1 Casting Implicit Role Linking as an Anaphora Resolution Task Implicit role = anaphora resolution Recent models for role binding mainly draw on techniques from SRL, enriched with concepts from CR. In this paper, we explicitly formulate implicit role linking as an anaphora resolution task. This is in line with the predominant conception in early work, and also highlights the close relationship with zero anaphora (Kameyama, 1985). Computational treatments of zero anaphora (e.g., Imamura et al. (2009)) are in fact employing techniques well-known from SRL. Recent work by Iida and Poesio (2011), by contrast, offers an analysis of zero anaphora in a CR architecture. Further support comes from psycholinguistic studies in Garrod and Terras (2000), who establish commonalities between implicit role reference and other types of anaphora resolution. The contributions of our work are as follows: i. We cast implicit role binding as a CR task, using an entity-mention model and discriminative classification for antecedent selection. ii. We examine the effectiveness of model features for classical SRL vs. CR features to clarify the nature of this special phenomenon. iii. We automatically acquir"
S12-1001,P09-2022,0,0.0279203,"ll as coreference patterns acquired from large corpora. This model achieves an F-score of 50.3 (baseline: 28.9). 3 3.1 Casting Implicit Role Linking as an Anaphora Resolution Task Implicit role = anaphora resolution Recent models for role binding mainly draw on techniques from SRL, enriched with concepts from CR. In this paper, we explicitly formulate implicit role linking as an anaphora resolution task. This is in line with the predominant conception in early work, and also highlights the close relationship with zero anaphora (Kameyama, 1985). Computational treatments of zero anaphora (e.g., Imamura et al. (2009)) are in fact employing techniques well-known from SRL. Recent work by Iida and Poesio (2011), by contrast, offers an analysis of zero anaphora in a CR architecture. Further support comes from psycholinguistic studies in Garrod and Terras (2000), who establish commonalities between implicit role reference and other types of anaphora resolution. The contributions of our work are as follows: i. We cast implicit role binding as a CR task, using an entity-mention model and discriminative classification for antecedent selection. ii. We examine the effectiveness of model features for classical SRL v"
S12-1001,P02-1014,0,0.0385168,"tops when the correct antecedent, i.e. a positive instance, as well as at least one negative instance have been found.1 (3) Classification. From the acquired training instances we learn a binary classifier that predicts for an instance instej ,dk whether it is positive, i.e. entity ej is a correct antecedent for DNI dk . Further, the classifier provides a probability estimate for instej ,dk being positive. We obtain classifications for all instances in Ik . Among the positive classified instances, we select the antecedent e with the highest estimate. That is, we apply the best-first strategy (Ng and Cardie, 2002). In case of a tie, we choose the antecedent which is closer to the target. If no instance is classified as positive, dk is left unfilled. 4 Data and Experiments 4.1 SEMEVAL 2010 task and data set We adhere to the SemEval 2010 task by Ruppenhofer et al. (2009) as test bed for our experiments. The main focus of our work is on part (iii), the identification of antecedents for DNIs. Subtasks (i) and (ii), the recognition and interpretation of NIs will be only tackled to enable comparison to the participating systems of the SemEval NI-only task. The SemEval task is based on fiction stories by A. C"
S12-1001,P86-1004,0,0.830213,"it roles that can be discoursebound to an antecedent as in (1), roles can be interpreted existentially, as in (2), with an unfilled TEXT role of the R EADING frame that cannot be anchored in prior discourse. The FrameNet paradigm (Fillmore et al., 2003) that was used for annotation in the SemEval task classifies these interpretation differences as definite (DNI) vs. indefinite (INI) null instantiations (NI) of roles, respectively. 2 Implicit Role Reference: A Short History Early studies. The phenomenon of implicit role reference is not new. It has been studied in a number of early approaches. Palmer et al. (1986) treated unfilled semantic roles as special cases of anaphora and coreference resolution (CR). Resolution was guided by domain knowledge encoded in a knowledgebased system. Similarly, Whittemore et al. (1991) analyzed the resolution of unexpressed event roles as a special case of CR. A formalization in DRT was fully worked out, but automation was not addressed. Later studies emphasize the role of implicit role reference in a frame-semantic discourse analysis. Fillmore and Baker (2001) provide an analysis of 1 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 1–10, c M"
S12-1001,J05-1004,0,0.13662,"data comprise 710 NIs (349 DNIs, 361 INIs), of which 259 DNIs are linked. 4.2 Heuristic data acquisition Since the training data has a critically small amount of linked DNIs, we heuristically labeled training data on the basis of data sets with manually annotated coreference information: OntoNotes 3.0 (Hovy et al., 2006), as well as ACE-2 (Mitchell et al., 2003) and MUC-6 (Chinchor and Sundheim, 2003). OntoNotes 3.0 was merged with gold SRL annotations from the CoNLL-2005 shared task. By means of SemLink-1.1 (Loper et al., 2007) and a mapping included in the SemEval data, these PropBank (PB, Palmer et al. (2005)) annotations were 1 We additionally impose several restrictions, e.g., a valid candidate must not already fill another role of the active frame. #ent avg avg #frames #frame#DNI #DNI #ent/doc size types types SemEval 141 141 9 1,370 317 245 155 ONotes 7899 23 3 12,770 258 2,220 270 ACE-2 3564 11 4 58,204 757 4,265 578 MUC-6 1841 15 3 20,140 654 997 310 corpus coref semantic roles ONotes manual manual PB CoNLL05, ported to FN ACE-2 manual automatic FN (Semafor) MUC-6 manual automatic FN (Semafor) Table 1: SemEval vs. heuristically acquired data mapped to their FrameNet (FN) counterparts, if exi"
S12-1001,W09-2417,0,0.208292,"tion of unfilled semantic roles of predicates in discourse interpretation. Such roles, while linguistically unexpressed, can often be anaphorically bound to antecedent referents in the discourse context. Capturing such implicit semantic roles and linking them to their antecedents is a challenging problem. But it bears immense potential for establishing discourse coherence and for getting closer to the aim of true NLU. Linking of implicit semantic roles in discourse has recently been introduced as a shared task in the SemEval 2010 competition Linking Events and Their Participants in Discourse (Ruppenhofer et al., 2009, 2010). The task consists in detecting unfilled semantic roles of events and determining antecedents in the discourse context that these roles ∗ The work reported in this paper is based on a Master’s Thesis conducted at Heidelberg University (Silberer, 2011). In contrast to implicit roles that can be discoursebound to an antecedent as in (1), roles can be interpreted existentially, as in (2), with an unfilled TEXT role of the R EADING frame that cannot be anchored in prior discourse. The FrameNet paradigm (Fillmore et al., 2003) that was used for annotation in the SemEval task classifies thes"
S12-1001,S10-1008,0,0.496561,"(2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. The WordNet classes over which the preferences are defined are WordNet lexicographer’s files (supersenses). The selectional association values Λ(dni, ss) of the DNI’s selectional preferences are retrieved for the supersense ss of each candidate antecedent’s head. As for Feat. 1, we define a candidate’s feature value by its rank in the ordered list of these Λs. 4.5 Experiments Evaluation measures. We adopt the precision (P), recall (R) and F1 measures in Ruppenhofer et al. (2010). A true positive is a DNI which has been linked to the correct entity as given by the gold data. Classifiers and feature selection. For DNI linking, we use BayesNet (Cooper and Herskovits, 1992) as classifier, implemented in Weka (Witten and Frank, 2000).6 For each parameter combination, we perform feature selection by means of leave-oneout 10-fold cross-validation on the SemEval training data with successively removing/determining the best features. The resulting models Mi are then evaluated on the SemEval test data in different setups: Exp1: Linking DNIs. Exp1 evaluates our models on the DN"
S12-1001,R11-1046,0,0.538372,"Missing"
S12-1001,S10-1065,0,0.658572,"Missing"
S12-1001,W11-0908,0,0.557294,".40 1.21 13.0 6.0 8 P 6.0 9.2 7.0 5.9 6.9 Table 6: Exp2 results obtained for our models (lines 1-5) and comparable systems (lines 6-8). Column 5 gives the score for correctly recognized NIs. Cols. 6 and 7 report precision for correctly interpreted NIs on the basis of the correctly recognized (relative) vs. all gold NIs to be recognized (absolute). The scores in the last column (F1 (crf)) were obtained with gold CR annotations. task participants10 (lines 7-8) shows that our models clearly outperform these systems – with a gain of +5.7 and +8.89 points in F1 -score in DNI linking.11 Compared to Tonelli and Delmonte (2011) (T&D), M1 has a higher F1 -score in linking of +2.1 points. In contrast to our method, their linking approach is (admittedly) heavily lexicalized and strongly tailored to the domain of the used data. 6 Conclusion We cast the problem of linking implicit semantic roles as a special case of (zero) anaphora resolution, drawing on insights from earlier work and parallels observed with zero anaphora. Our results strongly support this analysis: (i) Feature selection clearly determines CR-related features as strongest support for DNI linking. (ii) Our models beat a strong baseline using a prominence"
S12-1001,P91-1003,0,0.881702,"se. The FrameNet paradigm (Fillmore et al., 2003) that was used for annotation in the SemEval task classifies these interpretation differences as definite (DNI) vs. indefinite (INI) null instantiations (NI) of roles, respectively. 2 Implicit Role Reference: A Short History Early studies. The phenomenon of implicit role reference is not new. It has been studied in a number of early approaches. Palmer et al. (1986) treated unfilled semantic roles as special cases of anaphora and coreference resolution (CR). Resolution was guided by domain knowledge encoded in a knowledgebased system. Similarly, Whittemore et al. (1991) analyzed the resolution of unexpressed event roles as a special case of CR. A formalization in DRT was fully worked out, but automation was not addressed. Later studies emphasize the role of implicit role reference in a frame-semantic discourse analysis. Fillmore and Baker (2001) provide an analysis of 1 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 1–10, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics a newspaper text that indicates the importance of frames and roles in establishing discourse coherence. Burchardt et al. (2005)"
S12-1001,C04-1033,0,0.0132708,"sider for DNI linking. For each DNI dk to be linked, a subset of candidates Ek ⊂ E is chosen as candidate search space for resolving dk . We experiment with different strategies for constructing Ek (cf. Section 4). (2) Instance Creation. The next step consists in the creation of (training) instances for classification including the extraction of features for all instances. An instance instej ,dk consists of the active DNI dk , its frame and a candidate entity ej ∈ Ek . Instance creation follows an entity-based adaption of the standard procedure of Soon et al. (2001), which has been applied by Yang et al. (2004, 2008). Processing the discourse from left to right, for each DNI dk , instances Ik are created by processing Ek from right to left according to each entity’s most recent mention, starting with the entity closest to dk . Note that, as entities instead of mentions are considered, only one instance is created for an entity which is mentioned several times in the search space. 4 In training, the instance creation stops when the correct antecedent, i.e. a positive instance, as well as at least one negative instance have been found.1 (3) Classification. From the acquired training instances we lear"
S12-1001,P08-1096,0,0.0254532,"Missing"
S12-1001,N10-1138,0,\N,Missing
S12-1001,H86-1011,0,\N,Missing
S12-1001,P07-1028,0,\N,Missing
S12-1001,J12-4003,0,\N,Missing
S12-1001,J01-4004,0,\N,Missing
S12-1030,J08-1001,0,0.0161348,"lly or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is modeled by the observation of sentence-to-sentence realization patterns of individual entities. The learned model reflects a key idea from Centering Theory, namely that adjacent sentences in a coherent discourse are likely to involve the same entities. One shortcoming of Barzilay and Lapata’s model (and extensions of it) is that it only investigates overt realization patterns in terms of grammatical functions. These functions reflect explicit realizations of predicate argument structures (PAS), but they do not capture the full range of salience factors."
S12-1030,N04-1015,0,0.0429036,"rse. We henceforth refer to such cases as non-realized arguments. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants. As a first step towards this overall goal, we describe the construction of a resource that contains more than 160,000 document pairs that are known to talk about the sa"
S12-1030,P01-1008,0,0.103415,"e potential benefits of the created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that hav"
S12-1030,W09-2816,0,0.0612606,"n and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a coherent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ruppenhofer et al., 2010), which aims at linking events and their particip"
S12-1030,C10-3009,0,0.0564182,"Missing"
S12-1030,C10-1011,0,0.0228425,"tal of 9.8 million newswire articles from seven distinct sources. For construction of our corpus we make use of all combinations of agency pairs in Gigaword. 3.1 Corpus Creation In order to extract pairs of articles describing the same news event, we implemented the pairwise similarity method presented by Wubben et al. (2009). The method is based on measuring word overlap in news headlines, weighting each word by its TF*IDF score to give a higher impact to words occurring with lower frequency. As our focus is to provide 220 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2 . We chose this constraint as lo"
S12-1030,P09-1092,0,0.0209954,"ra that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a coherent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ru"
S12-1030,J08-4005,0,0.142242,"baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently compute the F1 -score as the harmonic mean between precision and recall. Table 2 presents the results for our model and the two baselines. From all four approaches, WordAlign performs worst. We identify two main reasons for this: On the one hand, the paraphrase detection does not perform perfectly. Hen"
S12-1030,I05-5002,0,0.0603853,"cluding paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identi"
S12-1030,P07-1041,0,0.054786,"xist a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a coherent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work i"
S12-1030,J95-2003,0,0.521808,"investigation of discourse coherence phenomena. Initial experiments on the task of predicting predicate alignments across text pairs show promising results. Our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks. 1 Introduction Research in the fields of discourse and pragmatics has led to a number of theories that try to explain and formalize the effect of discourse coherence inducing elements either locally or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is m"
S12-1030,2005.mtsummit-papers.11,0,0.0413496,"ted tasks. Section 3 introduces the new task together with a description of how we prepared a suitable data set. Section 4 discusses the potential benefits of the created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse co"
S12-1030,N06-1014,0,0.0742809,"ation of four different similarity measures. Subsequently, we also tune the weighting scheme for similarity measures on the development set. We found the best performing combination of weights to be 0.09, 0.19, 0.48 and 0.24 for simWN , simVN , simDist and simArgs , respectively. Baselines. A simple baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently comp"
S12-1030,J05-1004,0,0.00661811,"rder to extract pairs of articles describing the same news event, we implemented the pairwise similarity method presented by Wubben et al. (2009). The method is based on measuring word overlap in news headlines, weighting each word by its TF*IDF score to give a higher impact to words occurring with lower frequency. As our focus is to provide 220 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2 . We chose this constraint as longer text pairs contain a high number of unrelated predicates, making this task difficult to manage for the annotators. Sure and possible links. Following standard practice in word alignmen"
S12-1030,S10-1008,0,0.286229,"9) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ruppenhofer et al., 2010), which aims at linking events and their participants in discourse. The provided data sets for this task, however, are critically small (438 train and 525 test sentences). Eventually, the corpus we present in this paper could also be beneficial for data-driven approaches to role linking in discourse. 3 A Corpus for Aligning Predications across Comparable Texts a high-quality data set for predicate alignment and follow-up tasks, we impose an additional date constraint to favor precision over recall. We apply this constraint by requiring a pair of articles to be published within a two-day time f"
S12-1030,P10-1098,0,0.0570476,"ts. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants. As a first step towards this overall goal, we describe the construction of a resource that contains more than 160,000 document pairs that are known to talk about the same events and participants. Example (1), extracted from our corpus"
S12-1030,U06-1019,0,0.0377495,"to be 0.09, 0.19, 0.48 and 0.24 for simWN , simVN , simDist and simArgs , respectively. Baselines. A simple baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently compute the F1 -score as the harmonic mean between precision and recall. Table 2 presents the results for our model and the two baselines. From all four approaches, WordAlign performs worst. We i"
S12-1030,W09-0621,0,0.0313421,"Missing"
S13-1043,P05-1018,0,0.0218793,"SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun m"
S13-1043,J08-1001,0,0.0283184,"y treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity m"
S13-1043,P11-2040,0,0.01551,"tically by only considering PAS with identical predicate form and constituent order. We found that this restriction constrains affected arguments to be modifiers, prepositional phrases and direct objects. We argue that this is actually a desirable property because more complicated alternations could affect coherence by themselves; resulting interplays would make it difficult to distinguish between the isolated effect of argument realization itself and other effects, triggered for example by sentence order (Gordon et al., 1993). 5.2 Annotation We set up a web experiment using the NLTK package (Belz and Kow, 2011) to collect (local) coherence ratings for implicit and explicit arguments. For this experiment, we compiled a data set of 150 document pairs. As described in Section 5.1, each text pair consists of mostly the same text, with the only difference being one argument realization. We presented all 150 pairs to two annotators7 and asked them to indicate their preference for one alternative over the other using a continuous slider scale. The annotators got to see the full texts, with the alternatives presented next to each other. To make texts easier to read and differences easier to spot, we collaps"
S13-1043,C10-3009,0,0.04757,"Missing"
S13-1043,C10-1011,0,0.0896888,"Missing"
S13-1043,E09-1018,0,0.125959,"originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set"
S13-1043,S10-1059,0,0.300227,"Missing"
S13-1043,P08-2011,0,0.153737,"but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We"
S13-1043,P11-1118,0,0.0841378,"ons (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mo"
S13-1043,P11-2022,0,0.0792339,"ons (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mo"
S13-1043,W07-2321,0,0.024618,"th manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 198"
S13-1043,J12-4003,0,0.062769,"reasons for this are the scarcity of annotated data, and the inherent difficulty of inferring discourse antecedents automatically. In this paper, we propose to induce implicit arguments and discourse antecedents by exploiting complementary (explicit) information obtained from monolingual comparable texts (Section 3). We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5). We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6). 2 task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their e"
S13-1043,W13-0111,0,0.642617,"Missing"
S13-1043,J95-2003,0,0.684005,"Missing"
S13-1043,W13-0114,0,0.107611,"Missing"
S13-1043,D12-1045,0,0.0299107,"set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been released by Roth and Frank (2012a).1 This data 1 cf. http://www.cl.uni-heidelberg.de/%7Emroth/ Sentence that comprises a PAS with an (correctly predict"
S13-1043,J13-4004,0,0.0192441,"t induced antecedent The [∅A0 ] [operatingA3 ] loss, as measured by . . . widened to 189 million euros . . . It was handed over to Mozambican control . . . 33 years after [∅A0 ] independence. . . . [local officials A0 ] failed to immediately report [the accident A1 ] [∅A2 ] . . . T-Online[’s] Mozambique[’s] [to] the government Table 1: Three positive examples of automatically induced implicit argument and antecedent pairs. Cross-document coreference. We apply crossdocument coreference resolution to induce antecedents for implicit arguments. In practice, we use the Stanford Coreference System (Lee et al., 2013) and run it on pairs of texts by simply providing a single document as input, comprising of a concatenation of the two texts. To perform this step with high precision, we only use the most precise resolution sieves: “String Match”, “Relaxed String Match”, “Precise Constructs”, “Strict Head Match [A-C]”, and “Proper Head Noun Match”. Figure 1: Illustration of the induction approach: texts consist of PAS (represented by overlapping circles); we exploit alignments between corresponding predicates across texts (marked by solid lines) and co-referring entities (marked by dotted lines) to infer impl"
S13-1043,N10-1043,0,0.0210972,"ession and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reflected by the non-realization of an entity. Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example (1) shows a text fragment, in which argument realization is necessary in the first sentence but redundant in the second. Implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing. One reason for this lies in the scarce amount of annotated data sets available. We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and"
S13-1043,W12-4511,0,0.0428612,"Missing"
S13-1043,P09-1025,0,0.0180409,"lts showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit a"
S13-1043,W13-0211,1,0.806288,"Missing"
S13-1043,P86-1004,0,0.716489,"uments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references. 1 (1) Introduction Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning (Palmer et al., 2010). Information relevant to these structures, however, can be non-local in natural language texts (Palmer et al., 1986; Fillmore, 1986, inter alia). In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse. In a coherent discourse, each utterance focuses on a salient set of entities, also called “foci” (Sidner, 1979) or “centers” (Joshi and Kuhn, 1979). According to the theory of Centering (Grosz El Salvador is now the only Latin American country which still has troops in [Iraq]. Nicaragua, Honduras and the Dominican Republic have withdrawn their troops [∅]. From a semantic processing perspective, a human reader can easily infer that “Iraq”, the mar"
S13-1043,D08-1020,0,0.126358,"ns as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been applied in coherence evaluation and readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008), and story generation (McIntyre and Lapata, 2009). Based on the original model, a few extensions have been proposed: for example, Filippova and Strube (2007) and Elsner and Charniak (2011b) suggested additional features to characterize semantic relatedness between entities and features specific to single entities, respectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring refe"
S13-1043,S12-1030,1,0.826033,"have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been r"
S13-1043,D12-1016,1,0.850005,"have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities across the texts using coreference resolution techniques on concatenated document pairs (Lee et al., 2012). These preprocessing steps are described in more detail in Section 3.1. Given the preprocessed comparable texts and aligned PAS, we propose to heuristically identify implicit arguments and link them to their antecedents via the cross-document coreference chains. We describe the details of this approach in Section 3.2. 3.1 Data preparation The starting point for our approach is the data set of automatically aligned predicate pairs that has been r"
S13-1043,S10-1008,0,0.158797,"spectively. Other entity-based approaches to coherence modeling include the pronoun model by Charniak and Elsner (2009) and the discourse-new model by Elsner and Charniak (2008). All of these approaches are, however, based on explicitly realized entity mentions only, ignoring references that are inferrable. The role of implicit arguments has been studied early on in the context of semantic processing (Fillmore, 1986; Palmer et al., 1986). Yet, the phenomenon has mostly been ignored in semantic role labeling. First data sets, focusing on implicit arguments, have only recently become available: Ruppenhofer et al. (2010) organized a SemEval shared 307 The aim of this work is to automatically construct a data set of implicit arguments and their discourse antecedents. We propose an induction approach that exploits complementary information obtained from pairs of comparable texts. As a basis for this approach, we rely on several preparatory steps proposed in the literature that first identify information two documents have in common (cf. Figure 1). In particular, we align corresponding predicateargument structures (PAS) using graph-based clustering (Roth and Frank, 2012b). We then determine co-referring entities"
S13-1043,S12-1001,1,0.832791,"data set and outline directions for future work (Section 6). 2 task on “linking events and participants in discourse”, Gerber and Chai (2012) made available implicit argument annotations for the NomBank corpus (Meyers et al., 2008) and Moor et al. (2013) provide annotations for parts of the OntoNotes corpus (Weischedel et al., 2011). However, these resources are very limited: The annotations by Moor et al. and Gerber and Chai are restricted to 5 and 10 predicate types, respectively. The training set of the SemEval task contains only 245 resolved implicit arguments in total. As pointed out by Silberer and Frank (2012), additional training data can be heuristically created by treating anaphoric mentions as implicit arguments. Their experimental results showed that artificial training data can indeed improve results, but only when obtained from corpora with manual semantic role annotations (on the sentence level) and gold coreference chains. 3 Identifying and linking implicit arguments Related work The most prominent approach to entity-based coherence modeling nowadays is the entity grid model by Barzilay and Lapata (2005). It has originally been proposed for automatic sentence ordering but has also been app"
S13-1043,S10-1065,0,0.305685,"Missing"
S13-1043,W11-0908,0,0.242624,"mple of an original and modified (marked by an asterik) sentence: (2) [The Dalai Lama’sA0 ] visit [to FranceA1 ] ends on Tuesday. * [The Dalai Lama’sA0 ] visit ends on Tuesday. Note that adding and removing arguments at random can lead to structures that are semantically implausible. Hence, we restrict this procedure to predicate-argument structures (PAS) that actually occur and are aligned across two texts, and create modifications by replacing a single argument position in one text with the corresponding argument position in the comparable text. Examples (2) and (3) 5 Results as reported in Tonelli and Delmonte (2011) Results computed as an average over the scores given for both test files; rounded towards the number given for the test file that contained more instances. 6 311 show two such comparable texts. The original PAS in Example (2) contains an explicit argument that is implicit in the aligned PAS and hence removed in the modified version. Vice versa, the original text in (3) involves an implicit argument, which is made explicit in the modified version. (3) [The Dalai Lama’sA0 ] visit coincides with the Beijing Olympics. * [The Dalai Lama’sA0 ] visit [to FranceA1 ] coincides with the Beijing Olympic"
S13-1043,C00-2137,0,0.084284,"Missing"
S13-1043,H86-1011,0,\N,Missing
S16-2005,P15-1039,0,0.0200959,"e predicate.2 Arguments are attached to their nearest predicate and cannot be attached to more than one, as might occur in cases of embedded clauses. Cross-lingual Annotation Projection Aside from English, resources for SRL only exist for a select number of languages. For the languages that have such resources, annotated data still tends to vastly underrepresent the variability and breadth of coverage that exists for English. To extend SRL to new languages without reliance on manual annotation, models for role transference have been developed under both the supervised (Pad´o and Lapata, 2009; Akbik et al., 2015) and unsupervised (Kozhevnikov and Titov, 2013) setting. Most relevant to our work are previous studies that address the problem of projecting semantic role annotations across parallel corpora. To transfer semantic annotations across languages, Pad´o and Lapata (2009) score the constituents of word-aligned parallel sentences and project role labels for the arguments that achieve highest constituent alignment scores. Akbik et al (2015) use filtered projection by constraining alignments through lexical and syntactic filters to ensure accuracy in predicate and argument role projection. Complete p"
S16-2005,P13-1104,0,0.0671251,"Missing"
S16-2005,P02-1033,0,0.127235,"Missing"
S16-2005,S15-1005,0,0.0433289,"Missing"
S16-2005,P10-1160,0,0.0205784,"(3), the role farmers is dropped entirely in the aligned German sentence: (3) 2 2.1 Related Work Implicit Semantic Role Labeling Previous resources for implicit SRL were developed over diverging schemas, texts, and predicate types. An initial dataset was constructed in the SemEval-2010 Shared Task “Linking Events and Their Participants in Discourse”, under the FrameNet paradigm; authors annotated short stories with implicit arguments and their antecedents, resulting in approx. 500 resolvable and 700 nonresolvable implicit roles out of roughly 3,000 frame instances (Ruppenhofer et al., 2010). Gerber and Chai (2010) focused on the implicit arguments of a constrained set of 10 nominal predicates in the NomBank scheme, annotating 966 implicit role instances for these specific predicates. The only change is that [farmers] are not required to produce. Die einzige Neuerung ist, dass nicht gefordert The only change is, that not required wird zu produzieren. are to produce. Numerous studies on the recovery of implicit roles have concluded that a lack of training data has been the stopping point towards improvements on the implicit role labeling task (Gorinski et al., 2013; Laparra and Rigau, 2013). To address t"
S16-2005,W13-0111,0,0.0295057,"instances (Ruppenhofer et al., 2010). Gerber and Chai (2010) focused on the implicit arguments of a constrained set of 10 nominal predicates in the NomBank scheme, annotating 966 implicit role instances for these specific predicates. The only change is that [farmers] are not required to produce. Die einzige Neuerung ist, dass nicht gefordert The only change is, that not required wird zu produzieren. are to produce. Numerous studies on the recovery of implicit roles have concluded that a lack of training data has been the stopping point towards improvements on the implicit role labeling task (Gorinski et al., 2013; Laparra and Rigau, 2013). To address this problem, Silberer and Frank (2012) generated artificial training data by removing arguments from coreference chains and showed that adding such instances yields performance gains. However, their quality was low and later work (Roth and Frank, 2015) has shown that smaller numbers of naturally occurring training data performed better. Roth and Frank (2015) applied a graph-based method for automatically acquiring high-quality data for non-local SRL using comparable monolingual corpora. They detect implicit semantic roles across documents and their antec"
S16-2005,2005.mtsummit-papers.11,0,0.0652772,"ere asked to judge whether the marked argument is a correct semantic role for the English predicate. The second subTable 1: Features investigated for classification, where Xling are cross-lingual features. ear kernel, Decision Tree, and Gradient Boosting, under the framework of the Scikit-learn library (Pedregosa et al., 2011). 4 Constructing a Dataset for Classifying Implicit Arguments This section presents the construction of our experimental dataset for implicit role detection. 4.1 Annotation of Poorly Aligned Arguments Corpora and Tools We conduct our experiments over the Europarl corpus (Koehn, 2005), which contains over 1.9 million aligned sentences in our target languages. Anticipating noise in the automatic word alignments, we first take sentences from manually word-aligned German-English Europarl data (Pad´o and Lapata, 2005) to conduct our initial experiments. These sentences give us an upper bound for the number of implicit roles we should expect to obtain. Automatic word alignments are generated with GIZA++ (Och and Ney, 2003). Predicates and their arguments are first detected 49 After a review of these difficult cases, annotation guidelines were modified and annotators were re-tra"
S16-2005,W09-2417,0,0.0607831,"Missing"
S16-2005,P13-1117,0,0.0845181,"not correspond to any referent in the text at all. In the examples below, the argument for the predicate withdrawn in (1) is resolvable while the implicit argument for reading in (2) is not: Introduction Understanding events and their participants is a core NLP task, and SRL is the standard approach for identification and labeling of these events in text. SRL systems (T¨ackstr¨om et al., 2015; Roth and Woodsend, 2014) have benefited NLP applications, and many approaches have been proposed to transfer semantic roles from English to other languages without further reliance on manual annotation (Kozhevnikov and Titov, 2013; Pad´o and Lapata, 2009). However, event structures – both predicates and their arguments – are known to shift in the translation process, and this poor correspondence presents a bottleneck for the transference of semantic roles across languages. In some cases, the semantic content of an entire argument can be missing from the scope of its translated predicate. Arguments that are omitted are often treated as noise in state-of-the-art projection models; however, our work views them as a valuable source of (1) El Salvador is now the only Latin American country which still has troops in [Iraq]1"
S16-2005,P13-1116,0,0.0191084,"et al., 2010). Gerber and Chai (2010) focused on the implicit arguments of a constrained set of 10 nominal predicates in the NomBank scheme, annotating 966 implicit role instances for these specific predicates. The only change is that [farmers] are not required to produce. Die einzige Neuerung ist, dass nicht gefordert The only change is, that not required wird zu produzieren. are to produce. Numerous studies on the recovery of implicit roles have concluded that a lack of training data has been the stopping point towards improvements on the implicit role labeling task (Gorinski et al., 2013; Laparra and Rigau, 2013). To address this problem, Silberer and Frank (2012) generated artificial training data by removing arguments from coreference chains and showed that adding such instances yields performance gains. However, their quality was low and later work (Roth and Frank, 2015) has shown that smaller numbers of naturally occurring training data performed better. Roth and Frank (2015) applied a graph-based method for automatically acquiring high-quality data for non-local SRL using comparable monolingual corpora. They detect implicit semantic roles across documents and their antecedents from the prior cont"
S16-2005,P14-1130,0,0.0253636,"n implicit role in German. Our dataset, described in Section 4.2, consists of instances of poorly aligned roles that have been annotated as either implicit, not implicit, Classifiers We experimented with three classifiers, a Support Vector Machine (SVM) with a lin48 TYPE FEATURE Argument Lemma through dependency parses on English and German parallel corpora. Parses are generated for English with ClearNLP (Choi and McCallum, 2013). German sentences are run through the MarMot morphological analyzer (Mueller et al., 2013), and dependency parses for German are then generated using the RBG Parser (Lei et al., 2014). The Universal Dependencies project facilitates crosslingual consistency in parsing and provides better compatibility amongst multiple languages. We trained the RBG Parser with the Universal Dependencies tagset (Rosa et al., 2014), and thus our argument detection can be applied to other languages in the Universal Dependencies project. XLING POS Grammatical relation to predicate Distance to predicate Number of dependents Syntactic path to predicate Alignment type of neighboring argument (aligned, unaligned) Predicate + Lemma POS Total number of arguments Sentential Number of aligned arguments"
S16-2005,S12-1001,1,0.842599,"he implicit arguments of a constrained set of 10 nominal predicates in the NomBank scheme, annotating 966 implicit role instances for these specific predicates. The only change is that [farmers] are not required to produce. Die einzige Neuerung ist, dass nicht gefordert The only change is, that not required wird zu produzieren. are to produce. Numerous studies on the recovery of implicit roles have concluded that a lack of training data has been the stopping point towards improvements on the implicit role labeling task (Gorinski et al., 2013; Laparra and Rigau, 2013). To address this problem, Silberer and Frank (2012) generated artificial training data by removing arguments from coreference chains and showed that adding such instances yields performance gains. However, their quality was low and later work (Roth and Frank, 2015) has shown that smaller numbers of naturally occurring training data performed better. Roth and Frank (2015) applied a graph-based method for automatically acquiring high-quality data for non-local SRL using comparable monolingual corpora. They detect implicit semantic roles across documents and their antecedents from the prior context, again following cross-document links. In contra"
S16-2005,P09-1033,0,0.068748,"Missing"
S16-2005,P14-2120,0,0.0170244,"y was low and later work (Roth and Frank, 2015) has shown that smaller numbers of naturally occurring training data performed better. Roth and Frank (2015) applied a graph-based method for automatically acquiring high-quality data for non-local SRL using comparable monolingual corpora. They detect implicit semantic roles across documents and their antecedents from the prior context, again following cross-document links. In contrast, our work does not rely on semantic resources (SRL and lexical ontologies), but builds on parallel corpora enriched with dependencies and word alignments. Finally, Stern and Dagan (2014) generate training data for implicit SRL from textual entailment data sets. However, this type of resource needs to be manually curated. The challenge in detecting implicit roles across languages is that these omissions represent only a fraction of the kinds of poor alignments that can occur. In fact, different types of translational shifts may occur that do not constitute cases of implicit role omission. Such factors include: change in part-of-speech from a verbal predicate to a noun or adjective, light verb constructions, single predicates that are expressed as both a verb and complement in"
S16-2005,D13-1032,0,0.0369534,"Missing"
S16-2005,Q15-1003,0,0.0580296,"Missing"
S16-2005,J03-1002,0,0.005195,"experimental dataset for implicit role detection. 4.1 Annotation of Poorly Aligned Arguments Corpora and Tools We conduct our experiments over the Europarl corpus (Koehn, 2005), which contains over 1.9 million aligned sentences in our target languages. Anticipating noise in the automatic word alignments, we first take sentences from manually word-aligned German-English Europarl data (Pad´o and Lapata, 2005) to conduct our initial experiments. These sentences give us an upper bound for the number of implicit roles we should expect to obtain. Automatic word alignments are generated with GIZA++ (Och and Ney, 2003). Predicates and their arguments are first detected 49 After a review of these difficult cases, annotation guidelines were modified and annotators were re-trained. Context - 2 preceding English sentences —— The only change is that [farmers] are not –required– to produce . Die einzige Neuerung ist , dass nicht –gefordert– wird zu produzieren . Annotation Quality Inter-annotator agreement was measured by Cohen’s Kappa scores over 114 instances, and the entire 700 candidates were then completed by Annotator 1. One of the authors adjudicated for agreement. Results are given in Table 2 where “Role"
S16-2005,rosa-etal-2014-hamledt,0,0.0416104,"Missing"
S16-2005,J15-4003,1,0.752727,"duce. Die einzige Neuerung ist, dass nicht gefordert The only change is, that not required wird zu produzieren. are to produce. Numerous studies on the recovery of implicit roles have concluded that a lack of training data has been the stopping point towards improvements on the implicit role labeling task (Gorinski et al., 2013; Laparra and Rigau, 2013). To address this problem, Silberer and Frank (2012) generated artificial training data by removing arguments from coreference chains and showed that adding such instances yields performance gains. However, their quality was low and later work (Roth and Frank, 2015) has shown that smaller numbers of naturally occurring training data performed better. Roth and Frank (2015) applied a graph-based method for automatically acquiring high-quality data for non-local SRL using comparable monolingual corpora. They detect implicit semantic roles across documents and their antecedents from the prior context, again following cross-document links. In contrast, our work does not rely on semantic resources (SRL and lexical ontologies), but builds on parallel corpora enriched with dependencies and word alignments. Finally, Stern and Dagan (2014) generate training data f"
S16-2005,D14-1045,0,0.0233987,"cal scope of the predicate. One complicating factor is that these implicit arguments can either be found in the context, and thereby are recoverable, or they could be existentially interpreted and might not correspond to any referent in the text at all. In the examples below, the argument for the predicate withdrawn in (1) is resolvable while the implicit argument for reading in (2) is not: Introduction Understanding events and their participants is a core NLP task, and SRL is the standard approach for identification and labeling of these events in text. SRL systems (T¨ackstr¨om et al., 2015; Roth and Woodsend, 2014) have benefited NLP applications, and many approaches have been proposed to transfer semantic roles from English to other languages without further reliance on manual annotation (Kozhevnikov and Titov, 2013; Pad´o and Lapata, 2009). However, event structures – both predicates and their arguments – are known to shift in the translation process, and this poor correspondence presents a bottleneck for the transference of semantic roles across languages. In some cases, the semantic content of an entire argument can be missing from the scope of its translated predicate. Arguments that are omitted ar"
S16-2005,S10-1008,0,\N,Missing
S17-1027,P15-1123,0,0.0608045,"re easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We present a novel take on modeling and exploiting genre information and test it on"
S17-1027,W16-2803,1,0.91818,"the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4,"
S17-1027,P10-2013,0,0.0559985,"Missing"
S17-1027,P14-1062,0,0.0646515,"Missing"
S17-1027,D14-1181,0,0.0271982,"deling” ♠ Heidelberg University, Department of Computational Linguistics ♣ University of North Texas, Department of Linguistics {mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de alexis.palmer@unt.edu Abstract Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings (Mikolov et al., 2013b). Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the indivi"
S17-1027,J92-4003,0,0.43155,"Missing"
S17-1027,N16-1030,0,0.0618447,"state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidd"
S17-1027,W14-4012,0,0.0847139,"Missing"
S17-1027,P16-1004,0,0.18652,"successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model. A strong motivation for developing NN-b"
S17-1027,loaiciga-etal-2014-english,0,0.0424883,"Missing"
S17-1027,W14-4921,1,0.86387,"Missing"
S17-1027,J93-2004,0,0.060196,"Missing"
S17-1027,P16-1166,1,0.352268,"GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model. A strong motivation for developing NN-based systems is that they can be transferred with low cost to other languages without major feature engineering or use of hand-crafted linguistic knowledge resources. Given the highly-engineered feature sets used for SE classification so far (Friedrich et al., 2016), porting such classifiers to other languages is a non-trivial issue. We test the portab"
S17-1027,W15-2702,1,0.892449,"show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We present a novel take on modeling and exploiting genre information and test it on the English multi-genre corpus of Friedrich et al. (2016). distribution of SE types in text passages and discourse modes, e.g., narrative, informative, or argumentative (Palmer and Friedrich, 2014; Mavridou et al., 2015; Becker et al., 2016a). Our aims and contributions are: (i) We study the performance of GRU-based models enhanced with attention for modeling local and non-local characteristics of semantic clause types. (ii) We compare the effectiveness of the learned attention weights as features for a sequence labeling system to the explicitly defined syntactic-semantic features in (Friedrich et al., 2016). (iii) We define extensions of our models that integrate external knowledge about genre and show that this can be used to improve classification performance across genres. (iv) We test the portability of"
S17-1027,W15-1603,1,0.839673,"ing both attention and genre information leads to a 2.13 pp increase over the model that uses only attention. Adding context information beyond the local clause – a window of up to three previous clauses – improves the wordbased attention models slightly, but a wider window (four or more clauses) causes a major drop Baseline systems. The feature-based system of Palmer07 (Palmer et al., 2007) (Palmer07 in Table 2) simulates context through predicted labels from previous clauses. Friedrich et al. (2016) (Fried16 in Table 2) report results for their CRF-based SE 7 The Wiki texts were selected by Friedrich et al. (2015) precisely in order to target G ENERIC S ENTENCE clauses. 8 The cross validation splits of the data used by Friedrich et al. (2016) are not available. 235 GRU + att + gLab (1) GRU + att + gLab (2) GRU + att + gLab (3) GRU + att + gLab (4) GRU + att + gLab (5) GRU + att + gLab + genre (1) GRU + att + gLab + genre (2) GRU + att + gLab + genre (3) GRU + att + gLab + genre (4) GRU + att + gLab + genre (5) Acc 72.71 72.68 72.66 72.61 73.40 73.44 73.45 72.84 73.12 73.34 F1 65.37 66.51 65.03 64.33 66.39 66.76 66.51 66.29 66.21 66.13 Table 4: SE-type classification on English test set, sequence oracle"
S17-1027,N03-1030,0,0.361054,"Missing"
S17-1027,N13-1090,0,0.12279,"ling Context and Genre Characteristics with Recurrent Neural Networks and Attention Maria Becker♦♠ , Michael Staniek♦♠ , Vivi Nastase♦♠ , Alexis Palmer♣ , Anette Frank♦♠ ♦ Leibniz ScienceCampus “Empirical Linguistics and Computational Language Modeling” ♠ Heidelberg University, Department of Computational Linguistics ♣ University of North Texas, Department of Linguistics {mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de alexis.palmer@unt.edu Abstract Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings (Mikolov et al., 2013b). Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initia"
S17-1027,P15-1150,0,0.094046,"Missing"
S17-1027,P16-1105,0,0.0374456,"with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidden state. Attention has been used successfully e.g. in aspect-based sentiment classification (Wang et al., 2016), for modeling relations between words or phrases in encoder-decoder models for translation (Bahdanau et al., 2015), or bi-clausal classification tasks such as textu"
S17-1027,N16-1065,0,0.0546958,"Missing"
S17-1027,P07-1113,1,0.66066,"ant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Procee"
S17-1027,D16-1058,0,0.172418,"nal neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our"
S17-1027,P16-2067,0,0.0229775,"to keep. ht is the hidden state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete inp"
S17-1027,Q16-1027,0,0.0166683,"hbrenner et al., son/thing/situation the clause is about, often realized as its grammatical subject. 2 Code and data: https://github.com/annefried/sitent The main referent of a clause is roughly the per231 rent connections, which allow them to find patterns in – and thus model – sequences. Simple RNNs cannot capture long-term dependencies (Bengio et al., 1994) because the gradients tend to vanish or grow out of control with long sequences. Gated Recurrent Unit (GRU) RNNs, proposed by Cho et al. (2014), address this shortcoming. GRUs have fewer parameters and thus need less data to generalize (Zhou et al., 2016) than LSTM RNNs, and also outperform the LSTM in many cases (Yin et al., 2017), which makes them a good choice for our relatively small dataset.3 The relevant equations for a GRU are below. xt is the input at time t, rt is a reset gate which determines how to combine the new input with the previous memory, and the update gate zt defines how much of the previous memory to keep. ht is the hidden state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tan"
S17-1027,P15-1109,0,0.0139583,"nd U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidden state. Attention has been used successfully e.g. in aspect-based sentimen"
S19-1024,W13-2322,0,0.488289,"encoding models. Both approaches greatly profit from large amounts of silver training data. The silver data is obtained with self-training (Konstas et al., 2017) or the aid of additional parsers, where only parses with considerable agreement are chosen to extend the training data (van Noord and Bos, 2017b). Lyu and Titov (2018) formulate a neural model that jointly predicts alignments, concepts and relations. Their system – henceforth called GPLA (Graph Prediction with Latent Alignments) – defines the current state-of-the-art in AMR parsing. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) represents the semantic structure of a sentence, including concepts, semantic operators and relations, sense-disambiguated predicates and their arguments. As a machine readable representation of the meaning of a sentence, AMR is potentially useful for many NLP tasks. Among other applications it has been used in machine translation (Jones et al., 2012), text A system that can perform accuracy prediction for AMR parsing can be used in a variety of ways: (i) estimating the quality of downstream tasks that deploy AMR parses. E.g., in a document summarization scenario, we might expect lower qual21"
S19-1024,E17-1051,0,0.621331,"roduced dependency parses. The latter approach uses active learning and ensemble parsing in combination with variational inference. They predict edge labelling and attachment errors and use a back-and-forth encoding mechanism from non-structured to structured tree data in order to provide the variational inference model with the Contributions We define AMR accuracy prediction as the task of predicting a rich suite of metrics to assess various subtasks covered by AMR parsing (e.g. negation detection or semantic role labeling). To approach this task, we use the AMR evaluation suite suggested by Damonte et al. (2017) and develop a hierarchical multi-output regression model for automatically performing evaluation of 12 different tasks involved in AMR parsing (Sections §3 and §4; our code is publicly accessible1 ). We perform experiments in three different scenarios on unseen in-domain and out-of-domain data and show that our model (i) is able to predict scores with significant correlation to gold scores and (ii) can be used to rank parses on a sentencelevel or to rank parsers on a corpus-level (§5). 2 Related Work Automatic accuracy prediction for syntactic parsing comes closest to what we are doing. Ravi"
S19-1024,S16-1176,0,0.0309001,"Missing"
S19-1024,W11-2929,0,0.0291684,"elemans, 2010) and in machine translation. For example, Soricut and Narsale (2012) predict BLEU scores for machine-produced translations. Under the umbrella of quality estimation researchers try to predict, i.a., the post-editing time or missing words in an automatic translation (Cai and Knight, 2013; Joshi et al., 2016; Chatterjee et al., 2018; Kim et al., 2017; Specia et al., 2013). The fact that manually creating an AMR graph is significantly more costly than a translation provides another compelling argument for investigating automatic AMR accuracy prediction techniques .2 In recent work, Dickinson and Smith (2011, 2017); Jain et al. (2015); Rehbein and Ruppenhofer (2018) detect annotation errors in automatically produced dependency parses. The latter approach uses active learning and ensemble parsing in combination with variational inference. They predict edge labelling and attachment errors and use a back-and-forth encoding mechanism from non-structured to structured tree data in order to provide the variational inference model with the Contributions We define AMR accuracy prediction as the task of predicting a rich suite of metrics to assess various subtasks covered by AMR parsing (e.g. negation det"
S19-1024,S16-1182,0,0.0303377,"tance consists of a sentence and an AMR parse as input and a vector of metric scores as target. Our second data set, LDC2015R36, comprises submissions to the SemEval-2016 Task 8 (May, 2016). We have 1053 parses from each of the 11 team submissions (and 2 baseline systems).6 Our Training For the optimization of the accuracy prediction model we use only the development and training sections of LDC2015E86 and the corresponding automatic parses together with the gold scores. Details on the training cycle can be found in the Supplemental Material §A (the loss is deDynamicPower (Butler, 2016), TMF (Bjerva et al., 2016), UCL+Sheffield (Goodman et al., 2016) and CU-NLP (Foland and Martin, 2016). 7 TMF-1 and TMF-2 (van Noord and Bos, 2017a), DANGNT (Nguyen and Nguyen, 2017), Oxford (Buys and Blunsom, 2017), RIGOTRIO (Gruzitis et al., 2017) and JAMR (Flanigan et al., 2016) 8 https://spacy.io/ 5 https://github.com/ChunchuanLv/ amr-evaluation-tool-enhanced 6 Riga (Barzdins and Gosko, 2016), CMU (equal to JAMR) (Flanigan et al., 2016), Brandeis (Wang et al., 2016b), UofR (Peng and Gildea, 2016), ICL-HD (Brandt et al., 2016), M2L (Puzikov et al., 2016), UMD (Rao et al., 2016), 216 ρ BioAMRTest P R F1 P R F1 Smatch"
S19-1024,S16-1179,0,0.0399188,"Missing"
S19-1024,S16-1186,0,0.0537889,"Missing"
S19-1024,S16-1177,0,0.0136141,"ction task. Each instance consists of a sentence and an AMR parse as input and a vector of metric scores as target. Our second data set, LDC2015R36, comprises submissions to the SemEval-2016 Task 8 (May, 2016). We have 1053 parses from each of the 11 team submissions (and 2 baseline systems).6 Our Training For the optimization of the accuracy prediction model we use only the development and training sections of LDC2015E86 and the corresponding automatic parses together with the gold scores. Details on the training cycle can be found in the Supplemental Material §A (the loss is deDynamicPower (Butler, 2016), TMF (Bjerva et al., 2016), UCL+Sheffield (Goodman et al., 2016) and CU-NLP (Foland and Martin, 2016). 7 TMF-1 and TMF-2 (van Noord and Bos, 2017a), DANGNT (Nguyen and Nguyen, 2017), Oxford (Buys and Blunsom, 2017), RIGOTRIO (Gruzitis et al., 2017) and JAMR (Flanigan et al., 2016) 8 https://spacy.io/ 5 https://github.com/ChunchuanLv/ amr-evaluation-tool-enhanced 6 Riga (Barzdins and Gosko, 2016), CMU (equal to JAMR) (Flanigan et al., 2016), Brandeis (Wang et al., 2016b), UofR (Peng and Gildea, 2016), ICL-HD (Brandt et al., 2016), M2L (Puzikov et al., 2016), UMD (Rao et al., 2016), 216 ρ BioAM"
S19-1024,P14-1134,0,0.496343,"s for submissions from two AMR shared tasks on the basis of their predicted parse accuracy averages. All experiments are carried out across two different domains and show that our method is effective. 1 Figure 1: Humanly produced AMR for: There is no asbestos in our products now. Numbered predicates refer to PropBank senses (Palmer et al., 2005). summarization (Liu et al., 2015; Dohare and Karnick, 2017) and question answering (Mitra and Baral, 2016). Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step (Flanigan et al., 2014, 2016) or transition-based parsers relying on dependency annotation (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word-based (Konstas et al., 2017) encoding models. Both approaches greatly profit from large amounts of silver training data. The silver data is obtained with self-training (Konstas et al., 2017) or the aid of additional parsers, where only parses with considerable agreement ar"
S19-1024,S17-2157,0,0.0609963,"Missing"
S19-1024,S16-1185,0,0.0204852,"metric scores as target. Our second data set, LDC2015R36, comprises submissions to the SemEval-2016 Task 8 (May, 2016). We have 1053 parses from each of the 11 team submissions (and 2 baseline systems).6 Our Training For the optimization of the accuracy prediction model we use only the development and training sections of LDC2015E86 and the corresponding automatic parses together with the gold scores. Details on the training cycle can be found in the Supplemental Material §A (the loss is deDynamicPower (Butler, 2016), TMF (Bjerva et al., 2016), UCL+Sheffield (Goodman et al., 2016) and CU-NLP (Foland and Martin, 2016). 7 TMF-1 and TMF-2 (van Noord and Bos, 2017a), DANGNT (Nguyen and Nguyen, 2017), Oxford (Buys and Blunsom, 2017), RIGOTRIO (Gruzitis et al., 2017) and JAMR (Flanigan et al., 2016) 8 https://spacy.io/ 5 https://github.com/ChunchuanLv/ amr-evaluation-tool-enhanced 6 Riga (Barzdins and Gosko, 2016), CMU (equal to JAMR) (Flanigan et al., 2016), Brandeis (Wang et al., 2016b), UofR (Peng and Gildea, 2016), ICL-HD (Brandt et al., 2016), M2L (Puzikov et al., 2016), UMD (Rao et al., 2016), 216 ρ BioAMRTest P R F1 P R F1 Smatch 0.74 0.79 0.78 0.54 0.41 0.47 Concepts Frames IgnoreVars Named Ent. Negatio"
S19-1024,P13-2131,0,0.656296,"o our knowledge, we are the first to propose an accuracy prediction model for AMR parsing, and offer the first general end-to-end parse accuracy prediction model that predicts an ensemble of scores for different linguistic aspects. Automatic accuracy prediction has also been researched for PoS-tagging (Van Asch and Daelemans, 2010) and in machine translation. For example, Soricut and Narsale (2012) predict BLEU scores for machine-produced translations. Under the umbrella of quality estimation researchers try to predict, i.a., the post-editing time or missing words in an automatic translation (Cai and Knight, 2013; Joshi et al., 2016; Chatterjee et al., 2018; Kim et al., 2017; Specia et al., 2013). The fact that manually creating an AMR graph is significantly more costly than a translation provides another compelling argument for investigating automatic AMR accuracy prediction techniques .2 In recent work, Dickinson and Smith (2011, 2017); Jain et al. (2015); Rehbein and Ruppenhofer (2018) detect annotation errors in automatically produced dependency parses. The latter approach uses active learning and ensemble parsing in combination with variational inference. They predict edge labelling and attachmen"
S19-1024,W18-1804,0,0.0278923,"se an accuracy prediction model for AMR parsing, and offer the first general end-to-end parse accuracy prediction model that predicts an ensemble of scores for different linguistic aspects. Automatic accuracy prediction has also been researched for PoS-tagging (Van Asch and Daelemans, 2010) and in machine translation. For example, Soricut and Narsale (2012) predict BLEU scores for machine-produced translations. Under the umbrella of quality estimation researchers try to predict, i.a., the post-editing time or missing words in an automatic translation (Cai and Knight, 2013; Joshi et al., 2016; Chatterjee et al., 2018; Kim et al., 2017; Specia et al., 2013). The fact that manually creating an AMR graph is significantly more costly than a translation provides another compelling argument for investigating automatic AMR accuracy prediction techniques .2 In recent work, Dickinson and Smith (2011, 2017); Jain et al. (2015); Rehbein and Ruppenhofer (2018) detect annotation errors in automatically produced dependency parses. The latter approach uses active learning and ensemble parsing in combination with variational inference. They predict edge labelling and attachment errors and use a back-and-forth encoding me"
S19-1024,S16-1180,0,0.0208234,"MR parse as input and a vector of metric scores as target. Our second data set, LDC2015R36, comprises submissions to the SemEval-2016 Task 8 (May, 2016). We have 1053 parses from each of the 11 team submissions (and 2 baseline systems).6 Our Training For the optimization of the accuracy prediction model we use only the development and training sections of LDC2015E86 and the corresponding automatic parses together with the gold scores. Details on the training cycle can be found in the Supplemental Material §A (the loss is deDynamicPower (Butler, 2016), TMF (Bjerva et al., 2016), UCL+Sheffield (Goodman et al., 2016) and CU-NLP (Foland and Martin, 2016). 7 TMF-1 and TMF-2 (van Noord and Bos, 2017a), DANGNT (Nguyen and Nguyen, 2017), Oxford (Buys and Blunsom, 2017), RIGOTRIO (Gruzitis et al., 2017) and JAMR (Flanigan et al., 2016) 8 https://spacy.io/ 5 https://github.com/ChunchuanLv/ amr-evaluation-tool-enhanced 6 Riga (Barzdins and Gosko, 2016), CMU (equal to JAMR) (Flanigan et al., 2016), Brandeis (Wang et al., 2016b), UofR (Peng and Gildea, 2016), ICL-HD (Brandt et al., 2016), M2L (Puzikov et al., 2016), UMD (Rao et al., 2016), 216 ρ BioAMRTest P R F1 P R F1 Smatch 0.74 0.79 0.78 0.54 0.41 0.47 Concepts"
S19-1024,E17-1053,0,0.02855,"Missing"
S19-1024,P18-1170,0,0.0921748,"Missing"
S19-1024,S16-1166,0,0.022367,"e still obtain a considerable amount of deficient parses for training. Based on the parser outputs we compute evaluations comparing the automatic parses with the gold parses by using amrevaluation-tool-enhanced5 , a bug-fixed version of the script that computes the metrics of Damonte et al. (2017). This allows us to create full-fledged training, development and test instances for our accuracy prediction task. Each instance consists of a sentence and an AMR parse as input and a vector of metric scores as target. Our second data set, LDC2015R36, comprises submissions to the SemEval-2016 Task 8 (May, 2016). We have 1053 parses from each of the 11 team submissions (and 2 baseline systems).6 Our Training For the optimization of the accuracy prediction model we use only the development and training sections of LDC2015E86 and the corresponding automatic parses together with the gold scores. Details on the training cycle can be found in the Supplemental Material §A (the loss is deDynamicPower (Butler, 2016), TMF (Bjerva et al., 2016), UCL+Sheffield (Goodman et al., 2016) and CU-NLP (Foland and Martin, 2016). 7 TMF-1 and TMF-2 (van Noord and Bos, 2017a), DANGNT (Nguyen and Nguyen, 2017), Oxford (Buys"
S19-1024,S17-2159,0,0.0269574,"Missing"
S19-1024,S17-2090,0,0.0262365,"Missing"
S19-1024,C12-1083,0,0.0804996,"Missing"
S19-1024,S17-2156,0,0.133003,"n machine translation. For example, Soricut and Narsale (2012) predict BLEU scores for machine-produced translations. Under the umbrella of quality estimation researchers try to predict, i.a., the post-editing time or missing words in an automatic translation (Cai and Knight, 2013; Joshi et al., 2016; Chatterjee et al., 2018; Kim et al., 2017; Specia et al., 2013). The fact that manually creating an AMR graph is significantly more costly than a translation provides another compelling argument for investigating automatic AMR accuracy prediction techniques .2 In recent work, Dickinson and Smith (2011, 2017); Jain et al. (2015); Rehbein and Ruppenhofer (2018) detect annotation errors in automatically produced dependency parses. The latter approach uses active learning and ensemble parsing in combination with variational inference. They predict edge labelling and attachment errors and use a back-and-forth encoding mechanism from non-structured to structured tree data in order to provide the variational inference model with the Contributions We define AMR accuracy prediction as the task of predicting a rich suite of metrics to assess various subtasks covered by AMR parsing (e.g. negation detection"
S19-1024,S17-2160,0,0.0342788,"Missing"
S19-1024,P17-1014,0,0.272316,"ization (Liu et al., 2015; Dohare and Karnick, 2017) and question answering (Mitra and Baral, 2016). Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step (Flanigan et al., 2014, 2016) or transition-based parsers relying on dependency annotation (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word-based (Konstas et al., 2017) encoding models. Both approaches greatly profit from large amounts of silver training data. The silver data is obtained with self-training (Konstas et al., 2017) or the aid of additional parsers, where only parses with considerable agreement are chosen to extend the training data (van Noord and Bos, 2017b). Lyu and Titov (2018) formulate a neural model that jointly predicts alignments, concepts and relations. Their system – henceforth called GPLA (Graph Prediction with Latent Alignments) – defines the current state-of-the-art in AMR parsing. Introduction Abstract Meaning Representation (AMR)"
S19-1024,J05-1004,0,0.118734,"accuracies and test whether it can reliably assign high scores to gold parses. Secondly, we perform parse selection based on predicted parse accuracies of candidate parses from alternative systems, with the aim of improving overall results. Finally, we predict system ranks for submissions from two AMR shared tasks on the basis of their predicted parse accuracy averages. All experiments are carried out across two different domains and show that our method is effective. 1 Figure 1: Humanly produced AMR for: There is no asbestos in our products now. Numbered predicates refer to PropBank senses (Palmer et al., 2005). summarization (Liu et al., 2015; Dohare and Karnick, 2017) and question answering (Mitra and Baral, 2016). Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step (Flanigan et al., 2014, 2016) or transition-based parsers relying on dependency annotation (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word"
S19-1024,S16-1183,0,0.0338067,"Missing"
S19-1024,N15-1114,0,0.0227683,"eliably assign high scores to gold parses. Secondly, we perform parse selection based on predicted parse accuracies of candidate parses from alternative systems, with the aim of improving overall results. Finally, we predict system ranks for submissions from two AMR shared tasks on the basis of their predicted parse accuracy averages. All experiments are carried out across two different domains and show that our method is effective. 1 Figure 1: Humanly produced AMR for: There is no asbestos in our products now. Numbered predicates refer to PropBank senses (Palmer et al., 2005). summarization (Liu et al., 2015; Dohare and Karnick, 2017) and question answering (Mitra and Baral, 2016). Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step (Flanigan et al., 2014, 2016) or transition-based parsers relying on dependency annotation (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word-based (Konstas et al., 2017) enc"
S19-1024,S16-1178,0,0.0270515,"Missing"
S19-1024,P18-1037,0,0.246106,"ion (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word-based (Konstas et al., 2017) encoding models. Both approaches greatly profit from large amounts of silver training data. The silver data is obtained with self-training (Konstas et al., 2017) or the aid of additional parsers, where only parses with considerable agreement are chosen to extend the training data (van Noord and Bos, 2017b). Lyu and Titov (2018) formulate a neural model that jointly predicts alignments, concepts and relations. Their system – henceforth called GPLA (Graph Prediction with Latent Alignments) – defines the current state-of-the-art in AMR parsing. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) represents the semantic structure of a sentence, including concepts, semantic operators and relations, sense-disambiguated predicates and their arguments. As a machine readable representation of the meaning of a sentence, AMR is potentially useful for many NLP tasks. Among other applications it has been"
S19-1024,N15-1040,0,0.289133,"averages. All experiments are carried out across two different domains and show that our method is effective. 1 Figure 1: Humanly produced AMR for: There is no asbestos in our products now. Numbered predicates refer to PropBank senses (Palmer et al., 2005). summarization (Liu et al., 2015; Dohare and Karnick, 2017) and question answering (Mitra and Baral, 2016). Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step (Flanigan et al., 2014, 2016) or transition-based parsers relying on dependency annotation (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word-based (Konstas et al., 2017) encoding models. Both approaches greatly profit from large amounts of silver training data. The silver data is obtained with self-training (Konstas et al., 2017) or the aid of additional parsers, where only parses with considerable agreement are chosen to extend the training data (van Noord and Bos, 2017b). Lyu and Titov (2018) f"
S19-1024,D08-1093,0,0.131828,"2017) and develop a hierarchical multi-output regression model for automatically performing evaluation of 12 different tasks involved in AMR parsing (Sections §3 and §4; our code is publicly accessible1 ). We perform experiments in three different scenarios on unseen in-domain and out-of-domain data and show that our model (i) is able to predict scores with significant correlation to gold scores and (ii) can be used to rank parses on a sentencelevel or to rank parsers on a corpus-level (§5). 2 Related Work Automatic accuracy prediction for syntactic parsing comes closest to what we are doing. Ravi et al. (2008) propose a feature-based SVM regression model with RBF kernel that predicts syntactic parser performance on different domains. Like us, they aim at a cheap and effective means for estimating a parser’s performance. However, in contrast to their work, our method is domain 1 https://gitlab.cl.uni-heidelberg.de/ opitz/quamr 2 Creating an AMR graph requires trained linguists and takes on average 8 to 13 minutes, cf. Banarescu et al. (2013) 213 (a / asbestos (a / asbestos :time (n / now) :polarity :polarity :location (p / product) :location (p / product :time (n / now)) :poss (w / we))) ___________"
S19-1024,C18-1010,0,0.0239432,", Soricut and Narsale (2012) predict BLEU scores for machine-produced translations. Under the umbrella of quality estimation researchers try to predict, i.a., the post-editing time or missing words in an automatic translation (Cai and Knight, 2013; Joshi et al., 2016; Chatterjee et al., 2018; Kim et al., 2017; Specia et al., 2013). The fact that manually creating an AMR graph is significantly more costly than a translation provides another compelling argument for investigating automatic AMR accuracy prediction techniques .2 In recent work, Dickinson and Smith (2011, 2017); Jain et al. (2015); Rehbein and Ruppenhofer (2018) detect annotation errors in automatically produced dependency parses. The latter approach uses active learning and ensemble parsing in combination with variational inference. They predict edge labelling and attachment errors and use a back-and-forth encoding mechanism from non-structured to structured tree data in order to provide the variational inference model with the Contributions We define AMR accuracy prediction as the task of predicting a rich suite of metrics to assess various subtasks covered by AMR parsing (e.g. negation detection or semantic role labeling). To approach this task, w"
S19-1024,W12-3121,0,0.0269746,"ways from a syntactic tree. Nodes in AMR do not explicitly correspond to words (as in dependency trees) or phrases (as in constituency trees). AMR structure elements can exist without any alignment to words in the sentence. To our knowledge, we are the first to propose an accuracy prediction model for AMR parsing, and offer the first general end-to-end parse accuracy prediction model that predicts an ensemble of scores for different linguistic aspects. Automatic accuracy prediction has also been researched for PoS-tagging (Van Asch and Daelemans, 2010) and in machine translation. For example, Soricut and Narsale (2012) predict BLEU scores for machine-produced translations. Under the umbrella of quality estimation researchers try to predict, i.a., the post-editing time or missing words in an automatic translation (Cai and Knight, 2013; Joshi et al., 2016; Chatterjee et al., 2018; Kim et al., 2017; Specia et al., 2013). The fact that manually creating an AMR graph is significantly more costly than a translation provides another compelling argument for investigating automatic AMR accuracy prediction techniques .2 In recent work, Dickinson and Smith (2011, 2017); Jain et al. (2015); Rehbein and Ruppenhofer (201"
S19-1024,P13-4014,0,0.0614772,"Missing"
S19-1024,W10-2605,0,0.0631416,"Missing"
S19-1024,S16-1181,0,0.389593,"Missing"
S19-1024,P15-2141,0,0.326379,"averages. All experiments are carried out across two different domains and show that our method is effective. 1 Figure 1: Humanly produced AMR for: There is no asbestos in our products now. Numbered predicates refer to PropBank senses (Palmer et al., 2005). summarization (Liu et al., 2015; Dohare and Karnick, 2017) and question answering (Mitra and Baral, 2016). Since the introduction of AMR, many approaches to AMR parsing have been proposed: graph-based pipeline systems which rely on an alignment step (Flanigan et al., 2014, 2016) or transition-based parsers relying on dependency annotation (Wang et al., 2015b,a, 2016a). In the following we will denote the former by JAMR and the latter by CAMR. More recently, endto-end neural systems have been proposed which produce linearized AMR graphs within characterbased (van Noord and Bos, 2017b) or word-based (Konstas et al., 2017) encoding models. Both approaches greatly profit from large amounts of silver training data. The silver data is obtained with self-training (Konstas et al., 2017) or the aid of additional parsers, where only parses with considerable agreement are chosen to extend the training data (van Noord and Bos, 2017b). Lyu and Titov (2018) f"
S19-1025,D14-1162,0,0.085968,"redicates and arguments often lie more than 10 words apart and a non-negligible amount of cases consists of distances of more than 20 words. Since the work of Teichert et al. (2017), the SPRL problem has been phrased as follows: given a (sentence, predicate, argument) tuple, we need to predict for all possible argument properties from a given property-inventory whether they hold or not (regression: how likely are they to hold?). Following previous work (Rudinger et al., 2018), the backbone of our model is a Bi-LSTM. To ensure further comparability, pretrained 300 dimensional GloVe embeddings (Pennington et al., 2014) are used for building the input sequence 226 80 70 60 50 `=− 40 30 where o?p,0 = I(¬p) and o?p,1 = I(p) i.e. the gold label indicator. 20 10 0 λ X ? (op,1 log op,1 + o?p,2 log op,2 ), (4) |P | p∈P p count In case of the multi-label formulation, our main loss for an example is the average cross entropy loss over every property: SPR1 #words betw. arg & verb SPR1 sent len SPR2 #words betw. arg & verb SPR2 sent len 0 10 20 30 40 #words betw. arg & verb or sent len 4 Data We use the same data setup and split as Teichert et al. (2017); Rudinger et al. (2018); Tenney et al. (2019). For determining t"
S19-1025,J02-3001,0,0.455952,"-LSTM (Hochreiter and Schmidhuber, 1997) to produce a sequence of hidden states. The prediction is based on the hidden state corresponding to the head of the argument phrase, which is determined by inspection of the gold syntax tree. Recently, Tenney et al. (2019) have demonstrated the capacities of contextualized word embeddings across a wide variety of tasks, including SPRL. However, for SPRL labeling they proceed similar to Rudinger et al. (2018) in the sense SRL The task of automatically identifying predicate-argument structures and assigning roles to arguments was firstly investigated by Gildea and Jurafsky (2002). Over the past years, SRL has witnessed a large surge in interest. Recently, very competitive end-to-end neural systems have been proposed (He et al., 2018a; Cai et al., 2018; He et al., 2018b). Strubell et al. (2018) show that injection of syntax can help SRL models and Li et al. (2019) bridge the gap between span-based and dependency-based SRL, achieving new stateof-the-art results both on the span based CoNLL data (Carreras and M`arquez, 2005; Pradhan et al., 2013) and the dependency-based CoNLL data (Surdeanu et al., 2008; Hajiˇc et al., 2009). A fully end-to-end S(P)RL system has to solv"
S19-1025,N18-1202,0,0.0188304,"erent model components. (ii) we observe that the small SPR data size introduces a severe sensitivity to different random initializations of our neural model. We find that combining multiple models in a simple voter ensemble makes SPRL predictions not only slightly better but also significantly more robust. We share our code with the community and make it publicly available.1 2 that they extract the gold heads of arguments in their dependency-based SPRL approach. Instead of using an LSTM to convert the input sentence to a sequence of vectors they make use of large language models such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018). The contextual vectors corresponding to predicate and the (gold) argument head are processed by a projection layer, self-attention pooling (Lee et al., 2017) and a two-layer feed forward neural network with sigmoid output activation functions. To compare with Rudinger et al. (2018), our basic model uses standard GloVe embeddings. When our model is fed with contextual embeddings a further observable performance gain can be achieved. To summarize, previous state-of-the-art SPRL systems suffer from a common problem: they are dependency-based and their results rely"
S19-1025,W13-3516,0,0.0317713,"Missing"
S19-1025,P18-2058,0,0.0113417,"phrase, which is determined by inspection of the gold syntax tree. Recently, Tenney et al. (2019) have demonstrated the capacities of contextualized word embeddings across a wide variety of tasks, including SPRL. However, for SPRL labeling they proceed similar to Rudinger et al. (2018) in the sense SRL The task of automatically identifying predicate-argument structures and assigning roles to arguments was firstly investigated by Gildea and Jurafsky (2002). Over the past years, SRL has witnessed a large surge in interest. Recently, very competitive end-to-end neural systems have been proposed (He et al., 2018a; Cai et al., 2018; He et al., 2018b). Strubell et al. (2018) show that injection of syntax can help SRL models and Li et al. (2019) bridge the gap between span-based and dependency-based SRL, achieving new stateof-the-art results both on the span based CoNLL data (Carreras and M`arquez, 2005; Pradhan et al., 2013) and the dependency-based CoNLL data (Surdeanu et al., 2008; Hajiˇc et al., 2009). A fully end-to-end S(P)RL system has to solve multiple sub-tasks: identification of predicate-argument structures, sense disambiguation of predicates and as the main and final step, labeling their arg"
S19-1025,Q15-1034,0,0.0495646,"Missing"
S19-1025,P17-1044,0,0.0222481,"oals of corpus creation was to explore possible SPR annotation protocols for humans. We hope that a side-effect of this paper is to spark more interest in SPR and SPRL. 3 (e1 , ..., eT ). In contrast to Rudinger et al. (2018), we multiply a sequence of marker embeddings (m1 , ..., mT ) element-wise with theNsequence of word vectors: (e1 · m1 , ..., eT · mT ) ( , Figure 2). We distinguish three different marker embeddings that indicate the position of the argument in question (red, Figure 2), the predicate (green, Figure 2) and remaining parts of the sentence. This is to some extent similar to He et al. (2017) who learn two predicate indicator embeddings which are concatenated to the input vectors and serve the purpose of showing the model whether a token is the predicate or not. However, in SPRL we are also provided with the argument phrase. We will see in the ablation experiments that it is paramount to learn a dedicated embedding. Embedding multiplication instead of concatenation has the advantage of fewer LSTM parameters (smaller input dimension). Besides, it provides the model with the option to learn large coefficients of the word vector dimensions of predicate and argument vectors. This shou"
S19-1025,D18-1114,0,0.0227041,"Missing"
S19-1025,P18-1192,0,0.0135291,"phrase, which is determined by inspection of the gold syntax tree. Recently, Tenney et al. (2019) have demonstrated the capacities of contextualized word embeddings across a wide variety of tasks, including SPRL. However, for SPRL labeling they proceed similar to Rudinger et al. (2018) in the sense SRL The task of automatically identifying predicate-argument structures and assigning roles to arguments was firstly investigated by Gildea and Jurafsky (2002). Over the past years, SRL has witnessed a large surge in interest. Recently, very competitive end-to-end neural systems have been proposed (He et al., 2018a; Cai et al., 2018; He et al., 2018b). Strubell et al. (2018) show that injection of syntax can help SRL models and Li et al. (2019) bridge the gap between span-based and dependency-based SRL, achieving new stateof-the-art results both on the span based CoNLL data (Carreras and M`arquez, 2005; Pradhan et al., 2013) and the dependency-based CoNLL data (Surdeanu et al., 2008; Hajiˇc et al., 2009). A fully end-to-end S(P)RL system has to solve multiple sub-tasks: identification of predicate-argument structures, sense disambiguation of predicates and as the main and final step, labeling their arg"
S19-1025,D18-1548,0,0.0304298,"Missing"
S19-1025,W08-2121,0,0.036703,"Missing"
S19-1025,D17-1018,0,0.0568348,"Missing"
S19-1025,D17-3004,0,0.0310523,"Missing"
S19-1025,D16-1177,0,0.0607531,"Missing"
W04-1906,P98-1013,0,0.0620344,"annotated LFG corpus, to derive general frame assignment rules that can be applied to new sentences. We evaluate the results by applying the induced frame assignment rules to LFG parser output.1 1 Introduction There is a growing insight that high-quality NLP applications for information access are in need of deeper, in particular, semantic analysis. A bottleneck for semantic processing is the lack of large domain-independent lexical semantic resources. There are now efforts for the creation of large lexical semantic resources that provide information on predicate-argument structure. FrameNet (Baker et al., 1998), building on Fillmore’s theory of frame semantics, provides definitions of frames and their semantic roles, a lexical database and a manually annotated corpus of example sentences. A strictly corpus-based approach is carried out with ‘PropBank’ – a manual predicate-argument annotation on top of the Penn treebank (Kingsbury et al., 2002). First approaches for learning stochastic models for semantic role assignment from annotated corpora have emerged with Gildea and Jurafsky (2002) and Fleischman et al. (2003). While current competitions explore the potential of shallow parsing 1 The research r"
W04-1906,P03-1068,0,0.0311329,"Missing"
W04-1906,W03-1007,0,0.012829,"cal semantic resources that provide information on predicate-argument structure. FrameNet (Baker et al., 1998), building on Fillmore’s theory of frame semantics, provides definitions of frames and their semantic roles, a lexical database and a manually annotated corpus of example sentences. A strictly corpus-based approach is carried out with ‘PropBank’ – a manual predicate-argument annotation on top of the Penn treebank (Kingsbury et al., 2002). First approaches for learning stochastic models for semantic role assignment from annotated corpora have emerged with Gildea and Jurafsky (2002) and Fleischman et al. (2003). While current competitions explore the potential of shallow parsing 1 The research reported here was conducted in a cooperation project of the German Research Center for Artificial Intelligence, DFKI Saarbr¨ucken with the Computational Linguistics Department of the University of the Saarland at Saarbr¨ucken. Jiˇr´ı Semeck´y Institute of Formal and Applied Linguistics Charles University Malostransk´e n´amˇest´ı 25 11800 Prague, Czech Republic semecky@ufal.ms.mff.cuni.cz for role labelling, Gildea and Palmer (2002) emphasise the role of deeper syntactic analysis for semantic role labelling. We"
W04-1906,W03-2404,0,0.0301855,"gure 6: SALSA-2-LFG-TIGER transfer 4 Corpus-based induction of an LFG frame semantics interface 4.1 Porting SALSA annotations to LFG A challenge for corpus-based induction of a syntaxsemantics interface for frame assignment is the transposition of the corpus annotations from a given syntactic annotation scheme to the target syntactic framework. The basis for our work are annotations of the SALSA/TIGER corpus (Erk et al., 2003), encoded in an XML annotation scheme that extends the syntactic TIGER XML annotation scheme. The TIGER treebank has been converted to a parallel LFG f-structure corpus (Forst, 2003). The SALSA/TIGER and LFG-TIGER corpora could be used to learn corresponding syntactic paths in the respective structures. Thus, we could establish the paths of frame constituting elements in the SALSA/TIGER corpus, and port the annotations to the corresponding path in the LFG-TIGER corpus. However, we could apply a more precise method, by exploiting the fact that the LFG-TIGER corpus preserves the original TIGER constituent identifiers, as f-structure features TI - ID (see Fig. 7). We use these ’anchors’ to port the SALSA annotations to the parallel LFG-TIGER treebank. Thus, in a first step w"
W04-1906,J02-3001,0,0.0604158,"Missing"
W04-1906,P02-1031,0,0.0164771,"ignment from annotated corpora have emerged with Gildea and Jurafsky (2002) and Fleischman et al. (2003). While current competitions explore the potential of shallow parsing 1 The research reported here was conducted in a cooperation project of the German Research Center for Artificial Intelligence, DFKI Saarbr¨ucken with the Computational Linguistics Department of the University of the Saarland at Saarbr¨ucken. Jiˇr´ı Semeck´y Institute of Formal and Applied Linguistics Charles University Malostransk´e n´amˇest´ı 25 11800 Prague, Czech Republic semecky@ufal.ms.mff.cuni.cz for role labelling, Gildea and Palmer (2002) emphasise the role of deeper syntactic analysis for semantic role labelling. We follow this line and explore the potential of deep syntactic analysis for role labelling, choosing Lexical Functional Grammar as underlying syntactic framework. We aim at a computational interface for frame semantics processing that can be used to (semi-)automatically extend the size of current training corpora for learning stochastic models for role labelling, and – ultimately – as a basis for automatic frame assignment in NLP tasks, based on the acquired stochastic models. We discuss advantages of semantic role"
W04-1906,W89-0203,0,0.10263,"ion group’. Underspecification may also affect frame elements of a single frame. A motion (Antrag), e.g., may be both MEDIUM and SPEAKER of a REQUEST. Finally, a constituent may or may not be interpreted as a frame element of a given frame. It is then represented as a single element of an underspecification group. We model underspecification as disjunction, which is encoded by optional transfer rules that create alternative (disjunctive) contexts. Optionality is modeled by a single optional rule. Figure 10 displays the result of underspecified frame element assignment in an f-structure chart (Maxwell and Kaplan, 1989). Context c1 displays the reading where Antrag is assigned the SPEAKER role, alternatively, in context c2, it is assigned the role MEDIUM. In a symbolic account disjunction doesn’t correctly model the intended meaning of underspecification. Yet, a stochastic model for frame assignment should render the vagueness involved in underspecification by close stochastic weights. Thus, underspecified annotation instances provide alternative frames in the training data and can be used for finegrained evaluation of frame assignment models. Multiword Expressions The treatment of multiword expressions (idi"
W04-1906,P02-1035,0,0.0794548,"Missing"
W04-1906,C98-1013,0,\N,Missing
W06-3001,W04-2509,0,0.215646,"Missing"
W06-3001,E89-1039,0,\N,Missing
W06-3001,P83-1025,0,\N,Missing
W06-3001,J86-3001,0,\N,Missing
W06-3001,A00-1041,0,\N,Missing
W07-1402,W05-1206,0,\N,Missing
W07-1402,P98-1013,0,\N,Missing
W07-1402,C98-1013,0,\N,Missing
W07-1402,P02-1035,0,\N,Missing
W08-2231,D07-1074,0,0.0117433,"one specific ontology class.1 Therefore, we compare our approach to previous work on sense disambiguation. Since in our approach, we aim at minimizing the degree of language- and resource dependency, our focus is on the amount of external knowledge used. One method towards sense disambiguation that has been studied is to use different kinds of text overlap: Ruiz-Casado et al. (2005) calculate vector similarity between a Wikipedia article and WordNet glosses based on term frequencies. Obviously, such glosses are not available for all languages, domains and applications. Wu and Weld (2007) and Cucerzan (2007) calculate the overlap between contexts of named entities and candidate articles from Wikipedia, using overlap ratios or similarity scores in a vector space model, respectively. Both approaches disambiguate named entities using textual context. Since our aim is to acquire concept-related text sources, these methods are not applicable. A general corpus-based approach has been proposed by Reiter and Buitelaar (2008): Using a domain corpus and a domain-independent reference corpus, they select the article with the highest domain relevance score among multiple candidates. This approach works reaso"
W08-2231,N07-1025,0,0.0440123,"text sources, these methods are not applicable. A general corpus-based approach has been proposed by Reiter and Buitelaar (2008): Using a domain corpus and a domain-independent reference corpus, they select the article with the highest domain relevance score among multiple candidates. This approach works reasonably well but relies on the availability of domain-specific corpora and fails at selecting the appropriate among multiple in-domain senses. In contrast, our resource-poor approach does not rely on additional textual resources, as ontologies usually do not contain contexts for classes. 1 Mihalcea (2007) shows that Wikipedia can indeed be used as a sense inventory for sense disambiguation. A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 383 3 Linking Ontology classes to Wikipedia articles This section briefly reviews relevant information about Wikipedia and describes our method for linking ontology classes to Wikipedia articles. Our algorithm consists of two steps: (i) extracting candidate articles from Wikipedia and (ii) selecting the most appropriate one. The algorithm is independent of the choice of a specific ontology.2 3.1 Wikipedia The online encyclopedia Wikipedia cu"
W09-2814,W08-1109,0,0.0246511,"Missing"
W09-2814,P98-1013,0,0.0420771,"Missing"
W09-2814,W09-0628,0,0.0304965,"learned graphs, we will generate directions for a wide range of possible routes. Dale et al. (2005) developed a system that takes GIS data as input and uses a pipeline architecture to generate verbal route directions. In contrast to their approach, our approach will be based on an integrated architecture allowing for more interaction between the different stages of generation. The idea of combining verbal directions with scenes from a virtual 3D environment has recently lead to a new framework for evaluating NLG systems: The Challenge on Generating Instructions in Virtual Environments (GIVE) (Byron et al., 2009) is planned to become a regular event for the NLG community. This work describes first steps towards building a system that synchronously generates multimodal (textual and visual) route directions for pedestrians. We pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages. We conducted an empirical study to collect verbal route directions, and annotated the acquired texts on different levels. Here we describe the experimental setting and an analysis of the collected data. 1 Michael Strube† Introduction Route directions guide a per"
W09-2814,W07-1203,0,0.0433801,"Missing"
W09-2814,J09-2005,0,0.0246238,"Missing"
W09-2814,kunze-lemnitzer-2002-germanet,0,0.0204592,"Missing"
W09-2814,W05-0618,1,0.78264,"Missing"
W09-2814,J93-4001,0,0.0302683,"Missing"
W09-2814,P09-4010,1,0.701736,"surface generation model. We observed a variety of coherence-inducing elements that are generic in nature and thus seem well-suited for a corpusbased generation model. As other languages are known to exhibit differences in verbal realization of directions (von Stutterheim et al., 2002), we have to extend our data collection in order to generate systematic linguistic variations from a single underlying semantic structure for all languages. The linguistic annotation levels of frames and roles, syntactic dependencies, and basic word categories have been tested successfully with a similar corpus (Roth & Frank, 2009). The next steps will consist in the alignment of physical routes and landmarks with semantic representations in an integrated generation architecture. Table 3: Frequencies of temporal adverbs indicating linear (&gt; t ) and reversed linear order (&lt; t ) the following action or situation is not supposed to take place (e.g. Gehen Sie vorher rechts ‘beforehand turn right’). Backward-looking event anaphors and references to result states: We also found explicit references to past events (e.g. Nach dem Durchqueren ‘after traversing’) and result states of events, e.g. the adverbial phrase unten angekom"
W09-2814,burchardt-etal-2006-salto,1,\N,Missing
W09-2814,J03-1003,0,\N,Missing
W09-2814,C98-1013,0,\N,Missing
W10-2415,W03-0430,0,0.00801887,"ld standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically P ERS, L OC, O RG, M ISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers 93"
W10-2415,P08-1003,0,0.039537,"Missing"
W10-2415,P06-1102,0,0.0194731,"Missing"
W10-2415,U06-1013,0,0.0287566,"ty Recognition and Classification Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto Department of Computational Linguistics Heidelberg University, Germany {ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de Abstract addressed the problem of recognizing and categorizing fine-grained NE classes (such as biologist, composer, or athlete) in an open-domain setting. Fine-grained NERC is expected to be beneficial for a wide spectrum of applications, including Information Retrieval (Mandl and WomserHacker, 2005), Information Extraction (Pas¸ca et al., 2006a) or Question-Answering (Pizzato et al., 2006). However, manually compiling widecoverage gazetteers for fine-grained NE classes is time-consuming and error-prone. Also, without an extrinsic evaluation, it is difficult to define a priori which classes are relevant for a particular domain or task. Finally, prior research in FG-NERC is difficult to evaluate, due to the diversity of NE classes and datasets used. Accordingly, in the interest of a general approach, we address the challenge of capturing a broad range of NE classes at various levels of conceptual granularity. By turning FG-NERC into a widely applicable task, applications are free"
W10-2415,E06-1003,0,0.0545995,"Missing"
W10-2415,W02-2024,0,0.0209751,"tual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically P ERS, L OC, O RG, M ISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers 93 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics 2 Related work fu"
W10-2415,W03-0420,0,0.059724,"nces for a given target class c, we add all instances labeled with the hyponyms of c to I. All other instances (not in that subtree) are labeled as being Outside (O-) a NE. This approach ensures that, for each node, the dataset contains two classes (NE and O) only, and implicitly ‘propagates’ the instances up the tree. As a result, non-leaf nodes that did not have any instance in the original dataset become populated. Also, the classification of classes at higher levels is based on larger datasets. NERC using a MaxEnt tagger Our baseline system is modeled following a Maximum Entropy approach (Bender et al., 2003, inter alia). The MaxEnt model produces a probability for each class label t (the NE tag) of a classification instance, conditioned on its context of occurrence h. This probability is calculated by:   n X 1 λj fj (h, t) (1) exp  P (t|h) = Z(h) j=1 where fj (h, t) is the j-th feature with associated weight λj and Z(h) is a normalization constant to ensure a proper probability distribution.3 Given a word wi to be classified as Beginning, Inside or Outside (IOB) of a NE, we extract as features: 1. Context words. The words occurring within i+2 the context window wi−2 = wi−2 . . . wi+2 . 2. Wo"
W10-2415,E06-1002,0,0.10149,"Missing"
W10-2415,C02-1130,0,0.578916,"Missing"
W10-2415,W03-0425,0,0.0098922,"ure frequency cutoff. The best configuration is found by optimizing the F1 measure on the development data with various feature representations. The chosen features are: 1, 2 (with n = 3), 4, 5, 6, 7 and 8. Evaluation on the test set is performed blindly, using this feature set. The results are presented in Table 2. The MaxEnt labeler achieves performance comparable with the CoNLL-2003 task participants, ranking 12th among the 16 systems participating in the task, with a 2 point margin off the F1 of the most similar system of Bender et al. (2003) and 7 points below the best-performing system (Florian et al., 2003). The former used a relatively complex set of features and different gazetteers extracted from unannotated data. The latter combined four diverse classifiers, namely a robust linear classifier, maximum entropy, transformationbased learning and a hidden Markov model. They used different feature sets, unannotated data and an additional NE tagger. In comparison, our NERC system is simpler and based on a small set of features that can be easily obtained for many languages. Besides, it does not make use of any external resources and still shows state-of-the-art performance on the overall data. 6 6."
W10-2415,D07-1026,0,0.0336034,"Missing"
W10-2415,C08-1034,0,0.0579714,"Missing"
W10-2415,W09-1125,0,0.580182,"Missing"
W10-2415,W00-0730,0,0.110963,"sly associates proper names with their corresponding semantic class. Mapping to the WordNet person domain. In order to perform a hierarchical classification of people, we need a taxonomy for the domain at hand. We achieve this by mapping the extracted class labels to WordNet synsets. In our setting, we map against all synsets found under person#n#1, Pattern-based extraction of NE-concept pairs. NEs are often introduced by so-called appositional structures as in (1), which overtly express which semantic class (here, painter) the NE (Kandinsky) belongs to. Appositions involving 1 We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking. 95 Level #C 1 1 2 29 3 57 4 63 5 37 6 18 7 6 8 2 all 213 which are direct hypernyms of at least one instance in WordNet (CW N pers+Inst ).2 Since our goal is to map class labels to synsets (i.e. our future NE classes), we check each class label candidate against all synonyms contained in the synset. At this point we have to deal with two cases: two extracted class label candidates (synonyms such as doctor, physician) will map to a single synset, while ambiguous class labels (e.g. director) can be mapped to more than one synset. In the latter case, we heuristically"
W10-2415,W02-1006,0,0.0271859,"our baseline MaxEnt tagger is very local, including at most the two preceding and succeeding words. Hence, the classifier is not able to capture informative contextual clues in a larger context. Previous work has related FG-NERC to WSD approaches (Alfonseca and Manandhar, 2002). Accordingly, we investigate two context-sensitive approaches inspired from WSD proposals, which consider a more global context for classification. We first define a new feature set to induce a new MaxEnt model (MaxEnt-B) which only uses lexical features from a larger context window, as used in standard supervised WSD (Lee and Ng, 2002): 1. PoS context. The part-of-speech occurring within the context window posi+3 i−3 = posi−3 . . . posi+3 . 2. Local collocation. Local collocations Cnm surrounding wi . We use C−2,−1 and C1,2 . 3. Content words in surrounding context. We i+3 consider all unigrams in contexts wi−3 = wi−3 . . . wi+3 of wi (crossing sentence boundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation symbols. We define a feature vector of length 10 using the 10 most frequent content words. Given a classification instance, the feature corresponding to token"
W10-2415,W00-0726,0,\N,Missing
W11-2506,D10-1115,0,0.0610678,"n 6. Section 7 concludes. 2 Related Work Recent work in distributional semantics has engendered different perspectives on how to characterize the semantics of adjectives and adjective-noun phrases. Almuhareb (2006) aims at capturing the semantics of adjectives in terms of attributes they denote using lexico-syntactic patterns. His approach suffers from severe sparsity problems and does not account for the compositional nature of adjective-noun phrases, as it disregards the meaning contributed by the noun. It is therefore unable to perform disambiguation of adjectives in the context of a noun. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent composition53 ality in adjective-noun phrases considering different types of composition operators. These works adhere to a fully latent representation of meaning, whereas Hartung and Frank (2010) assign symbolic attribute meanings to adjectives, nouns and composed phrases by incorporating attributes as dimensions in a compositional VSM. By holding the attribute meaning of adjectives and nouns in distinct vector representations and combining them through vector composition, their approach improves on both weaknesses of Almuhareb’s work. Howeve"
W11-2506,W10-2415,1,0.883107,"Missing"
W11-2506,W10-2805,0,0.0798577,"ed Work Recent work in distributional semantics has engendered different perspectives on how to characterize the semantics of adjectives and adjective-noun phrases. Almuhareb (2006) aims at capturing the semantics of adjectives in terms of attributes they denote using lexico-syntactic patterns. His approach suffers from severe sparsity problems and does not account for the compositional nature of adjective-noun phrases, as it disregards the meaning contributed by the noun. It is therefore unable to perform disambiguation of adjectives in the context of a noun. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent composition53 ality in adjective-noun phrases considering different types of composition operators. These works adhere to a fully latent representation of meaning, whereas Hartung and Frank (2010) assign symbolic attribute meanings to adjectives, nouns and composed phrases by incorporating attributes as dimensions in a compositional VSM. By holding the attribute meaning of adjectives and nouns in distinct vector representations and combining them through vector composition, their approach improves on both weaknesses of Almuhareb’s work. However, their account is"
W11-2506,C10-1049,1,0.671388,"djectives in terms of attributes they denote using lexico-syntactic patterns. His approach suffers from severe sparsity problems and does not account for the compositional nature of adjective-noun phrases, as it disregards the meaning contributed by the noun. It is therefore unable to perform disambiguation of adjectives in the context of a noun. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent composition53 ality in adjective-noun phrases considering different types of composition operators. These works adhere to a fully latent representation of meaning, whereas Hartung and Frank (2010) assign symbolic attribute meanings to adjectives, nouns and composed phrases by incorporating attributes as dimensions in a compositional VSM. By holding the attribute meaning of adjectives and nouns in distinct vector representations and combining them through vector composition, their approach improves on both weaknesses of Almuhareb’s work. However, their account is still closely tied to Almuhareb’s pattern-based approach in that counts of co-occurrence patterns linking adjectives and nouns to attributes are used to populate the vector representations. These, however, are inherently sparse"
W11-2506,D11-1050,1,0.877351,"of the phrase hot pepper is expected to yield high component values on the dimensions TASTE and SMELL, rather than TEM PERATURE. The underlying relations between adjectives and nouns, respectively, and the attributes they denote is captured by way of latent semantic information obtained from Latent Dirichlet Allocation (LDA; Blei et al. (2003)). Thus, we treat attributes as an abstract meaning layer that generalizes over latent topics inferred by LDA and utilize this interpretable layer as the dimensions of our VSM. This approach has been shown to be effective in an attribute selection task (Hartung and Frank, 2011), where the goal is to predict the most prominent attribute(s) “hidden” in the compositional semantics of adjective-noun phrases. In this paper, our main interest is to assess the potential of modeling adjective semantics in terms of discrete, interpretable attribute meanings in a similarity judgement task, as opposed to a representation in latent semantic space that is usually applied to tasks of this kind. 52 Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 52–61, c Edinburgh, Scotland, UK, July 31, 2011. 2011 Association for Comput"
W11-2506,P08-1028,0,0.172904,"Missing"
W11-2506,D09-1045,0,0.0621462,"ns linking adjectives and nouns to attributes are used to populate the vector representations. These, however, are inherently sparse. The resulting model therefore still suffers from sparsity of co-occurrence data. Finally, Latent Dirichlet Allocation, originally designed for tasks such as text classification and document modeling (Blei et al., 2003), found its way into lexical semantics. Ritter et al. (2010) and ´ S´eaghdha (2010), e.g., model selectional restricO tions of verb arguments by inducing topic distributions that characterize mixtures of topics observed in verb argument positions. Mitchell and Lapata (2009, 2010) were the first to use LDA-inferred topics as dimensions in VSMs. Hartung and Frank (2011) adopt a similar approach, by embedding LDA into a VSM for adjective-noun meaning composition, with LDA topics providing latent variables for attribute meanings. That is, contrary to M&L, LDA is used to convey information about interpretable semantic attributes rather than latent topics. In fact, Hartung and Frank (2011) are able to show that “injecting” topic distributions inferred from LDA into a VSM alleviates sparsity problems that persisted with the pattern-based VSM of Hartung and Frank (2010"
W11-2506,P10-1045,0,0.0693963,"Missing"
W11-2506,P10-1044,0,0.0372143,"gh vector composition, their approach improves on both weaknesses of Almuhareb’s work. However, their account is still closely tied to Almuhareb’s pattern-based approach in that counts of co-occurrence patterns linking adjectives and nouns to attributes are used to populate the vector representations. These, however, are inherently sparse. The resulting model therefore still suffers from sparsity of co-occurrence data. Finally, Latent Dirichlet Allocation, originally designed for tasks such as text classification and document modeling (Blei et al., 2003), found its way into lexical semantics. Ritter et al. (2010) and ´ S´eaghdha (2010), e.g., model selectional restricO tions of verb arguments by inducing topic distributions that characterize mixtures of topics observed in verb argument positions. Mitchell and Lapata (2009, 2010) were the first to use LDA-inferred topics as dimensions in VSMs. Hartung and Frank (2011) adopt a similar approach, by embedding LDA into a VSM for adjective-noun meaning composition, with LDA topics providing latent variables for attribute meanings. That is, contrary to M&L, LDA is used to convey information about interpretable semantic attributes rather than latent topics. I"
W11-2506,C08-1114,0,0.0221611,"rs involving adjectives with vague and highly ambiguous attribute meanings, such as good, new, certain, general. These are difficult to capture, especially in combination with abstract noun concepts such as information, effect or circumstance. An interesting type of similarity is represented by early evening – previous day. In this case, we observe a contrast in the semantics of the nouns involved, while the pair exhibits strong similarity on the attribute level, which is reflected in the system’s similarity score. This type of similarity is reminiscent of relational analogies investigated in Turney (2008). A related example is rural community – federal assembly. Unlike the human judges, C-LDA predicts high similarity for both pairs. The examples given in −AgrM&L , by contrast, clearly point to a lack in capturing adjective semantics, with misjudgements such as effective way – efficient use, large number – vast amount or large quantity – great majority. Turning to −AgrC-LDA-A again, we find 9/10 items exhibit values greater than 0.67 (average: 0.78). This means the model yields a high number of false positives in rating similarity (with explanations and some reservations just discussed). All it"
W13-0211,S10-1059,0,0.517109,"Missing"
W13-0211,P10-1160,0,0.197188,"Missing"
W13-0211,J12-4003,0,0.28181,"ssing this task, using supervised and recently also semi- and unsupervised methods (Palmer et al., 2010). Traditional SRL is restricted to the local syntactic domain. In discourse interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure. Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure (Fillmore and Baker, 2001; Burchardt et al., 2005). The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm (Meyers et al., 2004). In contrast, Ruppenhofer et al. (2010) follow the FrameNet paradigm, which is not restricted to nominal predicates. However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2). Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel e"
W13-0211,meyers-etal-2004-annotating,0,0.0850802,"local syntactic domain. In discourse interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure. Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure (Fillmore and Baker, 2001; Burchardt et al., 2005). The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm (Meyers et al., 2004). In contrast, Ruppenhofer et al. (2010) follow the FrameNet paradigm, which is not restricted to nominal predicates. However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2). Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel et al., 2011) as underlying corpus. A qualitative analysis of the produced annotations leads to a number of hypotheses on implicit role realiz"
W13-0211,J05-1004,0,0.50929,"Missing"
W13-0211,R11-1046,0,0.714775,"Missing"
W13-0211,S10-1008,0,0.44858,"e interpretation, however, we typically find locally unrealized argument roles that are contextually bound to antecedents beyond their local structure. Thus, by using strictly local methods, we are far from capturing the full potential offered by semantic argument structure (Fillmore and Baker, 2001; Burchardt et al., 2005). The task of resolving the reference of implicit arguments has been addressed in previous work: Gerber and Chai (2012) address the task in the nominal domain by learning a model from manually annotated data following the NomBank paradigm (Meyers et al., 2004). In contrast, Ruppenhofer et al. (2010) follow the FrameNet paradigm, which is not restricted to nominal predicates. However, their data set suffers from considerable sparsity with respect to annotation instances per predicate (cf. Section 2). Our contribution addresses the problem of sparse training resources for implicit role binding by providing a higher volume of predicate-specific annotations for non-local role binding, using OntoNotes (Weischedel et al., 2011) as underlying corpus. A qualitative analysis of the produced annotations leads to a number of hypotheses on implicit role realization. Using the extended set of annotat"
W13-0211,S12-1001,1,0.836729,"and the influence of constructional licensing. 5 Evaluation Experiments We evaluate the impact of predicate-specific annotations for classification using two scenarios: (CV) we examine the linking performance of models trained and tested on the same predicate by adopting the 10-fold Cross-Validation scenario used by Gerber and Chai (2012) (G&C).11 (SemEval) Secondly, we examine the direct effect of using our annotations as additional training data for linking NIs in the SemEval 2010 task on implicit role binding. We use the state-of-the-art system and best performing feature set described in Silberer and Frank (2012) (S&F) to make direct comparisons to previous results. CV. As positive training and test samples for this scenario, we use all annotated (and resolvable) NIs and randomly split them into 10 folds. Negative training samples (i.e., incorrect NI fillers) are automat11 Note that this is not a direct comparison as both the annotation paradigm and the data sets are different. verb give put leave bring pay — average Cross-Validation precision recall 48.8 33.3 48.3 72.7 35.4 — 47.7 33.3 20.0 56.0 27.6 20.0 — 31.4 F1 39.6 25.0 51.9 40.0 25.6 — 36.4 training data S&F’12 no additional data + best heurist"
W13-0211,S10-1065,0,0.434924,"Missing"
W13-0211,W11-0908,0,0.861322,"Missing"
W15-2705,W13-0501,0,0.259632,"Missing"
W15-2705,baker-etal-2010-modality,0,0.228442,"Missing"
W15-2705,P10-1005,1,0.829769,"res using sequences of POS tags of the verbal complex, following Loaiciga et al. (2014). The boolean features perfect and progressive indicate the respective grammatical aspect; voice indicates active or passive voice. We extract person and number of the subject and the noun type (common, proper, pronoun). Person is identified via personal pronoun features, and the other features are extracted from POS tags. The countability of the noun is obtained from the Celex database (Baayen et al., 1996). Lexical semantic features for the subject NP are extracted from WordNet (Fellbaum, 1999). Following Reiter and Frank (2010), we take the most frequent sense of the noun in WN (subject sense0), add the direct hypernym of this sense, the direct hypernym of that hypernym, etc., resulting in features subject sense[1, 2, 3]. We also extract the top sense in the WN hierarchy subject sense top (e.g. entity) and the WN lexical filename (e.g. person). LA: Lexical aspectual class. Verbs can be used in a dynamic or stative sense, e.g. I ate an apple vs. I like apples (Vendler, 1957). The lexical aspect of a verb in context influences modal sense in some cases. In contrast to (4.a), for example, where the eventive verb return"
W15-2705,ruppenhofer-rehbein-2012-yes,0,0.169135,"to lexical, proposition-level, and discourse-level semantic factors. Besides improved classification performance, especially for difficult sense distinctions, closer examination of interpretable feature sets allows us to obtain a better understanding of relevant semantic and contextual factors in modal sense classification. 1 (2) a. Geez, Buddha must be so annoyed! (epistemic – possibility) b. We must have clear European standards. (deontic – permission/request) c. She can’t even read them. (dynamic – ability) Modal sense tagging is typically framed as a supervised classification task, as in Ruppenhofer and Rehbein (2012), who manually annotated the modal verbs must, may, can, could, shall and should in the MPQA corpus of Wiebe et al. (2005). The obtained data set comprises 1340 instances. Maximum entropy classifiers trained on this data yield accuracies from 68.7 to 93.5 for the different lexical classifier models. While these accuracies seem high, we note a strong distributional bias in their data set. Due to the small data set size (200-600 instances per modal verb) and its distributional bias, classifiers trained on this corpus are prone to overfitting and hardly beat the majority baseline. Indeed, none of"
W15-2705,J00-4004,0,0.118118,"Missing"
W15-2705,P02-1033,0,0.0343069,". Our annotation reliability is largely comparable. (5) a. He may be home by now. (possibility) b. You may enter this building. (permission) c. May you live 100 years. (wish) (6) a. Vielleicht ist er schon zu Hause. M AYBE IS HE ALREADY AT HOME . b. Es ist gestattet, das Geb¨aude zu betreten. I T IS PERMITTED THE BUILDING TO ENTER c. Hoffentlich werden Sie 100 Jahre. H OPEFULLY BECOME YOU 100 YEARS Capitalizing on the paraphrasing capacity of such expressions, we apply a semi-supervised cross-lingual projection approach, similar to prior work in annotation projection (Yarowsky and Ngai, 2001; Diab and Resnik, 2002): (i) we select a seed set of cross-lingual sense indicating paraphrases, (ii) we extract modal verbs in context that are in direct alignment with one of the seed expressions in word-aligned parallel corpora, and (iii) we project the label of the sense-indicating paraphrase to the aligned modal verb. Experimental setup and annotation scheme. German is our source language, and we project into English. We adopt R&R’s annotation scheme, which is grounded in Kratzer’s modal senses epistemic, deontic and dynamic. While R&R add the novel categories conditional, concessive and optative,1 we subsume t"
W15-2705,tiedemann-2012-parallel,0,0.0413797,"ntic Features for Modal Sense Classification In our work we expand the feature inventory used for modal sense classification to incorporate semantic factors at various levels. An overview of our semantic features is given in Table 1. We define specific feature groups for focused experimental investigation in Section 5. Feature extraction is performed using Stanford’s CoreNLP (Manning et al., 2014) and Stanford parser (Klein and Manning, 2002) to obtain syntactic dependencies. Seed selection. The seeds were manually selected from PPDB (Ganitkevitch et al., 2013) and parallel corpora from OPUS (Tiedemann, 2012). The major criterion, besides frequency of occurrence, was non-ambiguity regarding the modal 2 We split permission and request to make the task more accessible and merged them to deontic later. 3 Cohen’s Kappa, Cohen (1960) 1 Examples: “Should anyone call, please take a message” (conditional), “But, fool though he may be, he is powerful” (concessive), and “Long may she live!” (optative). (R&R) 46 VB: Lexical features of the embedded verb. The embedded verb in the scope of the modal plays an important role in determining modal sense. For instance, with the embedded verb fly in (7.a), we prefer"
W15-2705,P14-2085,1,0.915633,"Missing"
W15-2705,N13-1092,0,0.0399079,"Missing"
W15-2705,J86-2003,0,0.331126,"lex database (Baayen et al., 1996). Lexical semantic features for the subject NP are extracted from WordNet (Fellbaum, 1999). Following Reiter and Frank (2010), we take the most frequent sense of the noun in WN (subject sense0), add the direct hypernym of this sense, the direct hypernym of that hypernym, etc., resulting in features subject sense[1, 2, 3]. We also extract the top sense in the WN hierarchy subject sense top (e.g. entity) and the WN lexical filename (e.g. person). LA: Lexical aspectual class. Verbs can be used in a dynamic or stative sense, e.g. I ate an apple vs. I like apples (Vendler, 1957). The lexical aspect of a verb in context influences modal sense in some cases. In contrast to (4.a), for example, where the eventive verb return triggers the deontic sense, perfect aspect in (10) coerces the clause to stative, triggering the epistemic sense of must. (10) The prisoners must have returned their weapons. TVA: Tense/voice/grammatical aspect features. These features capture tense and grammatical aspect of the embedded verb complex. LA below notes how grammatical aspect influences modal sense. At the same time, tense is an important factor for modal sense disambiguation. (10) clear"
W15-2705,2005.mtsummit-papers.11,0,0.00980443,"Missing"
W15-2705,N01-1026,0,0.0188503,"rent modal verbs was 0.67. Our annotation reliability is largely comparable. (5) a. He may be home by now. (possibility) b. You may enter this building. (permission) c. May you live 100 years. (wish) (6) a. Vielleicht ist er schon zu Hause. M AYBE IS HE ALREADY AT HOME . b. Es ist gestattet, das Geb¨aude zu betreten. I T IS PERMITTED THE BUILDING TO ENTER c. Hoffentlich werden Sie 100 Jahre. H OPEFULLY BECOME YOU 100 YEARS Capitalizing on the paraphrasing capacity of such expressions, we apply a semi-supervised cross-lingual projection approach, similar to prior work in annotation projection (Yarowsky and Ngai, 2001; Diab and Resnik, 2002): (i) we select a seed set of cross-lingual sense indicating paraphrases, (ii) we extract modal verbs in context that are in direct alignment with one of the seed expressions in word-aligned parallel corpora, and (iii) we project the label of the sense-indicating paraphrase to the aligned modal verb. Experimental setup and annotation scheme. German is our source language, and we project into English. We adopt R&R’s annotation scheme, which is grounded in Kratzer’s modal senses epistemic, deontic and dynamic. While R&R add the novel categories conditional, concessive and"
W15-2705,P14-5010,0,0.00398233,"Kratzer’s modal senses epistemic, deontic and dynamic. While R&R add the novel categories conditional, concessive and optative,1 we subsume the former two as cases of epistemic and optative as a subtype of deontic. 4 Semantic Features for Modal Sense Classification In our work we expand the feature inventory used for modal sense classification to incorporate semantic factors at various levels. An overview of our semantic features is given in Table 1. We define specific feature groups for focused experimental investigation in Section 5. Feature extraction is performed using Stanford’s CoreNLP (Manning et al., 2014) and Stanford parser (Klein and Manning, 2002) to obtain syntactic dependencies. Seed selection. The seeds were manually selected from PPDB (Ganitkevitch et al., 2013) and parallel corpora from OPUS (Tiedemann, 2012). The major criterion, besides frequency of occurrence, was non-ambiguity regarding the modal 2 We split permission and request to make the task more accessible and merged them to deontic later. 3 Cohen’s Kappa, Cohen (1960) 1 Examples: “Should anyone call, please take a message” (conditional), “But, fool though he may be, he is powerful” (concessive), and “Long may she live!” (opt"
W15-2705,loaiciga-etal-2014-english,0,\N,Missing
W15-3703,D07-1115,0,0.0295748,"f Tang poetry is an active subfield in Chinese philology, with a vast literature (Watson, 1971; Kao and Mei, 1971; Kao and Mei, 1978). In this paper, we seek to analyze the sentiment (i.e., 1 Although these lexicons are for contemporary Chinese, some words keep the same meaning and polarity as in classical Chinese poetry. 15 Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 15–24, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing 2006; Kaji and Kitsuregawa, 2007; Kiritchenko et al., 2014) and dictionary-based approaches (Kamps et al., 2004; Esuli and Sebastiani, 2005; Mohammad et al., 2009; Baccianella et al., 2010). Unlike previous graph-based approaches which create sentiment lexicons based on existing lexical resources (e.g., WordNet, thesauri) (Takamura et al., 2005; Rao and avichandran, 2009; Hassan et al., 2011), there are no such lexical resources for classical Chinese poetry. Therefore, we choose a corpus-based approach. While our approach for building sentiment lexicons is domain independent, in this paper we apply it to classical Chinese po"
W15-3703,kamps-etal-2004-using,0,0.211986,"Missing"
W15-3703,W06-1642,0,0.122352,"Missing"
W15-3703,W09-4104,0,0.488242,"Missing"
W15-3703,P11-2104,0,0.0164112,"Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 15–24, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing 2006; Kaji and Kitsuregawa, 2007; Kiritchenko et al., 2014) and dictionary-based approaches (Kamps et al., 2004; Esuli and Sebastiani, 2005; Mohammad et al., 2009; Baccianella et al., 2010). Unlike previous graph-based approaches which create sentiment lexicons based on existing lexical resources (e.g., WordNet, thesauri) (Takamura et al., 2005; Rao and avichandran, 2009; Hassan et al., 2011), there are no such lexical resources for classical Chinese poetry. Therefore, we choose a corpus-based approach. While our approach for building sentiment lexicons is domain independent, in this paper we apply it to classical Chinese poetry. This is not a trivial task. There are a variety of reliable resources for English sentiment analysis. However, only a few sentiment lexicons for Chinese are available. In particular, these lexicons are for contemporary Chinese. Moreover, given that these lexicons are developed for contemporary Chinese, they will only have partial coverage for classical Ch"
W15-3703,ku-etal-2006-tagging,0,0.0206343,"atively, wij can be viewed as the probability of lexical item vj occurring nearby, given the lexical item vi . As a result, we construct a lexical network containing 8656 lexical items (4779 are singlecharacter items, 3877 are two-character items) and 8,832,234 edges. This lexical network contains the word co-occurrence information in the Complete Anthology of Tang Poetry. 3.3 4.1 Intrinsic Evaluation Test Datasets. To evaluate our approach, we compile two test datasets. The first dataset (SentiLexicon) contains 933 sentiment words taken from three Chinese sentiment lexicons: HowNet3 , NTUSD (Ku et al., 2006), and Tsinghua sentiment lexicon4 . Although these lexicons are for contemporary Chinese, some words keep the same meaning and polarity as in classical Chinese poetry. We merge these three lexicons by removing duplicate or contradictory entries. This yields a big sentiment lexicon containing 12,945 positive words5 and 17,114 negative words. We then create SentiLexicon by choosing single-character words and two-character words from the big sentiment lexicon if they do not appear in the set of sentiment seeds (Table 1) and occur at least 50 times6 in the Complete Anthology of Tang Poetry. This l"
W15-3703,W10-4116,0,0.0172277,"sk. There are a variety of reliable resources for English sentiment analysis. However, only a few sentiment lexicons for Chinese are available. In particular, these lexicons are for contemporary Chinese. Moreover, given that these lexicons are developed for contemporary Chinese, they will only have partial coverage for classical Chinese poetry. There might also be divergences due to the change of language over several thousand years. To improve sentiment analysis for Chinese, one line of work seeks to leverage rich English sentiment resources through machine translation (Wan, 2008; Wan, 2009; He et al., 2010). These approaches depend on the quality of machine translation and translation of classical Chinese poetry to English is hard even for professional translators. Our work is similar to Zagibalov and Carroll (2008) in the sense that both approaches are weakly supervised. They build a sentiment lexicon iteratively, starting from a small set of seed items and several lexical patterns (negated adverbial constructions) which can indicate lexical polarity. However, such lexical patterns (e.g., 不 (not) 很 (quite) + 满意 (satisfied) (target word) ) are not applicable in classical Chinese poetry. is the s"
W15-3703,P13-1160,0,0.0272878,"rry out an extrinsic evaluation to judge whether our sentiment lexicon can be utilized to analyze sentiment orientation of classical Chinese 19 – Draw the document’s sentiment distribution πd ∼ Dirichlet(γ) – For each sentiment label l, draw a topic distribution θd,l ∼ Dirichlet(α) – For each couplet, 1. choose a sentiment label li ∼ M ultinomial(πd ) 2. choose a topic zi ∼ M ultinomial(θd,li ) 3. generate words w ∼ ϕli ,zi LDA has been extended to capture correlations between sentiment and topic from textual data (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009; He et al., 2011; Lazaridou et al., 2013; Li et al., 2013). Here we modify a joint sentiment-topic model (JST) (Lin and He, 2009) to extract topics associated with positive/negative sentiment. Lin and He (2009) assume that topics are generated dependent on sentiment distributions and words are generated conditioned on the sentiment-topic pairs. JST can detect sentiment and topics simultaneously by encoding word prior sentiment information. However, words in the JST model are position-unaware, i.e., words from the same sentence/clause thus can have different topics or sentiments. We modify the JST model by assuming that lexical items"
W15-3703,P11-1013,0,0.0751471,"Missing"
W15-3703,N12-1020,0,0.0295597,"themes which are tightly associated with positive/negative sentiment. Our model builds in specific assumptions that characterize sentiment expression in classical Chinese poetry. It assumes that lexical items from the same region are generated from a single sentiment-topic pair. We compare sentiment among different famous poets and show that our results are in accordance with studies in Chinese philology. Computational analysis of classical Chinese poetry. There has been previous work focusing on classical Chinese poetry generation (Zhou et al., 2010; He et al., 2012; Zhang and Lapata, 2014). Lee and Kong (2012) develop a dependency treebank for the Complete Anthology of Tang Poetry. On the basis of this corpus, Lee and Tak-sum (2012) quantitatively analyze the semantic content and word usage in the Complete Anthology of Tang Poetry. Voigt and Jurafsky (2013) find that the classical characters of Chinese poetry decreased across the century by comparing classical The poetry sentiment lexicon described in the paper as well as all test datasets are freely available at http://www.cl.uni-heidelberg. de/˜hou/resources.mhtml. 2 Related Work Sentiment lexicons. In recent years, considerable attention has bee"
W15-3703,C12-2061,0,0.0362197,"terize sentiment expression in classical Chinese poetry. It assumes that lexical items from the same region are generated from a single sentiment-topic pair. We compare sentiment among different famous poets and show that our results are in accordance with studies in Chinese philology. Computational analysis of classical Chinese poetry. There has been previous work focusing on classical Chinese poetry generation (Zhou et al., 2010; He et al., 2012; Zhang and Lapata, 2014). Lee and Kong (2012) develop a dependency treebank for the Complete Anthology of Tang Poetry. On the basis of this corpus, Lee and Tak-sum (2012) quantitatively analyze the semantic content and word usage in the Complete Anthology of Tang Poetry. Voigt and Jurafsky (2013) find that the classical characters of Chinese poetry decreased across the century by comparing classical The poetry sentiment lexicon described in the paper as well as all test datasets are freely available at http://www.cl.uni-heidelberg. de/˜hou/resources.mhtml. 2 Related Work Sentiment lexicons. In recent years, considerable attention has been given to the creation of large polarity (positive and negative) lexicons, including various corpus-based approaches (Turney"
W15-3703,D13-1199,0,0.0311701,"luation to judge whether our sentiment lexicon can be utilized to analyze sentiment orientation of classical Chinese 19 – Draw the document’s sentiment distribution πd ∼ Dirichlet(γ) – For each sentiment label l, draw a topic distribution θd,l ∼ Dirichlet(α) – For each couplet, 1. choose a sentiment label li ∼ M ultinomial(πd ) 2. choose a topic zi ∼ M ultinomial(θd,li ) 3. generate words w ∼ ϕli ,zi LDA has been extended to capture correlations between sentiment and topic from textual data (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009; He et al., 2011; Lazaridou et al., 2013; Li et al., 2013). Here we modify a joint sentiment-topic model (JST) (Lin and He, 2009) to extract topics associated with positive/negative sentiment. Lin and He (2009) assume that topics are generated dependent on sentiment distributions and words are generated conditioned on the sentiment-topic pairs. JST can detect sentiment and topics simultaneously by encoding word prior sentiment information. However, words in the JST model are position-unaware, i.e., words from the same sentence/clause thus can have different topics or sentiments. We modify the JST model by assuming that lexical items from the same cou"
W15-3703,P09-1027,0,0.0380987,"trivial task. There are a variety of reliable resources for English sentiment analysis. However, only a few sentiment lexicons for Chinese are available. In particular, these lexicons are for contemporary Chinese. Moreover, given that these lexicons are developed for contemporary Chinese, they will only have partial coverage for classical Chinese poetry. There might also be divergences due to the change of language over several thousand years. To improve sentiment analysis for Chinese, one line of work seeks to leverage rich English sentiment resources through machine translation (Wan, 2008; Wan, 2009; He et al., 2010). These approaches depend on the quality of machine translation and translation of classical Chinese poetry to English is hard even for professional translators. Our work is similar to Zagibalov and Carroll (2008) in the sense that both approaches are weakly supervised. They build a sentiment lexicon iteratively, starting from a small set of seed items and several lexical patterns (negated adverbial constructions) which can indicate lexical polarity. However, such lexical patterns (e.g., 不 (not) 很 (quite) + 满意 (satisfied) (target word) ) are not applicable in classical Chines"
W15-3703,D09-1063,0,0.0213947,"In this paper, we seek to analyze the sentiment (i.e., 1 Although these lexicons are for contemporary Chinese, some words keep the same meaning and polarity as in classical Chinese poetry. 15 Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 15–24, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing 2006; Kaji and Kitsuregawa, 2007; Kiritchenko et al., 2014) and dictionary-based approaches (Kamps et al., 2004; Esuli and Sebastiani, 2005; Mohammad et al., 2009; Baccianella et al., 2010). Unlike previous graph-based approaches which create sentiment lexicons based on existing lexical resources (e.g., WordNet, thesauri) (Takamura et al., 2005; Rao and avichandran, 2009; Hassan et al., 2011), there are no such lexical resources for classical Chinese poetry. Therefore, we choose a corpus-based approach. While our approach for building sentiment lexicons is domain independent, in this paper we apply it to classical Chinese poetry. This is not a trivial task. There are a variety of reliable resources for English sentiment analysis. However, only a few se"
W15-3703,E09-1077,0,0.0296757,"the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 15–24, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing 2006; Kaji and Kitsuregawa, 2007; Kiritchenko et al., 2014) and dictionary-based approaches (Kamps et al., 2004; Esuli and Sebastiani, 2005; Mohammad et al., 2009; Baccianella et al., 2010). Unlike previous graph-based approaches which create sentiment lexicons based on existing lexical resources (e.g., WordNet, thesauri) (Takamura et al., 2005; Rao and avichandran, 2009; Hassan et al., 2011), there are no such lexical resources for classical Chinese poetry. Therefore, we choose a corpus-based approach. While our approach for building sentiment lexicons is domain independent, in this paper we apply it to classical Chinese poetry. This is not a trivial task. There are a variety of reliable resources for English sentiment analysis. However, only a few sentiment lexicons for Chinese are available. In particular, these lexicons are for contemporary Chinese. Moreover, given that these lexicons are developed for contemporary Chinese, they will only have partial cov"
W15-3703,C08-1135,0,0.0370467,"e. Moreover, given that these lexicons are developed for contemporary Chinese, they will only have partial coverage for classical Chinese poetry. There might also be divergences due to the change of language over several thousand years. To improve sentiment analysis for Chinese, one line of work seeks to leverage rich English sentiment resources through machine translation (Wan, 2008; Wan, 2009; He et al., 2010). These approaches depend on the quality of machine translation and translation of classical Chinese poetry to English is hard even for professional translators. Our work is similar to Zagibalov and Carroll (2008) in the sense that both approaches are weakly supervised. They build a sentiment lexicon iteratively, starting from a small set of seed items and several lexical patterns (negated adverbial constructions) which can indicate lexical polarity. However, such lexical patterns (e.g., 不 (not) 很 (quite) + 满意 (satisfied) (target word) ) are not applicable in classical Chinese poetry. is the symbol of beauty, love and rectitude. We show that our method outperforms the very competitive PMI-based approach when evaluating on both datasets (Section 4.1). Our method also outperforms the baseline on an extri"
W15-3703,P05-1017,0,0.0560387,"try. 15 Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 15–24, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing 2006; Kaji and Kitsuregawa, 2007; Kiritchenko et al., 2014) and dictionary-based approaches (Kamps et al., 2004; Esuli and Sebastiani, 2005; Mohammad et al., 2009; Baccianella et al., 2010). Unlike previous graph-based approaches which create sentiment lexicons based on existing lexical resources (e.g., WordNet, thesauri) (Takamura et al., 2005; Rao and avichandran, 2009; Hassan et al., 2011), there are no such lexical resources for classical Chinese poetry. Therefore, we choose a corpus-based approach. While our approach for building sentiment lexicons is domain independent, in this paper we apply it to classical Chinese poetry. This is not a trivial task. There are a variety of reliable resources for English sentiment analysis. However, only a few sentiment lexicons for Chinese are available. In particular, these lexicons are for contemporary Chinese. Moreover, given that these lexicons are developed for contemporary Chinese, they"
W15-3703,D14-1074,0,0.1809,"t-topic model to extract themes which are tightly associated with positive/negative sentiment. Our model builds in specific assumptions that characterize sentiment expression in classical Chinese poetry. It assumes that lexical items from the same region are generated from a single sentiment-topic pair. We compare sentiment among different famous poets and show that our results are in accordance with studies in Chinese philology. Computational analysis of classical Chinese poetry. There has been previous work focusing on classical Chinese poetry generation (Zhou et al., 2010; He et al., 2012; Zhang and Lapata, 2014). Lee and Kong (2012) develop a dependency treebank for the Complete Anthology of Tang Poetry. On the basis of this corpus, Lee and Tak-sum (2012) quantitatively analyze the semantic content and word usage in the Complete Anthology of Tang Poetry. Voigt and Jurafsky (2013) find that the classical characters of Chinese poetry decreased across the century by comparing classical The poetry sentiment lexicon described in the paper as well as all test datasets are freely available at http://www.cl.uni-heidelberg. de/˜hou/resources.mhtml. 2 Related Work Sentiment lexicons. In recent years, considera"
W15-3703,P08-1036,0,0.0555357,"xtensively to discover topics from text. Recently, We also carry out an extrinsic evaluation to judge whether our sentiment lexicon can be utilized to analyze sentiment orientation of classical Chinese 19 – Draw the document’s sentiment distribution πd ∼ Dirichlet(γ) – For each sentiment label l, draw a topic distribution θd,l ∼ Dirichlet(α) – For each couplet, 1. choose a sentiment label li ∼ M ultinomial(πd ) 2. choose a topic zi ∼ M ultinomial(θd,li ) 3. generate words w ∼ ϕli ,zi LDA has been extended to capture correlations between sentiment and topic from textual data (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009; He et al., 2011; Lazaridou et al., 2013; Li et al., 2013). Here we modify a joint sentiment-topic model (JST) (Lin and He, 2009) to extract topics associated with positive/negative sentiment. Lin and He (2009) assume that topics are generated dependent on sentiment distributions and words are generated conditioned on the sentiment-topic pairs. JST can detect sentiment and topics simultaneously by encoding word prior sentiment information. However, words in the JST model are position-unaware, i.e., words from the same sentence/clause thus can have different topics or sentime"
W15-3703,W13-1403,0,0.106724,"om a single sentiment-topic pair. We compare sentiment among different famous poets and show that our results are in accordance with studies in Chinese philology. Computational analysis of classical Chinese poetry. There has been previous work focusing on classical Chinese poetry generation (Zhou et al., 2010; He et al., 2012; Zhang and Lapata, 2014). Lee and Kong (2012) develop a dependency treebank for the Complete Anthology of Tang Poetry. On the basis of this corpus, Lee and Tak-sum (2012) quantitatively analyze the semantic content and word usage in the Complete Anthology of Tang Poetry. Voigt and Jurafsky (2013) find that the classical characters of Chinese poetry decreased across the century by comparing classical The poetry sentiment lexicon described in the paper as well as all test datasets are freely available at http://www.cl.uni-heidelberg. de/˜hou/resources.mhtml. 2 Related Work Sentiment lexicons. In recent years, considerable attention has been given to the creation of large polarity (positive and negative) lexicons, including various corpus-based approaches (Turney and Littman, 2003; Kanayama and Nasukawa, 16 ank vector R over G can be calculated as follows: poetry and contemporary prose."
W15-3703,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W15-3703,E09-1005,0,\N,Missing
W15-3703,D08-1058,0,\N,Missing
W16-1613,ide-etal-2008-masc,0,0.0321864,"Missing"
W16-1613,D14-1181,0,0.0423291,"de senses of the target words. A CNN for modal sense classification We aim at a NN approach to MSC that (i) improves over existing feature-based classifiers, (ii) alleviates manual crafting of features, (iii) generalizes over various text genres, and (iv) is easily portable to novel languages. Besides this, MSC is a special kind of WSD, in that modal verbs have a restricted sense inventory shared across languages, and act as operators that take a full proposition as argument. We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al. (2011). Unlike Kim (2014) we will use only one channel, but experiment with various types of word vectors. A CNN represents a sentence with a fixed size vector, passed to classifier to classify the sentence into task-specific target categories. In our case, it will classify sentences into three modal sense categories. The input layer is a matrix x ∈ Rs×d , with each row corresponding to a d-dimensional word embedding xi ∈ Rd of a word in the sentence of length s. Word embeddings can be randomly initialized or pre-trained vectors, e.g. word2vec (Mikolov et al., 2"
W16-1613,P14-1062,0,0.0659716,"t returns a vector usually referred to as a feature map. Formally, let xi−n+1:i be the submatrix of the input matrix x from the (i−n+1)-th row to the i-th row and let h. , .iF denote the sum of elements of the component-wise inner product of two matrices, known as Frobenius inner product. The i-th component of the feature map c is obtained by taking the Frobenius inner product of the sub-matrix xi−n+1:i with the filter matrix w Sentence classification using CNNs. Recent work investigates NN architectures and their ability to capture the semantics of sentences for various classification tasks. Kalchbrenner et al. (2014) construct a dynamic CNN that builds on unparsed input and achieves performance beyond strong baselines for sentiment and question type classification. By contrast, recursive neural networks (Socher et al., 2013) take parsed input, recursively generate representations for intermediate phrases, and perform classification on the basis of the full sentence representation. Kim (2014) evaluates a one-layer CNN on various benchmark tasks for sentence classification. CNNs trained on pre-trained (static) embeddings perform well and can be further improved by tuning them to the task (non-static). Using"
W16-1613,D15-1189,0,0.0687734,"fication (MSC) is a special WSD task that depends on the meaning of the proposition in the modal’s scope. We explore a CNN architecture for classifying modal sense in English and German. We show that CNNs are superior to manually designed feature-based classifiers and a standard NN classifier. We analyze the feature maps learned by the CNN and identify known and previously unattested linguistic features. We benchmark the CNN on a standard WSD task, where it compares favorably to models using sense-disambiguated target vectors. 1 Introduction 2 Factuality recognition (de Marneffe et al., 2012; Lee et al., 2015) is a subtask in information extraction that differentiates facts from hypotheses and speculation, expressed through signals of modality, most prominently, modal verbs and adverbs. Modal verbs are, however, ambiguous between an epistemic sense (possibility) as opposed to nonepistemic deontic (permission/obligation) or dynamic (capability) senses, as in: He could be at home (epistemic), You can enter now (deontic) and Only John can solve this problem (capability). Modal sense classification (MSC) is a special case of sense disambiguation that is also relevant in areas of dialogue act and plan r"
W16-1613,baker-etal-2010-modality,0,0.104382,"and Rehbein (2012) and follow-up work in Zhou et al. (2015) (henceforth, R&R and Z+). R&R induced modal sense classifiers from manual annotations on the MPQA corpus (Wiebe et al., 2005) using word-based and syntactic features. Z+ propose an extended semantically informed model that significantly outperforms R&R’s results. Z+ also create heuristically sense-annotated training data from parallel corpora, to overcome sparsity and bias in the MPQA corpus. However, their models do not beat the majority sense baseline for the difficult modal verbs, may, can and could. 1 These senses correspond to (Baker et al., 2010)’s modal categories (with deontic split into requirement and permissive), and R&Rs inventory, with regrouping of concessive, conditional and circumstantial, cf. Zhou et al. (2015). 111 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 111–120, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Modal sense classification interacts with genre and domain differences. Prabhakaran et al. (2012) observe strong cross-genre effects and missing generalization capacities when applying their modality classifier to out-of-domain genres. nels did no"
W16-1613,P14-2050,0,0.0479701,"t al. (2011). Unlike Kim (2014) we will use only one channel, but experiment with various types of word vectors. A CNN represents a sentence with a fixed size vector, passed to classifier to classify the sentence into task-specific target categories. In our case, it will classify sentences into three modal sense categories. The input layer is a matrix x ∈ Rs×d , with each row corresponding to a d-dimensional word embedding xi ∈ Rd of a word in the sentence of length s. Word embeddings can be randomly initialized or pre-trained vectors, e.g. word2vec (Mikolov et al., 2013) or dependency-based (Levy and Goldberg, 2014) embeddings. Based on the input layer, a CNN builds up one or more convolutional layers. A convolution is an operation between sub-matrices of the input matrix x ∈ Rs×d and a filter parametrised by a weight matrix w ∈ Rn×d , that returns a vector usually referred to as a feature map. Formally, let xi−n+1:i be the submatrix of the input matrix x from the (i−n+1)-th row to the i-th row and let h. , .iF denote the sum of elements of the component-wise inner product of two matrices, known as Frobenius inner product. The i-th component of the feature map c is obtained by taking the Frobenius inner"
W16-1613,W04-0807,0,0.0574956,"ing context: embedding predicates, subjects, if clauses, etc. 5 Snaive -prod S-cosine 62.20 60.50 S-prod S-raw 64.30 63.10 CNN 66.50 Table 6: WSD accuracy on SensEval-3 dataset. sense prediction, they used the following feature vectors that are fed into a linear SVM classifier: Word sense disambiguation S-cosine = hcos(c, s(1) ), . . . , cos(c, s(k) )i , (1) (k) (k) S-product = hc1 s1 , . . . , cn s(1) n , . . . , c1 s1 , . . . , cn sn i , Next to modal sense classification, we evaluate our CNN model in a classical WSD task. As benchmark corpus we chose the SensEval-3 lexical sample data set (Mihalcea et al., 2004), which was recently applied in Rothe and Sch¨utze (2015) (henceforth R&S) and Taghipour and Ng (2015), using sense-specific embeddings and a NN architecture, respectively (cf. Section 2). The training data size for the 57 target word types ranges from 14 to 263 instances. Sense labels of test instances of a given target word are predicted using the CNN model trained on the training instances for the respective word type.22 We set the CNN hyperparameters to be the same as for MSC, except for mini-batch size and region sizes. Since the training data for some words is below 50 instances, mini-ba"
W16-1613,W12-3807,0,0.29472,"Missing"
W16-1613,P15-1173,0,0.0415895,"Missing"
W16-1613,ruppenhofer-rehbein-2012-yes,0,0.21715,"nd speculation, expressed through signals of modality, most prominently, modal verbs and adverbs. Modal verbs are, however, ambiguous between an epistemic sense (possibility) as opposed to nonepistemic deontic (permission/obligation) or dynamic (capability) senses, as in: He could be at home (epistemic), You can enter now (deontic) and Only John can solve this problem (capability). Modal sense classification (MSC) is a special case of sense disambiguation that is also relevant in areas of dialogue act and plan recognition in AI, as well as novel tasks such as argumentation mining. Prior work (Ruppenhofer and Rehbein, 2012; Zhou et al., 2015) addressed the task with featurebased classification. However, even with carefully designed semantic features the models have difficulties beating the majority sense baseline in cases of difficult sense distinctions and when applying the models to heterogenous text genres. Prior and related work Modal sense classification (MSC). We focus on disambiguation of modal verbs, adopting the sense inventory established in formal semantics: epistemic, deontic/bouletic and circumstantial/dynamic.1 We compare to prior work in Ruppenhofer and Rehbein (2012) and follow-up work in Zhou e"
W16-1613,D13-1170,0,0.00387527,"t-wise inner product of two matrices, known as Frobenius inner product. The i-th component of the feature map c is obtained by taking the Frobenius inner product of the sub-matrix xi−n+1:i with the filter matrix w Sentence classification using CNNs. Recent work investigates NN architectures and their ability to capture the semantics of sentences for various classification tasks. Kalchbrenner et al. (2014) construct a dynamic CNN that builds on unparsed input and achieves performance beyond strong baselines for sentiment and question type classification. By contrast, recursive neural networks (Socher et al., 2013) take parsed input, recursively generate representations for intermediate phrases, and perform classification on the basis of the full sentence representation. Kim (2014) evaluates a one-layer CNN on various benchmark tasks for sentence classification. CNNs trained on pre-trained (static) embeddings perform well and can be further improved by tuning them to the task (non-static). Using two chanci = hxi−n+1:i ,wiF , (1) for i ∈ {n, . . . , s}3 . Afterwards, we add a bias term, b ∈ R to every component of the feature map and apply an activation function f , 2 Modal verbs are not or not systemati"
W16-1613,N15-1035,0,0.0765261,"earning for NLP, pages 111–120, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Modal sense classification interacts with genre and domain differences. Prabhakaran et al. (2012) observe strong cross-genre effects and missing generalization capacities when applying their modality classifier to out-of-domain genres. nels did not significantly improve results. Overall, the CNNs show consistently strong performance, improving on state-of-the-art results in 4 out of 7 tasks, i.a., sentiment and opinion classification. 3 Word Embeddings and Sense Disambiguation. Taghipour and Ng (2015) investigate the impact of word embeddings on classical WSD, using pretrained embeddings and tuning them to the task using a NN. Both variants, integrated into the stateof-the-art system IMS (Zhong and Ng, 2012), improve WSD performance on benchmark tasks. Ordinary word embeddings do not differentiate word senses. Rothe and Sch¨utze (2015) explore supervised WSD using sense-specific embeddings, which they induce by exploiting sense encodings and constraints given by a lexical resource.2 Integrating the sense-specific vectors into IMS yields significant improvements and small gains relative to"
W16-1613,tiedemann-2012-parallel,0,0.0248295,"iverse linguistic features, and offers greater flexibility compared to a conventional WSD model with a fixed window size centered around the target word. To investigate these special properties of the CNN model, we test it on English and German data. While in English, subject, modal and embedded verb are in a close syntactic context, in German, they can be distributed over wider distances, and the feature maps are expected to capture properties over wider distances. 3) EPOSG Following the method of Z+, we constructed a German data set EPOSG from the Europarl and OpenSubtitles corpora of OPUS (Tiedemann, 2012) by projecting modal sense categories from English to German, using selected modal sense identifying English paraphrases. The resulting corpus with sense-tagged German modal verbs 4 4.1 Modal sense classification Data 2) MASC A subset of the multi-genre corpus MASC (Ide et al., 2008), consisting of 19 genres was manually annotated (Anonymous) with modal senses for the same modal verbs. The annotated data consists of ≈100 instances for each genre.6 4 More detailed information will be provided through accompanying material with the final version. The annotated MASC and EPOSG data sets will be ma"
W16-1613,P10-4014,0,0.0333296,"models of R&S using AutoExtend embeddings for WSD. It achieves slightly higher results without explicitly marking the target word, whereas the AutoExtend embeddings encode much richer information: what is the target word, how many possible sense it has, and knowledge-intense sense embeddings for each of its synsets. The CNN is able to compete with the rich AutoExtend model, and future work needs to investigate whether – similar to the S-product setting in R&S – the CNN model can achieve competitive state-of-the-art results by incorporating features corresponding to those of the IMS system of Zhong and Ng (2010). 6 Conclusion and future work We presented an account for multilingual modal sense classification using a CNN architecture. We apply the same architecture in a standard WSD task and achieve competitive results compared to a system using richer embedding information. Our one-layer CNN architecture outperforms strong baselines and prior art for MSC in English, including a NN and MaxEnt model, and proves particularly robust in cross-genre classification. We applied the CNN model to German, on a data set of modest size, obtained using crosslingual projection techniques. The CNN-G classifier outpe"
W16-1613,P12-1029,0,0.0156344,"observe strong cross-genre effects and missing generalization capacities when applying their modality classifier to out-of-domain genres. nels did not significantly improve results. Overall, the CNNs show consistently strong performance, improving on state-of-the-art results in 4 out of 7 tasks, i.a., sentiment and opinion classification. 3 Word Embeddings and Sense Disambiguation. Taghipour and Ng (2015) investigate the impact of word embeddings on classical WSD, using pretrained embeddings and tuning them to the task using a NN. Both variants, integrated into the stateof-the-art system IMS (Zhong and Ng, 2012), improve WSD performance on benchmark tasks. Ordinary word embeddings do not differentiate word senses. Rothe and Sch¨utze (2015) explore supervised WSD using sense-specific embeddings, which they induce by exploiting sense encodings and constraints given by a lexical resource.2 Integrating the sense-specific vectors into IMS yields significant improvements and small gains relative to Taghipour and Ng (2015). Hence, word embeddings – tuned to the task or sensespecific – prove beneficial for supervised WSD. The CNN approach we investigate in our work does not employ a fixed feature space or a"
W16-2108,D14-1181,0,0.00273858,"Our evaluation needs to take into account that many labels underlie a skewed distribution (cf. Table 4). For example, 8 One of the authors, with experience in history sciences, annotated the data. In future work we plan to obtain comparable annotations possibly with help of experts in online history forums. 9 Our data set and further details of experimental settings are available at https://cl.uni-heidelberg.de/ ˜opitz/ri/ Using Convolutional NNs Recently, CNNs have been successfully applied to various text and semantic sentence classification tasks, and often achieved very good performance (Kim, 2014; Zhang et al., 2015). Since 77 consider that one label only is positive among 100 test samples. A classifier that labels all instances as negative yields a deceivingly high score of 0.99 accuracy. Hence we employ Balanced Accuracy, the mean of Recall (Sensitivity) and inverse Recall (Specificity10 ), defined as Accbal = Sensitivity+Specif icity . 2 In the above example, where Accuracy yields a biased score of almost one, balanced Accuracy yields a more realistic value of 0.5. Given the unbalanced distribution of our test data set, we report balanced accuracy for each of the 12 binary problems"
W16-2803,E12-1027,0,0.0146156,", c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular elements in the argumenta"
W16-2803,W13-2707,0,0.0262467,"text type of the passage. For more detail see Section 2. Furthermore, SE types are recognizable (and annotatable) through a combination of linguistic features of the clause and its main verb, and models have recently been released for their automatic classification (Friedrich et al., 2016). Our approach is the first we know of to link clause type to argumentative structure, although features of the verb have been widely used in previous work for classifying argumentative vs. nonargumentative sentences. For example, Moens et al. (2007) include verb lemmas and modal auxiliaries as features, and Florou et al. (2013) find that, for Greek web texts related to public policy issues, tense and mood features of verbal constructions are helpful for determining the role of the sentences within argumentative structures. Our analysis is performed on German texts. Taking the argumentative microtext corpus (Peldszus and Stede, 2015a) as a set of prototypical argumentative text passages, we annotated each clause with its SE type (Section 3).1 In this way we are able to investigate which SE types are most prevalent in argumentative texts and, further, to link the clause Argumentative texts have been thoroughly analyze"
W16-2803,W14-4921,1,0.94697,"texts and clause types Maria Becker, Alexis Palmer, and Anette Frank Leibniz ScienceCampus Empirical Linguistics and Computational Language Modeling Department of Computational Linguistics, Heidelberg University Institute of German Language, Mannheim {mbecker,palmer,frank}@cl.uni-heidelberg.de Abstract for classifying the argumentative functions served by premises. Specifically, this is an empirical investigation of the semantic types of clauses found in argumentative text passages, using the inventory of clause types developed by Smith (2003) and extended in later work (Palmer et al., 2007; Friedrich and Palmer, 2014). Situation entity (SE) types describe how clauses behave in discourse, and as such they are aspectual rather than ontological categories. Individual clauses of text evoke different types of situations (for example, states, events, generics, or habituals), and the situations evoked in a text passage are linked to the text type of the passage. For more detail see Section 2. Furthermore, SE types are recognizable (and annotatable) through a combination of linguistic features of the clause and its main verb, and models have recently been released for their automatic classification (Friedrich et a"
W16-2803,D15-1110,0,0.402966,"s the first we know of to link clause type to argumentative structure, although features of the verb have been widely used in previous work for classifying argumentative vs. nonargumentative sentences. For example, Moens et al. (2007) include verb lemmas and modal auxiliaries as features, and Florou et al. (2013) find that, for Greek web texts related to public policy issues, tense and mood features of verbal constructions are helpful for determining the role of the sentences within argumentative structures. Our analysis is performed on German texts. Taking the argumentative microtext corpus (Peldszus and Stede, 2015a) as a set of prototypical argumentative text passages, we annotated each clause with its SE type (Section 3).1 In this way we are able to investigate which SE types are most prevalent in argumentative texts and, further, to link the clause Argumentative texts have been thoroughly analyzed for their argumentative structure, and recent efforts aim at their automatic classification. This work investigates linguistic properties of argumentative texts and text passages in terms of their semantic clause types. We annotate argumentative texts with Situation Entity (SE) classes, which combine notion"
W16-2803,P15-1123,0,0.171345,"16 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular elements in the argumentation graphs? 3. Do particular clause types cor"
W16-2803,P10-1005,1,0.677539,"ment Mining, pages 21–30, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular e"
W16-2803,P16-1166,1,0.854985,"Palmer, 2014). Situation entity (SE) types describe how clauses behave in discourse, and as such they are aspectual rather than ontological categories. Individual clauses of text evoke different types of situations (for example, states, events, generics, or habituals), and the situations evoked in a text passage are linked to the text type of the passage. For more detail see Section 2. Furthermore, SE types are recognizable (and annotatable) through a combination of linguistic features of the clause and its main verb, and models have recently been released for their automatic classification (Friedrich et al., 2016). Our approach is the first we know of to link clause type to argumentative structure, although features of the verb have been widely used in previous work for classifying argumentative vs. nonargumentative sentences. For example, Moens et al. (2007) include verb lemmas and modal auxiliaries as features, and Florou et al. (2013) find that, for Greek web texts related to public policy issues, tense and mood features of verbal constructions are helpful for determining the role of the sentences within argumentative structures. Our analysis is performed on German texts. Taking the argumentative mi"
W16-2803,J00-4004,0,0.0488191,"o clauses is discussed in Section 3.2. 21 Proceedings of the 3rd Workshop on Argument Mining, pages 21–30, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with re"
W16-2803,W15-2702,1,0.842794,"nden nicht den Arzte-Status” tragen. (GEN) It doesn’t matter after all \ that those who administer the treatment don’t have ’doctor status’. Table 3: Sample microtext (micro b010), both German and English versions, with SE labels. generic, and (c) whether the clause is habitual (a pattern of occurrences), episodic (a fixed number of occurrences), or static (an attribute, characteristic, or quality). Using these feature values in a decision tree has been shown to improve human agreement on the SE type annotation task (Friedrich and Palmer, 2014). annotators and one expert annotator. Following Mavridou et al. (2015), with a modified and translated version of an existing SE annotation manual,5 student annotators were trained on a set of longer texts from different genres, automatically segmented as described above: fiction (47 segments), reports (42 segments), TED talks (50 segAnnotators and annotator training. Each text was annotated by two trained (but novice) student 5 www.coli.uni-saarland.de/projects/ sitent/page.php?id=resources 25 ments), and commentary (127 segments). Inter-annotator agreement. We compute agreement separately for SE type and for the three features introduced above, as shown in Tab"
W16-2803,W13-2313,0,0.0265232,"ust 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular elements in the argumentation graphs? 3. Do"
W16-2803,L16-1167,0,0.0836072,"characterized by an above-average number of G ENERIC S ENTENCES 28 linguistically. Due to the small dataset, our results can be interpreted solely as tendencies which have to be confirmed by more extensive studies in the future. Nonetheless there is some evidence that the observed tendencies can be deployed for automatic recognition and fine-grained classification of argumentative text passages. In addition to the ongoing annotation work which will give us more data to analyze, we intend to cross-match SE types with the newly-available discourse structure annotations for the microtext corpus (Stede et al., 2016). We would additionally explore the role of modal verbs within this intersection of SE type and argument structure status. The end goal of this investigation, of course, is to deploy automatically-labeled SE types as features for argument mining. Mit der BA-Arbeit kann man jedoch die Interessen und die Fachkenntnisse besonders gut zeigen. Schlielich ist man nicht in jedem Fach sehr gut. (G ENERIC S ENTENCE) (Translation: With a BA dissertation one can, however, demonstrate interests and subject matter expertise particularly well. After all one doesn’t excel in every subject.) Es bleibt jedoch"
W16-2803,C14-1002,0,0.0210569,"Missing"
W16-2803,J86-2003,0,0.0978753,"curring events, such as habits of individuals. 4. G ENERIC S ENTENCE (GEN): Birds can fly. / Scientific papers make arguments. 5. G ENERALIZING S ENTENCE (GS): Fei travels to India every year. Theoretical background The phrase situation entity refers to the fact that clauses of text evoke situations within a discourse. For example, the previous sentence describes two situations: (i) the meaning of situation entity, and (ii) what clauses of text do, in general. The second situation is embedded as part of the first. Notions related to SE type have been widely studied in theoretical linguistics (Vendler, 1957; Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher, 1993; Carlson and Pelletier, 1995, among others) and have The next category of SE types is broadly referred to as Abstract Entities. This type of clause presents semantic content in a manner that draws attention to its epistemic status. We focus primarily on a small subset of constructions - factive and propositional predicates with clausal complements. Of course a wide range of linguistic constructions can be used to convey such information, and to address them all would require a comprehensive treament of subjective language. In the examples"
W16-2803,P07-1113,1,0.890231,"Missing"
W16-2803,zarcone-lenci-2008-computational,0,0.0175346,"Section 3.2. 21 Proceedings of the 3rd Workshop on Argument Mining, pages 21–30, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2."
W16-2803,W13-2324,0,\N,Missing
W16-2803,J17-3005,0,\N,Missing
W16-4011,P98-1013,0,0.0649142,"syntactic structures. This includes, but is not limited to the tools mentioned in Section 2. WebAnno 3 was developed and implemented in close coordination with users in the context of an annotation project (cf. M´ujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates, their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts. Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the sense label typically determines the available argument slots. The example below shows an annotation usi"
W16-4011,burchardt-etal-2006-salto,1,0.893566,"Missing"
W16-4011,N13-3004,0,0.131331,"ject in Section 4. While joint WSD and SRL annotations are conveniently supported and facilitated using constraints, they can also be performed separately and independently of one another. 2 Related Work We briefly review presently available annotation tools that could be used for annotating semantic structures. Since we aim to support geographically distributed annotation teams, we consider recent generic webbased annotation tools. Additionally, we examine tools from earlier semantic annotation projects that are specialised for SRL but not web-based. 2.1 Web-based Annotation Tools Anafora by Chen and Styler (2013) is a recent web-based annotation tool for event-like structures. Specifically, it supports the annotation of spans and n-ary relations. Spans are anchored on text while relations exist independently from the text and consist of slots that can be filled with spans. Annotations are visualised using a coloured text background. Selecting a relation highlights the participating spans by placing boxes around them. Anafora is not suited for annotation tasks that require an alignment of the semantic structures with syntactic structures such as constituent or dependency parse trees. brat by Stenetorp"
W16-4011,N10-2004,0,0.0183683,"s with FrameNet categories. Once a frame for a predicate has been selected, applicable roles from FrameNet can be assigned to nodes in the parse tree by drag-and-drop. SALTO supports discontinuous annotations, multi-token annotations, and cross-sentence annotations. It also offers basic team management functionalities including workload assignment and curation. However, annotators cannot correct mistakes in the underlying treebank because the parse tree is not editable. This is problematic for automatically preprocessed input. The final release of SALTO was in 2012. Jubilee and Cornerstone by Choi et al. (2010) are tools for annotating PropBank. Jubilee supports the annotation of PropBank instances, while its sister tool Cornerstone allows editing the frameset XML files that provide the annotation scheme to Jubilee. The user interface (UI) of Jubilee displays a treebank view and allows annotating nodes in the parse tree with frameset senses and roles. Jubilee supports annotation and adjudication of annotations in small teams. It is a Java application that stores all data on the file system. Thus, the annotation team needs to be able to access a common file system, which does not meet our needs for a"
W16-4011,W97-0802,0,0.114672,"since 2014. 2.3 Requirements of Semantic Annotation The annotation of semantic structures imposes two main requirements on annotation tools: 1) the flexibility to support multiple layers of annotation including syntactic and semantic layers using freely configurable annotation schemes and 2) the ability to handle large, interdependent tagsets. Flexible multi-layer annotation. While the usage-driven design of dedicated SRL annotation tools allows for a very efficient annotation, users face a serious lack of flexibility when trying to combine different annotation schemes (e.g. GermaNet senses (Hamp and Feldweg, 1997) and VerbNet roles), or when trying to use data preprocessed in different ways (e.g. for a crowdsourcing approach, automatically pre-annotating predicate and argument spans can be helpful, while experts may find pre-annotated dependency relations beneficial). This is not supported by current web-based annotation tools. Handling rich annotation schemes. Tools need to specifically support rich semantic annotation schemes—like FrameNet—with interdependent labels (i.e. sense labels determine available argument roles). Manually typing sense and role labels is error-prone, and selecting them from a"
W16-4011,L16-1484,1,0.824737,"Missing"
W16-4011,J05-1004,0,0.026006,"ludes, but is not limited to the tools mentioned in Section 2. WebAnno 3 was developed and implemented in close coordination with users in the context of an annotation project (cf. M´ujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates, their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts. Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the sense label typically determines the available argument slots. The example below shows an annotation using FrameNet; the predicate ask r"
W16-4011,E12-2021,0,0.0315475,"Missing"
W16-4011,P13-4001,1,0.642633,"Missing"
W16-4011,P14-5016,1,0.655131,"Missing"
W16-4011,C98-1013,0,\N,Missing
W17-0814,C16-1058,0,0.0139423,"33 145 444 466 456 221 8 216 25 176 165 1,022 1,025 1,017 Table 1: Data statistics for SR3de (PB, VN, FN). Training data generation In this work, we use a corpus-based, monolingual approach to training data expansion. F¨urstenau and Lapata (2012) propose monolingual annotation projection for lowerresourced languages: they create data labeled with FrameNet frames and roles based on a small set of labeled seed sentences in the target language. We apply their approach to the different SRL frameworks, and for the first time to VerbNet-style labels. Other approaches apply cross-lingual projection (Akbik and Li, 2016) or paraphrasing, replacing FrameNet predicates (Pavlick et al., 2015) or PropBank arguments (Woodsend and Lapata, 2014) in labeled texts. We do not employ these approaches, because they assume large role-labeled corpora. 3 dev Datasets and Data Expansion Method SR3de: a German parallel SRL dataset The VerbNet-style dataset by M´ujdricza-Maydt et al. (2016) covers a subset of the PropBank-style CoNLL 2009 annotations, which are based on the German FrameNet-style SALSA corpus. This allowed us to create SR3de, the first corpus with parallel sense and role labels from SALSA, PropBank, and GermaNe"
W17-0814,W09-1206,0,0.0447772,"Missing"
W17-0814,C10-1011,0,0.0161969,"cate lemma and align dependency graphs of seeds and expansions based on lexical similarity of the graph nodes and syntactic similarity of the edges. The alignment is then used to map predicate and role labels from the seed sentences to the expansion sentences. For each seed instance, the k best-scoring expansions are selected. Given a seed set of size n and the maximal number of expansions per seed k, we get up to n · k additional training instances. Lexical and syntactic similarity are balanced using the weight parameter α. Our adjusted re-implementation uses the matetools dependency parser (Bohnet, 2010) and word2vec embeddings (Mikolov et al., 2013) trained on deWAC (Baroni et al., 2009) for word similarity calculation. We tune the parameter α via intrinsic evaluation on the SR3de dev set. We project the seed set SR3de-train directly to SR3dedev and compare the labels from the k=1 best seeds for a dev sentence to the gold label, measuring F1 for all projections. Then we use the best-scoring α value for each framework to project annotations from the SR3de training set to deWAC for predicate lemmas occurring at least 10 times. We vary the number of expansions k, selecting k from {1, 3, 5, 10,"
W17-0814,burchardt-etal-2006-salsa,1,0.888828,"Missing"
W17-0814,J12-1005,0,0.319427,"Missing"
W17-0814,W97-0802,0,0.170745,"style roles using labels A0 and A1 for Agent- and Patient-like roles, and continuing up to A9 for other arguments. Instead of spans, arguments were defined by their dependency heads for CoNLL. The resulting dataset was used as a benchmark dataset in the CoNLL 2009 shared task. For VerbNet, M´ujdricza-Maydt et al. (2016) recently published a small subset of the CoNLL shared task corpus with VerbNet-style roles. It contains 3,500 predicate instances for 275 predicate lemma types. Since there is no taxonomy of verb classes for German corresponding to original VerbNet classes, they used GermaNet (Hamp and Feldweg, 1997) to label predicate senses. GermaNet provides a fine-grained sense inventory similar to the English WordNet (Fellbaum, 1998). Comparison of SRL frameworks Previous experimental work compares VerbNet and PropBank: Zapirain et al. (2008) find that PropBank SRL is more robust than VerbNet SRL, generalizing better to unseen or rare predicates, and relying less on predicate sense. Still, they aspire to use more mean2 Cf. Hajiˇc et al. (2009), Sun et al. (2010), Boas (2009). 116 Automatic SRL systems for German State-ofthe-art SRL systems for German are only available for PropBank labels: Bj¨orkelun"
W17-0814,P09-1033,0,0.491813,"Missing"
W17-0814,L16-1484,1,0.871771,"Missing"
W17-0814,J05-1004,0,0.189767,"te and role labels from the different frameworks and applying the same conditions and criteria for training and testing. Previous work comparing these frameworks either provides theoretical investigations, for instance for the pair PropBank–FrameNet (Ellsworth et al., 2004), or presents experimental investigations for the pair PropBank–VerbNet (Zapirain et al., 2008; Merlo and van der Plas, 2009). Theoretical analyses contrast the richness of the semantic model of FrameNet with efficient annotation of PropBank labels and their suitability for system training. Verb1 See Fillmore et al. (2003), Palmer et al. (2005) and KipperSchuler (2005), respectively. 115 Proceedings of the 11th Linguistic Annotation Workshop, pages 115–121, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Our work explores the generalization properties of three SRL frameworks in a contrastive setup, assessing SRL performance when training and evaluating on a dataset with parallel annotations for each framework in a uniform SRL system architecture. We also explore to what extent the frameworks benefit from training data generation via annotation projection (F¨urstenau and Lapata, 2012). Since all three"
W17-0814,P15-2067,0,0.116907,"Missing"
W17-0814,Q15-1032,0,0.0355932,"Missing"
W17-0814,D14-1045,0,0.156842,"Missing"
W17-0814,C10-1119,0,0.0636362,"Missing"
W17-0814,P08-1063,0,0.650213,"Missing"
W17-0913,P84-1044,0,0.21366,"Missing"
W17-0913,D14-1162,0,0.0889317,"Missing"
W17-0913,P14-5010,0,0.00631006,"2 of Ending1 and Ending2, where L is the token length of the Story sequence. We build the final representation ose1e2 by concatenating the e1 and e2 representations. Finally, for classification we use a softmax layer over the output ose1e2 by mapping it into the target space of the two classes (Good, Bad) using a parameter matrix Mo and bias bo as given in (Eq.1). We train using the cross-entropy loss. Part of speech (POS) based word vector similarities. For each sentence in the given four sentence story and the candidate endings we performed part of speech tagging using the Stanford CoreNLP (Manning et al., 2014) parser, and computed similarities between centroid vectors of words with a specific tag from Story and the centroid vector of Ending. Extracted features for POS similarities include symmetric and asymmetric combinations: for example we calculate the similarity between Nouns from Story with Nouns from Ending and similarity between Nouns from Story with Verbs from Ending and vice versa. The assumption is that embeddings for some parts of speech between Story and Ending might be closer to those of other parts of speech for the Good ending of a given story. 3.2 labelprob = sof tmax(W o ose1e2 + b"
W17-0913,K16-2014,1,0.873306,"connected by a specific semantic relation or some piece of common sense knowledge. Their representation vectors should thus stand in a specific similarity relation to each other. We use their cosine similarity as a feature. Similarity between the story sentences and a candidate ending has already been proposed as a baseline by Mostafazadeh et al. (2016b) but it does not perform well as a single feature. A Baseline Method For tackling the problem of right story ending selection we follow a feature-based classification approach that was previously applied to bi-clausal classification tasks in (Mihaylov and Frank, 2016; Mihaylov and Nakov, 2016). It uses features based on word embeddings to represent the clauses and semantic similarity measured between these representations for the clauses. Here, we adopt this approach to model parts of the story and the candidate endings. For the given Story and the given Maximized similarity. This measure ranks each word in the Story according to its similarity with the centroid vector of Ending, and we compute the average similarity for the top-ranked N 88 4.1 words. We chose the similarity scores of the top 1,2,3 and 5 words as features. Our assumption is that the avera"
W17-0913,S15-2038,0,0.0217221,"Story according to its similarity with the centroid vector of Ending, and we compute the average similarity for the top-ranked N 88 4.1 words. We chose the similarity scores of the top 1,2,3 and 5 words as features. Our assumption is that the average similarity between the Story representation and the top N most similar words in the Ending might characterize the proper ending as the Good ending. We also extract maximized aligned similarity. For each word in Story, we choose the most similar word from the yield of Ending and take the average of all best word pair similarities, as suggested in Tran et al. (2015). We are using the raw LSTM output of the encoder. We also experiment with an encoder with attention to model the relation between a story and a candidate ending, following (Rockt¨aschel et al., 2015). (i) Raw LSTM representations. For each given instance (Story, Ending1, Ending2) we first encode the Story token word vector representations using a recurrent neural network (RNN) with long shortterm memory (LSTM) units. We use the last output hsL and csL states of the Story to initialize the first LSTM cells for the respective encodings e1 and e2 of Ending1 and Ending2, where L is the token leng"
W17-0913,S16-1136,1,0.860376,"emantic relation or some piece of common sense knowledge. Their representation vectors should thus stand in a specific similarity relation to each other. We use their cosine similarity as a feature. Similarity between the story sentences and a candidate ending has already been proposed as a baseline by Mostafazadeh et al. (2016b) but it does not perform well as a single feature. A Baseline Method For tackling the problem of right story ending selection we follow a feature-based classification approach that was previously applied to bi-clausal classification tasks in (Mihaylov and Frank, 2016; Mihaylov and Nakov, 2016). It uses features based on word embeddings to represent the clauses and semantic similarity measured between these representations for the clauses. Here, we adopt this approach to model parts of the story and the candidate endings. For the given Story and the given Maximized similarity. This measure ranks each word in the Story according to its similarity with the centroid vector of Ending, and we compute the average similarity for the top-ranked N 88 4.1 words. We chose the similarity scores of the top 1,2,3 and 5 words as features. Our assumption is that the average similarity between the S"
W17-0913,N13-1090,0,0.0728669,"Missing"
W17-0913,N16-1098,0,0.453703,"act Our contribution is that we set a new baseline for the task, showing that a simple linear model based on distributed representations and semantic similarity features achieves state-of-the-art results. We also evaluate the ability of different embedding models to represent common knowledge required for this task. We present an LSTM-based classifier with different representation encodings that tries to model the relation between the story and alternative endings and argue about its inability to do so. This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of"
W17-0913,W16-2505,0,0.329007,"act Our contribution is that we set a new baseline for the task, showing that a simple linear model based on distributed representations and semantic similarity features achieves state-of-the-art results. We also evaluate the ability of different embedding models to represent common knowledge required for this task. We present an LSTM-based classifier with different representation encodings that tries to model the relation between the story and alternative endings and argue about its inability to do so. This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of"
W17-4117,W13-3520,0,0.0316136,"e not per se a problem for parsing, as long as we are able to learn something about their morphological properties. Table 5: Perplexity for different language models on German texts from Wikipedia. In our experiment, we use the framework1 and setup described in Vania and Lopez (2017) to build a language model for German texts. The framework includes implementations for word and subword-based (morpheme, character or character n-gram) embeddings and uses either bidirectional LSTMs or addition as the combination function of subwords. The German data sets are from the preprocessed Wikipedia data (Al-Rfou et al., 2013). Hyperlinks have been removed and the input texts have been lower-cased before learning the word- and compound-based embeddings. For the characterbased embeddings, the upper-cased letters have been preserved. We split the data into training, development and test sets, with approximately 1.2M, 150K and 150K tokens, respectively. For training and evaluation we closely follow Vania and Lopez (2017). We report results for three language models. The word model and the character model (using a bidirectional LSTM as composition function)2 are already implemented in the framework. For the compound em"
W17-4117,D15-1041,0,0.0439121,"Missing"
W17-4117,N16-1155,0,0.0270466,"Missing"
W17-4117,W16-1603,0,0.0378133,"Missing"
W17-4117,W16-2512,0,0.0400817,"Missing"
W17-4117,D14-1082,0,0.0287546,"Missing"
W17-4117,D15-1176,0,0.052651,"Missing"
W17-4117,N15-1140,0,0.0461573,"Missing"
W17-4117,D15-1188,0,0.0694282,"Missing"
W17-4117,W13-3512,0,0.11745,"Missing"
W17-4117,W15-0122,0,0.0694294,"Missing"
W17-4117,J93-2004,0,0.0604779,"Missing"
W17-4117,P16-2067,0,0.0230562,"Missing"
W17-4117,W14-6111,0,0.0249988,"Missing"
W17-4117,W13-3204,0,0.0610145,"Missing"
W17-4117,P17-1184,0,0.0147119,"ord but, crucially, morphological information. This was confirmed by the improved results for using character-based embeddings instead of the compound-based ones, where we were able to make up for the decrease in LAS that resulted from removing POS information from the input. Our results are important, as they show that unknown words are not per se a problem for parsing, as long as we are able to learn something about their morphological properties. Table 5: Perplexity for different language models on German texts from Wikipedia. In our experiment, we use the framework1 and setup described in Vania and Lopez (2017) to build a language model for German texts. The framework includes implementations for word and subword-based (morpheme, character or character n-gram) embeddings and uses either bidirectional LSTMs or addition as the combination function of subwords. The German data sets are from the preprocessed Wikipedia data (Al-Rfou et al., 2013). Hyperlinks have been removed and the input texts have been lower-cased before learning the word- and compound-based embeddings. For the characterbased embeddings, the upper-cased letters have been preserved. We split the data into training, development and test"
W17-4117,weller-heid-2012-analyzing,0,0.0208002,"he weight and bias vectors. We now outline our compositional model for compound embeddings. We assume that most compounds have a transparent meaning that can be inferred from the meaning of its components and hypothesize that providing the parser with subword embeddings that combine the representations of the individual components will help the model to handle unseen compounds. To that end, we first split each compound into lexemes and then combine the sequence of lexemes as we did for the character-based embeddings, using a bidirectional LSTM. For compound splitting, we use the IMS splitter (Weller and Heid, 2012) which adopts a frequency-based approach with additional linguistic features. The input information for the splitter (frequencies, POS and lemmas) was extracted from SdeWac (Faaß and Eckart, 2013), a cleanedup version of the deWac corpus (Baroni et al., 2009) with automatic POS tags and lemmas. 4 Experiments 4.1 Parsing Model Our parsing model is an extension of the headselection parser of Zhang et al. (2017) (figure 1). Given the sentence S = (w0 , w1 , ..., wN ) and xi as the input representation of word wi , the model 118 animals comrades comrades ROOT hROOT hAll hanimals hare hcomrades biL"
W17-4117,E17-1063,0,0.0346326,"ompound into lexemes and then combine the sequence of lexemes as we did for the character-based embeddings, using a bidirectional LSTM. For compound splitting, we use the IMS splitter (Weller and Heid, 2012) which adopts a frequency-based approach with additional linguistic features. The input information for the splitter (frequencies, POS and lemmas) was extracted from SdeWac (Faaß and Eckart, 2013), a cleanedup version of the deWac corpus (Baroni et al., 2009) with automatic POS tags and lemmas. 4 Experiments 4.1 Parsing Model Our parsing model is an extension of the headselection parser of Zhang et al. (2017) (figure 1). Given the sentence S = (w0 , w1 , ..., wN ) and xi as the input representation of word wi , the model 118 animals comrades comrades ROOT hROOT hAll hanimals hare hcomrades biLSTM biLSTM biLSTM biLSTM biLSTM Module Word emb. POS emb. Character-based emb. Compound emb. BiLSTM Regularization Optimization xROOT xAll xanimals xare xcomrades Others Figure 1: The parsing as head selection model (4) B hB i = LSTMB (xi , hi+1 ) (5) hi = [hFi ; hB i ] 4.2 (6) Word Embeddings (+word) Each word w in the lexicon is represented as a vector ew in the lookup table. We do not use any pre-trained e"
W17-6525,L16-1262,0,0.222686,"on that the observed results of their experiments are “unconvincing and not very promising” (Mareˇcek et al., 2013). Versley and Kirilin (2015) look at the influence of languages and annotation schemes in universal dependency parsing, comparing 5 different parsers on 5 languages using two variants of UD schemes. They state that encoding content words as head has a negative impact on parsing results and that PP attachment errors account for a large portion of 219 the differences in accuracy between the different parsers and between treebanks of varying sizes. Recent work by Gulordava and Merlo (2016) has looked at word order variation and its impact on dependency parsing of 12 languages. They focus on word order freedom and dependency length as two properties of word order that systematically vary between different languages. To assess their impact on parsing accuracy, they modify the original treebanks by minimising the dependency lengths and the entropy of the headdirection (whether the head of dependent dep can be positioned to the left, to the right, or either way), thus creating artificial treebanks with systematically different word order properties. Parsing results on the modified"
W17-6525,petrov-etal-2012-universal,0,0.0226163,"efined feature templates but selects the most probable head for each token based on word representations learned by a bidirectional long-short memory model (LSTM) (Hochreiter and Schmidhuber, 1997). Despite its simplicity and the lack of global optimisation, Zhang et al. (2017) report competetive results for English, Czech, and German. For the first two parsers, we use default settings and the provided feature templates (for the RBG parser we use the standard setting without pretrained word embeddings), with no languagespecific parameter optimisation.8 We use the coarse-grained universal POS (Petrov et al., 2012) for all languages. The RBG and IMSTrans parser 8 Please note that our goal is not to improve, or compare, results for individual languages but to assess the impact of different encoding decisions on the parsing accuracy for one language. 222 are trained on gold POS and morphological features provided by the UD project, the headselection model is trained without morphological information, using word and POS embeddings only. We choose the head-selection model to test whether a potential positive impact of the conversion might simply be a bias introduced by the feature templates, which might fav"
W17-6525,P13-1051,0,0.0514112,"Missing"
W17-6525,D07-1013,0,\N,Missing
W17-6525,de-marneffe-etal-2014-universal,0,\N,Missing
W17-6525,W10-1401,1,\N,Missing
W17-6525,C12-1147,0,\N,Missing
W17-6525,W15-2210,0,\N,Missing
W17-6525,W15-2131,0,\N,Missing
W17-6525,W15-2134,0,\N,Missing
W17-6525,P14-1130,0,\N,Missing
W17-6525,W15-2112,0,\N,Missing
W17-6525,Q16-1025,0,\N,Missing
W17-6525,W17-0411,0,\N,Missing
W17-6525,E17-2001,0,\N,Missing
W17-6525,E17-1063,0,\N,Missing
W17-6525,W10-2927,0,\N,Missing
W18-3027,P17-1044,0,0.272908,"a and benchmarking against state-of-the-art sequence labeling models. We show that our model is able to solve the SRL argument labeling task on English data, yet further structural decoding constraints will need to be added to make the model truly competitive. Our work represents a first step towards more advanced, generative SRL labeling setups. 1 Figure 1: An input sentence (top), its PropBank predicate-argument structure (middle) and its linearized labeled sequence produced by our system. Recent end-to-end neural models considerably improved the state-of-the-art results for SRL in English (He et al., 2017; Marcheggiani and Titov, 2017). In general, such models treat the problem as a supervised sequence labeling task, using deep LSTM architectures that assign a label to each token within the sentence. SRL training resources for other languages are more restricted in size and thus, models suffer from sparseness problems because specific predicate-role instances occur only a handful of times in the training set. Since annotating SRL data in larger amounts is expensive, the use of a generative neural network model could be beneficial for automatically obtaining more labeled data in low-resource se"
W18-3027,P98-1013,0,0.509936,"Missing"
W18-3027,W05-0620,0,0.406188,"Missing"
W18-3027,P17-1014,0,0.0637291,"Missing"
W18-3027,E17-1060,0,0.06839,"Missing"
W18-3027,D15-1166,0,0.228534,"with respect to the predicate. SRL has been formalized in different frameworks, the most prominent being FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). In this work we focus on argument identification and labeling using the PropBank (PB) annotation scheme. 207 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 207–216 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Sequence-to-sequence (seq2seq) models were pioneered by Sutskever et al. (2014), and later enhanced with an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). They have been successfully applied in many related structure prediction tasks such as syntactic parsing (Vinyals et al., 2015), parsing into Abstract Meaning Representation (Konstas et al., 2017), semantic parsing (Dong and Lapata, 2016), and cross-lingual Open Information Extraction (Zhang et al., 2017). When applying a seq2seq model with attention in a monolingual SRL labeling setup, we need to restrict the decoder to reproduce the original input sentence, while in addition inserting PropBank labels into the target sequence in the decoding process (see Figure 1). To achieve this, we encod"
W18-3027,D14-1179,0,0.035044,"Missing"
W18-3027,K17-1041,0,0.0297835,"d features and treated the task as an IOB sequence labeling problem. Later, Zhou and Xu (2015) proposed a deep bi-directional LSTM model with a CRF layer on top. This model takes only the original text as input and assigns a label to each individual word in the sentence. He et al. (2017) also treat SRL as a IOB tagging problem, and use again a deep bi-LSTM incorporating highway connections, recurrent dropout and hard decoding constraints together with an ensemble of experts. This represents the best performing system on two span-based benchmark datasets so far (namely, CoNLL-05 and CoNLL-12). Marcheggiani et al. (2017) show that it is possible to construct a very accurate dependency-based SRL system without using any kind of explicit syntactic information. In subsequent work, Marcheggiani and Titov (2017) combine their LSTM model with a graph convolutional network to encode syntactic information at word level, which improves their LSTM classifier results on the dependency-based benchmark dataset (CoNLL-09). 6 Conclusions In this paper we explore the properties of a Sequence-to-Sequence model for identifying and labeling PropBank roles. This is motivated by the fact that using a seq2seq model gives more flex"
W18-3027,D17-1159,0,0.140325,"g against state-of-the-art sequence labeling models. We show that our model is able to solve the SRL argument labeling task on English data, yet further structural decoding constraints will need to be added to make the model truly competitive. Our work represents a first step towards more advanced, generative SRL labeling setups. 1 Figure 1: An input sentence (top), its PropBank predicate-argument structure (middle) and its linearized labeled sequence produced by our system. Recent end-to-end neural models considerably improved the state-of-the-art results for SRL in English (He et al., 2017; Marcheggiani and Titov, 2017). In general, such models treat the problem as a supervised sequence labeling task, using deep LSTM architectures that assign a label to each token within the sentence. SRL training resources for other languages are more restricted in size and thus, models suffer from sparseness problems because specific predicate-role instances occur only a handful of times in the training set. Since annotating SRL data in larger amounts is expensive, the use of a generative neural network model could be beneficial for automatically obtaining more labeled data in low-resource settings. The model that we prese"
W18-3027,P16-1004,0,0.210088,"g the PropBank (PB) annotation scheme. 207 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 207–216 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Sequence-to-sequence (seq2seq) models were pioneered by Sutskever et al. (2014), and later enhanced with an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). They have been successfully applied in many related structure prediction tasks such as syntactic parsing (Vinyals et al., 2015), parsing into Abstract Meaning Representation (Konstas et al., 2017), semantic parsing (Dong and Lapata, 2016), and cross-lingual Open Information Extraction (Zhang et al., 2017). When applying a seq2seq model with attention in a monolingual SRL labeling setup, we need to restrict the decoder to reproduce the original input sentence, while in addition inserting PropBank labels into the target sequence in the decoding process (see Figure 1). To achieve this, we encode each input sentence into a suitable representation that will be used by the decoder to regenerate word tokens as given in the source sentence and introducing SRL labels in appropriate positions to label argument spans with semantic roles."
W18-3027,D15-1112,0,0.0372341,"Missing"
W18-3027,J05-1004,0,0.0900036,"stem performance against existing sequence labeling models for SRL on well known labeled evaluation data. Introduction Semantic Role Labeling (SRL) is the task of assigning semantic argument structure to constituents or phrases in a sentence, to answer the question: Who did what to whom, where and when? This task is normally accomplished in two steps: first, identifying the predicate and second, labeling its arguments and the roles that they play with respect to the predicate. SRL has been formalized in different frameworks, the most prominent being FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). In this work we focus on argument identification and labeling using the PropBank (PB) annotation scheme. 207 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 207–216 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Sequence-to-sequence (seq2seq) models were pioneered by Sutskever et al. (2014), and later enhanced with an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). They have been successfully applied in many related structure prediction tasks such as syntactic parsing (Vinyals et al., 2015), parsing into Abstra"
W18-3027,P16-1154,0,0.526499,"q2seq model with attention in a monolingual SRL labeling setup, we need to restrict the decoder to reproduce the original input sentence, while in addition inserting PropBank labels into the target sequence in the decoding process (see Figure 1). To achieve this, we encode each input sentence into a suitable representation that will be used by the decoder to regenerate word tokens as given in the source sentence and introducing SRL labels in appropriate positions to label argument spans with semantic roles. In order to avoid lexical deviations in the output string, we add a copying mechanism (Gu et al., 2016) to the model. This technique was originally proposed to deal with rare words by copying them directly from the source when appropriate. We apply this mechanism in a novel way, with the aim of guiding the decoder to reproduce the input as closely as possible, while otherwise giving it the option of generating role labels in appropriate positions in the target sequence. Our main contributions in this work are: (i) We propose a novel neural architecture for SRL using a seq2seq model enhanced with attention and copying mechanisms. (ii) We evaluate this model in a monolingual setting, performing P"
W18-3027,D14-1162,0,0.0802885,"Missing"
W18-3027,J08-2005,0,0.105116,"ith the use of attention. To measure the labeling performance difficulty with increasing sequence length, we partitioned the system outputs in six different bins containing groups of sentences of similar length (see Figure 6). As expected, the F1 score degrades proportionally to the length of the sequence, especially in sentences with more than 30 tokens. Distance to predicate He et al. (2017) show 5 Related Work Semantic Role Labeling. Traditional approaches to SRL relied on carefully designed features and expensive techniques to achieve global consistency such as Integer Linear Programming (Punyakanok et al., 2008) or dynamic programming 213 ing and generation system proposed by Konstas et al. (2017). This work successfully constructs a two-way mapping: generation of text given AMR representations as well as AMR parsing of natural language sentences. Finally, Zhang et al. (2017) went one step further by proposing a cross-lingual end-to-end system that learns to encode natural language (i.e. Chinese source sentences) and to decode them into sentences on the target side containing open semantic relations in English, using a parallel corpus for training. (T¨ackstr¨om et al., 2015). First neural SRL attempt"
W18-3027,P16-1113,0,0.0326684,"ences. Finally, Zhang et al. (2017) went one step further by proposing a cross-lingual end-to-end system that learns to encode natural language (i.e. Chinese source sentences) and to decode them into sentences on the target side containing open semantic relations in English, using a parallel corpus for training. (T¨ackstr¨om et al., 2015). First neural SRL attempts tried to mix syntactic features with neural network representations. For example, FitzGerald et al. (2015) created argument and role representations using a feed-forward NN, and used a graphical model to enforce global constraints. Roth and Lapata (2016), on the other hand, proposed a neural classifier using dependency path embeddings to assign semantic labels to syntactic arguments. Collobert et al. (2011) proposed the first SRL neural model that did not depend on hand-crafted features and treated the task as an IOB sequence labeling problem. Later, Zhou and Xu (2015) proposed a deep bi-directional LSTM model with a CRF layer on top. This model takes only the original text as input and assigns a label to each individual word in the sentence. He et al. (2017) also treat SRL as a IOB tagging problem, and use again a deep bi-LSTM incorporating"
W18-3027,Q15-1003,0,0.158056,"Missing"
W18-3027,E17-2011,0,0.146533,"Missing"
W18-3027,P15-1109,0,0.341195,"ing with each other so that the model learns when to copy directly from the source and when to generate the next token. 2 The label set L contains one common opening bracket (# for all argument types to indicate the beginning of an argument span, and several labelspecific closing brackets, such as P0:A1), which indicates in this case that the span for argument A1 is ending (see also Table 1). In our current setup we restrict role labeling to a single predicate per sentence. If a sentence has more than one predicate, we create a separate copy for each predicate; the same setting was applied in Zhou and Xu (2015). In each sentence copy the predicate whose roles are to be labeled is preceded by a special token &lt;PRED&gt; that marks the position of the predicate under consideration. This helps the decoder to focus on generating argument labels for that specific predicate (see Table 1.) 2.1 We assume a unique vocabulary for both encoder and decoder that comprises the words occurring during training, the out-of-vocabulary token, and the special symbol used to mark the position of the predicate, thus V = {v1 , ..., vN } ∪ {U N K, &lt; P RED &gt;}. In addition, we employ a set L = {l1 , ..., lM } with all the possibl"
W18-4105,P98-1013,0,0.120051,"Missing"
W18-4105,D15-1075,0,0.011728,"tion detection was able to detect both (and only both) candidates and linked the pronoun to one of those. All other cases we have to treat as unresolved. The downloaded system yields an accuracy of 0.99 (397 correct, 3 incorrect, 164 unresolved) on WSC R. In our evaluation Table we present the result from their paper (Table 1: SOTA) As an additional baseline we use as a representation of the input sentences the representations predicted by a trained sentence embedding model, here InferSent (Conneau et al., 2017). InferSent has been been trained on large-scale natural language inference tasks (Bowman et al., 2015) and therefore may have internalized valuable information about whether sentence readings are coherent or rather nonsensical. We infer 4096-dimensional sentence vectors with the trained model provided by the authors10 and fit a linear ranker SVM, using randomly sampled development data to find a suitable regularization parameter. Experimental Setup and Evaluation. We evaluate our models in two testing scenarios: (i) Train:WSC R+Test:WSC L: In this setup we train the model on the full WSC R data and test on the unseen WSC L data, to test the generalization capability of our models across data s"
W18-4105,P08-1090,0,0.0667634,"Missing"
W18-4105,Y14-1042,0,0.0316657,"Missing"
W18-4105,W11-1902,0,0.0642971,"Missing"
W18-4105,D17-1018,0,0.0301282,"ery rarely in natural language texts and cannot be properly resolved by traditional coreference resolution (CR) systems. The primary reason is that standard CR systems heavily rely on features such as gender or number agreement or mention-distance information. However, such features do not give away any knowledge that would be useful for resolving WS problems. Given a random baseline of 0.5 accuracy, the Stanford resolver (Lee et al., ), winner of the CoNLL 2011 Shared Task (Pradhan et al., 2011), achieves a sobering accuracy of 0.53 when facing Winograd Schema problems (Rahman and Ng, 2012). Lee et al. (2017) describe a state-of-the-art neural system for general neural coreference resolution and observe that, while trained on much more data than is available in 41 Proceedings of the First International Workshop on Language Cognition and Computational Models, pages 41–52 Santa Fe, New Mexico, United States, August 20, 2018. https://doi.org/10.18653/v1/P17 the WSC, their system shows little advance in the uphill battle of resolving hard pronoun coreference problems that require world knowledge. As our main contribution we are proposing a novel and very general take on the WSC task that we formulate"
W18-4105,K15-1002,0,0.122778,"middle of the lawn. After a while, it got up and moved to a spot under the tree, because it was hot. 3 url: http://www.hlt.utdallas.edu/˜vince/papers/emnlp12.html 2 42 differently, e.g. instead of querying Google directly, the Google n-gram dataset (Brants and Franz, 2006) was used. The authors present a system that extracts representative examples from the web. Both systems were tested on a subset of the WSC R test set (for the problems where web examples were found). The reimplemented system yielded 0.56 accuracy while their own approach yielded 0.69 accuracy. Integer Linear Program (ILP). Peng et al. (2015b) use an ILP (Schrijver, 1986) inference approach with a novel way of knowledge representation. Their system yields 0.76 accuracy on WSC R, which is the current state-of-the-art result on this data. In their approach “Predicate Schemas” are instantiated and scored using knowledge acquired from external knowledge bases compiled into constraints for a decision. Consider ‘The bee landed on the flower because it {was hungry, had pollen}’, where the gold resolution is that (i) the bee was hungry and (ii) the flower had pollen. A simple predicate schema for this problem is instantiated as hungry(be"
W18-4105,N15-1082,0,0.140307,"middle of the lawn. After a while, it got up and moved to a spot under the tree, because it was hot. 3 url: http://www.hlt.utdallas.edu/˜vince/papers/emnlp12.html 2 42 differently, e.g. instead of querying Google directly, the Google n-gram dataset (Brants and Franz, 2006) was used. The authors present a system that extracts representative examples from the web. Both systems were tested on a subset of the WSC R test set (for the problems where web examples were found). The reimplemented system yielded 0.56 accuracy while their own approach yielded 0.69 accuracy. Integer Linear Program (ILP). Peng et al. (2015b) use an ILP (Schrijver, 1986) inference approach with a novel way of knowledge representation. Their system yields 0.76 accuracy on WSC R, which is the current state-of-the-art result on this data. In their approach “Predicate Schemas” are instantiated and scored using knowledge acquired from external knowledge bases compiled into constraints for a decision. Consider ‘The bee landed on the flower because it {was hungry, had pollen}’, where the gold resolution is that (i) the bee was hungry and (ii) the flower had pollen. A simple predicate schema for this problem is instantiated as hungry(be"
W18-4105,D14-1162,0,0.0824099,"do five random initializations with different seeds. We choose the model parameterizations from the epochs where they performed best on the development set. These models predict the test set and we compute mean and standard deviation of accuracy. We also introduce two ensembles, the na¨ıve ensemble (Na¨ıveE) and the Siamese ensemble (SiamE), which are majority voters informed by the predictions of the five different random seed models. Parameter Search. We examine the Na¨ıve model and the Siamese model, using all discussed features and pretrained, fixed 300 dimensional GloVe word embeddings (Pennington et al., 2014). Dependency edge embeddings with 10 dimensions are initialized randomly from N10 (0, 1). The two embeddings for the anonymized mentions are drawn from N300 (0, 1). The Bi-LSTMs have 32 hidden units each, the weight matrix used for the linear transformation of the inputs is initialized according to Glorot and Bengio (2010), who proposed this initialization scheme to bring substantially faster convergence. The weight matrix used for the linear transformation of the recurrent state is initialized as a random orthonormal matrix (Saxe et al., 2013; Mishkin and Matas, 2015) and the biases are initi"
W18-4105,W11-1901,0,0.0920778,"Missing"
W18-4105,D12-1071,0,0.639378,"es 1.1 and 1.2 occur very rarely in natural language texts and cannot be properly resolved by traditional coreference resolution (CR) systems. The primary reason is that standard CR systems heavily rely on features such as gender or number agreement or mention-distance information. However, such features do not give away any knowledge that would be useful for resolving WS problems. Given a random baseline of 0.5 accuracy, the Stanford resolver (Lee et al., ), winner of the CoNLL 2011 Shared Task (Pradhan et al., 2011), achieves a sobering accuracy of 0.53 when facing Winograd Schema problems (Rahman and Ng, 2012). Lee et al. (2017) describe a state-of-the-art neural system for general neural coreference resolution and observe that, while trained on much more data than is available in 41 Proceedings of the First International Workshop on Language Cognition and Computational Models, pages 41–52 Santa Fe, New Mexico, United States, August 20, 2018. https://doi.org/10.18653/v1/P17 the WSC, their system shows little advance in the uphill battle of resolving hard pronoun coreference problems that require world knowledge. As our main contribution we are proposing a novel and very general take on the WSC task"
W18-4105,P16-1030,0,0.124737,"n and consider the case of one additional matrix type feature: after the matrix has been shaped to the same dimensionality l × n we can use concatenation, element-wise addition and element-wise multiplication to merge the additional feature representation with the sentence representation into a representation of dimension l × 4n before it is fed into the next layer. As additional matrix-type features we experiment with dependency edge sequences and information about the connotation of the arguments induced by their predicate as stated in the resource Connotation Frames (Rashkin et al., 2015; Rashkin et al., 2016). The features and motivation for usage are more extensively discussed in the next paragraphs. We can also incorporate features which come as real valued vectors: we use an averaged semantic embedding of the tokens of the candidate noun phrase (described more closely in the next paragraph) to provide useful information for cases where the candidate noun phrase is not a generic person name but carries meaning. The vector can be injected into the model between the 2d-1d encoder and the output activation computation. A FF-layer is used to shape the vector so that it matches the output dimension h"
W19-0801,W16-5311,0,0.0981394,"c relation classification covers a wide range of methods and learning paradigms for representing relation instances (see Nastase et al. 2013 for an overview). Typically, the data is presented to the learner as independent instances, with or without a sentential context. Relation classification models represent the meaning of the arguments (attributional features) and if context is available, also the relation (relational features). Recently Deep Learning has strongly influenced semantic relation learning. Word embeddings can provide attributional features for a variety of learning frameworks (Attia et al., 2016; Vylomova et al., 2016), and the sentential context – in its entirety, or only the structured (through grammatical relations) or unstructured phrase expressing the relation – can be modeled through a variety of neural architectures – CNN (Tan et al., 2018; Ren et al., 2018) or RNN variations (Zhang et al., 2018). 2.2 C ONCEPT N ET Relation Classification Speer et al. (2008) introduce AnalogySpace, a representation of concepts and relations in C ONCEPTN ET built by factorizing a matrix with concepts on one axis and their features or properties (according to C ONCEPT N ET) on the other. This lo"
W19-0801,S18-1172,0,0.0227727,"Missing"
W19-0801,D13-1072,0,0.111195,"amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of relation types that hold between two given concepts from C ONCEPT N ET. In future work, the predicted relations can then be used for enriching C ONCEPT N ET by adding relations between concepts which are not yet linked in the network. We design the task of multi-label neural"
W19-0801,P16-1137,0,0.276295,"Missing"
W19-0801,K16-1006,0,0.0159662,"tings. The overall best performing model across all settings is FF NN +R NN (as opposed to FF NN with centroid argument representations) with relation-specific label prediction thresholds (as opposed to one global threshold value). In the OW setting we achieve overall F1-scores of 0.68 (OW-1) and 0.65 (OW-2). The CW setting leads to best results with 0.71 F1. The models improve by 4pp (OW-1), 7pp (OW-2) and 3 We additionally tested Numberbatch embeddings (Speer et al., 2017), GloVe embeddings trained on Wikipedia and Gigaword (Pennington et al., 2014), context2vec embeddings trained on UkWaC (Melamud et al., 2016). In our experiments we discovered that all of these alternatives perform worse than the word2vec embeddings. 6 Setting Model ISA H AS A AT L OCATION H AS P ROPERTY U SED F OR C APABLE O F R ECEIVES ACTION C AUS .D ES . D ESIRES M OTICATED B Y G OAL H AS P REREQUISITE H AS F IRST S UBEVENT H AS S UBEVENT C AUSES OTHER R ANDOM Weighted F1 FF .58 (.57) .67 (.66) .69 (.68) .66 (.65) .76 (.75) .61 (.61) .82 (.82) .87 (.87) .91 (.85) .61 (.60) .45 (.41) .54 (.53) .24 (.22) .60 (.59) .61 (.58) .64 (.63) OpenWorld OW-1 FF+RNN .62 (.60) .80 (.79) .78 (.78) .81 (.80) .78 (.77) .67 (.65) .91 (.91) .90 ("
W19-0801,P18-1076,1,0.824435,"complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work. 1 Introduction Commonsense knowledge can be seen as a large amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of re"
W19-0801,N13-1090,0,0.0168912,"of the arguments, coverage and completeness. A successful relation classification system should take these into account. Given the heterogeneity of sources of C ONCEPT N ET, we focus on its core part, in particular C N -O MCS C LN, a subset selected from C N -O MCS that includes ca. 180K triples from 36 relation types, restricted to known vocabulary from the GoogleNews Corpus (see §4.1 for further details). 3.3.1 Representing the Inputs Word embeddings have been shown to provide useful semantic representations, capturing lexical properties of words and relative positioning in semantic space (Mikolov et al., 2013b), which has been exploited for semantic relation classification (Vylomova et al., 2016; Attia et al., 2016). Following this work, we represent a pair of concepts hci , cj i whose relation we want to classify through their embeddings vci and vcj . These argument representations can be combined by subtraction (v ci − v cj ) (DiffVec; cf. wee , rol ), addition v ci + v cj (AddVec) or concatenation [v ci , v cj ] (ConcatVec, cf. bar ). One of the issues in using such representations for C ONCEPT N ET is the fact that most C ONCEPTN ET concepts are multi-word expressions (1.93 words on average, c"
W19-0801,S18-1119,0,0.0956526,"ce that can be expanded in future work. 1 Introduction Commonsense knowledge can be seen as a large amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of relation types that hold between two given concepts from C ONCEPT N ET. In future work, the predicted relations can then be used for enriching C ONCEPT N ET by adding relations betwee"
W19-0801,D14-1162,0,0.0788332,"ble 4 summarizes the results in open (OW) and closed world (CW) settings. The overall best performing model across all settings is FF NN +R NN (as opposed to FF NN with centroid argument representations) with relation-specific label prediction thresholds (as opposed to one global threshold value). In the OW setting we achieve overall F1-scores of 0.68 (OW-1) and 0.65 (OW-2). The CW setting leads to best results with 0.71 F1. The models improve by 4pp (OW-1), 7pp (OW-2) and 3 We additionally tested Numberbatch embeddings (Speer et al., 2017), GloVe embeddings trained on Wikipedia and Gigaword (Pennington et al., 2014), context2vec embeddings trained on UkWaC (Melamud et al., 2016). In our experiments we discovered that all of these alternatives perform worse than the word2vec embeddings. 6 Setting Model ISA H AS A AT L OCATION H AS P ROPERTY U SED F OR C APABLE O F R ECEIVES ACTION C AUS .D ES . D ESIRES M OTICATED B Y G OAL H AS P REREQUISITE H AS F IRST S UBEVENT H AS S UBEVENT C AUSES OTHER R ANDOM Weighted F1 FF .58 (.57) .67 (.66) .69 (.68) .66 (.65) .76 (.75) .61 (.61) .82 (.82) .87 (.87) .91 (.85) .61 (.60) .45 (.41) .54 (.53) .24 (.22) .60 (.59) .61 (.58) .64 (.63) OpenWorld OW-1 FF+RNN .62 (.60) ."
W19-0801,C18-1100,0,0.0480483,"Missing"
W19-0801,K18-1014,0,0.311075,"Missing"
W19-0801,W16-5413,0,0.0587866,"Missing"
W19-0801,P16-1158,0,0.120832,"ation covers a wide range of methods and learning paradigms for representing relation instances (see Nastase et al. 2013 for an overview). Typically, the data is presented to the learner as independent instances, with or without a sentential context. Relation classification models represent the meaning of the arguments (attributional features) and if context is available, also the relation (relational features). Recently Deep Learning has strongly influenced semantic relation learning. Word embeddings can provide attributional features for a variety of learning frameworks (Attia et al., 2016; Vylomova et al., 2016), and the sentential context – in its entirety, or only the structured (through grammatical relations) or unstructured phrase expressing the relation – can be modeled through a variety of neural architectures – CNN (Tan et al., 2018; Ren et al., 2018) or RNN variations (Zhang et al., 2018). 2.2 C ONCEPT N ET Relation Classification Speer et al. (2008) introduce AnalogySpace, a representation of concepts and relations in C ONCEPTN ET built by factorizing a matrix with concepts on one axis and their features or properties (according to C ONCEPT N ET) on the other. This low-dimensional representa"
W19-0801,S18-1120,0,0.232442,"ty are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work. 1 Introduction Commonsense knowledge can be seen as a large amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of relation types that hold between two given c"
W19-0801,S10-1006,0,\N,Missing
W19-4503,W17-5107,0,0.27498,"different model types with a type-variable T ∈ {CI, CB, FA}. Moens, 2011) is the task for which we aim to examine the context-content relationship. It is concerned with predicting and analyzing relations between argumentative units such as, for example, support or attack. Besides works discussed above (Nguyen and Litman, 2016; Stab and Gurevych, 2014b, 2017), this task has also been addressed by Cocarascu and Toni (2017) who develop a neural model to label the edge between two EAUs with {attack, support, ∅}. The task has also been approached by taking global graph context into account. E.g., Hou and Jochim (2017) jointly model argument relation classification and stance classification in the DebatePedia3 corpus using Markov logic networks (Richardson and Domingos, 2006). Peldszus and Stede (2015) experiment with Microtexts and show that it can be beneficial to model argumentative relations jointly in a network with a minimum spanning tree decoding algorithm. Our work focuses on local relation prediction and labeling using the well-established StudentEssayv02 data (Stab and Gurevych, 2017)4 with 402 argumentative essays and thousands of annotated relations between EAUs. 3 3.1 Now, we introduce a classi"
W19-4503,C14-1141,0,0.0728366,"stem from different documents it is easy to imagine a textual context where (2) is not introduced by however but instead by an ‘inverse’ form such as e.g. moreover). our work investigates how (de-)contextualization of argumentative units affects automatic argumentative relation classification models. Notions of context Various notions of context are being used in the area of argumentation mining. For example, Lippi and Torroni (2016) develop a context-independent claim detection system, where by context-independent they refer to a system which is not tailored to a specific topic (analogously, Levy et al. (2014) aim at contextdependency). Another notion of context concerns the graph context in which relations and EAUs are embedded (Kuribayashi et al., 2018). On the other hand, we adopt a more textual notion of context, that is we take a given EAU span as content and text which is not in the EAU span as context. This goes in the same direction as Stab and Gurevych (2014b, 2017); Persing and Ng (2016) and Aker et al. (2017) who incorporate features derived from EAU-surrounding text in their classification systems. However, they do not clearly separate between a word indicator feature extracted from wit"
W19-4503,P14-5010,0,0.00353873,"and argumentative relations (Stab and Gurevych, 2017). The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level. This entails that contextual clues tend not to be contained in the annotated span (e.g., only people should not smoke is annotated as EAU in the sentence Therefore, people should not smoke.). In this work, we are concerned with classifying relations between argumentative units into support or attack and thus do not consider other annotations. For feature extraction, we process all documents with Stanford CoreNLP (Manning et al., 2014) with the following annotation layers: sentence tokenize, word tokenize, constituency parse and constituency-sentiment. For extraction of the discourse-features, we proceed by parsing all documents with the PDTB-parser7 developed by Lin et al. (2014). For the joint task of predicting three link classes (including a non-linked class), we extract as non-linked EAU pairs all EAU pairs which are not linked on a document level. Data set statistics are displayed in Table 1. Embeddings We use the element-wise sum of 300-dimensional pre-trained GloVe vectors (Pennington et al., 2014) corresponding to"
W19-4503,W17-5112,0,0.419229,"and Torroni (2016) develop a context-independent claim detection system, where by context-independent they refer to a system which is not tailored to a specific topic (analogously, Levy et al. (2014) aim at contextdependency). Another notion of context concerns the graph context in which relations and EAUs are embedded (Kuribayashi et al., 2018). On the other hand, we adopt a more textual notion of context, that is we take a given EAU span as content and text which is not in the EAU span as context. This goes in the same direction as Stab and Gurevych (2014b, 2017); Persing and Ng (2016) and Aker et al. (2017) who incorporate features derived from EAU-surrounding text in their classification systems. However, they do not clearly separate between a word indicator feature extracted from within (or outside) the EAU span. For example, when computing features for an EAU, they also take into account EAU-preceding tokens. The preceding tokens, often contain shallow discourse markers which highlight the relationship between two EAUs (e.g., because, however, etc.). To the best of our knowledge, prior work has not yet thoroughly investigated the impact of features extracted from the EAU vs. features extracte"
W19-4503,P16-1107,0,0.392795,"0); Peldszus and Stede (2013) observe that elementary discourse units (EDUs) in RST (Mann and Thompson, 1987) share great similarity with elementary argumentative units (EAUs) in argumentation analysis.2 Wachsmuth et al. (2018) experiment with a modified version of the Microtext corpus (Peldszus and Stede, 2016), which is an extensively annotated albeit small corpus. Similar to us, they separate argumentative units from discursive contextual markers. While Wachsmuth et al. (2018) conduct a human evaluation to investigate the separation of Logos and Pathos aspects of arguments, Context matters Nguyen and Litman (2016); Nguyen (2018) extract additional features from the text between source and target EAUs (on the StudentEssay-v01 data (Stab and Gurevych, 2014a)) which results in enhanced predictive performance. However, having seen the clear advantages of incorporating context (performancewise), we find that the downsides of incorporating context remain untold. In this work, we demonstrate that systems which are offered EAU context may be prone to neglect the EAU content, an issue that can have undesired effects. 2 Throughout this work we often drop “elementary” and use the phrases EAU and (elementary) argu"
W19-4503,J17-3005,0,0.244685,"EAUs with {attack, support, ∅}. The task has also been approached by taking global graph context into account. E.g., Hou and Jochim (2017) jointly model argument relation classification and stance classification in the DebatePedia3 corpus using Markov logic networks (Richardson and Domingos, 2006). Peldszus and Stede (2015) experiment with Microtexts and show that it can be beneficial to model argumentative relations jointly in a network with a minimum spanning tree decoding algorithm. Our work focuses on local relation prediction and labeling using the well-established StudentEssayv02 data (Stab and Gurevych, 2017)4 with 402 argumentative essays and thousands of annotated relations between EAUs. 3 3.1 Now, we introduce a classification of three different prediction models used in the argumentative relation prediction literature. We will inspect all of them and show that all can suffer from severe issues when focusing (too much) on the context. The model h adopts a discourse parsing view on argumentative relation prediction and predicts one outgoing edge for an argumentative unit (oneoutgoing edge). Model f assumes a connected graph with argumentative units and is tasked with predicting edge labels for u"
W19-4503,D18-1402,0,0.0119086,"very prone to neglect the actual textual content of the EAU span. Instead they heavily rely on contextual markers, such as conjunctions or adverbials, as a basis for prediction. We argue that a system’s capacity of predicting the correct relation based on the argumentative units’ content is important in many circumstances, e.g., when an argumentative debate crosses document boundaries. Introduction In recent years we have witnessed a great surge in activity in the area of computational argument analysis (e.g. Peldszus and Stede (2013); Stab and Gurevych (2014b); Rasooli and Tetreault (2015); Stab et al. (2018)), and the emergence of dedicated venues such as the ACL Argument Mining workshop series starting in 2014 (Green et al., 2014). Argumentative relation classification is a subtask of argument analysis that aims to determine relations between argumentative units A and B, for example, A supports B; A attacks B. Consider the following argumentative units (1) and (2), given the topic (0) “Marijuana should be legalized”: 1 In case of (1) and (2): By setting age restrictions on legalization of Marijuana, increased use by teens can be (expected to be) prevented, thus we can infer that (2) attacks (1)."
W19-4503,D15-1110,0,0.146422,"d analyzing relations between argumentative units such as, for example, support or attack. Besides works discussed above (Nguyen and Litman, 2016; Stab and Gurevych, 2014b, 2017), this task has also been addressed by Cocarascu and Toni (2017) who develop a neural model to label the edge between two EAUs with {attack, support, ∅}. The task has also been approached by taking global graph context into account. E.g., Hou and Jochim (2017) jointly model argument relation classification and stance classification in the DebatePedia3 corpus using Markov logic networks (Richardson and Domingos, 2006). Peldszus and Stede (2015) experiment with Microtexts and show that it can be beneficial to model argumentative relations jointly in a network with a minimum spanning tree decoding algorithm. Our work focuses on local relation prediction and labeling using the well-established StudentEssayv02 data (Stab and Gurevych, 2017)4 with 402 argumentative essays and thousands of annotated relations between EAUs. 3 3.1 Now, we introduce a classification of three different prediction models used in the argumentative relation prediction literature. We will inspect all of them and show that all can suffer from severe issues when fo"
W19-4503,C18-1318,0,0.150766,"Missing"
W19-4503,D14-1162,0,0.0830426,"th Stanford CoreNLP (Manning et al., 2014) with the following annotation layers: sentence tokenize, word tokenize, constituency parse and constituency-sentiment. For extraction of the discourse-features, we proceed by parsing all documents with the PDTB-parser7 developed by Lin et al. (2014). For the joint task of predicting three link classes (including a non-linked class), we extract as non-linked EAU pairs all EAU pairs which are not linked on a document level. Data set statistics are displayed in Table 1. Embeddings We use the element-wise sum of 300-dimensional pre-trained GloVe vectors (Pennington et al., 2014) corresponding to the words within the EAU span (CB) and the words of the EAU-surrounding context (CI). Additionally, we compute the element-wise subtraction of the source EAU vector from the target EAU vector, with the aim of modelling directions in distributional space, similarly to Mikolov et al. (2013). Words with no corresponding pre-trained word vector and empty sequences (e.g., no preceding context available) are treated as a zero-vector. Sentiment Tree-based sentiment annotations are sentiment scores assigned to nodes in constituency parse trees (Socher et al., 2013). We represent thes"
W19-4503,N16-1164,0,0.369805,"mining. For example, Lippi and Torroni (2016) develop a context-independent claim detection system, where by context-independent they refer to a system which is not tailored to a specific topic (analogously, Levy et al. (2014) aim at contextdependency). Another notion of context concerns the graph context in which relations and EAUs are embedded (Kuribayashi et al., 2018). On the other hand, we adopt a more textual notion of context, that is we take a given EAU span as content and text which is not in the EAU span as context. This goes in the same direction as Stab and Gurevych (2014b, 2017); Persing and Ng (2016) and Aker et al. (2017) who incorporate features derived from EAU-surrounding text in their classification systems. However, they do not clearly separate between a word indicator feature extracted from within (or outside) the EAU span. For example, when computing features for an EAU, they also take into account EAU-preceding tokens. The preceding tokens, often contain shallow discourse markers which highlight the relationship between two EAUs (e.g., because, however, etc.). To the best of our knowledge, prior work has not yet thoroughly investigated the impact of features extracted from the EA"
W19-4503,D13-1170,0,0.00265623,"GloVe vectors (Pennington et al., 2014) corresponding to the words within the EAU span (CB) and the words of the EAU-surrounding context (CI). Additionally, we compute the element-wise subtraction of the source EAU vector from the target EAU vector, with the aim of modelling directions in distributional space, similarly to Mikolov et al. (2013). Words with no corresponding pre-trained word vector and empty sequences (e.g., no preceding context available) are treated as a zero-vector. Sentiment Tree-based sentiment annotations are sentiment scores assigned to nodes in constituency parse trees (Socher et al., 2013). We represent these scores by a one-hot vector of dimension 5 (5 is very positive, 1 is very negative). We determine the contextual (CI) sentiment by looking at the highest possible node of the context which does not contain the EAU (ADVP in Figure 2). The sentiment for an EAU span (CB) is assigned to the highest possible node covering the EAU span which does not contain the context subtree (S in Figure 2). The full-access (FA) score is assigned to the lowest possible node which covers both the EAU span and its surrounding context (S’ in Figure 2). Next to the sentiment scores for the selecte"
W19-4503,C14-1142,0,0.381515,"al context surrounding an argumentative unit’s span – are very prone to neglect the actual textual content of the EAU span. Instead they heavily rely on contextual markers, such as conjunctions or adverbials, as a basis for prediction. We argue that a system’s capacity of predicting the correct relation based on the argumentative units’ content is important in many circumstances, e.g., when an argumentative debate crosses document boundaries. Introduction In recent years we have witnessed a great surge in activity in the area of computational argument analysis (e.g. Peldszus and Stede (2013); Stab and Gurevych (2014b); Rasooli and Tetreault (2015); Stab et al. (2018)), and the emergence of dedicated venues such as the ACL Argument Mining workshop series starting in 2014 (Green et al., 2014). Argumentative relation classification is a subtask of argument analysis that aims to determine relations between argumentative units A and B, for example, A supports B; A attacks B. Consider the following argumentative units (1) and (2), given the topic (0) “Marijuana should be legalized”: 1 In case of (1) and (2): By setting age restrictions on legalization of Marijuana, increased use by teens can be (expected to be"
W19-4503,D14-1006,0,\N,Missing
W19-4503,D17-1144,0,\N,Missing
