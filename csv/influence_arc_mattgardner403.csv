2020.acl-main.196,N18-1204,0,0.22989,"Missing"
2020.acl-main.196,P14-2006,0,0.0690508,"Missing"
2020.acl-main.196,N16-1024,0,0.174665,"ical results that reinforce the validity of importance sampling for evaluating latent language models. 1 Sameer Singh Univ. of California, Irvine sameer@uci.edu Introduction Latent language models are generative models of text that jointly represent the text and the latent structure underlying it, such as: the syntactic parse, coreference chains between entity mentions, or links of entities and relations mentioned in the text to an external knowledge graph. The benefits of modeling such structure include interpretability (Hayashi et al., 2020), better performance on tasks requiring structure (Dyer et al., 2016; Ji et al., 2017), and improved ability to generate consistent mentions of entities (Clark et al., 2018) and factually accurate text (Logan et al., 2019). Unfortunately, demonstrating that these models provide better performance than traditional language models by evaluating their likelihood on benchmark data can be difficult, as exact computation requires marginalizing over all possible latent structures. In this paper, we seek to fill in this missing knowledge, and put this practice on more rigorous footing. First, we review the theory of importance sampling, providing proof that importance"
2020.acl-main.196,D17-1195,0,0.256391,"einforce the validity of importance sampling for evaluating latent language models. 1 Sameer Singh Univ. of California, Irvine sameer@uci.edu Introduction Latent language models are generative models of text that jointly represent the text and the latent structure underlying it, such as: the syntactic parse, coreference chains between entity mentions, or links of entities and relations mentioned in the text to an external knowledge graph. The benefits of modeling such structure include interpretability (Hayashi et al., 2020), better performance on tasks requiring structure (Dyer et al., 2016; Ji et al., 2017), and improved ability to generate consistent mentions of entities (Clark et al., 2018) and factually accurate text (Logan et al., 2019). Unfortunately, demonstrating that these models provide better performance than traditional language models by evaluating their likelihood on benchmark data can be difficult, as exact computation requires marginalizing over all possible latent structures. In this paper, we seek to fill in this missing knowledge, and put this practice on more rigorous footing. First, we review the theory of importance sampling, providing proof that importance sampled perplexit"
2020.acl-main.196,N19-1114,0,0.071848,"s of evaluation data comprised of instances (token sequences) xn , estimates can be formed at the instance level:   N   1 X  , d I = exp − PPL log p(x ˆ ) (5) n   T n=1 or at the corpus level: ! 1 d C = exp − log p(x PPL ˆ C) , T (6) i.e., average is either over each instance or the whole corpus.2 RNNG and EntityNLM perform instance-level aggregation, whereas KGLM performs corpus-level aggregation. Note that these 1 τ = 0.5 τ = 0.9 τ = 1.0 88 τ = 1.1 τ = 2.0 No Peeking 86 84 0 Sample Size Typically, only 100 samples are used for computing the perplexity. A notable exception is Kim et al. (2019)’s follow-up to RNNG that uses 1000 samples. 200 400 600 800 1000 800 1000 E NTITY NLM 118 Perplexity 116 114 112 110 108 0 200 400 600 KGLM 125 Perplexity Proposal Distribution Previous work uses proposal distributions q(z|x) that are essentially discriminative versions of the generative model (e.g., they are models that predict the latent state conditioned on the text), with one key distinction: they are conditioned not only on the sequence of tokens that have been observed so far, but also on future tokens that the model will be evaluated on (a trait we will refer to as peeking). This condi"
2020.acl-main.196,P19-1598,1,0.813312,"@uci.edu Introduction Latent language models are generative models of text that jointly represent the text and the latent structure underlying it, such as: the syntactic parse, coreference chains between entity mentions, or links of entities and relations mentioned in the text to an external knowledge graph. The benefits of modeling such structure include interpretability (Hayashi et al., 2020), better performance on tasks requiring structure (Dyer et al., 2016; Ji et al., 2017), and improved ability to generate consistent mentions of entities (Clark et al., 2018) and factually accurate text (Logan et al., 2019). Unfortunately, demonstrating that these models provide better performance than traditional language models by evaluating their likelihood on benchmark data can be difficult, as exact computation requires marginalizing over all possible latent structures. In this paper, we seek to fill in this missing knowledge, and put this practice on more rigorous footing. First, we review the theory of importance sampling, providing proof that importance sampled perplexity estimates are stochastic upper bounds of the true perplexity—a previously unnoted justification for this evaluation technique. In addi"
2020.acl-main.196,J93-2004,0,0.0700986,"use the preBased both on the cited papers and available source code. One could also consider token-level estimates. To our knowledge, these have been unused by existing work. 2 2173 3 4 https://github.com/harvardnlp/urnng https://github.com/rloganiv/kglm-model trained model weights. For EntityNLM we train the model from scratch following the procedure described by Ji et al. (2017); results may not be directly comparable due to differences in data preprocessing and hyperparameters. We evaluate models on the datasets used in their original papers: RNNG is evaluated on the Penn Treebank corpus (Marcus et al., 1993), EntityNLM is evaluated on English data from the CoNLL 2012 shared task (Pradhan et al., 2014), and KGLM is evaluated on the Linked WikiText-2 corpus (Logan et al., 2019). Experiments For EntityNLM and KGLM, we experiment with two kinds of proposal distributions: (1) the standard peeking proposal distribution that conditions on future evaluation data, and (2) a non-peeking variant that is conditioned only on the data observed by the model (this is akin to estimating perplexity by ancestral sampling). For RNNG we only experiment with peeking proposals, since a non-peeking variant generates inv"
2020.acl-main.495,N16-1181,0,0.171489,"d field goal. The Texans tried to cut the lead with QB Matt Schaub getting a 8-yard TD pass to WR Andre Johnson, but the Titans would pull away with RB Javon Ringer throwing a 7-yard TD pass . The Texans tried to come back into the game in the fourth quarter, but only came away with Schaub throwing a 12-yard TD pass to WR Kevin Walter. relocate[who threw] find-max-num filter [the second half] find [touchdown pass] Figure 2: An example for a mapping of an utterance to a gold program and a perfect execution in a reasoning problem from NLVR2 (top) and DROP (bottom). Neural module networks (NMNs; Andreas et al., 2016) parse an input utterance into an executable program composed of learnable modules that are designed to perform atomic reasoning tasks and can be composed to perform complex reasoning against an unstructured context. NMNs are appealing since their output is interpretable; they provide a logical meaning representation of the utterance and also the outputs of the intermediate steps (modules) to reach the final answer. However, because module parameters are typically learned from end-task supervision only, it is possible that the program will not be a faithful explanation of the behaviour of the"
2020.acl-main.495,D17-1160,1,0.826889,"d outputs a distribution over the passage tokens and find-num outputs a distribution over the numbers in the passage. We extend their model and introduce additional modules; addition and subtraction to add or subtract passage numbers, and extract-answer which directly predicts an answer span from the representations of passage tokens without any explicit compositional reasoning. We use BERT-base (Devlin et al., 2019) to encode the input question and passage. The Text-NMN does not have access to gold programs, and thus we implement a parser as an encoder-decoder model with attention similar to Krishnamurthy et al. (2017), which takes the utterance as input, and outputs a linearized abstract syntax tree of the predicted program. 5596 3 Module-wise Faithfulness Neural module networks (NMNs) facilitate interpretability of their predictions via the reasoning steps in the structured program and providing the outputs of those intermediate steps during execution. For example, in Figure 2, all reasoning steps taken by both the Visual-NMN and Text-NMN can be discerned from the program and the intermediate module outputs. However, because module parameters are learned from an end-task, there is no guarantee that the mo"
2020.acl-main.495,P19-1416,1,0.915983,"Missing"
2020.acl-main.495,Q19-1016,0,0.0333891,"Missing"
2020.acl-main.495,C00-2137,0,\N,Missing
2020.acl-main.495,P17-2034,0,\N,Missing
2020.acl-main.495,D18-1259,0,\N,Missing
2020.acl-main.495,N19-1246,1,\N,Missing
2020.acl-main.495,N19-1423,0,\N,Missing
2020.acl-main.495,P19-1644,0,\N,Missing
2020.acl-main.495,D19-1002,0,\N,Missing
2020.acl-main.495,D19-1455,0,\N,Missing
2020.acl-main.495,2020.acl-main.386,0,\N,Missing
2020.acl-main.497,D19-1609,0,0.0358835,"Missing"
2020.acl-main.497,P07-1036,0,0.0746863,"e dataset. Our work shows that collecting intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending"
2020.acl-main.497,D19-1606,1,0.892547,"Missing"
2020.acl-main.497,N19-1423,0,0.0281192,"we train multiple models for the DROP and Quoref datasets, and evaluate the benefits of intermediate annotations as compared to traditional QA pairs. In particular, we will focus on the cost vs benefit tradeoff of intermediate annotations, along with evaluating their ability to mitigate bias in the training data. 3.1 Setup We study the impact of annotations on DROP on two models at the top of the leaderboard: NABERT1 and MTMSN (Hu et al., 2019). Both the models employ a similar arithmetic block introduced in the baseline model (Dua et al., 2019) on top of contextual representations from BERT (Devlin et al., 2019). For Quoref, we use the baseline XLNet (Yang et al., 2019) model released with the dataset. We supervise these models with the annotations in a simple way, by jointly predicting intermediate annotation and the final answer. We add two auxiliary loss terms to the marginal loglikelihood loss function. The first is a cross-entropy loss between the gold annotations (g) and predicted annotations, which are obtained by passing the final BERT representations through a linear layer to get a score per token p, then normalizing each token’s score of being selected as an annotation 5628 1 https://github"
2020.acl-main.497,N19-1246,1,0.935372,"ss on 4th-and-goal with 15 seconds left in regulation. The Bears then took a knee to force overtime.... The Bears then won on Jay Cutler’s game-winning 39-yard TD pass to wide receiver Devin Aromashodu. With the loss, not only did the Vikings fall to 11-4, they also surrendered homefield advantage to the Saints. Figure 1: Example from DROP, showing the intermediate annotations that we collected via crowd-sourcing. Introduction Recently many reading comprehension datasets requiring complex and compositional reasoning over text have been introduced, including HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), Quoref (Dasigi et al., 2019), and ROPES (Lin et al., 2019). However, models trained on these datasets (Hu et al., 2019; Andor et al., 2019) only have the final answer as supervision, leaving the model guessing at the correct latent reasoning. Figure 1 shows an example from DROP, which requires first locating various operands (i.e. relevant spans) in the text and then performing filter and count operations over them to get the final answer “3”. However, the correct answer can also be obtained by extracting the span “3” from the passage, or by adding or subtracting various numbers in the passa"
2020.acl-main.497,D19-1107,0,0.0635601,"Missing"
2020.acl-main.497,N18-2017,0,0.0382312,"Missing"
2020.acl-main.497,D19-1170,0,0.0868799,"on Jay Cutler’s game-winning 39-yard TD pass to wide receiver Devin Aromashodu. With the loss, not only did the Vikings fall to 11-4, they also surrendered homefield advantage to the Saints. Figure 1: Example from DROP, showing the intermediate annotations that we collected via crowd-sourcing. Introduction Recently many reading comprehension datasets requiring complex and compositional reasoning over text have been introduced, including HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), Quoref (Dasigi et al., 2019), and ROPES (Lin et al., 2019). However, models trained on these datasets (Hu et al., 2019; Andor et al., 2019) only have the final answer as supervision, leaving the model guessing at the correct latent reasoning. Figure 1 shows an example from DROP, which requires first locating various operands (i.e. relevant spans) in the text and then performing filter and count operations over them to get the final answer “3”. However, the correct answer can also be obtained by extracting the span “3” from the passage, or by adding or subtracting various numbers in the passage. The lack of intermediate hints makes learning challenging and can lead the model to rely on data biases, limiting it"
2020.acl-main.497,D16-1011,0,0.0274704,"ll vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. We proposed a simple semi-supervision technique to expose the model to these annotations. We believe that in future they can be used more directly to yield better performance gains. We have also released these annotations for the research com"
2020.acl-main.497,D19-5808,1,0.88554,"Missing"
2020.acl-main.497,P08-1099,0,0.0438387,"reasoning steps for entire dataset. Our work shows that collecting intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that w"
2020.acl-main.497,P19-1416,1,0.8916,"Missing"
2020.acl-main.497,Q19-1016,0,0.0316898,"appendix. 4 Related Work Similar to our work, Zaidan et al. (2007) studied the impact of providing explicit supervision via rationales, rather than generating them, for varying fractions of training set in text classification. However, we study the benefits of such supervision for complex compositional reading comprehension datasets. In the field of computer vision, Donahue and Grauman (2011) collected similar annotations, for visual recognition, where crowd-workers highlighted relevant regions in images. Within reading comprehension, various works like HotpotQA (Yang et al., 2018) and CoQA (Reddy et al., 2019) have collected similar reasoning steps for entire dataset. Our work shows that collecting intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain k"
2020.acl-main.497,N16-3020,1,0.133874,"otations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. We proposed a simple semi-supervision technique to expose the model to these annotations. We believe that in future they can be used more directly to yield better performance gains. We have also released these annotations for the research community at https: //gith"
2020.acl-main.497,N15-1118,1,0.835281,"intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. We proposed a simple semi-supervisio"
2020.acl-main.86,D19-1606,1,0.842367,"strategy is used by the baseline model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref, the second smallest dataset,"
2020.acl-main.86,N19-1246,1,0.913652,"mportant, it is also important to explore the optimal way to structure training; as we will show, training on instances from diverse datasets (tasks) means that unlike in a single-task setting, ample instances from each task distribution must be represented during training to properly capture that diversity. We explore 2 fundamental aspects of structuring multi-task training: how many instances are sampled from each task per epoch and how those instances are organized within the epoch. We investigate the importance of this structuring by training a multi-task model on the 8 datasets from ORB (Dua et al., 2019b), a recent multi-task reading comprehension benchmark. We first explore the sampling distribution over datasets at each epoch: how many instances from each dataset should be used to train. Prior work has typically either made this a uniform distribution over datasets (implicitly favoring smaller datasets), a distribution proportional to the sizes of the datasets (implicitly favoring larger datasets), or some combination of the two. Because these sampling strategies favor some datasets over others, they can lead to catastrophic forgetting in the non-favored datasets. We introduce a dynamic sa"
2020.acl-main.86,D19-5801,0,0.018457,"nswer” questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch. Three training sessions were conducted per sampling method and the EM and F1 scores shown are averaged over those three sessions. Note that NarrativeQA is evaluated using only ROUGE F1 score. Due to GPU memory constraints, we are limited to a batch size of 4, so we are unable replicate the Uniform Batches configuration of MRQA (requires a batch size of 8 to fit 1 instance from each of the 8 datasets). Uniform Batches This scheduling strategy is used by the baseline model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and S"
2020.acl-main.86,D19-5808,1,0.829585,"line model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref, the second smallest dataset, is still relatively high,"
2020.acl-main.86,P18-2124,0,0.0523173,"Missing"
2020.acl-main.86,D16-1264,0,0.0492107,"ared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref, the second smallest dataset, is still relatively high, which might be explained by its"
2020.acl-main.86,P19-1485,0,0.037067,"tegies, mitigating the catastrophic forgetting that is common in multi-task learning. We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level. Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark. 1 Introduction Building multi-task reading comprehension systems has received significant attention and been a focus of active research (Talmor and Berant, 2019; Xu et al., 2019). These approaches mostly focus on model architecture improvements or generalizability to new tasks or domains. While these contributions are important, it is also important to explore the optimal way to structure training; as we will show, training on instances from diverse datasets (tasks) means that unlike in a single-task setting, ample instances from each task distribution must be represented during training to properly capture that diversity. We explore 2 fundamental aspects of structuring multi-task training: how many instances are sampled from each task per epoch and"
2020.acl-main.86,W17-2623,0,0.0206412,". Uniform Batches This scheduling strategy is used by the baseline model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref,"
2020.emnlp-demos.17,D15-1075,0,0.0446295,"ay have multiple annotations. Although the number of dataset formats can be arbitrary, we observe that the most basic formats fall into the following categories: multiple-choice, span selection, and free text generation. For instance, to emulate the data collection process used for the CoNLL-2003 shared task on named entity recognition (Tjong Kim Sang and De Meulder, 2003), one could use a combination of a span selection (for selecting a named entity) and a multiple-choice question (selecting whether it is a person, location, etc.); for the process used for natural language inference in SNLI (Bowman et al., 2015), one could use an input box (for writing a hypothesis) and a multiple-choice question (for selecting whether the hypothesis entails or contradicts the premise); for reading comprehension tasks in the question-answering (QA) format, one could use an input box (for writing a question) and a multiplechoice question (for yes/no answers; Clark et al. (2019)), a span selection (for span-based answers; Rajpurkar et al. (2016)), or another input box (for free text answers; Koˇcisk`y et al. (2018)). These annotation types are built in C ROWDAQ,5 which requesters can easily use to compose complex UIs."
2020.emnlp-demos.17,N19-1300,0,0.0194037,"d De Meulder, 2003), one could use a combination of a span selection (for selecting a named entity) and a multiple-choice question (selecting whether it is a person, location, etc.); for the process used for natural language inference in SNLI (Bowman et al., 2015), one could use an input box (for writing a hypothesis) and a multiple-choice question (for selecting whether the hypothesis entails or contradicts the premise); for reading comprehension tasks in the question-answering (QA) format, one could use an input box (for writing a question) and a multiplechoice question (for yes/no answers; Clark et al. (2019)), a span selection (for span-based answers; Rajpurkar et al. (2016)), or another input box (for free text answers; Koˇcisk`y et al. (2018)). These annotation types are built in C ROWDAQ,5 which requesters can easily use to compose complex UIs. For our example project, we would like the annotator to select a quantity from the “snippet” object in the contexts, and then tell us whether it is relevant to COVID-19 (see below for how to build it and Fig. 6 in the appendix for visualization). ""annotations"": [ { ""type"": ""span-from-text"", ""from_context"": ""snippet"", ""prompt"": ""Select one quantity from"
2020.emnlp-demos.17,N16-1104,0,0.0206969,"projects, if desired, e.g., using the same E XAM, to get similarly-qualified workers on their follow-up project. Although Fig. 1 shows a complete pipeline of using C ROWDAQ and MTurk, C ROWDAQ is implemented in such a way that data requesters have the flexibility to use only part of it. For instance, one can only use I NSTRUCTION to host and render Markdown files, only use E XAM to test annotators, or only use TASK S ET to quickly build annotation UIs. One can also create even more advanced workflows, e.g., using multiple E XAMS and filtering annotators sequentially (e.g., Gated Instruction; Liu et al., 2016), creating a second TASK S ET to validate previous annotations, or splitting a single target dataset into multiple components, each of which has its own E XAM and TASK S ET. In addition, data collection with in-house annotators can be done on C ROWDAQ directly, instead of via MTurk. For instance, data requesters can conveniently create a contrast set (Gardner et al., 2020) on C ROWDAQ by themselves. We have put more use cases into the appendix, including DROP (Dua et al., 2019), MATRES (Ning et al., 2018), TORQUE (Ning et al., 2020), VQA-E (Li et al., 2018), and two ongoing projects. 5 Customi"
2020.emnlp-demos.17,2020.emnlp-main.88,1,0.786045,"nd filtering annotators sequentially (e.g., Gated Instruction; Liu et al., 2016), creating a second TASK S ET to validate previous annotations, or splitting a single target dataset into multiple components, each of which has its own E XAM and TASK S ET. In addition, data collection with in-house annotators can be done on C ROWDAQ directly, instead of via MTurk. For instance, data requesters can conveniently create a contrast set (Gardner et al., 2020) on C ROWDAQ by themselves. We have put more use cases into the appendix, including DROP (Dua et al., 2019), MATRES (Ning et al., 2018), TORQUE (Ning et al., 2020), VQA-E (Li et al., 2018), and two ongoing projects. 5 Customizable UI To the best of our knowledge, existing works on customizable annotation UI, e.g., MMAX211 (M¨uller and Strube, 2006), PALinkA (Or˘asan, 2003), and BRAT12 (Stenetorp et al., 2012), were mainly designed for in-house annotators on classic NLP tasks, and their adaptability and extensibility are limited. Related Work Crowdsourcing Platforms The most commonly used platform at present is MTurk, and the features C ROWDAQ provides are overall complementary to it. C ROWDAQ provides integration with MTurk, but it also allows for in-ho"
2020.emnlp-demos.17,D19-1224,0,0.0197142,"orking independently to achieve sufficient scale, either in dataset size or collection time. To work with multiple annotators, data requesters (i.e., AI researchers and engineers) usually need to design a user-friendly annotation interface and a quality control mechanism. However, this involves a lot of overhead: we often spend most of the time resolving frontend bugs and manually checking or communicating with individual annotators to filter out those who are unqualified, instead of focusing on core research questions. Another issue that has recently gained more attention is reproducibility. Dodge et al. (2019) and Pineau (2020) provide suggestions for system reproducibility, and Bender and Friedman (2018) and 1 Crowdsourcing with Automated Qualifcation; https: //www.crowdaq.com/ 2 This holds not only for collecting static data annotations, but also for collecting human judgments of system outputs. Gebru et al. (2018) propose “data statements” and “datasheets for datasets” for data collection reproducibility. However, due to irreproducible human interventions in training and selecting annotators and the potential difficulty in replicating the annotation interfaces, it is often difficult to reuse or"
2020.emnlp-demos.17,P18-1122,1,0.829601,".g., using multiple E XAMS and filtering annotators sequentially (e.g., Gated Instruction; Liu et al., 2016), creating a second TASK S ET to validate previous annotations, or splitting a single target dataset into multiple components, each of which has its own E XAM and TASK S ET. In addition, data collection with in-house annotators can be done on C ROWDAQ directly, instead of via MTurk. For instance, data requesters can conveniently create a contrast set (Gardner et al., 2020) on C ROWDAQ by themselves. We have put more use cases into the appendix, including DROP (Dua et al., 2019), MATRES (Ning et al., 2018), TORQUE (Ning et al., 2020), VQA-E (Li et al., 2018), and two ongoing projects. 5 Customizable UI To the best of our knowledge, existing works on customizable annotation UI, e.g., MMAX211 (M¨uller and Strube, 2006), PALinkA (Or˘asan, 2003), and BRAT12 (Stenetorp et al., 2012), were mainly designed for in-house annotators on classic NLP tasks, and their adaptability and extensibility are limited. Related Work Crowdsourcing Platforms The most commonly used platform at present is MTurk, and the features C ROWDAQ provides are overall complementary to it. C ROWDAQ provides integration with MTurk,"
2020.emnlp-demos.17,N19-1246,1,0.848214,"more advanced workflows, e.g., using multiple E XAMS and filtering annotators sequentially (e.g., Gated Instruction; Liu et al., 2016), creating a second TASK S ET to validate previous annotations, or splitting a single target dataset into multiple components, each of which has its own E XAM and TASK S ET. In addition, data collection with in-house annotators can be done on C ROWDAQ directly, instead of via MTurk. For instance, data requesters can conveniently create a contrast set (Gardner et al., 2020) on C ROWDAQ by themselves. We have put more use cases into the appendix, including DROP (Dua et al., 2019), MATRES (Ning et al., 2018), TORQUE (Ning et al., 2020), VQA-E (Li et al., 2018), and two ongoing projects. 5 Customizable UI To the best of our knowledge, existing works on customizable annotation UI, e.g., MMAX211 (M¨uller and Strube, 2006), PALinkA (Or˘asan, 2003), and BRAT12 (Stenetorp et al., 2012), were mainly designed for in-house annotators on classic NLP tasks, and their adaptability and extensibility are limited. Related Work Crowdsourcing Platforms The most commonly used platform at present is MTurk, and the features C ROWDAQ provides are overall complementary to it. C ROWDAQ provi"
2020.emnlp-demos.17,W03-2120,0,0.264013,"Missing"
2020.emnlp-demos.17,D16-1264,0,0.0637398,"tion (for selecting a named entity) and a multiple-choice question (selecting whether it is a person, location, etc.); for the process used for natural language inference in SNLI (Bowman et al., 2015), one could use an input box (for writing a hypothesis) and a multiple-choice question (for selecting whether the hypothesis entails or contradicts the premise); for reading comprehension tasks in the question-answering (QA) format, one could use an input box (for writing a question) and a multiplechoice question (for yes/no answers; Clark et al. (2019)), a span selection (for span-based answers; Rajpurkar et al. (2016)), or another input box (for free text answers; Koˇcisk`y et al. (2018)). These annotation types are built in C ROWDAQ,5 which requesters can easily use to compose complex UIs. For our example project, we would like the annotator to select a quantity from the “snippet” object in the contexts, and then tell us whether it is relevant to COVID-19 (see below for how to build it and Fig. 6 in the appendix for visualization). ""annotations"": [ { ""type"": ""span-from-text"", ""from_context"": ""snippet"", ""prompt"": ""Select one quantity from below."", ""id"": ""quantity"", }, { ""type"": ""multiple-choice"", ""prompt"":"
2020.emnlp-demos.17,E12-2021,0,0.114103,"Missing"
2020.emnlp-demos.17,Q18-1023,0,0.0904756,"Missing"
2020.emnlp-demos.17,W03-0419,0,0.576046,"Missing"
2020.emnlp-main.105,P13-2009,0,0.0126787,"however, is that hypotheses in NLI and questions in RC are typically only paired with single inputs. In fact, they typically only make sense for a single input, and thus it is hard to characterize these narrow questions as “task descriptions”. Lastly, the problem of learning from task descriptions is fundamentally one of translating a natural language description into some executable function that can operate on arbitrary inputs. This problem has been well-studied for narrow domains in the semantic parsing literature (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Andreas et al., 2013), though the input is typically a single static database, not arbitrary natural language text. Attempts to generalize semantic parsing to more open domains are still nascent (Chen et al., 2020; Gupta et al., 2020). 3 Instantiating the Framework Section 2 showed a framework for training and testing a general purpose system that could perform unseen NLP tasks. An ideal system in this framework would be able to read the descriptions of the tasks in the GLUE suite (Wang et al., 2019) and perform well with no additional training. However, this goal is far beyond the current capabilities of today’s"
2020.emnlp-main.105,P11-1060,0,0.0237023,"n. A key difference, however, is that hypotheses in NLI and questions in RC are typically only paired with single inputs. In fact, they typically only make sense for a single input, and thus it is hard to characterize these narrow questions as “task descriptions”. Lastly, the problem of learning from task descriptions is fundamentally one of translating a natural language description into some executable function that can operate on arbitrary inputs. This problem has been well-studied for narrow domains in the semantic parsing literature (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Andreas et al., 2013), though the input is typically a single static database, not arbitrary natural language text. Attempts to generalize semantic parsing to more open domains are still nascent (Chen et al., 2020; Gupta et al., 2020). 3 Instantiating the Framework Section 2 showed a framework for training and testing a general purpose system that could perform unseen NLP tasks. An ideal system in this framework would be able to read the descriptions of the tasks in the GLUE suite (Wang et al., 2019) and perform well with no additional training. However, this goal is far beyond the current c"
2020.emnlp-main.105,C16-1017,0,0.029553,"Socher et al., 2013; Norouzi et al., 2013) asks systems to generalize to unseen classes at test time. In this approach, the task is the same at both train and test time—models are only asked to generalize to new classes. In terms of the graphical model in Fig. 1, prior work attaches a natural language description to some new yi at test time. In contrast, our approach asks models to generalize to entire unseen tasks, attaching the natural language description to the task variable τ . Zero-shot learning has been widely adopted including for classification (Dauphin et al., 2013), entity typing (Ma et al., 2016; Zhou et al., 2018) and relation extraction (Levy et al., 2017; Shi and Lin, 2019). More closely related to our approach are the zero-shot experiments in Radford et al. (2019); Brown et al. (2020) that provide a generative language model with a prompt (that could be viewed as a type of task description) and asks for a completion. This is similar to the observation in Petroni et al. (2019) that it is possible to extract knowledge graph relationships from large language models with an appropriate prompt. Z EST provides a benchmark dataset for systematically measuring how well models can general"
2020.emnlp-main.105,N18-1202,1,0.406771,"output pairs. In contrast, humans learn to perform the same task by reading a description, after which they are able to perform the task in a zero-shot manner—indeed, this is how crowd-sourced NLP datasets are constructed. In this paper, we argue that learning from task descriptions in this way is a necessary attribute of a general purpose NLP system, and we propose it as a new paradigm to train and test NLP systems. Recent work in NLP has shown significant progress in learning tasks from examples. Large pretrained language models have dramatically improved performance on standard benchmarks (Peters et al., 2018; Devlin et al., 2019; Raffel et al., ∗Work done while at the Allen Institute for AI. Data, evaluation code, baseline models, and leaderboard at https://allenai.org/data/zest 1 (b) Despite this progress, there are many serious issues that come with learning from examples. There is an almost infinite number of tasks that a person might wish to solve with a general-purpose NLP system. Learning to solve these tasks by reading a description instead of observing a collection of examples would solve the problem of having to create training sets for each language task. Such a system would also be mor"
2020.emnlp-main.105,D19-1250,0,0.0513557,"Missing"
2020.emnlp-main.105,N18-1101,0,0.0478422,"oposed framework and tasks such as natural language inference (NLI) or reading comprehension (RC), where two natural language inputs (a 1363 premise and a hypothesis for NLI, and a question and passage for RC) are used to predict some output. In our case, we have two observed variables, x and dτ , which influence the prediction of the output y (Fig. 1). Indeed, the baseline model that we discuss in Section 5 takes a similar approach to NLI and RC and jointly models the two textual inputs. This correspondence has been used in prior work, where Yin et al. (2019) used a model pretrained on MNLI (Williams et al., 2018) to perform zero-shot text classification. A key difference, however, is that hypotheses in NLI and questions in RC are typically only paired with single inputs. In fact, they typically only make sense for a single input, and thus it is hard to characterize these narrow questions as “task descriptions”. Lastly, the problem of learning from task descriptions is fundamentally one of translating a natural language description into some executable function that can operate on arbitrary inputs. This problem has been well-studied for narrow domains in the semantic parsing literature (Zelle and Moone"
2020.emnlp-main.105,D18-1231,0,0.0187276,"013; Norouzi et al., 2013) asks systems to generalize to unseen classes at test time. In this approach, the task is the same at both train and test time—models are only asked to generalize to new classes. In terms of the graphical model in Fig. 1, prior work attaches a natural language description to some new yi at test time. In contrast, our approach asks models to generalize to entire unseen tasks, attaching the natural language description to the task variable τ . Zero-shot learning has been widely adopted including for classification (Dauphin et al., 2013), entity typing (Ma et al., 2016; Zhou et al., 2018) and relation extraction (Levy et al., 2017; Shi and Lin, 2019). More closely related to our approach are the zero-shot experiments in Radford et al. (2019); Brown et al. (2020) that provide a generative language model with a prompt (that could be viewed as a type of task description) and asks for a completion. This is similar to the observation in Petroni et al. (2019) that it is possible to extract knowledge graph relationships from large language models with an appropriate prompt. Z EST provides a benchmark dataset for systematically measuring how well models can generalize to many tasks in"
2020.emnlp-main.245,P19-1285,0,0.0491807,"Missing"
2020.emnlp-main.245,N19-1423,0,0.0346298,"bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage (background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension model that performs chai"
2020.emnlp-main.245,N19-1246,1,0.875678,"Missing"
2020.emnlp-main.245,P19-1222,0,0.0170661,"omprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differ"
2020.emnlp-main.245,N16-1181,0,0.225016,"he variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to understand and chain together free-form predicates and logical connectives. The proposed model is inspired by neural module networks (NMNs), which were proposed for visual question answering (Andreas et al., 2016b,a). NMNs assemble a network from a collection of specialized modules where each module performs some learnable function, such as locating a question word in an image, or recognizing relationships between objects in the image. The modules are composed together specific to what is asked in the question, then executed to obtain an answer. We design general modules that are targeted at the reasoning necessary for ROPES and compose them together to answer questions. We design three kinds of basic modules to learn the neuro-symbolic multi-step inference over questions, situations, and background p"
2020.emnlp-main.245,D19-5817,1,0.83692,"19; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to d"
2020.emnlp-main.245,D19-1455,0,0.0745637,"tion, then successively chains them, making a prediction after including the situation in the chaining. ∗ Work done during an internship at Allen Institute for Artificial Intelligence. Prior work addressing this problem has largely 3040 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3040–3050, c November 16–20, 2020. 2020 Association for Computational Linguistics either used symbolic reasoning, such as markov logic networks (Khot et al., 2015) and integer linear programming (Khashabi et al., 2016), or blackbox neural networks (Jiang et al., 2019; Jiang and Bansal, 2019). Symbolic methods give some measure of interpretability and the ability to handle logical operators to track polarity, but they are brittle, unable to handle the variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to understand and chain together fr"
2020.emnlp-main.245,P19-1261,0,0.0780502,"background and question, then successively chains them, making a prediction after including the situation in the chaining. ∗ Work done during an internship at Allen Institute for Artificial Intelligence. Prior work addressing this problem has largely 3040 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3040–3050, c November 16–20, 2020. 2020 Association for Computational Linguistics either used symbolic reasoning, such as markov logic networks (Khot et al., 2015) and integer linear programming (Khashabi et al., 2016), or blackbox neural networks (Jiang et al., 2019; Jiang and Bansal, 2019). Symbolic methods give some measure of interpretability and the ability to handle logical operators to track polarity, but they are brittle, unable to handle the variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to underst"
2020.emnlp-main.245,D15-1080,0,0.0761646,"Missing"
2020.emnlp-main.245,P18-1249,0,0.0998005,"tial experiments, and we performed an analysis of the data to figure out the cause. ROPES used an annotator split to separate the train, dev, and test sets in order to avoid annotator bias (Geva et al., 2019), but we discovered that this led to a large distributional shift between train/dev and test, which we explore in this section. In light of this analysis, we recommend treating the dev set as an in-domain test set, and the original test set as an out-of-domain test. Answer types Our analysis is based on looking at the syntactic category of the answer phrase. We use the syntactic parser of Kitaev and Klein (2018) to obtain constituent trees for the passages in ROPES. We take the constituent label of the lowest subtree that covers the answer span3 as the answer type. The four most frequent answer types in ROPES are noun phrase (NP), verb phrase (VP), adjective phrase (ADJP) and adverb phrase (ADVP). Table 1 shows examples for each type. Most NP answers 3 The passages could have more than one span that matches the answer; we use the last occurrence of the answer span for our analysis. 3043 Type NP VP ADJP ADVP Others Passage ...The child poured two spoonfuls of sugar into cup A and three spoonfuls of su"
2020.emnlp-main.245,D19-5808,1,0.903659,"ion: Which category of flowers would be more likely to have brightly colored petals? Introduction Answer: category B Performing chained inference over natural language text is a long-standing goal in artificial intelligence (Grosz et al., 1986; Reddy, 2003). This kind of inference requires understanding how natural language statements fit together in a way that permits drawing conclusions. This is very challenging without a formal model of the semantics underlying the text, and when polarity needs to be tracked across many statements. For instance, consider the example in Figure 1 from ROPES (Lin et al., 2019), a recently released reading comprehension dataset that requires applying information contained in a background paragraph to a new situation. To answer the question, one must associate each category of flowers with a polarity for having brightly colored petals, which must be done by going through the information about pollinators given in the situation and linking it to what was said about pollinators and brightly colored petals in the background paragraph, along with tracking the polarity of those statements. (a) background question situation S ELECT C HAIN C HAIN P REDICT category B (b) Fig"
2020.emnlp-main.245,2021.ccl-1.108,0,0.180833,"Missing"
2020.emnlp-main.245,P19-1613,0,0.0172757,"several reading comprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker"
2020.emnlp-main.245,N18-1202,1,0.510184,", where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage (background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension mod"
2020.emnlp-main.245,D16-1264,0,0.121792,"Missing"
2020.emnlp-main.245,D19-1374,0,0.01434,"background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension model that performs chained inference over natural language text. We have demonstrated that our model substantially outperforms prior work on ROPES, a challenging new reading comprehension dataset. We have additionally presented some analysis of ROPES that should inform future work on this dataset. While our model is not a neural module network, as our model uses"
2020.emnlp-main.245,P19-1260,0,0.0183596,"ize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage"
2020.emnlp-main.245,W18-5446,0,0.0697258,"Missing"
2020.emnlp-main.245,Q18-1021,0,0.0301516,"Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning"
2020.emnlp-main.245,D18-1259,0,0.0516665,"to say, the crash rate per cyclist goes down as the cycle volume increases... Situation: ...Day 1 had 500 cyclists left. Day 2 had Related Work 400 cyclists left. Day 3 had 300 cyclists left. Day 4 had Neural Module Networks were originally proposed for visual question answering tasks (Andreas et al., 2016b,a), and recently have been used on several reading comprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the docu"
2020.emnlp-main.245,P19-1009,0,0.0506588,"Missing"
2020.emnlp-main.245,P19-1218,0,0.021383,"20), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the eff"
2020.emnlp-main.528,W05-0909,0,0.254165,"wide range of RC phenomena such as commonsense reasoning and understanding narrative over movie scripts. After collecting all annotations, we follow work on creating more robust evaluation sets (Kaushik et al., 2020; Gardner et al., 2020) and augment the test set of MOCHA by manually writing a small set of minimal pairs (Table 3). The set of minimal pairs serve as a harder evaluation set for probing metric robustness. Using MOCHA, we train a Learned Metric for Reading Comprehension which we abbreviate as LERC. We compare LERC against two sets of baselines: (1) existing metrics such as METEOR (Banerjee and Lavie, 2005) and BERTScore (Zhang et al., 2019); and (2) a sentence similarity model trained on STS-B (Cer et al., 2017). To ensure fair comparison, we evaluate LERC in an out-of-dataset setting: LERC is trained on all datasets except the one it is being evaluated on. On the test set, LERC outperforms baselines by as much as 36 Pearson correlation points and on the minimal pairs set, by as much as 26 accuracy points. Error analysis and minimal pair results indicate that there is substantial room to improve the robustness of LERC and its sensitivity to different linguistic phenomena. We hope that MOCHA and"
2020.emnlp-main.528,D18-1454,0,0.030811,"tion for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of this work is in developing and evaluating metrics for generative RC. However, we wanted to see whether a learned metric could do well on span-selection datasets. We collected Collecting Candidates Candidates on all four generative datasets are generated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for NarrativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each question has two references. We treat the second reference as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT2 models for DROP. Models are trained on the training sets of each constituent dataset and candidates are produced on instances from the validation set (and test set if available). We filtered out candidates that exactly matc"
2020.emnlp-main.528,W17-4755,0,0.0500144,"Missing"
2020.emnlp-main.528,W16-2302,0,0.0600135,"Missing"
2020.emnlp-main.528,S17-2001,0,0.0817922,"Missing"
2020.emnlp-main.528,D19-5817,1,0.872004,"is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6521–6532, c November 16–20, 2020. 2020 Association for Computational Linguistics rent metrics also only consider the reference and are agnostic to the end-task being evaluated. Fig. 1 demonstrates that this is problematic for generative RC because scoring a candidate may require a metric to also consider the passage and the question. Without cheap and reliable evaluation, progress in generative reading comprehension has been extremely slow. To addres"
2020.emnlp-main.528,N19-1300,0,0.196703,"with a MSE loss. yˆi = W hi [CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new datase"
2020.emnlp-main.528,P19-1264,0,0.108636,"with a MSE loss. yˆi = W hi [CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new datase"
2020.emnlp-main.528,D19-1606,1,0.893089,"Missing"
2020.emnlp-main.528,N19-1423,0,0.0477598,"e human judgement scores, s1 and s2 , collected using the same interface in Fig. 2. The minimal pair is created so that c1 has a higher score (i.e. is a better answer) than c2 . Each minimal pair is designed to capture a particular linguistic phenomenon (see Table 3). Using this set of minimal pairs, we can study how often a metric prefers the better candidate. We create 200 minimal pairs (50 for each generative QA dataset), which we use for evaluation separately from the original test set. 3 A Learned Metric We provide details on LERC, our learned metric. LERC is initialized using BERT-base (Devlin et al., 2019) We define as input a tuple consisting of a passage, p, a question, q, a reference answer, a, and a candidate answer, ˆa. The input to BERT is 6525 Metric NarrativeQA Dev Test MCScript Dev Test CosmosQA Dev Test SocialIQA Dev Test DROP Dev Test Quoref Dev Test Avg. r Dev Test BLEU-1 METEOR ROUGE-L BERTScore 0.403 0.605 0.434 0.419 0.472 0.615 0.495 0.534 0.181 0.461 0.224 0.172 0.260 0.502 0.297 0.194 0.660 0.696 0.701 0.803 0.670 0.711 0.701 0.779 0.595 0.644 0.599 0.604 0.549 0.637 0.558 0.584 0.409 0.664 0.480 0.174 0.387 0.568 0.366 0.328 0.674 0.729 0.712 0.207 0.578 0.716 0.604 0.286 0.4"
2020.emnlp-main.528,N19-1246,1,0.915534,"t are diverse in their domains and answer types. This ensures that training and evaluation with MOCHA does not overfit to the characteristics of any constituent dataset. NarrativeQA (Kocisk´y et al., 2017) tests reasoning about events, entities, and their relations on movie scripts and book summaries. MCScript (Ostermann et al., 2018) tests reasoning on stories written for a child-level reader. CosmosQA (Huang et al., 2019) tests commonsense reasoning on blogs describing everyday events. SocialIQA (Sap et al., 2019) tests social reasoning with passages constructed from a knowledge base. DROP (Dua et al., 2019) tests predicate argument structure and numerical reasoning on Wikipedia articles concerning American football games, census results, and history. Quoref (Dasigi et al., 2019) tests coreferential reasoning on Wikipedia articles. NarrativeQA was created as a generative RC dataset. CosmosQA, MCScript, and SocialIQA were created as MC datasets which we re-purpose as generative datasets by using the correct choice as the reference. Our motivation for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of th"
2020.emnlp-main.528,2020.emnlp-main.751,0,0.0731735,"Missing"
2020.emnlp-main.528,D19-1107,0,0.0223833,"e (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pag"
2020.emnlp-main.528,D19-1243,0,0.0380112,"Missing"
2020.emnlp-main.528,D17-1215,0,0.0326025,"sing a span-selection or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Metho"
2020.emnlp-main.528,D17-1082,0,0.144271,"e. Human Judgement: 5 out of 5 LERC: 4.98 out of 5 BLEU-1: 0.07 ROUGE-L: 0.15 METEOR: 0.17 Figure 1: Generative reading comprehension example. Properly scoring the candidate requires access to the passage. Current metrics, such as BLEU, ROUGE and METEOR, are agnostic to the end-task while LERC is trained with the passage and question as input. As a result, LERC assigns a score that better reflects human judgement. Introduction Reading comprehension (RC) has seen significant progress in the last few years, with a number of question answering (QA) datasets being created (Rajpurkar et al., 2016; Lai et al., 2017; Talmor et al., 2018). However, a majority of datasets are presented using a span-selection or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and o"
2020.emnlp-main.528,W04-1013,0,0.109362,"Missing"
2020.emnlp-main.528,W18-6450,0,0.0134631,"to question answering, where the passage and question should be assimilated. The final category consists of metrics learned end-to-end from human judgements (Cui et al., 2018; Sellam et al., 2020). These metrics are flexible in that they can be tuned to the specific evaluation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Usin"
2020.emnlp-main.528,W17-4768,0,0.0165145,"but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing metric, considerab"
2020.emnlp-main.528,W19-5302,0,0.040145,"Missing"
2020.emnlp-main.528,W14-3336,0,0.0730042,"Missing"
2020.emnlp-main.528,P19-1416,1,0.819886,"or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Lang"
2020.emnlp-main.528,L18-1564,0,0.0490233,"lar a candidate is to a reference using the passage and the question. ing human judgement scores, and creating minimal pairs for evaluation. candidates on two span-based datasets, DROP and Quoref, to test this. 2.1 2.2 Datasets Candidates in MOCHA come from 6 constituent QA datasets that are diverse in their domains and answer types. This ensures that training and evaluation with MOCHA does not overfit to the characteristics of any constituent dataset. NarrativeQA (Kocisk´y et al., 2017) tests reasoning about events, entities, and their relations on movie scripts and book summaries. MCScript (Ostermann et al., 2018) tests reasoning on stories written for a child-level reader. CosmosQA (Huang et al., 2019) tests commonsense reasoning on blogs describing everyday events. SocialIQA (Sap et al., 2019) tests social reasoning with passages constructed from a knowledge base. DROP (Dua et al., 2019) tests predicate argument structure and numerical reasoning on Wikipedia articles concerning American football games, census results, and history. Quoref (Dasigi et al., 2019) tests coreferential reasoning on Wikipedia articles. NarrativeQA was created as a generative RC dataset. CosmosQA, MCScript, and SocialIQA were"
2020.emnlp-main.528,2001.mtsummit-papers.68,0,0.015855,"Missing"
2020.emnlp-main.528,W15-3049,0,0.0113584,"uation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing m"
2020.emnlp-main.528,N18-1023,0,0.0446668,"Missing"
2020.emnlp-main.528,D13-1020,0,0.0444038,"CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new dataset where we do not have human judgeme"
2020.emnlp-main.528,D19-1454,0,0.0562355,"Missing"
2020.emnlp-main.528,2020.acl-main.704,0,0.0246621,"of metrics that use some variant of n-gram matching (Papineni et al., 2001; Lin, 2004; Banerjee and Lavie, 2005). They are easy to implement, but lack flexibility by focusing only on token overlap. The second cateogry of metrics eschew some of the aforementioned issues by calculating a softer similarity score using embeddings of tokens (Clark et al., 2019b; Zhang et al., 2019). However, it is unclear how to tailor them to question answering, where the passage and question should be assimilated. The final category consists of metrics learned end-to-end from human judgements (Cui et al., 2018; Sellam et al., 2020). These metrics are flexible in that they can be tuned to the specific evaluation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Sha"
2020.emnlp-main.528,P16-1009,0,0.0497328,"aset. CosmosQA, MCScript, and SocialIQA were created as MC datasets which we re-purpose as generative datasets by using the correct choice as the reference. Our motivation for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of this work is in developing and evaluating metrics for generative RC. However, we wanted to see whether a learned metric could do well on span-selection datasets. We collected Collecting Candidates Candidates on all four generative datasets are generated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for NarrativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each question has two references. We treat the second reference as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT2 models for DROP. Models are trained on the training sets of"
2020.emnlp-main.528,W18-6456,0,0.0135806,"arge corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing metric, considerable work remains. Error an"
2020.emnlp-main.528,W15-3031,0,0.065281,"Missing"
2020.emnlp-main.528,2020.acl-main.450,0,0.0233687,"which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6521–6532, c November 16–20, 2020. 2020 Association for Computational Linguistics rent metrics also only consider the reference and are agnostic to the end-task being evaluated. Fig. 1 demonstrates that this is problematic for generative RC because scoring a candidate may require a metric to also consider the passage and the question. Without cheap and reliable evaluation, progress in generative reading comprehension has been extremely slow. To address the need for better evaluation metrics"
2020.emnlp-main.86,P17-1171,0,0.115694,"ref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-i"
2020.emnlp-main.86,N19-1405,0,0.0300682,"Missing"
2020.emnlp-main.86,D13-1184,0,0.027616,"Missing"
2020.emnlp-main.86,2020.tacl-1.30,0,0.287457,"question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answer the question (Chen et al.,"
2020.emnlp-main.86,D19-1606,1,0.825824,". The answer is the underlined span. Introduction Humans often read text with the goal of obtaining information. Given that a single document is unlikely to contain all the information a reader might need, the reading process frequently involves identifying the information present in the given document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi"
2020.emnlp-main.86,N19-1423,0,0.0355161,"C Task Overview Formally, a system tackling IIRC is provided with the following inputs: a question Q; a passage P ; a set of links contained in the passage, L = {li }N i=1 ; and the set of articles those links lead to, A = {ai }N i=1 . The surface form of each link, li is a Baseline Model 1. Identify relevant links 2. Select passages from linked articles 3. Pass the concatenated passages to a QA model 3.2.1 Identifying Links To identify the set of relevant links, L0 , in a passage, P, for a question, Q, the model first encodes the concatenation of the question and original passage using BERT (Devlin et al., 2019). It then concatenates the encoded representations of the first and last tokens of each link as input to a scoring function, following the span classification procedure used by Joshi et al. (2013), selecting any links that score above a threshold g. P 0 = BERT([Q||P ]) Score(l) = f ([p0i kp0j ]), l = (pi ...pj , a) L0 = {l : Score(l) &gt; g} where l is a link covering tokens pi ...pj linking to article a. 3.2.2 Selecting Context Given the set, L0 from the previous step, the model then must select relevant context passages from the documents. For each document, it first splits the document into ov"
2020.emnlp-main.86,N19-1246,1,0.90994,"tion for answering the question. The answer is the underlined span. Introduction Humans often read text with the goal of obtaining information. Given that a single document is unlikely to contain all the information a reader might need, the reading process frequently involves identifying the information present in the given document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answeri"
2020.emnlp-main.86,P17-1147,0,0.10626,"2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datase"
2020.emnlp-main.86,2020.emnlp-main.550,0,0.0351698,"separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answer the question (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017; Yang et al., 2018; Seo et al., 2019; Karpukhin et al., 2020; Min et al., 2019a). IIRC is similar in that it also requires the retrieval of missing information. However, the questions are grounded in a given paragraph, meaning that a system must examine more than just the question in order to know what to retrieve. Most questions in IIRC do not make sense in an open-domain setting, without their associated paragraphs. Unanswerable questions Unlike SQuAD 2.0 (Rajpurkar et al., 2018) where the unanswerable questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not written with the goal of bein"
2020.emnlp-main.86,D19-1281,1,0.845722,"able questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not written with the goal of being unanswerable, a property that our dataset shares with NewsQA (Trischler et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TyDi QA (Clark et al., 2020). Results shown in Section 4.3 indicate that these questions cannot be trivially distinguished from answerable questions. Incomplete Information QA A few prior datasets have explored question answering given incomplete information, such as science facts (Mihaylov et al., 2018; Khot et al., 2019). However, these datasets contain multiple choice questions, and the answer choices provide hints as to what information may be needed. Yuan et al. (2020) explore this as well using a POMDP in which the context in existing QA datasets is hidden from the model until it explicitly searches for it. 7 Conclusion We introduced IIRC, a new dataset of incompleteinformation reading comprehension questions. These questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex wa"
2020.emnlp-main.86,Q19-1026,0,0.234108,"n et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answe"
2020.emnlp-main.86,2021.ccl-1.108,0,0.0756007,"Missing"
2020.emnlp-main.86,D18-1260,1,0.883113,"018) where the unanswerable questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not written with the goal of being unanswerable, a property that our dataset shares with NewsQA (Trischler et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TyDi QA (Clark et al., 2020). Results shown in Section 4.3 indicate that these questions cannot be trivially distinguished from answerable questions. Incomplete Information QA A few prior datasets have explored question answering given incomplete information, such as science facts (Mihaylov et al., 2018; Khot et al., 2019). However, these datasets contain multiple choice questions, and the answer choices provide hints as to what information may be needed. Yuan et al. (2020) explore this as well using a POMDP in which the context in existing QA datasets is hidden from the model until it explicitly searches for it. 7 Conclusion We introduced IIRC, a new dataset of incompleteinformation reading comprehension questions. These questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved infor"
2020.emnlp-main.86,D19-1284,1,0.917274,"16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datasets such as HotpotQA (Yang et al., 2018). However, they do not capture the information-seeking questions that arise from reading a single document with partial information (Min et al., 2019b; Chen and Durrett, 2019). We present a new dataset of incomplete information reading comprehension questions, IIRC, to address both of these limitations. IIRC is a crowdsourced dataset of 13441 questions over 5698 paragraphs from English Wikipedia, with most of the questions requiring information from one or more documents hyperlinked to the associated paragraphs, in addition to the original paragraphs themselves. Our crowdsourcing process (Section 2) ensures the questions are naturally information-seeking by decoupling question and answer collection pipelines. Crowd workers are instructed t"
2020.emnlp-main.86,P19-1416,1,0.923311,"16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datasets such as HotpotQA (Yang et al., 2018). However, they do not capture the information-seeking questions that arise from reading a single document with partial information (Min et al., 2019b; Chen and Durrett, 2019). We present a new dataset of incomplete information reading comprehension questions, IIRC, to address both of these limitations. IIRC is a crowdsourced dataset of 13441 questions over 5698 paragraphs from English Wikipedia, with most of the questions requiring information from one or more documents hyperlinked to the associated paragraphs, in addition to the original paragraphs themselves. Our crowdsourcing process (Section 2) ensures the questions are naturally information-seeking by decoupling question and answer collection pipelines. Crowd workers are instructed t"
2020.emnlp-main.86,D16-1241,0,0.027219,"se questions can be answered by focusing on just one of the facts used for building the questions (Min et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a s"
2020.emnlp-main.86,P18-2124,0,0.0867562,"hat provide the missing information for answering the question. The answer is the underlined span. Introduction Humans often read text with the goal of obtaining information. Given that a single document is unlikely to contain all the information a reader might need, the reading process frequently involves identifying the information present in the given document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open"
2020.emnlp-main.86,D16-1264,0,0.0476757,"d context for unanswerable questions. We do this because by definition, unanswerable questions do not have annotated answer context. 4 4.1 Experiments Evaluation Metrics We use two evaluation metrics to compare model performance: Exact-Match (EM), and a numeracyfocused (macro-averaged) F1 score, which measures overlap between a bag-of-words representation of the gold and predicted answers. Due to the number of numeric answers in the data, we follow the evaluation methods used by DROP (Dua et al., 2019b). Specifically, we employ the same implementation of Exact-Match accuracy as used by SQuAD (Rajpurkar et al., 2016), which removes articles and does other simple normalization, and our F1 score is based on that used by SQuAD. We define F1 to be 0 when there is a number mismatch between the gold and predicted answers, regardless of other word overlap. When an answer has multiple spans, we first perform a one-to-one alignment greedily based on bag-of-word overlap on the set of spans and then compute average F1 over each span. For numeric answers, we ignore the units. Binary and unanswerable questions are both treated as span questions. In the unanswerable case, the answer is a special NONE token, and in the"
2020.emnlp-main.86,D19-1251,0,0.0366514,"en document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retri"
2020.emnlp-main.86,P18-1156,0,0.0465345,"cts used for building the questions (Min et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, an"
2020.emnlp-main.86,P19-1436,1,0.842729,"ehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answer the question (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017; Yang et al., 2018; Seo et al., 2019; Karpukhin et al., 2020; Min et al., 2019a). IIRC is similar in that it also requires the retrieval of missing information. However, the questions are grounded in a given paragraph, meaning that a system must examine more than just the question in order to know what to retrieve. Most questions in IIRC do not make sense in an open-domain setting, without their associated paragraphs. Unanswerable questions Unlike SQuAD 2.0 (Rajpurkar et al., 2018) where the unanswerable questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not writt"
2020.emnlp-main.86,P19-1485,0,0.0330224,"Missing"
2020.emnlp-main.86,Q18-1021,0,0.0414415,"of the SQuAD or DROP data requires external links, this evaluation could only negatively impact precision. We find that precision dropped by 8 points, compared to a drop of 28 points when the model trained only on IIRC was used, indicating that the model is able to learn to identify when no external information is required. 6 Related Work Questions requiring multiple contexts Prior multi-context reading comprehension datasets were built by starting from discontiguous contexts, and forming compositional questions by stringing multiple facts either by relying on knowledge graphs as in QAngaroo (Welbl et al., 2018), or by having crowdworkers do so, as in HotpotQA (Yang et al., 2018). It has been shown that many of these questions can be answered by focusing on just one of the facts used for building the questions (Min et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second q"
2020.emnlp-main.86,D18-1259,0,0.438203,"7) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datasets such as HotpotQA (Yang et al., 2018). However, they do not capture the information-seeking questions that arise from reading a single document with partial information (Min et al., 2019b; Chen and Durrett, 2019). We present a new dataset of incomplete information reading comprehension questions, IIRC, to address both of these limitations. IIRC is a crowdsourced dataset of 13441 questions over 5698 paragraphs from English Wikipedia, with most of the questions requiring information from one or more documents hyperlinked to the associated paragraphs, in addition to the original paragraphs themselves. Our crowdsourcing process (Sect"
2020.emnlp-main.86,2020.acl-main.211,0,0.0349318,"ng unanswerable, a property that our dataset shares with NewsQA (Trischler et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TyDi QA (Clark et al., 2020). Results shown in Section 4.3 indicate that these questions cannot be trivially distinguished from answerable questions. Incomplete Information QA A few prior datasets have explored question answering given incomplete information, such as science facts (Mihaylov et al., 2018; Khot et al., 2019). However, these datasets contain multiple choice questions, and the answer choices provide hints as to what information may be needed. Yuan et al. (2020) explore this as well using a POMDP in which the context in existing QA datasets is hidden from the model until it explicitly searches for it. 7 Conclusion We introduced IIRC, a new dataset of incompleteinformation reading comprehension questions. These questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways. Our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even"
2020.emnlp-main.88,S16-1165,0,0.0235264,"hat happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These relationships between an event and a time point (e.g., “won the championshi"
2020.emnlp-main.88,S17-2093,0,0.0585138,"Missing"
2020.emnlp-main.88,P14-2082,0,0.317752,"niently incorporate different modes of events. Figure 7 shows how to accurately query the relation between “having a meal” and “sleeping” in different modes (original sentences can be found in Fig. 3). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig. 8). Third, a major issue that prior works wanted to address was deciding when two events should have a relation (Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b). To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the1161 When should two events have a relation? Penalizing shortcuts by contrast questions Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. He ate his breakfast and went out. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Q: What happened"
2020.emnlp-main.88,Q14-1022,0,0.13069,"large meal, lions may sleep longer. E1 is before and overlapped with E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula doe"
2020.emnlp-main.88,P17-2001,0,0.0210933,"how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorm"
2020.emnlp-main.88,D19-1606,1,0.782633,"Missing"
2020.emnlp-main.88,N19-1423,0,0.0968295,"Missing"
2020.emnlp-main.88,E17-2118,0,0.0176129,"lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IA"
2020.emnlp-main.88,D12-1062,1,0.857082,"code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific"
2020.emnlp-main.88,N19-1246,1,0.761929,"ns are grouped. “birth” and “mourning” is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfall” or if ”disruption” st"
2020.emnlp-main.88,E17-1108,0,0.0440822,"Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018"
2020.emnlp-main.88,D18-1155,0,0.613672,"long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at covering more phenomena but s"
2020.emnlp-main.88,K17-1034,0,0.137647,"l many relations that cannot be expressed because the assumption that every event has a time interval is inaccurate: The time scope of an event may be fuzzy, an event can have a nonfactual modality, or events can be repetitive and invoke multiple intervals (see Fig. 5). To better handle these phenomena, we move away from the fixed set of relations used in prior work and instead use natural language to annotate the relationships between events, as described in the next section. 3 Natural Language Annotation of Temporal Relations Motivated by recent works (He et al., 2015; Michael et al., 2017; Levy et al., 2017; Gardner et al., 2019b), we propose using natural language question answering as an annotation for2 We could also include relationships between two fixed time points (e.g., compare 2011-03-24 with 2011-04-05), but these are mostly trivial, so we do not discuss them further. 1160 Confusing relations between the following events Questions that query events in different modes Fuzzy time scope: Heavy snow is causing disruption to transport across the UK, with heavy rainfall bringing flooding to the southwest of England. [Negated] What didn’t the lion do after a large meal? “Follow” is negated: Co"
2020.emnlp-main.88,W17-2341,0,0.0161293,"The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy"
2020.emnlp-main.88,D19-5808,1,0.793705,"is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfall” or if ”disruption” started before “flooding.” SNOW, RAINFALL"
2020.emnlp-main.88,2021.ccl-1.108,0,0.26394,"Missing"
2020.emnlp-main.88,S15-2134,0,0.0363119,"2015) and QAMR (Michael et al., 2017), where QA pairs were used as representations for predicate-argument structures. In zeroshot relation extraction (RE), they reduced relation slot filling to an MRC problem so as to build very large distant training data and improve zero-shot learning performance (Levy et al., 2017). However, our work differs from zero-shot RE since it centers around entities, while T ORQUE is about events; the way to ask and answer questions, and the way to design a corresponding crowdsourcing pipeline, are thus significantly different between us. The QA-TempEval workshop (Llorens et al., 2015), desipte its name, is actually not studying temporal relations in an RC setting. The differences between T ORQUE and QA-TempEval 1165 are as follows. First, QA TempEval is an evaluation approach for systems that generate TimeML annotations and actually is not a QA task. For instance, QA TempEval is to evaluate whether a system can answer questions like “I S <E NTITY 1> <R ELATION> <E NTITY 2>?”, where one clearly knows which event that <E NTITY> is referring to and where R ELATION is selected from a predefined label set. Second, QA-TempEval’s annotation relies on the existence of a TimeML cor"
2020.emnlp-main.88,P18-1049,0,0.0823981,"n things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at"
2020.emnlp-main.88,W15-0809,0,0.0260832,") ) E2: [?!""#$"" , ?&'( ] [Hypothetical] If the lion has a large meal, it will sleep for 24 hours. [Repetitive] The lion used to sleep for 24 hours after having large meals. E1 starts with E2 E1 is equal to E2 E1 starts E2 E1 includes E2 E1 ends with E2 [Generic] After having a large meal, lions may sleep longer. E1 is before and overlapped with E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb a"
2020.emnlp-main.88,W16-1007,0,0.0771783,"fferent modes of events. Figure 7 shows how to accurately query the relation between “having a meal” and “sleeping” in different modes (original sentences can be found in Fig. 3). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig. 8). Third, a major issue that prior works wanted to address was deciding when two events should have a relation (Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b). To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the1161 When should two events have a relation? Penalizing shortcuts by contrast questions Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. He ate his breakfast and went out. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Q: What happened after he ate his breakfast"
2020.emnlp-main.88,D17-1108,1,0.902955,"?”), or modify it to ask about the start/end time (e.g., “what happened after he started eating his breakfast?” or “what would finish after he ate his breakfast?”). We also instructed workers to make sure that the answers to the new question are different from the original one to avoid trivial modifications (e.g., changing “what happened” to “what occurred”). 4 Data Collection We used Amazon Mechanical Turk to build T ORQUE. Following prior work, we focus on passages that consist of two contiguous sentences, as this is sufficient to capture the vast majority of non-trivial temporal relations (Ning et al., 2017). We took all the articles used in the TempEval3 (TE3) workshop (2.8k articles) (UzZaman et al., 2013) and created a pool of 26k two-sentence passages. Given a random passage from this pool, the annotation process for crowd workers was: 1. Label all the events 2. Repeatedly do the following3 (a) Ask a temporal relation question and point out all the answers from the list of events (b) Modify the temporal relation to create one or more new questions and answer them The annotation guidelines4 and interface5 are public. In the following sections, we further discuss issues of quality control and c"
2020.emnlp-main.88,P18-1212,1,0.933706,"ith E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing"
2020.emnlp-main.88,D19-1642,1,0.780018,"equire temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at covering more phenomena but suffered from low IAA"
2020.emnlp-main.88,P17-2035,0,0.0725231,"is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler I"
2020.emnlp-main.88,S13-2001,0,0.478562,"ile a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These relationships between an event a"
2020.emnlp-main.88,N18-2026,0,0.0208006,"wer than but already comparable to that of using the entire training set. This means that the learning curve on T ORQUE is already flat and the current size of T ORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior work"
2020.emnlp-main.88,P18-1122,1,0.934335,"ith E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing"
2020.emnlp-main.88,S07-1014,0,0.484675,"andslide Q6: What happened while a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These rela"
2020.emnlp-main.88,W16-5706,0,0.191361,"Missing"
2020.emnlp-main.88,P18-2124,0,0.0526628,"Missing"
2020.emnlp-main.88,D19-1332,1,0.800439,"omparable to that of using the entire training set. This means that the learning curve on T ORQUE is already flat and the current size of T ORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle th"
2020.emnlp-main.88,2020.acl-main.678,1,0.892419,"Missing"
2020.emnlp-main.88,D16-1264,0,0.0598335,"in color and contrast questions are grouped. “birth” and “mourning” is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfal"
2020.emnlp-main.88,D15-1076,0,\N,Missing
2020.emnlp-main.88,Q14-1012,0,\N,Missing
2020.emnlp-main.88,S18-1011,0,\N,Missing
2020.emnlp-tutorials.3,N18-2017,0,0.0213835,"w.ericswallace.com/interpretability. 1 Sameer Singh UC Irvine sameer@uci.edu Tutorial Description Neural models have become the de-facto standard tool for NLP tasks. These models are becoming increasingly powerful—recent work shows that large neural models substantially improve accuracy on a wide range of downstream tasks (Devlin et al., 2019; Brown et al., 2020). However, today’s models still make egregious errors: they reinforce racial biases (Sap et al., 2019), fail in counterintuitive ways (Jia and Liang, 2017; Feng et al., 2018), and often solve tasks using simple surface-level patterns (Gururangan et al., 2018; Min et al., 2019). These model insufficiencies are exacerbated by the inability to understand why models made the predictions they do. Interpretation methods seek to fill this void. In particular, example-specific interpretations provide post-hoc explanations for indi2 Details and Prerequisites The tutorial will be of the cutting-edge type. The tutorial slides and the accompanying code is available online at https://www.ericswallace. com/interpretability. Prerequisites Attendees should have a basic understanding of different tasks in NLP such as text classification, sequence tagging, and rea"
2020.emnlp-tutorials.3,N19-1357,0,0.0573089,"Missing"
2020.emnlp-tutorials.3,D17-1215,0,0.460906,"ovide an introduction to the various types of example-specific interpretations. We will present the technical details of existing methods, including saliency maps, adversarial attacks, input perturbations, influence functions, and other methods. We will cover how these interpretations are applied to various tasks and input-output formats, e.g., text classification using LSTMs, masked language modeling using BERT (Devlin et al., 2019), and text generation using GPT-2 (Radford et al., 2019). For each task, we will walk through example use cases of interpretations: highlighting model weaknesses (Jia and Liang, 2017), increasing/decreasing user trust (Feng et al., 2018), and understanding hard-to-formalize criteria such as bias, safety, and fairness (Doshi-Velez and Kim, 2017). Alongside the tutorial, we will present source code implementations of various interpretation methods using AllenNLP Interpret (Wallace et al., 2019b). Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the"
2020.emnlp-tutorials.3,N19-1112,1,0.806739,"l discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardner et al., 2020) • “Probing”, i.e., inspecting a model’s embeddings for certain properties (Liu et al., 2019; Tenney et al., 2019). • Rationale-based explanations, i.e., a model generates text for why it made its prediction. • Example-specific interpretations (our tutorial’s focus), e.g., saliency maps (Simonyan et al., 2014), LIME (Ribeiro et al., 2016), adversarUnderstanding Which Training Examples Caused a Prediction This section will discuss how to trace model predictions back to the training data, i.e., identifying “influential” training points. We will cover influence functions (Koh and Liang, 2017) and representor points (Yeh et al., 2018). Coding Interpretations This section will walk throug"
2020.emnlp-tutorials.3,P19-1334,0,0.0220508,"re now standard interpretations. Wallace et al. (2019b) provides example NLP interpretations (interested readers can inspect their code). 3 Tutorial Outline The tutorial will present three hours of content with a thirty-minute break. Break Understanding How Global Decision Rules Led to a Prediction This section will discuss how certain global “decision rules” can explain model predictions. We will cover Anchors (Ribeiro et al., 2018a) and Universal Adversarial Triggers (Wallace et al., 2019a). We will also discuss how spurious patterns in datasets, e.g., lexical overlap in textual entailment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardne"
2020.emnlp-tutorials.3,D19-1221,1,0.922771,"s and input-output formats, e.g., text classification using LSTMs, masked language modeling using BERT (Devlin et al., 2019), and text generation using GPT-2 (Radford et al., 2019). For each task, we will walk through example use cases of interpretations: highlighting model weaknesses (Jia and Liang, 2017), increasing/decreasing user trust (Feng et al., 2018), and understanding hard-to-formalize criteria such as bias, safety, and fairness (Doshi-Velez and Kim, 2017). Alongside the tutorial, we will present source code implementations of various interpretation methods using AllenNLP Interpret (Wallace et al., 2019b). Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will first situate example-specific interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-specific interpretations, including saliency maps, input perturbations (e.g., LIME, input reduc"
2020.emnlp-tutorials.3,P19-1416,1,0.816158,"retability. 1 Sameer Singh UC Irvine sameer@uci.edu Tutorial Description Neural models have become the de-facto standard tool for NLP tasks. These models are becoming increasingly powerful—recent work shows that large neural models substantially improve accuracy on a wide range of downstream tasks (Devlin et al., 2019; Brown et al., 2020). However, today’s models still make egregious errors: they reinforce racial biases (Sap et al., 2019), fail in counterintuitive ways (Jia and Liang, 2017; Feng et al., 2018), and often solve tasks using simple surface-level patterns (Gururangan et al., 2018; Min et al., 2019). These model insufficiencies are exacerbated by the inability to understand why models made the predictions they do. Interpretation methods seek to fill this void. In particular, example-specific interpretations provide post-hoc explanations for indi2 Details and Prerequisites The tutorial will be of the cutting-edge type. The tutorial slides and the accompanying code is available online at https://www.ericswallace. com/interpretability. Prerequisites Attendees should have a basic understanding of different tasks in NLP such as text classification, sequence tagging, and reading comprehension"
2020.emnlp-tutorials.3,D19-3002,1,0.839545,"s and input-output formats, e.g., text classification using LSTMs, masked language modeling using BERT (Devlin et al., 2019), and text generation using GPT-2 (Radford et al., 2019). For each task, we will walk through example use cases of interpretations: highlighting model weaknesses (Jia and Liang, 2017), increasing/decreasing user trust (Feng et al., 2018), and understanding hard-to-formalize criteria such as bias, safety, and fairness (Doshi-Velez and Kim, 2017). Alongside the tutorial, we will present source code implementations of various interpretation methods using AllenNLP Interpret (Wallace et al., 2019b). Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will first situate example-specific interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-specific interpretations, including saliency maps, input perturbations (e.g., LIME, input reduc"
2020.emnlp-tutorials.3,C18-1198,0,0.016511,"ment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardner et al., 2020) • “Probing”, i.e., inspecting a model’s embeddings for certain properties (Liu et al., 2019; Tenney et al., 2019). • Rationale-based explanations, i.e., a model generates text for why it made its prediction. • Example-specific interpretations (our tutorial’s focus), e.g., saliency maps (Simonyan et al., 2014), LIME (Ribeiro et al., 2016), adversarUnderstanding Which Training Examples Caused a Prediction This section will discuss how to trace model predictions back to the training data, i.e., identifying “influential” training points. We will cover influence functions (K"
2020.emnlp-tutorials.3,N19-5001,1,0.830443,"ions focus on classification models; how are interpretations best applied to the complex input-output formats seen in NLP tasks (e.g., machine translation)? • Closing the loop with Humans: Humans are the end-users of interpretations; how can we make interpretations interactive, collaborative, customizable, and ultimately more effective? • Pretrained Transformer Models: How do our methods, and the field of interpretability, change with the rise of massively-pretrained models? his PhD from the University of Massachusetts, Amherst in 2014. Sameer presented the Deep Adversarial Learning Tutorial (Wang et al., 2019) at NAACL 2019 and the Mining Knowledge Graphs from Text Tutorial at WSDM 2018 and AAAI 2017. Sameer has also received teaching awards at UCI. Website: http: //sameersingh.org/ 4 Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On adversarial examples for character-level neural machine translation. In COLING. References Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Jacob Devlin, Ming-Wei Chang, Kenton"
2020.emnlp-tutorials.3,N18-1202,1,0.509455,"understanding of neural network methods for NLP, including: 20 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 20–23 c Online, November 19 - 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 • How backpropagation can compute gradients with respect to the parameters. • How tokens/words are represented (i.e., word and sub-word embeddings). • High-level ideas behind different model architectures (e.g., RNNs, Transformers). • Optional knowledge of contextualized embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Finally, a portion of the tutorial will walk through Python code samples in PyTorch and AllenNLP (Gardner et al., 2018b). Participants do not need to understand this code to follow the main tutorial material. ial attacks (Szegedy et al., 2014), and input perturbations (Feng et al., 2018). Example-specific Interpretations This section will introduce example-specific interpretations in more detail. We will discuss the challenges and approaches to evaluating such interpretations. We will also cover the critiques and shortcomings of using attention as explanations"
2020.emnlp-tutorials.3,P19-1073,0,0.016237,"in datasets, e.g., lexical overlap in textual entailment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardner et al., 2020) • “Probing”, i.e., inspecting a model’s embeddings for certain properties (Liu et al., 2019; Tenney et al., 2019). • Rationale-based explanations, i.e., a model generates text for why it made its prediction. • Example-specific interpretations (our tutorial’s focus), e.g., saliency maps (Simonyan et al., 2014), LIME (Ribeiro et al., 2016), adversarUnderstanding Which Training Examples Caused a Prediction This section will discuss how to trace model predictions back to the training data, i.e., identifying “influential”"
2020.emnlp-tutorials.3,N16-3020,1,0.544481,"critiques and shortcomings of using attention as explanations (Jain and Wallace, 2019; Serrano and Smith, 2019). We will then explain why we focus on gradient-based methods: they are model-agnostic, easy to compute, and (largely) faithful to a model’s behavior. Understanding What Parts of An Input Led to a Prediction This section will discuss: • Saliency maps, i.e., generating visualizations of “salient” input tokens. We will discuss how to generate saliency maps using gradient-based techniques (Simonyan et al., 2014; Sundararajan et al., 2017; Smilkov et al., 2017)) and black-box techniques (Ribeiro et al., 2016). • Input Perturbations, i.e., showing how changes to the input do (or do not) change the prediction. For example, leave-one-out (Li et al., 2016) and input reduction (Feng et al., 2018). We will also cover adversarial perturbations such as token flipping (Ebrahimi et al., 2018) and adding distractor sentences (Jia and Liang, 2017). Reading List Doshi-Velez and Kim (2017) provide a great overview and motivation for interpretability research. Lipton (2018) and Jain and Wallace (2019) discuss some of the challenges of defining and evaluating interpretability. Jia and Liang (2017) help demonstrat"
2020.emnlp-tutorials.3,P18-1079,1,0.827998,"of defining and evaluating interpretability. Jia and Liang (2017) help demonstrate the fragility of NLP models. LIME (Ribeiro et al., 2016) and saliency maps (Simonyan et al., 2014) are now standard interpretations. Wallace et al. (2019b) provides example NLP interpretations (interested readers can inspect their code). 3 Tutorial Outline The tutorial will present three hours of content with a thirty-minute break. Break Understanding How Global Decision Rules Led to a Prediction This section will discuss how certain global “decision rules” can explain model predictions. We will cover Anchors (Ribeiro et al., 2018a) and Universal Adversarial Triggers (Wallace et al., 2019a). We will also discuss how spurious patterns in datasets, e.g., lexical overlap in textual entailment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretati"
2020.emnlp-tutorials.3,P19-1163,0,0.0165641,"d, e.g., evaluating, extending, and improving interpretation methods. The tutorial slides and the accompanying code is available online at https: //www.ericswallace.com/interpretability. 1 Sameer Singh UC Irvine sameer@uci.edu Tutorial Description Neural models have become the de-facto standard tool for NLP tasks. These models are becoming increasingly powerful—recent work shows that large neural models substantially improve accuracy on a wide range of downstream tasks (Devlin et al., 2019; Brown et al., 2020). However, today’s models still make egregious errors: they reinforce racial biases (Sap et al., 2019), fail in counterintuitive ways (Jia and Liang, 2017; Feng et al., 2018), and often solve tasks using simple surface-level patterns (Gururangan et al., 2018; Min et al., 2019). These model insufficiencies are exacerbated by the inability to understand why models made the predictions they do. Interpretation methods seek to fill this void. In particular, example-specific interpretations provide post-hoc explanations for indi2 Details and Prerequisites The tutorial will be of the cutting-edge type. The tutorial slides and the accompanying code is available online at https://www.ericswallace. com/"
2020.emnlp-tutorials.3,P19-1282,0,0.0191242,"2019). Finally, a portion of the tutorial will walk through Python code samples in PyTorch and AllenNLP (Gardner et al., 2018b). Participants do not need to understand this code to follow the main tutorial material. ial attacks (Szegedy et al., 2014), and input perturbations (Feng et al., 2018). Example-specific Interpretations This section will introduce example-specific interpretations in more detail. We will discuss the challenges and approaches to evaluating such interpretations. We will also cover the critiques and shortcomings of using attention as explanations (Jain and Wallace, 2019; Serrano and Smith, 2019). We will then explain why we focus on gradient-based methods: they are model-agnostic, easy to compute, and (largely) faithful to a model’s behavior. Understanding What Parts of An Input Led to a Prediction This section will discuss: • Saliency maps, i.e., generating visualizations of “salient” input tokens. We will discuss how to generate saliency maps using gradient-based techniques (Simonyan et al., 2014; Sundararajan et al., 2017; Smilkov et al., 2017)) and black-box techniques (Ribeiro et al., 2016). • Input Perturbations, i.e., showing how changes to the input do (or do not) change the"
2020.findings-emnlp.117,W07-2441,0,0.0137691,"1. UD Parsing Finally, we discuss dependency parsing in the universal dependencies (UD) formalism (Nivre et al., 2016). We look at dependency parsing to show that contrast sets apply not only to modern “high-level” NLP tasks but also to longstanding linguistic analysis tasks. We first chose a specific type of attachment ambiguity to target: the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), e.g. We ate spaghetti with forks versus We ate spaghetti with meatballs. We use a subset of the English UD treebanks: GUM (Zeldes, 2017), the English portion of LinES (Ahrenberg, 2007), the English portion of ParTUT (Sanguinetti and Bosco, 2015), and the dependency-annotated English Web Treebank (Silveira et al., 2014). We searched these treebanks for sentences that include a potentially structurally ambiguous attachment from the head of a PP to either a noun or a verb. We then perturbed these sentences by altering one of their noun phrases such that the semantics of the perturbed sentence required a different attachment for the PP. We then re-annotated these perturbed sentences to indicate the new attachment(s). Summary While the overall process we recommend for constructi"
2020.findings-emnlp.117,D15-1075,0,0.0445289,"ow frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distribut"
2020.findings-emnlp.117,W18-6433,0,0.0341655,"Missing"
2020.findings-emnlp.117,D19-1606,1,0.924571,"round a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates"
2020.findings-emnlp.117,N19-1423,0,0.0110664,"st sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set de"
2020.findings-emnlp.117,N19-1246,1,0.944307,"arts of the gaps around a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while pe"
2020.findings-emnlp.117,D18-1407,1,0.898157,"uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011)"
2020.findings-emnlp.117,D19-1107,1,0.925439,"l language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps"
2020.findings-emnlp.117,P18-2103,0,0.0419316,"ded minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed inst"
2020.findings-emnlp.117,N18-2017,1,0.922108,"uction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in th"
2020.findings-emnlp.117,D19-1170,0,0.0120239,"work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set design, etc. On IMDb and PERSPECTRUM, the model achieves a reasonably high consistency, suggesting that, while there is definitely still room fo"
2020.findings-emnlp.117,D17-1263,0,0.0339764,"h to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring lo"
2020.findings-emnlp.117,D17-1215,0,0.560991,"rks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Lev"
2020.findings-emnlp.117,D19-1423,0,0.0613359,"Missing"
2020.findings-emnlp.117,2020.emnlp-main.12,1,0.714022,"ple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits simil"
2020.findings-emnlp.117,W17-5401,0,0.118381,"Missing"
2020.findings-emnlp.117,P19-1554,1,0.81477,"ctive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot confirm that a model does align with it. This is because annotators cannot exhaustively label all inputs near a pivot and thus a contrast set will necessarily be incomplete. However, note that this problem is not unique to contrast sets—similar issues hold for the original test set as well as adversarial test sets (Jia and Liang, 2017), challenge sets (Naik et al., 2018), and input perturbations (Ribeiro et al., 2018a; Feng et al., 2018). See Feng et al. (2019) for a detailed discussion of how dataset analysis methods only have negative predictive power. Dataset-Specific Instantiations The process for creating contrast sets is dataset-specific: although we present general guidelines that hold across many tasks, experts must still characterize the type of phenomena each individual dataset is intended to capture. Fortunately, the original dataset authors should already have thought deeply about such phenomena. Hence, creating contrast sets should be well-defined and relatively straightforward. 3 How to Create Contrast Sets Here, we walk through our pr"
2020.findings-emnlp.117,D19-5808,1,0.8991,"n the contrast sets (not including the original example). We report percentage accuracy for NLVR2, IMDb, PERSPECTRUM, MATRES, and BoolQ; F1 scores for DROP and Q UOREF; Exact Match (EM) scores for ROPES and MC-TACO; and unlabeled attachment score on modified attachments for the UD English dataset. We also report contrast consistency: the percentage of the “# Sets” contrast sets for which a model’s predictions are correct for all examples in the set (including the original example). More details on datasets, models, and metrics can be found in §A and §B. • Quoref (Dasigi et al., 2019) • ROPES (Lin et al., 2019) • BoolQ (Clark et al., 2019) • MC-TACO (Zhou et al., 2019) We choose these datasets because they span a variety of tasks (e.g., reading comprehension, sentiment analysis, visual reasoning) and input-output formats (e.g., classification, span extraction, structured prediction). We include high-level tasks for which dataset artifacts are known to be prevalent, as well as longstanding formalism-based tasks, where data artifacts have been less of an issue (or at least have been less well-studied). 4.2 Contrast Set Construction The contrast sets were constructed by NLP researchers who were deeply"
2020.findings-emnlp.117,2021.ccl-1.108,0,0.144976,"Missing"
2020.findings-emnlp.117,P11-1015,0,0.07085,"ena they are most interested in studying and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ"
2020.findings-emnlp.117,J93-2004,0,0.068933,"ation: Two similarly-colored and similarly-posed chow dogs are face to face in one image. Figure 1: An example contrast set for NLVR2 (Suhr and Artzi, 2019). The label for the original example is T RUE and the label for all of the perturbed examples is FALSE. The contrast set allows probing of a model’s decision boundary local to examples in the test set, which better evaluates whether the model has captured the relevant phenomena than standard metrics on i.i.d. test data. Introduction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input"
2020.findings-emnlp.117,D18-1151,0,0.0148944,"1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in systematic gaps. Thus contrast sets often require less effort to create, and they remain grounded i"
2020.findings-emnlp.117,N19-1314,0,0.0179195,"the task definition than a random selection of input / output pairs. 2.3 Contrast sets in practice Given these definitions, we now turn to the actual construction of contrast sets in practical NLP settings. There were two things left unspecified in the definitions above: the distance function d to use in discrete input spaces, and the method for sampling from a local decision boundary. While there has been some work trying to formally characterize dis2 In this discussion we are talking about the true decision boundary, not a model’s decision boundary. tances for adversarial robustness in NLP (Michel et al., 2019; Jia et al., 2019), we find it more useful in our setting to simply rely on expert judgments to generate a similar but meaningfully different x0 given x, addressing both the distance function and the sampling method. Future work could try to give formal treatments of these issues, but we believe expert judgments are sufficient to make initial progress in improving our evaluation methodologies. And while expertcrafted contrast sets can only give us an upper bound on a model’s local alignment with the true decision boundary, an upper bound on local alignment is often more informative than a pot"
2020.findings-emnlp.117,P19-1416,1,0.832263,"dissolute alcoholic. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap"
2020.findings-emnlp.117,C18-1198,0,0.264897,"nt work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that dataset authors manually perturb instances fro"
2020.findings-emnlp.117,D19-1642,1,0.874527,"Missing"
2020.findings-emnlp.117,P18-1122,1,0.846317,"and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ 339 70 RoBERTa 86.1 71.1 (–15.0) 59.0 MC-"
2020.findings-emnlp.117,P19-1459,0,0.0145999,"uestion: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be e"
2020.findings-emnlp.117,N18-1202,1,0.523856,"es from the different contrast sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model a"
2020.findings-emnlp.117,S18-2023,0,0.0681783,"Missing"
2020.findings-emnlp.117,P19-1621,1,0.860914,"Missing"
2020.findings-emnlp.117,P18-1079,1,0.679043,"of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that data"
2020.findings-emnlp.117,N18-2002,0,0.0515743,"Missing"
2020.findings-emnlp.117,E17-2060,0,0.0150328,"ds have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in system"
2020.findings-emnlp.117,2020.acl-main.468,0,0.0236124,"ing set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distributional biases in the data that was chosen for annotation, as well as numerous other biases that are more subtle and harder to discern (Shah et al., 2020). Completely removing these gaps in the initial data collection process would be ideal, but is likely impossible—language has too much inherent variability in a very high-dimensional space. Instead, we use contrast sets to fill in gaps in the test data to give more thorough evaluations than what the original data provides. 1309 2.2 Definitions We begin by defining a decision boundary as a partition of some space into labels.2 This partition can be represented by the set of all points in the space with their associated labels: {(x, y)}. This definition differs somewhat from the canonical defini"
2020.findings-emnlp.117,silveira-etal-2014-gold,0,0.0413118,"Missing"
2020.findings-emnlp.117,P19-1644,1,0.918073,"hanging questions asking for counts to questions asking for sets (How many countries. . . to Which countries. . . ). Finally, we changed the ordering of events. A large number of questions about war paragraphs ask which of two events happened first. We changed (1) the order the events were asked about in the question, (2) the order that the events showed up in the passage, and (3) the dates associated with each event to swap their temporal order. NLVR2 We next consider NLVR2, a dataset where a model is given a sentence about two provided images and must determine whether the sentence is true (Suhr et al., 2019). The data collection process encouraged highly compositional language, which was intended to require understanding the relationships between objects, properties of objects, and counting. We constructed NLVR2 contrast sets by modifying the sentence or replacing one of the images with freely-licensed images from web searches. For example, we might change The left image contains twice the number of dogs as the right image to The left image contains three times the number of dogs as the right image. Similarly, given an image pair with four dogs in the left and two dogs in the right, we can replac"
2020.findings-emnlp.117,D19-1608,1,0.822267,"esources that we have created, is giving a simple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various pheno"
2020.findings-emnlp.117,D18-1009,0,0.0260643,"is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that"
2020.findings-emnlp.117,P19-1472,0,0.0134417,"annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that model. 2.5 Limitations of Contrast Sets Solely Negative Predictive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot"
2020.findings-emnlp.225,P18-1033,0,0.073795,"output program. Nevertheless, prior work (FineganDollak et al., 2018; Herzig and Berant, 2019; Keysers et al., 2020) has shown that data splits that require generalizing to new program templates result in drastic loss of performance. However, past work did not investigate how different modeling choices interact with compositional generalization. In this paper, we thoroughly analyze the impact of different modeling choices on compositional generalization in 5 semantic parsing datasets—four that are text-to-SQL datasets, and DROP, a dataset for executing programs over text paragraphs. Following Finegan-Dollak et al. (2018), we examine performance on a compositional split, where target programs are partitioned into “program templates”, and templates appearing at test time are unobserved at training time. We examine the effect of standard practices, such as contextualized representations (§3.1) and grammar-based decoding (§3.2). Moreover, we propose novel extensions to decoder attention (§3.3), the component responsible for aligning sub-structures in the question and program: (a) supervising attention based on precomputed token alignments, (b) attending over constituency spans, and (c) encouraging the decoder 248"
2020.findings-emnlp.225,P19-1444,0,0.0236547,"trained from (x, z) pairs. Similar to Finegan-Dollak et al. (2018), our baseline semantic parser is a standard sequence-tosequence model (Dong and Lapata, 2016) that encodes the question x with a BiLSTM encoder (Hochreiter and Schmidhuber, 1997) over GloVe embeddings (Pennington et al., 2014), and decodes the program z token-by-token from left to right with an attention-based LSTM decoder (Bahdanau et al., 2015). 3.1 Contextualized Representations Pre-trained contextualized representations revolutionized natural language processing in recent years, and semantic parsing has been no exception (Guo et al., 2019; Wang et al., 2019). We hypothesize that better representations for question tokens should improve compositional generalization, because they reduce language variability and thus may help improve the mapping from input to output tokens. We evaluate the effect of using ELM O (Peters et al., 2018) and BERT (Devlin et al., 2019) to represent question tokens.1 3.2 Grammar-Based Decoding A unique property of semantic parsing, compared to other generation tasks, is that programs have a clear hierarchical structure that is based on the target formal language. Decoding the output program token-by-tok"
2020.findings-emnlp.225,D17-1160,1,0.829696,".1 3.2 Grammar-Based Decoding A unique property of semantic parsing, compared to other generation tasks, is that programs have a clear hierarchical structure that is based on the target formal language. Decoding the output program token-by-token from left to right (Dong and Lapata, 2016; Jia and Liang, 2016) can thus generate programs that are not syntactically valid, and the model must effectively learn the syntax of the target language at training time. Grammar-based decoding resolves this issue and has been shown to consistently improve in-distribution performance (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). In grammar-based decoding, the decoder outputs the abstract syntax tree of the program based on a formal grammar of the target language. At each step, a production rule from the grammar is chosen, eventually outputting a topdown left-to-right linearization of the program tree. Because decoding is constrained by the grammar, the model outputs only valid programs. We refer the reader to the aforementioned papers for details on grammar-based decoding. Compositional generalization involves combining known sub-structures in novel ways. In grammar-based decoding, the structu"
2020.findings-emnlp.225,D17-1018,0,0.0244953,"rd phrase “do for a living” aligns to the KB relation Profession. Allowing the model to directly attend to multi-token phrases could induce more meaningful alignments that improve compositional generalization. Here, rather than computing an attention distribution over input tokens (x1 , . . . xn ), we compute a distribution over the set of spans corresponding to all constituents (including all tokens) as predicted by an off-the-shelf constituency parser (Joshi et al., 2018). Spans are represented using a self-attention mechanism over the hidden representations of the tokens in the span, as in Lee et al. (2017). (c) Coverage Questions at test time are sometimes similar to training questions, but include new information expressed by a few tokens. A model that memorizes a mapping from question templates to programs can ignore this new information, hampering compositional generalization. To encourage models to attend to the entire question, we add the attention-coverage mechanism from See et al. (2017) to our model. Specifically, at each decoding step the decoder holds a coverage vector c = (c1 , . . . , cn ), where ci corresponds to the sum of attention probabilities over xi in all previous time steps"
2020.tacl-1.13,P13-1023,0,0.0507346,"Missing"
2020.tacl-1.13,N19-1027,0,0.0194134,"the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed to use words from a lexicon Lx , which contains: (a) words appearing in the question (or their automatically computed inflections), (b) words from a small pre-defined list of 66 function word such as, ‘if’, ‘on’, ‘for each’, or (c) reference tokens that refer to the results"
2020.tacl-1.13,N16-1181,0,0.249915,"Missing"
2020.tacl-1.13,P15-1127,0,0.0288983,"l details in §7.2). COMBINED and BREAKRC were compared to COMBINEDR and BREAKRCR , which use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, bridging between natural la"
2020.tacl-1.13,W10-2903,0,0.0397036,"models should improve performance and generalization in tasks that require multi-step reasoning or that do not have access to substantial amounts of data. In this work we propose question understanding as a standalone language understanding task. We introduce a formalism for representing the meaning of questions that relies on question decomposition, and is agnostic to the information source. Our formalism, Question Decomposition Meaning Representation (QDMR), is inspired by database query languages (SQL; SPARQL), and by semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al., 2010), in which questions are given full meaning representations. We express complex questions via simple (‘‘atomic’’) questions that can be executed in sequence to answer the original question. Each atomic question can be mapped into a small set of formal operations, where each operation either selects a set of entities, retrieves information about Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes th"
2020.tacl-1.13,H94-1010,0,0.464354,"Missing"
2020.tacl-1.13,N19-1423,0,0.0339406,"o-logical forms, which can be used as 190 5 QDMR for Open-domain QA A natural setup for QDMR is in answering complex questions that require multiple reasoning steps. We compare models that exploit question decompositions to baselines that do not. We use the open-domain QA (‘‘full-wiki"") setting of the HOTPOTQA dataset (Yang et al., 2018): Given a question, the QA model retrieves the relevant Wikipedia paragraphs and answers the question using these paragraphs. 5.1 Experimental Setup We compare BREAKRC, a model that utilizes question decomposition to BERTQA, a standard QA model, based on BERT (Devlin et al., 2019), and present COMBINED, an approach that enjoys the benefits of both models. BREAKRC Algorithm 1 describes the BREAKRC model, which uses high-level QDMR structures for answering open-domain multi-hop questions. We assume access to an Information Retrieval (IR) model and an RC model, and denote by ANSWER(·) a function that takes a question as input, runs the IR model to obtain paragraphs, and then feeds those paragraphs as context for an RC model that returns a distribution over answers. Given an input QDMR, s = hs1 , ..., sn i, iterate over s step-by-step and perform the following. First, we e"
2020.tacl-1.13,W13-2322,0,0.0713232,"x. Domain-independent intermediate representations for semantic parsers were proposed by Kwiatkowski et al. (2013) and Reddy et al. (2016). As there is no consensus on the ideal meaning representation for semantic parsing, representations are often chosen based on the particular execution setup: SQL is used for relational databases (Yu et al., 2018), SPARQL for graph KBs (Yih et al., 2016), while other ad-hoc languages are used based on the task at hand. We frame QDMR as an easy-to-annotate formalism that can be potentially converted to other representations, depending on the task. Last, AMR (Banarescu et al., 2013) is a meaning representation for sentences. Instead of representing general language, QDMR represents questions, which are important for QA systems, and for probing models for reasoning. questions from 10 datasets and 3 modalities (DB, images, text). We presented the utility of QDMR for both open-domain question answering and semantic parsing, and constructed a QDMR parser with reasonable performance. QDMR proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions. Acknowledgments This wo"
2020.tacl-1.13,N19-1246,1,0.922364,"it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query lan"
2020.tacl-1.13,P17-1171,0,0.0297332,"the reference to the previous step in si with its already computed answer, and then run ANSWER(·). For FILTER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraph"
2020.tacl-1.13,N19-1405,0,0.0304319,"ems from the design of QDMR as a domain-agnostic meaning representation (§2). QDMR abstracts away from a concrete KB schema by assuming an underlying ‘‘idealized’’ KB, which contains all of its arguments. Figure 6: Examples of PROJECT and COMPARISON questions in HOTPOTQA (high-level QDMR). Model BERTQA BREAKRCP BREAKRCG PROJECT COMPARISON EM F1 IR EM F1 IR 22.8 31.0 31.6 42.9 51.7 75.8 25.4 33.7 52.9 34.7 50.4 68.9 32.2 41.9 59.8 44.5 57.6 78.0 Table 7: Results on PROJECT and COMPARISON questions from HOTPOTQA development set. to reasoning shortcuts, i.e. they necessitate multistep reasoning (Chen and Durrett, 2019; Jiang and Bansal, 2019; Min et al., 2019a). In Table 7 we report BREAKRC results on these question types, where it notably outperforms BERTQA. Ablations In BREAKRC, multiple IR queries are issued, one at each step. To examine whether these multiple queries were the cause for performance gains, we built IR-NP, a model that issues multiple IR queries, one for each noun phrase in the question. Similar to COMBINED, the question and union of retrieved paragraphs are given as input to BERTQA. We observe that COMBINED substantially outperforms IR-NP, indicating that the structure of QDMR, rather th"
2020.tacl-1.13,P16-1154,0,0.0275769,"authors generate sequences of simple questions which crowdworkers paraphrase into a compositional question. Questions in BREAK are composed by humans, and are then decomposed to QDMR. Table 8: The decomposition rules of RULEBASED. Rules are based on dependency labels, part-ofspeech tags and coreference edges. Text fragments used for decomposition are in boldface. • S2SDYNAMIC: SEQ2SEQ with a dynamic output vocabulary restricted to the closed set of tokens Lx available to crowd-workers (see §3). • COPYNET: SEQ2SEQ with an added copy mechanism that allows copying tokens from the input sequence (Gu et al., 2016). 7.3 Results Table 9 presents model performance on BREAK. Neural models outperform the RULEBASED baseline and perform reasonably well, with COPYNET obtaining the best scores across all metrics. This can be attributed to most of the tokens in a QDMR parse being copied from the original question. Semantic Formalism Annotation Labeling corpora with a semantic formalism has often been reserved for expert annotators (Dahl et al., 1994; Zelle and Mooney, 1996; Abend and Rappoport, 2013; Yu et al., 2018). Recent work has focused on cheaply eliciting quality annotations from nonexperts through crowds"
2020.tacl-1.13,P19-1262,0,0.0376279,"DMR as a domain-agnostic meaning representation (§2). QDMR abstracts away from a concrete KB schema by assuming an underlying ‘‘idealized’’ KB, which contains all of its arguments. Figure 6: Examples of PROJECT and COMPARISON questions in HOTPOTQA (high-level QDMR). Model BERTQA BREAKRCP BREAKRCG PROJECT COMPARISON EM F1 IR EM F1 IR 22.8 31.0 31.6 42.9 51.7 75.8 25.4 33.7 52.9 34.7 50.4 68.9 32.2 41.9 59.8 44.5 57.6 78.0 Table 7: Results on PROJECT and COMPARISON questions from HOTPOTQA development set. to reasoning shortcuts, i.e. they necessitate multistep reasoning (Chen and Durrett, 2019; Jiang and Bansal, 2019; Min et al., 2019a). In Table 7 we report BREAKRC results on these question types, where it notably outperforms BERTQA. Ablations In BREAKRC, multiple IR queries are issued, one at each step. To examine whether these multiple queries were the cause for performance gains, we built IR-NP, a model that issues multiple IR queries, one for each noun phrase in the question. Similar to COMBINED, the question and union of retrieved paragraphs are given as input to BERTQA. We observe that COMBINED substantially outperforms IR-NP, indicating that the structure of QDMR, rather than multiple IR queries,"
2020.tacl-1.13,P19-1444,0,0.0312823,"use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, bridging between natural language and full logical forms. 7 QDMR Parsing We now present evaluation metrics and mod"
2020.tacl-1.13,D18-1239,0,0.0422473,"Missing"
2020.tacl-1.13,D13-1161,0,0.0328552,"ency tree of the question (full details in §7.2). COMBINED and BREAKRC were compared to COMBINEDR and BREAKRCR , which use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, brid"
2020.tacl-1.13,Q19-1026,0,0.0790769,"Missing"
2020.tacl-1.13,D16-1258,0,0.062318,"Missing"
2020.tacl-1.13,P17-1089,0,0.0546298,"Missing"
2020.tacl-1.13,J13-2005,0,0.0690178,"Missing"
2020.tacl-1.13,N18-2089,0,0.0398159,"Missing"
2020.tacl-1.13,P17-1167,0,0.0532124,"Missing"
2020.tacl-1.13,P19-1416,1,0.939891,"ER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, tha"
2020.tacl-1.13,P19-1613,0,0.228693,"ER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, tha"
2020.tacl-1.13,Q18-1021,0,0.0375168,"nverted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1"
2020.tacl-1.13,P15-1142,0,0.0707424,"n semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1974), the same intuition can be applied to other modalities, such as images and"
2020.tacl-1.13,Q16-1029,0,0.0219436,"sition can be further decomposed. To measure such variations, we introduce two types of evaluation metrics. Sequence-based metrics treat the decomposition as a sequence of tokens, applying standard text generation metrics. As such metrics ignore the QDMR graph structure, we also use graph-based metrics that compare the predicted graph Gˆs to the gold QDMR graph Gs (see §2). Sequence-based scores, where higher values are better, are denoted by ⇑. Graph-based scores, where lower values are better, are denoted by ⇓. • Exact Match ⇑: Measures exact match between s and ˆs, either 0 or 1. • SARI ⇑ (Xu et al., 2016): SARI is commonly used in tasks such as text simplification. Given s, we consider the sets of added, deleted, and kept n-grams when mapping the question x to s. We compute these three sets for both s and ˆs using the standard of up to 4-grams, then average (a) the F1 for added n-grams between s and ˆs, (b) the F1 for kept n-grams, and (c) the precision for the deleted n-grams. cost. GED computes the minimal-cost graph edit path required for transitioning from Gs to Gˆs (and vice versa), normalized by max(|Gs |, |Gˆs |). Operation costs are 1 for insertion and deletion of nodes and edges. The"
2020.tacl-1.13,H90-1020,0,0.415169,"81 7,982 32,164 13,935 29,680 13,517 11,214 5,520 34,689 96,567 23,066 2,988, 2,991high 10,230, 10,262high 10,575high 83,978 Figure 4: User interface for decomposing a complex question that uses a closed lexicon of tokens. Table 3: The QA datasets in BREAK. Lists the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed t"
2020.tacl-1.13,D18-1259,0,0.13256,"Missing"
2020.tacl-1.13,D19-1261,0,0.0255197,"to BREAKRC, that is trained on SQUAD, BERTQA is trained on the target dataset (HOTPOTQA), giving it an advantage over BREAKRC. A COMBINED Approach Last, we present an approach that combines the strengths of BREAKRC and BERTQA. In this approach, we use the QDMR decomposition to improve retrieval only. Given a question x and its QDMR s, we run BREAKRC on s, but in addition to storing answers, we also store all the paragraphs retrieved by the IR model. We then run BERTQA on the question x and the top-10 paragraphs retrieved by BREAKRC, sorted by their IR ranking. This approach resembles that of Qi et al. (2019). The advantage of COMBINED is that we do not need to develop an answering procedure for each QDMR operator separately, which involves dif6 INTERSECTION steps are handled in a manner similar to FILTER, but we omit the exact description for brevity. 191 Model BERTQA BREAKRCP BREAKRCG COMBINEDP COMBINEDG IR-NP BREAKRCR COMBINEDR EM 33.6 28.8 34.6 38.3 41.2 31.7 18.9 32.7 HOTPOTQA F1 43.3 37.7 44.6 49.3 52.4 41.2 26.5 42.6 IR 46.3 52.5 59.2 52.5 59.2 40.8 40.3 40.3 Table 6: Open-domain QA results on HOTPOTQA. ferent discrete operations such as comparison and intersection. Instead, we use BREAKRC"
2020.tacl-1.13,P16-2033,0,0.0282538,"s = hs1 , . . . , sn i do 4: op ← OPTYPE(si ) 5: ref s ← REFERENCEDSTEPS(si ) 6: if op is SELECT then 7: ans ← ANSWER(si ) 8: else if op is FILTER then 9: sˆi ← EXTRACTQUESTION(si ) 10: anstmp ← ANSWER(ˆ si ) 11: ans ← INTERSECT(anstmp , ansrs[ref s[0]]) 12: else if op is COMPARISON then 13: ans ← COMPARESTEPS(ref s,s) 14: else ⊲ op is PROJECT 15: sˆi ← SUBSTITUTEREF (si , ansrs[ref s[0]]) 16: ans ← ANSWER(ˆ si ) 17: ansrs[i] ← ans 18: return ansrs[n] Figure 5: Examples and justifications of expert judgment on collected QDMRs in BREAK. a cheap intermediate representation for semantic parsers (Yih et al., 2016), further discussed in §6. QDMR operator (f i ) and its arguments from each step (si ). To infer these formal representations, we developed an algorithm that goes over the QDMR structure step-by-step, and for each step si , uses a set of predefined templates to identify f i and its arguments, expressed in si . This results in an execution graph (Figure 2), where the execution result of a parent node serves as input to its child. Figure 1 presents three QDMR decompositions along with the formal graphs output by our algorithm (lower box). Each node lists its operator (e.g., GROUP), its constant"
2020.tacl-1.13,D16-1264,0,0.0946606,"‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, that is trained on SQUAD, BERTQA is trained on the targ"
2020.tacl-1.13,D18-1425,0,0.0326117,"567 23,066 2,988, 2,991high 10,230, 10,262high 10,575high 83,978 Figure 4: User interface for decomposing a complex question that uses a closed lexicon of tokens. Table 3: The QA datasets in BREAK. Lists the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed to use words from a lexicon Lx , which contains: (a) words appeari"
2020.tacl-1.13,Q16-1010,0,0.0640017,"Missing"
2020.tacl-1.13,N18-1059,1,0.945937,"SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1974), the same intuition"
2020.tacl-1.13,P11-1060,0,\N,Missing
2020.tacl-1.13,W18-2501,1,\N,Missing
2020.tacl-1.13,P19-1644,0,\N,Missing
2021.acl-short.22,W10-2903,0,0.0585114,"llow object above a black object with x. Our consistency reward would score z2 the highest since it maps the shared phrase most similarly compared to z 0 . Introduction Semantic parsers map a natural language utterance into an executable meaning representation, called a logical form or program (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These programs can be executed against a context (e.g., database, image, etc.) to produce a denotation (e.g., answer) for the input utterance. Methods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score pr"
2021.acl-short.22,N19-1273,1,0.907074,"hods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score programs. In this work we encourage consistency between the output programs of related natural language utterances to mitigate the issue of spurious programs. Consider related utterances, There are two boxes with three yellow squares and There are three yellow squares, both containing the phrase three yellow squares. Ideally, the correct programs for the utterances should contain similar sub-parts that corresponds to the shared phrase. To incorporate this intuition during search, we propose a con"
2021.acl-short.22,P19-1266,0,0.0222816,"Missing"
2021.acl-short.22,P17-1097,0,0.0173791,", answer) for the input utterance. Methods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score programs. In this work we encourage consistency between the output programs of related natural language utterances to mitigate the issue of spurious programs. Consider related utterances, There are two boxes with three yellow squares and There are three yellow squares, both containing the phrase three yellow squares. Ideally, the correct programs for the utterances should contain similar sub-parts that corresponds to the shared phrase. To incorporate this"
2021.acl-short.22,D17-1160,1,0.901987,"rograms in a given set of logical forms Zi , all of which evaluate to the correct denotation. The set Zi is constructed either by performing a heuristic search, or generated from a trained semantic parser. The reward-based method maximizes the (approximate) expected value of a reward function R. max θ In this section we provide a background on the NLVR dataset (Suhr et al., 2017) and the semantic parser of Dasigi et al. (2019). 3 Weakly supervised iterative search parser We use the semantic parser of Dasigi et al. (2019) which is a grammar-constrained encoder-decoder with attention model from Krishnamurthy et al. (2017). It learns to map a natural language utterance x into a program z such that it evaluates to the Ep˜(zi |xi ;θ) R(xi , zi , ci , yi ) (1) ∀i Here, p˜ is the re-normalization of the probabilities assigned to the programs on the beam, and the reward function R = 1 if zi evaluates to the correct denotation for all images in ci , or 0 otherwise. Please refer Dasigi et al. (2019) for details. Background Natural Language Visual Reasoning (NLVR) dataset contains human-written natural language utterances, where each utterance is paired with 4 synthetically-generated images. Each (utterance, image) pai"
2021.acl-short.22,P11-1060,0,0.114325,"lack object with x. Our consistency reward would score z2 the highest since it maps the shared phrase most similarly compared to z 0 . Introduction Semantic parsers map a natural language utterance into an executable meaning representation, called a logical form or program (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These programs can be executed against a context (e.g., database, image, etc.) to produce a denotation (e.g., answer) for the input utterance. Methods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score programs. In this work"
2021.acl-short.22,P17-2034,0,0.0296424,"denotations, their approach iteratively alternates between two phases to train the parser: Maximum marginal likelihood (MML) and a Reward-based method (RBM). In MML, for an utterance xi , the model maximizes the marginal likelihood of programs in a given set of logical forms Zi , all of which evaluate to the correct denotation. The set Zi is constructed either by performing a heuristic search, or generated from a trained semantic parser. The reward-based method maximizes the (approximate) expected value of a reward function R. max θ In this section we provide a background on the NLVR dataset (Suhr et al., 2017) and the semantic parser of Dasigi et al. (2019). 3 Weakly supervised iterative search parser We use the semantic parser of Dasigi et al. (2019) which is a grammar-constrained encoder-decoder with attention model from Krishnamurthy et al. (2017). It learns to map a natural language utterance x into a program z such that it evaluates to the Ep˜(zi |xi ;θ) R(xi , zi , ci , yi ) (1) ∀i Here, p˜ is the re-normalization of the probabilities assigned to the programs on the beam, and the reward function R = 1 if zi evaluates to the correct denotation for all images in ci , or 0 otherwise. Please refe"
2021.crac-1.16,W99-0211,0,0.132471,"dhan et al., 2012) and PreCo (Chen et al., 2018) datasets, we high1 Introduction light the low-precision, high-recall nature of the detector. While traditionally only recall is emphaCoreference resolution identifies mentions in a docsized for the detector as a design decision (Lee ument that co-refer to the same entity. It is an et al., 2011; Lee et al., 2017), we show huge degraimportant task facilitating many applications such dation from noisy mentions and, perhaps surprisas reading comprehension (Dasigi et al., 2019) and ingly, increasing the number of candidates considtext summarization (Azzam et al., 1999). ered by the baseline linker only deteriorates the Lee et al. (2017) proposed the first neural end-toperformance. While some classical coreference end architecture for coreference resolution. Most pipelines focused on detector precision (Uryupina, recent systems use it as a backbone and employ bet2009), it is rarely emphasized for current end-toter scoring functions (Zhang et al., 2018), pruning end systems. We hence stress the importance of a procedures (Lee et al., 2018), or token represenprecision-recall balance for the detector and demontations (Joshi et al., 2019, 2020).1 Despite this st"
2021.crac-1.16,D18-1016,0,0.0992519,"to its inability to make imporfamily, SpanBERT (Joshi et al., 2020) + c2ftant anaphoricity decisions. We also highlight coref (Lee et al., 2018), and investigate the interacthe enormous room for improving the linker tion between its two components: mention detector and show that the rest of its errors mainly inand mention linker. We study how their errors involve pronoun resolution. We propose promisdependently or jointly affect the final clustering. ing next steps and hope our findings will help future research in coreference resolution. Using the CoNLL-2012 (Pradhan et al., 2012) and PreCo (Chen et al., 2018) datasets, we high1 Introduction light the low-precision, high-recall nature of the detector. While traditionally only recall is emphaCoreference resolution identifies mentions in a docsized for the detector as a design decision (Lee ument that co-refer to the same entity. It is an et al., 2011; Lee et al., 2017), we show huge degraimportant task facilitating many applications such dation from noisy mentions and, perhaps surprisas reading comprehension (Dasigi et al., 2019) and ingly, increasing the number of candidates considtext summarization (Azzam et al., 1999). ered by the baseline linker"
2021.crac-1.16,D19-1606,1,0.89184,"Missing"
2021.crac-1.16,P18-2058,0,0.0167957,"r and that the 150 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics remaining errors mainly involve pronoun resolution. We hope this work sheds light on the internals of the mainstream coreference system and, with our proposed next steps, catalyze future research. We believe some of our findings may also transfer to other tasks with a similar joint span detection and span (pair) classification architecture, such as SRL (He et al., 2018), IE (Luan et al., 2019), and entity linking (Kolitsas et al., 2018). See Jiang et al. (2020) which subsumes many other tasks under such a span-based framework. 2 Background Model We study the coarse-to-fine coreference system (c2f-coref; Lee et al. 2018). It assigns an antecedent for every span in a document of length T , including a dummy that indicates non-mentions or non-anaphoric mentions. The final clustering is the transitive closure of connected spans. The system consists of a mention detector and a mention linker. The detector scores all O(T 2 ) spans up to length L and outputs the λT"
2021.crac-1.16,2020.acl-main.192,0,0.0116504,"naphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics remaining errors mainly involve pronoun resolution. We hope this work sheds light on the internals of the mainstream coreference system and, with our proposed next steps, catalyze future research. We believe some of our findings may also transfer to other tasks with a similar joint span detection and span (pair) classification architecture, such as SRL (He et al., 2018), IE (Luan et al., 2019), and entity linking (Kolitsas et al., 2018). See Jiang et al. (2020) which subsumes many other tasks under such a span-based framework. 2 Background Model We study the coarse-to-fine coreference system (c2f-coref; Lee et al. 2018). It assigns an antecedent for every span in a document of length T , including a dummy that indicates non-mentions or non-anaphoric mentions. The final clustering is the transitive closure of connected spans. The system consists of a mention detector and a mention linker. The detector scores all O(T 2 ) spans up to length L and outputs the λT highest-scoring spans as possibly anaphoric mentions. The linker links each mention candidat"
2021.crac-1.16,2020.tacl-1.5,0,0.0785466,"., 2015; Martschat and ior of its two components: mention detector Strube, 2015; Wiseman et al., 2016, inter alia). and mention linker. While the detector tradiHowever, it is unknown if observations on such tionally focuses heavily on recall as a design classical feature-based and often pipelined systems decision, we demonstrate the importance of extend to current neural end-to-end models. precision, calling for their balance. However, we point out the difficulty in building a preWe consider the best instantiation of this model cise detector due to its inability to make imporfamily, SpanBERT (Joshi et al., 2020) + c2ftant anaphoricity decisions. We also highlight coref (Lee et al., 2018), and investigate the interacthe enormous room for improving the linker tion between its two components: mention detector and show that the rest of its errors mainly inand mention linker. We study how their errors involve pronoun resolution. We propose promisdependently or jointly affect the final clustering. ing next steps and hope our findings will help future research in coreference resolution. Using the CoNLL-2012 (Pradhan et al., 2012) and PreCo (Chen et al., 2018) datasets, we high1 Introduction light the low-pr"
2021.crac-1.16,D19-1588,0,0.0616983,"nsidtext summarization (Azzam et al., 1999). ered by the baseline linker only deteriorates the Lee et al. (2017) proposed the first neural end-toperformance. While some classical coreference end architecture for coreference resolution. Most pipelines focused on detector precision (Uryupina, recent systems use it as a backbone and employ bet2009), it is rarely emphasized for current end-toter scoring functions (Zhang et al., 2018), pruning end systems. We hence stress the importance of a procedures (Lee et al., 2018), or token represenprecision-recall balance for the detector and demontations (Joshi et al., 2019, 2020).1 Despite this strate how pruning hyperparameters, in addition to usage, little in-depth analysis has been done to reducing computational complexity, control this better understand the inner workings of such an trade-off. However, we show the difficulty of obinfluential system. Xu and Choi (2020) analyzed taining a precise detector by demonstrating the imthe effect of the high-order inference, while Subportance of anaphoricity decisions and the inability ramanian and Roth (2019) and Zhao et al. (2018) of the detector to make them. Finally, we high1 Except Wu et al. (2020) which has not"
2021.crac-1.16,K18-1050,0,0.0243627,"ional Models of Reference, Anaphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics remaining errors mainly involve pronoun resolution. We hope this work sheds light on the internals of the mainstream coreference system and, with our proposed next steps, catalyze future research. We believe some of our findings may also transfer to other tasks with a similar joint span detection and span (pair) classification architecture, such as SRL (He et al., 2018), IE (Luan et al., 2019), and entity linking (Kolitsas et al., 2018). See Jiang et al. (2020) which subsumes many other tasks under such a span-based framework. 2 Background Model We study the coarse-to-fine coreference system (c2f-coref; Lee et al. 2018). It assigns an antecedent for every span in a document of length T , including a dummy that indicates non-mentions or non-anaphoric mentions. The final clustering is the transitive closure of connected spans. The system consists of a mention detector and a mention linker. The detector scores all O(T 2 ) spans up to length L and outputs the λT highest-scoring spans as possibly anaphoric mentions. The linker li"
2021.crac-1.16,D13-1027,0,0.0306247,"hemselves ... Match ... those private , er , buildings , that is , the (11) business community , ah , is willing to ... Other And Dr. Andy Henry notices something else Match (7) Dr. Mann says they ’ve narrowed it down ... ... Hong Kong cinema has nurtured many Semantic Proximity internationally renowned directors ... ... memorializing Hong Kong ’s 100 - year (12) film history . Others But [Paul Kelly] [Steve Sodbury] and Mel (5) Anderson ... had no idea ... Table 6: Examples of categorized conflated entity errors in the CoNLL-12 development set with a perfect detector. Following past studies (Kummerfeld and Klein, 2013; Joshi et al., 2019), we consider all deictic terms as pronouns. Each example contains two incorrectly linked entities in bold. Square brackets are added to separate mentions. ity decisions without explicit inter-span relational modeling. In §C we also show the degradation of the confusion index with shorter spans. Given the importance of anaphoric mention precision (§4), more research in improving anaphoricity decisions in the detector would be fruitful, for example, by more explicitly attending to neighboring spans. Alternatively, as Zhong and Chen (2021) showed the benefit of disentangling"
2021.crac-1.16,D17-1018,0,0.019215,"mention linker. We study how their errors involve pronoun resolution. We propose promisdependently or jointly affect the final clustering. ing next steps and hope our findings will help future research in coreference resolution. Using the CoNLL-2012 (Pradhan et al., 2012) and PreCo (Chen et al., 2018) datasets, we high1 Introduction light the low-precision, high-recall nature of the detector. While traditionally only recall is emphaCoreference resolution identifies mentions in a docsized for the detector as a design decision (Lee ument that co-refer to the same entity. It is an et al., 2011; Lee et al., 2017), we show huge degraimportant task facilitating many applications such dation from noisy mentions and, perhaps surprisas reading comprehension (Dasigi et al., 2019) and ingly, increasing the number of candidates considtext summarization (Azzam et al., 1999). ered by the baseline linker only deteriorates the Lee et al. (2017) proposed the first neural end-toperformance. While some classical coreference end architecture for coreference resolution. Most pipelines focused on detector precision (Uryupina, recent systems use it as a backbone and employ bet2009), it is rarely emphasized for current e"
2021.crac-1.16,N18-2108,0,0.0398491,"Missing"
2021.crac-1.16,2020.emnlp-main.536,0,0.0204818,"correctly linked, a case that necessitates better higher-order inference. Third person pronouns with different referents are conflated 29 times. Errors with first or second person pronouns occur 37 times, usually due to speaker switching. Similar to §5.1, separately parameterizing the linker’s encoder may help reduce conflation: intuitively, the span representation for mention detection may promote homogeneity. Meanwhile, the lack of discerning span-internal content for certain error types including pronoun resolution and exact match, combined with current systems’ trend to rely on such cues (Lu and Ng, 2020), calls for more focus on improving their contextual reasoning. 6 Conclusion We analyzed the complex interaction between the mention detector and linker in the mainstream coarse-to-fine coreference system. Using oracle experiments, we showed that, while detector recall is important, higher anaphoric mention precision would lead to dramatically better linker performance, though achieving this is difficult. We also demonstrated that the oracle linker performance is near perfect and that the vast majority of remaining linker errors besides anaphoricity decisions are about pronoun resolution. We h"
2021.crac-1.16,N19-1308,0,0.0141881,"ceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics remaining errors mainly involve pronoun resolution. We hope this work sheds light on the internals of the mainstream coreference system and, with our proposed next steps, catalyze future research. We believe some of our findings may also transfer to other tasks with a similar joint span detection and span (pair) classification architecture, such as SRL (He et al., 2018), IE (Luan et al., 2019), and entity linking (Kolitsas et al., 2018). See Jiang et al. (2020) which subsumes many other tasks under such a span-based framework. 2 Background Model We study the coarse-to-fine coreference system (c2f-coref; Lee et al. 2018). It assigns an antecedent for every span in a document of length T , including a dummy that indicates non-mentions or non-anaphoric mentions. The final clustering is the transitive closure of connected spans. The system consists of a mention detector and a mention linker. The detector scores all O(T 2 ) spans up to length L and outputs the λT highest-scoring spans a"
2021.crac-1.16,Q15-1029,0,0.0644174,"Missing"
2021.crac-1.16,N16-1115,0,0.0574546,"Missing"
2021.crac-1.16,N15-1082,0,0.0608176,"Missing"
2021.crac-1.16,W12-4501,0,0.2241,"of this model cise detector due to its inability to make imporfamily, SpanBERT (Joshi et al., 2020) + c2ftant anaphoricity decisions. We also highlight coref (Lee et al., 2018), and investigate the interacthe enormous room for improving the linker tion between its two components: mention detector and show that the rest of its errors mainly inand mention linker. We study how their errors involve pronoun resolution. We propose promisdependently or jointly affect the final clustering. ing next steps and hope our findings will help future research in coreference resolution. Using the CoNLL-2012 (Pradhan et al., 2012) and PreCo (Chen et al., 2018) datasets, we high1 Introduction light the low-precision, high-recall nature of the detector. While traditionally only recall is emphaCoreference resolution identifies mentions in a docsized for the detector as a design decision (Lee ument that co-refer to the same entity. It is an et al., 2011; Lee et al., 2017), we show huge degraimportant task facilitating many applications such dation from noisy mentions and, perhaps surprisas reading comprehension (Dasigi et al., 2019) and ingly, increasing the number of candidates considtext summarization (Azzam et al., 1999"
2021.crac-1.16,N18-2003,0,0.0159398,", 2018), or token represenprecision-recall balance for the detector and demontations (Joshi et al., 2019, 2020).1 Despite this strate how pruning hyperparameters, in addition to usage, little in-depth analysis has been done to reducing computational complexity, control this better understand the inner workings of such an trade-off. However, we show the difficulty of obinfluential system. Xu and Choi (2020) analyzed taining a precise detector by demonstrating the imthe effect of the high-order inference, while Subportance of anaphoricity decisions and the inability ramanian and Roth (2019) and Zhao et al. (2018) of the detector to make them. Finally, we high1 Except Wu et al. (2020) which has not seen wide adoption. light the high potential of the linker and that the 150 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics remaining errors mainly involve pronoun resolution. We hope this work sheds light on the internals of the mainstream coreference system and, with our proposed next steps, catalyze future research. We believe some"
2021.crac-1.16,N13-1071,0,0.0748834,"Missing"
2021.crac-1.16,S19-1021,0,0.0227181,"Missing"
2021.crac-1.16,2021.naacl-main.5,0,0.0132324,"ector. Following past studies (Kummerfeld and Klein, 2013; Joshi et al., 2019), we consider all deictic terms as pronouns. Each example contains two incorrectly linked entities in bold. Square brackets are added to separate mentions. ity decisions without explicit inter-span relational modeling. In §C we also show the degradation of the confusion index with shorter spans. Given the importance of anaphoric mention precision (§4), more research in improving anaphoricity decisions in the detector would be fruitful, for example, by more explicitly attending to neighboring spans. Alternatively, as Zhong and Chen (2021) showed the benefit of disentangling the span representations for entity detection and relation extraction in information extraction based on the intuition that they are disparate tasks, one may split the task of anaphoricity decision from mention linking and introduce a separately parameterized anaphoricity module, similarly considering the discrepancy between the two tasks. Recasens et al. (2013); Moosavi and Strube (2016); inter alia have pursued similar ideas in the pre-neural era, but it has still not yet been explored with deep models. 5.2 The Linker’s Errors with a stronger linker: in T"
2021.crac-1.16,N16-1114,0,0.0563054,"Missing"
2021.crac-1.16,2020.acl-main.622,0,0.0137527,"montations (Joshi et al., 2019, 2020).1 Despite this strate how pruning hyperparameters, in addition to usage, little in-depth analysis has been done to reducing computational complexity, control this better understand the inner workings of such an trade-off. However, we show the difficulty of obinfluential system. Xu and Choi (2020) analyzed taining a precise detector by demonstrating the imthe effect of the high-order inference, while Subportance of anaphoricity decisions and the inability ramanian and Roth (2019) and Zhao et al. (2018) of the detector to make them. Finally, we high1 Except Wu et al. (2020) which has not seen wide adoption. light the high potential of the linker and that the 150 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics remaining errors mainly involve pronoun resolution. We hope this work sheds light on the internals of the mainstream coreference system and, with our proposed next steps, catalyze future research. We believe some of our findings may also transfer to other tasks with a similar joint s"
2021.crac-1.16,2020.emnlp-main.686,0,0.0183654,"s use it as a backbone and employ bet2009), it is rarely emphasized for current end-toter scoring functions (Zhang et al., 2018), pruning end systems. We hence stress the importance of a procedures (Lee et al., 2018), or token represenprecision-recall balance for the detector and demontations (Joshi et al., 2019, 2020).1 Despite this strate how pruning hyperparameters, in addition to usage, little in-depth analysis has been done to reducing computational complexity, control this better understand the inner workings of such an trade-off. However, we show the difficulty of obinfluential system. Xu and Choi (2020) analyzed taining a precise detector by demonstrating the imthe effect of the high-order inference, while Subportance of anaphoricity decisions and the inability ramanian and Roth (2019) and Zhao et al. (2018) of the detector to make them. Finally, we high1 Except Wu et al. (2020) which has not seen wide adoption. light the high potential of the linker and that the 150 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 150–157 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics rem"
2021.emnlp-main.135,N19-1300,0,0.0843142,"s us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used t"
2021.emnlp-main.135,D19-1418,0,0.115214,"s us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used t"
2021.emnlp-main.135,2020.findings-emnlp.272,0,0.46513,"Missing"
2021.emnlp-main.135,W95-0103,0,0.378446,"each appearOur null hypothesis is that the binomial proportion ance in an instance as a separate occurrence5 for pb (y |xi ) = 0.5 = p0 , or equivalently, that ri = 0. 4 The use of a z-statistic depends on the normal approxiOur alternative hypothesis is that pb (y |xi ) ≥ 0.5. mation to a binomial distribution, which holds for large n. 5 Let pˆ be the observed probability. We can compute We remove punctuation and tokenize on whitespace only. 1804 UD English Web Treebank Next we turn to dependency parsing. In particular, we focus on the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), which involves determining whether a PP attaches to a verb (e.g., We ate spaghetti with forks) or a noun (e.g., We ate spaghetti with meatballs). We heuristically extract (verb, noun, prepositional phrase) constructions with ambiguous attachment from the UD English Web Treebank (EWT) training data.7 We treat (verb, preposition) tuples as features and attachment types (noun or verb) as labels, and we compute a z-statistic for each tuple. Figure 2 shows the z-statistic for each tuple that appears 10 or more times in the data. We labeled tuples that also appear in the locally edited samples fro"
2021.emnlp-main.135,W19-3504,0,0.0659607,"y problems, we need data that accurately reflects the competency assumption, both to evaluate systems and (presumably) to train them. However, humans suffer from blind spots, social bias, priming, and other psychological effects that make collecting data for competency problems challenging. Examples of these effects include instructions in a crowdsourcing task that prime workers to use particular language,3 or distributional effects in source material, such as the “amazing” examples above, or racial bias in face recognition (Buolamwini and Gebru, 2018) and abusive language detection datasets (Davidson et al., 2019; Sap et al., 2019). In order to formally analyze the impact of human bias on collecting data for competency problems, we need a plausible model of this bias. We represent bias as rejection sampling from the target competency distribution based on single feature values. Specifically, we assume the following dataset collection procedure. First, a person samples an instance from an unbiased distribution pu (x, y) where the competency assumption holds. The person examines this instance, and if feature xi = 1 appears with label y = 0, the person rejects the instance and samples a new one, with pro"
2021.emnlp-main.135,N19-1246,1,0.886483,"Missing"
2021.emnlp-main.135,D19-1107,0,0.159203,"Missing"
2021.emnlp-main.135,N18-2017,1,0.738595,"Missing"
2021.emnlp-main.135,2020.insights-1.13,0,0.0362039,"ses some additional empirical evidence for models’ reliance on these artifacts. 5 Mitigating Artifacts with Local Edits ately sensitive edit model, where sensitivity refers to how often a change to inputs results in the label changing. However, because humans are involved in making these changes, achieving appropriate sensitivity is challenging, and bias in this process can lead to the introduction of new artifacts. This suggests that care must be taken when performing edit-based data augmentation, as large edited training datasets are not likely to be artifact-free (cf. Tafjord et al., 2019; Huang et al., 2020). Imagine a new dataset De consisting of samples x0 , y 0 generated by making local edits according to the following repeated procedure: 1. Randomly sample an instance x from a dataset Db of n instances created under pb . 2. Make some changes to x to arrive at x0 . 3. Manually label y 0 and add hx0 , y 0 i to De . We examine the expected probability pe (y 0 |x0i ) under this edit process. To derive this probability, we will need to know the probability that a change to x changes y. We define the edit sensitivity s to be this probability, i.e., s = pb (y 0 = ¬y). The other quantity of interest"
2021.emnlp-main.135,D17-1215,0,0.0551645,"ability of the label given the presence of the word. The label associated with each point is marked by color and superscript. All features above the blue line have detectable correlation with class labels, using a very conservative Bonferronicorrected statistical test. Attempts by the natural language processing community to get machines to understand language or read text are often stymied in part by issues in our datasets (Chen et al., 2016; Sugawara et al., 2018). Many recent papers have shown that popular datasets are prone to shortcuts, dataset artifacts, bias, and spurious correlations (Jia and Liang, 2017; Rudinger et al., 2018; Costa-jussà et al., 2019). While these empirical demonstrations of Equal contribution 0.6 = 0.01/28k neutral contradict entailment 0.2 Introduction ∗ nobodyc catsc n vacation cat c sleepingc first n noc outdoorse competitionn animal e 1.0 deficiencies in the data are useful, they often leave unanswered fundamental questions of what exactly makes a correlation “spurious”, instead of a feature that is legitimately predictive of some target label. In this work we attempt to address this question theoretically. We begin with the assumption that in a language understanding"
2021.emnlp-main.135,2020.acl-main.769,0,0.0999329,"Missing"
2021.emnlp-main.135,D19-5808,1,0.823901,"ly differs from 0.5. In this section, we will show that an artifact emerges if there is a bias at dimension i in the sampling procedure, which is inevitable for some features in practice. We will formalize this bias in terms of a rejection sampling probability ri . For a single sample x, y, we first derive the joint and marginal probabilities pb (y, xi ) and pb (xi ), from which we can obtain pb (y|xi ). These formulas use a recurrence relation obtained from the rejection sampling procedure. 3 This is ubiquitous in crowdsourcing; see, e.g., common patterns in DROP (Dua et al., 2019) or ROPES (Lin et al., 2019) that ultimately derive from annotator instructions. 1803 1 1 pb (y, xi ) = fi + fi ri pb (y, xi ) 2 2 fi ∴ pb (y, xi ) = 2 − fi ri 1 1 1 pb (xi ) = fi + fi (1 − ri ) + fi ri pb (xi ) 2 2 2 2fi − fi ri ∴ pb (xi ) = 2 − fi ri pb (y, xi ) 1 ∴ pb (y |xi ) = = pb (xi ) 2 − ri With no bias (ri = 0), this probability is 0.5, as expected, and it rises to 1 as ri increases to 1. We define pˆ(y|xi ) as the empirical expectation of pb (y|xi ) over n samples containing xi , with different samples indexed by superscript j. pˆ(y|xi ) = 1 Pn j ˆ is a conditional binomial j=1 y . Note that p n random variabl"
2021.emnlp-main.135,P11-1015,0,0.0437068,"in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used the term sensitivity in an informal way to describe the probability that a local edit changes the label. This term also has a related formal definition in the study of bo"
2021.emnlp-main.135,2020.emnlp-demos.16,0,0.067072,"Missing"
2021.emnlp-main.135,W17-5525,0,0.0666648,"Missing"
2021.emnlp-main.135,2020.findings-emnlp.225,1,0.71033,"orrelated? This could happen due to societal biases, word usage frequencies, or priming effects from data collection instructions given to all annotators. Surely across any pool of annotators there will be some dimensions along which r values are correlated, and other dimensions along with they are not. Increasing the number of annotators thus helps mitigate the problem, but does not solve it completely. Data filtering A recent trend is to remove data from a training set that is biased in some way in order to get a model that generalizes better (Le Bras et al., 2020; Swayamdipta et al., 2020; Oren et al., 2020). While this method can be effective for very biased datasets, it is somewhat unsatisfying to 6 Other Mitigation Techniques remove entire instances because of bias in a single feature. In the extreme case where ri ≈ 1, such as In this section we briefly discuss the implications of with “nobody” in SNLI (Fig. 1), this process could our theoretical analysis for other artifact mitigation effectively remove xi from the observed feature techniques that have been proposed in the literature. space. Our analysis in this section is not rigorous and is To understand the effect of these automated meant o"
2021.emnlp-main.135,N18-2002,0,0.0362563,"Missing"
2021.emnlp-main.135,P19-1163,1,0.817395,"a that accurately reflects the competency assumption, both to evaluate systems and (presumably) to train them. However, humans suffer from blind spots, social bias, priming, and other psychological effects that make collecting data for competency problems challenging. Examples of these effects include instructions in a crowdsourcing task that prime workers to use particular language,3 or distributional effects in source material, such as the “amazing” examples above, or racial bias in face recognition (Buolamwini and Gebru, 2018) and abusive language detection datasets (Davidson et al., 2019; Sap et al., 2019). In order to formally analyze the impact of human bias on collecting data for competency problems, we need a plausible model of this bias. We represent bias as rejection sampling from the target competency distribution based on single feature values. Specifically, we assume the following dataset collection procedure. First, a person samples an instance from an unbiased distribution pu (x, y) where the competency assumption holds. The person examines this instance, and if feature xi = 1 appears with label y = 0, the person rejects the instance and samples a new one, with probability ri . If y"
2021.emnlp-main.135,E17-2060,0,0.0241905,"or less uniformly, then ei ≈ 0 in a high-dimensional space, so engineering s ≈ 0.5 should produce artifact-free additional samples with pe (y 0 |x0i ) ≈ 0.5. Furthermore, edit sensitivity and edit dimension are empirically measurable when constructing a dataset. This empirical measurement can give theoretical guarantees for the degree to which the local editing will alleviate artifacts (i.e., this gives us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classifi"
2021.emnlp-main.135,2020.acl-main.468,0,0.0189388,"orse n competition animal e 0.6 outsidee persone 0.4 peoplee 0.2 0.0 102 103 n 104 105 Figure 4: Statistical artifacts in ambiguous instances (Swayamdipta et al., 2020; above) versus a random (same-size) sample from the SNLI training set (below). The filtering done by ambiguous instance detection targets statistical artifacts across the whole range of the statistical test, not just high PMI values. importance of the competency assumption.12 7 Other Related Work Theoretical analysis of bias Several recent works explore sources and theoretical treatments of bias or spurious correlations in NLP (Shah et al., 2020a; Kaushik et al., 2020) or ML more broadly (Shah et al., 2020b). Our work differs by introducing a competency assumption and exploring its implications. The difference between our biased and unbiased distributions is an instance of covariate shift (Quionero-Candela et al., 2009). Competent models An interesting question is whether we can inject a “competency inductive bias” into models, i.e., discourage relying on individual features. The closest works we are aware of are methods that ensemble weak models together with strong models during training (Clark et al., 2020; Dagaev et al., 2021), o"
2021.emnlp-main.135,silveira-etal-2014-gold,0,0.042419,"Missing"
2021.emnlp-main.135,D18-1453,0,0.0164836,"ividual feature (here words) should give information about the class label, plotting the number of occurrences of each word against the conditional probability of the label given the presence of the word. The label associated with each point is marked by color and superscript. All features above the blue line have detectable correlation with class labels, using a very conservative Bonferronicorrected statistical test. Attempts by the natural language processing community to get machines to understand language or read text are often stymied in part by issues in our datasets (Chen et al., 2016; Sugawara et al., 2018). Many recent papers have shown that popular datasets are prone to shortcuts, dataset artifacts, bias, and spurious correlations (Jia and Liang, 2017; Rudinger et al., 2018; Costa-jussà et al., 2019). While these empirical demonstrations of Equal contribution 0.6 = 0.01/28k neutral contradict entailment 0.2 Introduction ∗ nobodyc catsc n vacation cat c sleepingc first n noc outdoorse competitionn animal e 1.0 deficiencies in the data are useful, they often leave unanswered fundamental questions of what exactly makes a correlation “spurious”, instead of a feature that is legitimately predictive"
2021.emnlp-main.135,2020.emnlp-main.746,1,0.882498,"Missing"
2021.emnlp-main.135,D19-1608,1,0.825157,"text. Section 6 discusses some additional empirical evidence for models’ reliance on these artifacts. 5 Mitigating Artifacts with Local Edits ately sensitive edit model, where sensitivity refers to how often a change to inputs results in the label changing. However, because humans are involved in making these changes, achieving appropriate sensitivity is challenging, and bias in this process can lead to the introduction of new artifacts. This suggests that care must be taken when performing edit-based data augmentation, as large edited training datasets are not likely to be artifact-free (cf. Tafjord et al., 2019; Huang et al., 2020). Imagine a new dataset De consisting of samples x0 , y 0 generated by making local edits according to the following repeated procedure: 1. Randomly sample an instance x from a dataset Db of n instances created under pb . 2. Make some changes to x to arrive at x0 . 3. Manually label y 0 and add hx0 , y 0 i to De . We examine the expected probability pe (y 0 |x0i ) under this edit process. To derive this probability, we will need to know the probability that a change to x changes y. We define the edit sensitivity s to be this probability, i.e., s = pb (y 0 = ¬y). The other"
2021.emnlp-main.135,D19-1221,1,0.953469,"ve high performance, but will necessarily fail if the learner is evaluated under the competency setting. With a hypothesis test in hand, we can examine existing datasets for evidence of statisticallysignificant feature bias, and then explore the extent to which this bias impacts models supervised with this data. Prior work has used pointwise mutual information (PMI) to find features that have high correlation with labels (e.g., Gururangan et al., 2018). This measure is useful for understanding why certain features might get used as deterministic decision rules by models (Ribeiro et al., 2018; Wallace et al., 2019). However, studies involving PMI have also intuitively understood that PMI by itself does not tell the whole story, as a strict ranking by PMI would return features that only appear once in the dataset. To account for this problem, they used arbitrary cutoffs and included information about feature occurrence in addition to their PMI ranking. A benefit of our approach to defining and detecting artifacts is that we have a single statistical test that takes into account both the number of times a feature appears and how correlated it is with a single label. We use this test to find features with"
2021.emnlp-main.135,2020.emnlp-main.105,1,0.827376,"Missing"
2021.emnlp-main.135,2020.emnlp-demos.6,0,0.0369909,"Missing"
2021.emnlp-main.135,N18-2003,0,0.020093,"y, then ei ≈ 0 in a high-dimensional space, so engineering s ≈ 0.5 should produce artifact-free additional samples with pe (y 0 |x0i ) ≈ 0.5. Furthermore, edit sensitivity and edit dimension are empirically measurable when constructing a dataset. This empirical measurement can give theoretical guarantees for the degree to which the local editing will alleviate artifacts (i.e., this gives us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in t"
2021.emnlp-main.466,W18-2501,1,0.728493,"Missing"
2021.emnlp-main.466,P07-1036,1,0.753615,"Missing"
2021.emnlp-main.466,D19-1418,0,0.0257944,"To counter the authors and do not reflect the official policy or dataset biases, model-based data pruning (AFLite; position of the Department of Defense or the U.S. Bras et al., 2020) and subsampling (Oren et al., Government. 2020) have been proposed. Many of the techniques above modify the training-data distribution to remove a model’s propensity to find artificially sim- References ple decision boundaries, whereas we modify the Jacob Andreas. 2020. Good-enough compositional training objective to try to accomplish the same data augmentation. In ACL. goal. Ensemble-based training methodology (Clark et al., 2019; Stacey et al., 2020) has been proposed Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In to learn models robust to dataset artifacts; howCVPR. ever, they require prior knowledge about the kind of artifacts present in the data. Akari Asai and Hannaneh Hajishirzi. 2020. LogicOur approach, in spirit, is related to a large body guided data augmentation and regularization for consistent question answering. In ACL. of work on learning structured latent variable models. For example, prior work has incorporated indiDzmitry Bahdanau, Kyunghyun Cho, and Yo"
2021.emnlp-main.466,P17-1123,0,0.0285745,"years after the Battle of Rullion Green was the Battle of Drumclog? would result in the construction of When did the Battle of Rullion Green? and When did the Battle of Drumclog?. Inverting Superlatives For questions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and lab"
2021.emnlp-main.466,N19-1246,1,0.943113,"ciation for Computational Linguistics puts using related examples and provides the model with a richer training signal than what is provided by a single example. For example, What was the shortest field goal? shares the substructure of finding all field goals with How many field goals were scored?. For this paired example, our proposed objective would enforce that the output of this latent decision for the two questions is the same. We demonstrate the benefits of our paired training objective using a textual-NMN (Gupta et al., 2020a) designed to answer complex compositional questions on DROP (Dua et al., 2019), a dataset requiring natural language and symbolic reasoning against a paragraph of text. While there can be many ways of acquiring paired examples, we explore three directions. First, we show how naturally occurring paired questions can be automatically found from within the dataset. Further, since our method does not require end-task supervision for the paired example, one can also use data augmentation techniques to acquire paired questions without requiring additional annotation. We show how paired questions can be constructed using simple templates, and how a question generation model ca"
2021.emnlp-main.466,N16-1024,0,0.0321259,") where f , g, and h perform the three sub-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect l"
2021.emnlp-main.466,P18-1033,0,0.157998,"substructures provides the model with a dense enough training signal to learn correct module execution. That is, not only does performance improve by using the paired objective, this result shows that the model’s performance is improving for the right reasons. In §5.4 we explore how this faithfulness is actually achieved. 5.3 Evaluating Compositional Generalization A natural expectation from structured models is that the explicit structure should help the model learn reusable operations that generalize to novel contexts. We test this capability using the compositional generalization setup of Finegan-Dollak et al. (2018), where the model is tested on questions whose program templates are unseen during training. In our case, this tests whether module executions generalize to new contexts in a program. We create two test sets to measure our model’s capability to generalize to such out-of-distribution examples. In both settings, we identify certain program templates to keep in a held-out test set, and use the remaining questions for training and validation purposes. Complex Arithmetic This set contains questions that require addition and subtraction operations in complex contexts: questions whose program contain"
2021.emnlp-main.466,2021.acl-short.22,1,0.78435,"Missing"
2021.emnlp-main.466,N18-2017,0,0.0217897,"d training examLearning such models using just the end-task ples share internal substructure, we add an adsupervision is difficult, since the decision boundary ditional training objective to encourage conthat the model is trying to learn is complex, and sistency between their latent decisions. Such the lack of any supervision for the latent decisions an objective does not require external superprovides only a weak training signal. Moreover, the vision for the values of the latent output, or presence of dataset artifacts (Lai and Hockenmaier, even the end task, yet provides an additional 2014; Gururangan et al., 2018, among others), and training signal to that provided by individdegeneracy in the model, where incorrect latent ual training examples themselves. We apply our method to improve compositional question decisions can still lead to the correct output, further answering using neural module networks on complicates learning. As a result, models often the DROP dataset. We explore three ways fail to predict meaningful intermediate outputs and to acquire paired questions in DROP: (a) disinstead end up fitting to dataset quirks, thus hurting covering naturally occurring paired examples generalization (Su"
2021.emnlp-main.466,D19-1170,0,0.0449838,"Missing"
2021.emnlp-main.466,D17-1195,0,0.0252874,"(xi )) (2) zj = f (k(g(xj ))) (3) where f , g, and h perform the three sub-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learn"
2021.emnlp-main.466,D07-1031,0,0.0821882,"ard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct prediction further complicates learning. Consequently, models fail to learn to perform these latent tasks correctly and usually end up modeling irrelevant correlations in the data (Johnson, 2007; Subramanian et al., 2020). In this work, we propose a method to leverage paired examples—examples whose one or more latent decisions are related to each other—to provide an indirect supervision to these latent decisions. Consider paired training examples xi and xj with the following computation trees: We focus on structured compositional models for reasoning that perform an explicit problem decomposition and predict interpretable latent decisions that are composed to predict the final output. These intermediate outputs are often grounded in real5775 Figure 1: Proposed paired objective: For t"
2021.emnlp-main.466,D16-1032,0,0.0280223,"The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct prediction further complicates learning. Consequently, models fail to learn to perform these"
2021.emnlp-main.466,W19-4801,0,0.0253044,"uding the one in our model) often fail to general- better count performance. Using paired examples ize compositionally (Finegan-Dollak et al., 2018; only for max and count questions (L max+count ) does Lake and Baroni, 2018; Bahdanau et al., 2019). Re- not constrain the find operation sufficiently—the cent advancements in semantic parsing models that model has freedom to optimize the paired objective aim at compositional generalization should help by learning to incorrectly ground to the max-event improve overall model performance (Lake, 2019; mention for both the original and constructed quesKorrel et al., 2019; Herzig and Berant, 2020). tion’s find operation. This analysis reveals that augmented paired examples are most useful when 4 The test set size is quite small, so while the w/ G.P. results they form enough indirect connections between difare significantly better than MTMSN (p = 0.05), we can’t ferent types of instances to densely characterize the completely rule out noise as the cause for w/o G.P. outperforming MTMSN (p = 0.5), based on the Student’s t-test. decision boundary around the latent decisions. 5781 Model NMN + Lmax+min + Lmax+count + Lmax+min+count Test F1 Faithful.Overall Min-Max"
2021.emnlp-main.466,P19-1224,0,0.0216156,"attle of Rullion Green was the Battle of Drumclog? would result in the construction of When did the Battle of Rullion Green? and When did the Battle of Drumclog?. Inverting Superlatives For questions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program f"
2021.emnlp-main.466,S14-2055,0,0.02464,"Missing"
2021.emnlp-main.466,2020.acl-main.703,0,0.0194548,"min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program find-{num/date}(find). We use the whole ques2 We explain the reason for this in §A.2 tion apart from the Wh-word as the string argument to find. Similar to §4.1, for each of the find modules in a DROP question’s program, we see if a gen"
2021.emnlp-main.466,D19-1405,0,0.0240789,"n]), where BERT(xi , p) is the contextualized representation of xi -th question/passage, and [m : n] is its slice for the m through n token. g = find in all cases. See §3 for details. Here, since the outputs of the shared substructures should be the same, S would encourage equality between them. These trees share the internal substructure g(x). In such a scenario, we propose an additional training objective S(Jg(xi )K, Jg(xj )K) to enforce consistency of partial model execution for the shared substructure: Lpaired = S(Jg(xi )K, Jg(xj )K) (4) work. A few approaches (Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020) use an additional objective on model outputs to enforce consistency between paired examples; this is a special case of our framework where S is used on the outputs (yi , yj ), instead of the latent decisions. Using paired examples for indirect supervision on latent decisions should be broadly applicable to a wide class of models, and our general formulation of this technique is, we believe, novel. However, the specific application of this method to any particular problem is non-trivial, as work needs to be done to acquire paired data and design a suitable S for the"
2021.emnlp-main.466,P19-1598,1,0.850477,"b-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct pr"
2021.emnlp-main.466,K18-1007,0,0.171852,": n]) = g(BERT(xi , p)[m : n]), where BERT(xi , p) is the contextualized representation of xi -th question/passage, and [m : n] is its slice for the m through n token. g = find in all cases. See §3 for details. Here, since the outputs of the shared substructures should be the same, S would encourage equality between them. These trees share the internal substructure g(x). In such a scenario, we propose an additional training objective S(Jg(xi )K, Jg(xj )K) to enforce consistency of partial model execution for the shared substructure: Lpaired = S(Jg(xi )K, Jg(xj )K) (4) work. A few approaches (Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020) use an additional objective on model outputs to enforce consistency between paired examples; this is a special case of our framework where S is used on the outputs (yi , yj ), instead of the latent decisions. Using paired examples for indirect supervision on latent decisions should be broadly applicable to a wide class of models, and our general formulation of this technique is, we believe, novel. However, the specific application of this method to any particular problem is non-trivial, as work needs to be done to acquire paired data and design a s"
2021.emnlp-main.466,2020.findings-emnlp.225,1,0.843383,"Missing"
2021.emnlp-main.466,P19-1621,1,0.734336,"ork in complex compositional question answering. We also show that using our paired objective leads to improved prediction of latent decisions. The challenge in learning models for complex problems can be viewed as the emergence of artificially simple decision boundaries due to data sparsity and the presence of spurious dataset biases (Gardner et al., 2020). To counter data sparsity, data augmentation techniques have been proposed to provide a compositional inductive bias to the model (Chen et al., 2020; Andreas, 2020) or induce consistent outputs (Xie et al., 2020; Asai and Hajishirzi, 2020; Ribeiro et al., 2019). In order to induce correct internal learning, Teney et al. (2019) use auxiliary Acknowledgements relations between questions in VQA to enforce constraints between related questions’ embeddings, We would like to thank Dan Deutsch and the anonyand Teney et al. (2020) propose an auxiliary objec- mous reviewers for their helpful comments. This tive for the gradient update of an example based work was supported by Contract FA8750-19-2on existing counterfactual data. However, appli- 0201 with the US Defense Advanced Research cability of these approaches is limited to prob- Projects Agency (DARPA),"
2021.emnlp-main.466,P05-1044,0,0.564274,"are most useful when 4 The test set size is quite small, so while the w/ G.P. results they form enough indirect connections between difare significantly better than MTMSN (p = 0.05), we can’t ferent types of instances to densely characterize the completely rule out noise as the cause for w/o G.P. outperforming MTMSN (p = 0.5), based on the Student’s t-test. decision boundary around the latent decisions. 5781 Model NMN + Lmax+min + Lmax+count + Lmax+min+count Test F1 Faithful.Overall Min-Max Count score (↓) 57.4 60.9 60.8 71.1 82.1 85.5 81.4 85.4 36.2 39.7 43.0 58.8 110.4 56.5 99.2 25.9 tures (Smith and Eisner, 2005; Chang et al., 2010). These approaches use auxiliary objectives on a single training instance or global conditions on posterior distributions, whereas our training objective uses paired examples. 7 Conclusion Table 4: Using constructed paired examples for all three types of questions—min, max, and count—leads to dramatically better count performance. Without all three, the model finds shortcuts to satisfy the consistency constraint and does not learn correct module execution. 6 Related Work We propose a method to leverage paired examples— instances that share internal substructure—to provide"
2021.emnlp-main.466,2020.emnlp-main.665,0,0.023609,"rs and do not reflect the official policy or dataset biases, model-based data pruning (AFLite; position of the Department of Defense or the U.S. Bras et al., 2020) and subsampling (Oren et al., Government. 2020) have been proposed. Many of the techniques above modify the training-data distribution to remove a model’s propensity to find artificially sim- References ple decision boundaries, whereas we modify the Jacob Andreas. 2020. Good-enough compositional training objective to try to accomplish the same data augmentation. In ACL. goal. Ensemble-based training methodology (Clark et al., 2019; Stacey et al., 2020) has been proposed Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In to learn models robust to dataset artifacts; howCVPR. ever, they require prior knowledge about the kind of artifacts present in the data. Akari Asai and Hannaneh Hajishirzi. 2020. LogicOur approach, in spirit, is related to a large body guided data augmentation and regularization for consistent question answering. In ACL. of work on learning structured latent variable models. For example, prior work has incorporated indiDzmitry Bahdanau, Kyunghyun Cho, and Yoshua rect supervision"
2021.emnlp-main.466,2020.acl-main.495,1,0.914301,"18, among others), and training signal to that provided by individdegeneracy in the model, where incorrect latent ual training examples themselves. We apply our method to improve compositional question decisions can still lead to the correct output, further answering using neural module networks on complicates learning. As a result, models often the DROP dataset. We explore three ways fail to predict meaningful intermediate outputs and to acquire paired questions in DROP: (a) disinstead end up fitting to dataset quirks, thus hurting covering naturally occurring paired examples generalization (Subramanian et al., 2020). within the dataset, (b) constructing paired exWe propose a method to leverage related trainamples using templates, and (c) generating paired examples using a question generation ing examples to provide an indirect supervision to model. We empirically demonstrate that our these intermediate decisions. Our method is based proposed approach improves both in- and outon the intuition that related examples involve simiof-distribution generalization and leads to corlar sub-tasks; hence, we can use an objective on the rect latent decision predictions. outputs of these sub-tasks to provide an additio"
2021.emnlp-main.466,2020.acl-main.450,0,0.0176762,"stions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program find-{num/date}(find). We use the whole ques2 We explain the reason for this in §A.2 tion apart from the Wh-word as the string argument to find. Similar to §4.1, for each of the find modul"
2021.emnlp-main.466,2020.tacl-1.13,1,0.836504,"und, it is used as a paired example for the DROP question to enforce consistency between the find modules. For example, How many percentage points did the population of non-Hispanic Whites drop from 1990 to 2010? is paired with the generated question What percentage of the population was nonHispanic Whites in 2010?. 5 Experiments Dataset and Setup We perform experiments on the subset of the DROP dataset (Dua et al., 2019) that is covered by the modules in Text-NMN. This subset is a union of the data used by Gupta et al. (2020a) and the question decomposition annotations in the B REAK dataset (Wolfson et al., 2020). All questions in our dataset contain program annotations (heuristically annotated by Gupta et al. (2020a); crowd-sourced in B REAK). The program annotations only supervise the layouts of the modules, and not the intermediate outputs. We only use these programs for training; all test results are based on predicted programs. Our complete subset of DROP contains 23215 question-answer pairs. For an i.i.d. split, since the DROP test set is hidden, we split the training set into train/validation and use the provided validation set as the test set. Our train/validation/test sets contain 18299/2460/"
2021.emnlp-main.497,D13-1160,0,0.0619786,"t documents to account for set-valued retrieval of variable sizes, which is crucial for multidocument QA. RAG (Lewis et al., 2020) adopted a similar method as REALM, but for sequence generation tasks. RAG can be adapted to perform multi-hop question answering (Xiong et al., 2020), and we directly compared our results with multihop RAG on HotpotQA fullwiki in the experiments. Finally, our work leverages marginalization over latent variables to deal with weak and noisy supervision signals, which is reminiscent of using maximum marginal likelihood for training weakly supervised semantic parsers (Berant et al., 2013; Krishnamurthy et al., 2017, among others). 9 Conclusion We proposed a new probabilistic model for retrieving set-valued contexts for multi-document QA and show that training the QA model with marginalization over this set can help mitigate the false negatives in evidence annotations. Experiments on IIRC and HotpotQA fullwiki show that our proposed framework can learn to retrieve unlabeled alternative contexts and improves QA F1 by 5.5 on IIRC and 8.9 on HotpotQA. Acknowledgements The authors would like to thank Tushar Khot, Hannaneh Hajishirzi, James Ferguson, Dragomir Radev, and the anonymo"
2021.emnlp-main.497,P17-1171,0,0.0175188,"rocess. Such issue was also found by preour contributions include modeling document se- vious work on HotpotQA, as Xiong et al. (2020) lection as a sequence and condition selection based noted that many of the ""errors"" in their document on previous selected documents as done by Asai retrieval model are actually valid alternative conet al. (2020) and Xiong et al. (2020), and using texts. The false-negative contexts can also lead stronger language models (e.g., ELECTRA (Clark to false-negative annotations of answer spans. In et al., 2019)). Note that all three modules of our some previous work (Chen et al., 2017; Hu et al., framework are simple classification models on top 2019; Asai et al., 2020), it was shown effective to of BERT, and that the formulation of our proposed manually add distantly-supervised examples in the set-valued retrieval and marginalization are model- reasoning model’s training, while we try to solve agnostic. this problem from the retrieval part. 6156 IIRC E XAMPLE Question: Who won the famous duel that took place in Weehawken? Answer: Aaron Burr Evidence in initial paragraph: Hudson County, New Jersey: Weehawken became notorious for duels, including the nation’s most famous be"
2021.emnlp-main.497,2020.tacl-1.30,0,0.029872,"Missing"
2021.emnlp-main.497,N19-1423,0,0.0137269,"88.7 62.8 87.3 62.0 29.6 41.7 44.2 46.5 33.0 45.8 47.4 50.0 46.9 50.6 Test Retrieval Reasoning Doc-F1 Rt-Recall QA EM QA F1 - - 90.8 67.9 91.0 67.5 27.7 41.3 37.8 42.0 31.1 44.3 41.0 45.1 47.4 50.5 Table 1: Main results on IIRC. ""Baseline"" refers to the performance reported in Ferguson et al. (2020) and ""-"" denotes that no results are available. Work marked with † is by Yoran et al. (2021), which appeared after our initial submission. All pipeline models shares the same retrieval model and its output thus the same retrieval performance. we use NumNet+ (Ran et al., 2019) for IIRC and BERT-wwm (Devlin et al., 2019) for HotpotQA. Note that our general modeling framework is agnostic to the choices of specific models for retrieval and reasoning. We choose these models to experiment with since they are easy to use and present strong results as shown in previous work (Groeneveld et al., 2020). More implementation details can be found in Appendix A. 4.3 Evaluation Metrics For HotpotQA, we follow previous work (Yang et al., 2018; Asai et al., 2020; Xiong et al., 2020) and use F1 score and Exact Match (EM) for the answer (QA) and supporting facts (SP) prediction. Similarly, we report QA F1 and EM for IIRC as in"
2021.emnlp-main.497,2020.emnlp-main.710,0,0.186702,"lent information is marked in blue, and only the snippets with ""Ë"" are annotated as gold evidence. False negatives are outlined in red and both are retrieved by our proposed framework. that is suitable for the reasoning model to perform the end-task effectively. Recent advances in reading comprehension have resulted in models that Multi-document question answering refers to the have been shown to answer questions requiring task of answering questions that require reading complex reasoning types such as bridging, comparmultiple documents, extracting relevant facts, and ison (Asai et al., 2020; Fang et al., 2020) or even reasoning over them. Systems built for this task typically involve retrieval and reasoning compo- arithmetic (Ran et al., 2019; Gupta et al., 2020), given adequate context. However, when the connents that work in tandem. The retrieval component text needs to be retrieved from a large text corpus needs to extract information from the documents (e.g., Wikipedia), the performance of such reading ∗ Majority of the work done as an intern at AI2. 1 comprehension models is greatly affected by the Code available at https://github.com/ niansong1996/retrieval_marginalization. quality of the ret"
2021.emnlp-main.497,2020.emnlp-main.86,1,0.907814,"tions for academic papers), which can be used to constrain the space of retrieval. Inevitable False-negatives in Context Retrieval Annotations Even when supporting evidence is annotated, we claim that the learning signal provided by those labels may be weak and noisy when retrieving from a large corpus such as Wikipedia. This is due to the redundancy of information in such large corpora: it is common to have multiple sets of evidence snippets that can answer the same question, as in Figure 1. To quantify how often alternative contexts exist for the multi-document QA problem, we analyzed IIRC (Ferguson et al., 2020), an information seeking multi-document QA dataset. We sampled 50 answerable questions with their annotated gold context and manually checked if equivalent information can be found in sentences not labeled as supporting evidence, in the same document. We found that more than half of the questions have at least one alternative evidence, and on average 2 there is more than one sentence we can find in the same document that contains the We design a training procedure for handling these false negatives, as well as cases where retrieval should fail (i.e., when the question is unanswerable). Specifi"
2021.emnlp-main.497,2020.emnlp-main.711,0,0.01507,"in Ferguson et al. (2020) and ""-"" denotes that no results are available. Work marked with † is by Yoran et al. (2021), which appeared after our initial submission. All pipeline models shares the same retrieval model and its output thus the same retrieval performance. we use NumNet+ (Ran et al., 2019) for IIRC and BERT-wwm (Devlin et al., 2019) for HotpotQA. Note that our general modeling framework is agnostic to the choices of specific models for retrieval and reasoning. We choose these models to experiment with since they are easy to use and present strong results as shown in previous work (Groeneveld et al., 2020). More implementation details can be found in Appendix A. 4.3 Evaluation Metrics For HotpotQA, we follow previous work (Yang et al., 2018; Asai et al., 2020; Xiong et al., 2020) and use F1 score and Exact Match (EM) for the answer (QA) and supporting facts (SP) prediction. Similarly, we report QA F1 and EM for IIRC as in Ferguson et al. (2020). In addition, we define the following metrics for understanding the retrieval performance: (1) Document selection F1 (DocF1) measures the performance of the document retrieval model given the documents marked as gold; (2) Overall retrieval recall(Rt-Reca"
2021.emnlp-main.497,P19-1221,0,0.0431404,"Missing"
2021.emnlp-main.497,2020.emnlp-main.550,0,0.0257868,"Missing"
2021.emnlp-main.497,D17-1160,1,0.799605,"t for set-valued retrieval of variable sizes, which is crucial for multidocument QA. RAG (Lewis et al., 2020) adopted a similar method as REALM, but for sequence generation tasks. RAG can be adapted to perform multi-hop question answering (Xiong et al., 2020), and we directly compared our results with multihop RAG on HotpotQA fullwiki in the experiments. Finally, our work leverages marginalization over latent variables to deal with weak and noisy supervision signals, which is reminiscent of using maximum marginal likelihood for training weakly supervised semantic parsers (Berant et al., 2013; Krishnamurthy et al., 2017, among others). 9 Conclusion We proposed a new probabilistic model for retrieving set-valued contexts for multi-document QA and show that training the QA model with marginalization over this set can help mitigate the false negatives in evidence annotations. Experiments on IIRC and HotpotQA fullwiki show that our proposed framework can learn to retrieve unlabeled alternative contexts and improves QA F1 by 5.5 on IIRC and 8.9 on HotpotQA. Acknowledgements The authors would like to thank Tushar Khot, Hannaneh Hajishirzi, James Ferguson, Dragomir Radev, and the anonymous reviewers for the useful"
2021.emnlp-main.497,Q19-1026,0,0.0539435,"Missing"
2021.emnlp-main.497,P19-1612,0,0.0361448,"Missing"
2021.emnlp-main.497,D19-1258,0,0.0599335,"Missing"
2021.emnlp-main.497,D19-1251,0,0.260543,"d both are retrieved by our proposed framework. that is suitable for the reasoning model to perform the end-task effectively. Recent advances in reading comprehension have resulted in models that Multi-document question answering refers to the have been shown to answer questions requiring task of answering questions that require reading complex reasoning types such as bridging, comparmultiple documents, extracting relevant facts, and ison (Asai et al., 2020; Fang et al., 2020) or even reasoning over them. Systems built for this task typically involve retrieval and reasoning compo- arithmetic (Ran et al., 2019; Gupta et al., 2020), given adequate context. However, when the connents that work in tandem. The retrieval component text needs to be retrieved from a large text corpus needs to extract information from the documents (e.g., Wikipedia), the performance of such reading ∗ Majority of the work done as an intern at AI2. 1 comprehension models is greatly affected by the Code available at https://github.com/ niansong1996/retrieval_marginalization. quality of the retrieval model. Given supervision 6149 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process"
2021.emnlp-main.497,D18-1259,0,0.253501,"ld fail (i.e., when the question is unanswerable). Specifically, we assign probabilities to documents, evidence candidates, and potential answers with parameterized models, and marginalize over a set of potential contexts by combining top retrieved evidence from each document, allowing the model to score false negatives highly. To make the marginalization feasible, we decompose the retrieval problem into document selection and evidence retrieval and show how we can still model contexts as sets. We evaluate our model on two multi-document QA datasets: IIRC (Ferguson et al., 2020) and HotpotQA (Yang et al., 2018). We see 2 2.8 and 4.8 F1 point improvement on IIRC and HotWe count up to 5 alternative evidence snippets per docupotQA respectively by jointly modeling our pro- ment when measuring the average. Some documents contain many more alternative evidence snippets. An example would posed set-valued retrieval and the reasoning steps, be when a question seeks information on the ""winner of World and a further 2.7 and 4.1 F1 point improvement re- War I"" in relevant Wikipedia pages. 6150 same information as the gold evidence. Note that it is also possible to have alternative evidence in a different docume"
2021.emnlp-main.561,P19-1620,0,0.028017,"t necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors"
2021.emnlp-main.561,2020.acl-main.485,0,0.0226352,"Missing"
2021.emnlp-main.561,P18-1078,1,0.843692,"tor is less biased and less affected by conservative changes (Ben-David et al., 2010) to the data distribution. While the end to end QA performance of our model is comparable (↓ 0.3 F1) to Fang et al. (2020) on the original dev-set, on the adversarial set our method is better than Fang et al. (2020) (↑ 1.2 F1). Table 3 shows that the decoder of generative passage selector was able to generate multi-hop style questions from a pair of contexts. 3.2 Context pairs vs. Sentences Some context selection models for HotpotQA use a multi-label classifier that chooses top-k sentences (Fang et al., 2020; Clark and Gardner, 2018) which result in limited inter-document interaction than context pairs. To compare these two input types, we construct a multi-label sentence classifier p(s|q, C) that selects relevant sentences. This classifier projects a concatenated sentence and question representation, followed by a sigmoid, to predict if the sentence should be selected. This model has a 3.1 Adversarial Evaluation better performance over the context-pair selector but is more biased (Table 4). We use an existing adversarial set (Jiang and Bansal, 2019) for HotpotQA to test the robustness We performed similar experiments wit"
2021.emnlp-main.561,2020.acl-main.497,1,0.781523,"ocated in Richmond, Virginia. Dartmouth was founded in 1938 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but r"
2021.emnlp-main.561,D17-1090,0,0.0186536,"especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such"
2021.emnlp-main.561,2020.emnlp-main.580,0,0.0179387,"etting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is"
2021.emnlp-main.561,2020.emnlp-main.710,0,0.0268086,"eneration network gets contextual representations for context-pair candidates from the encoder and uses them to generate the question, via the decoder. The objective function increases the likelihood of the question for gold context pairs and the unlikelihood (Welleck et al., 2020) for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(θ) = |question| X log p(qt |q<t , cgold ) Model Original Adversarial Acc F1 Acc F1 Standard Selector Generative Selector 95.3 97.5 79.5 81.9 91.4 96.3 76.0 80.1 Tu et al. (2020) Fang et al. (2020) 94.5 - 80.2 82.2 - 61.1 78.9 Table 1: HotpotQA: Passage selection accuracy and end-to-end QA F1 on the original and adversarial set (Jiang and Bansal, 2019) of the HotpotQA dataset. The results of Tu et al. (2020) and Fang et al. (2020) are as reported by Perez et al. (2020). t=1 + X |question| X n∈|neg.pairs| t=1 Model log (1 − p(qt |q<t , cn )) Standard Selector Generative Selector Accuracy EM/F1 96.8 97.2 72.8/79.9 73.5/80.2 (5) 3 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA (Yang et al., 2018) and WikiHop (Welbl et al., 2018). We use a pre-trained T5"
2021.emnlp-main.561,D19-1107,0,0.0310612,"Missing"
2021.emnlp-main.561,N18-2017,0,0.0550553,"Missing"
2021.emnlp-main.561,D17-1215,0,0.0280852,"n (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the selected set of passages. These dataset are Consider an example from HotpotQA in Figoften collected via crowdsourcing, which"
2021.emnlp-main.561,P19-1262,0,0.248753,"-12 VCU Rams men’s basketball team, led by third year head coach Shaka Smart, represented the university which was founded in what year? Gold Answer: 1838 Passage 1: The 2011-12 VCU Rams men’s basketball team represented Virginia Commonwealth University during the 2011-12 NCAA Division I men’s basketball season... Passage 2: Virginia Commonwealth University (VCU) is a public research university located in Richmond, Virginia. VCU was founded in 1838 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... Prediction: 1838 Adversarial context from Jiang and Bansal (2019): Dartmouth University is a public research university located in Richmond, Virginia. Dartmouth was founded in 1938 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases,"
2021.emnlp-main.561,P17-1147,0,0.0243601,"entence selection helped improve performance of the discriminative passage selector, we add an auxiliary loss term to our generative passage selector that also predicts the relevant sentences in the context pair when generating the question (p(q, s|ci j , Ψ)), in a multi-task manner. We see slight performance improvements by using relevant sentences as an additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body o"
2021.emnlp-main.561,2020.emnlp-main.550,0,0.0202736,"n additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent"
2021.emnlp-main.561,W13-2114,0,0.0352709,"ried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors can be attributed to existing bias in Hot- ing substantially better"
2021.emnlp-main.561,2021.ccl-1.108,0,0.0352242,"Missing"
2021.emnlp-main.561,D19-1284,0,0.0962305,"a in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the s"
2021.emnlp-main.561,P19-1416,1,0.927103,"a in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the s"
2021.emnlp-main.561,2020.emnlp-main.134,1,0.718829,", 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context select"
2021.emnlp-main.561,2020.emnlp-main.713,0,0.0133695,", 2020) for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(θ) = |question| X log p(qt |q<t , cgold ) Model Original Adversarial Acc F1 Acc F1 Standard Selector Generative Selector 95.3 97.5 79.5 81.9 91.4 96.3 76.0 80.1 Tu et al. (2020) Fang et al. (2020) 94.5 - 80.2 82.2 - 61.1 78.9 Table 1: HotpotQA: Passage selection accuracy and end-to-end QA F1 on the original and adversarial set (Jiang and Bansal, 2019) of the HotpotQA dataset. The results of Tu et al. (2020) and Fang et al. (2020) are as reported by Perez et al. (2020). t=1 + X |question| X n∈|neg.pairs| t=1 Model log (1 − p(qt |q<t , cn )) Standard Selector Generative Selector Accuracy EM/F1 96.8 97.2 72.8/79.9 73.5/80.2 (5) 3 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA (Yang et al., 2018) and WikiHop (Welbl et al., 2018). We use a pre-trained T5 (Raffel et al., 2019) encoder-decoder model for obtaining contextual representations, which are further trained to estimate all individual probability distributions. The answering model is a fine-tuned T5-large model which has an oracle EM/F1, p(a |q, cgold ), of 74.5/83.5 a"
2021.emnlp-main.561,D18-1052,0,0.0229146,"ant sentences as an additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5"
2021.emnlp-main.561,P16-1056,0,0.0294284,"multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors can be attributed to e"
2021.emnlp-main.561,Q18-1021,0,0.145131,"showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the selected set of passages. These dataset are Consider an example from Ho"
2021.emnlp-main.561,D18-1259,0,0.125503,"ent of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way"
2021.emnlp-main.584,2020.acl-main.676,0,0.0271617,"but may not necessarily move to the positive pair, unlike CE. Second, because it assumes a conditional probabilistic model of p(a|q), it is not clear how to use alternative questions in the bundle. 3 Data Augmentation If the bundle B contains instances that were not present in the training data (e.g., the bundle could be generated using simple heuristics; see §3), the simplest use of the bundle is to add all instances to the training data and use MLE under the standard i.i.d. assumption. This is the standard approach to using this kind of data, and it has been done numerous times previously (Andreas, 2020; Zmigrod et al., 2019). This is not applicable if the bundle was obtained by mining the existing training instances, however. Bundling Heuristics In this section we discuss how we obtain instance bundles for use with contrastive estimation and other related baselines. A naive way to create a bundle would be to exploit the fact that all the questions associated with a context are likely to be related, and simply make bundles consisting of all QA pairs associated with the context. However, this approach poses two problems. First, there could be many questions associated with any particular cont"
2021.emnlp-main.584,2020.acl-main.499,0,0.0829922,"tion that the training instances sampled from some data distribution are independent and identically distributed. However, this assump- Figure 1: Example of contrastive QA pairs and effect of tion can cause the learner to ignore distinguish- different loss functions while training with such data. ing cues (Dietterich et al., 1997) between related or minimally different questions associated with a given context, resulting in inconsistent model preThis problem can be addressed by training moddictions (Jia and Liang, 2017; Ribeiro et al., 2019; els with sets of related question-answer (QA) pairs Asai and Hajishirzi, 2020). In cases like ROPES, simultaneously, instead of having a loss function where the dataset contains only minimally different that decomposes over independent examples. We questions, we see that the performance of a com- use the term instance bundle to refer to these sets of petitive model (RoBERTA) is close to random (Lin closely contrasting examples. Consider an instance et al., 2019). One potential reason for this poor per- bundle from HotpotQA in Figure 1a, containing formance is that the model considers each question two contrastive QA pairs, which differ in their inindependently, instead"
2021.emnlp-main.584,W11-1904,0,0.041989,"xamples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2018), reading comprehension (Asai and Hajishirzi, 2020; Gupta et al., 2021), and visual question answering (Teney et al., 2019, 2020; Jacovi et al., 2021). In open domain QA, re-ranking extracted answer spans from a baseline model has shown promising"
2021.emnlp-main.584,D19-1606,1,0.867074,"Missing"
2021.emnlp-main.584,2021.emnlp-main.466,1,0.838335,"Missing"
2021.emnlp-main.584,D17-1215,0,0.0169619,"both the contrastive questions). Machine learning models are typically trained with the assumption that the training instances sampled from some data distribution are independent and identically distributed. However, this assump- Figure 1: Example of contrastive QA pairs and effect of tion can cause the learner to ignore distinguish- different loss functions while training with such data. ing cues (Dietterich et al., 1997) between related or minimally different questions associated with a given context, resulting in inconsistent model preThis problem can be addressed by training moddictions (Jia and Liang, 2017; Ribeiro et al., 2019; els with sets of related question-answer (QA) pairs Asai and Hajishirzi, 2020). In cases like ROPES, simultaneously, instead of having a loss function where the dataset contains only minimally different that decomposes over independent examples. We questions, we see that the performance of a com- use the term instance bundle to refer to these sets of petitive model (RoBERTA) is close to random (Lin closely contrasting examples. Consider an instance et al., 2019). One potential reason for this poor per- bundle from HotpotQA in Figure 1a, containing formance is that the m"
2021.emnlp-main.584,2020.findings-emnlp.171,0,0.0426244,"Missing"
2021.emnlp-main.584,2020.emnlp-main.750,0,0.0422782,"elihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2018), reading comprehension (Asai and Hajishirzi, 2020; Gupta et al., 2021), and visual question answering (Teney et al., 2019, 2020; Jacovi et al., 2021). In open domain QA, re-ranking extracted answer spans from a baseline model has shown promising improvements and shares connections with our answer condi"
2021.emnlp-main.584,D19-5808,1,0.888862,"Missing"
2021.emnlp-main.584,2020.emnlp-main.245,1,0.887224,"Missing"
2021.emnlp-main.584,2021.ccl-1.108,0,0.0635493,"Missing"
2021.emnlp-main.584,K18-1007,0,0.0146524,"y (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2018), reading comprehension (Asai and Hajishirzi, 2020; Gupta et al., 2021), and visual question answering (Teney et al., 2019, 2020; Jacovi et al., 2021). In open domain QA, re-ranking extracted answer spans from a baseline model has shown promising improvements and shares connections with our answer conditional setup (Iyer et al., 2020). Instead of training just a ranking model (which is similar to answer conditional CE) on top of a baseline (MLE) model, we jointly train a single QA model with both objectives. This promotes better representation learning in the baseline QA model. 7 Conclusion We"
2021.emnlp-main.584,2020.acl-main.309,0,0.0153867,"ask. Manual analysis additionally found that most of the top predictions were ungrammatical variations of the top-1 answer, similar to (but more extreme than) what was seen on the full HotpotQA dataset. This could explain why the top-k bundling heuristic is not as effective in the case of Quoref as the other two datasets. More generally, these results indicate the importance of effective instance bundling heuristics, and future work could focus on identifying more general ways to create bundles. dialogue generation (Cai et al., 2020), word embeddings (Mikolov et al., 2013), language modeling (Noji and Takamura, 2020), etc., and computer vision tasks such as image captioning (Dai and Lin, 2017), unsupervised representation learning (Hadsell et al., 2006), etc. Similarly, mutual information minimization based learners in question answering (Yeh and Chen, 2019) and image classification (Hjelm et al., 2019) try to decrease the mutual information between positive and negative samples. Natural language applications often sample negative examples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often"
2021.emnlp-main.584,P19-1621,1,0.834447,"questions). Machine learning models are typically trained with the assumption that the training instances sampled from some data distribution are independent and identically distributed. However, this assump- Figure 1: Example of contrastive QA pairs and effect of tion can cause the learner to ignore distinguish- different loss functions while training with such data. ing cues (Dietterich et al., 1997) between related or minimally different questions associated with a given context, resulting in inconsistent model preThis problem can be addressed by training moddictions (Jia and Liang, 2017; Ribeiro et al., 2019; els with sets of related question-answer (QA) pairs Asai and Hajishirzi, 2020). In cases like ROPES, simultaneously, instead of having a loss function where the dataset contains only minimally different that decomposes over independent examples. We questions, we see that the performance of a com- use the term instance bundle to refer to these sets of petitive model (RoBERTA) is close to random (Lin closely contrasting examples. Consider an instance et al., 2019). One potential reason for this poor per- bundle from HotpotQA in Figure 1a, containing formance is that the model considers each qu"
2021.emnlp-main.584,2020.acl-main.442,1,0.83215,"on minimization based learners in question answering (Yeh and Chen, 2019) and image classification (Hjelm et al., 2019) try to decrease the mutual information between positive and negative samples. Natural language applications often sample negative examples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2"
2021.emnlp-main.584,2021.findings-acl.336,0,0.0335299,"Missing"
2021.emnlp-main.584,P05-1044,0,0.54318,"ings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7347–7357 c November 7–11, 2021. 2021 Association for Computational Linguistics the training set, traditional maximum likelihood estimation will incentivize the model to figure out the difference between the inputs that leads to the difference in the answers, but the instances are likely to be seen far apart from each other during training, giving only a weak and indirect signal about the relationship between the pair. To learn from these instance bundles more effectively, we draw on contrastive estimation (Smith and Eisner, 2005), a method for re-normalizing an unsupervised probabilistic model using a neighborhood of related examples (originally a set of perturbations of some observed text). We extend this technique to apply to supervised reading comprehension problems by carefully selecting appropriate “neighborhoods” from instance bundles. The simplest choice of neighborhood is the set of contrasting answers from the instance bundle, resulting in a method similar to unlikelihood training (Welleck et al., 2020) or noise-contrastive estimation (Gutmann and Hyvärinen, 2010). However, there are other choices, including"
2021.emnlp-main.584,D18-1259,0,0.0734253,"Missing"
2021.emnlp-main.584,D19-1333,0,0.0216622,"tic is not as effective in the case of Quoref as the other two datasets. More generally, these results indicate the importance of effective instance bundling heuristics, and future work could focus on identifying more general ways to create bundles. dialogue generation (Cai et al., 2020), word embeddings (Mikolov et al., 2013), language modeling (Noji and Takamura, 2020), etc., and computer vision tasks such as image captioning (Dai and Lin, 2017), unsupervised representation learning (Hadsell et al., 2006), etc. Similarly, mutual information minimization based learners in question answering (Yeh and Chen, 2019) and image classification (Hjelm et al., 2019) try to decrease the mutual information between positive and negative samples. Natural language applications often sample negative examples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is comple"
2021.emnlp-main.584,P19-1161,0,0.012763,"cessarily move to the positive pair, unlike CE. Second, because it assumes a conditional probabilistic model of p(a|q), it is not clear how to use alternative questions in the bundle. 3 Data Augmentation If the bundle B contains instances that were not present in the training data (e.g., the bundle could be generated using simple heuristics; see §3), the simplest use of the bundle is to add all instances to the training data and use MLE under the standard i.i.d. assumption. This is the standard approach to using this kind of data, and it has been done numerous times previously (Andreas, 2020; Zmigrod et al., 2019). This is not applicable if the bundle was obtained by mining the existing training instances, however. Bundling Heuristics In this section we discuss how we obtain instance bundles for use with contrastive estimation and other related baselines. A naive way to create a bundle would be to exploit the fact that all the questions associated with a context are likely to be related, and simply make bundles consisting of all QA pairs associated with the context. However, this approach poses two problems. First, there could be many questions associated with any particular context, and smaller, more"
2021.emnlp-main.774,2021.naacl-main.225,0,0.051292,"Missing"
2021.emnlp-main.98,J90-1003,0,0.214262,"Missing"
2021.emnlp-main.98,I05-5002,0,0.26697,"Missing"
2021.emnlp-main.98,2020.emnlp-main.480,0,0.0393938,"ons; Wang et al., 2012), and Two filters applied are (i) a similarity filter to documents who is saying certain words influences its offensive- from other corpora, and (ii) deduplication. 1293 ten missing (Bender et al., 2021; Paullada et al., 2020). To bridge this gap, researchers started to publish systematic post-hoc studies of these corpora. Gehman et al. (2020) provide an in-depth analysis with respect to toxicity and fake news of O PEN W EB T EXT. Caswell et al. (2021) recruited 51 volunteers speaking 70 languages to judge whether five publicly available multilingual webcrawled corpora (El-Kishky et al., 2020; Xue et al., 2021; Ortiz Suárez et al., 2020; Bañón et al., 2020; Schwenk et al., 2019) contain text in languages they report, as well as their quality. Jo and Gebru (2020) discuss parallels between creating historical archives and the curation of machine learning datasets including pretraining corpora. Hutchinson et al. (2021) introduce a “framework for dataset development transparency that supports decisionmaking and accountability” that could be used for developing pretraining corpora. The Masakhane organization advocates for participatory research (Nekoto et al., 2020), a set of methodolo"
2021.emnlp-main.98,2021.acl-long.295,0,0.0329535,"Missing"
2021.emnlp-main.98,2020.findings-emnlp.301,1,0.934501,"ocumentation when creating web-crawled corpora. On the right, we include some example of types of documentation that we provide for the C4. EN dataset. characteristics (Bender and Friedman, 2018; Gebru et al., 2018; Hutchinson et al., 2021). However, given the challenges of applying these practices to massive collections of unlabeled text scraped from the web, thorough documentation is typically not done. This leaves consumers of pretrained language models in the dark about the influences of pretraining data on their systems, which can inject subtle biases in downstream uses (Li et al., 2020; Gehman et al., 2020; Groenwold et al., 2020). In this work we provide some of the first documentation of a web-scale dataset: the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020). C4 is one of the largest language datasets available, with more than 156 billion tokens collected from more than 365 million domains across the internet (Table 1).1 C4 has been used to train models such as T5 and the Switch Transformer (Fedus et al., 2021), two of the largest pretrained English language models. While Raffel et al. (2020) provided scripts to recreate C4, simply running the available scripts costs thousands of dol"
2021.emnlp-main.98,W07-1401,0,0.189334,"Missing"
2021.emnlp-main.98,2020.emnlp-main.473,0,0.0417614,"ating web-crawled corpora. On the right, we include some example of types of documentation that we provide for the C4. EN dataset. characteristics (Bender and Friedman, 2018; Gebru et al., 2018; Hutchinson et al., 2021). However, given the challenges of applying these practices to massive collections of unlabeled text scraped from the web, thorough documentation is typically not done. This leaves consumers of pretrained language models in the dark about the influences of pretraining data on their systems, which can inject subtle biases in downstream uses (Li et al., 2020; Gehman et al., 2020; Groenwold et al., 2020). In this work we provide some of the first documentation of a web-scale dataset: the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020). C4 is one of the largest language datasets available, with more than 156 billion tokens collected from more than 365 million domains across the internet (Table 1).1 C4 has been used to train models such as T5 and the Switch Transformer (Fedus et al., 2021), two of the largest pretrained English language models. While Raffel et al. (2020) provided scripts to recreate C4, simply running the available scripts costs thousands of dollars. Reproducible scienc"
2021.emnlp-main.98,2020.acl-main.740,1,0.884087,"Missing"
2021.emnlp-main.98,L16-1146,0,0.0647215,"Missing"
2021.emnlp-main.98,D16-1057,0,0.0550345,"Missing"
2021.emnlp-main.98,2020.findings-emnlp.171,0,0.0156661,").1 C4 has been used to train models such as T5 and the Switch Transformer (Fedus et al., 2021), two of the largest pretrained English language models. While Raffel et al. (2020) provided scripts to recreate C4, simply running the available scripts costs thousands of dollars. Reproducible science is only possible when data is broadly acModels pretrained on unlabeled text corpora are the backbone of many modern NLP systems (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020, inter alia). This paradigm incentivizes the use of ever larger corpora (Kaplan et al., 2020; Henighan et al., 2020), with the biggest models now training on a substantial fraction of the publicly-available internet (Raffel et al., 2020; Brown et al., 2020). Of course, as with all machine learning systems, the data such models are trained on has a large impact on their behavior. For structured, task-specific NLP datasets, best prac1 tices have emerged around documenting the collecOther, similar datasets have been created (e.g., Brown tion process, composition, intended uses, and other et al., 2020), but unfortunately were not made available. 1286 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.98,K16-1028,0,0.0531454,"Missing"
2021.emnlp-main.98,W12-1603,0,0.0155433,"oaded from 41 monthly “snapshots” from 2016– that goes beyond detecting “bad” words; hateful 2019, and it constitutes 45TB of compressed text and lewd content can be expressed without negative keywords (e.g., microaggressions, innuendos; Bre- before filtering23 and 570GB after (∼400 billion itfeller et al., 2019; Dinan et al., 2019). Importantly, byte-pair-encoded tokens). Since analyzing pretraining corpora is challengthe meaning of seemingly “bad” words heavily deing due to their size, their documentation is ofpends on the social context (e.g., impoliteness can 23 serve prosocial functions; Wang et al., 2012), and Two filters applied are (i) a similarity filter to documents who is saying certain words influences its offensive- from other corpora, and (ii) deduplication. 1293 ten missing (Bender et al., 2021; Paullada et al., 2020). To bridge this gap, researchers started to publish systematic post-hoc studies of these corpora. Gehman et al. (2020) provide an in-depth analysis with respect to toxicity and fake news of O PEN W EB T EXT. Caswell et al. (2021) recruited 51 volunteers speaking 70 languages to judge whether five publicly available multilingual webcrawled corpora (El-Kishky et al., 2020;"
2021.emnlp-main.98,Q19-1040,0,0.0482229,"Missing"
2021.emnlp-main.98,2021.naacl-main.41,0,0.018846,"and Two filters applied are (i) a similarity filter to documents who is saying certain words influences its offensive- from other corpora, and (ii) deduplication. 1293 ten missing (Bender et al., 2021; Paullada et al., 2020). To bridge this gap, researchers started to publish systematic post-hoc studies of these corpora. Gehman et al. (2020) provide an in-depth analysis with respect to toxicity and fake news of O PEN W EB T EXT. Caswell et al. (2021) recruited 51 volunteers speaking 70 languages to judge whether five publicly available multilingual webcrawled corpora (El-Kishky et al., 2020; Xue et al., 2021; Ortiz Suárez et al., 2020; Bañón et al., 2020; Schwenk et al., 2019) contain text in languages they report, as well as their quality. Jo and Gebru (2020) discuss parallels between creating historical archives and the curation of machine learning datasets including pretraining corpora. Hutchinson et al. (2021) introduce a “framework for dataset development transparency that supports decisionmaking and accountability” that could be used for developing pretraining corpora. The Masakhane organization advocates for participatory research (Nekoto et al., 2020), a set of methodologies that includes"
2021.naacl-main.365,2020.findings-emnlp.66,0,0.0659748,"Missing"
2021.naacl-main.365,D18-1241,0,0.0207644,"dHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), QuAC (Choi et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020), and IIRC (Ferguson et al., 2020). Unlike Q ASPER, Natural Questions and TyDiQA12 12 TyDiQA uses short snippets to prime annotators to write questions of interest, but the annotation process does not requestions are not grounded in any contexts, and the associated documents are linked to the questions after they are written. In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after reading the title and the abstract. The priming lets the r"
2021.naacl-main.365,2020.tacl-1.30,0,0.0543757,"This setup results in questions requiring more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger quesMachines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human’s need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; Clark et al., 2020) have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; Liu et al., 2020). However, most 1 Loosely derived from Question Answering over Scienof those benchmarks focus on an “open domain” tific Research Papers. The dataset, baseline code, and other setting where the questions are not anchored in any information about the project can be found at https:// particular user context. The result is an emphasis allenai.org/project/qasper. 4599 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Huma"
2021.naacl-main.365,D19-1606,1,0.874501,"Missing"
2021.naacl-main.365,N19-1423,0,0.0740128,"Missing"
2021.naacl-main.365,N19-1246,1,0.878072,"Missing"
2021.naacl-main.365,2020.emnlp-main.86,1,0.823881,"ey include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), QuAC (Choi et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020), and IIRC (Ferguson et al., 2020). Unlike Q ASPER, Natural Questions and TyDiQA12 12 TyDiQA uses short snippets to prime annotators to write questions of interest, but the annotation process does not requestions are not grounded in any contexts, and the associated documents are linked to the questions after they are written. In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after reading the title and the abstract. The priming lets the readers ask detailed questions that are specific to the papers in context, those that require a deeper underst"
2021.naacl-main.365,P18-1031,0,0.0684602,"Missing"
2021.naacl-main.365,D19-1259,0,0.0256337,"In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after reading the title and the abstract. The priming lets the readers ask detailed questions that are specific to the papers in context, those that require a deeper understanding of the contexts, like those shown in Figure 1 and Table 1. QuAC used similar data collection method but with focus on entities, which Q ASPER does not impose. Domain-Specific Information-seeking QA Some work has been done on information-seeking QA on academic research papers. PubmedQA (Jin et al., 2019) derives Yes/No/Maybe questions from PubMed paper titles answered from the conclusion sections of the corresponding abstracts. BioAsq benchmarks (Balikas et al., 2013; Nentidis et al., 2018; Krallinger et al., 2020) focus on open-domain QA over PubMed abstracts. Like Q ASPER, BioAsq answers can take different forms (e.g., yes/no, extracted span(s)). Q ASPER differs from BioAsq in that questions are grounded in a single paper of interest. Furthermore, Q ASPER uses the paper full text, not just the abstract. To the best of our knowledge, Q ASPER is the first information-seeking QA dataset in a c"
2021.naacl-main.365,P17-1147,0,0.0202731,"4: Model performance on the Q ASPER test set on answering questions given gold evidence. We do not show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant"
2021.naacl-main.365,2020.findings-emnlp.171,0,0.0591944,"ned Transformer (Vaswani et al., 2017) models which currently produce state-of-the-art results on a majority of QA tasks.9 Recall that Q ASPER introduces two main modeling challenges – different answer types and long input documents. First, Q ASPER includes a variety of answer types, including extractive, abstractive, yes/no, and unanswerable questions, which means a typical span-selection BERT-based QA model (Devlin et al., 2019) is not sufficient to support all these answer types. We address this by converting all answer types into a single task: generating answer text (Raffel et al., 2020; Khashabi et al., 2020).10 This is a sequence-to-sequence formulation that requires an encoder-decoder Transformer model where the encoder reads the question and the document and the decoder generates the answer text. Second, research papers are much longer than the typical 512 or 1024 token limit of most BERTlike models, so we need a Transformer model that can process long inputs. We use the LongformerEncoder-Decoder (LED; Beltagy et al., 2020), an encoder-decoder Transformer model that can efficiently process input sequences thousands of tokens long. With LED’s support for input sequence length of 16K tokens, we c"
2021.naacl-main.365,Q18-1023,0,0.0604265,"Missing"
2021.naacl-main.365,Q19-1026,0,0.0850267,"required to arrive at it. This setup results in questions requiring more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger quesMachines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human’s need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; Clark et al., 2020) have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; Liu et al., 2020). However, most 1 Loosely derived from Question Answering over Scienof those benchmarks focus on an “open domain” tific Research Papers. The dataset, baseline code, and other setting where the questions are not anchored in any information about the project can be found at https:// particular user context. The result is an emphasis allenai.org/project/qasper. 4599 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computatio"
2021.naacl-main.365,2020.acl-main.703,0,0.024624,"d dure estimates the human performance to be 60.9 on the type uses a different head to predict the corresponding Answer-F1 , and 71.6 Evidence-F1 . Note that given answer. This model performed much worse than the proposed the disagreements among the workers estimated seq2seq formulation. 4603 of Longformer. This allows each token to attend to only its local window and a pre-specified set of global locations of interest, thereby scaling self-attention computation linearly with the input size (as opposed to quadratically with full context self-attention). LED has a similar architecture to BART (Lewis et al., 2020) in terms of number of layers and hidden state sizes, with the distinction that it has a larger position embeddings matrix, allowing it to process inputs of up to 16K tokens long (up from 1K tokens in the original BART model). In practice, LED’s parameters are initialized from a pretrained BART model, and LED copies BART’s position embeddings 16 times to fill the entire 16K position embeddings matrix. For all experiments we use the LED-base sized model, which uses BART-base weights. Input and Output Encoding For the input, we follow the Longformer QA models (Beltagy et al., 2020) and encode th"
2021.naacl-main.365,2020.acl-main.604,0,0.0132563,"racts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger quesMachines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human’s need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; Clark et al., 2020) have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; Liu et al., 2020). However, most 1 Loosely derived from Question Answering over Scienof those benchmarks focus on an “open domain” tific Research Papers. The dataset, baseline code, and other setting where the questions are not anchored in any information about the project can be found at https:// particular user context. The result is an emphasis allenai.org/project/qasper. 4599 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599–4610 June 6–11, 2021. ©2021 Association for Computational Linguistics tions by"
2021.naacl-main.365,2020.acl-main.447,1,0.716329,"nsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful. 2 Building the Q ASPER Dataset We now describe our process for constructing the dataset. We began with a set of open-access NLP papers, recruited NLP practitioners who are regular readers of research papers, and designed two different data collection interfaces: one for collecting follow-up questions given titles and abstracts, and another for obtaining evidence and answers to those questions. 2.1 Papers We filtered S2ORC (Lo et al., 2020),2 a collection of machine-readable full text for open-access pa2 We accessed both release versions 20190928 and 20200705v1. pers, to (i) those from arXiv with an associated LaTeX source file,3 and (ii) are in the computational linguistics domain.4 We limited our domain to computational linguistics to ensure high quality as we have access to realistic users through our research network; broader domain collection is left to future work and should be enabled by the proof-of-concept of our protocols given in this paper. We used the S2ORC parser (which normalizes multi-file LaTeX sources and resol"
2021.naacl-main.365,2020.emnlp-demos.17,1,0.853597,"Missing"
2021.naacl-main.365,D18-1258,0,0.0200319,"or evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQ"
2021.naacl-main.365,L18-1439,0,0.0218284,"nderstanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsM"
2021.naacl-main.365,2020.bionlp-1.15,0,0.0158903,"achines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), Qu"
2021.naacl-main.365,D16-1264,0,0.0237654,"9.11 28.92 44.96 60.03 61.39 Table 4: Model performance on the Q ASPER test set on answering questions given gold evidence. We do not show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While thes"
2021.naacl-main.365,D19-1500,0,0.0258485,"Missing"
2021.naacl-main.365,Q19-1016,0,0.0206369,"o and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et"
2021.naacl-main.365,W17-2623,0,0.0147692,"ioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), QuAC (Choi et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020), and IIRC (Ferguson et al., 2020). Unlike Q ASPER, Natural Questions and TyDiQA12 12 TyDiQA uses short snippets to prime annotators to write questions of interest, but the annotation process does not requestions are not grounded in any contexts, and the associated documents are linked to the questions after they are written. In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after r"
2021.naacl-main.365,Q18-1021,0,0.0206324,"given gold evidence. We do not show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions"
2021.naacl-main.365,D15-1237,0,0.0322932,"Missing"
2021.naacl-main.365,D18-1259,0,0.0200587,"show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-wor"
D13-1080,C92-2082,0,0.0751417,"na¨ıve addition of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to lear"
D13-1080,P03-1009,0,0.0130843,"els are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach"
D13-1080,D11-1049,1,0.790658,"need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., hplaysSport, sportOfTournamenti, to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augm"
D13-1080,D12-1093,0,0.818427,"represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., hplaysSport, sportOfTournamenti, to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen833 Proceedings of the 2013 Conference on"
D13-1080,P02-1006,0,0.0359393,"ificant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et"
D13-1080,N13-1008,0,0.0279984,"ns the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013). 3 3.1 Method Path Ranking Algorithm (PRA) In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012)"
D13-1080,P06-1101,0,0.0241815,"LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to"
D13-1080,D11-1132,0,0.0318212,"eature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labe"
D14-1044,D13-1080,1,0.723034,"a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base relations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical inference. One major deficiency of random walk inference is the connectivity of the knowledge base graph— if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA (Lao et al., 2012; Gardner et al., 2013). This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and different relations have distinct meanings, this is not Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment"
D14-1044,D12-1093,0,0.080969,"ed as features in a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base relations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical inference. One major deficiency of random walk inference is the connectivity of the knowledge base graph— if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA (Lao et al., 2012; Gardner et al., 2013). This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and different relations have distinct meanings, this is not Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such la"
D14-1044,D12-1048,0,0.00909139,"rity between the two edge types. This lets us combine notions of distributional similarity with symbolic logical inference, with the result of decreasing the sparsity of the feature space considered by PRA. We show with experiments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference. 2 To create a graph from a corpus, we first preprocess the corpus to obtain a collection of surface relations, such as those extracted by open information extraction systems like OLLIE (Mausam et al., 2012). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by Talukdar et al. (2012), a dependency path, as done by Riedel et al. (2013), or OpenIE relations (Mausam et al., 2012)). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already represented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes corresponding to noun phrase types and edges corresponding to surfac"
D14-1044,mendes-etal-2012-dbpedia,0,0.0233272,"Missing"
D14-1044,W14-1609,0,0.00593772,"rge graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our experiments. steps of PRA, and spikiness and restart pa"
D14-1044,N13-1008,0,0.145809,"ments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference. 2 To create a graph from a corpus, we first preprocess the corpus to obtain a collection of surface relations, such as those extracted by open information extraction systems like OLLIE (Mausam et al., 2012). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by Talukdar et al. (2012), a dependency path, as done by Riedel et al. (2013), or OpenIE relations (Mausam et al., 2012)). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already represented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes corresponding to noun phrase types and edges corresponding to surface relation triples. So far these two subgraphs we have created are entirely disconnected, with the KB graph containing nodes representing entities, and the surface relation graph containing nodes representing noun phrases, wit"
D14-1044,P13-1045,0,0.00644971,"yet appear to be scalable enough to handle the large graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our exper"
D14-1044,D13-1170,0,0.0021682,"yet appear to be scalable enough to handle the large graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our exper"
D15-1127,C08-1007,0,0.0669954,"Missing"
D15-1127,P14-5004,0,0.0226708,"Missing"
D15-1127,E14-1049,0,0.171658,"Missing"
D15-1127,N13-1092,0,0.0184137,"Missing"
D15-1127,P15-1119,0,0.0481052,"Missing"
D15-1173,D13-1160,0,0.0100185,"nique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 1 Introduction Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link p"
D15-1173,D14-1165,0,0.0251102,"e KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB completion. There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships. Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011). These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al., 2014; Wang et al., 2014). These methods perform well when there is structural redundancy in the knowledge base tensor, but when the tensor (or individual relations in the tensor) has high rank, learning good embeddings can be challenging. The ARE model (Nickel et al., 2014) attempted to address this by only making the embeddings capture the residual of the tensor that cannot be readily predicted from the graph-based techniques mentioned below. 1490 Dataset Freebase NELL Method Probabilities Binarized Probabilities Binarized MAP .337 .344 .303 .319 Table 1: Using binary"
D15-1173,P15-1127,0,0.0235711,"on Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the f"
D15-1173,D13-1080,1,0.739227,"one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix). This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix. In this paper we consider whether this computational effort is wellspent, or whether we might mor"
D15-1173,P11-1055,0,0.196105,"res than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 1 Introduction Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm ("
D15-1173,D12-1069,1,0.625098,"Missing"
D15-1173,D11-1049,1,0.848931,"hnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between n"
D15-1173,D12-1093,0,0.0644912,"is work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix). This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix. In this paper we consider whether this computational effort is wellspent, o"
D15-1173,mendes-etal-2012-dbpedia,0,0.018079,"ll subgraph feature extraction (SFE). In addition to being conceptually simpler than PRA, SFE is much more efficient, reducing computation by an order of magnitude, and more expressive, allowing for much richer features than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 1 Introduction Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by l"
D15-1173,P09-1113,0,0.0627354,"knowledge base completion have the same goal: to predict new instances of relations in a formal knowledge base such as Freebase or NELL. The difference is that relation extraction focuses on determining what relationship is expressed by a particular sentence, while knowledge base completion tries to predict which relationships hold between which entities. A relation extraction system can be used for knowledge base completion, but typical KB completion methods do not make predictions on single sentences. This is easily seen in the line of work known as distantly-supervised relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riede"
D15-1173,D14-1044,1,0.641963,"ming knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix). This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix. In this paper we consider whether this computational effort is wellspent, or whether we might more profitably spend comp"
D15-1173,P15-1016,0,0.445107,"nto this category, as does ProPPR (Wang et al., 2013) and many other logic-based systems. PRA, the main subject of this paper, also fits in this line of work. Work specifically with PRA has ranged from incorporating a parsed corpus as additional evidence when doing random walk inference (Lao et al., 2012), to introducing better representations of the text corpus (Gardner et al., 2013; Gardner et al., 2014), and using PRA in a broader context as part of Google’s Knowledge Vault (Dong et al., 2014). An interesting piece of work that combines embedding methods with graph-based methods is that of Neelakantan et al. (2015), which uses a recursive neural network to create embedded representations of PRA-style paths. 4 Motivation We motivate our modifications to PRA with three observations. First, it appears that binarizing the feature matrix produced by PRA, removing most of the information gained in PRA’s second step, has no significant impact on prediction performance in knowledge base completion tasks. We show this on the NELL KB and the Freebase KB in Table 1.3 The fact that random walk probabilities carry no additional information for this task over binary features is surprising, and it shows that the secon"
D15-1173,N13-1008,0,0.0603883,"2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB completion. There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships. Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011). These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al., 2014; Wang et al., 2"
D15-1173,D12-1042,0,0.0113149,"l: to predict new instances of relations in a formal knowledge base such as Freebase or NELL. The difference is that relation extraction focuses on determining what relationship is expressed by a particular sentence, while knowledge base completion tries to predict which relationships hold between which entities. A relation extraction system can be used for knowledge base completion, but typical KB completion methods do not make predictions on single sentences. This is easily seen in the line of work known as distantly-supervised relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB compl"
D15-1173,D13-1136,0,0.0858641,"action (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB completion. There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships. Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011). These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al."
D17-1160,P13-2009,0,0.0183022,"for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers (Jia and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approach has the advantage of predicting the logical form directly from the question without latent variables, which simplifies learning, but the disadvantage of ignoring type constraints on logical forms. Our type-constrained neural semantic parser inherits the advantages of both approaches: it only generates well-typed logical forms and has no syntactic latent variables as every logical form has a unique derivation. Recent work has explored s"
D17-1160,Q13-1005,0,0.0705048,"nstrate the importance of both type constraints and entity linking to achieving high accuracy on this task. 2 Related Work Semantic parsers vary along a few important dimensions: Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial Grammar (Zettlemoyer 1516 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers (Jia and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; L"
D17-1160,D13-1160,0,0.938783,"nking to achieving high accuracy on this task. 2 Related Work Semantic parsers vary along a few important dimensions: Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial Grammar (Zettlemoyer 1516 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers (Jia and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approach has the advantage o"
D17-1160,P13-1042,0,0.0313486,"d efficiently and are more effective (Yih et al., 2016). However, a key advantage of question-answer pairs is that they are agnostic to the domain representation and logical form Data Sets We use W IKI TABLE Q UESTIONS to evaluate our parser as this data set exhibits both a broad domain and complex questions. Early data sets, such as G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994), have small domains with only a handful of different predicates. More recent data sets for question answering against Freebase have a much broader domain, but simple questions (Berant et al., 2013; Cai and Yates, 2013). 3 Model This section describes our semantic parsing model and training procedure. For clarity, we describe the model on W IKI TABLE Q UESTIONS, though it is also applicable to other problems. The input to our model is a natural language question and a context in which it is to be answered, which in our task is a table. The model predicts the answer to the question by semantically parsing it to a logical form then executing it against the table. We train the parser using question-answer pairs as supervision using an approximation of marginal loglikelihood based on enumerating logical forms vi"
D17-1160,H94-1010,0,0.0677527,"questionanswer pairs (Liang et al., 2011; Berant et al., 2013). Question-answer pairs were considered easier to obtain than labeled logical forms, though recent work has demonstrated that logical forms can be collected efficiently and are more effective (Yih et al., 2016). However, a key advantage of question-answer pairs is that they are agnostic to the domain representation and logical form Data Sets We use W IKI TABLE Q UESTIONS to evaluate our parser as this data set exhibits both a broad domain and complex questions. Early data sets, such as G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994), have small domains with only a handful of different predicates. More recent data sets for question answering against Freebase have a much broader domain, but simple questions (Berant et al., 2013; Cai and Yates, 2013). 3 Model This section describes our semantic parsing model and training procedure. For clarity, we describe the model on W IKI TABLE Q UESTIONS, though it is also applicable to other problems. The input to our model is a natural language question and a context in which it is to be answered, which in our task is a table. The model predicts the answer to the question by semantica"
D17-1160,P16-1004,0,0.527941,"nts to incorporate in neural semantic parsers. 1 Introduction Semantic parsing is the problem of translating human language into computer language, and therefore is at the heart of natural language understanding. A typical semantic parsing task is question answering against a database, which is accomplished by translating questions into executable logical forms (i.e., programs) that output their answers. Recent work has shown that recurrent neural networks can be used for semantic parsing by encoding the question then predicting each token of the logical form in sequence (Jia and Liang, 2016; Dong and Lapata, 2016). These approaches, while effective, have two major limitations. First, they treat the logical form as an unstructured sequence, thereby ignoring type constraints on wellformed programs. Second, they do not address entity linking, which is a critical subproblem of semantic parsing (Yih et al., 2015). This paper introduces a novel neural semantic parsing model that addresses these limitations of prior work. Our parser uses an encoder-decoder architecture with two key innovations. First, the decoder generates from a grammar that guarantees that generated logical forms are well-typed. This gramma"
D17-1160,P17-1097,0,0.153644,"correct, then backpropagating through a term for each. The wide beam is required to find correct logical forms; however, such a wide beam is prohibitively expensive with a neural model due to the cost of each backpropagation pass. Another approach is to train the network with REINFORCE (Williams, 1992), which essentially samples a logical form instead of using beam search. This approach is known to be difficult to apply when the space of outputs is large and the reward signal is sparse, and recent work has found that maximizing marginal loglikelihood is more effective in these circumstances (Guu et al., 2017). Our approach makes it tractable to maximize marginal loglikelihood with a neural model by using DPD to enumerate correct logical forms beforehand. This up-front enumeration, combined with the local normalization of the neural model, makes it possible to restrict the beam search to correct logical forms in the gradient computation, which enables training with a small beam size. 1521 4 Evaluation We evaluate our parser on the W IKI TABLE Q UES TIONS data set by comparing it to prior work and ablating several components to understand their contributions. 4.1 Experimental Setup We used the stand"
D17-1160,P16-1002,0,0.648563,"are valuable components to incorporate in neural semantic parsers. 1 Introduction Semantic parsing is the problem of translating human language into computer language, and therefore is at the heart of natural language understanding. A typical semantic parsing task is question answering against a database, which is accomplished by translating questions into executable logical forms (i.e., programs) that output their answers. Recent work has shown that recurrent neural networks can be used for semantic parsing by encoding the question then predicting each token of the logical form in sequence (Jia and Liang, 2016; Dong and Lapata, 2016). These approaches, while effective, have two major limitations. First, they treat the logical form as an unstructured sequence, thereby ignoring type constraints on wellformed programs. Second, they do not address entity linking, which is a critical subproblem of semantic parsing (Yih et al., 2015). This paper introduces a novel neural semantic parsing model that addresses these limitations of prior work. Our parser uses an encoder-decoder architecture with two key innovations. First, the decoder generates from a grammar that guarantees that generated logical forms are"
D17-1160,D12-1069,1,0.674961,"Missing"
D17-1160,D13-1161,0,0.0765204,"Missing"
D17-1160,D11-1140,0,0.510642,"al., 2017). We further perform several ablation studies that demonstrate the importance of both type constraints and entity linking to achieving high accuracy on this task. 2 Related Work Semantic parsers vary along a few important dimensions: Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial Grammar (Zettlemoyer 1516 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers"
D17-1160,P11-1060,0,0.706188,"raints and entity linking to achieving high accuracy on this task. 2 Related Work Semantic parsers vary along a few important dimensions: Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial Grammar (Zettlemoyer 1516 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers (Jia and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approac"
D17-1160,P16-1057,0,0.0812455,"Missing"
D17-1160,D16-1197,0,0.0621319,"Missing"
D17-1160,P15-1142,0,0.435593,"er great britain 2002 kim yu-na south korea 2014 ... LSTM Attention Attention LSTM LSTM g0 g1 to decoder Knowledge graph Embedding Question Entity Linking ... ... from encoder 2010 Logical ((reverse athlete) (and (nation south_korea) Form (year (num2cell (&gt;= 2010))))) Which athlete was from South Korea after the year 2010? Figure 1: Overview of our semantic parsing model. The encoder performs entity embedding and linking before encoding the question with a bidirectional LSTM. The decoder predicts a sequence of grammar rules that generate a well-typed logical form. 3.1 Preliminaries We follow (Pasupat and Liang, 2015) in using the same table structure representation and λ-DCS language for expressing logical forms. In this representation, tables are expressed as knowledge graphs over 6 types of entities: cells, cell parts, rows, columns, numbers and dates. Each entity also has a name, which is typically a string value in the table. Our parser uses both the entity names and the knowledge graph structure to construct embeddings for each entity. The logical form language consists of a collection of named sets and entities, along with operators on them. The named sets are used to select table cells, e.g., unite"
D17-1160,P16-1003,0,0.588258,"emantic parsing model and training procedure. For clarity, we describe the model on W IKI TABLE Q UESTIONS, though it is also applicable to other problems. The input to our model is a natural language question and a context in which it is to be answered, which in our task is a table. The model predicts the answer to the question by semantically parsing it to a logical form then executing it against the table. We train the parser using question-answer pairs as supervision using an approximation of marginal loglikelihood based on enumerating logical forms via dynamic programming on denotations (Pasupat and Liang, 2016) (Section 3.4). This approximation makes it possible to train neural models with question-answer supervision, which is otherwise difficult for efficiency and gradient variance reasons. 1517 Encoder Table Knowledge Graph athlete nation on o1 LSTM LSTM Entity Linking Entity Linking which athlete year Predicted Grammar c Rules on Decoder c → &lt;r,c&gt; to decoder karl schafer great britain 2002 kim yu-na south korea 2014 ... LSTM Attention Attention LSTM LSTM g0 g1 to decoder Knowledge graph Embedding Question Entity Linking ... ... from encoder 2010 Logical ((reverse athlete) (and (nation south_korea"
D17-1160,P17-1105,0,0.566333,"and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approach has the advantage of predicting the logical form directly from the question without latent variables, which simplifies learning, but the disadvantage of ignoring type constraints on logical forms. Our type-constrained neural semantic parser inherits the advantages of both approaches: it only generates well-typed logical forms and has no syntactic latent variables as every logical form has a unique derivation. Recent work has explored similar ideas to ours in the context of Python code generation (Yin and Neubig, 2017; Rabinovich et al., 2017). language (e.g., lambda calculus or λ-DCS). This property is important for problems such as semistructured tables where the proper domain representation is unclear. Entity Linking Identifying the entities mentioned in a question is a critical subproblem of semantic parsing in broad domains and proper entity linking can lead to large accuracy improvements (Yih et al., 2015). However, semantic parsers have typically ignored this problem by assuming that entity linking is done beforehand (as the neural parsers above do) or using a simple parameterization for the entity linking portion (as the le"
D17-1160,N06-1056,0,0.104731,"sk. 2 Related Work Semantic parsers vary along a few important dimensions: Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial Grammar (Zettlemoyer 1516 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers (Jia and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approach has the advantage of predicting the logical form directly from t"
D17-1160,P07-1121,0,0.387064,"Missing"
D17-1160,P15-1128,0,0.101171,"hed by translating questions into executable logical forms (i.e., programs) that output their answers. Recent work has shown that recurrent neural networks can be used for semantic parsing by encoding the question then predicting each token of the logical form in sequence (Jia and Liang, 2016; Dong and Lapata, 2016). These approaches, while effective, have two major limitations. First, they treat the logical form as an unstructured sequence, thereby ignoring type constraints on wellformed programs. Second, they do not address entity linking, which is a critical subproblem of semantic parsing (Yih et al., 2015). This paper introduces a novel neural semantic parsing model that addresses these limitations of prior work. Our parser uses an encoder-decoder architecture with two key innovations. First, the decoder generates from a grammar that guarantees that generated logical forms are well-typed. This grammar is automatically induced from typed logical forms, and does not require any manual engineering to produce. Second, the encoder incorporates an entity linking and embedding module that enables it to learn to identify which question spans should be linked to entities. Finally, we also introduce a ne"
D17-1160,P16-2033,0,0.0131144,"s the table entities it links to (Section 3.2). Second, the action space of the decoder is defined by a type-constrained grammar which guarantees that generated logical forms satisfy type constraints (Section 3.3). Supervision Semantic parsers can be trained from labeled logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or questionanswer pairs (Liang et al., 2011; Berant et al., 2013). Question-answer pairs were considered easier to obtain than labeled logical forms, though recent work has demonstrated that logical forms can be collected efficiently and are more effective (Yih et al., 2016). However, a key advantage of question-answer pairs is that they are agnostic to the domain representation and logical form Data Sets We use W IKI TABLE Q UESTIONS to evaluate our parser as this data set exhibits both a broad domain and complex questions. Early data sets, such as G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Dahl et al., 1994), have small domains with only a handful of different predicates. More recent data sets for question answering against Freebase have a much broader domain, but simple questions (Berant et al., 2013; Cai and Yates, 2013). 3 Model This section describes ou"
D17-1160,J84-2007,0,0.748808,"Missing"
D17-1160,P17-1041,0,0.342173,"and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approach has the advantage of predicting the logical form directly from the question without latent variables, which simplifies learning, but the disadvantage of ignoring type constraints on logical forms. Our type-constrained neural semantic parser inherits the advantages of both approaches: it only generates well-typed logical forms and has no syntactic latent variables as every logical form has a unique derivation. Recent work has explored similar ideas to ours in the context of Python code generation (Yin and Neubig, 2017; Rabinovich et al., 2017). language (e.g., lambda calculus or λ-DCS). This property is important for problems such as semistructured tables where the proper domain representation is unclear. Entity Linking Identifying the entities mentioned in a question is a critical subproblem of semantic parsing in broad domains and proper entity linking can lead to large accuracy improvements (Yih et al., 2015). However, semantic parsers have typically ignored this problem by assuming that entity linking is done beforehand (as the neural parsers above do) or using a simple parameterization for the entity"
D17-1160,D07-1071,0,0.327708,"Missing"
D17-1160,N15-1162,0,0.146151,"gh accuracy on this task. 2 Related Work Semantic parsers vary along a few important dimensions: Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial Grammar (Zettlemoyer 1516 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and Collins, 2005, 2007; Kwiatkowski et al., 2011, 2013; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013) and others (Liang et al., 2011; Berant et al., 2013; Zhao and Huang, 2015; Wong and Mooney, 2006, 2007). These formalisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat semantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens (Andreas et al., 2013). This approach is taken by recent neural semantic parsers (Jia and Liang, 2016; Dong and Lapata, 2016; Locascio et al., 2016; Ling et al., 2016). This approach has the advantage of predicting the logic"
D18-1184,D15-1075,0,0.484435,"in the first sentence to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent"
D18-1184,P16-1139,0,0.0994531,"these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying"
D18-1184,P17-1152,0,0.213268,"the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans"
D18-1184,D16-1053,1,0.88839,"Missing"
D18-1184,D16-1001,0,0.015428,"ght child on a non-terminal node and the second term is the score for spanij being the left child. In Figure 2b, we illustrate the outside process with the target span spanij being the right 1556 child of a non-terminal node. This process is calculated recursively from root to bottom. The normalized marginal probability ρij for each span spanij , where 1 ≤ i < n, i < j ≤ n can be calculated by: ρij = αij βij /α0n (12) To compute the representations of all possible spans, we use Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber 1997) with max-pooling and minus features (Cross and Huang, 2016; Liu and Lapata, 2017). We represent each sentence as a sequence of word embeddings [wsos , w1 , · · · , wt , · · · , wn , weos ] and run a bidirectional LSTM to obtain the output vectors. ht = [~ht , h~t ] is the output vector for the tth word, and ~ht and h~t are the output vectors from the forward and backward directions, respectively. We represent a constituent from position i to j with a span vector spij : spmaxpool = max(hi , · · · , hj ) ij spminus = [~hj − ~hi−1 , h~i − h~j+1 ] ij spij = [spmaxpool , spminus ] ij ij (13) spans across the two sentences, and the attended vectors can be"
D18-1184,P08-1109,0,0.220956,"Missing"
D18-1184,Q15-1035,0,0.0238931,"all structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with structured attention over spans in the other sentence. These 1554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics span representations, weighted b"
D18-1184,N16-1108,0,0.0460947,"Missing"
D18-1184,D16-1073,0,0.0145631,"recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Cohen and Smith, 2009). The most recent work in this direction was the Neural E-DMV model of Jiang et al. (2016). While our goal is not to induce a grammar, we do produce a probabilistic grammar as a byproduct of our model. Our results suggest that training on more complex objectives may be a good way to pursue grammar induction in the future; forcing the model to construct consistent, comparable subtrees between the two sentences is a strong signal for grammar induction. Very recently, a few models attempt to infer latent dependency tree structures with neural models in sentence modeling tasks (Yogatama et al., 2017; Choi et al., 2018). 6 Conclusions In this paper we have considered the problem of comp"
D18-1184,P17-1147,0,0.0264384,"d find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of eac"
D18-1184,N09-1009,0,0.019384,"6), the use of inference algorithms as intermediate layers in end-to-end neural networks is a recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Cohen and Smith, 2009). The most recent work in this direction was the Neural E-DMV model of Jiang et al. (2016). While our goal is not to induce a grammar, we do produce a probabilistic grammar as a byproduct of our model. Our results suggest that training on more complex objectives may be a good way to pursue grammar induction in the future; forcing the model to construct consistent, comparable subtrees between the two sentences is a strong signal for grammar induction. Very recently, a few models attempt to infer latent dependency tree structures with neural models in sentence modeling tasks (Yogatama et al., 20"
D18-1184,P04-1061,0,0.182318,"del considers all possible span comparisons, weighted by the spans’ marginal likelihood. likely each span is to appear as a constituent in a parse of the sentence. We use the non-terminal nodes of a binary constituency parse to represent spans. Because of this choice of representation, we can use the nodes in a CKY parsing chart to efficiently marginalize span likelihood over all possible parses for each sentence, and compare nodes in each sentence’s chart. 3.1 Learning Latent Constituency Trees A constituency parser can be partially formalized as a graphical model with the following cliques (Klein and Manning, 2004): the latent variables cikj ∈ 0, 1 for all i < j, indicating whether the span from the i-th token to the j-th token (spanij ) is a constituency node built from the merging of sub-node spanik and span(k+1)j . Given a sentence x = [xi , · · · , xn ], the probability of a tree z is, Q cikj ∈z p(cikj = 1) Q p(z|x) = P (8) z 0 ∈Z cikj ∈z 0 p(cikj = 1) where Z represents all possible constituency trees for x. The parameters for the graph-based CRF constituency parser are δikj reflecting the scores of spanij forming a binary constituency node with k as the splitting point. It is possible to calculate"
D18-1184,W17-5301,0,0.0849449,"to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sent"
D18-1184,D16-1244,0,0.0890286,"Missing"
D18-1184,D14-1162,0,0.0837504,"t (Separated Parameters) QA-LSTM (Tan et al., 2016b) Attentive Pooling Network (Santos et al., 2016) Pairwise Word Interaction (He and Lin, 2016) Lexical Decomposition and Composition (Wang et al., 2016) Noise-Contrastive Estimation (Rao et al., 2016) BiMPM (Wang et al., 2017b) MAP 0.764 0.772 0.780 0.780 0.786 0.730 0.753 0.777 0.771 0.801 0.802 MRR 0.842 0.851 0.846 0.860 0.860 0.824 0.851 0.836 0.845 0.877 0.875 Table 1: Results of our models (top) and previously proposed systems (bottom) on the TREC-QA test set. For both tasks, we initialize our model with 300D 840B GloVe word embeddings (Pennington et al., 2014). The hidden size for the BiLSTM is 150. The feed-forward networks F1 and F2 are two-layer perceptrons with ReLU as the hidden activation function and the size of the hidden and output layers is set to 300. All hyperparameters are selected based on the model’s performance on the development set. 4.1 Answer Sentence Selection We first study the effectiveness of our model for answer sentence selection tasks. Given a question, answer sentence selection aims to rank a list of candidate answer sentences based on their relatedness to the question. We experiment on the TRECQA dataset (Wang et al., 20"
D18-1184,N16-1030,0,0.0192327,"l., 2017; Zhao et al., 2016). However, all of these models are pipelined; they obtain the sentence structure in a non-differentiable preprocessing step, losing the benefits of end-to-end training. Ours is the first model to allow comparison between latent tree structures, trained end-to-end on the comparison objective. Structured attention While it has long been known that inference in graphical models is differentiable (Li and Eisner, 2009; Domke, 2011), and using inference in, e.g., a CRF (Lafferty et al., 2001) as the last layer in a neural network is common practice (Liu and Lapata, 2017; Lample et al., 2016), the use of inference algorithms as intermediate layers in end-to-end neural networks is a recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Co"
D18-1184,D16-1264,0,0.0321899,"r sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate"
D18-1184,D09-1005,0,0.0804877,"a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with structured attention over spans in the other sentence. These 1554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics span repr"
D18-1184,D17-1133,1,0.925393,"inal node and the second term is the score for spanij being the left child. In Figure 2b, we illustrate the outside process with the target span spanij being the right 1556 child of a non-terminal node. This process is calculated recursively from root to bottom. The normalized marginal probability ρij for each span spanij , where 1 ≤ i < n, i < j ≤ n can be calculated by: ρij = αij βij /α0n (12) To compute the representations of all possible spans, we use Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber 1997) with max-pooling and minus features (Cross and Huang, 2016; Liu and Lapata, 2017). We represent each sentence as a sequence of word embeddings [wsos , w1 , · · · , wt , · · · , wn , weos ] and run a bidirectional LSTM to obtain the output vectors. ht = [~ht , h~t ] is the output vector for the tth word, and ~ht and h~t are the output vectors from the forward and backward directions, respectively. We represent a constituent from position i to j with a span vector spij : spmaxpool = max(hi , · · · , hj ) ij spminus = [~hj − ~hi−1 , h~i − h~j+1 ] ij spij = [spmaxpool , spminus ] ij ij (13) spans across the two sentences, and the attended vectors can be calculated as: eij,kl ="
D18-1184,Q18-1005,1,0.934904,"uashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between t"
D18-1184,W09-3714,0,0.0422386,"sequence. There are some efforts to strengthen the decomposable attention model with a recurrent neural network (Liu and Lapata, 2018) or intrasentence attention (Parikh et al., 2016). However, these models amount to simply changing the input vectors a and b, and still only perform a wordlevel alignment between the two sentences. 3 Structured Alignment Networks Language is inherently tree structured, and the meaning of sentences comes largely from composing the meanings of subtrees (Chomsky, 2002). It is natural, then, to compare the meaning of two sentences by comparing their substructures (MacCartney and Manning, 2009). For example, when determining the relationship between two sentences in Figure 1, the ideal units of comparison are spans determined by subtrees: “is in Seattle” compared to “based in Washington state”. The challenge with comparing spans drawn from subtrees is that the tree structure of the sentence is latent and must be inferred, either during pre-processing or in the model itself. In this section we present a model that operates on the latent tree structure of each sentence, comparing all possible spans in one sentence with all possible spans in the second sentence, weighted by how 1555 A:"
D18-1184,P14-5010,0,0.0148714,"re our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation for each word. The second baseline is a Simple Span Alignment model; we use an MLP layer over the LSTM outputs to calculate the unnormalized scores and replace the inside-outside algorithm with a simple softmax function to obtain the probability distribution over all candidate spans. We also introduce a pipelined baseline where we extract constituents from trees parsed by the CoreNLP (Manning et al., 2014) constituency parser, and use the Simple Span Alignment model to only align these constituents. As shown in Table 1, we use two variants of the Structured Alignment model, since the structure of the question and the answer sentence may be different; the first model shares parameters across the question and the answer for computing the structures, while the second one uses separate parameters. We view the sentence selection task as a binary classification problem and the final ranking is based on the predicted probability of the sentence containing the correct answer (positive label). We apply"
D18-1184,P15-1150,0,0.324285,"Missing"
D18-1184,P16-1044,0,0.162701,"detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level co"
D18-1184,D07-1003,0,0.0413725,"n et al., 2014). The hidden size for the BiLSTM is 150. The feed-forward networks F1 and F2 are two-layer perceptrons with ReLU as the hidden activation function and the size of the hidden and output layers is set to 300. All hyperparameters are selected based on the model’s performance on the development set. 4.1 Answer Sentence Selection We first study the effectiveness of our model for answer sentence selection tasks. Given a question, answer sentence selection aims to rank a list of candidate answer sentences based on their relatedness to the question. We experiment on the TRECQA dataset (Wang et al., 2007), in which all questions with only positive or negative answers are removed. This leaves us with 1,162 training questions, 65 development questions and 68 test questions. Experimental results are listed in Table 1. We measure performance by the mean average precision (MAP) and mean reciprocal rank (MRR) using the standard TREC evaluation script. In the first block of Table 1, we compare our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation f"
D18-1184,P17-1018,0,0.242835,"ultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a r"
D18-1184,C16-1127,0,0.0591136,"Missing"
D18-1184,C16-1212,0,0.0745934,"comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence"
D19-1221,D15-1075,0,0.210861,"Missing"
D19-1221,P17-1152,0,0.057013,"Missing"
D19-1221,C18-1055,0,0.153122,"orithm We first choose the trigger length: longer triggers are more effective, while shorter triggers are more stealthy. Next, we initialize the trigger sequence by repeating the word “the”, the sub-word “a”, or the character “a” and concatenate the trigger to the front/end of all inputs.3 We then iteratively replace the tokens in the trigger to minimize the loss for the target prediction over batches of examples. To determine how to replace the current tokens, we cannot directly apply adversarial attack methods from computer vision because tokens are discrete. Instead, we build upon HotFlip (Ebrahimi et al., 2018b), a method that approximates the effect of replacing a token using its gradient. To apply this method, the trigger tokens tadv , which are represented as one-hot vectors, are embedded to form eadv . Token Replacement Strategy Our HotFlipinspired token replacement strategy is based on 3 More complex initialization schemes perform similarly (Appendix A). ... (1) tadv bottle tennis set cost minute tony zoning tapping ... arg min Et∼T [L(˜ y , f (tadv ; t))] , ... Universal Setting In a universal targeted attack, the adversary optimizes tadv to minimize the loss for the target class y˜ for all i"
D19-1221,P18-2006,0,0.230089,"orithm We first choose the trigger length: longer triggers are more effective, while shorter triggers are more stealthy. Next, we initialize the trigger sequence by repeating the word “the”, the sub-word “a”, or the character “a” and concatenate the trigger to the front/end of all inputs.3 We then iteratively replace the tokens in the trigger to minimize the loss for the target prediction over batches of examples. To determine how to replace the current tokens, we cannot directly apply adversarial attack methods from computer vision because tokens are discrete. Instead, we build upon HotFlip (Ebrahimi et al., 2018b), a method that approximates the effect of replacing a token using its gradient. To apply this method, the trigger tokens tadv , which are represented as one-hot vectors, are embedded to form eadv . Token Replacement Strategy Our HotFlipinspired token replacement strategy is based on 3 More complex initialization schemes perform similarly (Appendix A). ... (1) tadv bottle tennis set cost minute tony zoning tapping ... arg min Et∼T [L(˜ y , f (tadv ; t))] , ... Universal Setting In a universal targeted attack, the adversary optimizes tadv to minimize the loss for the target class y˜ for all i"
D19-1221,D18-1407,1,0.881648,"Missing"
D19-1221,N18-1202,1,0.774422,"Missing"
D19-1221,S18-2023,0,0.0642901,"Missing"
D19-1221,N18-2017,0,0.0723009,"Missing"
D19-1221,D16-1264,0,0.243828,"Missing"
D19-1221,P18-1079,1,0.813058,"l american people” trigger bit.ly/squad-demo. 2160 7 Related Work Adversarial Attacks in NLP Most adversarial attacks in NLP are gradient-based. For instance, Ebrahimi et al. (2018b) use gradients to attack text classifiers. He and Glass (2019) and Cheng et al. (2018) do the same for text generation. Other attack methods are based on generative (Iyyer et al., 2018) or human-in-the-loop approaches (Wallace et al., 2019). We turn the reader to Zhang et al. (2019) for a recent survey. Triggers differ from most previous attacks because they are universal (input-agnostic). Universal Attacks in NLP Ribeiro et al. (2018) debug models using semantically equivalent adversarial rules (SEARs). Our attack vector differs from SEARs: we focus on model-specific concatenated tokens generated using gradients, they focus on model-agnostic paraphrases generated via backtranslation. Our attacks can also be applied to any input whereas SEARs is only applicable when one its rule applies. In parallel work, Behjati et al. (2019) consider universal adversarial attacks on text classification (compare to our Section 3). Our work is more extensive as we (1) develop a stronger attack algorithm, (2) consider a broader range of mode"
D19-1221,N18-1170,0,0.0385909,"the ELMo model while removing tokens to find the best reduction. The resulting triggers are shorter but significantly more effective (Table 5).9 This shows that the triggers still “overfit” the GloVe BiDAF models. 9 Demo of ELMo model using the “to kill american people” trigger bit.ly/squad-demo. 2160 7 Related Work Adversarial Attacks in NLP Most adversarial attacks in NLP are gradient-based. For instance, Ebrahimi et al. (2018b) use gradients to attack text classifiers. He and Glass (2019) and Cheng et al. (2018) do the same for text generation. Other attack methods are based on generative (Iyyer et al., 2018) or human-in-the-loop approaches (Wallace et al., 2019). We turn the reader to Zhang et al. (2019) for a recent survey. Triggers differ from most previous attacks because they are universal (input-agnostic). Universal Attacks in NLP Ribeiro et al. (2018) debug models using semantically equivalent adversarial rules (SEARs). Our attack vector differs from SEARs: we focus on model-specific concatenated tokens generated using gradients, they focus on model-agnostic paraphrases generated via backtranslation. Our attacks can also be applied to any input whereas SEARs is only applicable when one its"
D19-1221,P16-1162,0,0.0607629,"Missing"
D19-1221,D17-1215,0,0.128075,"hension Reading comprehension models are used to answer questions that are posed to search engines or home assistants. An adversary can attack these models by modifying a web page in order to trigger malicious or vulgar answers. Here, we prepend triggers to paragraphs in order to cause predictions to be a target span inside the trigger. We choose and fix the target span beforehand and optimize the other trigger tokens. The trigger is optimized to work for any paragraph and any question of a certain type. We focus on why, who, when, and where questions. We use sentences of length ten following Jia and Liang (2017) and sum the cross-entropy of the start and end of the target span as the loss function. Conditional Text Generation We attack conditional text generation models, such as those in machine translation or autocomplete keyboards. The failure of such systems can be costly, e.g., translation errors have led to a person’s arrest (Hern, 2018). We create triggers that are prepended before the user input t to cause the model to generate similar content to a set of targets Y.5 In 5 A strong language model will generate grammatically correct continuations of the user’s input. This makes it impossible to"
D19-1221,P19-1334,0,0.0746808,"Missing"
D19-1221,N19-1314,0,0.0612768,"“zoning tapping fienes”, which causes frequent negative predictions. a linear approximation of the task loss.4 We update the embedding for every trigger token eadvi to minimizes the loss’ first-order Taylor approximation around the current token embedding: |  arg min e0i − eadvi ∇eadvi L, (2) e0i ∈V where V is the set of all token embeddings in the model’s vocabulary and ∇eadvi L is the average gradient of the task loss over a batch. Computing the optimal e0i can be efficiently computed in brute-force with |V |d-dimensional dot products where d is the dimensionality of the token embedding (Michel et al., 2019). This brute-force solution is trivially parallelizable and less expensive than running a forward pass for all the models we consider. Finally, after finding each eadvi , we convert the embeddings back to their associated tokens. Figure 1 provides an illustration of the trigger search algorithm. We augment this token replacement strategy with beam search. We consider the top-k token candidates from Equation 2 for each token position in the trigger. We search left to right across 4 We also experiment with projected gradient descent (Appendix A) but find the linear approximation converges faster"
D19-1221,L18-1008,0,0.0274264,"Missing"
D19-1221,D16-1244,0,0.0856411,"Missing"
D19-1221,D13-1170,0,0.0256105,"Missing"
D19-1221,Q19-1029,1,0.846179,"reduction. The resulting triggers are shorter but significantly more effective (Table 5).9 This shows that the triggers still “overfit” the GloVe BiDAF models. 9 Demo of ELMo model using the “to kill american people” trigger bit.ly/squad-demo. 2160 7 Related Work Adversarial Attacks in NLP Most adversarial attacks in NLP are gradient-based. For instance, Ebrahimi et al. (2018b) use gradients to attack text classifiers. He and Glass (2019) and Cheng et al. (2018) do the same for text generation. Other attack methods are based on generative (Iyyer et al., 2018) or human-in-the-loop approaches (Wallace et al., 2019). We turn the reader to Zhang et al. (2019) for a recent survey. Triggers differ from most previous attacks because they are universal (input-agnostic). Universal Attacks in NLP Ribeiro et al. (2018) debug models using semantically equivalent adversarial rules (SEARs). Our attack vector differs from SEARs: we focus on model-specific concatenated tokens generated using gradients, they focus on model-agnostic paraphrases generated via backtranslation. Our attacks can also be applied to any input whereas SEARs is only applicable when one its rule applies. In parallel work, Behjati et al. (2019) c"
D19-1221,D14-1162,0,0.0874216,"dily transfer: the ELMobased DA model’s accuracy degrades the most, despite never being targeted in the trigger generation. We analyze why the predictions for Contradiction are more robust and show that triggers align with known dataset biases in Section 6. 4 Attacking Reading Comprehension We create triggers for SQuAD (Rajpurkar et al., 2016). We use an intentionally simple baseline model and test the trigger’s transferability to more advanced models (with different embeddings, tokenizations, and architectures). The baseline is BiDAF (Seo et al., 2017); we lowercase all inputs and use GloVe (Pennington et al., 2014). ESIM DA DA-ELMo nobody never sad scared championship 89.49 89.46 0.03 0.15 0.50 1.07 1.51 0.50 1.13 0.74 0.83 0.06 90.88 0.50 0.15 0.71 1.01 0.77 Avg. ∆ -88.69 -88.96 -90.25 nobody sleeps nothing none sleeping 84.62 0.53 4.57 1.71 5.96 6.06 79.71 8.45 14.82 23.61 17.52 15.84 83.04 13.61 22.34 14.63 15.41 28.86 Avg. ∆ -80.85 -63.66 -64.07 86.31 73.31 79.89 79.83 80.44 78.00 84.80 70.93 66.91 65.71 63.79 65.83 85.17 60.67 62.96 64.01 70.56 70.56 -8.02 -18.17 -19.42 joyously Contradiction anticipating talented impress inspiring Avg. ∆ Table 2: We prepend a single word (Trigger) to SNLI hypothes"
D19-1221,D18-1453,0,\N,Missing
D19-1378,P19-1448,1,0.90392,"quent word ‘nation’ is similar to the DB column country which belongs to the table singer, thus selecting the column singer.name from the same table is more likely. Second, the next appearance of the word ‘name’ is next to the phrase ’Hey’, which appears as the value in one of the cells of the column song.name. Assuming a one-toone mapping between words and DB constants, again singer.name is preferred. In this paper, we propose a semantic parser that reasons over the DB structure and question to make a global decision about which DB constants should be used in a query. We extend the parser of Bogin et al. (2019), which learns a representation for the 3659 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3659–3664, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics DB schema at parsing time. First, we perform message-passing through a graph neural network representation of the DB schema, to softly select the set of DB constants that are likely to appear in the output query. Second, we train a model that takes the top-K queries output by the autoregr"
D19-1378,J05-1003,0,0.0672435,"DB constants that appear in P y. We can now P add a relevance loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignmen"
D19-1378,N19-1240,0,0.0805353,"Missing"
D19-1378,P17-2025,0,0.0308538,"loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignment between question words and DB constants. The re-ranker is"
D19-1378,P06-2034,0,0.0647933,"r in P y. We can now P add a relevance loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignment between question wo"
D19-1378,D18-1190,1,0.893009,"Missing"
D19-1378,D17-1160,1,0.868222,"hema S was not seen at training time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constant, it first computes an attention distribution ove"
D19-1378,D08-1082,0,0.0473698,"P add a relevance loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignment between question words and DB consta"
D19-1378,P17-1105,0,0.0423086,"ing time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constant, it first computes an attention distribution over the question |x| words"
D19-1378,C18-1280,0,0.0292329,"the decoding sequence, especially in a top-down parser. DB schema encoding In the zero-shot setting, the schema structure of a new DB can affect the output query. To capture DB structure, Bogin et al. (2019) learned a representation hv for every DB constant, which the parser later used at decoding time. This was done by converting the DB schema into a graph, where nodes are DB constants, and edges connect tables and their columns, as well as primary and foreign keys (Figure 2, left). A graph convolutional network (GCN) then learned representations hv for nodes end-to-end (De Cao et al., 2019; Sorokin and Gurevych, 2018). To focus the GCN’s capacity on important nodes, a relevance probability ρv was computed for every node, and used to “gate” the input to the GCN, conditioned on the question. Specifically, given a learned embedding rv for every database constant, (0) the GCN input is hv = ρv · rv . Then, the GCN recurrence is applied for L steps. At each step, nodes re-compute their representation based on the representation of their neighbors, where different edge types are associated with different learned parameters (Li et al., 2016). The final representation (L) of each DB constant is hv = hv . Importantl"
D19-1378,P16-1127,0,0.0321369,"he correct SQL query. Importantly, the schema S was not seen at training time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constan"
D19-1378,P17-1041,0,0.0281408,"y. Importantly, the schema S was not seen at training time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constant, it first computes a"
D19-1378,D18-1193,0,0.166296,"Missing"
D19-1378,D18-1425,0,0.122647,"Missing"
D19-1534,P18-1198,0,0.0212208,"y; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). KhandelSynthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrap"
D19-1534,P19-1329,0,0.0385549,"training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al."
D19-1534,D14-1162,0,0.100642,"to numbers outside its training range. We are especially intrigued by the model’s ability to learn numeracy, i.e., how does the model know the value of a number given its embedding? The model uses standard embeddings (GloVe and a Char-CNN) and receives no direct supervision for number magnitude/ordering. To understand how numeracy emerges, we probe token embedding methods (e.g., BERT, GloVe) using synthetic list maximum, number decoding, and addition tasks (Section 3). We find that all widely-used pre-trained embeddings, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GloVe (Pennington et al., 2014), capture numeracy: number magnitude is present in the embeddings, even for numbers in the thousands. Among all embeddings, characterlevel methods exhibit stronger numeracy than word- and sub-word-level methods (e.g., ELMo excels while BERT struggles), and character-level models learned directly on the synthetic tasks are the strongest overall. Finally, we investigate why NAQANet had trouble extrapolating—was it a failure in the model or the embeddings? We repeat our probing tasks and test for model extrapolation, finding that neural models struggle to predict numbers outside the training rang"
D19-1534,D16-1264,0,0.0650128,", or addition/subtraction of numbers.) Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing number magnitude or performing explicit comparisons. We refer readers to Yu et al. (2018) and Dua et al. (2019) for further details. 2.1 2.3 2 Numeracy Case Study: DROP QA DROP Dataset DROP is a reading comprehension dataset that tests numerical reasoning operations such as counting, sorting, and addition (Dua et al., 2019). The dataset’s input-output format is a superset of SQuAD (Rajpurkar et al., 2016): the answers are paragraph spans, as well as question Comparative and Superlative Questions We focus on questions that NAQANet requires numeracy to answer, namely Comparative and Superlative questions.2 Comparative questions 1 Result as of May 21st, 2019. DROP addition, subtraction, and count questions do not require numeracy for NAQANet, see Appendix A. 5308 2 Question Type Example Reasoning Required Comparative (Binary) Comparative (Non-binary) Superlative (Number) Superlative (Span) Which country is a bigger exporter, Brazil or Uruguay? Which player had a touchdown longer than 20 yards? Ho"
D19-1534,N19-1423,0,0.51046,", BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact. Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2). The first step in performing numerical reasoning over natural language is numeracy: the abilEqual contribution; work done while interning at AI2. (a) Word2Vec (b) GloVe (c) ELMo (d) BERT (e)"
D19-1534,K19-1033,0,0.0951641,"Missing"
D19-1534,N19-1246,1,0.888358,"ccurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact. Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2). The first step in performing numerical reasoning over natural language is numeracy: the abilEqual contribution; work done while interning at AI2. (a) Word2Vec (b) GloVe (c) ELMo (d) BERT (e) Char-CNN (f) Char-LSTM 0 −500 500 0 −500 500 0 −500 −2000 −1000 0 1000 I put Number 2000−2000 −1000 0 1000 I put Number 2000 Figure 1: We train a probing model to decode a number from it"
D19-1534,P17-1025,0,0.0372483,"ger range [0,150] and evaluated on integers from the Test Range. The probing model struggles to extrapolate when trained on the pre-trained embeddings. 4 Discussion and Related Work An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more"
D19-1534,P18-1027,0,0.050128,"Missing"
D19-1534,D17-1160,1,0.827689,"ate-of-the-art NAQANet answers every question correct. Plausible answer candidates to the questions are underlined and the model’s predictions are shown in bold. spans, number answers (e.g., 35), and dates (e.g., 03/01/2014). The only supervision provided is the question-answer pairs, i.e., a model must learn to reason numerically while simultaneously learning to read and comprehend. 2.2 NAQANet Model This section examines the state-of-the-art model for DROP by investigating its accuracy on questions that require numerical reasoning. Modeling approaches for DROP include both semantic parsing (Krishnamurthy et al., 2017) and reading comprehension (Yu et al., 2018) models. We focus on the latter, specifically on Numerically-augmented QANet (NAQANet), the current state-of-the-art model (Dua et al., 2019).1 The model’s core structure closely follows QANet (Yu et al., 2018) except that it contains four output branches, one for each of the four answer types (passage span, question span, count answer, or addition/subtraction of numbers.) Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing numb"
D19-1534,Q16-1037,0,0.10323,"Missing"
D19-1534,N19-1112,1,0.797676,"t al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). KhandelSynthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrapolation: neural mod"
D19-1534,L18-1008,0,0.0281129,"Missing"
D19-1534,P18-1196,0,0.124058,"Missing"
D19-1534,P18-2117,0,0.163586,"extrapolate to other values. 2.6 Whence this behavior? NAQANet exhibits numerical reasoning capabilities that exceed our expectations. What enables this behavior? Aside from reading and comprehending the passage/question, this kind of numerical reasoning requires two components: numeracy (i.e., representing numbers) and comparison algorithms (i.e., computing the maximum of a list). Although the natural emergence of comparison algorithms is surprising, previous results show neural models are capable of learning to count and sort synthetic lists of scalar values when given explicit supervision (Weiss et al., 2018; Vinyals et al., 2016). NAQANet demonstrates that a model can learn comparison algorithms while simultane3 Probing Numeracy of Embeddings We use synthetic numerical tasks to probe the numeracy of token embeddings. 3.1 Probing Tasks We consider three synthetic tasks to evaluate numeracy (Figure 3). Appendix C provides further details on training and evaluation. List Maximum Given a list of the embeddings for five numbers, the task is to predict the index of the maximum number. Each list consists of values of similar magnitude in order to evaluate fine-grained comparisons (see Appendix C). As i"
D19-1534,P18-2102,0,0.027081,"struggles to extrapolate when trained on the pre-trained embeddings. 4 Discussion and Related Work An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. P"
D19-1606,W18-2501,1,0.779011,"Net (Yu et al., 2018), currently the best-performing published model on SQuAD 1.1 without data augmentation or pretraining; (2) QANet + BERT, which enhances the QANet model by concatenating frozen BERT representations to the original input embeddings; (3) BERT QA (Devlin et al., 2019), the adversarial baseline used in data construction, and (4) XLNet QA (Yang et al., 2019), another large pretrained language model based on the Transformer architecture (Vaswani et al., 2017) that outperforms BERT QA on reading comprehension 5927 benchmarks SQuAD and RACE (Lai et al., 2017). We use the AllenNLP (Gardner et al., 2018) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner (2018) and pytorch-transformers6 implementation of base BERT QA7 and base XLNet QA. BERT is pretrained on English Wikipedia and BookCorpus (Zhu et al., 2015) (3.87B wordpieces, 13GB of plain text) and XLNet additionally on Giga5 (Napoles et al., 2012), ClueWeb 2012-B (extended from Smucker et al., 2009), and Common Crawl8 (32.89B wordpieces, 78GB of plain text). 4.2 Heuristic Baselines In light of recent work exposing predictive artifacts in crowdsourced NLP datasets (Gururangan et al., 2018; Kaushik"
D19-1606,L16-1021,0,0.122979,"Missing"
D19-1606,N15-1117,0,0.0285915,"sets like those of Pradhan et al. (2007), Ghaddar and Langlais (2016), Chen et al. (2018) and Poesio et al. (2018), which aim to obtain complete coref5928 erence clusters, our questions require understanding coreference between only a few spans. While this means that the notion of coreference captured by our dataset is less comprehensive, it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing. Guha et al. (2015) present the limitations of annotating coreference in newswire texts alone, and like us, built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions. There is some other recent work (Poesio et al., 2019; Aralikatte and Søgaard, 2019) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well. Reading comprehension datasets There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019, inter alia). Most of these datasets principally require understanding local"
D19-1606,N18-2017,1,0.840015,"e AllenNLP (Gardner et al., 2018) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner (2018) and pytorch-transformers6 implementation of base BERT QA7 and base XLNet QA. BERT is pretrained on English Wikipedia and BookCorpus (Zhu et al., 2015) (3.87B wordpieces, 13GB of plain text) and XLNet additionally on Giga5 (Napoles et al., 2012), ClueWeb 2012-B (extended from Smucker et al., 2009), and Common Crawl8 (32.89B wordpieces, 78GB of plain text). 4.2 Heuristic Baselines In light of recent work exposing predictive artifacts in crowdsourced NLP datasets (Gururangan et al., 2018; Kaushik and Lipton, 2018, inter alia), we estimate the effect of predictive artifacts by training BERT QA and XLNet QA to predict a single start and end index given only the passage as input (passage-only). 4.3 Results Table 3 presents the performance of all baseline models on Q UOREF. The best performing model is XLNet QA, which reaches an F1 score of 70.5 in the test set. However, it is still more than 20 F1 points below human performance.9 BERT QA trained on Q UOREF under-performs XLNet QA, but still gets a decent F1 score of 66.4. Note that BERT QA trained on SQuAD would have achieved an"
D19-1606,P17-1147,0,0.0275088,"tching highlights are anchored. Next to the questions are the relevant coreferent mentions from the paragraph. They are bolded for the first question, italicized for the second, and underlined for the third in the paragraph. Introduction Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019, inter alia). However, these datasets focus largely on understanding local predicateargument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances (Pradhan et al., 2007; Versley, 2008; Recasens et al., 2011; Poesio et al., 2018), and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaways that let models arrive at the correc"
D19-1606,D18-1546,0,0.0301144,"., 2018) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner (2018) and pytorch-transformers6 implementation of base BERT QA7 and base XLNet QA. BERT is pretrained on English Wikipedia and BookCorpus (Zhu et al., 2015) (3.87B wordpieces, 13GB of plain text) and XLNet additionally on Giga5 (Napoles et al., 2012), ClueWeb 2012-B (extended from Smucker et al., 2009), and Common Crawl8 (32.89B wordpieces, 78GB of plain text). 4.2 Heuristic Baselines In light of recent work exposing predictive artifacts in crowdsourced NLP datasets (Gururangan et al., 2018; Kaushik and Lipton, 2018, inter alia), we estimate the effect of predictive artifacts by training BERT QA and XLNet QA to predict a single start and end index given only the passage as input (passage-only). 4.3 Results Table 3 presents the performance of all baseline models on Q UOREF. The best performing model is XLNet QA, which reaches an F1 score of 70.5 in the test set. However, it is still more than 20 F1 points below human performance.9 BERT QA trained on Q UOREF under-performs XLNet QA, but still gets a decent F1 score of 66.4. Note that BERT QA trained on SQuAD would have achieved an F1 score of 0, since our"
D19-1606,Q19-1026,0,0.216951,"e anchored. Next to the questions are the relevant coreferent mentions from the paragraph. They are bolded for the first question, italicized for the second, and underlined for the third in the paragraph. Introduction Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019, inter alia). However, these datasets focus largely on understanding local predicateargument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances (Pradhan et al., 2007; Versley, 2008; Recasens et al., 2011; Poesio et al., 2018), and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaways that let models arrive at the correct answer without performin"
D19-1606,N19-1225,1,0.83452,"best performing model is XLNet QA, which reaches an F1 score of 70.5 in the test set. However, it is still more than 20 F1 points below human performance.9 BERT QA trained on Q UOREF under-performs XLNet QA, but still gets a decent F1 score of 66.4. Note that BERT QA trained on SQuAD would have achieved an F1 score of 0, since our dataset was constructed with that model as the adversary. The extent to which BERT QA does well on Q UOREF might indicate its capacity for coreferential reasoning that was not exploited when it was trained on SQuAD (for a detailed discussion of this phenomenon, see Liu et al., 2019). Our analysis of model errors in §4.4 shows that some of the improved performance may also be due to artifacts in Q UOREF. We notice smaller improvements from XLNet QA over BERT QA (4.12 in F1 test score, 2.6 6 https://github.com/huggingface/ pytorch-transformers 7 The large BERT model does not fit in the available GPU memory. 8 https://commoncrawl.org/ 9 Human performance was estimated from the authors’ answers of 400 questions from the test set, scored with the same metric used for systems. in EM test score) on Q UOREF compared to other reading comprehension benchmarks: SQuAD and RACE (see"
D19-1606,W12-3018,0,0.0601011,"Missing"
D19-1606,N19-1176,0,0.0255683,"en only a few spans. While this means that the notion of coreference captured by our dataset is less comprehensive, it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing. Guha et al. (2015) present the limitations of annotating coreference in newswire texts alone, and like us, built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions. There is some other recent work (Poesio et al., 2019; Aralikatte and Søgaard, 2019) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well. Reading comprehension datasets There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019, inter alia). Most of these datasets principally require understanding local predicate-argument structure in a paragraph of text. Q UOREF also requires understanding local predicate-argument structure, but makes the reading task harder by explicitly querying anaphoric references, requiring a system to"
D19-1606,D16-1264,0,0.547128,"in paragraphs is where the questions with matching highlights are anchored. Next to the questions are the relevant coreferent mentions from the paragraph. They are bolded for the first question, italicized for the second, and underlined for the third in the paragraph. Introduction Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019, inter alia). However, these datasets focus largely on understanding local predicateargument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances (Pradhan et al., 2007; Versley, 2008; Recasens et al., 2011; Poesio et al., 2018), and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaw"
D19-1606,D13-1020,0,0.0932073,"es annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing. Guha et al. (2015) present the limitations of annotating coreference in newswire texts alone, and like us, built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions. There is some other recent work (Poesio et al., 2019; Aralikatte and Søgaard, 2019) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well. Reading comprehension datasets There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019, inter alia). Most of these datasets principally require understanding local predicate-argument structure in a paragraph of text. Q UOREF also requires understanding local predicate-argument structure, but makes the reading task harder by explicitly querying anaphoric references, requiring a system to track entities throughout the discourse. 6 Conclusion We present Q UOREF, a focused reading comprehension benchmark that evaluates the ability of models to resolve coreference. We crowdsourced questions over paragraphs from Wiki"
D19-1606,D17-1082,0,\N,Missing
D19-1606,W18-0702,0,\N,Missing
D19-1606,N19-1246,1,\N,Missing
D19-1606,P18-1078,1,\N,Missing
D19-1606,N19-1423,0,\N,Missing
D19-1608,D13-1160,0,0.257991,"Missing"
D19-1608,P18-1078,1,0.859212,"Missing"
D19-1608,N19-1423,0,0.0439343,"Missing"
D19-1608,D17-1160,1,0.898162,"Missing"
D19-1608,N19-1270,0,0.0166905,"he full task. First, a sentence Ki is retrieved from K using Qi as a search query. This is then supplied to BERT as [CLS] Ki [SEP] question-stem [SEP] answeroption [SEP] for each option. The [CLS] output token is projected to a single logit and fed through a softmax layer across answer options, using cross entropy loss, the highest being selected. This model is fine-tuned using Q UA RT Z (only). 4. BERT (IR upper bound): Same, but using the ideal (annotated) Ki rather than retrieved Ki . 5. BERT-PFT (no knowledge): BERT first finetuned (“pre-fine-tuned”) on the RACE dataset (Lai et al., 2017; Sun et al., 2019), and then fine-tuned on Q UA RT Z (questions only, no K, both train and test). Questions are supplied as [CLS] questionstem [SEP] answer-option [SEP]. 6. BERT-PFT (IR): Same as BERT (IR), except starting with the pre-fine-tuned BERT. All models were implemented using AllenNLP (Gardner et al., 2018). 6 Results The results are shown in Table 3, and provide insights into both the data and the models: 1. The dataset is hard. Our best model, BERTPFT (IR), scores only 73.7, over 20 points behind human performance (95.0), suggesting there are significant linguistic and semantic challenges to overcom"
D19-1608,D18-1259,0,0.0703663,"s plus a background corpus of 400 qualitative knowledge sentences; and (2) an analysis of the dataset, performance of BERT-based models, and a catalog of the challenges it poses, pointing the way towards solutions. 2 Related Work Despite rapid progress in general questionanswering (QA), e.g., (Clark and Gardner, 2018), and formal models for qualitative reasoning (QR), e.g., (Forbus, 1984; Weld and De Kleer, 2013), there has been little work on reasoning with textual qualitative knowledge, and no dataset available in this area. Although many datasets include a few qualitative questions, e.g., (Yang et al., 2018; Clark et al., 2018), the only one directly probing 5941 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5941–5946, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Differing Comparatives: Q1 Jan is comparing stars, specifically a small star and the larger Sun. Given the size of each, Jan can tell that the Sun puts out heat that is (A) greater (B) lesser K1 Bigger stars produce more energy, so their surfaces are hotter. Discrete Property"
D19-3002,2021.ccl-1.108,0,0.152362,"Missing"
D19-3002,N19-1423,0,0.202062,"A sentiment model incorrectly predicts the positive class due to the trigram “tony hawk style”. 2 2.2 We consider three saliency methods. Since our goal is to interpret why the model made its prediction (not the ground-truth answer), we use the model’s own output in the loss calculation. For each method, we reduce each token’s gradient (which is the same dimension as the token embedding) to a single value by taking the L2 norm. Vanilla Gradient This method visualizes the gradient of the loss with respect to each token (Simonyan et al., 2014). Figure 2 shows an example interpretation of BERT (Devlin et al., 2019). Integrated Gradients Sundararajan et al. (2017) introduce integrated gradients. They 0 define a baseline x , which is an input absent of information (we use a sequence of all zero embeddings). Word importance is determined by integrating the gradient along the path from this baseline to the original input. SmoothGrad Smilkov et al. (2017) average the gradient over many noisy versions of the input. For NLP, we add small Gaussian noise to every embedding and take the average gradient value. 2.3 Interpreting Model Predictions Adversarial Attacks We consider two adversarial attacks: replacing wo"
D19-3002,N18-1202,1,0.569277,"nd BiDAF models (Seo et al., 2017). Our AllenNLP Extension The core backbone of our toolkit is an extension to the Predictor class that allows interpretation methods to compute input gradients in a model-agnostic way. Creating this extension has two main implementation challenges: (1) the loss (with the model’s own predictions as the labels) must be computed for widely varying output formats (e.g., classification, tagging, or language modeling), and (2) the gradient of this loss with respect to the token embeddings must be computed for widely varying embedding types (e.g., word vectors, ELMo (Peters et al., 2018) embeddings, BERT embeddings). • Masked Language Modeling using the transformer models available in Pytorch Transformers2 , e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more. • Text Classification and Textual Entailment using BiLSTM and self-attention classifiers. • Named Entity Recognition (NER) and Coreference Resolution. These are examples of tasks with complex input-output structure; we can use the same function calls to analyze each predicted tag (e.g., Figure 1) or cluster. 1 We also adapt HotFlip to contextual embeddings; details provided in Section 3.2. 2 https://g"
D19-3002,N19-1246,1,0.888914,"Missing"
D19-3002,D16-1264,0,0.0653792,"Missing"
D19-3002,N16-3020,1,0.551905,"Interpret for NER. The model predicts three tags for an input (top). We interpret each tag separately, e.g., input reduction (Feng et al., 2018) (bottom) removes as many words as possible without changing a tag’s prediction. Input reduction shows that the words “named”, “at”, and “in downtown” are sufficient to predict the People, Organization, and Location tags, respectively. Instance-level interpretation methods help to answer this question by providing explanations for specific model predictions. These explanations come in many flavors, e.g., visualizing a model’s local decision boundary (Ribeiro et al., 2016), highlighting the saliency of the input features (Simonyan et al., 2014), or adversarially modifying the input (Ebrahimi et al., 2018). Interpretations are useful to illuminate the strengths and weaknesses of a model (Feng et al., 2018), increase user trust (Ribeiro et al., 2016), and evaluate hard-todefine criteria such as safety or fairness (DoshiVelez and Kim, 2017). Many open-source implementations exist for instance-level interpretation methods. However, most codebases focus on computer vision, are model- or task-specific (e.g., sentiment analysis), or contain implementations for a small"
D19-3002,P18-2006,0,0.158866,"al., 2018) (bottom) removes as many words as possible without changing a tag’s prediction. Input reduction shows that the words “named”, “at”, and “in downtown” are sufficient to predict the People, Organization, and Location tags, respectively. Instance-level interpretation methods help to answer this question by providing explanations for specific model predictions. These explanations come in many flavors, e.g., visualizing a model’s local decision boundary (Ribeiro et al., 2016), highlighting the saliency of the input features (Simonyan et al., 2014), or adversarially modifying the input (Ebrahimi et al., 2018). Interpretations are useful to illuminate the strengths and weaknesses of a model (Feng et al., 2018), increase user trust (Ribeiro et al., 2016), and evaluate hard-todefine criteria such as safety or fairness (DoshiVelez and Kim, 2017). Many open-source implementations exist for instance-level interpretation methods. However, most codebases focus on computer vision, are model- or task-specific (e.g., sentiment analysis), or contain implementations for a small number of interpretation methods. Thus, it is difficult for practitioners to interpret their model. As a reIntroduction Despite consta"
D19-3002,D18-1407,1,0.932932,"library of front-end visualization components. We demonstrate the toolkit’s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp. org/interpret. 1 Figure 1: An interpretation generated using AllenNLP Interpret for NER. The model predicts three tags for an input (top). We interpret each tag separately, e.g., input reduction (Feng et al., 2018) (bottom) removes as many words as possible without changing a tag’s prediction. Input reduction shows that the words “named”, “at”, and “in downtown” are sufficient to predict the People, Organization, and Location tags, respectively. Instance-level interpretation methods help to answer this question by providing explanations for specific model predictions. These explanations come in many flavors, e.g., visualizing a model’s local decision boundary (Ribeiro et al., 2016), highlighting the saliency of the input features (Simonyan et al., 2014), or adversarially modifying the input (Ebrahimi et"
D19-3002,N18-2017,0,0.0362058,"e hard-todefine criteria such as safety or fairness (DoshiVelez and Kim, 2017). Many open-source implementations exist for instance-level interpretation methods. However, most codebases focus on computer vision, are model- or task-specific (e.g., sentiment analysis), or contain implementations for a small number of interpretation methods. Thus, it is difficult for practitioners to interpret their model. As a reIntroduction Despite constant advances and seemingly superhuman performance on constrained domains, state-of-the-art models for NLP are imperfect: they latch on to superficial patterns (Gururangan et al., 2018), reflect unwanted social biases (Doshi-Velez and Kim, 2017), and significantly underperform humans on a myriad of tasks. These imperfections, coupled with today’s advances being driven by (seemingly black-box) neural models, leave researchers and practitioners scratching their heads, asking, “why did my model make this prediction?” 7 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 7–12 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics ing its weaknesses. We focus on methods that modify tokens in the input (e.g., replace or"
D19-3002,D18-2007,0,0.0272083,"sualization (requires making a one-line call to the reusable front-end components). isolate the effect of individual neurons (Karpathy et al., 2016). We focus on gradient-based methods because they are applicable to many models. Existing Interpretation Toolkits In computer vision, various open-source toolkits exist for explaining and attacking models (e.g., Papernot et al. (2016); Ozbulak (2019), inter alia); some toolkits also include interactive demos (Norton and Qi, 2017). Similar toolkits for NLP are significantly scarcer, and most toolkits focus on specific models or tasks. For instance, Liu et al. (2018), Strobelt et al. (2019), and Vig (2019) visualize attention weights for specific NLP models, while Lee et al. (2019) apply adversarial attacks to reading comprehension systems. Our toolkit differs because it is flexible and diverse; we can interpret and attack any AllenNLP model. New Model We also provide a tutorial for interpreting a new model. If your task is already available in the demos (e.g., text classification), you need to change a single line of code to replace the demo model with your model. If your task is not present in the demos, you will need to: 1. Write the predictions to lab"
D19-3002,W18-5416,1,0.840737,"the AllenNLP Demo, a web application for running AllenNLP models. We add HTML and JavaScript components that provide visualizations for saliency maps and adversarial attacks. These components are reusable and greatly simplify the process for adding new models and interpretation methods (Section 4). For example, a single line of HTML code can create the visualizations shown in Figures 1–3. Note that visualizing the interpretations is not required—AllenNLP Interpret can be run in an offline, batch manner. This is useful for aggregating interpretation results, e.g., as in Feng et al. (2018) and Wallace et al. (2018). Embedding-Agnostic Gradients To handle difficulty (2)—computing the gradients of varying token embeddings—we rely on the abstractions of AllenNLP. In particular, AllenNLP uses a TokenEmbedder interface to converts token ids into embeddings. We can thus compute the gradient for any embedding method by registering a PyTorch backward gradient hook on the model’s TokenEmbedder function. Our end result is a simple API for computing input gradients for any model: call predictions to labeled instances() and then get gradients(). 4 Adding a Model or Interpretation Context-Independent Embedding Matri"
D19-5808,N18-1023,0,0.114328,"Missing"
D19-5808,2021.ccl-1.108,0,0.246324,"Missing"
D19-5808,D18-1260,1,0.912028,"Missing"
D19-5808,D16-1264,0,0.107978,"Missing"
D19-5808,D13-1020,0,0.207718,"Missing"
D19-5808,D15-1075,0,0.126445,"Missing"
D19-5808,D18-1233,0,0.0634668,"Missing"
D19-5808,P16-1223,0,0.0240296,"ation than category A flower? Answer: more Question: Would category A flower have more or less efficient fertilization than category B flower? Answer: less Question: Which category of flowers would be more likely to have brightly colored petals? Answer: Category B Question: Which category of flowers would be less likely to have brightly colored petals? Answer: Category A Figure 1: Example questions in ROPES. Introduction Recent work in reading comprehension has seen impressive results, with models reaching human performance on well-established datasets (Devlin et al., 2019; Wang et al., 2017; Chen et al., 2016), but so far has mostly focused on extracting local predicate-argument structure, without the need to apply what was read to outside context. We introduce ROPES1 , a reading comprehension challenge that focuses on understanding causes and effects in an expository paragraph, requiring systems to apply this understanding to Comprehending a passage of text requires being able to understand the implications of the passage on other text that is read. For example, after reading a background passage about how animal pollinators increase the efficiency of fertilization in flowers, a human can easily d"
D19-5808,N19-1423,0,0.0238704,"wer have more or less efficient fertilization than category A flower? Answer: more Question: Would category A flower have more or less efficient fertilization than category B flower? Answer: less Question: Which category of flowers would be more likely to have brightly colored petals? Answer: Category B Question: Which category of flowers would be less likely to have brightly colored petals? Answer: Category A Figure 1: Example questions in ROPES. Introduction Recent work in reading comprehension has seen impressive results, with models reaching human performance on well-established datasets (Devlin et al., 2019; Wang et al., 2017; Chen et al., 2016), but so far has mostly focused on extracting local predicate-argument structure, without the need to apply what was read to outside context. We introduce ROPES1 , a reading comprehension challenge that focuses on understanding causes and effects in an expository paragraph, requiring systems to apply this understanding to Comprehending a passage of text requires being able to understand the implications of the passage on other text that is read. For example, after reading a background passage about how animal pollinators increase the efficiency of fertili"
D19-5808,P17-1018,0,0.0150372,"efficient fertilization than category A flower? Answer: more Question: Would category A flower have more or less efficient fertilization than category B flower? Answer: less Question: Which category of flowers would be more likely to have brightly colored petals? Answer: Category B Question: Which category of flowers would be less likely to have brightly colored petals? Answer: Category A Figure 1: Example questions in ROPES. Introduction Recent work in reading comprehension has seen impressive results, with models reaching human performance on well-established datasets (Devlin et al., 2019; Wang et al., 2017; Chen et al., 2016), but so far has mostly focused on extracting local predicate-argument structure, without the need to apply what was read to outside context. We introduce ROPES1 , a reading comprehension challenge that focuses on understanding causes and effects in an expository paragraph, requiring systems to apply this understanding to Comprehending a passage of text requires being able to understand the implications of the passage on other text that is read. For example, after reading a background passage about how animal pollinators increase the efficiency of fertilization in flowers,"
D19-5808,N19-1246,1,0.869684,"Missing"
D19-5808,D18-1259,0,0.151555,"Missing"
D19-5808,D19-1107,0,0.0834166,"his phone remotely. He called this test as case C. Which experiment would be less appropriate for case C, case A or case B? case A Table 4: Example questions and answers from ROPES, showing the relevant parts of the associated passage and the reasoning required to answer the question. In the last example, the situation grounds the desired outcome and asks which of two cases would achieve the desired outcome. Dataset split In initial experiments, we found splitting the dataset based on the situations resulted in high scores due to annotator bias from prolific workers generating many examples (Geva et al., 2019). We follow their proposal and separate training set annotators from test set annotators, and find that models have difficulty generalizing to new workers. 5 Development EM F1 EM Test F1 RoBERTa BASE - background 38.0 40.7 53.5 59.3 35.8 33.7 45.5 46.1 RoBERTa LARGE - background + RACE 59.7 48.7 60.1 70.2 55.2 73.5 55.4 53.6 55.5 61.1 60.4 61.6 Human - - 82.7 89.0 Table 5: Performance of baselines and human performance on the dev and test set. Baseline performance We use the RoBERTa question answering model proposed by Liu et al. (2019) as our baseline and concatenate the background and situat"
D19-5808,Q19-1026,0,\N,Missing
D19-5815,P17-1147,0,0.0660527,"Missing"
D19-5815,W13-2322,0,0.0634753,"Missing"
D19-5815,P16-1223,0,0.0995681,"Missing"
D19-5815,P18-1078,1,0.890843,"Missing"
D19-5815,Q18-1023,0,0.0674018,"Missing"
D19-5815,N19-1300,0,0.0236782,"al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these sorts of phenomena could be incredibly interesting, and very challenging. 4 Ways"
D19-5815,Q19-1026,0,0.011731,"swering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these sorts of phenomena could be incredibly interest"
D19-5815,P19-1612,0,0.0261817,"a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these"
D19-5815,D19-1606,1,0.90781,"ets that we construct. Paragraph-level structure While the input to a reading comprehension dataset is a paragraph of text, most datasets do not explicitly target questions that require understanding the entire paragraph, or how the sentences fit together into a coherent whole. Some post-hoc analyses attempt to reveal the percentage of questions that require more than one sentence, but it is better to design the datasets from the beginning to obtain questions that look at paragraph- or discourse-level phenomena, such as entity tracking, discourse relations, or pragmatics. For example, Quoref (Dasigi et al., 2019) is a dataset that targets entity tracking and coreference resolution. There are few linguistic formalisms targeting structures larger than a paragraph, but those that do exist, such as rhetorical structure theory (Mann and Thompson, 1988), could form the basis of an interesting and useful reading comprehension dataset. Sentence-level linguistic structure Most existing reading comprehension datasets implicitly target local predicate-argument structures. The incentives involved in the creation of SQuAD encouraged workers to create questions that were close paraphrases of some part of a paragrap"
D19-5815,D19-5808,1,0.90061,"Mary. Mary was just diagnosed with cancer. means also understanding that Bill will be sad. In some sense this can be seen as “grounding” the predicates in the text to some prior knowledge that includes the implications of that predicate, but it also includes the more general notion of reconstructing a model of the world being described by the text. There are two datasets that just scratch 107 the surface of this kind of reading: ShARC (Saeidi et al., 2018) requires reading rules and applying them to questions asked by users, though its format is not standard reading comprehension; and ROPES (Lin et al., 2019), which requires reading descriptions of causes and effects and applying them to situated questions. a similar meaning. Examples include NARRA TIVE QA (Koˇcisk` y et al., 2018), where question authors were shown a summary of a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which"
D19-5815,N19-1246,1,0.946639,"et local predicate-argument structures. The incentives involved in the creation of SQuAD encouraged workers to create questions that were close paraphrases of some part of a paragraph, replacing a noun phrase with a question word. This, and other cloze-style question construction, encourages very local reasoning that amounts to finding and then understanding the argument structure of a single sentence. This is an important aspect of meaning, but one could construct much harder datasets than this. One direction to push on linguistic structure is to move beyond locating a single sentence. DROP (Dua et al., 2019) largely involves the same level of linguistic structural analysis as SQuAD, but the questions require combining pieces from several parts of the passage, forcing a more comprehensive analysis of the passage contents. A separate direction one could push on sentence-level linguistic structure in reading comprehension is to target other phenomena than predicate argument structure. There are many rich problems in semantic analysis, such as negation scope, distributive vs. non-distributive coordination, factuality, deixis, briding and empty elements, preposition senses, noun compounds, and many mo"
D19-5815,D10-1123,0,0.0140052,"and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For example, in the question What is the capital of the largest economy in Europe? we would like the largest economy in Europe to be one answer we can use to modify the question to what is the capital of Germany. 4.5 Adversarial construction 4.7 Minimal question pairs ROPES (Lin et al., 2019) borrowed the idea of “minimal pairs” from linguistic analys"
D19-5815,P19-1416,1,0.897168,"easoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For e"
D19-5815,P18-2124,0,0.0366525,"a et al., 2018). Accordingly, more recent reading comprehension datasets are constructed with several different approaches to prevent such shortcuts in order to foster natural language understanding. 4.1 4.2 “No answer” option Most of the reasoning shortcuts in existing datasets arise due to the fact that the system can assume that the answer is guaranteed to exist in the given passage. Removing this assumption and requiring the system to identify whether the question is even answerable from the passage can prevent such shortcuts. One example of this kind of dataset construction is SQ UAD 2.0(Rajpurkar et al., 2018), which asked annotators to read the given passage and write a question which the passage does not contain the answer to but contains a plausible negative answer. A drawback of this approach is that annotators see the passage when asking the question, which can introduce its own biases and shortcuts. An alternative is to combine a “no answer” option with the approach the previous section, where an annotator writes questions without knowing the answer, and another annotator verifies whether they are answerable by the paired passage. Example datasets include N EWS QA (Trischler et al., 2016)4 ,"
D19-5815,D19-1243,0,0.0520439,"Missing"
D19-5815,Q19-1016,0,0.0223257,"ntial means of avoiding reasoning shortcuts. A person is not able to answer a simple question such as How many? without the additional context of a prior question describing what is being counted. Care needs to be taken with this method, however, as some datasets are amenable to input reduction (Feng et al., 2018), where there is only one plausible answer to such a short question. If done well, however, this method provides additional challenges such as clarification, coreference resolution, and aggregation of pieces scattered across conversation history. Q UAC (Choi et al., 2018) and C O QA (Reddy et al., 2019) are two datasets that focus on such setting. 4.4 4.6 One promising means of removing reasoning shortcuts is to encode those shortcuts into a learned system, and use that system to filter out questions that are too easy during dataset construction. DROP (Dua et al., 2019) and Quoref (Dasigi et al., 2019) used a model trained on SQuAD 1.1 (Rajpurkar et al., 2016) as an “adversarial” baseline when having crowd workers write questions. Because the people could see when the system answered their questions correctly, they learned to ask harder questions. This kind of adversarial construction can in"
D19-5815,D17-1215,0,0.0795702,"Missing"
D19-5815,D13-1020,0,0.0492515,"it is not even clear what it means to understand text, or how to judge whether a machine has achieved success at this task. Much recent research in the natural language processing community has converged on an approach to this problem called machine reading comprehension, where a system is given a passage of text and a natural language question that presumably requires some level of “understanding” of the passage in order to answer. While there have been many papers in the last few years studying this basic problem, as far as we are aware, there is no paper formally justifying this approach 1 Richardson et al. (2013) give a good overview of the early history of this approach, but provide only very little justification. 2 Though SQuAD was not nearly the first reading comprehension dataset, its introduction of the span extraction format was innovative and useful, and most new datasets follow its design. 105 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 105–112 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics so this is the definition we choose, while admitting that it is not perfect. Using natural language questions to test comprehens"
D19-5815,P19-1262,0,0.0173168,"ch require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For example, in the question W"
D19-5815,D18-1233,0,0.0558381,"Missing"
D19-5815,P18-1156,0,0.0337327,"the world being described by the text. There are two datasets that just scratch 107 the surface of this kind of reading: ShARC (Saeidi et al., 2018) requires reading rules and applying them to questions asked by users, though its format is not standard reading comprehension; and ROPES (Lin et al., 2019), which requires reading descriptions of causes and effects and applying them to situated questions. a similar meaning. Examples include NARRA TIVE QA (Koˇcisk` y et al., 2018), where question authors were shown a summary of a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al"
D19-5815,N18-1059,1,0.835931,"eems unsatisfying. Overall, however, we believe this is a good method that could be used more widely when collecting reading comprehension datasets. Complex reasoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functi"
D19-5815,P19-1485,1,0.878847,"Missing"
D19-5815,D18-1259,0,0.133395,"l, however, we believe this is a good method that could be used more widely when collecting reading comprehension datasets. Complex reasoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and no"
D19-5815,D18-1009,0,0.0295379,"ose shortcuts into a learned system, and use that system to filter out questions that are too easy during dataset construction. DROP (Dua et al., 2019) and Quoref (Dasigi et al., 2019) used a model trained on SQuAD 1.1 (Rajpurkar et al., 2016) as an “adversarial” baseline when having crowd workers write questions. Because the people could see when the system answered their questions correctly, they learned to ask harder questions. This kind of adversarial construction can introduce its own biases, however, especially if the questions being filtered are generated by machines instead of humans (Zellers et al., 2018). This also makes a dataset dependent on another dataset and model in complex ways, which has both positive and negative aspects to it. In some sense, it is a good thing to get a diverse set of reading comprehension questions, and encoding one dataset’s biases into a model to enforce a different distribution for new datasets helps in collecting diverse datasets. If crowd workers end up simply wordsmithing their questions in order to pass the adversary, however, this seems unsatisfying. Overall, however, we believe this is a good method that could be used more widely when collecting reading com"
D19-5817,W05-0909,0,0.426678,". In this work, we study all mentioned metrics in the context of question answering. BLEU is a precision-based metric developed for evaluating machine translation (Papineni et al., 2001). BLEU scores a candidate by computing the number of n-grams in the candidate that also appear in a reference. n is varied from 1 up to a specified N and the scores for varying n are aggregated with a geometric mean. In this work, we look at BLEU-1 and BLEU-4, where N = 1 and N = 4 respectively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another token if they are the same, are synonyms, or their stems match. The alignment is aggregated into precision and recall values, which are combined into an F-measure score in which more weight is given to recall. ROUGE is an F-measure metric designed for evaluating translation and summarization (Lin, 2004). There are a number of variants of ROUGE however in this work we focus on ROUGE-L. ROUGE-L is computed based on the longest common subsequ"
D19-5817,D18-1454,0,0.0486278,"of ROPES is F1 . A unique characteristic of ROPES is that questions generally present two possible answer choices, one of which is incorrect (Table 1). Because incorrect and correct answers often have some n-gram overlap, we believe F1 will struggle to accurately assign scores (Figure 1b). 4 Models We describe the models used to generate predictions for our datasets. These models have publicly available code and have reasonable performance compared to the current state-of-the-art models. Multi-hop Point Generator For NarrativeQA and SemEval, we use a multi-hop pointer generator (MHPG) model (Bauer et al., 2018)3 . MHPG represents its input using ELMo embeddings. The embeddings are then fed into a sequence of BiDAF (Seo et al., 2017) cells, where the output of one BiDAF cell is fed as the input into another BiDAF cell. This allows multi-hop reasoning over the context. The output layer consists of a generative decoder with a copying mechanism. We evaluate MHPG’s predictions using BLEU-1, BLEU-4, ROUGE-L, METEOR, SMS, BERTScore and Conditional BERTScore. BERT For ROPES, we finetune BERT as a span based QA model following the procedure used for 3 https://github.com/yicheng-w/ CommonSenseMultiHopQA https"
D19-5817,C04-1046,0,0.246488,"Missing"
D19-5817,P19-1264,0,0.0264628,"e (LCS), which searches for the longest co-occurring set of tokens common to both reference and candidate. An advantage of ROUGEL is that no predefined n-gram size is required. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover’s Similarity (SMS) is a recent metric based on earth mover’s distance for evaluated multi-sentence texts such as machinegenerated summaries (Clark et al., 2019) .1 SMS 120 1 https://github.com/eaclark07/sms first computes an embedding for each sentence in a document as an average its ELMo word representations (Peters et al., 2018). A linear program is then solved to obtain the distance of “moving” a candidate document’s sentences to match a reference document. SMS has shown better results over ROUGE-L in evaluating generated summaries and student essays. BERTScore is recent metric for evaluating translation (Zhang et al., 2019).2 BERTScore first obtains BERT representations of each word in the candidate and reference by feeding the candidate and refe"
D19-5817,N19-1423,0,0.033544,"d what batteries it required: two AA-sized batteries . . . Why did they throw away the old batteries? They were no longer useful ROPES 11,202 . . . A catalyst is a chemical that speeds up chemical reactions . . . [Mark] conducts two tests, test A and test B, on an organism. In test A he reduces catalysts from the organism, but in test B he induces catalysts in the organism ... Which test would see reactions taking place slower, test A or test B? test A Table 1: Examples for the datasets we use in our study. The # of QA Pairs column refers to the number of QA pairs in the training sets. SQuAD (Devlin et al., 2019). We evaluate BERT’s predictions using F1 , SMS, BERTScore, and Conditional BERTScore. 5 5.1 Evaluating QA Metrics using Human Judgements 5.3 Collecting Human Judgements After training our models on the three datasets, we extract 500, 500, and 300 data points from the validation sets of NarrativeQA, ROPES, and SemEval, respectively, along with the model predictions. When extracting data points to label, we filter out data points where the predicted answer exactly matches the gold answer. This filtering step is done as we are interested on how well metrics do when it cannot resort to exact stri"
D19-5817,N19-1246,1,0.798824,". Figure 1: Examples where existing n-gram based metrics fail to align with human judgements. Human judgements are between 1 and 5. (a) illustrates that because existing metrics do not use the context, they fail to capture coreferences. (b) illustrates that changing a single token can make a prediction incorrect while F1 assigns a non-zero score. Introduction Question answering (QA) has emerged as a burgeoning research field driven by the availability of large datasets. These datasets are built to test a variety of reading comprehension skills such as multihop (Welbl et al., 2017), numerical (Dua et al., 2019), and commonsense (Talmor et al., 2018) reasoning. A key component of a QA dataset is the evaluation metric associated with it, which aims to automatically approximate human accuracy judgments of a predicted answer against a gold answer. The metrics used to evaluate QA datasets have a number of ramifications. The first is that they drive research focus. Models that rank higher on a leaderboard according to a metric will receive 119 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 119–124 c Hong Kong, China, November 4, 2019. 2019 Association for Computational"
D19-5817,W04-1013,0,0.0728504,"ively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another token if they are the same, are synonyms, or their stems match. The alignment is aggregated into precision and recall values, which are combined into an F-measure score in which more weight is given to recall. ROUGE is an F-measure metric designed for evaluating translation and summarization (Lin, 2004). There are a number of variants of ROUGE however in this work we focus on ROUGE-L. ROUGE-L is computed based on the longest common subsequence (LCS), which searches for the longest co-occurring set of tokens common to both reference and candidate. An advantage of ROUGEL is that no predefined n-gram size is required. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover"
D19-5817,D19-5808,1,0.902171,"t is crucial that the metrics we use are able to assign scores that accurately reflect human judgements. Despite the value of metrics as drivers of research, a comprehensive study of QA metrics across a number of datasets has yet to be completed. This is important as present metrics are based on n-gram matching, which have a number of shortcomings (Figure 1). In this work, we survey the landscape of evaluation metrics for QA and study how well current metrics approximate (i.e. correlate with) human judgements. We conduct our study on three datasets: NarrativeQA (Kocisk´y et al., 2017), ROPES (Lin et al., 2019), and SemEval-2018 Task 11 (Ostermann et al., 2018). For the generative NarrativeQA dataset, we find that existing metrics provide reasonable correlation with human accuracy judgements while still leaving considerable room for improvement. We also study the span-based ROPES dataset, finding that it presents an interesting case where F1 struggles due to the high overlap in right and wrong answers. Finally, we convert the multiple-choice SemEval-2018 Task 11 dataset into a generative QA dataset. This produces a more difficult generative QA dataset compared to NarrativeQA as answers in SemEval ar"
D19-5817,D18-1429,0,0.0405241,"/question/answer triples during its pretraining. Finetuning a BERT model on QA datasets can potentially yield a better BERTScore-based metric. 6 Related Work N-gram based metrics such as BLEU and METEOR were originally developed and tested for evaluation of machine translation. These metrics have grown to become popular choices in evaluating all forms of natural language generation, including image captioning, question answering, and dialog systems. As these metrics continue to be used, there have been a number of papers that try to assess how suitable these metrics are for different domains. Nema and Khapra (2018) show that for question generation, n-gram metrics assign scores that correlate poorly to the notion of answerability (i.e., is a generated question answerable). Yang et al. (2018) study the effect of using BLEU and ROUGE in evaluating QA, focusing on yes-no and entity questions on the Chinese DuReader dataset (He et al., 2017). For these types of questions, changing a single word from a gold answer can lead to an incorrect answer. In these cases, BLEU and ROUGE assign scores that do not necessarily reflect the correctness of an answer. Our work is continuation of this line of work in assessin"
D19-5817,S18-1119,0,0.147813,"to assign scores that accurately reflect human judgements. Despite the value of metrics as drivers of research, a comprehensive study of QA metrics across a number of datasets has yet to be completed. This is important as present metrics are based on n-gram matching, which have a number of shortcomings (Figure 1). In this work, we survey the landscape of evaluation metrics for QA and study how well current metrics approximate (i.e. correlate with) human judgements. We conduct our study on three datasets: NarrativeQA (Kocisk´y et al., 2017), ROPES (Lin et al., 2019), and SemEval-2018 Task 11 (Ostermann et al., 2018). For the generative NarrativeQA dataset, we find that existing metrics provide reasonable correlation with human accuracy judgements while still leaving considerable room for improvement. We also study the span-based ROPES dataset, finding that it presents an interesting case where F1 struggles due to the high overlap in right and wrong answers. Finally, we convert the multiple-choice SemEval-2018 Task 11 dataset into a generative QA dataset. This produces a more difficult generative QA dataset compared to NarrativeQA as answers in SemEval are often more free-form in nature and have less over"
D19-5817,2001.mtsummit-papers.68,0,0.0372419,"ghtly improves results when evaluating generative QA, though not to an extant that is statistically significant. Overall, our results indicate that studying the evaluation of QA is an underresearched area with substantial room for further experimentation. 2 Metrics We provide a summary of popular n-gram based metrics, as well as sentence mover’s similarity, BERTScore, and an extension of BERTScore which we call conditional BERTScore. In this work, we study all mentioned metrics in the context of question answering. BLEU is a precision-based metric developed for evaluating machine translation (Papineni et al., 2001). BLEU scores a candidate by computing the number of n-grams in the candidate that also appear in a reference. n is varied from 1 up to a specified N and the scores for varying n are aggregated with a geometric mean. In this work, we look at BLEU-1 and BLEU-4, where N = 1 and N = 4 respectively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another toke"
D19-5817,N18-1202,1,0.468177,"ired. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover’s Similarity (SMS) is a recent metric based on earth mover’s distance for evaluated multi-sentence texts such as machinegenerated summaries (Clark et al., 2019) .1 SMS 120 1 https://github.com/eaclark07/sms first computes an embedding for each sentence in a document as an average its ELMo word representations (Peters et al., 2018). A linear program is then solved to obtain the distance of “moving” a candidate document’s sentences to match a reference document. SMS has shown better results over ROUGE-L in evaluating generated summaries and student essays. BERTScore is recent metric for evaluating translation (Zhang et al., 2019).2 BERTScore first obtains BERT representations of each word in the candidate and reference by feeding the candidate and reference through a BERT model separately. An alignment is then computed between candidate and reference words by computing pairwise cosine similarity. This alignment is then a"
D19-5817,W18-2611,0,0.0400521,"as BLEU and METEOR were originally developed and tested for evaluation of machine translation. These metrics have grown to become popular choices in evaluating all forms of natural language generation, including image captioning, question answering, and dialog systems. As these metrics continue to be used, there have been a number of papers that try to assess how suitable these metrics are for different domains. Nema and Khapra (2018) show that for question generation, n-gram metrics assign scores that correlate poorly to the notion of answerability (i.e., is a generated question answerable). Yang et al. (2018) study the effect of using BLEU and ROUGE in evaluating QA, focusing on yes-no and entity questions on the Chinese DuReader dataset (He et al., 2017). For these types of questions, changing a single word from a gold answer can lead to an incorrect answer. In these cases, BLEU and ROUGE assign scores that do not necessarily reflect the correctness of an answer. Our work is continuation of this line of work in assessing the quality of current metrics for use in evaluating question answering across a number of datasets. Because of the inherent limitations of n-gram metrics, recent work has focuse"
D19-5820,D19-1606,1,0.916125,"le test bed for exploring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a model for reading compr"
D19-5820,N19-1423,0,0.0550328,"Missing"
D19-5820,N19-1246,1,0.9285,"so it is a suitable test bed for exploring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a mo"
D19-5820,P19-1485,1,0.919946,"for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a model for reading comprehension. We chose datasets that adhere to two main properties: First, we exclude from consideration any multiple choice dataset, as these typically require very different model architectures, and they often have biases in how the dis147 Procee"
D19-5820,D17-1215,0,0.0278269,"name in the question for a different name from the passage to create with high probability a new question with no answer. Table 1: Dataset Statistics from summaries. In addition, crowd workers were required to provide answers that do not have high overlap with the context. In accordance with our format, we only use the version with the summaries as context in our evaluation server. 3 IC SEARs creates minimal changes in word selection or grammar while maintaining the original meaning of the question according to the rules described by Ribeiro et al. (2018). Synthetic Augmentations Prior works (Jia and Liang, 2017) have shown that RC models are brittle to minor perturbations in original dataset. Hence, to test the model’s ability to generalize to out-of-domain syntactic structures and be logically consistent in its answers, we automatically generate questions based on various heuristics. These heuristics fall in two broad categories. 1. The question is paraphrased to a minimal extent to create new syntactic structures, keeping the semantics of the question largely intact and without making any changes to the original context and answer. 2. The predicate-argument structures of a given question-answer pai"
D19-5820,W17-2623,0,0.0616804,"text, particularly dealing with the language of causes and effects. A system is given a background passage, perhaps describing the effects of deforestation on local climate and ecosystems, and a grounded situation involving the knowledge in the background passage, such as, City A has more trees than City B. The questions then require grounding the effects described in the background, perhaps querying which city would more likely have greater ecological diversity. This dataset can be helpful in understanding how to apply the knowledge contained in a passage of text to a new situation. NewsQA (Trischler et al., 2017) dataset focuses on paraphrased questions with predicate-argument structure understanding. To some extent it is similar to DuoRC, however the examples are collected from news articles and offers diverse linguistic structures. This crowd-sourced dataset was created by asking annotators to write questions from CNN/DailyMail articles as context. NarrativeQA (Koˇcisk´y et al., 2018) focuses on understanding temporal reasoning among various events that happen in a given movie plot. It also tests the models ability to “hop” between various parts of the context and not rely solely on sequential reaso"
D19-5820,Q18-1023,0,0.0906264,"Missing"
D19-5820,D19-5808,1,0.90129,"ring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a model for reading comprehension. We chose"
D19-5820,D17-2014,0,0.0248808,"s a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. We differ from GLUE and S ENT E VAL by focusing on reading comprehension tasks, and only evaluating a single model on all datasets, instead of allowing the model to be tuned to each dataset separately. Evaluation Platforms and Competitions in NLP The use of online evaluation platform with private test labels has been exercised by various leaderboards on Kaggle and CodaLab, as well as shared tasks at the SemEval and CoNLL conferences. Additional benchmarks such as PARL AI (Miller et al., 2017) and BA B I (Weston et al., 2016) proposed a hierarchy of tasks towards building question answering and reasoning models and language understanding. However these frameworks do not include a standardized evaluation suite for system performance nor do they offer a wide set of reading comprehension tasks. 6 Conclusion We have presented ORB, an open reading benchmark designed to be a comprehensive test of reading comprehension systems, in terms of their gen152 eralizability, understanding of various natural language phenomenon, capability to make consistent predictions, and ability to handle out-"
D19-5820,P18-2124,0,0.0398538,"ept where noted, we include both the development and test sets (including hidden test sets) in our evaluation server for all datasets. SQuAD (Rajpurkar et al., 2016) requires a model to perform lexical matching between the context and the question to predict the answer. This dataset provides avenues to learn predicate-argument structure and multi-sentence reasoning to some extent. It was collected by asking crowd-workers to create question-answer pairs from Wikipedia articles such that the answer is a single-span in the context. The dataset was later updated to include unanswerable questions (Rajpurkar et al., 2018), giving a harder question set without as many reasoning shortcuts. We include only the development sets of SQuAD 1.1 and SQuAD 2.0 in our evaluation server. DuoRC (Saha et al., 2018) tests if the model can generalize to answering semantically similar but syntactically different paraphrased questions. The questions are created on movie summaries obtained from two sources, Wikipedia and IMDB. The crowd-workers formalized questions based on Wikipedia contexts and in turn answered them based on the IMDB context. This ensured that the model will not rely solely on lexical matching, but rather util"
D19-5820,D16-1264,0,0.387419,"how models are trained, so it is a suitable test bed for exploring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable"
D19-5820,P19-1621,1,0.842061,"f-domain syntactic structures and be logically consistent in its answers, we automatically generate questions based on various heuristics. These heuristics fall in two broad categories. 1. The question is paraphrased to a minimal extent to create new syntactic structures, keeping the semantics of the question largely intact and without making any changes to the original context and answer. 2. The predicate-argument structures of a given question-answer pair are leveraged to create new WH-questions based on the object in the question instead of the subject. This rulebased method, adopted from (Ribeiro et al., 2019), changes the question and answer keeping the context fixed. We use five augmentation techniques, where the first four techniques fall into the first category and the last technique falls into the second category. Invert Choice transforms a binary choice question by changing the order in which the choices are presented, keeping the answer the same. More Wrong Choice transforms a binary choice question by substituting the wrong choice in the question with another wrong choice from the passage. Implication creates a new question-answer pair, where the object of the original question is replaced"
D19-5820,P18-1079,1,0.90589,"ets in some cases. We focus on datasets where the core problem is natural language understanding, not information retrieval; models are given a single passage of text and a single question and are required to produce an answer. As our goal is to provide a broad suite of questions that test a single model’s reading ability, we additionally provide synthetic augmentations to some of the datasets in our evaluation server. Several recent papers have proposed question transformations that result in out-of-distribution test examples, helping to judge the generalization capability of reading models (Ribeiro et al., 2018, 2019; Zhu et al., 2019). We collect the best of these, add some of our own, and keep those that generate reasonable and challenging questions. We believe this strategy of evaluating on many datasets, including distribution-shifted synthetic examples, will lead the field towards more robust and comprehensive reading comprehension models. Code for the evaluation server, including a script to run it on the dev sets of these datasets and a leaderboard showing results on their hidden tests, can be found at https://leaderboard. allenai.org/orb Reading comprehension is one of the crucial tasks for"
D19-5820,P18-1156,0,0.0158642,"rform lexical matching between the context and the question to predict the answer. This dataset provides avenues to learn predicate-argument structure and multi-sentence reasoning to some extent. It was collected by asking crowd-workers to create question-answer pairs from Wikipedia articles such that the answer is a single-span in the context. The dataset was later updated to include unanswerable questions (Rajpurkar et al., 2018), giving a harder question set without as many reasoning shortcuts. We include only the development sets of SQuAD 1.1 and SQuAD 2.0 in our evaluation server. DuoRC (Saha et al., 2018) tests if the model can generalize to answering semantically similar but syntactically different paraphrased questions. The questions are created on movie summaries obtained from two sources, Wikipedia and IMDB. The crowd-workers formalized questions based on Wikipedia contexts and in turn answered them based on the IMDB context. This ensured that the model will not rely solely on lexical matching, but rather utilize semantic understanding. The answer can be either a single-span from context or free form text written by the annotator. Quoref (Dasigi et al., 2019) focuses on understanding coref"
D19-5820,W18-5446,0,0.05862,"Missing"
D19-5820,P19-1415,0,0.0115705,"on datasets where the core problem is natural language understanding, not information retrieval; models are given a single passage of text and a single question and are required to produce an answer. As our goal is to provide a broad suite of questions that test a single model’s reading ability, we additionally provide synthetic augmentations to some of the datasets in our evaluation server. Several recent papers have proposed question transformations that result in out-of-distribution test examples, helping to judge the generalization capability of reading models (Ribeiro et al., 2018, 2019; Zhu et al., 2019). We collect the best of these, add some of our own, and keep those that generate reasonable and challenging questions. We believe this strategy of evaluating on many datasets, including distribution-shifted synthetic examples, will lead the field towards more robust and comprehensive reading comprehension models. Code for the evaluation server, including a script to run it on the dev sets of these datasets and a leaderboard showing results on their hidden tests, can be found at https://leaderboard. allenai.org/orb Reading comprehension is one of the crucial tasks for furthering research in na"
N18-1202,P17-1080,0,0.874363,"eneralize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using lan"
N18-1202,Q17-1010,0,0.531036,"eled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual"
N18-1202,D15-1075,0,0.770513,"Missing"
N18-1202,P17-1152,0,0.665298,"- 6, 2018. 2018 Association for Computational Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstr"
N18-1202,Q16-1026,0,0.779805,"Missing"
N18-1202,W14-4012,0,0.0858828,"Missing"
N18-1202,D16-1245,0,0.0232835,"Missing"
N18-1202,D13-1203,0,0.0178787,"Missing"
N18-1202,D17-1206,0,0.793935,"to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mi"
N18-1202,P17-1044,1,0.395137,"Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to pred"
N18-1202,P16-1085,0,0.422547,"nd “player”, “game” as nouns) but concentrated in the sportsrelated senses of “play”. In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM’s context representation of “play” in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence. These observations can be quantified using an 2233 resentations have F1 of 69.0 and are better at WSD then the first layer. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et"
N18-1202,P16-1101,0,0.700562,"compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder. Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important"
N18-1202,J93-2004,0,0.116591,"al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD"
N18-1202,K16-1006,0,0.86068,"eviously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful mo"
N18-1202,N16-1030,0,0.746938,"Missing"
N18-1202,D17-1018,1,0.121179,"Missing"
N18-1202,H94-1046,0,0.123123,"2017). To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task. Using this approach, it is also possible to compare to CoVe, and across each of the individual layers. Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud et al. (2016). To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense. At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training. Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a). Overall, the biLM top layer rep5.3 What information is captured by the biLM’s representations? Since adding ELMo improves task performance over word vec"
N18-1202,E17-1002,0,0.0183651,"Missing"
N18-1202,D14-1113,0,0.0176931,"nnington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed wi"
N18-1202,J05-1004,0,0.0609658,"Missing"
N18-1202,D14-1162,0,0.137952,"the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language u"
N18-1202,P17-1161,1,0.697802,"ting models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems. Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on superv"
N18-1202,W12-4501,0,0.343829,"Missing"
N18-1202,D17-1120,0,0.865474,"der {. . . } Olivia De Havilland signed to do a Broadway play for Garson {. . . } Nearest Neighbors playing, game, games, played, players, plays, player, Play, football, multiplayer Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play . {. . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM. Model WordNet 1st Sense Baseline Raganato et al. (2017a) Iacobacci et al. (2016) CoVe, First Layer CoVe, Second Layer biLM, First layer biLM, Second layer F1 65.9 69.9 70.1 59.4 64.7 67.4 69.0 Model Collobert et al. (2011) Ma and Hovy (2016) Ling et al. (2015) CoVe, First Layer CoVe, Second Layer biLM, First Layer biLM, Second Layer Acc. 97.3 97.6 97.8 93.3 92.8 97.3 96.8 Table 5: All-words fine grained WSD F1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. the task-specific"
N18-1202,E17-1010,0,0.821604,"der {. . . } Olivia De Havilland signed to do a Broadway play for Garson {. . . } Nearest Neighbors playing, game, games, played, players, plays, player, Play, football, multiplayer Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play . {. . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM. Model WordNet 1st Sense Baseline Raganato et al. (2017a) Iacobacci et al. (2016) CoVe, First Layer CoVe, Second Layer biLM, First layer biLM, Second layer F1 65.9 69.9 70.1 59.4 64.7 67.4 69.0 Model Collobert et al. (2011) Ma and Hovy (2016) Ling et al. (2015) CoVe, First Layer CoVe, Second Layer biLM, First Layer biLM, Second Layer Acc. 97.3 97.6 97.8 93.3 92.8 97.3 96.8 Table 5: All-words fine grained WSD F1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. the task-specific"
N18-1202,N16-1114,0,0.0336672,"Missing"
N18-1202,P15-1109,0,0.0952398,"Missing"
N18-1202,C16-1329,0,0.694006,"Missing"
N18-1202,D13-1170,0,0.0814613,"Missing"
N18-1202,P16-2038,0,0.361634,"biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai a"
N18-1202,P10-1040,0,0.0362285,"neural machine translation encoder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform 2227 Proceedings of NAACL-HLT 2018, pages 2227–2237 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors f"
N18-1202,P17-1018,0,0.676855,"Missing"
N18-1202,D16-1157,0,0.311052,"from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approache"
N18-1202,D15-1176,0,\N,Missing
N18-1202,W13-3516,0,\N,Missing
N18-1202,D16-1264,0,\N,Missing
N19-1112,E17-2039,0,0.0259669,"Missing"
N19-1112,J99-2004,0,0.205453,"dependently for each token (Belinkov et al., 2017a,b; Blevins et al., 2018, inter alia). We synthesize these disparate studies and build upon them by proposing additional probing tasks. The part-of-speech tagging (POS) task assesses whether CWRs capture basic syntax. We experiment with two standard datasets: the Penn Treebank (PTB; Marcus et al., 1993) and the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014). The CCG supertagging (CCG) task assesses the vectors’ fine-grained information about the syntactic roles of words in context. It is considered “almost parsing” (Bangalore and Joshi, 1999), since a sequence of supertags maps a sentence to a small set of possible parses. We use CCGbank (Hockenmaier and Steedman, 2007), a conversion of the PTB into CCG derivations. The syntactic constituency ancestor tagging tasks are designed to probe the vectors’ knowledge of hierarchical syntax. For a given word, the probing model is trained to predict the constituent la2 http://nelsonliu.me/papers/ contextual-repr-analysis 1074 where a prediction is made only for tokens corresponding to events (rather than every token in a sequence). Performance is measured using Pearson correlation (r); we r"
N19-1112,P17-1080,1,0.922231,"). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology (Belinkov et al., 2017a). We extend prior work by studying CWRs with a diverse set of sixteen probing tasks designed to assess a wide array of phenomena, such as coreference, knowledge of semantic relations, and entity information, among 1073 Proceedings of NAACL-HLT 2019, pages 1073–1094 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics others. The result is a broader view of the linguistic knowledge encoded within CWRs. With respect to transferability, pretraining contextualizers on the language modeling task has had the most empirical success, but we can also conside"
N19-1112,I17-1001,1,0.899042,"Missing"
N19-1112,C16-1333,0,0.0322078,"ion task. Prepositions are marked by boldface, immediately followed by their labeled function. If applicable, ; precedes the preposition’s labeled role. Figure reproduced from Schneider et al. (2018). bel of its parent (Parent), grandparent (GParent), or great-grandparent (GGParent) in the phrasestructure tree (from the PTB). In the semantic tagging task (ST), tokens are assigned labels that reflect their semantic role in context. These semantic tags assess lexical semantics, and they abstract over redundant POS distinctions and disambiguate useful cases within POS tags. We use the dataset of Bjerva et al. (2016); the tagset has since been developed as part of the Parallel Meaning Bank (Abzianidze et al., 2017). Preposition supersense disambiguation is the task of classifying a preposition’s lexical semantic contribution (the function; PS-fxn) and the semantic role or relation it mediates (the role; PSrole). This task is a specialized kind of word sense disambiguation, and examines one facet of lexical semantic knowledge. In contrast to the tagging tasks above, the model is trained and evaluated on single-token prepositions (rather than making a decision for every token in a sequence). We use the STRE"
N19-1112,P18-2003,0,0.156618,"nguage modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology (Belinkov et al., 2017a). We extend prior work by studying CWRs with a diverse set of sixteen probing tasks designed to assess a wide array of phenomena, such as coreference, knowledge of semantic relations, and entity information, among 1073 Proceedings of NAACL-HLT 2019, pages 1073–1094 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics others. The result is a broader view of the linguistic knowledge encoded within CWRs. With respect to transferability, pretraining contextualizers on the language modeling task has had the most empir"
N19-1112,P18-1246,0,0.0610796,"Missing"
N19-1112,P18-1008,0,0.0289435,"middle layers show varying performance. Across all models, the representations that are better-suited for language modeling are also those that exhibit worse probing task performance (Figure 3), indicating that contextualizer layers trade off between encoding general and task-specific features. These results also reveal a difference in the layerwise behavior of LSTMs and transformers; moving up the LSTM layers yields more taskspecific representations, but the same does not hold for transformers. Better understanding the differences between transformers and LSTMs is an active area of research (Chen et al., 2018; Tang et al., 2018), and we leave further exploration of these observations to future work. These observations motivate the gradual unfreezing method of Howard and Ruder (2018), where the model layers are progressively unfrozen (starting from the final layer) during the finetuning process. Given our observation that higherlevel LSTM layers are less general (and more pretraining task-specific), they likely have to be finetuned a bit more in order to make them appropriately task specific. Meanwhile, the base layer of the LSTM already learns highly transferable features, and may not benefit from"
N19-1112,W18-2501,1,0.861441,"Missing"
N19-1112,D17-1206,0,0.0593364,"Missing"
N19-1112,N16-1026,0,0.0599268,"Missing"
N19-1112,Q16-1037,0,0.0737692,"a variety of other methods to study the learned representations in neural models, such as directly examining the activations of individual neurons (Karpathy et al., 2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and dataset (Kuncoro et al., 2017; Gaddy et al., 2018; Khandelwal et al., 2018), or interpreting attention mechanisms (Bahdanau et al., 2015); see Belinkov and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve if it captures a particular phenomenon (Linzen et al., 2016; Jumelet and Hupkes, 2018; Wilcox et al., 2018; Futrell and Levy, 2019, inter alia). Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods diff"
N19-1112,J93-2004,0,0.0674375,"ork in probing the contents of representations.2 See Appendix A for details about task setup. 2.1 Token Labeling The majority of past work in probing the internal representations of neural models has examined various token labeling tasks, where a decision is made independently for each token (Belinkov et al., 2017a,b; Blevins et al., 2018, inter alia). We synthesize these disparate studies and build upon them by proposing additional probing tasks. The part-of-speech tagging (POS) task assesses whether CWRs capture basic syntax. We experiment with two standard datasets: the Penn Treebank (PTB; Marcus et al., 1993) and the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014). The CCG supertagging (CCG) task assesses the vectors’ fine-grained information about the syntactic roles of words in context. It is considered “almost parsing” (Bangalore and Joshi, 1999), since a sequence of supertags maps a sentence to a small set of possible parses. We use CCGbank (Hockenmaier and Steedman, 2007), a conversion of the PTB into CCG derivations. The syntactic constituency ancestor tagging tasks are designed to probe the vectors’ knowledge of hierarchical syntax. For a given word, the probing m"
N19-1112,S15-2153,0,0.101468,"Missing"
N19-1112,D14-1162,0,0.0883484,"and what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results. 1 Figure 1: An illustration of the probing model setup used to study the linguistic knowledge within contextual word representations. Introduction Pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component of state-of-the-art neural NLP models. Traditionally, these word vectors are static—a single * Work done while at the Allen Institute for Artificial Intelligence. vector is assigned to each word. Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context. CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on tasks with large datasets, such as machine translation (McCann et al., 20"
N19-1112,W19-4302,1,0.768829,"k, since such information is learnable by a task-specific contextualizer. This analysis also reveals insights about contextualizer fine-tuning, which seeks to specialize the CWRs for an end task (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). Our results confirm that task-trained contextualization is important when the end task requires specific information that may not be captured by the pretraining task (§4). However, such end-task– specific contextualization can come from either fine-tuning CWRs or using fixed output features as inputs to a task-trained contextualizer; Peters et al. (2019) begins to explore when each approach should be applied. 5 Analyzing Layerwise Transferability We quantify the transferability of CWRs by how well they can do on the range of probing tasks— representations that are more transferable will perform better than alternatives across tasks. When analyzing the representations produced by each layer of pretrained contextualizers, we observe marked patterns in layerwise transferability (Figure 3). The first layer of contextualization in recurrent models (original and 4-layer ELMo) is consistently the most transferable, even outperforming a scalar mix of"
N19-1112,N18-1202,1,0.897768,"te-of-the-art neural NLP models. Traditionally, these word vectors are static—a single * Work done while at the Allen Institute for Artificial Intelligence. vector is assigned to each word. Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context. CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on tasks with large datasets, such as machine translation (McCann et al., 2017) and language modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology"
N19-1112,D18-1179,1,0.89357,"te-of-the-art neural NLP models. Traditionally, these word vectors are static—a single * Work done while at the Allen Institute for Artificial Intelligence. vector is assigned to each word. Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context. CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on tasks with large datasets, such as machine translation (McCann et al., 2017) and language modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology"
N19-1112,W12-4501,0,0.0474045,"arsing, which score pairs of CWRs to make head attachment and arc labeling decisions (Dozat and Manning, 2016, 2018). To generate negative examples for the dependency arc prediction tasks, we take each positive example (whead , wmod ) and generate a new negative example (wrand , wmod ). wrand is a random token in the sentence that is not the head of wmod . Thus, the datasets used in these tasks are balanced. We also consider a coreference arc prediction task, where the model is trained to predict whether two entities corefer from their CWRs. We use the dataset from the CoNLL 2012 shared task (Pradhan et al., 2012). To generate negative examples, we follow a similar procedure as the dependency arc prediction tasks: given a positive example (wa , wb ), where wb occurs after wa and the two tokens share a coreference cluster, we create a negative example (wrandom entity , wb ), where wrandom entity is a token that occurs before wb and belongs to a different coreference cluster. 3 Models Probing Model We use a linear model as our probing model; limiting its capacity enables us to focus on what information can be easily extracted from CWRs. See Appendix B for probing model training hyperparameters and other"
N19-1112,N19-1162,0,0.032501,"yzing Layerwise Transferability We quantify the transferability of CWRs by how well they can do on the range of probing tasks— representations that are more transferable will perform better than alternatives across tasks. When analyzing the representations produced by each layer of pretrained contextualizers, we observe marked patterns in layerwise transferability (Figure 3). The first layer of contextualization in recurrent models (original and 4-layer ELMo) is consistently the most transferable, even outperforming a scalar mix of layers on most tasks (see Appendix D for scalar mix results). Schuster et al. (2019) see the same trend in English dependency parsing. By contrast, transformer-based contextualizers have no single most-transferable layer; the best performing layer for each task varies, and is usually near the middle. Accordingly, a scalar mix of transformer layers outperforms the best individual layer on most tasks (see Appendix D). Pretraining encourages the model to encode pretraining-task–specific information; they learn transferable features incidentally. We hypothesize that this is an inherent trade-off—since these models used fixed-sized vector representations, taskspecificity comes at"
N19-1112,D16-1248,0,0.459106,"ter understanding the linguistic knowledge and transferability of CWRs is necessary for their principled enhancement through new encoder architectures and pretraining tasks that build upon their strengths or alleviate their weaknesses (Linzen, 2018). This paper asks and answers: 1. What features of language do these vectors capture, and what do they miss? (§4) 2. How and why does transferability vary across representation layers in contextualizers? (§5) 3. How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? (§6) We use probing models1 (Shi et al., 2016b; Adi et al., 2017; Hupkes et al., 2018; Belinkov and Glass, 2019) to analyze the linguistic information within CWRs. Concretely, we generate features for words from pretrained contextualizers and train a model to make predictions from those features alone (Figure 1). If a simple model can be trained to predict linguistic information about a word (e.g., its part-of-speech tag) or a pair of words (e.g., their semantic relation) from the CWR (s) alone, we can reasonably conclude that the CWR (s) encode this information. Our analysis reveals interesting insights such as: 1. Linear models trained"
N19-1112,D16-1159,0,0.598727,"ter understanding the linguistic knowledge and transferability of CWRs is necessary for their principled enhancement through new encoder architectures and pretraining tasks that build upon their strengths or alleviate their weaknesses (Linzen, 2018). This paper asks and answers: 1. What features of language do these vectors capture, and what do they miss? (§4) 2. How and why does transferability vary across representation layers in contextualizers? (§5) 3. How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? (§6) We use probing models1 (Shi et al., 2016b; Adi et al., 2017; Hupkes et al., 2018; Belinkov and Glass, 2019) to analyze the linguistic information within CWRs. Concretely, we generate features for words from pretrained contextualizers and train a model to make predictions from those features alone (Figure 1). If a simple model can be trained to predict linguistic information about a word (e.g., its part-of-speech tag) or a pair of words (e.g., their semantic relation) from the CWR (s) alone, we can reasonably conclude that the CWR (s) encode this information. Our analysis reveals interesting insights such as: 1. Linear models trained"
N19-1112,P11-1019,0,0.0388573,"ang and Buchholz, 2000). Named entity recognition (NER) examines whether CWRs encode information about entity types. We use the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). Grammatical error detection (GED) is the task of identifying tokens which need to be edited in order to produce a grammatically correct sentence. Given that CWRs are extracted from models trained on large amounts of grammatical text, this task assesses whether embeddings encode features that indicate anomalies in their input (in this case, ungrammaticality). We use the First Certificate in English (Yannakoudakis et al., 2011) dataset, converted into sequence-labeling format by Rei and Yannakoudakis (2016). The conjunct identification (Conj) task challenges the model to identify the tokens that comprise the conjuncts in a coordination construction. Doing so requires highly specific syntactic knowledge. The data comes from the coordinationannotated PTB of Ficler and Goldberg (2016). 2.3 Pairwise Relations We also design probing tasks that examine whether relationships between words are encoded in CWRs. In these tasks, given a word pair w1 , w2 , we input [w1 , w2 , w1 w2 ] into the probing model; it is trained to pr"
N19-1112,N18-1089,0,0.0423259,"Missing"
N19-1112,W18-5448,0,0.0345799,"ning the activations of individual neurons (Karpathy et al., 2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and dataset (Kuncoro et al., 2017; Gaddy et al., 2018; Khandelwal et al., 2018), or interpreting attention mechanisms (Bahdanau et al., 2015); see Belinkov and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve if it captures a particular phenomenon (Linzen et al., 2016; Jumelet and Hupkes, 2018; Wilcox et al., 2018; Futrell and Levy, 2019, inter alia). Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods differ from our approach in that they require no extra parameters and directly assess the vectors, while our prob"
N19-1112,silveira-etal-2014-gold,0,0.135261,"Missing"
N19-1112,D18-1458,0,0.0621572,"Missing"
N19-1112,W00-0726,0,0.0824119,"Decompositional Semantics It Happened v2 dataset (Rudinger et al., 2018), and the model is trained to predict a (non)factuality value in the range [−3, 3]. Unlike the tagging tasks above, this task is treated as a regression problem, 2.2 Segmentation Several of our probing tasks involve segmentation using BIO or IO tags. Here the model is trained to predict labels from only a single word’s CWR. Syntactic chunking (Chunk) tests whether CWR s contain notions of spans and boundaries; the task is to segment text into shallow constituent chunks. We use the CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000). Named entity recognition (NER) examines whether CWRs encode information about entity types. We use the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). Grammatical error detection (GED) is the task of identifying tokens which need to be edited in order to produce a grammatically correct sentence. Given that CWRs are extracted from models trained on large amounts of grammatical text, this task assesses whether embeddings encode features that indicate anomalies in their input (in this case, ungrammaticality). We use the First Certificate in English (Yannakoudakis et al., 2"
N19-1112,D15-1243,0,0.0265978,"slation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods differ from our approach in that they require no extra parameters and directly assess the vectors, while our probing models must be trained. In this regard, our method is similar to QVEC (Tsvetkov et al., 2015). 8 Conclusion We study the linguistic knowledge and transferability of contextualized word representations with a suite of sixteen diverse probing tasks. The features generated by pretrained contextualizers are sufficient for high performance on a broad set of tasks. For tasks that require specific information not captured by the contextual word representation, we show that learning task-specific contextual features helps to encode the requisite knowledge. In addition, our analysis of patterns in the transferability of contextualizer layers shows that the lowest layer of LSTMs encodes the mos"
N19-1112,W18-5423,0,0.0214566,"d representations in neural models, such as directly examining the activations of individual neurons (Karpathy et al., 2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and dataset (Kuncoro et al., 2017; Gaddy et al., 2018; Khandelwal et al., 2018), or interpreting attention mechanisms (Bahdanau et al., 2015); see Belinkov and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve if it captures a particular phenomenon (Linzen et al., 2016; Jumelet and Hupkes, 2018; Wilcox et al., 2018; Futrell and Levy, 2019, inter alia). Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods differ from our approach in that they require no ex"
N19-1112,D17-1197,0,0.019245,"WR do not capture much transfer3 See Appendix C for references to the previous state of the art (without pretraining). 4 For brevity, in this section we omit probing tasks that cannot be compared to prior work. See Appendix D for pretrained contextualizer performance for all layers and all tasks. able information about entities and coreference phenomena in their input (e.g., the NER results in Table 1 and the coreference arc prediction results in Appendix D). To alleviate this weakness, future work could augment pretrained contextualizers with explicit entity representations (Ji et al., 2017; Yang et al., 2017; Bosselut et al., 2017). Probing Failures While probing models are at or near state-of-the-art performance across a number of tasks, they also do not perform as well on several others, including NER, grammatical error detection, and conjunct identification. This may occur because (1) the CWR simply does not encode the pertinent information or any predictive correlates, or (2) the probing model does not have the capacity necessary to extract the information or predictive correlates from the vector. In the former case, learning task-specific contextual features might be necessary for encoding t"
N19-1112,S14-2008,0,\N,Missing
N19-1112,J12-2002,0,\N,Missing
N19-1112,W03-0419,0,\N,Missing
N19-1112,J12-2003,0,\N,Missing
N19-1112,J07-3004,0,\N,Missing
N19-1112,P16-1079,0,\N,Missing
N19-1112,P16-1112,0,\N,Missing
N19-1112,E17-1117,1,\N,Missing
N19-1112,P18-2077,0,\N,Missing
N19-1112,Q19-1004,1,\N,Missing
N19-1112,N19-1423,0,\N,Missing
N19-1112,N16-1082,0,\N,Missing
N19-1246,D13-1160,0,0.349224,"nding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly."
N19-1246,N18-2017,0,0.0831929,"Missing"
N19-1246,P17-1044,0,0.0139727,"edicates and assigns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap is did yea r th e"
N19-1246,D14-1058,0,0.0307579,"complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and L"
N19-1246,D15-1075,0,0.0729969,"-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span. 2373 (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline. 5.3 Heuristic Baselines A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only predict answer spans from either the question or the paragraph. In addition, we devise a baseline that estimates the answer"
N19-1246,P17-1147,0,0.11449,"Missing"
N19-1246,W05-0620,0,0.13903,"Missing"
N19-1246,D18-1546,0,0.0390583,"ion, while the score for BERT is based on an open-source implementation from Hugging Face: https://github.com/huggingface/pytorch-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span. 2373 (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline. 5.3 Heuristic Baselines A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only"
N19-1246,P16-1223,0,0.146227,"Missing"
N19-1246,P18-1078,1,0.903373,"Missing"
N19-1246,N19-1423,0,0.270861,"Missing"
N19-1246,K17-3002,0,0.0174839,"es for polysemous predicates and assigns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap i"
N19-1246,N18-1023,0,0.0470213,"w in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we"
N19-1246,Q18-1023,0,0.0584383,"Missing"
N19-1246,D17-1160,1,0.897774,"ing Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP’s questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the W IKI TABLE Q UESTIONS tabular dataset (Pasupat and Liang, 2015). Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntactic relations, (2) Open Information Extraction (Banko et al., 2007, Open IE), a shallow semantic representation which directly links predicates and arguments; and (3) Semantic Role Labeling (Carreras and M`arquez, 2005, SRL), which disambiguates senses for polysemous predicates and assigns predicate-sp"
N19-1246,P14-1026,0,0.0365246,"e of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions"
N19-1246,P17-1003,0,0.0395215,"; Zellers et al., 2018; Zhang et al., 2019; Zellers et al., 2019). In our case, instead of using an adversarial baseline to filter automatically generated examples, we use it in a crowdsourcing task, to teach crowd workers to avoid easy questions, raising the difficulty level of the questions they provide. Neural symbolic reasoning DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning. We present one such model in Section 6. Other related work along these lines has been done by Reed and de Freitas (2016), Neelakantan et al. (2016), and Liang et al. (2017). 3 DROP Data Collection In this section, we describe our annotation protocol, which consists of three phases. First, we automatically extract passages from Wikipedia which are expected to be amenable to complex questions. Second, we crowdsource question-answer pairs on these passages, eliciting questions which require 2370 discrete reasoning. Finally, we validate the development and test portions of DROP to ensure their quality and report inter-annotator agreement. Passage extraction We searched Wikipedia for passages that had a narrative sequence of events, particularly with a high proportio"
N19-1246,P17-1015,0,0.0311548,"s of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tab"
N19-1246,W08-1301,0,0.0643822,"Missing"
N19-1246,D18-1260,0,0.0265468,"nswering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved,"
N19-1246,K18-1007,0,0.0469015,"Missing"
N19-1246,N18-1144,0,0.0225428,"uestion Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single"
N19-1246,L18-1564,0,0.0210105,"aset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired w"
N19-1246,D18-1258,0,0.0257774,"ng conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true;"
N19-1246,P16-1144,0,0.0594744,"Missing"
N19-1246,P15-1142,0,0.251793,"et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly. Adversarial dataset construction We continue 1 Some questions in our dataset require limited sports domain knowledge to answer; we expect that there are enough such ques"
N19-1246,D14-1162,0,0.0816178,"Missing"
N19-1246,N18-1202,1,0.603282,"Missing"
N19-1246,P18-2124,0,0.0476756,"t can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned"
N19-1246,Q19-1016,0,0.0743529,"Missing"
N19-1246,N18-1081,1,0.786086,"gns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap is did yea r th e how many yea rs at wh wa"
N19-1246,N18-1140,0,0.0196887,"e (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple r"
N19-1246,N18-1059,0,0.0558538,"st baseline system. The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud"
N19-1246,Q18-1021,0,0.0624305,"these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1"
N19-1246,D18-1259,0,0.0643269,"ncrease over the best baseline system. The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefi"
N19-1246,P17-1041,0,0.0121247,"d overlap. When an answer has multiple spans, we first perform a one-to-one alignment greedily based on bag-of-word overlap on the set of spans and then compute average F1 over each span. When there are multiple annotated answers, both metrics take a max over all gold answers. 5.1 Semantic Parsing Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP’s questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the W IKI TABLE Q UESTIONS tabular dataset (Pasupat and Liang, 2015). Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntact"
N19-1246,D18-1009,0,0.0711059,"Missing"
N19-1246,P18-1156,0,0.0679837,"s, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these"
N19-1273,Q13-1005,1,0.929968,"Missing"
N19-1273,D13-1160,0,0.754253,"the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision. 1 Introduction Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive"
N19-1273,W18-2501,1,0.861033,"Missing"
N19-1273,P96-1024,0,0.380485,"logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very hard for dynamic MML to bootstrap its way to finding good logical forms. To solve this problem, we interleave static MML, which has a consistent super"
N19-1273,P17-1097,0,0.348125,"tself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very hard for dynamic M"
N19-1273,P17-1167,0,0.0510016,"ntic parsing model itself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very"
N19-1273,P16-1002,0,0.0484511,"our contributions on the NLVR and W IKI TABLE Q UESTIONS datasets. Other work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2"
N19-1273,D16-1032,1,0.829456,"ore details. In addition, we slightly modify the constrained decoding architecture from (Krishnamurthy et al., 2017) to bias the predicted actions towards those that would decrease the value of S(yi , xi ). This is done using a coverage vector, viS for each training instance that keeps track of the production rules triggered by xi , and gets updated whenever one of those desired productions is produced by the decoder. That is, viS is a vector of 1s and 0s, with 1s indicating the triggered productions that are yet to be produced by the decoder. This is similar to the idea of checklists used by Kiddon et al. (2016). The decoder in the original architecture scores output actions at each time step by computing a dot product of the predicted action representation with the embeddings of each of the actions. We add a weighted sum of all the actions that are yet to produced: sai = ea .(pi + γ ∗ viS .E) (4) where sai is the score of action a at time step i, ea is the embedding of that action, pi is the predicted action representation, E is the set of embeddings of all the actions, and γ is a learned parameter for regularizing the bias towards yet-to-be produced triggered actions. 6.2 Experimental setup NLVR We"
N19-1273,D17-1160,1,0.873567,"in frequent search failures early during training when model parameters are close to random, and in general may only yield spurious logical forms in the absence of any guidance. Since modern semantic parsers typically operate without a lexicon, new techniques are essential to provide guidance to the search procedure (Goldman et al., 2018). One way of providing this guidance during search is to perform some kind of heuristic search up front to find a set of logical forms that evaluate to the correct denotation, and use those logical forms to approximate the inner summation (Liang et al., 2011; Krishnamurthy et al., 2017). The particulars of the heuristic search can have a large impact on performance; a smaller candidate set has lower noise, while a larger set makes it more likely that the correct logical form is in it, and one needs to strike the right balance. In this paper, we refer to the MML that does search during training as dynamic MML, and the one that does an offline search as static MML. The main benefit of dynamic MML is that it adapts its training signal over time. As the model learns, it can increasingly focus its probability mass on a small set of very likely logical forms. The main benefit of s"
N19-1273,D11-1140,1,0.781683,"and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with exactly three blocks. (object exists (yellow (top (object in box (member count equals all boxes 3))))) The tower with three blocks has a yellow block over a black block (object count greater equals (yellow (above (black (object in box (member count equals all boxes 3))))) 1) Table 7: Complexity of logical forms produced at"
N19-1273,P17-1003,0,0.0331307,"e denotation given the utterance. The semantic parsing model itself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where t"
N19-1273,P11-1060,0,0.732007,"aning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task. However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms (Berant et al., 2013; Artzi an"
N19-1273,D18-1266,0,0.0200734,"), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangl"
N19-1273,P15-1142,0,0.14764,"Missing"
N19-1273,P16-1003,0,0.420486,"Missing"
N19-1273,P17-1105,0,0.0325794,"r work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all box"
N19-1273,P17-1099,0,0.0230164,"esent modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with exactly three blocks. (object exists"
N19-1273,P06-2101,0,0.125269,"Missing"
N19-1273,N18-1203,0,0.023553,"Missing"
N19-1273,P17-2034,0,0.113175,"Missing"
N19-1273,N18-2071,0,0.019839,"t al., 2011; Berant et al., 2013), or trying to automatically infer logical forms from denotations (Pasupat and Liang, 2016). However, matching the performance of a fully supervised semantic parser with only weak supervision remains a significant challenge (Yih et al., 2016). The main contributions of this work deal with training semantic parsers with weak supervision, and we gave a detailed discussion of related training methods in §2.2. We evaluate our contributions on the NLVR and W IKI TABLE Q UESTIONS datasets. Other work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include M"
N19-1273,P16-1008,0,0.0132738,"018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with"
N19-1273,P16-2033,0,0.0236319,"duce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task. However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms (Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015; Guu et al., 2017, inter alia) but also dealing with spurious logical forms that evaluate to the correct denotation while not being s"
N19-1273,P17-1041,0,0.134843,"omparable setting, and on NLVR with significantly less supervision. 1 Introduction Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required b"
N19-1273,D17-1125,0,0.503711,"Missing"
P18-1078,D17-1215,0,0.0381945,"paragraphs, even when those correct spans have better contextual evidence. than another. Second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists. For example, the model might become too reliant on selecting answers that match semantic type the question is asking about, causing it be easily distracted by other entities of that type when they appear in irrelevant text. This kind of error has also been observed when distractor sentences are added to the context (Jia and Liang, 2017) We experiment with four approaches to training models to produce comparable confidence scores, shown in the following subsections. In all cases we will sample paragraphs that do not contain an answer as additional training points. 3.1 pled from the same context together during training. A paragraph separator token with a learned embedding is added before each paragraph. 3.3 We also experiment with allowing the model to select a special “no-answer” option for each paragraph. First we re-write our objective as: !   e gb esa = − log Pn s − log Pn gj i i=1 e j=1 e − log Pn i=1 esa +gb Pn j=1 e"
P18-1078,D13-1160,0,0.292213,"Missing"
P18-1078,P17-1147,0,0.147887,"phs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA. 1 There are two basic approaches to this task. Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a). Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a). Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph. As we shall show, naively trained models often struggle to meet this requirement. In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results. Then we introduce a method for tra"
P18-1078,P17-1171,0,0.429017,"t output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA. 1 There are two basic approaches to this task. Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a). Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a). Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph. As we shall show, naively trained models often struggle to meet this requirement. In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results. Then we introduce a method for training models to produce accurate per-paragraph confidence scores, and we show how combining this method with multiple paragraph selection further increases"
P18-1078,P16-1086,0,0.0675862,"acclaiming ‘Gordon of Khartoum’, a saint. However, historians have suggested that Gordon... Figure 1: Noisy supervision can cause many spans of text that contain the answer, but are not situated in a context that relates to the question (red), to distract the model from learning from more relevant spans (green). In a distantly supervised setup we label all text spans that match the answer text as being correct. This can lead to training the model to select unwanted answer spans. Figure 1 contains an example. To handle this difficulty, we use a summed objective function similar to the one from Kadlec et al. (2016), that optimizes the negative loglikelihood of selecting any correct answer span. The models we consider here work by independently predicting the start and end token of the answer span, so we take this approach for both predictions. For example, the objective for P predicting the answer start token becomes − log a∈A pa where A is the set of tokens that start an answer and pi is the answer-start probability predicted by the model for token i. This objective has the advantage of being agnostic to how the model distributes probability mass across the possible answer spans, allowing the model to"
P18-1078,P17-2081,0,0.0128605,"Missing"
P18-1078,D16-1053,0,0.0158726,"Missing"
P18-1078,D14-1162,0,0.112732,"agraphs contains an answer span, and both of those paragraphs are included in the same mini-batch. For TriviaQA wiki we repeat the process but use the top 8 paragraphs, and for TriviaQA unfiltered we use the top 16, because much more context is given in these settings. Experimental Setup Datasets 4.4 Implementation We train the model with the Adadelta optimizer (Zeiler, 2012) with a batch size 60 for TriviaQA and 45 for SQuAD. At test time we select the most probable answer span of length less than or equal to 8 for TriviaQA and 17 for SQuAD. The GloVe 300 dimensional word vectors released by Pennington et al. (2014) are used for word embeddings. On SQuAD, we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism. We found for TriviaQA, likely because there is more data, using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial. During training, we maintain an exponential moving average of the weights with a decay rate of 0.999. We use the weight averages at test time. We do not update the word vectors during training. Preprocessing We note that for TriviaQA web we do not subsample as was done by Joshi"
P18-1078,D16-1264,0,0.836082,"ive where paragraphs are processed independently, but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document. This requires the model to produce globally correct output even though each paragraph is processed independently. We evaluate our work on TriviaQA (Joshi et al., 2017) in the wiki, web, and unfiltered setting. Our model achieves a nearly 10 point lead over published prior work. We additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on a modified version of SQuAD (Rajpurkar et al., 2016) where only the correct document, not the correct paragraph, is known. Finally, we combine our model with a web search backend to build a demonstration end-to-end QA system1 , and show it performs well on questions from the TREC question answering task (Voorhees et al., 1999). We release our code2 to facilitate future work. 2 many tokens preceded it, and the number of question words it includes as features. The classifier is trained on the distantly supervised objective of selecting paragraphs that contain at least one answer span. On TriviaQA web, relative to truncating the document as done b"
P18-1078,P16-1145,0,0.0165828,"el reading comprehension abilities of the model, and adding a mechanism to handle document-level coreferences. 6 Related Work Reading Comprehension Datasets. The state of the art in reading comprehension has been rapidly advanced by neural models, in no small part due to the introduction of many large datasets. The first large scale datasets for training neural reading comprehension models used a Cloze-style task, where systems must predict a held out word from a piece of text (Hermann et al., 2015; Hill et al., 2015). Additional datasets including SQuAD (Rajpurkar et al., 2016), WikiReading (Hewlett et al., 2016), MS Marco (Nguyen et al., 2016) and TriviaQA (Joshi et al., 2017) provided more realistic questions. Another dataset of trivia questions, Quasar-T (Dhingra et al., 2017), was introduced recently that uses ClueWeb09 (Callan et al., 2009) as its source for documents. In this work we choose to focus on SQuAD because it is well studied, and TriviaQA because it is more challenging and features documents and multi-document contexts (Quasar T is similar, but was released after we started work on this project). Neural Reading Comprehension. Neural reading comprehension systems typically use some form"
P18-1078,P17-1018,0,0.443699,"lves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA. 1 There are two basic approaches to this task. Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a). Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a). Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph. As we shall show, naively trained models often struggle to meet this requirement. In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results. Then we introduce a method for training models to pro"
P19-1416,P17-1147,1,0.809277,"r most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), or us"
P19-1416,Q18-1023,0,0.0788822,"Missing"
P19-1416,D16-1264,0,0.0696953,"e distractors can recover most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and"
P19-1416,N18-1059,0,0.20532,"difficulty. However, since only one of the ten paragraphs is about an animal, one can immediately locate the answer in Paragraph 1 using one hop. The full example is provided in Appendix A. established to protect?”, and then answer “What is the former name of that animal?”. However, when considering the evidence paragraphs, the question is solvable in a single hop by finding the only paragraph that describes an animal. Introduction Multi-hop reading comprehension (RC) requires reading and aggregating information over multiple pieces of textual evidence (Welbl et al., 2017; Yang et al., 2018; Talmor and Berant, 2018). In this work, we argue that it can be difficult to construct large multi-hop RC datasets. This is because multi-hop reasoning is a characteristic of both the question and the provided evidence; even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. For example, the question in Figure 1 is compositional: a plausible solution is to find “What animal’s habitat was the R´eserve Naturelle Lomako Yokokala ∗ Equal Contribution. Our analysis is centered on H OTPOT QA (Yang et al., 2018), a dataset"
P19-1416,D13-1160,0,\N,Missing
P19-1416,D18-1453,0,\N,Missing
P19-1416,D18-1259,0,\N,Missing
P19-1416,P18-1078,1,\N,Missing
P19-1416,N19-1423,0,\N,Missing
P19-1416,Q18-1021,0,\N,Missing
P19-1448,P17-1005,0,0.0439472,"1 Types are tables, string columns, number columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schema-specific, i.e., a"
P19-1448,D14-1179,0,0.0555322,"Missing"
P19-1448,N19-1240,0,0.0708628,"Missing"
P19-1448,P18-1033,0,0.208367,"Missing"
P19-1448,W18-2501,1,0.823585,"Missing"
P19-1448,P17-1089,0,0.235403,"Missing"
P19-1448,D17-1160,1,0.921424,"Missing"
P19-1448,P17-1105,0,0.0425241,"r columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schema-specific, i.e., a schema item v was generated, gj is a learned emb"
P19-1448,C18-1280,0,0.0545797,"Missing"
P19-1448,P16-1127,0,0.0613657,"y-crafted features. 1 Types are tables, string columns, number columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schem"
P19-1448,P17-1041,0,0.0712993,"string columns, number columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schema-specific, i.e., a schema item v was gene"
P19-1448,D18-1193,0,0.118416,"Missing"
P19-1448,D18-1425,0,0.229563,"Missing"
P19-1598,N16-1024,0,0.0365848,"ring evaluation. Furthermore, as discussed in Section 2.2, the goal in language modelling P is to measure the marginal probability p(x) = E p(x, E) not the joint probability. However, this sum is intractable to compute due to the large combinatorial space of possible annotations. We address this problem by approximating the marginal distribution using importance sampling. Given samples from a proposal distribution q(E|x) the marginal distribution is: p(x) = X p (x, E) = E X p (x, E) 1 X p (x, E) ≈ N q (E|x) E∼q E q (E|x) q (E|x) This approach is used to evaluate models in Ji et al. (2017) and Dyer et al. (2016). Following Ji et al. (2017), we compute q (E|x) using a discriminative version of our model that predicts annotations for the current token instead of for the next token. 5 Experiments To evaluate the proposed language model, we first introduce the baselines, followed by an evaluation using perplexity of held-out corpus, accuracy on fact completion, and an illustration of how the model uses the knowledge graph. 5.1 Evaluation Setup Baseline Models We compare KGLM to the following baseline models: • AWD-LSTM (Merity et al., 2018): strong LSTM-based model used as the foundation of most state-of"
P19-1598,W18-6521,0,0.0146107,"iers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linked WikiText-2 can be compared to models trained on WikiText-2. Furthermore, because many"
P19-1598,W17-3518,0,0.0311906,"ne of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linked WikiText-2 can be compared to models trained on WikiText-2. F"
P19-1598,P16-1154,0,0.0413536,"same distribution over the vocabulary as in Eqn (1) - a softmax using ht,x . If there is an entity to render, we construct the distribution over the original vocabulary and a vocabulary containing all the tokens that appear in aliases of et . This distribution is conditioned on et in addition to xt . To compute the scores over the original vocabulary, ht,x is replaced by h′t,x = Wproj [ht,x ; vet ] where Wproj is a learned weight matrix that projects the concatenated vector into the same vector space as ht,x . To obtain probabilities for words in the alias vocabulary, we use a copy mechanism Gu et al. (2016). The token sequences comprising each alias {aj } are embedded then encoded using an LSTM to form vectors aj . Copy scores are computed as: h   i T p(xt = aj ) ∝ exp σ h′t,x Wcopy aj 3 Linked WikiText-2 Modeling aside, one of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint"
P19-1598,D17-1284,1,0.776783,"ing paragraphs describe each step in detail. Initial entity annotations We begin by identifying an initial set of entity mentions within the text. The primary source of these mentions is the humanprovided links between Wikipedia articles. Whenever a span of text is linked to another Wikipedia article, we associate its corresponding Wikidata entity with the span. While article links provide a large number of gold entity annotations, they are insufficient for capturing all of the mentions in the article since entities are only linked the first time they occur. Accordingly, we use the neural-el (Gupta et al., 2017) entity linker to identify additional links to Wikidata, and identify coreferences using Stanford CoreNLP2 to cover pronouns, nominals, and other tokens missed by the linker. Local knowledge graph The next step iteratively creates a generative story for the entities using relations in the knowledge graph as well as identifies new entities. To do this, we process the text token by token. Each time an entity is encountered, we add all of the related entities in Wikidata as candi5965 2 https://stanfordnlp.github.io/CoreNLP/ Tokens xt Super Mario Land is a Mention type Entity Mentioned Relation Pa"
P19-1598,D17-1195,0,0.355854,"ess to annotations during evaluation. Furthermore, as discussed in Section 2.2, the goal in language modelling P is to measure the marginal probability p(x) = E p(x, E) not the joint probability. However, this sum is intractable to compute due to the large combinatorial space of possible annotations. We address this problem by approximating the marginal distribution using importance sampling. Given samples from a proposal distribution q(E|x) the marginal distribution is: p(x) = X p (x, E) = E X p (x, E) 1 X p (x, E) ≈ N q (E|x) E∼q E q (E|x) q (E|x) This approach is used to evaluate models in Ji et al. (2017) and Dyer et al. (2016). Following Ji et al. (2017), we compute q (E|x) using a discriminative version of our model that predicts annotations for the current token instead of for the next token. 5 Experiments To evaluate the proposed language model, we first introduce the baselines, followed by an evaluation using perplexity of held-out corpus, accuracy on fact completion, and an illustration of how the model uses the knowledge graph. 5.1 Evaluation Setup Baseline Models We compare KGLM to the following baseline models: • AWD-LSTM (Merity et al., 2018): strong LSTM-based model used as the foun"
P19-1598,D16-1128,0,0.220907,") ∝ exp σ h′t,x Wcopy aj 3 Linked WikiText-2 Modeling aside, one of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linke"
P19-1598,P18-1196,0,0.0176554,"are mapped to a single UNK token. This is problematic for comparing the performance of the KGLM to traditional language models on Linked WikiText-2 since there are a large number of rare entities whose alias tokens are outof-vocabulary. That is, even if the KGLM identifies the correct entity and copies the correct alias token with high probability, other models can attain better perplexity by assigning a higher probability to UNK. Accordingly, we also measure unknown penalized perplexity (UPP) (a.k.a adjusted perplexity) introduced by Ueberla (1994), and used recently by Ahn et al. (2016) and Spithourakis and Riedel (2018). This metric penalizes the probability of UNK tokens by evenly dividing their probability mass over U , the set of tokens that get mapped to UNK . We can be compute UPP by replacing p(UNK) in the perplexity above by |U1 |p(UNK), where |U |is estimated from the data. We present the model perplexities in Table 3. To marginalize over annotations, perplexities for the E NTITY NLM, EntityCopyNet, and KGLM are estimated using the importance sampling approach described in Section 4. We observe that the KGLM attains substantially lower perplexity than the other entity-based language models (44.1 vs."
P19-1598,D17-1239,0,0.0535341,"Missing"
P19-1598,D17-1197,0,0.180297,"2 Modeling aside, one of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linked WikiText-2 can be compared to models tr"
W12-3009,D11-1142,0,0.0156784,"a Barack speak president When there are deterministically coreferent mentions, as with appositives, we combine the features from both mentions in preprocessing. We note here also that we use dependency links as features over which to learn distributional semantics because they are the deepest semantic representation that current tools will allow us to use at web scale. We would like to eventually move from dependency links to semantic roles, and to include relations expressed by the sentence or paragraph as features in our model. One possible way of doing that is to use something like ReVerb (Fader et al., 2011), setting its output as the value and an unobserved relation in the knowledge base as the feature type. This would learn distributional information about the textual expression of relations directly, which would also be very useful to have in web-scale knowledge bases. 3.3 Inference Inference in our model is done approximately in a MapReduce sampling framework. The map tasks sample the entity variables for each mention in a document, sequentially. The entity variables are constrained to either refer to an entity already seen in the document, or to a new entity from the knowledge base (or unkno"
W12-3009,P11-2050,0,0.0164484,"cent work in distantly supervised relation extraction, using facts from a knowledge base to determine which sentences in a corpus express certain relations in order to build relation classifiers (Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009). This work depends on first performing entity linking, finding sentences which contain pairs of knowledge base entities. Typically, this linking has been a simple string-matching heuristic, a noisy alignment that throws away a lot of useful information. Using coreference resolution after a noisy alignment can help to mitigate this issue (Gabbard et al., 2011), but it is still mostly a heuristic matching. A benefit of our approach to adding distributional semantics to web-scale knowledge bases is that in the process we will create a large entity-disambiguated corpus that can be used for further relational learning. 3 Entity Linking We add distributional semantics to knowledge base entities through performing entity linking. Specifically, given a knowledge base and a collection of dependency parsed documents, entity linking maps each noun phrase in the document collection to an entity in the knowledge base, or labels it as unknown (a deficiency we w"
W12-3009,N10-1061,0,0.149698,"The task of the knowledge extraction algorithm is to find new instances of these relations given some training examples, perhaps while jointly determining the set of relevant entities. 46 While these knowledge extraction approaches have focused on relational knowledge, knowing how UC Berkeley appears distributionally in text is also an important aspect of the entity that is potentially useful in a variety of tasks. For example, Pe˜nas and Hovy (2010) showed that a collection of distributional knowledge about football entities helped in interpreting noun compounds like “Young touchdown pass.” Haghighi and Klein (2010) used distributional information about entity types to achieve state-of-the-art coreference resolution results. It has long been known that word sense disambiguation and other tasks are best solved with distributional information (Firth, 1957), yet this information is lacking in web-scale knowledge bases. The primary reason that distributional information has not been included in web-scale knowledge bases is the inherent ambiguity of noun phrases. Knowledge bases typically aim to collect facts about entities, not about noun phrases, but distributional information is only easily obtained for no"
W12-3009,P11-1055,0,0.030162,"hey needed to resort to a severely restricted domain in order to overcome the challenges of constructing the lexicon. Because they only looked at a small set of news articles about football, they could accurately assume that all mentions of the word “Young” referred to a single entity, the former San Francisco 49ers quarterback. At web scale, such assumptions quickly break down. There has been much recent work in distantly supervised relation extraction, using facts from a knowledge base to determine which sentences in a corpus express certain relations in order to build relation classifiers (Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009). This work depends on first performing entity linking, finding sentences which contain pairs of knowledge base entities. Typically, this linking has been a simple string-matching heuristic, a noisy alignment that throws away a lot of useful information. Using coreference resolution after a noisy alignment can help to mitigate this issue (Gabbard et al., 2011), but it is still mostly a heuristic matching. A benefit of our approach to adding distributional semantics to web-scale knowledge bases is that in the process we will create a large entity-disamb"
W12-3009,W11-0102,0,0.0209741,"ny notion of global entities; they had global types whose parameters were shared across document-specific entities. Every time they saw the noun phrase “Barack Obama” in a new document, for example, they created a new entity of type “Person” for the mentions in the document. Even though they did not model individual entities, their system achieved state-of-theart coreference resolution results. We believe that their modeling of distributional semantics was key to the performance of their model, and we draw from those ideas in this paper. Our proposal is also very similar to ideas presented by Hovy (2011). Hovy describes a “new kind of lexicon” containing both relational information traditionally contained in knowledge bases and distributional information very similar to that used in Haghighi and Klein’s coreference model. Each item in this “new lexicon” is represented as a set of distributions over feature values. The lexical entry for “dog,” for example, might contain a feature “name,” with “Spot” and “Lassie” receiving high weight, and a feature “agent-of,” with highly probable values “eat,” “run,” and “bark.” While Hovy has presented this vision of a new lexicon, he has left as open questi"
W12-3009,P11-1058,0,0.0222768,"done approximately in a MapReduce sampling framework. The map tasks sample the entity variables for each mention in a document, sequentially. The entity variables are constrained to either refer to an entity already seen in the document, or to a new entity from the knowledge base (or unknown). Sampling over the entire knowledge base at every step would be intractable, and so when proposing a new entity from the knowledge base we only consider entities that the knowledge base considers possible for the given noun phrase (e.g., NELL has a “CanReferTo” relation mapping noun phrases to concepts (Krishnamurthy and Mitchell, 2011), and Freebase has a similar “alias” relation). Thus the first mention of an entity in a document must be a known alias of the entity, but subsequent mentions can be arbitrary noun phrases (e.g., “the college professor” could not refer to Michael Jordan 2 until he had been introduced with a noun phrase that the knowledge base knows to be an alias, such as “Michael I. Jordan”). This follows standard journalistic practice and aids the model in constraining the “topics” to refer to actual knowledge base entities. The reduce tasks reestimate the parameters for each entity by computing a maximum li"
W12-3009,P09-1113,0,0.0120652,"d domain in order to overcome the challenges of constructing the lexicon. Because they only looked at a small set of news articles about football, they could accurately assume that all mentions of the word “Young” referred to a single entity, the former San Francisco 49ers quarterback. At web scale, such assumptions quickly break down. There has been much recent work in distantly supervised relation extraction, using facts from a knowledge base to determine which sentences in a corpus express certain relations in order to build relation classifiers (Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009). This work depends on first performing entity linking, finding sentences which contain pairs of knowledge base entities. Typically, this linking has been a simple string-matching heuristic, a noisy alignment that throws away a lot of useful information. Using coreference resolution after a noisy alignment can help to mitigate this issue (Gabbard et al., 2011), but it is still mostly a heuristic matching. A benefit of our approach to adding distributional semantics to web-scale knowledge bases is that in the process we will create a large entity-disambiguated corpus that can be used for furthe"
W12-3009,C10-2113,0,0.0386107,"Missing"
W12-3009,P11-1080,0,\N,Missing
W17-4413,D15-1080,0,0.0431896,"Missing"
W17-4413,P16-1144,0,0.0883265,"Missing"
W17-4413,D15-1236,0,0.0754455,"n models (Sakaguchi et al., 2013) have been used in the past by Agarwal and Mannem (2011) for creating biology questions. Our model uses a random forest to rank candidates; it is agnostic towards taking cloze or humanly-generated questions, and it is learned specifically to generate distractors that resemble those in real science exam questions. Science Exam Question Answering. Existing models for multiple-choice science exam QA vary in their reasoning framework and training methodology. A set of sub-problems and solution strategies are outlined in Clark et al. (2013). The method described by Li and Clark (2015) evaluates the coherence of a scene constructed from the question enriched with background KB information, while Sachan et al. (2016) train an entailment model that derives the correct answer from background knowledge aligned with a maxmargin ranker. Probabilistic reasoning approaches include Markov logic networks (Khot et al., 2015) and an integer linear program-based model that assembles proof chains over structured knowledge (Khashabi et al., 2016). The Aristo ensemble (Clark et al., 2016) combines multiple reasoning strategies with shallow statistical methods based on lexical co-occurrence"
W17-4413,D12-1048,0,0.0179514,"rammatical despite the inclusion of part of speech features. Even where the predicted distractors are not fully coherent, showing them to a crowd worker still has a positive priming effect, helping the worker generate good distractors either by providing nearly-good-enough candidates, or by forcing the worker to think why a suggestion is not a good distractor for the question. • WordNet-based hypernymy indicators between tokens in q, a⇤ and a0 , in both directions and potentially via two steps; • indicators for 2-step connections between entities in a⇤ and a0 via a KB based on OpenIE triples (Mausam et al., 2012) extracted from pages in Simple Wikipedia about anatomical structures; • indicators for shared Wordnet-hyponymy of a⇤ and a0 to one of the concepts most frequently generalising all three question distractors in the training set (e.g. element, organ, organism). Distractor Selection Task. To actually generate a multiple choice science question, we show the result of the first task, a (q, a⇤ ) pair, to a crowd worker, along with the top six distractors suggested from the previously described model. The goal of this task is two-fold: (1) quality control (validating a previously generated (q, a⇤ )"
W17-4413,W03-0203,0,0.274315,"e aforementioned datasets in that it consists of natural language questions produced by people, instead of cloze-style questions. It also differs from prior work in that we aim at the narrower domain of science exams and in that we produce multiple choice questions, which are more difficult to generate. quality science questions, as there were problems with selecting relevant text, generating reasonable distractors, and formulating coherent questions. Several similarity measures have been employed for selecting answer distractors (Mitkov et al., 2009), including measures derived from WordNet (Mitkov and Ha, 2003), thesauri (Sumita et al., 2005) and distributional context (Pino et al., 2008; Aldabe and Maritxalar, 2010). Domainspecific ontologies (Papasalouros et al., 2008), phonetic or morphological similarity (Pino and Esknazi, 2009; Correia et al., 2010), probability scores for the question context (Mostow and Jang, 2012) and context-sensitive lexical inference (Zesch and Melamud, 2014) have also been used. In contrast to the aforementioned similaritybased selection strategies, our method uses a feature-based ranker to learn plausible distractors from original questions. Several of the above heurist"
W17-4413,W09-0207,0,0.0273148,"ataset for science exam QA. Our dataset differs from some of the aforementioned datasets in that it consists of natural language questions produced by people, instead of cloze-style questions. It also differs from prior work in that we aim at the narrower domain of science exams and in that we produce multiple choice questions, which are more difficult to generate. quality science questions, as there were problems with selecting relevant text, generating reasonable distractors, and formulating coherent questions. Several similarity measures have been employed for selecting answer distractors (Mitkov et al., 2009), including measures derived from WordNet (Mitkov and Ha, 2003), thesauri (Sumita et al., 2005) and distributional context (Pino et al., 2008; Aldabe and Maritxalar, 2010). Domainspecific ontologies (Papasalouros et al., 2008), phonetic or morphological similarity (Pino and Esknazi, 2009; Correia et al., 2010), probability scores for the question context (Mostow and Jang, 2012) and context-sensitive lexical inference (Zesch and Melamud, 2014) have also been used. In contrast to the aforementioned similaritybased selection strategies, our method uses a feature-based ranker to learn plausible di"
W17-4413,W12-2016,0,0.0239267,"ience questions, as there were problems with selecting relevant text, generating reasonable distractors, and formulating coherent questions. Several similarity measures have been employed for selecting answer distractors (Mitkov et al., 2009), including measures derived from WordNet (Mitkov and Ha, 2003), thesauri (Sumita et al., 2005) and distributional context (Pino et al., 2008; Aldabe and Maritxalar, 2010). Domainspecific ontologies (Papasalouros et al., 2008), phonetic or morphological similarity (Pino and Esknazi, 2009; Correia et al., 2010), probability scores for the question context (Mostow and Jang, 2012) and context-sensitive lexical inference (Zesch and Melamud, 2014) have also been used. In contrast to the aforementioned similaritybased selection strategies, our method uses a feature-based ranker to learn plausible distractors from original questions. Several of the above heuristics are used as features in this ranking model. Feature-based distractor generation models (Sakaguchi et al., 2013) have been used in the past by Agarwal and Mannem (2011) for creating biology questions. Our model uses a random forest to rank candidates; it is agnostic towards taking cloze or humanly-generated quest"
W17-4413,D14-1162,0,0.0976086,"Missing"
W17-4413,D16-1264,0,0.629775,"s the lack of large in-domain training sets. Creating a large, multiple choice science QA dataset is challenging, since crowd workers cannot be expected to have domain expertise, and questions can lack relevance and diversity in structure and content. Furthermore, poorly chosen answer distractors in a multiple choice setting can make questions almost trivial to solve. The first contribution of this paper is a general method for mitigating the difficulties of crowdsourcing QA data, with a particular focus on multiple choice science questions. The method is broadly similar to other recent work (Rajpurkar et al., 2016), relying mainly on showing crowd We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multipl"
W17-4413,P16-2076,0,0.100238,"Missing"
W17-4413,P13-2043,0,0.0196384,"ature-based distractor generation models (Sakaguchi et al., 2013) have been used in the past by Agarwal and Mannem (2011) for creating biology questions. Our model uses a random forest to rank candidates; it is agnostic towards taking cloze or humanly-generated questions, and it is learned specifically to generate distractors that resemble those in real science exam questions. Science Exam Question Answering. Existing models for multiple-choice science exam QA vary in their reasoning framework and training methodology. A set of sub-problems and solution strategies are outlined in Clark et al. (2013). The method described by Li and Clark (2015) evaluates the coherence of a scene constructed from the question enriched with background KB information, while Sachan et al. (2016) train an entailment model that derives the correct answer from background knowledge aligned with a maxmargin ranker. Probabilistic reasoning approaches include Markov logic networks (Khot et al., 2015) and an integer linear program-based model that assembles proof chains over structured knowledge (Khashabi et al., 2016). The Aristo ensemble (Clark et al., 2016) combines multiple reasoning strategies with shallow stati"
W17-4413,W05-0210,0,0.0196027,"t it consists of natural language questions produced by people, instead of cloze-style questions. It also differs from prior work in that we aim at the narrower domain of science exams and in that we produce multiple choice questions, which are more difficult to generate. quality science questions, as there were problems with selecting relevant text, generating reasonable distractors, and formulating coherent questions. Several similarity measures have been employed for selecting answer distractors (Mitkov et al., 2009), including measures derived from WordNet (Mitkov and Ha, 2003), thesauri (Sumita et al., 2005) and distributional context (Pino et al., 2008; Aldabe and Maritxalar, 2010). Domainspecific ontologies (Papasalouros et al., 2008), phonetic or morphological similarity (Pino and Esknazi, 2009; Correia et al., 2010), probability scores for the question context (Mostow and Jang, 2012) and context-sensitive lexical inference (Zesch and Melamud, 2014) have also been used. In contrast to the aforementioned similaritybased selection strategies, our method uses a feature-based ranker to learn plausible distractors from original questions. Several of the above heuristics are used as features in this"
W17-4413,W14-1817,0,0.0319186,"text, generating reasonable distractors, and formulating coherent questions. Several similarity measures have been employed for selecting answer distractors (Mitkov et al., 2009), including measures derived from WordNet (Mitkov and Ha, 2003), thesauri (Sumita et al., 2005) and distributional context (Pino et al., 2008; Aldabe and Maritxalar, 2010). Domainspecific ontologies (Papasalouros et al., 2008), phonetic or morphological similarity (Pino and Esknazi, 2009; Correia et al., 2010), probability scores for the question context (Mostow and Jang, 2012) and context-sensitive lexical inference (Zesch and Melamud, 2014) have also been used. In contrast to the aforementioned similaritybased selection strategies, our method uses a feature-based ranker to learn plausible distractors from original questions. Several of the above heuristics are used as features in this ranking model. Feature-based distractor generation models (Sakaguchi et al., 2013) have been used in the past by Agarwal and Mannem (2011) for creating biology questions. Our model uses a random forest to rank candidates; it is agnostic towards taking cloze or humanly-generated questions, and it is learned specifically to generate distractors that"
W18-2501,D17-1160,1,0.116332,"loning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source Software, pages 1–6 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics with v"
W18-2501,D17-1018,1,0.892756,"and install via pip, a Docker image, or cloning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source Software, pages 1–6 c Melbourne, Australia, July 20, 2018."
W18-2501,D15-1075,0,0.0275455,"Missing"
W18-2501,P14-5010,0,0.0133354,"models. 4 5 The design of AllenNLP allows researchers to focus on the high-level summary of their models rather than the details, and to do careful, reproducible research. Internally at the Allen Institute for Artificial Intelligence the library is widely adopted and has improved the quality of our research code, spread knowledge about deep learning, and made it easier to share discoveries between teams. AllenNLP is gaining traction externally and is growing an open-source community of contributors 7 . The AllenNLP team is comRelated Work Many existing NLP pipelines, such as Stanford CoreNLP (Manning et al., 2014) and spaCy6 , focus on predicting linguistic structures rather than modeling NLP architectures. While AllenNLP supports making predictions using pre-trained 6 Conclusion 7 See GitHub stars and issues on https://github.com/allenai/allennlp and mentions from publications at https://www.semanticscholar.org/search?q=allennlp. https://spacy.io/ 4 mitted to continuing work on this library in order to enable better research practices throughout the NLP community and to build a community of researchers who maintain a collection of the best models in natural language processing. Sepp Hochreiter and J¨u"
W18-2501,D17-2014,0,0.0500079,"Missing"
W18-2501,D16-1053,0,0.0107399,"n choose between pre-trained word embeddings, word embeddings concatenated with a character-level CNN encoding, or even pre-trained model token-incontext embeddings (Peters et al., 2017), which allows for easy controlled experimentation. Seq2SeqEncoder: A common operation in deep NLP models is to take a sequence of word vectors and pass them through a recurrent network to encode contextual information, producing a new sequence of vectors as output. There is a large number of ways to do this, including LSTMs (Hochreiter and Schmidhuber, 1997), GRUs (Cho et al., 2014), intra-sentence attention (Cheng et al., 2016), recurrent additive networks (Lee et al., 2017b), and many more. AllenNLP’s Seq2SeqEncoder abstracts away the decision of which particular encoder to use, allowLibrary Design AllenNLP is a platform designed specifically for deep learning and NLP research. AllenNLP is built on PyTorch (Paszke et al., 2017), which provides many attractive features for NLP research. PyTorch supports dynamic networks, has a clean “Pythonic” syntax, and is easy to use. The AllenNLP library provides (1) a flexible data API that handles intelligent batching and padding, (2) high-level abstractions for common operati"
W18-2501,D14-1179,0,0.00490643,"Missing"
W18-2501,J05-1004,0,0.0517239,"Missing"
W18-2501,P17-1044,1,0.288803,"Apache 2.0 license and is easy to download and install via pip, a Docker image, or cloning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source Software, pages 1"
W18-2501,P17-1161,1,0.174704,"ector sequences, (3) how vector sequences are merged into a single vector. TokenEmbedder: This abstraction takes input arrays generated by e.g. a TextField and returns a sequence of vector embeddings. Through the use of polymorphism and AllenNLP’s experiment framework (see Section 2.3), researchers can easily switch between a wide variety of possible word representations. Simply by changing a configuration file, an experimenter can choose between pre-trained word embeddings, word embeddings concatenated with a character-level CNN encoding, or even pre-trained model token-incontext embeddings (Peters et al., 2017), which allows for easy controlled experimentation. Seq2SeqEncoder: A common operation in deep NLP models is to take a sequence of word vectors and pass them through a recurrent network to encode contextual information, producing a new sequence of vectors as output. There is a large number of ways to do this, including LSTMs (Hochreiter and Schmidhuber, 1997), GRUs (Cho et al., 2014), intra-sentence attention (Cheng et al., 2016), recurrent additive networks (Lee et al., 2017b), and many more. AllenNLP’s Seq2SeqEncoder abstracts away the decision of which particular encoder to use, allowLibrar"
W18-2501,N18-1202,1,0.106576,"Missing"
W18-2501,D16-1264,0,0.0112895,"nd models are working as intended. Library features are built with testability in mind so new components can maintain a similar test coverage. 2.1 Text Data Processing AllenNLP’s data processing API is built around the notion of Fields. Each Field represents a single input array to a model. Fields are grouped together in Instances that represent the examples for training or prediction. The Field API is flexible and easy to extend, allowing for a unified data API for tasks as diverse as tagging, semantic role labeling, question answering, and textual entailment. To represent the SQuAD dataset (Rajpurkar et al., 2016), for example, which has a question and a passage as inputs and a span from the passage as output, each 4 NLP-Focused Abstractions https://codecov.io/gh/allenai/allennlp 2 ing the user to build an encoder-agnostic model and specify the encoder via configuration. In this way, a researcher can easily explore new recurrent architectures; for example, they can replace the LSTMs in any model that uses this abstraction with any other encoder, measuring the impact across a wide range of models and tasks. Seq2VecEncoder: Another common operation in NLP models is to merge a sequence of vectors into a s"
W18-2501,P17-1076,0,0.0132037,"a sequence of vectors into a single vector, using either a recurrent network with some kind of averaging or pooling, or using a convolutional network. This operation is encapsulated in AllenNLP by a Seq2VecEncoder. This abstraction again allows the model code to only describe a class of similar models, with particular instantiations of that model class being determined by a configuration file. SpanExtractor: A recent trend in NLP is to build models that operate on spans of text, instead of on tokens. State-of-the-art models for coreference resolution (Lee et al., 2017a), constituency parsing (Stern et al., 2017), and semantic role labeling (He et al., 2017) all operate in this way. Support for building this kind of model is built into AllenNLP, including a SpanExtractor abstraction that determines how span vectors get computed from sequences of token vectors. 2.3 or even to create their own new abstractions. While some entries in the configuration file are optional, many are required and if unspecified AllenNLP will raise a ConfigurationError when reading the configuration. Additionally, when a configuration file is loaded, AllenNLP logs the configuration values, providing a record of both specified"
W18-2501,P15-1109,0,0.0115804,"m has a permissive Apache 2.0 license and is easy to download and install via pip, a Docker image, or cloning the GitHub repository. It includes reference implementations for recent state-of-the-art models (see Section 3) that can be easily run (to make predictions on arbitrary new inputs) and retrained with different parameters or on new data. These pretrained models have interactive online demos3 Neural network models are now the state-of-theart for a wide range of tasks such as text classification (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difficult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a week to train on modern GPUs and are sensitive to initialization and hyperparameter settings. Furthermore, reference implementations often re-implement NLP components from scratch and make it difficult to 1 http://allennlp.org/ http://github.com/allenai/allennlp 3 http://demo.allennlp.org/ 2 1 Proceedings of Workshop for NLP Open Source"
W18-2501,D16-1244,0,\N,Missing
W18-2501,P17-4012,0,\N,Missing
