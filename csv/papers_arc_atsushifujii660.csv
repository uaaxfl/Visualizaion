D15-1074,Extracting Condition-Opinion Relations Toward Fine-grained Opinion Mining,2015,19,7,2,0,37780,yuki nakayama,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"A fundamental issue in opinion mining is to search a corpus for opinion units, each of which typically comprises the evaluation by an author for a target object from an aspect, such as xe2x80x9cThis hotel is in a good locationxe2x80x9d. However, few attempts have been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as xe2x80x9cfor traveling with small kidsxe2x80x9d. In this paper, we propose a method to extract condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally."
I13-1111,Extracting Evaluative Conditions from Online Reviews: Toward Enhancing Opinion Mining,2013,11,2,2,0,37780,yuki nakayama,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"A fundamental issue in opinion mining is to search a corpus for opinion units, which typically comprise the evaluation by an author for a target object from an aspect, such as xe2x80x9cThis hotel is in a good locationxe2x80x9d. However, no attempt has been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as xe2x80x9cfor traveling with small kidsxe2x80x9d. In this paper, we propose a method to extract such conditions, namely evaluative conditions, from sentences including opinion units. Our method uses supervised machine learning to determine whether each phrase is a constituent of an evaluative condition. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally."
W12-5213,Enhancing Lemmatization for {M}ongolian and its Application to Statistical Machine Translation,2012,10,0,2,0,42102,chimeddorj odbayar,Proceedings of the 10th Workshop on {A}sian Language Resources,0,"Lemmatization is crucial in natural language processing and information retrieval especially for highly inflected languages, such as Finnish and Mongolian. The state-of-the-art method of lemmatization for Mongolian does not need a noun dictionary and is scalable, but errors of this method are mainly caused by problems related to part of speech (POS) information. To resolve this problem, we integrate POS tagging and lemmatization for Mongolian. We evaluate the effectiveness of our method and its contribution to statistical machine translation."
fujii-etal-2012-effects,Effects of Document Clustering in Modeling {W}ikipedia-style Term Descriptions,2012,8,1,1,1,37781,atsushi fujii,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Reflecting the rapid growth of science, technology, and culture, it has become common practice to consult tools on the World Wide Web for various terms. Existing search engines provide an enormous volume of information, but retrieved information is not organized. Hand-compiled encyclopedias provide organized information, but the quantity of information is limited. In this paper, aiming to integrate the advantages of both tools, we propose a method to organize a search result based on multiple viewpoints as in Wikipedia. Because viewpoints required for explanation are different depending on the type of a term, such as animal and disease, we model articles in Wikipedia to extract a viewpoint structure for each term type. To identify a set of term types, we independently use manual annotation and automatic document clustering for Wikipedia articles. We also propose an effective feature for clustering of Wikipedia articles. We experimentally show that the document clustering reduces the cost for the manual annotation while maintaining the accuracy for modeling Wikipedia articles."
fujii-2010-modeling,Modeling {W}ikipedia Articles to Enhance Encyclopedic Search,2010,9,2,1,1,37781,atsushi fujii,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Reflecting the rapid growth of science, technology, and culture, it has become common practice to consult tools on the World Wide Web for various terms. Existing search engines provide an enormous volume of information, but retrieved information is not organized. Hand-compiled encyclopedias provide organized information, but the quantity of information is limited. To integrate the advantages of both tools, we have been proposing methods for encyclopedic search targeting information on the Web and patent information. In this paper, we propose a method to categorize multiple expository texts for a single term based on viewpoints. Because viewpoints required for explanation are different depending on the type of a term, such as animals and diseases, it is difficult to manually produce a large scale system. We use Wikipedia to extract a prototype of a viewpoint structure for each term type. We also use articles in Wikipedia for a machine learning method, which categorizes a given text into an appropriate viewpoint. We evaluate the effectiveness of our method experimentally."
2009.mtsummit-wpt.1,Exploiting Patent Information for the Evaluation of Machine Translation,2009,-1,-1,1,1,37781,atsushi fujii,Proceedings of the Third Workshop on Patent Translation,0,None
2009.mtsummit-wpt.2,Meta-evaluation of Automatic Evaluation Methods for Machine using Patent Translation Data in {NTCIR}-7,2009,-1,-1,4,0,26162,hiroshi echizenya,Proceedings of the Third Workshop on Patent Translation,0,None
fujii-etal-2008-producing,Producing a Test Collection for Patent Machine Translation in the Seventh {NTCIR} Workshop,2008,9,2,1,1,37781,atsushi fujii,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In aiming at research and development on machine translation, we produced a test collection for Japanese-English machine translation in the seventh NTCIR Workshop. This paper describes details of our test collection. From patent documents published in Japan and the United States, we extracted patent families as a parallel corpus. A patent family is a set of patent documents for the same or related invention and these documents are usually filed to more than one country in different languages. In the parallel corpus, we aligned Japanese sentences with their counterpart English sentences. Our test collection, which includes approximately 2,000,000 sentence pairs, can be used to train and test machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval and the contribution of machine translation to a patent retrieval task can also be evaluated. Our test collection will be available to the public for research purposes after the NTCIR final meeting."
fujii-2008-producing,Producing an Encyclopedic Dictionary using Patent Documents,2008,12,3,1,1,37781,atsushi fujii,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Although the World Wide Web has late become an important source to consult for the meaning of words, a number of technical terms related to high technology are not found on the Web. This paper describes a method to produce an encyclopedic dictionary for high-tech terms from patent information. We used a collection of unexamined patent applications published by the Japanese Patent Office as a source corpus. Given this collection, we extracted terms as headword candidates and retrieved applications including those headwords. Then, we extracted paragraph-style descriptions and categorized them into technical domains. We also extracted related terms for each headword. We have produced a dictionary including approximately 400,000 Japanese terms as headwords. We have also implemented an interface with which users can explore our dictionary by reading text descriptions and viewing a related-term graph."
I08-2086,Effects of Related Term Extraction in Transliteration into {C}hinese,2008,9,0,2,0,48617,haixiang huang,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"To transliterate foreign words, in Japanese and Korean, phonograms, such as Katakana and Hangul, are used. In Chinese, the pronunciation of a source word is spelled out using Kanji characters. Because Kanji is ideogrammatic representation, different Kanji characters are associated with the same pronunciation, but can potentially convey different meanings and impressions. To select appropriate Kanji characters, an existing method requests the user to provide one or more related terms for a source word, which is time-consuming and expensive. In this paper, to reduce this human effort, we use the World Wide Web to extract related terms for source words. We show the effectiveness of our method experimentally."
I08-2104,Statistical Machine Translation based Passage Retrieval for Cross-Lingual Question Answering,2008,15,1,3,0.833333,43140,tomoyosi akiba,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"In this paper, we propose a novel approach for Cross-Lingual Question Answering (CLQA). In the proposed method, the statistical machine translation (SMT) is deeply incorporated into the question answering process, instead of using it as the pre-processing of the mono-lingual QA process as in the previous work. The proposed method can be considered as exploiting the SMT-based passage retrieval for CLQA task. We applied our method to the English-toJapanese CLQA system and evaluated the performance by using NTCIR CLQA 1 and 2 test collections. The result showed that the proposed method outperformed the previous pre-translation approach."
I08-1001,A Lemmatization Method for {M}odern {M}ongolian and its Application to Information Retrieval,2008,19,0,2,0,48658,badamosor khaltar,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"In Modern Mongolian, a content word can be inflected when concatenated with suffixes. Identifying the original forms of content words is crucial for natural language processing and information retrieval. We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval. We use technical abstracts to show the effectiveness of our method experimentally."
2008.amta-papers.8,Toward the Evaluation of Machine Translation Using Patent Information,2008,12,6,1,1,37781,atsushi fujii,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"To aid research and development in machine translation, we have produced a test collection for Japanese/English machine translation. To obtain a parallel corpus, we extracted patent documents for the same or related inventions published in Japan and the United States. Our test collection includes approximately 2000000 sentence pairs in Japanese and English, which were extracted automatically from our parallel corpus. These sentence pairs can be used to train and evaluate machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval, which can be used to evaluate the contribution of machine translation to retrieving patent documents across languages. This paper describes our test collection, methods for evaluating machine translation, and preliminary experiments."
W06-1629,Modeling Impression in Probabilistic Transliteration into {C}hinese,2006,7,10,2,0,49763,lili xu,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"For transliterating foreign words into Chinese, the pronunciation of a source word is spelled out with Kanji characters. Because Kanji comprises ideograms, an individual pronunciation may be represented by more than one character. However, because different Kanji characters convey different meanings and impressions, characters must be selected carefully. In this paper, we propose a transliteration method that models both pronunciation and impression, whereas existing methods do not model impression. Given a source word and impression keywords related to the source word, our method derives possible transliteration candidates and sorts them according to their probability. We evaluate our method experimentally."
W06-0303,A System for Summarizing and Visualizing Arguments in Subjective Documents: Toward Supporting Decision Making,2006,-1,-1,1,1,37781,atsushi fujii,Proceedings of the Workshop on Sentiment and Subjectivity in Text,0,None
P06-1083,Extracting Loanwords from {M}ongolian Corpora and Producing a {J}apanese-{M}ongolian Bilingual Dictionary,2006,7,9,2,0,48658,badamosor khaltar,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary. We extract loanwords from Mongolian corpora using our own handcrafted rules. To complement the rule-based extraction, we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords. In addition, we correspond the extracted loanwords to Japanese words and produce a bilingual dictionary. We propose a stemming method for Mongolian to extract loanwords correctly. We verify the effectiveness of our methods experimentally."
ohishi-etal-2006-statistical,Statistical Analysis for Thesaurus Construction using an Encyclopedic Corpus,2006,0,2,4,0,50130,yasunori ohishi,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper proposes a discrimination method for hierarchical relationsbetween word pairs. The method is a statistical one using an Âencyclopedic corpusÂ' extracted and organized from Web pages.In the proposed method, we use the statistical naturethat hyponyms' descriptionstend to include hypernyms whereas hypernyms' descriptions do notinclude all of the hyponyms.Experimental results show that the method detected 61.7{\%} of therelations in an actual thesaurus."
fujii-etal-2006-test,Test Collections for Patent Retrieval and Patent Classification in the Fifth {NTCIR} Workshop,2006,5,11,1,1,37781,atsushi fujii,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the test collections produced for the Patent Retrieval Task in the Fifth NTCIR Workshop. We performed the invalidity search task, in which each participant group searches a patent collection for the patents that can invalidate the demand in an existing claim. For this purpose, we performed both document and passage retrieval tasks. We also performed the automatic patent classification task using the F-term classification system. The test collections will be available to the public for research purposes."
W04-1809,Term Extraction from {K}orean Corpora via {J}apanese,2004,2,4,1,1,37781,atsushi fujii,Proceedings of {C}ompu{T}erm 2004: 3rd International Workshop on Computational Terminology,0,"This paper proposes a method to extract foreign words, such as technical terms and proper nouns, from Korean corpora and produce a JapaneseKorean bilingual dictionary. Specific words have been imported into multiple countries simultaneously, if they are influential across cultures. The pronunciation of a source word is similar in different languages. Our method extracts words in Korean corpora that are phonetically similar to Katakana words, which can easily be identified in Japanese corpora. We also show the effectiveness of our method by means of experiments."
akiba-etal-2004-collecting,Collecting Spontaneously Spoken Queries for Information Retrieval,2004,9,4,2,0.833333,43140,tomoyosi akiba,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Motivated to realize the speech-driven information retrieval systems that accept spontaneously spoken queries, we developed a method to collect such speech data derived from the pre-defined search topics that had been systematically constructed for IR research. In order to evaluate both our method and the performance of the document retrieval by using the spontaneously spoken queries, we took place two experiments of collecting the speech data by our method using publicly available test collections of evaluating document retrieval. The first preliminary experiment took place with relatively small number of search topics selected from the NTCIR-3Web retrieval collection, which had been constructed for the TREC-style evaluation workshop, in order to test our method. The second experiment took place with all of the search topics released from the NTCIR-4 Web task to participate the formal run of the evaluation. The information about the collected data and the result of the evaluation with respect to both the speech recognition accuracy and the precision of document retrieval by using the collected data are presented in this paper."
fujii-etal-2004-test,Test Collections for Patent-to-Patent Retrieval and Patent Map Generation in {NTCIR}-4 Workshop,2004,3,8,1,1,37781,atsushi fujii,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop, and the test collections produced in this task. We perform the invalidity search task, in which each participant group searches a patent collection for the patents that can invalidate the demand in an existing claim. We also perform the automatic patent map generation task, in which the patents associated with a specific topic are organized in a multi-dimensional matrix."
C04-1093,Summarizing Encyclopedic Term Descriptions on the Web,2004,5,14,1,1,37781,atsushi fujii,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We are developing an automatic method to compile an encyclopedic corpus from the Web. In our previous work, paragraph-style descriptions for a term are extracted from Web pages and organized based on domains. However, these descriptions are independent and do not comprise a condensed text as in hand-crafted encyclopedias. To resolve this problem, we propose a summarization method, which produces a single text from multiple descriptions. The resultant summary concisely describes a term from different viewpoints. We also show the effectiveness of our method by means of experiments."
W03-2003,Overview of Patent Retrieval Task at {NTCIR}-3,2003,1,66,2,0,27879,makoto iwayama,Proceedings of the {ACL}-2003 Workshop on Patent Corpus Processing,0,"We describe the overview of patent retrieval task at NTCIR-3. The main task was the technical survey task, where participants tried to retrieve relevant patents to news articles. In this paper, we introduce the task design, the patent collections, the characteristics of the submitted systems, and the results overview. We also arranged the free-styled task, where participants could try anything they want as far as the patent collections were used. We describe the brief summaries of the proposals submitted to the free-styled task."
2003.mtsummit-systems.10,A system for {J}apanese/{E}nglish/{K}orean multilingual patent retrieval,2003,-1,-1,3,0,52997,mitsuharu makita,Proceedings of Machine Translation Summit IX: System Presentations,0,"In response to growing needs for cross-lingual patent retrieval, we propose PRIME (Patent Retrieval In Multilingual Environment system), in which users can retrieve and browse patents in foreign languages only by their native language. PRIME translates a query in the user language into the target language, retrieves patents relevant to the query, and translates retrieved patents into the user language. To update a translation dictionary, PRIME automatically extracts new translations from parallel patent corpora. In the current implementation, trilingual (J/E/K) patent retrieval is available. We describe the system design and its evaluation."
W02-1025,A Method for Open-Vocabulary Speech-Driven Text Retrieval,2002,17,14,1,1,37781,atsushi fujii,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"While recent retrieval techniques do not limit the number of index terms, out-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at retrieving information with spoken queries, we fill the gap between speech recognition and text retrieval in terms of the vocabulary size. Given a spoken query, we generate a transcription and detect OOV words through speech recognition. We then correspond detected OOV words to terms indexed in a target collection to complete the transcription, and search the collection for documents relevant to the completed transcription. We show the effectiveness of our method by way of experiments."
fujii-etal-2002-producing,Producing a Large-scale Encyclopedic Corpus over the Web,2002,9,2,1,1,37781,atsushi fujii,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Encyclopedias, which describe general/technical terms, are valuable language resources (LRs). As with other types of LRs relying on human introspection and supervision, constructing encyclopedias is quite expensive. To resolve this problem, we automatically produced a large-scale encyclopedic corpus over the World Wide Web. We first searched the Web for pages containing a term in question. Then we used linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organized extracted term descriptions based on domains. The resultant corpus contains approximately 100,000 terms. We also evaluated the quality of 2,000 test terms, and found that correct descriptions were obtained for 65% of test terms."
C02-1078,A Probabilistic Method for Analyzing {J}apanese Anaphora Integrating Zero Pronoun Detection and Resolution,2002,14,26,2,0,40641,kazuhiro seki,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper proposes a method to analyze Japanese anaphora, in which zero pronouns (omitted obligatory cases) are used to refer to preceding entities (antecedents). Unlike the case of general coreference resolution, zero pronouns have to be detected prior to resolution because they are not expressed in discourse. Our method integrates two probability parameters to perform zero pronoun detection and resolution in a single framework. The first parameter quantifies the degree to which a given case is a zero pronoun. The second parameter quantifies the degree to which a given entity is the antecedent for a detected zero pronoun. To compute these parameters efficiently, we use corpora with/without annotations of anaphoric relations. We show the effectiveness of our method by way of experiments."
W01-1208,Question Answering Using Encyclopedic Knowledge Generated from the Web,2001,15,2,1,1,37781,atsushi fujii,Proceedings of the {ACL} 2001 Workshop on Open-Domain Question Answering,0,"We propose a question answering system which uses an encyclopedia as a knowledge base. However, since existing encyclopedias lack technical/new terms, we use an encyclopedia automatically generated from the World Wide Web. For this purpose, we first search the Web for pages containing a term in question. Then linguistic patterns and HTML structures are used to extract text fragments describing the term. Finally, extracted term descriptions are organized based on word senses and domains. We also evaluate our system by way of experiments, where the Japanese Information-Technology Engineers Examination is used as a test collection."
P01-1026,Organizing Encyclopedic Knowledge based on the Web and its Application to Question Answering,2001,22,17,1,1,37781,atsushi fujii,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web. We first search the Web for pages containing a term in question. Then we use linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese Information-Technology Engineers Examination."
2001.mtsummit-papers.30,{PRIME}: a system for multi-lingual patent retrieval,2001,-1,-1,3,0,52998,shigeto higuchi,Proceedings of Machine Translation Summit VIII,0,None
P00-1062,Utilizing the World Wide Web as an Encyclopedia: Extracting Term Descriptions from Semi-Structured Texts,2000,10,33,1,1,37781,atsushi fujii,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we propose a method to extract descriptions of technical terms from Web pages in order to utilize the World Wide Web as an encyclopedia. We use linguistic patterns and HTML text structures to extract text fragments containing term descriptions. We also use a language model to discard extraneous descriptions, and a clustering method to summarize resultant descriptions. We show the effectiveness of our method by way of experiments."
fujii-ishikawa-2000-novelty,A Novelty-based Evaluation Method for Information Retrieval,2000,7,0,1,1,37781,atsushi fujii,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In information retrieval research, precision and recall have long been used to evaluate IR systems. However, given that a number of retrieval systems resembling one another are already available to the public, it is valuable to retrieve novel relevant documents, i.e., documents that cannot be retrieved by those existing systems. In view of this problem, we propose an evaluation method that favors systems retrieving as many novel documents as possible. We also used our method to evaluate systems that participated in the IREX workshop."
fujii-ishikawa-2000-applying,Applying machine translation to two-stage cross-language information retrieval,2000,22,12,1,1,37781,atsushi fujii,Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Cross-language information retrieval (CLIR), where queries and documents are in different languages, needs a translation of queries and/or documents, so as to standardize both of them into a common representation. For this purpose, the use of machine translation is an effective approach. However, computational cost is prohibitive in translating large-scale document collections. To resolve this problem, we propose a two-stage CLIR method. First, we translate a given query into the document language, and retrieve a limited number of foreign documents. Second, we machine translate only those documents into the user language, and re-rank them based on the translation result. We also show the effectiveness of our method by way of experiments using Japanese queries and English technical documents."
W99-0605,Cross-Language Information Retrieval for Technical Documents,1999,13,12,1,1,37781,atsushi fujii,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,None
J98-4002,Selective Sampling for Example-based Word Sense Disambiguation,1998,49,95,1,1,37781,atsushi fujii,Computational Linguistics,0,"This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system."
W97-0803,Extending a thesaurus by classifying words,1997,15,16,2,0,301,takenobu tokunaga,Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications,0,None
W97-0807,Integration of Hand-Crafted and Statistical Resources in Measuring Word Similarity,1997,13,5,1,1,37781,atsushi fujii,Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications,0,None
W96-0105,Selective Sampling of Effective Example Sentence Sets for Word Sense Disambiguation,1996,18,4,1,1,37781,atsushi fujii,Fourth Workshop on Very Large Corpora,0,None
C96-1012,To what extent does case contribute to verb sense disambiguation?,1996,13,7,1,1,37781,atsushi fujii,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Word sense disambugation has recently been utillized in corpus-based approaches, reflecting the growth in the number of machine readable texts. One category of approaches disambiguates an input verb sense based on the similarity between its governing case fillers and those in given examples. In this paper, we introduce the degree of contribution of case to verb sense disambiguation into this existing method. In this, greater diversity of semantic range of case filler examples will lead to that case contributing to verb sense disambiguation more. We also report the result of a comparative experiment, in which the performance of disambiguation is improved by considering this notion of semantic contribution."
