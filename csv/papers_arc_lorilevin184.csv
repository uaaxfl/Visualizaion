2020.lrec-1.350,A Resource for Computational Experiments on {M}apudungun,2020,-1,-1,6,0,17377,mingjun duan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a resource for computational experiments on Mapudungun, a polysynthetic indigenous language spoken in Chile with upwards of 200 thousand speakers. We provide 142 hours of culturally significant conversations in the domain of medical treatment. The conversations are fully transcribed and translated into Spanish. The transcriptions also include annotations for code-switching and non-standard pronunciations. We also provide baseline results on three core NLP tasks: speech recognition, speech synthesis, and machine translation between Spanish and Mapudungun. We further explore other applications for which the corpus will be suitable, including the study of code-switching, historical orthography change, linguistic structure, and sociological and anthropological studies."
2020.findings-emnlp.160,An Empirical Exploration of Local Ordering Pre-training for Structured Prediction,2020,-1,-1,3,0,1041,zhisong zhang,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Recently, pre-training contextualized encoders with language model (LM) objectives has been shown an effective semi-supervised method for structured prediction. In this work, we empirically explore an alternative pre-training method for contextualized encoders. Instead of predicting words in LMs, we {``}mask out{''} and predict word order information, with a local ordering strategy and word-selecting objectives. With evaluations on three typical structured prediction tasks (dependency parsing, POS tagging, and NER) over four languages (English, Finnish, Czech, and Italian), we show that our method is consistently beneficial. We further conduct detailed error analysis, including one that examines a specific type of parsing error where the head is misidentified. The results show that pre-trained contextual encoders can bring improvements in a structured way, suggesting that they may be able to capture higher-order patterns and feature combinations from unlabeled data."
2020.emnlp-main.360,Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings,2020,-1,-1,6,0,20394,naoki otani,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs. We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings. CWEs are trained on a word-translation task using the dictionaries that only contain single words. In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language. We release these dictionaries to the research community. We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE. We can translate MWEs at a top-10 precision of 30-60{\%}. The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations."
2020.coling-main.471,Automatic Interlinear Glossing for Under-Resourced Languages Leveraging Translations,2020,-1,-1,5,0,20395,xingyuan zhao,Proceedings of the 28th International Conference on Computational Linguistics,0,"Interlinear Glossed Text (IGT) is a widely used format for encoding linguistic information in language documentation projects and scholarly papers. Manual production of IGT takes time and requires linguistic expertise. We attempt to address this issue by creating automatic glossing models, using modern multi-source neural models that additionally leverage easy-to-collect translations. We further explore cross-lingual transfer and a simple output length control mechanism, further refining our models. Evaluated on three challenging low-resource scenarios, our approach significantly outperforms a recent, state-of-the-art baseline, particularly improving on overall accuracy as well as lemma and tag recall."
W18-4901,Annotation Schemes for Surface Construction Labeling,2018,0,0,1,1,17380,lori levin,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"In this talk I will describe the interaction of linguistics and language technologies in Surface Construction Labeling (SCL) from the perspective of corpus annotation tasks such as definiteness, modality, and causality. Linguistically, following Construction Grammar, SCL recognizes that meaning may be carried by morphemes, words, or arbitrary constellations of morpho-lexical elements. SCL is like Shallow Semantic Parsing in that it does not attempt a full compositional analysis of meaning, but rather identifies only the main elements of a semantic frame, where the frames may be invoked by constructions as well as lexical items. Computationally, SCL is different from tasks such as information extraction in that it deals only with meanings that are expressed in a conventional, grammaticalized way and does not address inferred meanings. I review the work of Dunietz (2018) on the labeling of causal frames including causal connectives and cause and effect arguments. I will describe how to design an annotation scheme for SCL, including isolating basic units of form and meaning and building a {``}constructicon{''}. I will conclude with remarks about the nature of universal categories and universal meaning representations in language technologies. This talk describes joint work with Jaime Carbonell, Jesse Dunietz, Nathan Schneider, and Miriam Petruck."
L18-1611,Parser combinators for {T}igrinya and {O}romo morphology,2018,0,0,8,0.942029,12351,patrick littell,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1196,{D}eep{C}x: A transition-based approach for shallow semantic parsing with complex constructional triggers,2018,0,1,3,1,23085,jesse dunietz,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper introduces the surface construction labeling (SCL) task, which expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered by complex constructions. We present DeepCx, a neural, transition-based system for SCL. As a test case for the approach, we apply DeepCx to the task of tagging causal language in English, which relies on a wider variety of constructions than are typically addressed in SSP. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways DeepCx could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available."
D18-1366,Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations,2018,16,3,3,0,831,aditi chaudhary,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU."
W17-2911,Code-Switching as a Social Act: The Case of {A}rabic {W}ikipedia Talk Pages,2017,10,3,4,0,2578,michael yoder,Proceedings of the Second Workshop on {NLP} and Computational Social Science,0,"Code-switching has been found to have social motivations in addition to syntactic constraints. In this work, we explore the social effect of code-switching in an online community. We present a task from the Arabic Wikipedia to capture language choice, in this case code-switching between Arabic and other languages, as a predictor of social influence in collaborative editing. We find that code-switching is positively associated with Wikipedia editor success, particularly borrowing technical language on pages with topics less directly related to Arabic-speaking regions."
W17-0812,The {BEC}au{SE} Corpus 2.0: Annotating Causality and Overlapping Relations,2017,17,2,2,1,23085,jesse dunietz,Proceedings of the 11th Linguistic Annotation Workshop,0,"Language of cause and effect captures an essential component of the semantics of a text. However, causal language is also intertwined with other semantic relations, such as temporal precedence and correlation. This makes it difficult to determine when causation is the primary intended meaning. This paper presents BECauSE 2.0, a new version of the BECauSE corpus with exhaustively annotated expressions of causal language, but also seven semantic relations that are frequently co-present with causation. The new corpus shows high inter-annotator agreement, and yields insights both about the linguistic expressions of causation and about the process of annotating co-present semantic relations."
Q17-1009,Automatically Tagging Constructions of Causation and Their Slot-Fillers,2017,41,2,2,1,23085,jesse dunietz,Transactions of the Association for Computational Linguistics,0,"This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that causation can take on. We therefore base our approach on the concept of constructions from the linguistic paradigm known as Construction Grammar (CxG). In CxG, a construction is a form/function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction{'}s form, as some attempts to employ CxG in NLP have done, we propose methods that offload that problem to machine learning. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with statistical classifiers that learn the subtler parameters of the constructions. Our results show that these approaches are promising: they significantly outperform na{\""\i}ve baselines for both construction recognition and cause and effect head matches."
E17-2002,"{URIEL} and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",2017,7,30,6,1,12351,patrick littell,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We introduce the URIEL knowledge base for massively multilingual NLP and the lang2vec utility, which provides information-rich vector identifications of languages drawn from typological, geographical, and phylogenetic databases and normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors."
N16-1161,Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning,2016,36,25,8,0.166667,3965,yulia tsvetkov,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually."
L16-1529,"Bridge-Language Capitalization Inference in {W}estern {I}ranian: {S}orani, {K}urmanji, Zazaki, and {T}ajik",2016,0,3,5,1,12351,patrick littell,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In Sorani Kurdish, one of the most useful orthographic features in named-entity recognition {--} capitalization {--} is absent, as the language{'}s Perso-Arabic script does not make a distinction between uppercase and lowercase letters. We describe a system for deriving an inferred capitalization value from closely related languages by phonological similarity, and illustrate the system using several related Western Iranian languages."
C16-1095,Named Entity Recognition for Linguistic Rapid Response in Low-Resource Languages: {S}orani {K}urdish and {T}ajik,2016,0,3,6,1,12351,patrick littell,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper describes our construction of named-entity recognition (NER) systems in two Western Iranian languages, Sorani Kurdish and Tajik, as a part of a pilot study of {``}Linguistic Rapid Response{''} to potential emergency humanitarian relief situations. In the absence of large annotated corpora, parallel corpora, treebanks, bilingual lexica, etc., we found the following to be effective: exploiting distributional regularities in monolingual data, projecting information across closely related languages, and utilizing human linguist judgments. We show promising results on both a four-month exercise in Sorani and a two-day exercise in Tajik, achieved with minimal annotation costs."
C16-1328,{P}an{P}hon: A Resource for Mapping {IPA} Segments to Articulatory Feature Vectors,2016,0,8,6,1,9801,david mortensen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper contributes to a growing body of evidence that{---}when coupled with appropriate machine-learning techniques{--}linguistically motivated, information-rich representations can outperform one-hot encodings of linguistic data. In particular, we show that phonological features outperform character-based models. PanPhon is a database relating over 5,000 IPA segments to 21 subsegmental articulatory features. We show that this database boosts performance in various NER-related tasks. Phonologically aware, neural CRF models built on PanPhon features are able to perform better on monolingual Spanish and Turkish NER tasks that character-based models. They have also been shown to work well in transfer models (as between Uzbek and Turkish). PanPhon features also contribute measurably to Orthography-to-IPA conversion tasks."
W15-1622,Annotating Causal Language Using Corpus Lexicography of Constructions,2015,14,7,2,1,23085,jesse dunietz,Proceedings of The 9th Linguistic Annotation Workshop,0,"Detecting and analyzing causal language is essential to extracting semantic relationships. To that end, we present an annotation scheme for English causal language (not metaphysical causality), and discuss two methodologies for annotation. The first uses only a coding manual to train annotators in distinguishing causal from non-causal language. To address low inter-coder agreement, we adopted a second methodology, in which we first created a causal language constructicon based on corpus analysis, then required annotators only to annotate instances based on the constructicon. (This resembles the methodology used for annotating the FrameNet and PropBank corpora.) Our contributions, in addition to the annotation scheme itself, are methodological: we discuss when constructicon-based methodology is appropriate, and address the validity of annotation schemes that require expertlevel metalinguistic awareness."
N15-1144,Unsupervised {POS} Induction with Word Embeddings,2015,20,18,4,1,4430,chucheng lin,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on downstream POS induction results."
W14-5133,Keynote Lecture 3: Modeling {N}on-{P}ropositional Semantics,2014,-1,-1,1,1,17380,lori levin,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-3909,The {CMU} Submission for the Shared Task on Language Identification in Code-Switched Data,2014,16,10,3,1,4430,chucheng lin,Proceedings of the First Workshop on Computational Approaches to Code Switching,0,"We describe the CMU submission for the 2014 shared task on language identification in code-switched data. We participated in all four language pairs: Spanishxe2x80x90English, Mandarinxe2x80x90English, Nepalixe2x80x90English, and Modern Standard Arabicxe2x80x90Arabic dialects. After describing our CRF-based baseline system, we discuss three extensions for learning from unlabeled data: semi-supervised learning, word embeddings, and word lists."
bhatia-etal-2014-unified,A Unified Annotation Scheme for the Semantic/Pragmatic Components of Definiteness,2014,19,4,3,0,11651,archna bhatia,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a definiteness annotation scheme that captures the semantic, pragmatic, and discourse information, which we call communicative functions, associated with linguistic descriptions such as {``}a story about my speech{''}, {``}the story{''}, {``}every time I give it{''}, {``}this slideshow{''}. A survey of the literature suggests that definiteness does not express a single communicative function but is a grammaticalization of many such functions, for example, identifiability, familiarity, uniqueness, specificity. Our annotation scheme unifies ideas from previous research on definiteness while attempting to remove redundancy and make it easily annotatable. This annotation scheme encodes the communicative functions of definiteness rather than the grammatical forms of definiteness. We assume that the communicative functions are largely maintained across languages while the grammaticalization of this information may vary. One of the final goals is to use our semantically annotated corpora to discover how definiteness is grammaticalized in different languages. We release our annotated corpora for English and Hindi, and sample annotations for Hebrew and Russian, together with an annotation manual."
levin-etal-2014-resources,Resources for the Detection of Conventionalized Metaphors in Four Languages,2014,11,6,1,1,17380,lori levin,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes a suite of tools for extracting conventionalized metaphors in English, Spanish, Farsi, and Russian. The method depends on three significant resources for each language: a corpus of conventionalized metaphors, a table of conventionalized conceptual metaphors (CCM table), and a set of extraction rules. Conventionalized metaphors are things like {``}escape from poverty{''} and {``}burden of taxation{''}. For each metaphor, the CCM table contains the metaphorical source domain word (such as {``}escape{''}) the target domain word (such as {``}poverty{''}) and the grammatical construction in which they can be found. The extraction rules operate on the output of a dependency parser and identify the grammatical configurations (such as a verb with a prepositional phrase complement) that are likely to contain conventional metaphors. We present results on detection rates for conventional metaphors and analysis of the similarity and differences of source domains for conventional metaphors in the four languages."
feely-etal-2014-cmu,The {CMU} {METAL} {F}arsi {NLP} Approach,2014,6,4,4,0,14102,weston feely,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"While many high-quality tools are available for analyzing major languages such as English, equivalent freely-available tools for important but lower-resourced languages such as Farsi are more difficult to acquire and integrate into a useful NLP front end. We report here on an accurate and efficient Farsi analysis front end that we have assembled, which may be useful to others who wish to work with written Farsi. The pre-existing components and resources that we incorporated include the Carnegie Mellon TurboParser and TurboTagger (Martins et al., 2010) trained on the Dadegan Treebank (Rasooli et al., 2013), the Uppsala Farsi text normalizer PrePer (Seraji, 2013), the Uppsala Farsi tokenizer (Seraji et al., 2012a), and Jon DehdariÂs PerStem (Jadidinejad et al., 2010). This set of tools (combined with additional normalization and tokenization modules that we have developed and made available) achieves a dependency parsing labeled attachment score of 89.49{\%}, unlabeled attachment score of 92.19{\%}, and label accuracy score of 91.38{\%} on a held-out parsing test data set. All of the components and resources used are freely available. In addition to describing the components and resources, we also explain the rationale for our choices."
littell-etal-2014-morphological,Morphological parsing of {S}wahili using crowdsourced lexical resources,2014,9,2,3,1,12351,patrick littell,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We describe a morphological analyzer for the Swahili language, written in an extension of XFST/LEXC intended for the easy declaration of morphophonological patterns and importation of lexical resources. Our analyzer was supplemented extensively with data from the Kamusi Project (kamusi.org), a user-contributed multilingual dictionary. Making use of this resource allowed us to achieve wide lexical coverage quickly, but the heterogeneous nature of user-contributed content also poses some challenges when adapting it for use in an expert system."
C14-1100,Automatic Classification of Communicative Functions of Definiteness,2014,45,2,9,0,11651,archna bhatia,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Definiteness expresses a constellation of semantic, pragmatic, and discourse propertiesxe2x80x94the communicative functionsxe2x80x94of an NP. We present a supervised classifier for English NPs that uses lexical, morphological, and syntactic features to predict an NPxe2x80x99s communicative function in terms of a language-universal classification scheme. Our classifiers establish strong baselines for future work in this neglected area of computational semantic analysis. In addition, analysis of the features and learned parameters in the model provides insight into the grammaticalization of definiteness in English, not all of which is obvious a priori."
W13-3403,Introducing Computational Concepts in a Linguistics Olympiad,2013,5,1,2,1,12351,patrick littell,Proceedings of the Fourth Workshop on Teaching {NLP} and {CL},0,"Linguistics olympiads, now offered in more than 20 countries, provide secondary-school students a compelling introduction to an unfamiliar field. The North American Computational Linguistics Olympiad (NACLO) includes computational puzzles in addition to purely linguistic ones. This paper explores the computational subject matter we seek to convey via NACLO, as well as some of the challenges that arise when adapting problems in computational linguistics to an audience that may have no background in computer science, linguistics, or advanced mathematics. We present a small library of reusable design patterns that have proven useful when composing puzzles appropriate for secondary-school students. 1 What is a Linguistics Olympiad? A linguistics olympiad (LO) (Payne and Derzhanski, 2010) is a puzzle contest for secondary-school students in which contestants compete to solve self-contained linguistics problem sets. LOs have their origin in the Moscow Traditional Olympiad in Linguistics, established in 1965, and have since spread around the world; an international contest (http://www.ioling.org) has been held yearly since 2003. In an LO, every problem set is self-contained, so no prior experience in linguistics is necessary to compete. In fact, LO contests are fun and rewarding for exactly this reason: by the end of the contest, contestants are managing to read hieroglyphics, conjugate verbs in Swahili, and perform other amazing feats. Furthermore, they have accomplished this solely through their own analytical abilities and linguistic intuition. Based on our experience going into high schools and presenting our material, this xe2x80x9clinguisticxe2x80x9d way of thinking about languages almost always comes as a novel surprise to students. They largely think about languages as collections of known facts that you learn in classes and from books, not something that you can dive into and figure out for yourself. This is a hands-on antidote to the common public misconception that linguists are fundamentally polyglots, rather than language scientists, and students come out of the experience having realized that linguistics is a very different field (and hopefully a more compelling one) than they had assumed it to be. 2 Computational Linguistics at the LO Our goal, since starting the North American Computational Linguistics Olympiad (NACLO) in 2007 (Radev et al., 2008), has been to explore how this LO experience can be used to introduce students to computational linguistics. Topics in computational linguistics have been featured before in LOs, occasionally in the Moscow LO and with some regularity in the Bulgarian LO. Our deliberations began with some troubling statistics regarding enrollments in computer science programs (Zweben, 2013). Between 2003 and 2007 enrollments in computer science dropped dramatically. This was attributed in part to the dip in the IT sector, but it also stemmed in"
W13-2234,Generating {E}nglish Determiners in Phrase-Based Translation with Synthetic Translation Options,2013,34,14,3,0.166667,3965,yulia tsvetkov,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation optionsxe2x80x94phrasal translations that are generated by auxiliary translation and postediting processesxe2x80x94to augment the default phrase inventory learned from parallel data. We apply our technique to the problem of producing English determiners when translating from Russian and Czech, languages that lack definiteness morphemes. Our approach augments the English side of the phrase table using a classifier to predict where English articles might plausibly be added or removed, and then we decode as usual. Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier."
P13-2134,The Effects of Lexical Resource Quality on Preference Violation Detection,2013,15,0,2,1,23085,jesse dunietz,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements."
W12-3807,Statistical Modality Tagging from Rule-based Annotations and Crowdsourcing,2012,19,15,5,0,90,vinodkumar prabhakaran,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,0,We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.
J12-2006,Modality and Negation in {SIMT} Use of Modality and Negation in Semantically-Informed Syntactic {MT},2012,45,34,7,1,43387,kathryn baker,Computational Linguistics,0,"This article describes the resource-and system-building efforts of an 8-week Johns Hopkins University Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set.n n We apply our MN annotation scheme to statistical machine translation using a syntactic framework that supports the inclusion of semantic annotations. Syntactic tags enriched with semantic annotations are assigned to parse trees in the target-language training texts through a process of tree grafting. Although the focus of our work is modality and negation, the tree grafting procedure is general and supports other types of semantic information. We exploit this capability by including named entities, produced by a pre-existing tagger, in addition to the MN elements produced by the taggers described here. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English test set. This finding supports the hypothesis that both syntactic and semantic information can improve translation quality."
baker-etal-2010-modality,A Modality Lexicon and its use in Automatic Tagging,2010,10,35,5,1,43387,kathryn baker,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one taggerâa structure-based taggerâresults in precision around 86{\%} (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data."
2010.amta-papers.7,Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach,2010,13,9,6,1,43387,kathryn baker,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality{---}and further demonstrates that large gains can be achieved for low-resource languages with different word order than English."
W09-3012,Committed Belief Annotation and Tagging,2009,12,45,2,0.0531905,7377,mona diab,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"We present a preliminary pilot study of belief annotation and automatic tagging. Our objective is to explore semantic meaning beyond surface propositions. We aim to model people's cognitive states, namely their beliefs as expressed through linguistic means. We model the strength of their beliefs and their (the human) degree of commitment to their utterance. We explore only the perspective of the author of a text. We classify predicates into one of three possibilities: committed belief, non committed belief, or not applicable. We proceed to manually annotate data to that end, then we build a supervised framework to test the feasibility of automatically predicting these belief states. Even though the data is relatively small, we show that automatic prediction of a belief class is a feasible task. Using syntactic features, we are able to obtain significant improvements over a simple baseline of 23% F-measure absolute points. The best performing automatic tagging condition is where we use POS tag, word type feature AlphaNumeric, and shallow syntactic chunk information CHUNK. Our best overall performance is 53.97% F-measure."
2009.eamt-1.2,"Adaptable, Community-Controlled, Language Technologies for Language Maintenance",2009,-1,-1,1,1,17380,lori levin,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,None
W08-0708,Evaluating an Agglutinative Segmentation Model for {P}ara{M}or,2008,20,9,4,1,44751,christian monson,Proceedings of the Tenth Meeting of {ACL} Special Interest Group on Computational Morphology and Phonology,0,"This paper describes and evaluates a modification to the segmentation model used in the unsupervised morphology induction system, ParaMor. Our improved segmentation model permits multiple morpheme boundaries in a single word. To prepare ParaMor to effectively apply the new agglutinative segmentation model, two heuristics improve ParaMor's precision. These precision-enhancing heuristics are adaptations of those used in other unsupervised morphology induction systems, including work by Hafer and Weiss (1974) and Goldsmith (2006). By reformulating the segmentation model used in ParaMor, we significantly improve ParaMor's performance in all language tracks and in both the linguistic evaluation as well as in the task based information retrieval (IR) evaluation of the peer operated competition Morpho Challenge 2007. ParaMor's improved morpheme recall in the linguistic evaluations of German, Finnish, and Turkish is higher than that of any system which competed in the Challenge. In the three languages of the IR evaluation, our enhanced ParaMor significantly outperforms, at average precision over newswire queries, a morphologically naive baseline; scoring just behind the leading system from Morpho Challenge 2007 in English and ahead of the first place system in German."
W08-0410,Inductive Detection of Language Features via Clustering Minimal Pairs: Toward Feature-Rich Grammars in Machine Translation,2008,14,1,3,0,3375,jonathan clark,Proceedings of the {ACL}-08: {HLT} Second Workshop on Syntax and Structure in Statistical Translation ({SSST}-2),0,"Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase-Based Statistical Machine Translation (PBSMT). Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC's upcoming LCTL language packs. The presented method discovers a mapping between morphemes and linguistically relevant features. By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved. We conclude by outlining how the resulting output can then be used in inducing a morphosyntactically feature-rich grammar for AVENUE, a modern syntax-based MT system."
W08-0211,The {N}orth {A}merican Computational Linguistics Olympiad ({NACLO}),2008,2,5,2,0,2447,dragomir radev,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"NACLO (North American Computational Linguistics Olympiad) is an annual Olympiad-style contest for high school students, focusing on linguistics, computational linguistics, and language technologies."
clark-etal-2008-toward,Toward Active Learning in Data Selection: Automatic Discovery of Language Features During Elicitation,2008,12,4,3,0,3375,jonathan clark,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Data Selection has emerged as a common issue in language technologies. We define Data Selection as the choosing of a subset of training data that is most effective for a given task. This paper describes deductive feature detection, one component of a data selection system for machine translation. Feature detection determines whether features such as tense, number, and person are expressed in a language. The database of the The World Atlas of Language Structures provides a gold standard against which to evaluate feature detection. The discovered features can be used as input to a Navigator, which uses active learning to determine which piece of language data is the most important to acquire next."
monson-etal-2008-linguistic,Linguistic Structure and Bilingual Informants Help Induce Machine Translation of Lesser-Resourced Languages,2008,14,10,4,1,44751,christian monson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Producing machine translation (MT) for the many minority languages in the world is a serious challenge. Minority languages typically have few resources for building MT systems. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, our research programs on minority language MT have focused on leveraging to the maximum extent two resources that are available for minority languages: linguistic structure and bilingual informants. All natural languages contain linguistic structure. And although the details of that linguistic structure vary from language to language, language universals such as context-free syntactic structure and the paradigmatic structure of inflectional morphology, allow us to learn the specific details of a minority language. Similarly, most minority languages possess speakers who are bilingual with the major language of the area. This paper discusses our efforts to utilize linguistic structure and the translation information that bilingual informants can provide in three sub-areas of our rapid development MT program: morphology induction, syntactic transfer rule learning, and refinement of imperfect learned rules."
W07-1315,{P}ara{M}or: Minimally Supervised Induction of Paradigm Structure and Morphological Analysis,2007,9,19,4,1,44751,christian monson,Proceedings of Ninth Meeting of the {ACL} Special Interest Group in Computational Morphology and Phonology,0,"Paradigms provide an inherent organizational structure to natural language morphology. ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German. ParaMor consists of two phases. Our algorithm first constructs sets of affixes closely mimicking the paradigms of a language. And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries. To set ParaMor's few free parameters we analyze a training corpus of Spanish. Without adjusting parameters, we induce the morphological structure of English and German. Adopting the evaluation methodology of Morpho Challenge 2007 (Kurimo et al., 2007), we compare ParaMor's morphological analyses with Morfessor (Creutz, 2006), a modern minimally supervised morphology induction system. ParaMor consistently achieves competitive F1 measures."
2007.tmi-papers.1,An assessment of language elicitation without the supervision of a linguist,2007,-1,-1,2,1,48478,alison alvarez,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
N06-2002,The {MILE} Corpus for Less Commonly Taught Languages,2006,4,7,2,1,48478,alison alvarez,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper describes a small, structured English corpus that is designed for translation into Less Commonly Taught Languages (LCTLs), and a set of re-usable tools for creation of similar corpora. The corpus systematically explores meanings that are known to affect morphology or syntax in the world's languages. Each sentence is associated with a feature structure showing the elements of meaning that are represented in the sentence. The corpus is highly structured so that it can support machine learning with only a small amount of data. As part of the REFLEX program, the corpus will be translated into multiple LCTLs, resulting in parallel corpora can be used for training of MT and other language technologies. Only the untranslated English corpus is described in this paper."
N06-1018,Understanding Temporal Expressions in Emails,2006,13,11,3,0,50079,benjamin han,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"Recent years have seen increasing research on extracting and using temporal information in natural language applications. However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts. In this paper we report our work on anchoring temporal expressions in a novel genre, emails. The highly under-specified nature of these expressions fits well with our constraint-based representation of time, Time Calculus for Natural Language (TCNL). We have developed and evaluated a Temporal Expression Anchoror (TEA), and the result shows that it performs significantly better than the baseline, and compares favorably with some of the closely related work."
rambow-etal-2006-parallel,Parallel Syntactic Annotation of Multiple Languages,2006,12,10,8,0,1354,owen rambow,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components."
2005.mtsummit-posters.10,Semi-Automated Elicitation Corpus Generation,2005,10,4,2,1,48478,alison alvarez,Proceedings of Machine Translation Summit X: Posters,0,"In this document we will describe a semi-automated process for creating elicitation corpora. An elicitation corpus is translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. The corpus sentences are automatically generated from detailed feature structures using the GenKit generation program. Feature structures themselves are automatically generated from information that is provided by a linguist using our corpus specification software. This helps us to build small, flexible corpora for testing and development of machine translation systems."
W04-2709,Interlingual Annotation of Multilingual Text Corpora,2004,14,20,5,0,50484,stephen helmreich,Proceedings of the Workshop Frontiers in Corpus Annotation at {HLT}-{NAACL} 2004,0,"This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content. After presenting the background and objectives of the effort, we will go on to describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself. We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen."
W04-0107,Unsupervised Induction of Natural Language Morphology Inflection Classes,2004,13,19,4,1,44751,christian monson,Proceedings of the 7th Meeting of the {ACL} Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology,0,"We propose a novel language-independent framework for inducing a collection of morphological inflection classes from a monolingual corpus of full form words. Our approach involves two main stages. In the first stage, we generate a large data structure of candidate inflection classes and their interrelationships. In the second stage, search and filtering techniques are applied to this data structure, to identify a select collection of true inflection classes of the language. We describe the basic methodology involved in both stages of our approach and present an evaluation of our baseline techniques applied to induction of major inflection classes of Spanish. The preliminary results on an initial training corpus already surpass an F1 of 0.5 against ideal Spanish inflectional morphology classes."
monson-etal-2004-data,Data Collection and Analysis of {M}apudungun Morphology for Spelling Correction,2004,4,2,2,1,44751,christian monson,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes part of a three year collaboration between Carnegie Mellon University's Language Technologies Institute, the Programa de Educacion Intercultural Bilingue of the Chilean Ministry of Education, and Universidad de La Frontera (Temuco, Chile). We are currently constructing a spelling checker for Mapudungun, a polysynthetic language spoken by the Mapuche people in Chile and Argentina. The spelling checker will be built in MySpell, the spell checking system used by the open source office suite OpenOffice. This paper also describes the spoken language corpus that is used as a source of data for developing the spelling checker."
2004.eamt-1.14,A trainable transfer-based {MT} approach for languages with limited resources,2004,11,24,5,0,13539,alon lavie,Proceedings of the 9th EAMT Workshop: Broadening horizons of machine translation and its applications,0,"We describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources. Our approach assumes the availability of a small number of bi-lingual speakers of the two languages, but these need not be linguistic experts. The bi-lingual speakers create a comparatively small corpus of word aligned phrases and sentences (on the order of magnitude of a few thousand sentence pairs) using a specially designed elicitation tool. From this data, the learning module of our system automatically infers hierarchical syntactic transfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources."
reeder-etal-2004-interlingual,Interlingual annotation for {MT} development,2004,16,7,7,0,46657,florence reeder,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"MT systems that use only superficial representations, including the current generation of statistical MT systems, have been successful and useful. However, they will experience a plateau in quality, much like other {``}silver bullet{''} approaches to MT. We pursue work on the development of interlingual representations for use in symbolic or hybrid MT systems. In this paper, we describe the creation of an interlingua and the development of a corpus of semantically annotated text, to be validated in six languages and evaluated in several ways. We have established a distributed, well-functioning research methodology, designed a preliminary interlingua notation, created annotation manuals and tools, developed a test collection in six languages with associated English translations, annotated some 150 translations, and designed and applied various annotation metrics. We describe the data sets being annotated and the interlingual (IL) representation language which uses two ontologies and a systematic theta-role list. We present the annotation tools built and outline the annotation process. Following this, we describe our evaluation methodology and conclude with a summary of issues that have arisen."
W03-2118,Domain Specific Speech Acts for Spoken Language Translation,2003,14,38,1,1,17380,lori levin,Proceedings of the Fourth {SIG}dial Workshop of Discourse and Dialogue,0,"We describe a coding scheme for machine translation of spoken taskoriented dialogue. The coding scheme covers two levels of speaker intention xe2x88x92 domain independent speech acts and domain dependent domain actions. Our database contains over 14,000 tagged sentences in English, Italian, and German. We argue that domain actions, and not speech acts, are the relevant discourse unit for improving translation quality. We also show that, although domain actions are domain specific, the approach scales up to large domains without an explosion of domain actions and can be coded with high inter-coder reliability across research sites. Furthermore, although the number of domain actions is on the order of ten times the number of speech acts, sparseness is not a problem for the training of classifiers for identifying the domain action. We describe our work on developing high accuracy speech act and domain action classifiers, which is the core of the source language analysis module of our NESPOLE machine translation system."
N03-4015,{S}peechalator: Two-Way Speech-to-Speech Translation in Your Hand,2003,2,20,7,0,5073,alex waibel,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"This demonstration involves two-way automatic speech-to-speech translation on a consumer off-the-shelf PDA. This work was done as part of the DARPA-funded Babylon project, investigating better speech-to-speech translation systems for communication in the field. The development of the Speechalator software-based translation system required addressing a number of hard issues, including a new language for the team (Egyptian Arabic), close integration on a small device, computational efficiency on a limited platform, and scalable coverage for the domain."
W02-0703,Spoken Language Parsing Using Phrase-Level Grammars and Trainable Classifiers,2002,11,6,3,0,52566,chad langley,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification. The job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances. The goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages."
W02-0708,Balancing Expressiveness and Simplicity in an Interlingua for Task Based Dialogue,2002,8,18,1,1,17380,lori levin,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"In this paper we compare two interlingua representations for speech translation. The basis of this paper is a distributional analysis of the C-STAR II and NESPOLE databases tagged with interlingua representations. The C-STAR II database has been partially re-tagged with the NESPOLE interlingua, which enables us to make comparisons on the same data with two types of interlinguas and on two types of data (C-STAR II and NESPOLE) with the same interlingua. The distributional information presented in this paper show that the NESPOLE interlingua maintains the language-independence and simplicity of the C-STAR II speech-act-based approach, while increasing semantic expressiveness and scalability."
2002.tmi-papers.17,Challenges in automated elicitation of a controlled bilingual corpus,2002,-1,-1,2,1,48480,katharina probst,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
2002.tmi-papers.19,Rapid adaptive development of semantic analysis grammars,2002,-1,-1,3,0,49102,alicia tribble,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
lavie-etal-2002-nespole,The {NESPOLE}! speech-to-speech translation system,2002,9,19,2,0.692532,13539,alon lavie,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: System Descriptions,0,"NESPOLE! is a speech-to-speech machine translation research system designed to provide fully functional speech-to-speech capabilities within real-world settings of common users involved in e-commerce applications. The project is funded jointly by the European Commission and the US NSF. The NESPOLE! system uses a client-server architecture to allow a common user, who is browsing web-pages on the internet, to connect seamlessly in real-time to an agent of the service provider, using a video-conferencing channel and with speech-to-speech translation services mediating the conversation. Shared web pages and annotated images supported via a Whiteboard application are available to enhance the communication."
carbonell-etal-2002-automatic,Automatic rule learning for resource-limited {MT},2002,8,21,7,0,10837,jaime carbonell,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Machine Translation of minority languages presents unique challenges, including the paucity of bilingual training data and the unavailability of linguistically-trained speakers. This paper focuses on a machine learning approach to transfer-based MT, where data in the form of translations and lexical alignments are elicited from bilingual speakers, and a seeded version-space learning algorithm formulates and refines transfer rules. A rule-generalization lattice is defined based on LFG-style f-structures, permitting generalization operators in the search for the most general rules consistent with the elicited data. The paper presents these methods and illustrates examples."
H01-1018,Domain Portability in Speech-to-Speech Translation,2001,12,10,2,0.795455,13539,alon lavie,Proceedings of the First International Conference on Human Language Technology Research,0,"Speech-to-speech translation has made significant advances over the past decade, with several high-visibility projects (C-STAR, Verb-mobil, the Spoken Language Translator, and others) significantly advancing the state-of-the-art. While speech recognition can currently effectively deal with very large vocabularies and is fairly speaker independent, speech translation is currently still effective only in limited, albeit large, domains. The issue of domain portability is thus of significant importance, with several current research efforts designed to develop speech-translation systems that can be ported to new domains with significantly less time and effort than is currently possible."
2001.mtsummit-road.7,Design and implementation of controlled elicitation for machine translation of low-density languages,2001,10,17,5,1,48480,katharina probst,Workshop on MT2010: Towards a Road Map for MT,0,"NICE is a machine translation project for low-density languages. We are building a tool that will elicit a controlled corpus from a bilingual speaker who is not an expert in linguistics. The corpus is intended to cover major typological phenomena, as it is designed to work for any language. Using implicational universals, we strive to minimize the number of sentences that each informant has to translate. From the elicited sentences, we learn transfer rules with a version space algorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages."
W00-0203,Evaluation of a Practical Interlingua for Task-Oriented Dialogue,2000,3,28,1,1,17380,lori levin,{NAACL}-{ANLP} 2000 Workshop: Applied Interlinguas: Practical Applications of Interlingual Approaches to {NLP},0,"IF (Interchange Format), the interlingua used by the C-STAR consortium, is a speech-act based interlingua for task-oriented dialogue. IF was designed as a practical interlingua that could strike a balance between expressivity and simplicity. If it is too simple, components of meaning will be lost and coverage of unseen data will be low. On the other hand, if it is too complex, it cannot be used with a high degree of consistency by collaborators on different continents. In this paper, we suggest methods for evaluating the coverage of IF and the consistency with which it was used in the C-STAR consortium."
levin-etal-2000-lessons,Lessons Learned from a Task-based Evaluation of Speech-to-Speech Machine Translation,2000,6,3,1,1,17380,lori levin,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"For several years we have been conducting Accuracy Based Evaluations (ABE) of the JANUS speech-to-speech MT system (Gates et al., 1997) which measure quality and delity of translation. Recently we have begun to design a Task Based Evaluation for JANUS (Thomas, 1999) which measures goal completion. This paper describes what we have learned by comparing the two types of evaluation. Both evaluations (ABE and TBE) were conducted on a common set of user studies in the semantic domain of travel planning."
ries-etal-2000-shallow,Shallow Discourse Genre Annotation in {C}all{H}ome {S}panish,2000,14,10,2,0,53920,klaus ries,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The classification of speech genre is not yet an established task in language technologies. However we believe that it is a task that will become fairly important as large amounts of audio (and video) data become widely available. The technological cability to easily transmit and store all human interactions in audio and video could have a radical impact on our social structure. The major open question is how this information can be used in practical and beneficial ways. As a first approach to this question we are looking at issues involving information access to databases of human-human interactions. Classification by genre is a first step in the process of retrieving a document out of a large collection. In this paper we introduce a local notion of speech activities that are exist side-by-side in conversations that belong to speech-genre: While the genre of CallHome Spanish is personal telephone calls between family members the actual instances of these calls contain activities such as storytelling, advising, interrogation and so forth. We are presenting experimental work on the detection of those activities using a variety of features. We have also observed that a limited number of distinguised activities can be defined that describes most of the activities in this database in a precise way. Proceedings of the Second International Conference On Language Ressources And Evaluation, LREC 2000, Athens, Greece, 31st May-2nd June 2000"
W99-0306,Tagging of Speech Acts and Dialogue Games in {S}panish Call Home,1999,5,24,1,1,17380,lori levin,Towards Standards and Tools for Discourse Tagging,0,"The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow, corpus-based methods of analysis. For the Clarity project, we have tagged speech acts and dialogue games in the Call Home Spanish corpus. We have done preliminary cross-level experiments on the relationship of word and speech act n-grams to dialogue games. Our results show that the label of a game cannot be predicted from n-grams of words it contains. We get better than baseline results for predicting the label of a game from the sequence of speech acts it contains, but only when the speech acts are hand tagged, and not when they are automatically detected. Our future research will focus on finding linguistic cues that are more predictive of game labels. The automatic classification of speech acts and games is carried out in a multi-level architecture that integrates classification at multiple discourse levels instead of performing them sequentially."
P98-2185,An Interactive Domain Independent Approach to Robust Dialogue Interpretation,1998,14,3,2,0.909091,2584,carolyn rose,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"We discuss an interactive approach to robust interpretation in a large scale speech-to-speech translation system. Where other interactive approaches to robust interpretation have depended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences."
C98-2180,An Interactive Domain Independent Approach to Robust Dialogue Interpretation,1998,14,3,2,0.909091,2584,carolyn rose,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"We discuss an interactive approach to robust interpretation in a large scale speech-to-speech translation system. Where other interactive approaches to robust interpretation have depended upon domain dependent repair rules, the approach described here operates efficiently without any such hand-coded repair knowledge and yields a 37% reduction in error rate over a corpus of noisy sentences."
woszczcyna-etal-1998-modular,A modular approach to spoken language translation for large domains,1998,8,18,6,0,55460,monika woszczcyna,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The MT engine of the JANUS speech-to-speech translation system is designed around four main principles: 1) an interlingua approach that allows the efficient addition of new languages, 2) the use of semantic grammars that yield low cost high quality translations for limited domains, 3) modular grammars that support easy expansion into new domains, and 4) efficient integration of multiple grammars using multi-domain parse lattices and domain re-scoring. Within the framework of the C-STAR-II speech-to-speech translation effort, these principles are tested against the challenge of providing translation for a number of domains and language pairs with the additional restriction of a common interchange format."
W97-0410,Expanding the Domain of a Multi-lingual Speech-to-Speech Translation System,1997,6,7,2,1,13539,alon lavie,Spoken Language Translation,0,"JANUS is a multi-lingual speech-to-speech translation system, which has been designed to translate spontaneous spoken language in a limited domain. In this paper, we describe our recent preliminary efforts to expand the domain of coverage of the system from the rather limited Appointment Scheduling domain, to the much richer Travel Planning domain. We compare the two domains in terms of out-of-vocabulary rates and linguistic complexity. We discuss the challenges that these differences impose on our translation system and some planned changes in the design of the system. Initial evaluations on Travel Planning data are also presented."
C96-1075,Multi-lingual Translation of Spontaneously Spoken Language in a Limited Domain,1996,9,11,6,1,13539,alon lavie,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"JANUS is a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain. In an attempt to achieve both robustness and translation accuracy we use two different translation components: the GLR module, designed to be more accurate, and the Phoenix module, designed to be more robust. We analyze the strengths and weaknesses of each of the approaches and describe our work on combining them. Another recent focus has been on developing a detailed end-to-end evaluation procedure to measure the performance and effectiveness of the system. We present our most recent Spanish-to-English performance evaluation results."
1996.amta-1.30,{JANUS}: multi-lingual translation of spontaneous speech in limited domain,1996,-1,-1,2,1,13539,alon lavie,Conference of the Association for Machine Translation in the Americas,0,None
P95-1005,Discourse Processing of Dialogues with Multiple Threads,1995,13,52,3,0.909091,2584,carolyn rose,33rd Annual Meeting of the Association for Computational Linguistics,1,In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-to-speech translation system. We will demonstrate that theories of discourse which postulate a strict tree structure of discourse on either the intentional or attentional level are not totally adequate for handling spontaneous dialogues. We will present our extension to this approach along with its implementation in our plan-based discourse processor. We will demonstrate that the implementation of our approach outperforms an implementation based on the strict tree structure approach.
1995.tmi-1.13,Using Context in Machine Translation of Spoken Language,1995,-1,-1,1,1,17380,lori levin,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
C94-1057,The Correct Place of Lexical Semantics in Interlingual {MT},1994,13,11,1,1,17380,lori levin,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
1994.amta-1.41,{PANGLOSS},1994,0,0,7,0,10837,jaime carbonell,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
W91-0202,Syntax-Driven and Ontology-Driven Lexical Semantics,1991,18,28,2,0,32552,sergei nirenburg,Lexical Semantics and Knowledge Representation,0,"We describe the scopes of two schools in lexical semantics, which we call syntax-driven lexical semantics and ontology-driven lexical semantics, respectively. Both approaches are used in various applications at The Center for Machine Translation. We believe that a comparative analysis of these positions and clarification of claims and coverage is essential for the field as a whole."
E89-1010,Ambiguity Resolution in the {DMTRANS} {PLUS},1989,19,9,3,0,48217,hiroaki kitano,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a cost-based (or energy-based) model of disambiguation. When a sentence is ambiguous, a parse with the least cost is chosen from among multiple hypotheses. Each hypothesis is assigned a cost which is added when: (1) a new instance is created to satisfy reference success, (2) links between instances are created or removed to satisfy constraints on concept sequences, and (3) a concept node with insufficient priming is used for further processing. This method of ambiguity resolution is implemented in DMTRANS PLUS, which is a second generation bi-directional English/Japanese machine translation system based on a massively parallel spreading activation paradigm developed at the Center for Machine Translation at Carnegie Mellon University."
