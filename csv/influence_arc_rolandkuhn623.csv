2006.jeptalnrecital-poster.23,J90-2002,0,0.601185,"Missing"
2006.jeptalnrecital-poster.23,2003.mtsummit-papers.15,1,0.841473,"Missing"
2006.jeptalnrecital-poster.23,koen-2004-pharaoh,0,0.14163,"Missing"
2006.jeptalnrecital-poster.23,2005.mtsummit-papers.11,0,0.0436754,"Missing"
2006.jeptalnrecital-poster.23,W05-0820,0,0.0221996,"Missing"
2006.jeptalnrecital-poster.23,2002.jeptalnrecital-long.2,0,0.0726745,"Missing"
2006.jeptalnrecital-poster.23,P00-1056,0,0.156796,"Missing"
2006.jeptalnrecital-poster.23,P02-1038,0,0.0750405,"Missing"
2006.jeptalnrecital-poster.23,N04-1021,0,0.0385281,"Missing"
2006.jeptalnrecital-poster.23,P02-1040,0,0.0928412,"Missing"
2006.jeptalnrecital-poster.23,W05-0822,1,0.856627,"Missing"
2009.mtsummit-papers.2,J93-2003,0,0.011587,",  is the frequency of source phrase  and target phrase  linked to each other. Most of the longer phrases are seen only once in the training corpus. Therefore, relative frequencies overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being em"
2009.mtsummit-papers.2,W05-0801,0,0.014402,"occur; otherwise 0. ability assuming a binomial distribution. Without simplification, this probability can be expressed by: &quot;BC 3,   D 8 81  ED E 8,  81,  ? F G 8  (9) However, because this probability is very small, we follow (Johnson et al., 2007) in computing the negative of the natural logs of the p-value associated with the hypergeometric distribution as the feature functions: HBC ,   0log ∑L &quot;  (10) ,8,  BC Therefore, the higher the value of HBC ,  , the stronger the association between phrase and phrase  . 4) Link probability (Moore, 2005) is the conditional probability of the phrase pair being linked given that they co-occur in a given sentence pair. It is estimated as:   ,  :&quot;,   8 ,  (11) where ,   is the number of times that and  are linked in sentence pairs. 3.3 Phrase-table smoothing As (Foster et al., 2006) shows, the phrase table can be improved by applying smoothing techniques. A motivation for this is our observation that the phrase pairs which co-occur only once: 3,  1 are amazingly frequent in the phrase table even when the training corpus is very large. To compensate for this ov"
2009.mtsummit-papers.2,P03-1021,0,0.00640825,"overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being employed; • Distortion model, which assigns a penalty based on the number of source words which are skipped when generating a new target phrase; • Language model(s) trained using SRILM to"
2009.mtsummit-papers.2,P02-1038,0,0.0597455,"hrases in the target language,  is the source language string, 2 where ,  is the frequency of source phrase  and target phrase  linked to each other. Most of the longer phrases are seen only once in the training corpus. Therefore, relative frequencies overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard"
2009.mtsummit-papers.2,J03-1002,0,0.011046,"Missing"
2009.mtsummit-papers.2,P02-1040,0,0.0760108,"Missing"
2009.mtsummit-papers.2,P96-1041,0,0.128888,"Missing"
2009.mtsummit-papers.2,P08-1010,0,0.0264371,"Missing"
2009.mtsummit-papers.2,J96-1001,0,0.197505,"Missing"
2009.mtsummit-papers.2,J93-1003,0,0.216972,"Missing"
2009.mtsummit-papers.2,P06-1091,0,0.025513,"Missing"
2009.mtsummit-papers.2,W06-1607,1,0.856654,"Missing"
2009.mtsummit-papers.2,D07-1103,1,0.848548,"imes among N sentencepairs by chance, given the marginal frequencies 3 and 3 . We computed this prob1 There are at least three ways to count the number of co-occurrence of  and  , if one or both of them have more than one occurrence in a given sentence pair (Melamed, 1998). We choose to count the cooccurrence as 1 if they both occur; otherwise 0. ability assuming a binomial distribution. Without simplification, this probability can be expressed by: &quot;BC 3,   D 8 81  ED E 8,  81,  ? F G 8  (9) However, because this probability is very small, we follow (Johnson et al., 2007) in computing the negative of the natural logs of the p-value associated with the hypergeometric distribution as the feature functions: HBC ,   0log ∑L &quot;  (10) ,8,  BC Therefore, the higher the value of HBC ,  , the stronger the association between phrase and phrase  . 4) Link probability (Moore, 2005) is the conditional probability of the phrase pair being linked given that they co-occur in a given sentence pair. It is estimated as:   ,  :&quot;,   8 ,  (11) where ,   is the number of times that and  are linked in sentence pairs. 3.3 Phra"
2009.mtsummit-papers.2,N03-1017,0,0.0126144,"the features to predict the usefulness of a new association feature given the existing features. 1 Introduction Phrase-based translation is one of the dominant current approaches to statistical machine translation (SMT). A phrase translation model, incorporated in a data structure known as a phrase table, is the most important component of a phrasebased SMT system, since the translations are generated by concatenating target phrases stored in the phrase table. The pairs of source and corresponding target phrases are extracted from the word-aligned bilingual training corpus (Och and Ney, 2003; Koehn et al., 2003). These phrase pairs together with useful feature functions, are called collectively a phrase translation model or phrase table. A phrase translation model embedded in a state-of-the-art phrase-based SMT system normally exploits the feature functions involving conditional translation probabilities and lexical weights (Koehn et al., 2003). The phrase-based conditional translation probabilities are estimated from the relative frequencies of the source and target phrase in a given phrase pair. To avoid over-training, lexical weights are used to validate the quality of a phrase translation pair. T"
2009.mtsummit-papers.2,P06-1096,0,0.03176,"Missing"
2009.mtsummit-papers.2,W04-3243,0,0.0167998,"into account. The statistics can be organized in a contingency table, e.g. in Table 1. When collecting the statistics of the data, we only need to count 3,  , 3 , 3  and N; the other counts could be easily calculated accordingly. Then, we may compute the following association features: 1) Dice coefficient (Dice, 1945) as in Equation (7). It compares the co-occurrence count of phrase pair and  with the sum of the independent occurrence counts of and  . 45,  678,  8 98  (7) 2) Log-likelihood-ratio (Dunning, 1993) as in Equation (8) which is presented by Moore (2004). ::; ,  ?78?, ? 3? ,  ? =>   ∑  ?)&,1+  ?)& ,1 + 8?78 ? ∑ (8) where ? and  ? are variables ranging over  1A and & , 1 + respectively, the values @,  3 ? ,  ? is the joint count for the values of ? and  ?, 3?  and 3 ?  are the frequencies of values of ? and  ?. 3) Hyper-geometric distribution is the probability of the phrase-pair globally cooccurring 3,   times among N sentencepairs by chance, given the marginal frequencies 3 and 3 . We computed this prob1 There are at least three ways to count the number of co"
2009.mtsummit-papers.2,C96-2141,0,0.433628,"inear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being employed; • Distortion model, which assigns a penalty based on the number of source words which are skipped when generating a new target phrase; • Language model(s) trained using SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing method (Chen et al, 1996); • Word/phrase penalties. 3 3.1 Phrase Translation Model with Association-based Features Traditional phrase-table features A typical phrase translation model exploits features estimating phrase conditional translation probabilities and lexical"
2009.mtsummit-papers.2,2003.mtsummit-papers.53,0,0.0461404,"Missing"
2009.mtsummit-papers.2,N04-1033,0,0.0438114,"Missing"
2009.mtsummit-papers.2,W04-3227,0,0.0445291,"Missing"
2009.mtsummit-plenaries.15,W09-1505,1,0.837952,"ex to child nodes of ‘b’ in bytes root node value size of index to child nodes of root in bytes index key for ‘a’ coming from root relative offset of node ‘a’ (13 − 8 = 5) index key for ‘b’ coming from root relative offset of node ‘b’ (13 − 2 = 11) (c) Trie representation in a contiguous byte array. In practice, each field may vary in length. Figure 3: A count table (a) stored in a trie structure (b) and the trie’s sequential representation in a file (c). As the size of the count table increases, the trie-based storage becomes more efficient, provided that the keys have common prefixes ( from Germann et al. (2009)). on desktop computers: first, to provide a sandbox, a safe testbed for software development that allows a machine to crash without crashing the actual computer or interfering with the outside world. And second, to run two operating systems concurrently on a single computer. Since the physical host computer and the guest running as a virtual machine can communicate via networking protocols, it is possible, for example, to emulate a Linux server on a PC running Windows. Virtualization technology thus allows deployment of software developed on one OS on another without the need to port software"
2009.mtsummit-plenaries.15,W05-0822,1,0.737099,", we first briefly describe the Portage machine translation system developed at the National Research Council Canada. Section 3 introduces the concept of virtualization. In Section 4 we discuss the design and use of PortageLive, an instantiation of Portage as a virtual machine. We tested PortageLive in a number of scenarios, from small, compact machines that could run on a laptop to parallelizations on computer networks composed of workstations that are typical of office environments. The results of these experiments are reported in Section 5. Section 6 concludes the paper. 2 Portage Portage (Sadat et al., 2005) is a state-of-the-art system for phrase-based statistical machine translation (Koehn et al., 2003). The advantage of statistical approaches to machine translation is that they require much less human labor than language-specific systems carefully designed and constructed by human experts in particular languages or language pairs. Instead of human expertise, the system relies on existing collections of translations to learn how to translate. Under active development since 2004, Portage has been employed to translate between a wide variety of language pairs, involving such languages as Arabic,"
2009.mtsummit-plenaries.15,N03-1017,0,\N,Missing
2010.amta-papers.24,N04-4006,0,0.0180521,"The most obvious is to condition other models, for instance the TM, on document context. The methods outlined above could probably be adapted to this relatively easily, provided suitable smoothing techniques were used. A harder challenge is to find better ways of conditioning on the document structure information. One interesting possibility would be to assume a latent hierarchical structure for the features in order to apply recent hierachical adaptation techniques (Finkel and Manning, 2009), suitably modified for multinomial language and translation models, possibly using MAP combinations (Bacchiani et al., 2004). A related idea is to treat the features themselves as hints rather than performing the kind of hard matching used by the methods above. The multiple-feature smoothing methods are a step in this direction, and it would be interesting to apply a similar approach to the whole training corpus, in order to more accurately learn relations between features. Finally, it would be interesting to experiment with other structured domains to determine if the Hansard is an outlier either in its availability of structured information or in the degree to which this is useful for translation. Conclusion And"
2010.amta-papers.24,N03-2002,0,0.0253597,"y depends on an IBM-model training process that degrades badly on small data. For this preliminary effort, we therefore concentrated on the language model. As just mentioned, the challenge in constructing a language model p(w|h, d), where h is an ngram context for word w, is data sparsity due to the conditioning on d. The normal method for dealing with sparsity in h—backing off to shorter contexts (Goodman, 2001)—cannot be applied directly because there is no natural back-off ordering for the features in d. Methods for handling similar situations have been devised in a factored model setting (Bilmes and Kirchhoff, 2003), but these lack straightforward training procedures. We opted instead for two simple solutions: splitting d into its component features di and training a specific model for each; and clustering rare vectors d together to increase the amount of data available for each. 2.1 Feature-Specific Models The training procedure for feature-specific models is extremely simple. For each feature di in d: 1. Partition the target half of the training corpus into sets of sentences characterized by each different value that di can take on. 2. Train an LM on each corpus partition. This yields models p(w|h, dij"
2010.amta-papers.24,N09-1025,0,0.0246649,"pose that a training corpus in which each sentence pair is tagged with document features is available. (Obviously, this will apply only in cases where the training and testing domains are identical or closely related.) Given the typical SMT model structure, there are three main sites for incorporating conditioning on d: the top-level loglinear model1 , the language model (LM), and the translation model (TM). The log-linear model is not ideal for directly capturing dependence on d, since it is trained on a small development set of approximately 1000 sentences: even using a MIRA-like algorithm (Chiang et al., 2009), and assuming simple 1 That is, log p(t|s, d), assumed to be a weighted linear combination of features that can be interpreted as log probabilities. features that connect target words to d, one would have to carefully select only a very small subset of all potential features. The language and translation models can use much larger feature sets, but they also face a sparsity problem in that the number of training examples available for a particular vector d can be arbitrarily small. This is especially severe for the translation model, which lacks the powerful backoff-based smoothing algorithms"
2010.amta-papers.24,W08-0334,0,0.0880097,"mation, our method is related to information-retrieval inspired approaches to domain adaptation (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007), which seek parts of the training corpus that resemble sentences in the current document. However, those approaches pool matches across sentences, and build a single adapted model for each source document, rather than (potentially) one for each sentence. Our method is also different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, and also, unlike all previous work we are aware of, characterize sentences along multiple axes (eg, a sentence might be attributed to a particular speaker and belong to a specific section within a document). Finally, unlike anaphora resolution and discourse analysis applied to translation (Marcu et al., 2000), our method does not explicitly depend on the content of sentences other than the current one. We formalize our notion of document translation in section 2, and present two a"
2010.amta-papers.24,N09-1068,0,0.0362832,"ove, using a linear combination of feature-specific models. 5 There are many possibilities for extending this exploratory work. The most obvious is to condition other models, for instance the TM, on document context. The methods outlined above could probably be adapted to this relatively easily, provided suitable smoothing techniques were used. A harder challenge is to find better ways of conditioning on the document structure information. One interesting possibility would be to assume a latent hierarchical structure for the features in order to apply recent hierachical adaptation techniques (Finkel and Manning, 2009), suitably modified for multinomial language and translation models, possibly using MAP combinations (Bacchiani et al., 2004). A related idea is to treat the features themselves as hints rather than performing the kind of hard matching used by the methods above. The multiple-feature smoothing methods are a step in this direction, and it would be interesting to apply a similar approach to the whole training corpus, in order to more accurately learn relations between features. Finally, it would be interesting to experiment with other structured domains to determine if the Hansard is an outlier e"
2010.amta-papers.24,W07-0717,1,0.950916,"ch different value that di can take on. 2. Train an LM on each corpus partition. This yields models p(w|h, dij ), one for each jth value of each ith feature. These could be used directly for translating test-set sentences characterized by dij , but there is still no guarantee that dij occurs often enough in the training corpus to produce an LM that generalizes well. We therefore smooth using a word-level mixture with a global LM: ps (w|h, dij ) = αij p(w|h, dij ) + (1 − αij )p(w|h). (2) To set mixture weights αij , we used a dynamic smoothing technique similar to dynamic LM domain adaptation (Foster and Kuhn, 2007). First, steps 1 and 2 above are repeated on the source half of the training corpus to produce a set of source-language LMs p0 (w|h, dij ) that correspond one to one with their target-language counterparts. Then, for each dij that occurs in the current source document to 0 is learned usbe translated, a mixture weight αij ing the EM algorithm to maximize the probability of the source sentences tagged with dij , according to the source-language counterpart of (2).2 Finally, the source-side weights are simply transferred to the tar0 → α ) and used in (2). Although this get side (αij ij procedure"
2010.amta-papers.24,2005.eamt-1.17,0,0.025122,"val inspired approaches to domain adaptation (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007), which seek parts of the training corpus that resemble sentences in the current document. However, those approaches pool matches across sentences, and build a single adapted model for each source document, rather than (potentially) one for each sentence. Our method is also different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, and also, unlike all previous work we are aware of, characterize sentences along multiple axes (eg, a sentence might be attributed to a particular speaker and belong to a specific section within a document). Finally, unlike anaphora resolution and discourse analysis applied to translation (Marcu et al., 2000), our method does not explicitly depend on the content of sentences other than the current one. We formalize our notion of document translation in section 2, and present two alternative modeling techniques. Section 3 descri"
2010.amta-papers.24,W07-0711,0,0.0214402,"eing able to share information across partitions. As this does not occur with the all-versusone approach, we also tried mixing over all values of a given feature (ie, over all LMs learned from the training corpus for that feature): ps (w|h, dij ) = X αijk p(w|h, dik ), (3) k P where k αijk = 1, and for notational convenience we designate the global model—included in the mixture—as p(w|h, dij0 ). The weights αijk were learned using the dynamic smoothing procedure above. To counter overfitting due to the large number of parameters, we incorporated MAP smoothing into the EM procedure, following (He, 2007). This uses a modified M-step update: p(x) = [c(x) + P λp0 (x)]/( x c(x) + λ), where p(x) is a mixturecomponent probability, c(x) is a corresponding expected count, and p0 (x) is a prior probability for which we used weights αi learned by pooling all values of feature di in the current source document. The prior weight λ was set to 10, based on preliminary 2 For values of dij that don’t occur in the training corpus, is set to zero. 0 αij experiments with a development set.3 The procedure we have just described constructs a family of smoothed language models ps (w|h, dij ) for feature di that i"
2010.amta-papers.24,2005.eamt-1.19,0,0.0283769,"ccur in an introductory paragraph or in the main body of a text; it may be a quotation or a line spoken by a character in a work of fiction; or it may play a particular rhetorical role within a monograph. All these factors can influence translation, and hence Our approach is to characterize each sentence in a document with a vector of feature values derived from the document’s structure, then translate it with a model optimized for those values. In its use of sentence-level information, our method is related to information-retrieval inspired approaches to domain adaptation (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007), which seek parts of the training corpus that resemble sentences in the current document. However, those approaches pool matches across sentences, and build a single adapted model for each source document, rather than (potentially) one for each sentence. Our method is also different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, a"
2010.amta-papers.24,W07-0733,0,0.0372484,"operate at sentence-level granularity, and are trained on corpus partitions. Unlike us, they modify all components of their log-linear model, they make only a single binary distinction (interrogative versus declarative sentences), and they use a maxent classifier to assign these properties to source sentences rather than relying on document structure. Other relevant work is on domain adaptation. One way of viewing our approach is that it splits the source document into many micro-domains, and attempts to adapt to each. From this viewpoint, recent work on SMT adaptation (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007; Tam et al., 2007) is applicable, in addition to the IR approaches already mentioned. However, most of this work does not deal with adaptation along different axes simultaneously (for instance adapting to topic and genre). An exception is (Matsoukas et al., 2009), who build a model for weighting phrasepair joint counts during relative-frequency TM estimation that can depend on arbitrary features of the training corpus. Their feature weights are set discriminatively using a dev set and do not distinguish between sentences in that set, but it might be possible to extend the ap"
2010.amta-papers.24,W04-3250,0,0.0347196,"r to be slightly better than the log-linear. This may be due to problems with MERT, since the log-linear combinations involve five language model features instead of just one as in all other approaches in tables 3 and 4. Finally, the results for English to French translation appear to be somewhat better than for French to English translation, with the maximum gain over the baseline being approximately 0.5 BLEU points in the former case, and 0.3 BLEU points in the latter. The gains in both cases are statistically significant at the 0.95 level, however, according to paired bootstrap resampling (Koehn, 2004). This also holds for the results from the linear 1-versus-all combination in both translation settings; this technique offers a good combination of performance and efficiency, and is a good candidate for the best approach among the ones tested in this paper. 4 Related Work As mentioned above, we are unaware of any previous work on translating structured documents in SMT. The closest related work is due to Finch and Sumita (2008). Like us, they use models that operate at sentence-level granularity, and are trained on corpus partitions. Unlike us, they modify all components of their log-linear"
2010.amta-papers.24,D07-1036,0,0.0397225,"Missing"
2010.amta-papers.24,A00-2002,0,0.156394,"lso different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, and also, unlike all previous work we are aware of, characterize sentences along multiple axes (eg, a sentence might be attributed to a particular speaker and belong to a specific section within a document). Finally, unlike anaphora resolution and discourse analysis applied to translation (Marcu et al., 2000), our method does not explicitly depend on the content of sentences other than the current one. We formalize our notion of document translation in section 2, and present two alternative modeling techniques. Section 3 describes English/French translation experiments with a version of the Canadian Hansard corpus that has rich structural information encoded in XML markup. Related work is discussed in section 4, and section 5 concludes and suggests some possibilities for future work. 2 Document Translation SMT seeks the translation hypothesis tˆ that has highest probability according to a model co"
2010.amta-papers.24,D09-1074,0,0.0119958,"erties to source sentences rather than relying on document structure. Other relevant work is on domain adaptation. One way of viewing our approach is that it splits the source document into many micro-domains, and attempts to adapt to each. From this viewpoint, recent work on SMT adaptation (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007; Tam et al., 2007) is applicable, in addition to the IR approaches already mentioned. However, most of this work does not deal with adaptation along different axes simultaneously (for instance adapting to topic and genre). An exception is (Matsoukas et al., 2009), who build a model for weighting phrasepair joint counts during relative-frequency TM estimation that can depend on arbitrary features of the training corpus. Their feature weights are set discriminatively using a dev set and do not distinguish between sentences in that set, but it might be possible to extend the approach to allow for the sentencelevel dependencies required for handling structural features. We tested both these methods on a corpus of structured Hansard documents, using five features capturing mostly complementary information. Results were mildly positive across the spectrum o"
2010.amta-papers.24,P03-1021,0,0.162969,"λ was set to 10, based on preliminary 2 For values of dij that don’t occur in the training corpus, is set to zero. 0 αij experiments with a development set.3 The procedure we have just described constructs a family of smoothed language models ps (w|h, dij ) for feature di that is specific to the current source document D. To decode a sentence from D, we use the value dij that di takes on for that sentence to select the appropriate member of the family. We tried two ways of combining model families from different document features di : a log-linear combination, with model weights set by MERT (Och, 2003); and a linear combination, with weights set to maximize the likelihood of a target-language development corpus. Both combinations involve one weight per document feature di . 2.2 Clustered Models A weakness of the approach outlined in the previous section is that it implicitly assumes features are independent. Clearly this will not always be the case. Furthermore, many feature vectors occur often enough in the training corpus to allow reliable LMs to be produced. To capitalize on this, we used a simple clustering method that attempts to group lowfrequency vectors together to increase the reli"
2010.amta-papers.24,P07-1066,0,0.0217577,"e trained on corpus partitions. Unlike us, they modify all components of their log-linear model, they make only a single binary distinction (interrogative versus declarative sentences), and they use a maxent classifier to assign these properties to source sentences rather than relying on document structure. Other relevant work is on domain adaptation. One way of viewing our approach is that it splits the source document into many micro-domains, and attempts to adapt to each. From this viewpoint, recent work on SMT adaptation (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007; Tam et al., 2007) is applicable, in addition to the IR approaches already mentioned. However, most of this work does not deal with adaptation along different axes simultaneously (for instance adapting to topic and genre). An exception is (Matsoukas et al., 2009), who build a model for weighting phrasepair joint counts during relative-frequency TM estimation that can depend on arbitrary features of the training corpus. Their feature weights are set discriminatively using a dev set and do not distinguish between sentences in that set, but it might be possible to extend the approach to allow for the sentencelevel"
2010.amta-papers.24,N04-1033,0,0.0313305,"39.71 39.89 39.75 39.62 39.81 39.73 39.65 39.88 39.89 39.87 39.96 Table 3: Results for French to English translation. Feature weights were set using Och’s MERT algorithm (Och, 2003) to maximize dev-set BLEU score. The training corpus was word-aligned using both HMM and IBM2 models; the phrase table consists of the union of phrases extracted from these separate alignments, with a phrase length limit of 7. It was filtered to retain the top 30 translations for each source phrase using the TM part of the current loglinear model. Lexical probabilities were estimated using the method described in (Zens and Ney, 2004). 3.3 Results A preliminary step to generating translation results is clustering the corpus as described in section 2.2. Table 2 shows the results, for various frequencythreshold (f) values (for the 20n value, clustering was run until exactly 20 clusters remained). The results are similar for clustering on the English (for translation into English) and French (for translation into French) sides of the corpus. The largest cluster size is identical for f = 0 (no clustering) and for f = 1k, indicating that it was not merged at this threshold; in fact, the largest 100 or so clusters survived intac"
2010.amta-papers.24,C04-1059,0,0.0369248,"Missing"
2011.iwslt-evaluation.19,P07-1004,0,0.0497399,"Missing"
2011.iwslt-evaluation.19,D08-1076,0,0.0846177,"Missing"
2011.iwslt-evaluation.19,D07-1103,1,0.763628,"Missing"
2011.iwslt-evaluation.19,N03-1017,0,0.0718706,"Missing"
2011.iwslt-evaluation.19,C10-1069,1,0.846539,"Missing"
2011.iwslt-evaluation.19,P05-1074,0,0.0938211,"Missing"
2011.iwslt-evaluation.19,N06-1003,0,0.100229,"Missing"
2011.iwslt-evaluation.19,D10-1064,0,\N,Missing
2011.iwslt-evaluation.19,J93-2003,0,\N,Missing
2011.iwslt-evaluation.19,C96-2141,0,\N,Missing
2011.iwslt-evaluation.19,D09-1040,0,\N,Missing
2011.iwslt-evaluation.19,P02-1040,0,\N,Missing
2011.iwslt-evaluation.19,W06-1607,1,\N,Missing
2011.iwslt-evaluation.19,N04-1033,0,\N,Missing
2011.mtsummit-papers.30,J93-2003,0,0.05186,"SRF = Ȝ1 ×log[c(s,t)]+Ȝ2 ×log[c(s)]+Ȝ3×log[c(t)]? (7) MERT can still choose λ’s that are equivalent to using the two RF estimates. In principle, nothing is lost and a degree of freedom, achieved by dropping the additive constraint, is gained (but this extra degree of freedom may lead to search errors). The obvious objection is that this “3-count” replacement for the two RF features doesn’t model probabilities. However, the inclusion of PRF(t|s) among features can’t be justified probabilistically either. Originally, the objective function for SMT was derived via Bayes’s Theorem as P(s|t)×P(t) (Brown et al., 1993). The inclusion of P(t|s) happened later – it’s a heuristic that defies Bayes’s Theorem (Och and Ney, 2002). Once the forward and backward estimates have been unpacked into their three constituent counts, these counts can be transformed and generalized slightly by adding or subtracting constants (while ensuring the logarithm is defined). Therefore, we have two different log-linear feature sets obtained from the three basic statistics: 3-count: {log[c(s,t)], log[c(s)], log[c(t)]} Generalized 3-count: {log[c(s,t)+k1], log[c(s)+k2], log[c(t)+k3]}; where k1, k2, k3&gt; -11. “Generalized 3-count” is r"
2011.mtsummit-papers.30,D08-1076,0,0.0305775,"tice MERT details We evaluated several new and several known techniques with our in-house phrase-based SMT system, whose decoder resembles Moses (Koehn et al., 2007). In addition to phrase count features, all systems had forward and backward lexical probabilities, of the type described in (Zens and Ney, 2004), and lexicalized and distance-based distortion models. The LW estimates employed in our experiments are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. Weights on the feature functions are found by lattice MERT (LMERT) (Macherey et al., 2008). These authors pruned the lattices output by their decoder; they also aggregated lattices over iterations (clarified via personal communication with W. Macherey). By contrast, an earlier version of LMERT employed by our group (Larkin et al. 2010) did not involve pruning or aggregation. Initially, we followed the Larkin et al. algorithm: this provides rapid convergence to reasonable optima. However, we decided that some aggregation should be tried to discourage random walk behaviour. In experiments for this paper, we found that without lattice aggregation, adding features led to worse optima o"
2011.mtsummit-papers.30,P03-1021,0,0.0244482,") ≤ τ ; N [c ( s , t ) ≤ τ ] = ® ¯0 otherwise. (1) (2) c ( s, t ) − D + α (t ) × Pb ( s ) c(t ) (6) This feature “punishes” low-frequency phrase pairs. In the paper (Mauser et al., 2007), three different low-frequency features were used, with the three values of τ lying in the interval between 0.9 and 3.0 (the system in the paper allows fractional values of c(s,t)). 3 Unpacking and Transforming Feature Functions Taking just the two RF features from Equation (3), we have: SRF = Ȝ1×log[PRF(t|s)]+ Ȝ2 ×log[PRF(s|t)] The λ’s are often estimated by the minimum error rate training (MERT) algorithm (Och, 2003). For the following two, the implementation details are as in (Foster et al., 2006). Good-Turing: observed counts c are modified according to the formula (Church and Gale, 1991): cg = (c + 1) nc +1 / nc (4) 270 = Ȝ1×(log[c(s,t)]–log[c(s)])+Ȝ2×(log[c(s,t)]–log[c(t)]) = (Ȝ1+Ȝ2)× log[c(s,t)]–Ȝ1×log[c(s)]–Ȝ2×log[c(t)]. This is a combination of three terms, with an additive constraint. Wouldn’t it be simpler to fit the following expression: SRF = Ȝ1 ×log[c(s,t)]+Ȝ2 ×log[c(s)]+Ȝ3×log[c(t)]? (7) MERT can still choose λ’s that are equivalent to using the two RF estimates. In principle, nothing is lost"
2011.mtsummit-papers.30,P02-1038,0,0.650975,"e frequency (RF) estimates of “forward” probability PRF(t|s) and “backward” probability PRF(s|t) are c ( s, t ) PRF (t |s) = c ( s) c ( s, t ) PRF ( s |t ) = c (t ) where cg is a modified count replacing c in subsequent RF estimates, and nc is the number of events having count c. Kneser-Ney (modified): an absolute discounting variant with PKN (s |t ) = These RF estimates are often combined with “lexical weighting” (LW) estimates of the same probabilities PLW(t|s) and PLW(s|t), based on cooccurrence counts of the individual words making up s and t. Thus, the TM score is typically of this form (Och and Ney, 2002): STM = Ȝ1×log[PRF(t|s)]+ Ȝ2×log[PRF(s|t)]+ (3) Ȝ3 ×log[PLW(t|s)]+Ȝ4×log[PLW(s|t)]. (5) where α (t ) = D × n1+ (*, t ) / c(t ) , and Pb ( s ) = n1+ ( s,*) / ¦s n1+ (s,*) . Here, n1+(*,t) is the number of unique sourcelanguage phrases t is aligned with; n1+(s,*) has an analogous definition. PKN(t|s) is defined symmetrically. Kneser-Ney gives a bonus to phrase pairs (s,t) such that s and t have been aligned to many different phrases. Modified Kneser-Ney defines different discounts D depending on the value of c(s,t). We used “KN3”, where D1 is used when c(s,t) = 1, D2 when c(s,t) = 2, and D3 when"
2011.mtsummit-papers.30,J03-1002,0,0.00498293,"is solves the memory and speed problems and provides better performance. 4.2 Results were obtained for Chinese-to-English (CE), and French-to-English (FE). There were two CE data conditions. The first is the small data condition where only the FBIS 3 corpus (10.5M target words) is used to train the translation model. For this condition, we built the phrase table using two phrase extractors: the inhouse one extracts phrase pairs from merged counts of symmetrized IBM model 2 (Brown et al., 1993) and HMM (Vogel et al., 1996) word alignments, while the other one extracts phrase pairs from GIZA++ (Och and Ney 2003) IBM model 4 word alignments (in all other experiments, we only used the in-house extractor). The second is the large data condition where the pa3 272 Data LDC2003E14 rallel training data are from the NIST4 2009 CE evaluation (112.6M target words). We used the same two language models (LMs) for both CE conditions: a 5-gram LM trained on the target side of the large data corpus, and a 6-gram LM trained on the English Gigaword v4 corpus. We used the same development and test sets for the two CE data conditions. The development set comprises mainly data from the NIST 2005 test set, and also some"
2011.mtsummit-papers.30,D07-1103,1,0.912224,"Missing"
2011.mtsummit-papers.30,N03-1017,0,0.0136316,"owing. Section 2 will introduce some existing smoothing techniques. In section 3, we will unpack and transform the two RF feature functions and Kneser-Ney phrase table smoothing. Section 4 is the experiments and discussion. Section 5 ends the paper with conclusion and future work. 2 Existing Smoothing Techniques The phrase table consists of conditional probabilities of co-occurrence for source-language phrases s and target-language phrases t. “Relative frequency” (RF) estimates for these probabilities are obtained from a phrase pair extraction procedure applied to a bilingual training corpus (Koehn et al., 2003). Let c(s) be the count of a source phrase s, c(t) the count of a target phrase t, and c(s,t) the number of times s and t are aligned to form a phrase pair. Relative frequency (RF) estimates of “forward” probability PRF(t|s) and “backward” probability PRF(s|t) are c ( s, t ) PRF (t |s) = c ( s) c ( s, t ) PRF ( s |t ) = c (t ) where cg is a modified count replacing c in subsequent RF estimates, and nc is the number of events having count c. Kneser-Ney (modified): an absolute discounting variant with PKN (s |t ) = These RF estimates are often combined with “lexical weighting” (LW) estimates of"
2011.mtsummit-papers.30,W04-3250,0,0.109273,"e abbreviation for each system, we give in brackets 4 http://www.nist.gov/speech/tests/mt (http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_Const rainedResources.pdf provides the list of resources from which large data was drawn). 5 http://www.statmt.org/wmt10/ 273 the number of log-linear plus other weights that must be tuned for the non-lexical phrase count component of each: e.g., KN3 has two probability estimates, with associated log-linear weights λ1 and λ2 tuned by MERT, and three discounts D1, D2, and D3 (shared by forward and backward probabilities), giving (2+3) weights. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In Table 1-3, Symbols ** or * indicates that the result is significant better than the baseline at level p&lt;0.01 or p&lt;0.05 respectively. In-house extractor system (#wts) RF (2+0) 3CT (3+0) RF+LF3 (5+0) GT (2+0) RF+ELF (3+0) Gen3CT (3+3) Gen3CT+2EN (5+3) Gen3CT+Gen2EN (5+5) KN3 (2+3) KN3+ELF (3+3) NIST test 2006 2008 29.85 23.57 29.48 23.24 29.87 23.60 29.91 23.61 30.46** 24.16** 30.21 23.62 30.47** 23.90* 30.76** 24.36** 27.56 +0.85 30.91** 24.53** 27.72 +1.01 GIZA++ extractor system (#wts) RF (2+0) 3CT (3+0) RF+LF3 (5+0) GT (2+"
2011.mtsummit-papers.30,W10-1717,1,0.881935,"Missing"
2013.mtsummit-papers.23,D11-1033,0,0.0819155,"to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Because of the fact that linear mixtures occur within the log-probabilities assigned to phrase pairs, they are not accessible to standard tuning algorithms without non-trivial modification, and hence are usually trained nondiscriminatively for maximum likelihood. Our simulation is loos"
2013.mtsummit-papers.23,2011.iwslt-evaluation.18,0,0.0330396,"have been refined by (Sennrich, 2012b), who applied them to multiple sub-corpora and investigated alternative techniques such as count weighting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Beca"
2013.mtsummit-papers.23,2011.mtsummit-papers.30,1,0.785655,"bc gale bn gale nw gale wl hkh hkl hkn isi ne lex others nw fbis sinorama unv2 PT size (# pairs) 4.4M 3.6M 4.6M 1.8M 1.4M 62.2M 8.2M 26.6M 26.1M 1.9M 7.8M 19.6M 17.0M 360.1M weights mixtm-ml mixtm-samp 0.033 0.142 0.022 0.091 0.059 0.126 0.082 0.279 0.017 0.176 0.120 0.008 0.002 0.024 0.035 0.021 0.045 0.011 0.003 0.030 0.050 0.042 0.142 0.026 0.053 0.020 0.341 0.005 Table 4: Comparison of weights assigned to Chinese sub-corpora by standard and sampled ML mixture models. Table 3: Mixture TM Adaptation. ditional phrase pair estimates in both directions were obtained using Kneser-Ney smoothing (Chen et al., 2011), and were weighted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch"
2013.mtsummit-papers.23,N13-1114,1,0.88535,"Missing"
2013.mtsummit-papers.23,N12-1047,1,0.842261,"hted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 corpus Results Our first experiment compares a non-adapted baseline, trained on all available corpora, with standard maximum likelihood (ML) mixture TM adaptation as described in section 2.1 and the sampled variant described in section 2.2. The results are presented in table 3, in the form of BLEU scores averaged over both test sets for each language pair. ML mix187 ture adaptation (mixtm-ml) yields significant gains of 0.6 and 0.9 for Arabic and Chinese over the unadapted model. However, in our setting, this underperforms assigning uniform weights to all component models (mixtm-uni). Eq"
2013.mtsummit-papers.23,D08-1024,0,0.0797579,"Missing"
2013.mtsummit-papers.23,W07-0717,1,0.958969,"ues intended to address this problem have attracted significant research attention in recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it"
2013.mtsummit-papers.23,D10-1044,1,0.891722,"much better on the dev set than simplex. We conclude that it is very unlikely that a more sophisticated optimizer working with a better objective than raw BLEU would be able to find a set of linear weights that outperform the test results of our features.6 6 As noted above, however, it almost certainly does not accomplish this feat while remaining within the space of linear combinations. 188 4 Related Work Domain adaptation for SMT is currently a very active topic, encompassing a wide variety of approaches. TM linear mixture models of the kind we study here were first proposed by Foster et al (2010), and have been refined by (Sennrich, 2012b), who applied them to multiple sub-corpora and investigated alternative techniques such as count weighting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference or"
2013.mtsummit-papers.23,D08-1089,0,0.0891952,"0.035 0.021 0.045 0.011 0.003 0.030 0.050 0.042 0.142 0.026 0.053 0.020 0.341 0.005 Table 4: Comparison of weights assigned to Chinese sub-corpora by standard and sampled ML mixture models. Table 3: Mixture TM Adaptation. ditional phrase pair estimates in both directions were obtained using Kneser-Ney smoothing (Chen et al., 2011), and were weighted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 corpus Results Our first experiment compares a non-adapted baseline, trained on all available corpora, with standard maximum likelihood (ML) mixture TM adaptation as described in section 2.1 and the sampled variant described in section 2.2. The re"
2013.mtsummit-papers.23,W12-3154,0,0.053233,"ownweighting them severely, thereby discarding whatever useful information they might possess. Although linear mixtures are attractive for adaptation, they have the potential disadvantage that it is difficult to tune mixture weights directly for an SMT error metric such as BLEU. This is because, in order to allow for decoder factoring, models must be mixed at the local level, ie over ngrams or phrase/rule pairs. Thus the linear mixtures oc1 The assumption that train and test domains are fairly similar is shared by most current work on SMT domain adaptation, which focusses on modifying scores. Haddow and Koehn (2012) show that coverage problems dominate scoring problems when train and test are more distant. Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 183–190. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. cur inside the normal log probabilities that are assigned to these entities, making mixture weights inaccessible to standard tuning algorithms. Notice that log-linear mixtures do not suffer from this problem, since c"
2013.mtsummit-papers.23,W07-0733,0,0.432798,"have attracted significant research attention in recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning th"
2013.mtsummit-papers.23,P07-2045,0,0.00636067,"Missing"
2013.mtsummit-papers.23,P03-1021,0,0.0290162,"solution to the problem of setting linear mixture weights is to sidestep it by optimizing some other criterion, typically dev-set likelihood (Sennrich, 2012b), instead of BLEU. This achieves good empirical results, but leaves open the question of whether tuning linear weights directly for BLEU would work better, as one might naturally expect. This question—which to our knowledge has not yet been answered in the literature—is the one we address in this paper. Modifying standard tuning algorithms to accommodate local linear mixtures presents a challenge that varies with the algorithm. In MERT (Och, 2003) for instance, one could replace exact line maximization within Powell’s algorithm with a general-purpose line optimizer capable of handling local linear mixtures at the same time as the usual log-linear weights. For expected BLEU methods (Rosti et al., 2011), handling local weights would require computing gradient information for them. For MIRA (Chiang et al., 2008), the required modifications are somewhat less obvious. Rather than tackling the problem directly by attempting to modify our tuning algorithm, we opted for a simpler indirect approach in which features are used to simulate a discr"
2013.mtsummit-papers.23,P12-1099,1,0.872569,"ting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Because of the fact that linear mixtures occur within the log-probabilities assigned to phrase pairs, they are not accessible to standard tuning"
2013.mtsummit-papers.23,W11-2119,0,0.0207874,"ether tuning linear weights directly for BLEU would work better, as one might naturally expect. This question—which to our knowledge has not yet been answered in the literature—is the one we address in this paper. Modifying standard tuning algorithms to accommodate local linear mixtures presents a challenge that varies with the algorithm. In MERT (Och, 2003) for instance, one could replace exact line maximization within Powell’s algorithm with a general-purpose line optimizer capable of handling local linear mixtures at the same time as the usual log-linear weights. For expected BLEU methods (Rosti et al., 2011), handling local weights would require computing gradient information for them. For MIRA (Chiang et al., 2008), the required modifications are somewhat less obvious. Rather than tackling the problem directly by attempting to modify our tuning algorithm, we opted for a simpler indirect approach in which features are used to simulate a discriminatively-trained linear mixture, relying on the ability of MIRA to handle large feature sets. Our aim in doing so is not to achieve a close mathematical approximation, but rather to use features to capture the kinds of information we expect to be inherent"
2013.mtsummit-papers.23,2012.eamt-1.43,0,0.50844,"recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning them low probabilities. To avoid the co"
2013.mtsummit-papers.23,E12-1055,0,0.566545,"recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning them low probabilities. To avoid the co"
2013.mtsummit-wptp.9,2010.eamt-1.23,0,0.376849,"tion (MT) in the work flow of their translators, to make them more productive. The usual scenario is a variant of post-editing, in which an initial translation generated by MT is manually corrected by a human translator (who is now called a “post-editor” instead). Green et al. (2013) show that SMT followed by post-editing can improve translator productivity, and even translation quality. Some interesting questions arise. For instance, how should MT interact with other productivity tools used by translators, such as terminology databases and translation memories? Koehn and Senellart (2010) and Du et al. (2010) discuss these issues. Many translators resist using MT. Green et al. (2013) write bluntly: “Translators often show intense dislike for working with MT output.” This is confirmed by our own experience. Why does this particular productivity tool, unlike others mentioned above, attract so much hostility? Perhaps it is because of status anxiety: a translator who becomes a post-editor may perceive him/herself as having been proletarianized, going from being the machine’s master to its slave. However, there are also practical objections to putting out-of-the-box MT into the translation workflow. Th"
2013.mtsummit-wptp.9,2010.amta-papers.24,1,0.621795,"the phrase table to correctly place tags within segments (in addition to phrase pair information). We handle both TMX and XLIFF formats, while they handle only XLIFF (a defensible decision, as XLIFF is replacing TMX). The software described in Hudík and Ruopp (2011) has been released (M4Loc, 2012), but has undergone some subsequent changes. Finally, our work generalizes software mechanisms implemented by our colleague George Foster for transferring simple markup in Canadian parliamentary data, such as the political affili76 ations of people speaking in a debate, from one language to another (Foster et al., 2010). 4 4.1 Our Approach Data Flow We implemented two-stream tag transfer in our in-house system, a phrase-based SMT system resembling Moses (and which is licensed to translation agencies). As in Moses, the most frequent word alignment is stored with each phrase pair. Figure 1 illustrates the data flow:  XML file: This is the input file in XML format (either TMX or XLIFF).  Extract: From XML files, extract the list of input sentences to translate, including their formatting tags. For XLIFF, tags are kept as is: <g id=""i""> words… </g> for paired tags, <x id=""i""/> for isolated tags. TMX tags are m"
2013.mtsummit-wptp.9,2011.eamt-1.9,0,0.824732,"Missing"
2013.mtsummit-wptp.9,P07-2045,0,0.00403607,"Missing"
2013.mtsummit-wptp.9,W10-3806,0,0.434974,"Missing"
2014.amta-researchers.10,D11-1033,0,0.0469667,"Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets line"
2014.amta-researchers.10,W09-0432,0,0.0871166,"e six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. n"
2014.amta-researchers.10,P13-1141,0,0.0260984,"s. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing ne"
2014.amta-researchers.10,N13-1114,1,0.258614,"c-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indicate a particular combination of all these factors (Chen et al., 2013b). Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training data vary across domains, and translations across domains are unreliable. Therefore, we can often get better performance by adapting the SMT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches that have been tried for SMT model adaptation include self-training, data selection, data weighting, context-based DA, and topic-based DA. etc. We will review these techniques in the next Section. Among all these approaches, d"
2014.amta-researchers.10,P13-1126,1,0.392167,"c-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indicate a particular combination of all these factors (Chen et al., 2013b). Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training data vary across domains, and translations across domains are unreliable. Therefore, we can often get better performance by adapting the SMT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches that have been tried for SMT model adaptation include self-training, data selection, data weighting, context-based DA, and topic-based DA. etc. We will review these techniques in the next Section. Among all these approaches, d"
2014.amta-researchers.10,2011.mtsummit-papers.30,1,0.752758,"development and test sets. We use the evaluation sets from NIST 06, 08, and 09 as our development set and two test sets, respectively. All Chinese and Arabic dev and test sets have 4 references. 5.2 System Experiments were carried out with an in-house, state-of-the-art phrase-based system. The whole corpora were first word-aligned using IBM2, HMM, and IBM4 models, then split to subcorpora according to genre and origin; the phrase table was the union of phrase pairs extracted from these alignments, with a length limit of 7. The translation model (TM) was Kneser-Ney smoothed in both directions (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7, lexical weighting in both directions, word count, a distance-based reordering model, a 4-gram language model (LM) trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 5.3 Results This paper has described three modifications to existing techniques: smoothing for log-linear mixtures and two new versions of VSM - grouped VSM and distributional VSM. First, we did Al-Onaizan & Simar"
2014.amta-researchers.10,P08-2040,1,0.834358,"st improvements are for five of the six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data select"
2014.amta-researchers.10,N13-1003,0,0.0205184,"aseline, original VSM adaptation and distributional VSM adaptation. Table 5 compares original VSM and distributional VSM. It shows that the original VSM adaptation reported in (Chen et al., 2013b) does yield improvement over a non-adaptive baseline. However, if instead of computing a domain similarity score, we directly maximize BLEU on the dev set by tuning the weights of distribution features, we got further significant improvements. On the Chinese task, the further improvements were +0.7-0.8 BLEU, while on Arabic, the further improvements were smaller but still significant: +0.3-0.4 BLEU. (Cherry, 2013) showed that in the case of reordering features, directly maximizing BLEU outperforms maximum entropy optimization; the experiments in Table 5 yield a similar conclusion. Given that recently developed tuning algorithms such as MIRA can handle a very large feature set, we may consider having all possible features directly tuned to maximize BLEU (or similar criteria). Now, we compare all six DA techniques, Table 6 reports the results. All techniques improved on all test sets across two language pairs over the non-adaptive baseline, and all these improvements are significant. From the average abs"
2014.amta-researchers.10,N12-1047,1,0.887245,"Missing"
2014.amta-researchers.10,P11-2080,0,0.184624,"or Translation Model Adaptation Boxing Chen Roland Kuhn George Foster Boxing.Chen@nrc-cnrc.gc.ca Roland.Kuhn@nrc-cnrc.gc.ca George.Foster@nrc-cnrc.gc.ca National Research Council Canada, Ottawa, Canada Abstract In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indica"
2014.amta-researchers.10,D13-1107,0,0.0120428,"better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phra"
2014.amta-researchers.10,P11-2071,0,0.152169,"Missing"
2014.amta-researchers.10,D12-1025,0,0.0440133,"lations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into the SMT system. 3 Mixture model adaptation There are several adaptation scenarios for SMT, of which the most common is 1) The training material is heterogeneous, with some parts of it that are not too far from the test domain; 2) A bilingual dev set drawn from the test domain is available. A common approach to DA for this scenario is: 1) split the training data into sub-corpora by domain; 2) train sub-models or features on each sub-corpus; 3)"
2014.amta-researchers.10,P13-2119,0,0.0500397,"can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearl"
2014.amta-researchers.10,P12-2023,0,0.0197208,"in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into th"
2014.amta-researchers.10,2012.iwslt-papers.17,0,0.0367148,"Missing"
2014.amta-researchers.10,D10-1044,1,0.962878,"on about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixt"
2014.amta-researchers.10,W07-0717,1,0.867177,"mparison of Mixture and Vector Space Techniques for Translation Model Adaptation Boxing Chen Roland Kuhn George Foster Boxing.Chen@nrc-cnrc.gc.ca Roland.Kuhn@nrc-cnrc.gc.ca George.Foster@nrc-cnrc.gc.ca National Research Council Canada, Ottawa, Canada Abstract In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s s"
2014.amta-researchers.10,W06-1607,1,0.711799,"are then learned under the standard SMT log-linear framework. Experiments (see 5.3) show that this simple smoothing greatly improves the performance of log-linear mixture adaptation. 1 We use the models trained on the whole training data to align the dev set. This can be done with mgiza (http://www.kyloo.net/software/doku.php/mgiza) Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 127 3.3 Provenance features Provenance features (Chiang et al., 2011) are applied to lexical weights. There are slight variations in computing lexical weights (Foster et al., 2006), they all use forward and backward word translation probabilities T (s|t) and T (t|s) estimated from the word-aligned parallel text. The conditional probability for word pair (s, t) in a translation table is computed as below: count(s, t) . s count(si , t) T (s|t) = P (4) i We adopt the approach proposed in (Zens and Ney, 2004) to compute the lexical weights. It assumes that all source words are conditionally independent: plw (s|t) = n Y p(si |t) (5) i=1 and adopts a “noisy-or” combination, so that p(si |t) = 1 − m Y (1 − T (si |tj )) (6) j=1 where n and m are number of source and target word"
2014.amta-researchers.10,D08-1089,0,0.0150038,"as our development set and two test sets, respectively. All Chinese and Arabic dev and test sets have 4 references. 5.2 System Experiments were carried out with an in-house, state-of-the-art phrase-based system. The whole corpora were first word-aligned using IBM2, HMM, and IBM4 models, then split to subcorpora according to genre and origin; the phrase table was the union of phrase pairs extracted from these alignments, with a length limit of 7. The translation model (TM) was Kneser-Ney smoothed in both directions (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7, lexical weighting in both directions, word count, a distance-based reordering model, a 4-gram language model (LM) trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 5.3 Results This paper has described three modifications to existing techniques: smoothing for log-linear mixtures and two new versions of VSM - grouped VSM and distributional VSM. First, we did Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors"
2014.amta-researchers.10,2013.mtsummit-papers.23,1,0.900528,"Missing"
2014.amta-researchers.10,D11-1084,0,0.0212675,"from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which e"
2014.amta-researchers.10,D13-1109,0,0.187192,"Missing"
2014.amta-researchers.10,koen-2004-pharaoh,0,0.0610681,"-lin w/ smooth Chinese-English MT06 MT08 36.0 29.4 35.8 29.0 37.9**++ 31.1**++ Arabic-English MT08 MT09 46.4 49.2 47.7** 50.0** 48.2**+ 50.6**++ Table 3: Results of log-linear mixtures with or without smoothing. */** or +/++ means result is significantly better than baseline or system without smoothing (p < 0.05 or p < 0.01, respectively). experiments to see if these three modifications yield statistically significant improvements over the original techniques. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrap-resampling test for significance testing. We set the λ in Equation 3 and Equation 7 to 0.01 by looking at performance on the Arabic-English dev set. Table 3 shows the results of log-linear mixture model with or without smoothing. Without smoothing, log-linear mixture model DA hurt the performance of Chinese-to-English on both test sets, but gave moderate improvements on Arabic-to-English. The Chinese task has more (14) sub-corpora than the Arabic one, and they seem more diverse; as suggested earlier, the “veto power” of small sub-corpora seems to be particularly harmful"
2014.amta-researchers.10,W07-0733,0,0.0353986,"is can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for ea"
2014.amta-researchers.10,D07-1036,0,0.299343,"Missing"
2014.amta-researchers.10,D09-1074,0,0.268931,"uhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data"
2014.amta-researchers.10,P10-2041,0,0.0572885,"adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different"
2014.amta-researchers.10,P02-1040,0,0.100025,"=broadcast conversation, bn=broadcast news, ng=newsgroup, wl=weblog. baseline log-lin w/o smooth log-lin w/ smooth Chinese-English MT06 MT08 36.0 29.4 35.8 29.0 37.9**++ 31.1**++ Arabic-English MT08 MT09 46.4 49.2 47.7** 50.0** 48.2**+ 50.6**++ Table 3: Results of log-linear mixtures with or without smoothing. */** or +/++ means result is significantly better than baseline or system without smoothing (p < 0.05 or p < 0.01, respectively). experiments to see if these three modifications yield statistically significant improvements over the original techniques. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrap-resampling test for significance testing. We set the λ in Equation 3 and Equation 7 to 0.01 by looking at performance on the Arabic-English dev set. Table 3 shows the results of log-linear mixture model with or without smoothing. Without smoothing, log-linear mixture model DA hurt the performance of Chinese-to-English on both test sets, but gave moderate improvements on Arabic-to-English. The Chinese task has more (14) sub-corpora than the Arabic one, and they seem more diverse; as"
2014.amta-researchers.10,P12-1099,1,0.850794,"stem (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approac"
2014.amta-researchers.10,2008.iwslt-papers.6,0,0.124773,"for five of the six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of"
2014.amta-researchers.10,E12-1055,0,0.0466168,"ence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method"
2014.amta-researchers.10,P13-1082,0,0.0200048,"ixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev"
2014.amta-researchers.10,P07-1066,0,0.0266431,"he in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase ta"
2014.amta-researchers.10,W10-2602,0,0.0203879,"data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and K"
2014.amta-researchers.10,J07-1003,0,0.303587,"Missing"
2014.amta-researchers.10,N04-1033,0,0.0435798,"iza) Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 127 3.3 Provenance features Provenance features (Chiang et al., 2011) are applied to lexical weights. There are slight variations in computing lexical weights (Foster et al., 2006), they all use forward and backward word translation probabilities T (s|t) and T (t|s) estimated from the word-aligned parallel text. The conditional probability for word pair (s, t) in a translation table is computed as below: count(s, t) . s count(si , t) T (s|t) = P (4) i We adopt the approach proposed in (Zens and Ney, 2004) to compute the lexical weights. It assumes that all source words are conditionally independent: plw (s|t) = n Y p(si |t) (5) i=1 and adopts a “noisy-or” combination, so that p(si |t) = 1 − m Y (1 − T (si |tj )) (6) j=1 where n and m are number of source and target words in the phrase pair (s, t) respectively. To compute the provenance features, we first estimate the word translation tables T (s|t) and T (t|s) trained on the N sub-corpora. However, many word pairs are unseen for the word translation table of a given sub-corpus. Following (Chiang et al., 2011), we smooth the translation tables:"
2014.amta-researchers.10,P13-1140,0,0.155795,"domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into the SMT system. 3 Mixture model adaptation There are several adaptation scenarios for SMT, of which the most common is 1) The training material is heterogeneous, with some parts of it that are not too far from the test domain; 2) A bilingual dev set drawn from the test domain is available. A common approach to DA for this scenario is: 1) split the training data into sub-corpora by domain; 2) train sub-models or features on each sub-corpus; 3) weight these sub-model"
2014.amta-researchers.10,C04-1059,0,0.214758,"over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixtur"
2014.amta-researchers.10,P13-2122,0,\N,Missing
2014.amta-researchers.3,C14-1181,0,0.0319255,"Missing"
2014.amta-researchers.3,P11-1087,0,0.135447,"this makes clustering expensive. Och (1999) focuses on bilingual word clustering and discusses ideas similar to bitoken clustering, though not in the context of phrase-based SMT. Uszkoreit and Brant (2008) describe a highly efficient distributed version of exchange clustering. Faruqui and Dyer (2013) propose a bilingual word clustering method whose objective function combines same-language and cross-language mutual information. Applied to named entity recognition (NER), this yields significant improvements. Turian, Ratinov and Bengio (2010) apply Brown clustering to NER and chunking. Finally, Blunsom and Cohn (2011) improve Brown clustering by using a Bayesian prior to smooth estimates, by incorporating trigrams, and by exploiting morphological information. For word clustering, we chose a widely used program, mkcls: Blunsom and Cohn (2011) note its strong performance. We could have used POSs, but they have definitions that vary across languages; mkcls can be applied in a uniform way (though with the disadvantage that it gives each word a fixed class, instead of several possible classes as with POSs). Niesler et al (1998) found that automatically derived word classes outperform POSs. Until recently, the o"
2014.amta-researchers.3,J92-4003,0,0.619168,"Missing"
2014.amta-researchers.3,2011.mtsummit-papers.30,1,0.832739,"l-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 33 2.3. Experimental systems Eng&lt;>Fre Hansard experiments were performed with Portage, the National Research Council of Canada’s phrase-based system (this is the system described in Foster et al, 2013). The corpus was word-aligned with HMM and IBM2 models; the phrase table was the union of phrase pairs from these alignments, with a length limit of 7. We applied Kneser-Ney smoothing to find bidirectional conditional phrase pair estimates, and obtained bidirectional Zens-Ney lexical estimates (Chen et al, 2011). Hierarchical lexical reordering (Galley and Manning, 2008) was used. Additional features included standard distortion and word penalties (2 features) and a 4-gram LM trained on the target side of the parallel data: 13 features in total. The decoder used cube pruning and a distortion limit of 7. Our Chinese and Arabic baselines are strong phrase-based systems, similar to our entries in evaluations like NIST. The hierarchical lexical reordering model (HRM) of (Galley and Manning, 2008) along with the sparse reordering features of (Cherry, 2013) was used. Phrase extraction pools counts over sym"
2014.amta-researchers.3,N13-1003,0,0.247077,"g coarse biLMs like “(10,000, 10,000)” or “10,000 bi(400,400)” is infeasible. 1.2. Related work This section will discuss work on coarse models, source-side contextual information for SMT, and lexical clustering techniques (including mkcls, used for our experiments). Uszkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models. Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls. Like Cherry (2013), we found that DHRM outperformed the HRM version for Ara>Eng and Chi>Eng. However, experiments with English-French Hansard data showed only small gains for DHRM over HRM. Thus, while all the Ara>Eng and Chi>Eng experiments reported in this paper employ DHRM - a coarse reordering model - none of the Eng&lt;>Fre experiments do. In prior experiments, we also studied coarse phrase translati"
2014.amta-researchers.3,N12-1047,1,0.924126,"information from source words outside the current phrase pair is incorporated only indirectly, via target words that are translations of these source words, if the relevant target words are close enough to the current target word to affect LM scores. BiLMs address this by aligning each target word in the training data with source words to create “bitokens”. An N-gram bitoken LM is then trained. A coarse biLM is one whose words and/or bitokens have been clustered into classes. Our best results were obtained by combining coarse biLMs with coarse LMs. We tune our system with batch lattice MIRA (Cherry and Foster, 2012), which supports loglinear combinations that have many features. Figure 1 shows word-based and coarse biLMs for Eng>Fre. A target word and its aligned source words define a bitoken. Unaligned target words (e.g., French word “d’ ” in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”, aligned with two instances of “nous”) is duplicated: each target word aligned with it receives a copy of that source word. Figure 1. Creating bitokens & bitoken classes for a bilingual language model (biLM) BiLMs c"
2014.amta-researchers.3,P11-1105,0,0.0599442,"experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IBM” clustering, each word in vocabulary V initially defines a single class. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC"
2014.amta-researchers.3,N13-1073,0,0.0431711,"8) was used. Additional features included standard distortion and word penalties (2 features) and a 4-gram LM trained on the target side of the parallel data: 13 features in total. The decoder used cube pruning and a distortion limit of 7. Our Chinese and Arabic baselines are strong phrase-based systems, similar to our entries in evaluations like NIST. The hierarchical lexical reordering model (HRM) of (Galley and Manning, 2008) along with the sparse reordering features of (Cherry, 2013) was used. Phrase extraction pools counts over symmetrized word alignments from IBM2, HMM, IBM4, Fastalign (Dyer et al, 2013), and forced leave-one-out phrase alignment; the HRM pools counts in the same way. Phrase tables were Kneser-Ney smoothed as for the Eng&lt;>Fre experiments, and combined with mixture adaptation (Foster, 2007); indicator features tracked which extraction techniques produced each phrase. The Chinese system incorporated additional adaptation features (Foster et al, 2013). For both Arabic and Chinese, four LMs per system were trained: one LM on the English Gigaword corpus (5-gram with Good-Turing smoothing), one LM on monolingual webforum data and two LMs trained on selected material from the parall"
2014.amta-researchers.3,P13-2136,0,0.0343622,"Missing"
2014.amta-researchers.3,W14-1616,0,0.0813318,"ar et al, 2013; Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IBM” clustering, each word in vocabulary V initially defines a single class. Al-Onaizan & Simard (Eds."
2014.amta-researchers.3,2010.amta-papers.24,1,0.7713,"od-Turing for coarse models); we would use 8-gram coarse models (results differed only slightly along the range from 6-grams to 8-grams, but were marginally better for 8- Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 32 grams). We then began a second round of Eng>Fre experiments with the same devtest (see 2.4 and Table 4); the results informed all subsequent experiments. 2.2. Experimental data For English-French experiments in both directions, we used the high-quality Hansard corpus of Canadian parliamentary proceedings from 2001-2009 (Foster et al, 2010). We reserved the most recent five documents (from December 2009) for development and testing material, and extracted the dev and test corpora shown in Table 1. Some of the documents were much larger than typical devtest sizes, so we sampled subsets of them for the dev and test sets. Corpus # sentence pairs # words (English) Train 2.9M 60.5M Tune 2,002 40K Devtest1 2,148 43K Devtest2 2,166 45K Test1 (blind test) 1,975 39K Test2 (blind test) 2,340 49K Table 1. Corpus sizes for English&lt;>French Hansard data # words (French) 68.6M 45K 48K 50K 44K 55K For Ara>Eng and Chi>Eng, we used large-scale tr"
2014.amta-researchers.3,W07-0717,1,0.726634,"ing words/bitokens could be improved – e.g.., as in Blunsom and Cohn (2011). As a reviewer helpfully pointed out, coarse models of the same type but different granularities could be trained more efficiently with true IBM clustering (Brown et al, 1992) to create a hierarchy for words or bitokens that would yield many different granularities after a single run, rather than by running mkcls several times (once per granularity).  Coarse models could be used for domain adaptation - e.g., via mixture models that combine in-domain and out-of-domain or general-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012). In-domain statistics will be better-estimated in a coarse mixture than in a word-based one.  “Mirror-image” word-based or coarse target-to-source biLMs could be used to rescore N-best lists or lattices. If there has been word reordering, these would apply context information not seen during decoding. E.g., let source “A B C D E F G H” generate hypothesis “a b f h g c d e”, and “A” be aligned with “a”, “B” with “b”, etc. With trigram word-based biLMs, the trigrams involving “f” seen during decoding are “a_A b_B f_F”, “b_B f_F h_H”, and “f_F h_H g_G”. During mirror-image resc"
2014.amta-researchers.3,D08-1089,0,0.353237,"Missing"
2014.amta-researchers.3,D08-1039,0,0.0287366,"ective for morphologically rich languages (e.g., Ammar et al, 2013; Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IBM” clustering, each word in vocabulary V initiall"
2014.amta-researchers.3,D11-1125,0,0.0340526,"and combined with mixture adaptation (Foster, 2007); indicator features tracked which extraction techniques produced each phrase. The Chinese system incorporated additional adaptation features (Foster et al, 2013). For both Arabic and Chinese, four LMs per system were trained: one LM on the English Gigaword corpus (5-gram with Good-Turing smoothing), one LM on monolingual webforum data and two LMs trained on selected material from the parallel corpora (4-gram with Kneser-Ney smoothing); in the case of Chinese, the latter two LMs were mixture-adapted.. Both systems used the sparse features of (Hopkins and May, 2011; Cherry, 2013). The decoder used cube pruning and a distortion limit of 8. Tuning for all systems was performed with batch lattice MIRA (Cherry and Foster, 2012). The metric is the original IBM BLEU, with case-insensitive matching of n-grams up to n = 4. For all systems, we performed five random replications of parameter tuning (Clark et al, 2011). For Eng&lt;> Fre, coarse models were trained on all of “Train”. For Ara>Eng, word classes and two static coarse LMs were trained on “all” and “webforum” (no linear mixing), but biLMs were trained on “small”. For Chi>Eng, word classes and a large stati"
2014.amta-researchers.3,W07-0733,0,0.020693,"The method for hard-clustering words/bitokens could be improved – e.g.., as in Blunsom and Cohn (2011). As a reviewer helpfully pointed out, coarse models of the same type but different granularities could be trained more efficiently with true IBM clustering (Brown et al, 1992) to create a hierarchy for words or bitokens that would yield many different granularities after a single run, rather than by running mkcls several times (once per granularity).  Coarse models could be used for domain adaptation - e.g., via mixture models that combine in-domain and out-of-domain or general-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012). In-domain statistics will be better-estimated in a coarse mixture than in a word-based one.  “Mirror-image” word-based or coarse target-to-source biLMs could be used to rescore N-best lists or lattices. If there has been word reordering, these would apply context information not seen during decoding. E.g., let source “A B C D E F G H” generate hypothesis “a b f h g c d e”, and “A” be aligned with “a”, “B” with “b”, etc. With trigram word-based biLMs, the trigrams involving “f” seen during decoding are “a_A b_B f_F”, “b_B f_F h_H”, and “f_F h_H g_G”. D"
2014.amta-researchers.3,J06-4004,0,0.276637,"Missing"
2014.amta-researchers.3,W11-2124,0,0.461366,"rc.gc.ca Eric Joanis Eric.Joanis@nrc.gc.ca George Foster* George.Foster@nrc.gc.ca All authors originally at: National Research Council, Ottawa, Canada K1A 0R6 * This author is now at Google Inc., Mountain View, California 94043 Abstract Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these “coarse models”. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse"
2014.amta-researchers.3,E99-1010,0,0.37772,"Missing"
2014.amta-researchers.3,E12-1055,0,0.0220636,"d be improved – e.g.., as in Blunsom and Cohn (2011). As a reviewer helpfully pointed out, coarse models of the same type but different granularities could be trained more efficiently with true IBM clustering (Brown et al, 1992) to create a hierarchy for words or bitokens that would yield many different granularities after a single run, rather than by running mkcls several times (once per granularity).  Coarse models could be used for domain adaptation - e.g., via mixture models that combine in-domain and out-of-domain or general-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012). In-domain statistics will be better-estimated in a coarse mixture than in a word-based one.  “Mirror-image” word-based or coarse target-to-source biLMs could be used to rescore N-best lists or lattices. If there has been word reordering, these would apply context information not seen during decoding. E.g., let source “A B C D E F G H” generate hypothesis “a b f h g c d e”, and “A” be aligned with “a”, “B” with “b”, etc. With trigram word-based biLMs, the trigrams involving “f” seen during decoding are “a_A b_B f_F”, “b_B f_F h_H”, and “f_F h_H g_G”. During mirror-image rescoring, the biLM t"
2014.amta-researchers.3,P10-1040,0,0.132951,"Missing"
2014.amta-researchers.3,P08-1086,0,0.06041,"Missing"
2014.amta-researchers.3,D13-1138,0,0.0689579,"Missing"
2014.amta-researchers.3,N13-1002,0,0.0639192,"oarse language models. These are particularly effective for morphologically rich languages (e.g., Ammar et al, 2013; Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IB"
2016.amta-researchers.8,D11-1033,0,0.260007,"high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity betwee"
2016.amta-researchers.8,W15-3003,0,0.0599708,"ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domai"
2016.amta-researchers.8,K16-1031,1,0.892142,"15) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain and four language directions, show that this SSCNN method yields signiﬁcantly higher BLEU scores for the"
2016.amta-researchers.8,W12-3131,0,0.0978263,"an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO"
2016.amta-researchers.8,P14-1129,0,0.0302692,"a subset of data to be used for training an SMT system from a bilingual corpus, the user must specify the number N of sentence pairs to be chosen. The N sentence pairs with the highest global scores S(s, t) will be selected. This method is symmetrical - the roles of the source-language and target-language sides of the corpus are the same - and bilingual, because the IBM model 1 measures the degree to which each target sentence t is a good translation of its partner s, and vice versa. 2.2 Data Selection with Neural Net Joint Model (NNJM) The Neural Network Joint Model (NNJM), as described in (Devlin et al., 2014), is a joint language and translation model based on a feedforward neural net (NN). It incorporats a wide span of contextual information from the source sentence, in addition to the traditional n-gram information from preceding target-language words. Speciﬁcally, when scoring a target word wi , the NNJM inputs not only the n − 1 preceding words wi−n+1 , ..., wi−1 , but also 2m + 1 source words: the source word si most closely aligned with wi along with the m source words si−m , ..., si−1 to the left of si and the m source words si+1 , ..., si+m to the right of si . The NNJMs used in our experi"
2016.amta-researchers.8,P13-2119,0,0.133955,"picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language mod"
2016.amta-researchers.8,2015.mtsummit-papers.10,0,0.686015,"of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain"
2016.amta-researchers.8,2012.amta-papers.7,1,0.926825,"Missing"
2016.amta-researchers.8,2010.eamt-1.26,0,0.0690526,"ent over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URF"
2016.amta-researchers.8,N15-1011,0,0.0239437,"de up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al"
2016.amta-researchers.8,P14-1062,0,0.00850774,"on and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts"
2016.amta-researchers.8,D14-1181,0,0.00555345,"pair is made up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation"
2016.amta-researchers.8,W04-3250,0,0.208233,"f-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property for Arabic-to-English. In the table, the bilingua"
2016.amta-researchers.8,D07-1036,0,0.0723693,"Missing"
2016.amta-researchers.8,2011.iwslt-papers.5,0,0.0415047,"_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT s"
2016.amta-researchers.8,P10-2041,0,0.263994,"election, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language m"
2016.amta-researchers.8,J05-4003,0,0.19056,"pairs, can beneﬁt NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-lang"
2016.amta-researchers.8,W11-2124,0,0.186448,"nce. Essentially, it scores the extent to which both the source and target sentence are in-domain, but does not in any way penalize bad translations. We say that such a method is “symmetric”: it incorporates equal amounts of information from the source and the target language, but it is not “bilingual”: it does not incorporate information about the quality of translations. The main motivation for this paper is to explore CNN-based data selection techniques that are bilingual. It is based on semi-supervised CNNs that use bitokens as units instead of source or target words (Marino et al., 2006; Niehues et al., 2011). For the bitoken semi-supervised CNN, we should use the abbreviation “Bi-SSCNN”. We also experiment with the bilingual method that combines IBM model 1 and language model (LM) scores and neural network joint model. In this paper, we carried out experiments reported on two language pairs: Chinese-toEnglish and Arabic-to-English. We ﬁx the number of training sentences to be chosen for the data selection techniques so that they can be fairly compared, and measure the BLEU score on test data from the resulting MT systems. It turns out that three techniques have roughly the same performance in ter"
2016.amta-researchers.8,P02-1040,0,0.0972421,"ence as the criterion. This is considered to be a state-of-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property fo"
2016.amta-researchers.8,W16-2323,0,0.0611686,"Missing"
2016.amta-researchers.8,2014.amta-researchers.3,1,0.757845,"h SSCNNs that take as input the bitokens of (Marino et al., 2006; Niehues et al., 2011). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 2: Bitoken sequence. The paper (Niehues et al., 2011) describes a “bilingual language model” (biLM): the idea that SMT systems would beneﬁt from wider contextual information from the source sentence. BiLMs provide this context by aligning each target word in the training data with source words to create bitokens. An n-gram bitoken LM for the sequence of target words is then trained. Figure 2 (taken from (Stewart et al., 2014)) shows how a bitoken sequence is obtained from a word-aligned sentence pair for the English to French language pair. Unaligned target words (e.g., French word “d´’’ in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”) aligned with two instances of “nous” is duplicated: each target word aligned with it receives a copy of that source word. The word embeddings for bitokens are learned directly by word2vec, treating each bitoken as a word. For instance, in the French sentence shown in Figure 2,"
2016.amta-researchers.8,P15-2058,0,0.0114968,"ng sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts of in-domain data are available, we chose to u"
2016.amta-researchers.8,I08-2088,0,0.0573278,"e two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the d"
2016.amta-researchers.8,C04-1059,0,0.0497328,"will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for b"
2020.coling-main.516,L18-1530,0,0.0288831,"Missing"
2020.coling-main.516,2008.tc-1.1,1,0.514412,"est parallel corpus for an Indigenous language of the Americas or a polysynthetic language released to date (1.3 million aligned sentence pairs). The corpus is available at the NRC Digital Repository3 under the CC-BY-4.0 license; creation of the corpus and preliminary machine translation experiments are described in Joanis et al. (2020).4 The existence of the Nunavut Hansard has made possible a shared task for the Inuktut–English language pair in the 2020 Workshop for Machine Translation.5 Several years ago, researchers at NRC built a search engine for English-to-Inuktut translators, WeBInuk (Désilets et al., 2008). Given an English word or phrase, it would return matching Inuktut–English sentence pairs from a parallel corpus (the portion of the Nunavut Hansard that was then available). This online service lapsed for several years. One of the goals of the ILT project was to create a new bidirectional version of WeBInuk—i.e., to allow users to enter Inuktut search terms as well as English ones. We also aimed at providing other tools for Inuktut: a dictionary, a gister (i.e. a service that provides rough English renderings of the component morphemes of an Inuktut word), a spell checker, etc. Inuktut words"
2020.coling-main.516,2020.lrec-1.307,1,0.797532,"Missing"
2020.coling-main.516,2020.sltu-1.51,1,0.767945,"Missing"
2020.coling-main.516,E09-2008,0,0.0371998,"existing corpora are not large enough to produce accurate statistical models. 3.1.2 WordWeaver Since many Indigenous languages have rich inflectional morphology, we decided to carry out Owennatékha’s request by building a software tool that could be extended beyond Kanyen’kéha to other languages. The structure of the WordWeaver ecosystem consists of two main parts: a front-end interface (WordWeaver UI) implemented in Angular, and a back-end database and API implemented in Python (Fastapi & CouchDB). Initially, WordWeaver was tightly coupled to the instance’s language model, specifically Foma (Hulden, 2009). However, all data is now stored in a database. This architecture allows verb conjugators to be made without requiring knowledge about FSTs. 3.1.3 WordWeaver Instances The first instance of WordWeaver, Kawennón:nis, models the Western dialect of Kanyen’kéha that is taught at the Onkwawenna Kentyohkwa school. Kawennón:nis means “It Makes Words” in the language. We have since also created an instance for the Eastern dialect, spoken in the Kahnawà:ke community in Quebec. To design rules for the first Kawennón:nis FST, we relied on a textbook that describes the Western dialect (Maracle, 2017), al"
2020.coling-main.516,2020.lrec-1.312,1,0.642381,"om the Government of Nunavut and the Pirurvik Centre, the NRC project team created and released the sentence-aligned Inuktut–English “Nunavut Hansard” corpus based on the proceedings of the Legislative Assembly of Nunavut from April 1999 to June 2017. We believe this to be the largest parallel corpus for an Indigenous language of the Americas or a polysynthetic language released to date (1.3 million aligned sentence pairs). The corpus is available at the NRC Digital Repository3 under the CC-BY-4.0 license; creation of the corpus and preliminary machine translation experiments are described in Joanis et al. (2020).4 The existence of the Nunavut Hansard has made possible a shared task for the Inuktut–English language pair in the 2020 Workshop for Machine Translation.5 Several years ago, researchers at NRC built a search engine for English-to-Inuktut translators, WeBInuk (Désilets et al., 2008). Given an English word or phrase, it would return matching Inuktut–English sentence pairs from a parallel corpus (the portion of the Nunavut Hansard that was then available). This online service lapsed for several years. One of the goals of the ILT project was to create a new bidirectional version of WeBInuk—i.e.,"
2020.coling-main.516,C18-1222,1,0.853321,"ption bottleneck for speech recordings, software for implementing text prediction and read-along audiobooks for Indigenous languages, and several other subprojects. 1 Introduction This paper describes the Indigenous Languages Technology (ILT) project at the National Research Council of Canada (NRC). Phase I of this project received funding of $6 million over three years in the March 2017 budget of the Government of Canada; phase II is ongoing. The project’s goal is to produce software that will enhance the efforts of Indigenous communities in Canada to preserve and revitalize their languages. Littell et al. (2018) surveys many different efforts by a variety of organizations that have the same goal. This paper depicts a tiny corner of a big canvas; for space reasons, it does not even cover all aspects of our project. For a detailed technical report on the entire ILT project, see Kuhn et al. (2020). Different communities have very different linguistic needs, so the ILT project was made up of a diverse set of subprojects. Because there is relatively little textual or speech data for Indigenous languages in Canada (with the partial exception of Inuktut), most of the technologies developed within the projec"
2020.coling-main.516,C16-1328,1,0.815658,"perform in a “zeroshot” scenario (that is, where there is no data available in the language in question). One constructs an approximate mapping between target language phonemes and phonemes in a high-resource “donor” language (in our case, English), converts the target document into phones in the donor language, and trains an acoustic model on the donor language to recognize when each word is spoken. This is the assumed default when working with a new language in the Festival toolkit (Black et al., 1998). Based on this concept, our ReadAlong Studio9 combines a custom G2P engine, the PanPhon (Mortensen et al., 2016) phonetic distance library, and the lightweight PocketSphinx (Huggins-Daines et al., 2006) speech recognition library to allow a non-expert user to create a text/speech alignment system for a new language. ReadAlong Studio currently supports 22 languages; among Indigenous languages spoken in Canada it supports Anishinaabemowin (Ojibway), Atikamekw, Dakelh, East Cree, Gitxsan, Heiltsuk, Inuktut, Kanyen’kéha, Kwak’wala, SENĆOŦEN, Seneca, Tagish, Tŝilhqot’in, and Tsuut’ina (the other eight are not spoken in Canada). Adding a new language is the work of only a few hours, depending on the complexit"
2020.coling-main.516,wittenburg-etal-2006-elan,0,0.2916,"Missing"
2020.lrec-1.312,W18-1810,0,0.0358313,"Missing"
2020.lrec-1.312,C10-2010,0,0.0386339,"gorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence alignment (Thompson and Koehn, 2019) is reported to significantly improve alignment quality by incorporating bilingual word embeddings trained on large pre-existing parallel corpora. We experimented with this approach as well (see §3.3.). 3. Alignment of the Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 The Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 (NH 3.0) consists of 17,330,271 English words in 1,452,347 senten"
2020.lrec-1.312,J93-2003,0,0.18769,"nd chrF on dev and had the best BLEU and second-best chrF on devtest, with very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though NH 3.0 falls between their low-resource and high-resource settings. 18 github.com/rsennrich/subword-nmt 4.3.2. Statistical Machine Translation We trained our statistical machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 word"
2020.lrec-1.312,N12-1047,0,0.0144963,"hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE vocabularies as used in NMT. On the basis of BLEU and chrF results on dev and devtest for NH 3.0, we selected 30,000 merges for all reported English–Inuktitut SMT experiments, and 5,000 merges for Inuktitut–English SMT experiments. 4.4. Results and Evaluation We experiment with variations on the corpus to understand the effects of data size, recency, and alignment quality o"
2020.lrec-1.312,N13-1003,0,0.0124969,"near phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE"
2020.lrec-1.312,P14-1129,0,0.0349202,"gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE vocabularies as used in NMT. On the basis of BLEU and chrF results on dev and devtest for NH 3.0, we selected 30,000 merges for all reported English–Inuktitut SMT experiments, and 5,00"
2020.lrec-1.312,W19-6620,0,0.0346965,"U and chrF (Popović, 2015) results on dev and devtest, we selected 2,000 merges for all reported English–Inuktitut NMT experiments, and 5,000 merges for Inuktitut–English NMT experiments. Translating into English, 5,000 merges produced the best results (or tied for best) on both metrics. Into Inuktitut, 2,000 merges tied for the best result in terms of BLEU and chrF on dev and had the best BLEU and second-best chrF on devtest, with very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though NH 3.0 falls between their low-resource and high-resource settings. 18 github.com/rsennrich/subword-nmt 4.3.2. Statistical Machine Translation We trained our statistical machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. T"
2020.lrec-1.312,J93-1004,0,0.919827,"nal sentence into the target language. 2 assembly.nu.ca/hansard We are unable to confirm the exact date at which this change in production occurred, or whether it happened gradually. 3 2.3. Prior Releases and Work on Alignment The first Nunavut Hansard parallel corpus was released to the research community in 2003 (Martin et al., 2003). Nunavut Hansard 1.0 (NH 1.0), covered 155 days of proceedings of the Nunavut Assembly, from April 1, 1999 to November 1, 2002; it comprised 3,432,212 English tokens and 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Li"
2020.lrec-1.312,E17-3017,0,0.0318897,"e translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural machine translation system uses the Transformer architecture (Vaswani et al., 2017) implemented in Sockeye and following similar parameter settings to those described in Hieber et al. (2017): a 6-layer encoder, 6-layer decoder, a model dimension of 512 and 2048 hidden units in the feed-forward networks. As our byte-pair vocabularies were disjoint, we did not use weight-tying. The network was optimized using Adam (Kingma and Ba, 2015), with an initial learning rate of 10−4 , decreasing by a factor of 0.7 each time the development set BLEU did not improve for 8,000 updates, and stopping early when BLEU did not improve for 32,000 updates. We experimented with BPE vocabularies with 0.5, 1, 2, 5, 10, 15, 20, 25, 30, and 60 thousand merges. The maximum sentence length was set to 200, a"
2020.lrec-1.312,D11-1125,0,0.00975563,"machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tunin"
2020.lrec-1.312,P07-1019,0,0.0204863,"a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE vocabularies as used in NMT. On the basis of BLEU and chrF results on dev and devtest for NH 3.0, we selected 30,000 merges for all reported English–Inuktitut SMT experiments, and 5,000 merges for Inuktitut–English SMT experiments. 4.4. Results and Evaluation We experiment with variations on the corpus to understand the effects of data size, recency, and alignment quality on machine translation output. In keeping with prior work, most of our"
2020.lrec-1.312,W18-2709,0,0.0526136,"Missing"
2020.lrec-1.312,W06-3114,0,0.103089,"he NMT systems outperform SMT on the NH 2.0 size (approximately 6.7 million English tokens).19 The size comparison is not entirely fair on its own: it conflates data size and recency. To examine recency effects, we also selected subsets from the end of the NH 3.0 training corpus that matched the corresponding “(older)” subsets in size; we denote these “(recent)” in the table. For both NMT and SMT, we observe large (often 10+ BLEU) recency effects, with more recent data performing better. BLEU has long been criticized for not correlating well enough with human judgments on translation quality (Koehn and Monz, 2006; Ma et al., 2019). This problem is more apparent when evaluating translation into morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative lan"
2020.lrec-1.312,N03-1017,0,0.0335596,"very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though NH 3.0 falls between their low-resource and high-resource settings. 18 github.com/rsennrich/subword-nmt 4.3.2. Statistical Machine Translation We trained our statistical machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al.,"
2020.lrec-1.312,P07-2045,0,0.0139792,"nonbreaking characters on the Inuktitut side of the data (and thus not tokenized). 16 uniconv is distributed with Yudit: www.yudit.org Note that this romanization pipeline does not fully conform to all spelling and romanization conventions described in the Nunavut Utilities plugins for Microsoft Word (www.gov.nu.ca/ culture-and-heritage/information/computer-tools); we use the pipeline described here solely for MT experiments. 17 2567 Following conversion to romanized script, we ran identical preprocessing with English defaults on both the Inuktitut and English sides of the corpus using Moses (Koehn et al., 2007) scripts: punctuation normalization, tokenization (with aggressive hyphen splitting), cleaned the training corpus (sentence length ratio 15, minimum sentence length 1, maximum 200), trained a truecaser on the training data and then applied it to all data. We trained byte-pair encoding models on the training data using subword-nmt,18 with disjoint vocabularies ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman a"
2020.lrec-1.312,D18-2012,0,0.0306536,"e underlying form of many morphemes (Micher, 2017). Here is a typical example sentence from this corpus: ᐅᖃᖅᑏ, ᒐᕙᒪᒃᑯᑦ ᐊᐅᓚᔾᔭᐃᒋᐊᓪᓚᑦᑖᕋᓱᒻᒪᑕ ᐅᖃᐅᓯᖅ ᐆᒻᒪᖅᑎᑕᐅᒃᑲᓐᓂᕋᓱᓪᓗᓂ ᐊᒻᒪᓗ ᐃᓕᓐᓂᐊᖅᑏᑦ ᐃᓕᓐᓂᐊᖅᑎᑕᐅᓗᑎᒃ. uqaqtii, gavamakkut aulajjaigiallattaarasummata uqausiq uummaqtitaukkannirasulluni ammalu ilinniaqtiit ilinniaqtitaulutik. Mr. Speaker, this government is taking concrete action on language revitalization and student outcomes. This complexity contributes to the scientific interest of this corpus, raising questions like whether contemporary subword methods in natural language processing (Sennrich et al., 2015; Kudo and Richardson, 2018, among others) are sufficient to process one of the world’s most morphologically complex languages, and if not, how to integrate prior knowledge of Inuktitut morphology in a robust way. The work described in this paper can also be viewed in the context of a wide range of language technologies currently being developed for Indigenous languages spoken in Canada, as surveyed by Littell et al. (2018). 2.2. Legislative Assembly of Nunavut and Hansard The Legislative Assembly of Nunavut conducts business in both Inuktut and English, with the choice of language influenced by both Member preference a"
2020.lrec-1.312,W10-1717,1,0.866261,"es,6 with some heuristics to correct errors.7 The output is plain text in UTF-8 with one paragraph per line. For the other sessions and for all English documents, we used AlignFactory8 (AF) to extract the collection of paragraphs out of the Word documents.9 There are many opensource tools that can do the same task with similar results, but we find this commercial tool reliable and easy to use. AF creates a TMX file from which we extract the text to plain text files in UTF-8 with one paragraph per line. Segmentation of the paragraphs in both languages into sentences was done using the Portage (Larkin et al., 2010) sentence splitter, which is part of the Portage tokenizer. To remove parts of the corpus that were clearly not parallel, we searched for outliers using various heuristics: documents with unusually large or small paragraph or text length ratios between the languages; with similar imbalances in the appendices; or where a first pass of the baseline alignment methodology (see §3.3.) had unusually low alignment probability. These outliers were visually inspected and nonparallel text was manually removed. Examples of problems found include: an appendix occurring only in one language; an appendix oc"
2020.lrec-1.312,li-etal-2010-enriching,0,0.0497906,"Missing"
2020.lrec-1.312,C18-1222,1,0.771688,"outcomes. This complexity contributes to the scientific interest of this corpus, raising questions like whether contemporary subword methods in natural language processing (Sennrich et al., 2015; Kudo and Richardson, 2018, among others) are sufficient to process one of the world’s most morphologically complex languages, and if not, how to integrate prior knowledge of Inuktitut morphology in a robust way. The work described in this paper can also be viewed in the context of a wide range of language technologies currently being developed for Indigenous languages spoken in Canada, as surveyed by Littell et al. (2018). 2.2. Legislative Assembly of Nunavut and Hansard The Legislative Assembly of Nunavut conducts business in both Inuktut and English, with the choice of language influenced by both Member preference and the topic at hand (Okalik, 2011), and its proceedings, the Nunavut Hansard, are published in both Inuktitut and English. “The policy of Nunavut’s Hansard editors is to provide a verbatim transcript with a minimum of editing and without any alteration of the meaning of a Member’s speech.”2 Prior to approximately 2005–2006,3 the English version of the Hansard was a transcription of the English sp"
2020.lrec-1.312,W19-5358,1,0.847114,"0+ BLEU) recency effects, with more recent data performing better. BLEU has long been criticized for not correlating well enough with human judgments on translation quality (Koehn and Monz, 2006; Ma et al., 2019). This problem is more apparent when evaluating translation into morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Finnish and Turkish, at both sentence level and document level (Ma et al., 2018; Ma et al., 2019). In general, the YiSi-0 results follow the same trends as BLEU: (1) better aligned training data improves translation quality; (2) larger training data improves translation quality; and (3) more recent training data improves translation quality. 19 For NMT, we use the settings found to be optimal for"
2020.lrec-1.312,W15-1521,0,0.0139794,"t/llab/www/ 13 In an ideal world, we would have wanted a dev set to tune the BPE meta-parameters, but the desired output of this work is the best corpus possible and we only have a small amount of manually annotated data, so we simply retain the BPE settings that yield the best results over our gold standard. Shortly before submission, Vecalign (Thompson and Koehn, 2019) reported a new state of the art in alignment, using on high-quality pre-trained bilingual word vectors (“bivectors”). Since pretrained bivectors for Inuktitut– English do not exist, to the best of our knowledge, we use bivec (Luong et al., 2015) to train bivectors on our best system output (BPE 10k joint), and then use the results to realign the whole corpus with vecalign.14 The embeddings, of dimension 1024, are trained for 5 iterations using skip-gram, a maximum skip length between words of 11, 10 negative samples, a hierarchical softmax, and without discarding any words based on frequency counts. We create Vecalign overlaps using overlaps.py and an overlap window of 15 for the source and the target separately. Using bivec’s word embeddings, we average the word embeddings for each sentence in the overlap sets to produce their sente"
2020.lrec-1.312,W18-6450,0,0.029608,"to morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Finnish and Turkish, at both sentence level and document level (Ma et al., 2018; Ma et al., 2019). In general, the YiSi-0 results follow the same trends as BLEU: (1) better aligned training data improves translation quality; (2) larger training data improves translation quality; and (3) more recent training data improves translation quality. 19 For NMT, we use the settings found to be optimal for NH 3.0 throughout our remaining experiments; this may be suboptimal, as noted in Sennrich and Zhang (2019), but we still would not expect the smaller corpora to outperform the larger in this scenario. However, as we do not yet have human judgments on MT into Inuktitut, we urge c"
2020.lrec-1.312,W19-5302,0,0.0363237,"orm SMT on the NH 2.0 size (approximately 6.7 million English tokens).19 The size comparison is not entirely fair on its own: it conflates data size and recency. To examine recency effects, we also selected subsets from the end of the NH 3.0 training corpus that matched the corresponding “(older)” subsets in size; we denote these “(recent)” in the table. For both NMT and SMT, we observe large (often 10+ BLEU) recency effects, with more recent data performing better. BLEU has long been criticized for not correlating well enough with human judgments on translation quality (Koehn and Monz, 2006; Ma et al., 2019). This problem is more apparent when evaluating translation into morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Fi"
2020.lrec-1.312,W03-0320,0,0.447846,"the Hansard. The Hansard contains parenthetical annotations of when interpretation occurs (e.g., “interpretation begins”, “interpretation ends”). When the transcriber considers the interpretation to contain an error (e.g., missing information), they will instead translate the original sentence into the target language. 2 assembly.nu.ca/hansard We are unable to confirm the exact date at which this change in production occurred, or whether it happened gradually. 3 2.3. Prior Releases and Work on Alignment The first Nunavut Hansard parallel corpus was released to the research community in 2003 (Martin et al., 2003). Nunavut Hansard 1.0 (NH 1.0), covered 155 days of proceedings of the Nunavut Assembly, from April 1, 1999 to November 1, 2002; it comprised 3,432,212 English tokens and 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through"
2020.lrec-1.312,W05-0809,0,0.105721,"ior Releases and Work on Alignment The first Nunavut Hansard parallel corpus was released to the research community in 2003 (Martin et al., 2003). Nunavut Hansard 1.0 (NH 1.0), covered 155 days of proceedings of the Nunavut Assembly, from April 1, 1999 to November 1, 2002; it comprised 3,432,212 English tokens and 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence"
2020.lrec-1.312,W17-0114,1,0.9037,"sembly (as well as their interpreters and transcribers) have diverse linguistic backgrounds, users of this corpus should not necessarily take any given word or sentence to be representative of any particular variety of Inuktitut. Inuktut is known for having considerable morphological complexity. Words are typically multi-morphemic, with roots followed by multiple suffixes/enclitics from an inventory of hundreds of possible elements. Moreover, these el2562 ements undergo complex (albeit regular) morphophonemic changes in contact with each other, obscuring the underlying form of many morphemes (Micher, 2017). Here is a typical example sentence from this corpus: ᐅᖃᖅᑏ, ᒐᕙᒪᒃᑯᑦ ᐊᐅᓚᔾᔭᐃᒋᐊᓪᓚᑦᑖᕋᓱᒻᒪᑕ ᐅᖃᐅᓯᖅ ᐆᒻᒪᖅᑎᑕᐅᒃᑲᓐᓂᕋᓱᓪᓗᓂ ᐊᒻᒪᓗ ᐃᓕᓐᓂᐊᖅᑏᑦ ᐃᓕᓐᓂᐊᖅᑎᑕᐅᓗᑎᒃ. uqaqtii, gavamakkut aulajjaigiallattaarasummata uqausiq uummaqtitaukkannirasulluni ammalu ilinniaqtiit ilinniaqtitaulutik. Mr. Speaker, this government is taking concrete action on language revitalization and student outcomes. This complexity contributes to the scientific interest of this corpus, raising questions like whether contemporary subword methods in natural language processing (Sennrich et al., 2015; Kudo and Richardson, 2018, among others) are suffi"
2020.lrec-1.312,W18-4807,1,0.88702,"oming data sparsity for alignment, we perform experiments on subword and morphological segmentations of the text. The morphological experiments (Morpho surface and Morpho deep) use morphological segmentation, replacing Inuktitut words with either sequences of surface forms (simple morphological word segmentation) or sequences of deep forms (segmentation with substitution). The morphological analysis was done on romanized words using Uqailaut (Farley, 2009). Words that had no analysis from the Uqailaut analyzer were subsequently processed with the neural analyzer described by Micher (2017) and Micher (2018). While the Uqailaut analyzer produced multiple analyses per word, only the first analysis was used in these experiments. Furthermore, as the neural analyzer only produces a 1-best 2565 analysis for each word, each word in the full corpus has a single, 1-best analysis.12 We follow the same 4-pass alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of surface or deep forms instead of words. The morphological segmentation experiments rely on the existence of a morphological analyzer, a tool that is not available for all languages. To demonstrate that simp"
2020.lrec-1.312,moore-2002-fast,0,0.364503,"d 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence alignment (Thompson and Koehn, 2019) is reported to significantly improve alignment quality by incorporating bilingual word embeddings trained on large pre-existing parallel corpora. We experimented with this approach as well (see §3.3.). 3. Alignment of the Nunavut Hansard Inuktitut–English Parallel Cor"
2020.lrec-1.312,N12-1040,0,0.227577,"n the full corpus has a single, 1-best analysis.12 We follow the same 4-pass alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of surface or deep forms instead of words. The morphological segmentation experiments rely on the existence of a morphological analyzer, a tool that is not available for all languages. To demonstrate that simple heuristics can also perform well in the absence of morphological analysis tools, we performed experiments using the Prefix of each word as a stand-in for more complicated lemmatization (Simard et al., 1992; Och, 2005; Nicholson et al., 2012). As a polysynthetic language, Inuktitut has many very-low frequency words, and even English words have some morphology which a stemmer could normalize. Since syllabic characters represent whole syllables, we chose a prefix of three characters to approximate the stem of an Inuktitut word and we use a five letter prefix for English words. We follow the same alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of prefixes instead of words. Byte-pair encoding (BPE) is another approach to word segmentation that does not rely on existing language-specific too"
2020.lrec-1.312,J03-1002,0,0.0685157,"Missing"
2020.lrec-1.312,P02-1040,0,0.109109,"te-pair encoding models on the training data using subword-nmt,18 with disjoint vocabularies ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman and Federico, 2018; Micher, 2018; Toral et al., 2019). 4.3. Baseline Experiments We present baseline NMT and SMT results, over several versions of the data, in the hopes of encouraging future work on English–Inuktitut machine translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural machine translation system uses the Transformer architecture (Vaswani et al., 2017) implemented in Sockeye and following similar parameter settings to those described in Hieber et al. (2017): a 6-layer encoder, 6-layer decoder, a model di"
2020.lrec-1.312,W15-3049,0,0.0303091,"id not use weight-tying. The network was optimized using Adam (Kingma and Ba, 2015), with an initial learning rate of 10−4 , decreasing by a factor of 0.7 each time the development set BLEU did not improve for 8,000 updates, and stopping early when BLEU did not improve for 32,000 updates. We experimented with BPE vocabularies with 0.5, 1, 2, 5, 10, 15, 20, 25, 30, and 60 thousand merges. The maximum sentence length was set to 200, allowing us to compare the vocabularies without filtering out large numbers of training lines for the smaller vocabulary experiments. On the basis of BLEU and chrF (Popović, 2015) results on dev and devtest, we selected 2,000 merges for all reported English–Inuktitut NMT experiments, and 5,000 merges for Inuktitut–English NMT experiments. Translating into English, 5,000 merges produced the best results (or tied for best) on both metrics. Into Inuktitut, 2,000 merges tied for the best result in terms of BLEU and chrF on dev and had the best BLEU and second-best chrF on devtest, with very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though"
2020.lrec-1.312,W18-6319,0,0.016305,"ries ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman and Federico, 2018; Micher, 2018; Toral et al., 2019). 4.3. Baseline Experiments We present baseline NMT and SMT results, over several versions of the data, in the hopes of encouraging future work on English–Inuktitut machine translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural machine translation system uses the Transformer architecture (Vaswani et al., 2017) implemented in Sockeye and following similar parameter settings to those described in Hieber et al. (2017): a 6-layer encoder, 6-layer decoder, a model dimension of 512 and 2048 hidden units in the feed-forward networks. As our byt"
2020.lrec-1.312,P19-1021,0,0.0161047,"n shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Finnish and Turkish, at both sentence level and document level (Ma et al., 2018; Ma et al., 2019). In general, the YiSi-0 results follow the same trends as BLEU: (1) better aligned training data improves translation quality; (2) larger training data improves translation quality; and (3) more recent training data improves translation quality. 19 For NMT, we use the settings found to be optimal for NH 3.0 throughout our remaining experiments; this may be suboptimal, as noted in Sennrich and Zhang (2019), but we still would not expect the smaller corpora to outperform the larger in this scenario. However, as we do not yet have human judgments on MT into Inuktitut, we urge caution in the interpretation of any automatic metric. 5. Conclusion The main contribution of the work described in this paper is the release of a corpus of approximately 1.3 million aligned Inuktitut–English sentence pairs drawn from the proceedings of the Legislative Assembly of Nunavut. Care was taken to ensure that the sentence alignment was as accurate as possible: the performance of different sentence alignment algorit"
2020.lrec-1.312,1992.tmi-1.7,0,0.653436,"lysis for each word, each word in the full corpus has a single, 1-best analysis.12 We follow the same 4-pass alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of surface or deep forms instead of words. The morphological segmentation experiments rely on the existence of a morphological analyzer, a tool that is not available for all languages. To demonstrate that simple heuristics can also perform well in the absence of morphological analysis tools, we performed experiments using the Prefix of each word as a stand-in for more complicated lemmatization (Simard et al., 1992; Och, 2005; Nicholson et al., 2012). As a polysynthetic language, Inuktitut has many very-low frequency words, and even English words have some morphology which a stemmer could normalize. Since syllabic characters represent whole syllables, we chose a prefix of three characters to approximate the stem of an Inuktitut word and we use a five letter prefix for English words. We follow the same alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of prefixes instead of words. Byte-pair encoding (BPE) is another approach to word segmentation that does not re"
2020.lrec-1.312,D19-1136,0,0.0737348,"0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence alignment (Thompson and Koehn, 2019) is reported to significantly improve alignment quality by incorporating bilingual word embeddings trained on large pre-existing parallel corpora. We experimented with this approach as well (see §3.3.). 3. Alignment of the Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 The Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 (NH 3.0) consists of 17,330,271 English words in 1,452,347 sentences, and 8,068,977 Inuktitut words in 1,450,094 sentences, yielding approximately 1.3 million aligned sentence pairs.5 It covers the proceedings through June 8, 2017. 3.1. Text Extraction from Word Do"
2020.lrec-1.312,W19-5343,0,0.0197083,"on, tokenization (with aggressive hyphen splitting), cleaned the training corpus (sentence length ratio 15, minimum sentence length 1, maximum 200), trained a truecaser on the training data and then applied it to all data. We trained byte-pair encoding models on the training data using subword-nmt,18 with disjoint vocabularies ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman and Federico, 2018; Micher, 2018; Toral et al., 2019). 4.3. Baseline Experiments We present baseline NMT and SMT results, over several versions of the data, in the hopes of encouraging future work on English–Inuktitut machine translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural"
2020.lrec-1.312,D13-1140,0,0.0347578,"rase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE"
C08-1115,E06-1005,1,\N,Missing
C08-1115,W07-0728,1,\N,Missing
C08-1115,W03-1729,1,\N,Missing
C08-1115,P07-1040,0,\N,Missing
C08-1115,W07-0732,1,\N,Missing
C08-1115,W07-0724,1,\N,Missing
C08-1115,W07-0718,0,\N,Missing
C08-1115,P07-1019,0,\N,Missing
C08-1115,W07-0717,1,\N,Missing
C08-1115,P03-1021,0,\N,Missing
C10-1069,N06-1003,0,0.104358,"enk et al. (2007) on continuous space Ngram models, where a neural network is employed to smooth translation probabilities. However, both Schwenk et al.’s smoothing technique 608 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, August 2010 and the system to which it is applied are quite different from ours. Phrase clustering is also somewhat related to work on paraphrases for SMT. Key papers in this area include (Bannard and Callison-Burch, 2005), which pioneered the extraction of paraphrases from bilingual parallel corpora, (Callison-Burch et al., 2006) which showed that paraphrase generation could improve SMT performance, (Callison-Burch, 2008) and (Zhao et al., 2008) which showed how to improve the quality of paraphrases, and (Marton et al., 2009) which derived paraphrases from monolingual data using distributional information. Paraphrases typically help SMT systems trained on under 100K sentence pairs the most. The phrase clustering algorithm in this paper outputs groups of source-language and targetlanguage phrases with similar meanings: paraphrases. However, previous work on paraphrases for SMT has aimed at finding translations for sour"
C10-1069,W08-2119,0,0.0531444,"Missing"
C10-1069,J10-4005,0,0.0154983,"n with small (less than 100K sentence pairs) amounts of training data, contrary to what one would expect from the paraphrasing literature. We have only begun to explore the original smoothing approach described here. 1 Introduction and Related Work The source-language and target-language “phrases” employed by many statistical machine translation (SMT) systems are anomalous: they are arbitrary sequences of contiguous words extracted by complex heuristics from a bilingual corpus, satisfying no formal linguistic criteria. Nevertheless, phrase-based systems perform better than word-based systems (Koehn 2010, pp. 127-129). In this paper, we look at what happens when we cluster together these anomalous but useful entities. Here, we apply phrase clustering to obtain better estimates for “backward” probability P(s|t) and “forward” probability P(t|s), where s is a source-language phrase, t is a target-language phrase, and phrase pair (s,t) was seen at least once in training data. The current work is thus related to work on smoothing P(s|t) and P(t|s) – see (Foster et al., 2006). The relative frequency estimates for P(s|t) and P(t|s) are PRF (s |t ) =# (s, t ) /# t and PRF (t |s ) =# (s , t ) /# s , w"
C10-1069,D08-1076,0,0.0318877,"word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a loglinear feature combination that includes language models, the distortion components, relative frequency estimators PRF(s|t) and PRF(t|s) and lexical weight estimators PLW(s|t) and PLW(t|s). The PLW() components are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. The phrasecluster-based components PPC(s|t) and PPC(t|s) are incorporated as additional loglinear feature functions. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 4.1 Data We evaluated our method on C-E and F-E tasks. For each pair, we carried out experiments on training corpora of different sizes. C-E data were from the NIST1 2009 evaluation; all the allowed bilingual corpora except the UN corpus, Hong Kong Hansard and Hong Kong Law corpus were used to estimate the translation model. For C-E, we trained two 5-gram language models: the first on the English side of the parallel data, and the second on the English Gigaword corpus. Our C-E development set is made up mainly of data from the NIST 2005 test set; it also includes some balanced-genre web-text"
C10-1069,P02-1040,0,0.0842533,"reference is provided for each source input sentence. Two language models are used in this task: one is the English side of the parallel data, and the second is the English side of the GigaFrEn corpus. Table 2 summarizes the training, development and test corpora for F-E tasks. 4.2 Amount of clustering and metric For both C-E and E-F, we assumed that phrases seen only once in training data couldn’t be clustered reliably, so we prevented these “count 1” phrases from participating in clustering. The key 2 http://www.statmt.org/wmt10/ 613 Results and discussion Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Our first experiment evaluated the effects of the phrase clustering features given various amounts of training data. Figure 4 gives the BLEU score improvements for the two language pairs, with results for each pair averaged over two test sets (training data size shown as #sentences). The improvement is largest for medium amounts of training data. Since the F-E training data has more words per sentence than C-E, the two peaks would have been closer together if we’d put #words on the x axis: improvements for both tasks peak aroun"
C10-1069,N04-1033,0,0.051161,"partial French phrase cluster 4 We carried out experiments on a standard onepass phrase-based SMT system with a phrase table derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a loglinear feature combination that includes language models, the distortion components, relative frequency estimators PRF(s|t) and PRF(t|s) and lexical weight estimators PLW(s|t) and PLW(t|s). The PLW() components are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. The phrasecluster-based components PPC(s|t) and PPC(t|s) are incorporated as additional loglinear feature functions. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 4.1 Data We evaluated our method on C-E and F-E tasks. For each pair, we carried out experiments on training corpora of different sizes. C-E data were from the NIST1 2009 evaluation; all the allowed bilingual corpora except the UN corpus, Hong Kong Hansard and Hong Kong Law corpus were used to estimate the transla"
C10-1069,P08-1089,0,0.0638985,"ver, both Schwenk et al.’s smoothing technique 608 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, August 2010 and the system to which it is applied are quite different from ours. Phrase clustering is also somewhat related to work on paraphrases for SMT. Key papers in this area include (Bannard and Callison-Burch, 2005), which pioneered the extraction of paraphrases from bilingual parallel corpora, (Callison-Burch et al., 2006) which showed that paraphrase generation could improve SMT performance, (Callison-Burch, 2008) and (Zhao et al., 2008) which showed how to improve the quality of paraphrases, and (Marton et al., 2009) which derived paraphrases from monolingual data using distributional information. Paraphrases typically help SMT systems trained on under 100K sentence pairs the most. The phrase clustering algorithm in this paper outputs groups of source-language and targetlanguage phrases with similar meanings: paraphrases. However, previous work on paraphrases for SMT has aimed at finding translations for source-language phrases in the system’s input that weren’t seen during system training. Our approach is completely useless"
C10-1069,D09-1040,0,\N,Missing
C10-1069,D08-1021,0,\N,Missing
C10-1069,D07-1045,0,\N,Missing
C10-1069,W06-1607,1,\N,Missing
C10-1069,P05-1074,0,\N,Missing
C10-1069,P05-1033,0,\N,Missing
C10-1069,P07-1019,0,\N,Missing
C18-1222,L18-1530,0,0.054109,"unlikely to be feasible in most Indigenous languages for the foreseeable future, at least not at a high degree of accuracy. However, even a low degree of accuracy can significantly assist human transcription; this technology, sometimes called Transcription Acceleration (TA), would probably be feasible for at least some languages now. 18 mothertongues.org firstvoices.com 20 altlab.artsrn.ualberta.ca/tools-applications/ 21 www.inuktitutcomputing.ca/Uqailaut/info.php 19 2625 Jimerson and Prud’hommeaux (2018) has developed a preliminary ASR system for the Seneca language, and the Persephone ASR (Adams et al., 2018) system is being adapted to provide transcription acceleration within the Dative Online Linguistic Database interface (Dunham, 2014), which currently powers dozens of Indigenous language documentation efforts in Canada. The frontier in speech recognition that is most promising for low-resource languages is multilingual recognition, in which a model trained on a large variety of languages can help compensate for a lack of transcribed speech data in the target language. A challenge for multilingual speech recognition is that some Indigenous languages, particularly in the Pacific Northwest, are g"
C18-1222,W17-0108,1,0.738302,"es is, of course, highly important and is often the foundational work that makes these technologies possible; it is just that such an inventory would be outside the scope of a document of this size.3 In terms of organization, technologies will be clustered from the point-of-view of the practical application of a technology (e.g., spell-checking or text prediction), rather than be organized by the computational model that makes the application possible (e.g., a finite-state transducer or statistical language model). A finite-state grammar of a language (e.g. Snoek et al. (2014), Dunham (2014), Arppe et al. (2017a), Bowers et al. (2017), Harrigan et al. (2017)) can power a number of practical applications from spell-checking, to morphologically-aware search and browsing of dictionaries and corpora, to computeraided language learning (Arppe et al., 2016). This document will categorize technologies into five groups, according to the feasibility of developing these for a wide range of Indigenous languages, ranging from “already available” to “infeasible for most languages” (§8). It should be emphasized that these are not ratings of desirability, impact, worthiness for funding, or the relative importance"
C18-1222,W17-0101,1,0.907139,"ly important and is often the foundational work that makes these technologies possible; it is just that such an inventory would be outside the scope of a document of this size.3 In terms of organization, technologies will be clustered from the point-of-view of the practical application of a technology (e.g., spell-checking or text prediction), rather than be organized by the computational model that makes the application possible (e.g., a finite-state transducer or statistical language model). A finite-state grammar of a language (e.g. Snoek et al. (2014), Dunham (2014), Arppe et al. (2017a), Bowers et al. (2017), Harrigan et al. (2017)) can power a number of practical applications from spell-checking, to morphologically-aware search and browsing of dictionaries and corpora, to computeraided language learning (Arppe et al., 2016). This document will categorize technologies into five groups, according to the feasibility of developing these for a wide range of Indigenous languages, ranging from “already available” to “infeasible for most languages” (§8). It should be emphasized that these are not ratings of desirability, impact, worthiness for funding, or the relative importance of these technologies to"
C18-1222,2008.tc-1.1,0,0.399896,"Missing"
C18-1222,L16-1514,1,0.894824,"Missing"
C18-1222,L18-1657,0,0.0863374,"Missing"
C18-1222,W13-5610,0,0.0134991,"(Littell et al., 2017). It concentrates on Pacific Northwest languages where, due to the extensive consonant inventories and phonological complexity of these languages, approximate search is particularly important. This algorithm powers the search function in e-dictionaries for 17 Indigenous languages spoken in British Columbia, including FirstVoices’19 mobile dictionary applications for iOS and Android, with dictionaries for 11 more languages currently in development. Morphologically-aware search allows the user to find instances of their search query that may differ in one or more morphemes(Johnson et al., 2013). The Giella infrastructure offers morphology-aware search in dictionaries that are generated by linking a morphological model with lexical resources (and possibly with text corpora). A user can search with any inflected word form of a lemma (or root), possibly taking into account common spelling errors and spelling relaxations (Moshagen et al., 2013). Snoek et al. (2014) and Harrigan et al. (2017) use this technology to allow searching a dictionary of Plains Cree for specific lemmas. Similar capabilities exist for East Cree (Arppe et al., 2017b), Tsuut’ina (Arppe et al., 2017a), Northern Haid"
C18-1222,L18-1368,1,0.844185,"he Giella infrastructure offers morphology-aware search in dictionaries that are generated by linking a morphological model with lexical resources (and possibly with text corpora). A user can search with any inflected word form of a lemma (or root), possibly taking into account common spelling errors and spelling relaxations (Moshagen et al., 2013). Snoek et al. (2014) and Harrigan et al. (2017) use this technology to allow searching a dictionary of Plains Cree for specific lemmas. Similar capabilities exist for East Cree (Arppe et al., 2017b), Tsuut’ina (Arppe et al., 2017a), Northern Haida (Lachler et al., 2018) and Odawa (Bowers et al., 2017).20 4.7 Machine translation Machine translation is one of the best-known language technologies, and receives significant attention from academia, industry, and the general public, so one of the more common queries from Indigenous groups is whether machine translation would be feasible for their languages. The current state-of-the-art of machine translation is relatively language neutral, but requires very large amounts of parallel text, which is currently unavailable in most Indigenous languages save Inuktitut. Even then, given the complexity of Inuktitut morpho"
C18-1222,W05-0810,0,0.0456929,"ext, which is currently unavailable in most Indigenous languages save Inuktitut. Even then, given the complexity of Inuktitut morphology and the limited corpus available, it is probable that such systems will be, at best, aides to human translators working within that domain, rather than a general-purpose consumer technology like Google Translate. Several prerequisite steps for Inuktitut machine translation have been achieved, including morphological segmentation (the Uqailaut analyzer21 and its neural generalization (Micher, 2017)), and sentence and word-level alignment (Martin et al., 2003; Langlais et al., 2005). There are several Inuktitut-English machine translation systems currently under development. The prerequisite steps can themselves power practical technology. For example, the WeBInuk translation memory system, an adaptation of the WeBiText system (D´esilets et al., 2008) mines InuktitutEnglish text and uses word alignments to suggest translations to Inuktitut translators. 5 Speech technologies There has been little development of Indigenous language speech technology so far, but consultation with language communities has suggested that speech technologies are greatly desired, as these langu"
C18-1222,W17-0119,1,0.818985,"ys the user experience can be further improved for a particular language: by adapting to actual user queries, by building phonetic knowledge into the system, by making the search aware of morpheme breakdowns. The East Cree16 and Innu17 dictionaries utilize relaxed search rules based on users’ habits (Junker and Stewart, 2008). 16 17 dictionary.eastcree.org dictionnaire.innu-aimun.ca 2624 Mother Tongues Dictionaries18 incorporates phonological background knowledge (e.g., that two sounds are similar and likely to be confused by users) in a finite-state approximate phonological search algorithm (Littell et al., 2017). It concentrates on Pacific Northwest languages where, due to the extensive consonant inventories and phonological complexity of these languages, approximate search is particularly important. This algorithm powers the search function in e-dictionaries for 17 Indigenous languages spoken in British Columbia, including FirstVoices’19 mobile dictionary applications for iOS and Android, with dictionaries for 11 more languages currently in development. Morphologically-aware search allows the user to find instances of their search query that may differ in one or more morphemes(Johnson et al., 2013)."
C18-1222,L18-1653,0,0.0138252,"fer complete coverage of Indigenous languages as well as support for other non-Indigenous languages. 4.2 Predictive text One common request concerning keyboards (particularly mobile keyboards) is “predictive text” or “autocomplete”, in which the keyboard offers shortcut buttons that suggest probable next words to the user depending on what they have already typed. This technology is especially desirable because it appeals to young users as well as to advanced second language learners. ´ The Multiling O8 keyboard app for Android offers dictionary-based predictive text in the SENCOTEN language. Maheshwari et al. (2018) examine word and character-based language models for text prediction of Mi’kmaq, based on a small web corpus. Given the relative paucity of digital text corpora for many languages, it is likely that most predictive text systems will not be able to rely entirely on statistical models, and will instead be built on rule-based (e.g. finite state) or hybrid statistical/rule-based systems. 4.3 Orthography conversion Almost all Indigenous languages have been written in several different orthographies. While there is a general trend towards orthographic unification in most communities, it is still co"
C18-1222,W03-0320,0,0.0149736,"amounts of parallel text, which is currently unavailable in most Indigenous languages save Inuktitut. Even then, given the complexity of Inuktitut morphology and the limited corpus available, it is probable that such systems will be, at best, aides to human translators working within that domain, rather than a general-purpose consumer technology like Google Translate. Several prerequisite steps for Inuktitut machine translation have been achieved, including morphological segmentation (the Uqailaut analyzer21 and its neural generalization (Micher, 2017)), and sentence and word-level alignment (Martin et al., 2003; Langlais et al., 2005). There are several Inuktitut-English machine translation systems currently under development. The prerequisite steps can themselves power practical technology. For example, the WeBInuk translation memory system, an adaptation of the WeBiText system (D´esilets et al., 2008) mines InuktitutEnglish text and uses word alignments to suggest translations to Inuktitut translators. 5 Speech technologies There has been little development of Indigenous language speech technology so far, but consultation with language communities has suggested that speech technologies are greatly"
C18-1222,W17-0114,0,0.353293,"agglutinative. It is commonly the case that a single word carries the meaning of what would be an entire clause in English and French. (1) iah th-a-etsi-te-w-ate-wistohsera-’tarih-´a:t-ha-k-e’ no NOT- WOULD - AGAIN - WE - ALL - OWN-butter-HOT- CAUSE - HABIT- CONTIN - PERF. ‘We will no longer keep heating up our butter.’ Mohawk (Mithun, 1996, p. 170) (2) Qanniqlaunngikkalauqtuqlu, aninngittunga qanniq-lak-uq-nngit-galauq-tuq-lu, ani-nngit-junga snow-a.little-frequently-NOT-although-3.IND . S-and go.out-NOT-1.IND . S ‘And even though it’s not snowing a great deal, I’m not going out.’ Inuktitut (Micher, 2017, p. 102) This complexity presents a challenge for many applications and algorithms, especially those that encode assumptions about the atomic word being the basic unit of meaning/structure, or even the assumption that concatenative morphological analysis is sufficient for finding sub-word units (Arppe et al., 2017a). 3.2 Limited training data For most languages, there is little to no digitized text or audio available for use as training data, at least not at the scale required for modern statistical or neural NLP. Existing technologies for Indigenous languages have therefore, with a few excep"
C18-1222,W13-5631,0,0.186939,"heckers would have a similar problem; even when a digital corpus is available, only a small fraction of possible derivations/inflections will occur it. Therefore, efforts to develop spell-checkers in Indigenous languages typically concentrate on finite-state technology, since this allows the specification of very large lexicons in an efficient and succinct manner. A Plains Cree spell-checker based on FST technology is available for system-wide use in recent versions of MacOS, and versions for Microsoft Office and Libre Office are in development (Arppe et al., 2016). The Giella infrastructure (Moshagen et al., 2013) offers an easy way to create FST-based spellcheckers that can be integrated into LibreOffice and, to a limited extent, into Microsoft Office. The spell-checkers use finite-state transducers as a backend, but it is possible to specify spelling relaxations as well as to include modules for likely or common errors. Theoretically the framework allows other types of language models as well, but they have been relatively untested. An unexpected problem with integrating spell-checkers into mainstream office software is tokenization, since some Indigenous languages use commas, colons, and apostrophes"
C18-1222,W14-2205,1,0.79114,"Missing"
D07-1103,N03-1017,0,0.134973,"tional c Natural Language Learning, pp. 967–975, Prague, June 2007. 2007 Association for Computational Linguistics 2 2.1 Background Theory Our Approach to Statistical Machine Translation We define a phrasetable as a set of source phrases (ngrams) s˜ and their translations (m-grams) t˜, along with associated translation probabilities p(˜ s|t˜) and p(t˜|˜ s). These conditional distributions are derived from the joint frequencies c(˜ s, t˜) of source / target n, m-grams observed in a word-aligned parallel corpus. These joint counts are estimated using the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). Phrases are limited to 8 tokens in length (n, m ≤ 8). Given a source sentence s, our phrase-based SMT system tries to find the target sentence ˆt that is the most likely translation of s. To make search more efficient, we use the Viterbi approximation and seek the most likely combination of t and its alignment a with s, rather than just the most likely t: ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t t,a where a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK ); t˜k are target phrases such that t = t˜1 ...t˜K ; s˜k are"
D07-1103,2001.mtsummit-papers.68,0,0.083145,"Missing"
D07-1103,N04-1033,0,0.0158942,"Missing"
D07-1103,W04-3243,0,\N,Missing
D07-1103,J93-2003,0,\N,Missing
D07-1103,P02-1040,0,\N,Missing
D07-1103,W06-1607,1,\N,Missing
D07-1103,D08-1076,0,\N,Missing
D10-1044,N04-4006,0,0.0192611,"s led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007). However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus. This suggests a direct parallel to (1): ! ! αi pi (s|t), (2) α ˆ = argmax p˜(s, t) log α s,t i where p˜(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004). For the TM, this is: p(s|t) = cI (s, t) + β po (s|t) , cI (t) + β (3) where cI (s, t) is the count in the IN phrase table of pair (s, t), po (s|t)""is its probability under the OUT TM, and cI (t) = s! cI (s! , t). This is motivated by taking β po (s|t) to be the parameters of a Dirichlet prior on phrase probabilities, then maximizing posterior estimates p(s|t) given the IN corpus. Intuitively, it places more weight on OUT when less evidence from IN is available. To set β, we used the same criterion as for α, over a dev corpus: βˆ = argmax β ! s,t p˜(s, t) log cI (s, t) + β po (s|t) . cI (t) +"
D10-1044,W09-0432,0,0.316476,"ombining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus"
D10-1044,W07-0722,0,0.0718684,"ond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The fea"
D10-1044,P07-1033,0,0.119667,"Missing"
D10-1044,W08-0334,0,0.206659,"tive weights are set as follows: ! ! p˜(w, h) log αi pi (w|h), (1) α ˆ = argmax α i w,h where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and p˜(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this. It is not immediately obvious how to formulate an equivalent to equation (1) for an adapted TM, because there is no well-defined objective for learning TMs from parallel corpora. This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007). However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus. This suggests a direct parallel to (1): ! ! αi pi (s|t), (2) α ˆ = argmax p˜(s, t) log α s,t i where p˜(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004). For the TM, this is: p(s|t) = cI (s, t) + β po (s|t) , cI (t)"
D10-1044,N09-1068,0,0.0403739,"t splits each feature into domain-specific and general copies. At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances. However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t). As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Se"
D10-1044,W07-0717,1,0.846999,"reported in. Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership. Finally, we make some improvements to baseline approaches. We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set. This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU. A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only. For comparison to information-retrieval inspired baselines, eg (L¨u et al., 2007), we select sentences from OUT using language model perplexities from IN. This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results. The paper is structured as follows. Section 2 describes our baseline techniques for SMT adaptation, and section 3 describes the instance-weighting approach. Experiments are presented in section 4. Section 5 covers"
D10-1044,W09-0439,1,0.776725,"baseline approach is to concatenate data from IN and OUT. Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN. When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination. An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003). This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009). 2.2 Linear Combinations Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates. This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain. This leads to a linear combination of domain-specific probabilities, with weights in [0, 1], normalized to sum to 1. Linear weights are d"
D10-1044,2005.eamt-1.19,0,0.676063,"rom IN is available. To set β, we used the same criterion as for α, over a dev corpus: βˆ = argmax β ! s,t p˜(s, t) log cI (s, t) + β po (s|t) . cI (t) + β 2 Using non-adapted IBM models trained on all available IN and OUT data. 453 The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 2.3 Sentence Selection Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004). The matching sentence pairs are then added to the IN corpus, and the system is re-trained. Although matching is done at the sentence level, this information is subsequently discarded when all matches are pooled. To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model. The number of top-ranked pairs to retain is chosen to optimize dev-set BLEU score. 3 Instance We"
D10-1044,P07-1034,0,0.585228,"ss effective in our setting, where IN and OUT are disparate. The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity. Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT. Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Our second contribution is to apply instance 451 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451–459, c MIT, Massachusetts, USA, 9-11 October 2010. !2010 Crown in Right of Canada. weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of do"
D10-1044,W07-0733,0,0.744354,"es, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t). As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al.,"
D10-1044,N03-1017,0,0.0138788,"27 1,894 1,664 1,357 Table 1: Corpora 8 www.itl.nist.gov/iad/mig//tests/mt/2009 456 The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa. Le m´edicament de r´ef´erence de Silapo est EPREX/ERYPO, qui contient de l’´epo´etine alfa. — I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court. Je voudrais pr´eciser, a` l’adresse du commissaire Liikanen, qu’il n’est pas ais´e de recourir aux tribunaux nationaux. Figure 1: Sentence pairs from EMEA (top) and Europarl text. We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count. Feature weights were set using Och’s MERT algorithm (Och, 2003). The corpus was wordaligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model. 4.2 Results Table 2 shows results for both settings and all"
D10-1044,D07-1036,0,0.765764,"Missing"
D10-1044,D09-1074,0,0.764711,"s in Natural Language Processing, pages 451–459, c MIT, Massachusetts, USA, 9-11 October 2010. !2010 Crown in Right of Canada. weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language. For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in. Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership. Finally, we make some improvements to baseline approaches. We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set. This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU. A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only. For comparison to information-retrieval inspired bas"
D10-1044,P03-1021,0,0.0597961,"dure for generating the phrase table from which the TM distributions are derived. 2.1 Simple Baselines The natural baseline approach is to concatenate data from IN and OUT. Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN. When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination. An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003). This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009). 2.2 Linear Combinations Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates. This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain. This leads"
D10-1044,2009.mtsummit-posters.17,0,0.0173592,"Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out"
D10-1044,P07-1066,0,0.266726,"in (IN) for which a smaller amount of parallel There is a fairly large body of work on SMT adaptation. We introduce several new ideas. First, we aim to explicitly characterize examples from OUT as belonging to general language or not. Previous approaches have tried to find examples that are similar to the target domain. This is less effective in our setting, where IN and OUT are disparate. The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity. Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT. Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Our second c"
D10-1044,P07-1004,0,0.160253,"nt work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve i"
D10-1044,P05-1058,0,0.0263712,"rk, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted"
D10-1044,2007.mtsummit-papers.68,0,0.0959361,"rand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted within a logistic model to give an overall weight that i"
D10-1044,C04-1059,0,0.614506,"Missing"
D10-1044,D08-1076,0,\N,Missing
D12-1058,P05-1074,0,0.428349,"g various types of corpora, monolingual corpora can be considered the best source for highcoverage paraphrase acquisition, because there is far more monolingual than bilingual text available. Most methods that exploit monolingual corpora rely on the Distributional Hypothesis (Harris, 1968): expressions that appear in similar contexts are expected to have similar meaning. However, if one uses purely distributional criteria, it is difficult to distinguish real paraphrases from pairs of expressions that are related in other ways, such as antonyms and cousin words. In contrast, since the work in (Bannard and Callison-Burch, 2005), bilingual parallel corpora have been acknowledged as a good source of highquality paraphrases: paraphrases are obtained by putting together expressions that receive the same translation in the other language (pivot language). Because translation expresses a specific meaning more directly than context in the aforementioned approach, pairs of expressions acquired in this manner tend to be correct paraphrases. However, the coverage problem remains: there is much less bilingual parallel than monolingual text available. Our objective in this paper is to obtain paraphrases that have high quality ("
D12-1058,P01-1008,0,0.253566,"nting context of target expression (contextual features), criteria for weighting and filtering features, and aggregation functions. A drawback of relying only on contextual similarity is that it tends to give high scores to semantically related but non-equivalent expressions, such as antonyms and cousin words. To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al., 2011). 2.2 Alignment-based Methods Pairs of expressions that get translated to the same expression in a different language can be regarded as paraphrases. On the basis of this hypothesis, Barzilay and McKeown (2001) and Pang et al. (2003) 632 where Tr (e1 , e2 ) stands for the set of shared translations of e1 and e2 . Each factor p(e|f ) and p(f |e) is estimated from the number of times e and f are aligned and the number of occurrences of each expression in each language. Kok and Brockett (2010) showed how one can discover paraphrases that do not share any translation in one language by traversing a graph created from multiple translation tables, each corresponding to a bilingual parallel corpus. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual para"
D12-1058,N03-1003,0,0.22822,"bilingual parallel corpus. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual comparable corpora have also been exploited as sources of paraphrases using alignmentbased methods. For instance, multiple news articles covering the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Wubben et al., 2009) have been used. Such corpora have also been created manually through crowdsourcing (Chen and Dolan, 2011). However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very small collections of paraphrases. Hashimoto et al. (2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual nonparallel Web documents, pairing sentences that define the same noun phras"
D12-1058,P08-1077,0,0.0522461,"|e1 ) = Literature on Paraphrase Acquisition ∑ p(e2 |f )p(f |e1 ), (1) f ∈Tr (e1 ,e2 ) This section summarizes existing corpus-based methods for paraphrase acquisition, following the classification in (Hashimoto et al., 2011): similaritybased and alignment-based methods. 2.1 Similarity-based Methods Techniques that use monolingual (non-parallel) corpora mostly rely on the Distributional Hypothesis (Harris, 1968). Because a large quantity of monolingual data is available for many languages, a large number of paraphrase candidates can be acquired (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008, etc.). The recipes proposed so far are based on three main ingredients, i.e., features used for representing context of target expression (contextual features), criteria for weighting and filtering features, and aggregation functions. A drawback of relying only on contextual similarity is that it tends to give high scores to semantically related but non-equivalent expressions, such as antonyms and cousin words. To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al., 2011). 2.2 Alignment-based Methods Pairs of expressions that get translated to the"
D12-1058,N06-1003,0,0.0708393,"Missing"
D12-1058,D08-1021,0,0.671303,"” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, expressions that match both elements of the pattern, except stop words, are collected from a given monolingual corpus. Pattern matching alone may generate inappropriate pairs, so we then assess the legitimacy of each collected slot-filler. Let LHS (w) and RHS (w) be the exp"
D12-1058,W11-2504,0,0.145596,"or acquiring paraphrases. The process is illustrated in Figure 1. First, a set of high-quality seed paraphrases, PSeed , is acquired from bilingual parallel corpora by using an alignment-based method. Then, our method collects further paraphrases through the following two steps. Generalization (Step 2): Paraphrase patterns are learned from the seed paraphrases, PSeed . Instantiation (Step 3): A novel set of paraphrase pairs, PHvst , is finally harvested from monolingual non-parallel corpora using the learned patterns; each newly acquired paraphrase pair is assessed by contextual similarity. 1 Chan et al. (2011) used monolingual corpora only for reranking paraphrases obtained from bilingual parallel corpora. To the best of our knowledge, bilingual comparable corpora have never been used as sources for acquiring paraphrases. 633 “health issue” ⇒ “problème de santé” “health problem” ⇒ “problème de santé” “look like” ⇒ “ressemble” “regional issue” ⇒ “problème régional” “regional problem” ⇒ “problème régional” “resemble” ⇒ “ressemble” Step 1. Seed Paraphrase Acquisition PSeed: Seed Paraphrases Monolingual Non-parallel Corpus “health issue” ⇒ “health problem” “look like” ⇒ “resemble” “regional issue” ⇒"
D12-1058,P11-1020,0,0.0599932,"tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual comparable corpora have also been exploited as sources of paraphrases using alignmentbased methods. For instance, multiple news articles covering the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Wubben et al., 2009) have been used. Such corpora have also been created manually through crowdsourcing (Chen and Dolan, 2011). However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very small collections of paraphrases. Hashimoto et al. (2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual nonparallel Web documents, pairing sentences that define the same noun phrase, and then finding corresponding phrases in each sentence pair. One limitation of this approach is that it requires a considerable amount of labele"
D12-1058,C08-1018,0,0.0702596,"Missing"
D12-1058,W11-2107,0,0.0233728,"a similar manner, i.e., a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp ′ (= lp) which satisfies the following conditions (see also Figure 3). • lp′ is a word sub-sequence of lp • lp ′ is a more likely source than lp, i.e., p(lp ′ |rp) > p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained. Candidate pairs are finally filtered on the basis of their reliability score. Traditionally, a threshold (th p ) on the conditional probability given by Eq. (1) is used (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2011, etc.). Furthermore, we also require that LHS and RHS phrases exceed a threshold (th s ) on their contextual similarity in a monolingual corpus. This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1). 2 cf. Denkowski and Lavie (2011); they only compared each RHS phrase to its corresponding LHS phrase. 634 3.2 Step 2. Paraphrase Pattern Induction From a set of seed paraphrases, PSeed , paraphrase patterns are induced. For instance, fro"
D12-1058,C04-1051,0,0.178329,"us. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual comparable corpora have also been exploited as sources of paraphrases using alignmentbased methods. For instance, multiple news articles covering the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Wubben et al., 2009) have been used. Such corpora have also been created manually through crowdsourcing (Chen and Dolan, 2011). However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very small collections of paraphrases. Hashimoto et al. (2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual nonparallel Web documents, pairing sentences that define the same noun phrase, and then finding"
D12-1058,D10-1041,0,0.0903118,"rase rp are also compared in a similar manner, i.e., a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp ′ (= lp) which satisfies the following conditions (see also Figure 3). • lp′ is a word sub-sequence of lp • lp ′ is a more likely source than lp, i.e., p(lp ′ |rp) > p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained. Candidate pairs are finally filtered on the basis of their reliability score. Traditionally, a threshold (th p ) on the conditional probability given by Eq. (1) is used (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2011, etc.). Furthermore, we also require that LHS and RHS phrases exceed a threshold (th s ) on their contextual similarity in a monolingual corpus. This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1). 2 cf. Denkowski and Lavie (2011); they only compared each RHS phrase to its corresponding LHS phrase. 634 3.2 Step 2. Paraphrase Pattern Induction From a set of seed paraphrases, PSeed , paraphrase p"
D12-1058,W07-1425,1,0.798663,"” ⇒ “Y in the X east” Word pairs of LHS and RHS phrases will be replaced with variable slots iff they are fully identical or singular-plural variants. Note that stop words are retained. While a deeper level of lexical correspondences, such as “eastern” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, expressions that match"
D12-1058,D11-1108,0,0.113302,"Missing"
D12-1058,P11-1109,0,0.285732,"Missing"
D12-1058,P99-1044,0,0.0736733,"c. “X eastern Y ” ⇒ “Y in the X east” Word pairs of LHS and RHS phrases will be replaced with variable slots iff they are fully identical or singular-plural variants. Note that stop words are retained. While a deeper level of lexical correspondences, such as “eastern” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, e"
D12-1058,D07-1103,1,0.929456,"lts with several filtering methods. The phrase pair extraction process of phrasebased SMT systems aims at high recall for increased robustness of the translation process. As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. For instance, the algorithm explained in Koehn (2009, p.134) extracts both “dass” and “, dass” as counterparts of “that” from the sentence pair. To reduce that kind of noise, we apply some filtering techniques to the candidate translation pairs. First, statistically unreliable translation pairs (Johnson et al., 2007) are filtered out. Then, we also filter out phrases made up entirely of stop words (including punctuation marks), both in the language of interest and in the pivot language. Let PRaw be the initial set of paraphrase pairs extracted from the sanitized translation table. We first p(rp|lp) .172 lp: controller rp: control system lp: control apparatus rp: the control device lp: the control apparatus .005 .004 .003 rp: control device of the lp: control apparatus of rp: controlling device lp: controlling unit .001 rp: control system of lp: control equipment .001 .001 rp: a control system for an lp: c"
D12-1058,N03-1017,0,0.00867146,"n be pooled with the set PHvst harvested in the last stage of the process. 3.1 Step 1. Seed Paraphrase Acquisition The goal of the first step is to obtain a set of highquality paraphrase pairs, PSeed . For this purpose, alignment-based methods with bilingual or monolingual parallel corpora are preferable to similarity-based methods applied to nonparallel corpora. Among various options, in this paper, we start from the standard technique proposed by Bannard and Callison-Burch (2005) with bilingual parallel corpora (see also Section 2.2). In particular, we assume the phrase-based SMT framework (Koehn et al., 2003). Then, we purify the results with several filtering methods. The phrase pair extraction process of phrasebased SMT systems aims at high recall for increased robustness of the translation process. As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. For instance, the algorithm explained in Koehn (2009, p.134) extracts both “dass” and “, dass” as counterparts of “that” from the sentence pair. To reduce that kind of noise, we apply some filtering techniques to the candidate translation pairs. First, statistically unre"
D12-1058,P09-5002,0,0.0145231,"in this paper, we start from the standard technique proposed by Bannard and Callison-Burch (2005) with bilingual parallel corpora (see also Section 2.2). In particular, we assume the phrase-based SMT framework (Koehn et al., 2003). Then, we purify the results with several filtering methods. The phrase pair extraction process of phrasebased SMT systems aims at high recall for increased robustness of the translation process. As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. For instance, the algorithm explained in Koehn (2009, p.134) extracts both “dass” and “, dass” as counterparts of “that” from the sentence pair. To reduce that kind of noise, we apply some filtering techniques to the candidate translation pairs. First, statistically unreliable translation pairs (Johnson et al., 2007) are filtered out. Then, we also filter out phrases made up entirely of stop words (including punctuation marks), both in the language of interest and in the pivot language. Let PRaw be the initial set of paraphrase pairs extracted from the sanitized translation table. We first p(rp|lp) .172 lp: controller rp: control system lp: con"
D12-1058,N10-1017,0,0.0311391,"nyms and cousin words. To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al., 2011). 2.2 Alignment-based Methods Pairs of expressions that get translated to the same expression in a different language can be regarded as paraphrases. On the basis of this hypothesis, Barzilay and McKeown (2001) and Pang et al. (2003) 632 where Tr (e1 , e2 ) stands for the set of shared translations of e1 and e2 . Each factor p(e|f ) and p(f |e) is estimated from the number of times e and f are aligned and the number of occurrences of each expression in each language. Kok and Brockett (2010) showed how one can discover paraphrases that do not share any translation in one language by traversing a graph created from multiple translation tables, each corresponding to a bilingual parallel corpus. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual compara"
D12-1058,J10-3003,0,0.0259774,"r, the coverage problem remains: there is much less bilingual parallel than monolingual text available. Our objective in this paper is to obtain paraphrases that have high quality (like those extracted from bilingual parallel corpora via pivoting) but can be generated in large quantity (like those extracted (1) Introduction Paraphrases are semantically equivalent expressions in the same language. Because “equivalence” is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010). In the last decade, automatic acquisition of knowledge about paraphrases from corpora has been drawing the attention of many researchers. Typically, the acquired knowledge is simply represented as pairs of semantically equivalent sub-sentential expressions as in (1). 631 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 631–642, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics from monolingual corpora via contextual similarity). To achieve this, we propose a meth"
D12-1058,D09-1040,0,0.0801327,"Missing"
D12-1058,W11-2128,0,0.0339243,"Missing"
D12-1058,D10-1064,0,0.047879,"compared in a similar manner, i.e., a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp ′ (= lp) which satisfies the following conditions (see also Figure 3). • lp′ is a word sub-sequence of lp • lp ′ is a more likely source than lp, i.e., p(lp ′ |rp) > p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained. Candidate pairs are finally filtered on the basis of their reliability score. Traditionally, a threshold (th p ) on the conditional probability given by Eq. (1) is used (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2011, etc.). Furthermore, we also require that LHS and RHS phrases exceed a threshold (th s ) on their contextual similarity in a monolingual corpus. This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1). 2 cf. Denkowski and Lavie (2011); they only compared each RHS phrase to its corresponding LHS phrase. 634 3.2 Step 2. Paraphrase Pattern Induction From a set of seed paraphrases, PSeed , paraphrase patterns are"
D12-1058,I05-1011,0,0.489306,"Missing"
D12-1058,N03-1024,0,0.508795,"Missing"
D12-1058,W05-0822,1,0.80561,"05 104 103 106 107 108 # of words in the English side of bilingual corpus 107 106 105 PRaw PRaw (thp=0.01) PSeed (thp=ε, ths=ε) PSeed (thp=0.01, ths=ε) 104 103 106 107 108 # of words in the English side of bilingual corpus Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent). Stop word lists for sanitizing translation pairs and paraphrase pairs were manually compiled: we enumerated 442 English words, 193 French words, and 149 Japanese morphemes, respectively. From a bilingual parallel corpus, a translation table was created by our in-house phrase-based SMT system, PORTAGE (Sadat et al., 2005). Phrase alignments of each sentence pair were identified by the heuristic “grow-diag-final”6 with a maximum phrase length 8. The resulting translation pairs were then filtered with the significance pruning technique of (Johnson et al., 2007), using α + ϵ as threshold. As contextual features for computing similarity of each paraphrase pair, all of the 1- to 4-grams of words adjacent to each occurrence of a phrase were counted. This is a compromise between less expensive but noisier approaches, such as bag-of-words, and more accurate but more expensive approaches that incorporate syntactic feat"
D12-1058,C08-1107,0,0.0412699,"e that stop words are retained. While a deeper level of lexical correspondences, such as “eastern” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, expressions that match both elements of the pattern, except stop words, are collected from a given monolingual corpus. Pattern matching alone may generate inappropriate pairs, so we"
D12-1058,W09-0621,0,0.108629,"Missing"
H91-1043,J91-3004,0,\N,Missing
H91-1043,H89-1043,0,\N,Missing
N06-1004,P05-1066,0,0.485406,"nal Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they assign a probability to each possible phrase reordering. These “segment choice” models (SCMs) can be trained on “segmen"
N06-1004,koen-2004-pharaoh,0,0.574965,"al., 2003; Och and Ney, 2004). Distortion in phrase-based MT occurs when the order of phrases in the source-language sentence changes during translation, so the order of corresponding phrases in the target-language translation is different. Some MT systems allow arbiOur model assumes that the source sentence is completely segmented prior to distortion. This simplifying assumption requires generation of hypotheses about the segmentation of the complete source sentence during decoding. The model also assumes that each translation hypothesis grows in a predetermined order. E.g., Koehn’s decoder (Koehn 2004) builds each new hypothesis by adding phrases to it left-to-right (order is deterministic for the target hypothesis). Our model doesn’t require this order of operation – it would support right-to-left or inwards-outwards hypothesis construction – but it does require a predictable order. One can keep track of how segments in the source sentence have been rearranged during decoding for a given hypothesis, using what we call a “distorted source-language hypothesis” (DSH). A similar concept appears in (Collins et al., 2005) (this paper’s preoccupations strongly resemble 25 Proceedings of the Human"
N06-1004,N03-1017,0,0.00873664,"bles consists of all parallel text available for the NIST MT05 Chinese-English evaluation, except the Xinhua corpora and part 3 of LDC's “MultipleTranslation Chinese Corpus” (MTCCp3). The English language model was trained on the same corpora, plus 250M words from Gigaword. The DTbased SCM was trained and tuned on a subset of this same training corpus (above). The dev corpus for optimizing component weights is MTCCp3. The experimental results below were obtained by testing on the evaluation set for MTeval NIST04. Phrase tables were learned from the training corpus using the “diag-and” method (Koehn et al., 2003), and using IBM model 2 to produce initial word alignments (these authors found this worked as well as IBM4). Phrase probabilities were based on unsmoothed relative frequencies. The model used by the decoder was a log-linear combination of a phrase translation model (only in the P(source|target) direction), trigram language model, word penalty (lexical weighting), an optional segmentation model (in the form of a phrase penalty) and distortion model. Weights on the components were assigned using the (Och, 2003) method for max-BLEU training on the development set. The decoder uses a dynamicprogr"
N06-1004,P03-1021,0,0.0267581,"Missing"
N06-1004,P02-1040,0,0.0721342,"se it is the leftmost RS: the “leftmost” predictor. Or, the last phrase in the DSH will be followed by the phrase that originally followed it, [8 9]: the “following” predictor. Or, perhaps positions in the source and target should be close, so since the next DSH position to be filled is 4, phrase [4] should be favoured: the “parallel” predictor. original: [0 1] [2 3] [4] [5] [6] [7] [8 9] DSH: [0 1] [5] [7], RS: [2 3], [4], [6], [8 9] Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it, as measured by a metric like BLEU (Papineni et al., 2002). However, training and 26 Figure 2. Segment choice prediction example Model B will be based on the “leftmost” predictor, giving the leftmost segment in the RS twice the probability of the other segments, and giving the others uniform probabilities. Model C will be based on the “following” predictor, doubling the probability for the segment in the RS whose first word was the closest to the last word in the DSH, and otherwise assigning uniform probabilities. Finally, Model D combines “leftmost” and “following”: where the leftmost and following segments are different, both are assigned double th"
N06-1004,P05-1069,0,0.0636859,"d Kuhn, Denis Yuen, Michel Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson Institute for Information Technology, National Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that"
N06-1004,N04-4026,0,0.426085,"Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson Institute for Information Technology, National Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they"
N06-1004,J93-2003,0,\N,Missing
N06-1004,C96-2141,0,\N,Missing
N06-1004,H05-1021,0,\N,Missing
N06-1004,J04-4002,0,\N,Missing
N06-1004,W06-3118,1,\N,Missing
N13-1114,D11-1033,0,0.194028,"nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set w"
N13-1114,W09-0432,0,0.0574768,"approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the senten"
N13-1114,P08-2040,1,0.901088,"7), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on"
N13-1114,2011.mtsummit-papers.30,1,0.81693,"Missing"
N13-1114,N12-1047,1,0.88809,"Missing"
N13-1114,W12-3125,0,0.0337185,"Missing"
N13-1114,W07-0717,1,0.955665,"adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield significant performance improvements. 1 In offline domain adaptation, the system is provided with a sample of translated sentences from the test domain prior to deployment. In a popular variant of offline adaptation, linear mixture model adaptation, each training corpus is used to generate a separate model component that forms part of a linear combination, and the sample is used to assign a weight to each component (Foster and Kuhn, 2007). If the sample resembles some of the corpora more than others, those corpora will receive higher weights in the combination. Introduction A phrase-based statistical machine translation (SMT) system typically has three main components: a translation model (TM) that contains information about how to translate word sequences (phrases) from the source language to the target language, a language model (LM) that contains information Previous research on domain adaptation for SMT has focused on the TM and the LM. Such research is easily motivated: translations across domains are unreliable. For exam"
N13-1114,D10-1044,1,0.922726,"Missing"
N13-1114,D08-1089,0,0.298034,"ically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase"
N13-1114,C10-1056,0,0.020848,"al sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of"
N13-1114,W07-0733,0,0.0860924,"model (TM) and language model (LM) adaptation. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and phrase sense disambiguation. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data"
N13-1114,P07-2045,0,0.0214996,"Missing"
N13-1114,koen-2004-pharaoh,0,0.452653,"ve not done so. Finally, the paper analyzes reordering to see why RM adaptation works. There seem to be two factors at work. First, the reordering behaviour of words and phrases often differs dramatically from one bilingual corpus to another. Second, there are corpora (for instance, comparable corpora and bilingual lexicons) which may contain very valuable information for the TM, but which are poor sources of RM information; RM adaptation downweights information from these corpora significantly, and thus improves the overall quality of the RM. 2 Reordering Model In early SMT systems, such as (Koehn, 2004), changes in word order when a sentence is translated were modeled by means of a penalty that is in939 curred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a"
N13-1114,D07-1036,0,0.0862795,"Missing"
N13-1114,D09-1074,0,0.206693,"omain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system"
N13-1114,P10-2041,0,0.135061,"veral different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far fro"
N13-1114,P02-1040,0,0.0876064,"2.1* 33.0** 32.8** Arabic 46.8 47.0 47.5** 48.2** Table 5: Comparison of LM, TM, and RM adaptation. Table 3: Results for variants of RM adaptation. system LM+TM adaptation +RMA+dev-smoothing+DF Chinese 33.2 33.5 Arabic 47.7 48.4** Table 4: RM adaptation improves over a baseline containing adapted LMs and TMs. tem was tuned with batch lattice MIRA (Cherry and Foster, 2012). 4.3 Results For our main baseline, we simply concatenate all training data. We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs. Our metric is case-insensitvie IBM BLEU-4 (Papineni et al., 2002); we report BLEU scores averaged across both test sets. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. Table 3 shows that reordering model adaptation helps in both data settings. Adding either documentfrequency weighting (equation 4) or dev-set smoothing makes the improvement significant in both settings. Using both techniques together yields highly significant improvements. Our second experiment measures the improvement from RM adaptat"
N13-1114,2011.mtsummit-papers.2,0,0.0248563,"hen used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on"
N13-1114,2008.iwslt-papers.6,0,0.0382513,"hree possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase"
N13-1114,2012.eamt-1.43,0,0.0613514,"ining data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on either TM or LM d"
N13-1114,P05-1069,0,0.0313324,"ntence is translated were modeled by means of a penalty that is in939 curred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a feature. However, starting with (Tillmann and Zhang, 2005; Koehn et al., 2005), a more sophisticated type of reordering model has often been adopted as well, and has yielded consistent performance gains. This type of RM typically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lex"
N13-1114,P07-1004,0,0.0230579,"(Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich featu"
N13-1114,C04-1059,0,0.124106,"Missing"
P10-1086,J93-2003,0,0.0245623,"MI (r , c ) is defined as: F ( r, c ) N w(r , c) = MI (r , c) = F (r ) F (c ) × log log N N log (3) F ( r , c) + k R ∑ ( F ( r , c) + k ) = F (r , c ) + k (5) F (c) + kR i i =1 where k is a tunable global smoothing constant, and R is the number of rules. 4 Similarity Functions There are many possibilities for calculating similarities between bags-of-words in different languages. We consider IBM model 1 probabilities and cosine distance similarity functions. 4.1 IBM Model 1 Probabilities For the IBM model 1 similarity function, we take the geometric mean of symmetrized conditional IBM model 1 (Brown et al., 1993) bag probabilities, as in Equation (6). (6) sim(α , γ ) = sqrt( P( B f |Be ) ⋅ P( Be |B f )) To compute P ( B f |Be ) , IBM model 1 as(2) where w f i and we j are values for each source 3.3 where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F (r ) . Thus (3) simplifies to: sumes that all source words are conditionally independent, so that: I P( B f |Be ) = ∏ p ( f i |Be ) (7) i =1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model"
P10-1086,D07-1007,0,0.0353197,"nings. Although this bias is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source"
P10-1086,W09-2404,0,0.0273299,"Missing"
P10-1086,P07-1005,0,0.0426806,"is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target con"
P10-1086,P05-1033,0,0.813888,"s between the source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: X → α,γ , ~ where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. α ( γ ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in α and γ . 1 There has been a lot of work (more details in Section 7) on applying word sense"
P10-1086,J07-2003,0,0.730767,"source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: X → α,γ , ~ where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. α ( γ ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in α and γ . 1 There has been a lot of work (more details in Section 7) on applying word sense disambiguation"
P10-1086,N09-1025,0,0.020523,"the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similar"
P10-1086,J90-1003,0,0.0978788,"Missing"
P10-1086,W08-0302,0,0.0122429,"iations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilin"
P10-1086,C08-1041,0,0.0792135,"work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another wo"
P10-1086,P90-1034,0,0.286648,"ranslation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity functi"
P10-1086,N03-1017,0,0.0120534,"t 2 Rule 3 Context 3 Rule 4 Context 4 会议 他, 出席, 了 他 X1 会议 出席, 了 出席 了 他,会议 the meeting he, attended he X1 the meeting attended attended he, the, meeting Figure 1: example of hierarchical rule pairs and their context features. Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P (γ |α ) and P (α |γ ) are direct and inverse rule-based conditional probabilities; • Pw (γ |α ) and Pw (α |γ ) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a cont"
P10-1086,W09-0424,0,0.0124616,"llowing (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P (γ |α ) and P (α |γ ) are direct and inverse rule-based conditional probabilities; • Pw (γ |α ) and Pw (α |γ ) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a context word which co-occurs with the translation rule. 3.1 Context Features In the hierarchical phrase-based translation method, the translation rules are extracted by abstracting some words from an initial phrase pair (Chiang, 2005). Consider a rule with nonterminals on the source and target side; for a given instance of the rule (a particular phrase pair in the tr"
P10-1086,P98-2127,0,0.260401,"improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton"
P10-1086,D08-1010,0,0.12289,"s into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to cho"
P10-1086,D09-1022,0,0.0330004,"tic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or targ"
P10-1086,P03-1021,0,0.00622246,"on 3 from the full training data for α and for γ , recooc spectively. C f v v f = {w f1 , w f 2 ,..., w f I } v va = {waf1 , waf 2 ,..., waf I } 4.3 Cecooc ⊆ Cefull . Therefore, the original similarity functions are to compare the two context vectors built on full training data directly, as shown in Equation (13). sim(α , γ ) = sim(C ffull , Cefull ) (13) Then, we propose a new similarity function as follows: sim(α , γ ) = sim(C ffull , C cooc )λ1 ⋅ sim(C cooc , Cecooc )λ2 ⋅ sim(Cefull , Cecooc )λ3 (14) f f where the parameters λi (i=1,2,3) can be tuned via minimal error rate training (MERT) (Och, 2003). (11) α The standard cosine distance is defined as the v v inner product of the two vectors v f and va norγ malized by their norms. Based on Equation (10) and (11), it is easy to derive the similarity as follows: v v v f ⋅ va v v sim(α , γ ) = cos( v f , v a ) = v v |v f |⋅ |va | (12) J ∑∑ w = i =1 j =1 I fi Pr( f i |e j )we j I 2 sqrt (∑ w 2fi )sqrt (∑ wafi ) I =1 i =1 are the contexts for they satisfy the constraints: C cooc ⊆ C ffull and f Naïve Cosine Distance Similarity I cooc and C e α and γ when α and γ co-occur. Obviously, where p( f i |e j ) is a lexical probability (we use IBM model"
P10-1086,J07-2002,0,0.0108003,"rt hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclid"
P10-1086,P02-1040,0,0.0849689,"sk. For German-to-English tasks, we used WMT 2006 4 data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua 5 , an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 5 838 http://www.statmt.org/wmt06/ http://www.cs.jhu.edu/~ccb/joshua/index.html racy, i.e., we keep only the top N context words with the highest feature value for each side of a rule 6 . In the following, we use “Alg"
P10-1086,P99-1067,0,0.234566,", City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared. However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora. The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units. Therefore, questions emerge: how good is the sense similarity computed v"
P10-1086,N09-2004,0,0.0398442,"ord-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to choose a translation given the current source context, while our work rates rule pairs i"
P10-1086,N04-1033,0,0.0655189,"on (6). (6) sim(α , γ ) = sqrt( P( B f |Be ) ⋅ P( Be |B f )) To compute P ( B f |Be ) , IBM model 1 as(2) where w f i and we j are values for each source 3.3 where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F (r ) . Thus (3) simplifies to: sumes that all source words are conditionally independent, so that: I P( B f |Be ) = ∏ p ( f i |Be ) (7) i =1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model 1 probability, as described in (Zens and Ney, 2004): p( fi |Be ) = 1 − p( f i |Be ) (8) J p( fi |Be ) ≈ 1 − ∏ (1 − p ( f i |e j )) (9) j =1 where p( f i |Be ) is the probability that f i is not in the translation of Be , and is the IBM model 1 probability. 4.2 Vector Space Mapping A common way to calculate semantic similarity is by vector space cosine distance; we will also 836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. There"
P10-1086,W04-3227,0,0.0214075,"836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. Therefore, we need to first map a vector into the space of the other vector, so that the similarity can be calculated. Fung (1998) and Rapp (1999) map the vector onedimension-to-one-dimension (a context word is a dimension in each vector space) from one language to another language via an initial bilingual dictionary. We follow (Zhao et al., 2004) to do vector space mapping. Our goal is – given a source pattern – to distinguish between the senses of its associated target patterns. Therefore, we map all vectors in target language into the vector space in the source language. What we want is a representav tion va in the source language space of the target v v vector ve . To get va , we can let waf i , the weight of the ith source feature, be a linear combination over target features. That is to say, given a source feature weight for fi, each target feature weight is linked to it with some probability. So that we can calculate a transform"
P10-1086,J10-1004,0,\N,Missing
P10-1086,P07-1020,0,\N,Missing
P10-1086,C98-2122,0,\N,Missing
P10-1086,W04-3250,0,\N,Missing
P12-1098,W05-0909,0,0.328714,"Missing"
P12-1098,P11-1103,0,0.0149301,"), geometric (G), harmonic (H), and quadratic (Q) mean. If all of the values to be averaged are positive, the order is min ≤ H ≤ G ≤ A ≤ Q ≤ max , with equality holding if and only if all the values being averaged are equal. We chose the quadratic mean to combine precision and recall, as follows: Qmean( N ) = ( Pa ( N ) × SBP) 2 + ( Ra ( N ) × SRP) 2 2 (10) PORT uses permutations. These encode one-toone relations but not one-to-many, many-to-one, many-to-many or null relations, all of which can occur in word alignments. We constrain the forbidden types of relation to become one-to-one, as in (Birch and Osborne, 2011). Thus, in a one-tomany alignment, the single source word is forced to align with the first target word; in a many-to-one alignment, monotone order is assumed for the target words; and source words originally aligned to null are aligned to the target word position just after the previous source word’s target position. After the normalization above, suppose we have two permutations for the same source n-word input. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Here, each pij is an integer denoting position in the original source (e.g."
P12-1098,W08-0309,0,0.136376,"Missing"
P12-1098,E06-1032,0,0.0653375,"Missing"
P12-1098,N10-1080,0,0.0668049,"Missing"
P12-1098,P08-1007,0,0.173175,"(Snover et al., 2006), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguis"
P12-1098,W11-2105,1,0.653181,", 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930–939, c Jeju, Republic of Korea, 8-14 Ju"
P12-1098,D08-1064,0,0.0177585,"If there are multiple references, we use closest reference length for each translation hypothesis to compute the numbers of the reference n-grams. 2.1 BLEU BLEU is composed of precision Pg(N) and brevity penalty BP: BLEU = Pg ( N ) × BP (3) where Pg(N) is the geometric average of n-gram precisions 1  N N Pg ( N ) =  ∏ p (n)   n =1  (4) The BLEU brevity penalty punishes the score if the translation length len(T) is shorter than the reference length len(R); it is: BP = min 1.0, e1−len ( R ) / len (T ) (5) ( 2.2 ) PORT PORT has five components: precision, recall, strict brevity penalty (Chiang et al., 2008), strict redundancy penalty (Chen and Kuhn, 2011) and an ordering measure v. The design of PORT is based on exhaustive experiments on a development data set. We do not have room here to give a rationale for all the choices we made when we designed PORT. However, a later section (3.3) reconsiders some of these design decisions. 2.2.1 Precision and Recall The average precision and average recall used in PORT (unlike those used in BLEU) are the arithmetic average of n-gram precisions Pa(N) and recalls Ra(N): 1 N ∑ p ( n) N n=1 1 N Ra ( N ) = ∑ r (n) N n=1 Pa ( N ) = (6) (7) We use two penalties t"
P12-1098,W10-1751,0,0.0590586,"Missing"
P12-1098,W10-1753,0,0.0305202,"Missing"
P12-1098,D10-1092,0,0.111313,"Missing"
P12-1098,P07-2045,0,0.00743306,"an tokens and 50.8M English tokens. We translate both German-to-English (deen) and English-to-German (en-de). The two conditions both use an LM trained on the target side of the parallel training data, and de-en also uses the English Gigaword 5-gram LM. News test 2008 set is used as dev set; News test 2009, 2010, 2011 are used as test sets. One reference is provided for all dev and test sets. 2 3 LDC2003E14 http://www.nist.gov/speech/tests/mt 934 All experiments were carried out with α in Eq. (17) set to 0.25, and involved only lowercase European-language text. They were performed with MOSES (Koehn et al., 2007), whose decoder includes lexicalized reordering, translation models, language models, and word and phrase penalties. Tuning was done with n-best MERT, which is available in MOSES. In all tuning experiments, both BLEU and PORT performed lower case matching of n-grams up to n = 4. We also conducted experiments with tuning on a version of BLEU that incorporates SBP (Chiang et al., 2008) as a baseline. The results of original IBM BLEU and BLEU with SBP were tied; to save space, we only report results for original IBM BLEU here. 3.2.2 Comparisons with automatic metrics First, let us see if BLEU-tun"
P12-1098,W10-1754,0,0.104194,"LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930–939, c Jej"
P12-1098,D11-1035,0,0.036546,"tric PORT. In the table, TER scores are presented as 1-TER to ensure that for all metrics, higher scores mean higher quality. All scores are averages over the relevant test sets. There are twenty comparisons in the table. Among these, there is one case (FrenchEnglish assessed with METEOR) where BLEU outperforms PORT, there are seven ties, and there are twelve cases where PORT is better. Table 3 shows that fr-en outputs are very similar for both tuning types, so the fr-en results are perhaps less informative than the others. Overall, PORT tuning has a striking advantage over BLEU tuning. Both (Liu et al., 2011) and (Cer et al., 2011) showed that with MERT, if you want the best possible score for a system’s translations according to metric M, then you should tune with M. This doesn’t appear to be true when PORT and BLEU tuning are compared in Table 4. For the two Chinese-to-English tasks in the table, PORT tuning yields a better BLEU score than BLEU tuning, with significance at p < 0.05. We are currently investigating why PORT tuning gives higher BLEU scores than BLEU tuning for ChineseEnglish and German-English. In internal tests we have found no systematic difference in dev-set BLEUs, so we specula"
P12-1098,P11-1023,0,0.0461895,"Missing"
P12-1098,P03-1021,0,0.0729119,"Missing"
P12-1098,J03-1002,0,0.00549957,"ference. Several ordering measures have been integrated into MT evaluation metrics recently. Birch and Osborne (2011) use either Hamming Distance or Kendall’s τ Distance (Kendall, 1938) in their metric LRscore, thus obtaining two versions of LRscore. Similarly, Isozaki et al. (2011) adopt either Kendall’s τ Distance or Spearman’s ρ (Spearman, 1904) distance in their metrics. Our measure, v, is different from all of these. We use word alignment to compute the two permutations (LRscore also uses word alignment). The word alignment between the source input and reference is computed using GIZA++ (Och and Ney, 2003) beforehand with the default settings, then is refined with the heuristic grow-diag-finaland; the word alignment between the source input and the translation is generated by the decoder with the help of word alignment inside each phrase pair. Let ν1 = 1 − DIST1 ( P1 , P2 ) n( n + 1) / 2 (12) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reorderings too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this ex"
P12-1098,P09-1034,0,0.0268432,"Missing"
P12-1098,P02-1040,0,0.0982367,"Missing"
P12-1098,W11-2111,0,0.0164433,"tic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930–939, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Compu"
P12-1098,2006.amta-papers.25,0,0.180093,"Missing"
P12-1098,W09-0441,0,0.0219228,"dington, 2002), WER, PER, TER (Snover et al., 2006), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Associati"
P12-1098,C96-2141,0,0.221564,"rd alignment between the source input and the translation is generated by the decoder with the help of word alignment inside each phrase pair. Let ν1 = 1 − DIST1 ( P1 , P2 ) n( n + 1) / 2 (12) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reorderings too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this example: Ref: Recently, I visited Paris Hyp: I visited Paris recently Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure is based on jump width. This punishes a sequence of words that moves a long distance with its internal order conserved, only once rather than on every word. In the following, only two groups of words have moved, so the jump width punishment is light: Ref: In the winter of 2010, I visited Paris Hyp: I visited Paris in the winter of 2010 So the second distance measure is 932 (11) i =1 n DIST2 ( P1 , P2 ) = ∑ |( p1i − p1i −1 ) − ( p2i − p2i −1 ) |(13) i =1 where we set p10 = 0 and p20 = 0 . Let DIST2 ( P1 , P2 ) n2 −1 v2 = 1 − (14) As with v1, v2 is also from 0 to 1,"
P12-1098,W07-0734,0,\N,Missing
P12-1098,W10-1703,0,\N,Missing
P13-1126,D11-1033,0,0.368647,"models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a s"
P13-1126,W09-0432,0,0.103531,"st approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically us"
P13-1126,P08-2040,1,0.823159,"2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Associati"
P13-1126,2011.mtsummit-papers.30,1,0.887165,"Missing"
P13-1126,N12-1047,1,0.101994,"ing IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. The other is a linear combination of TMs trained on each subcorpus, with the weights of each model learned with an EM algorithm to maximize the likelihood of joint empirical phrase pair counts for in-domain dev data. For details, refer to (Foster and Kuhn, 200"
P13-1126,W07-0717,1,0.91413,"omain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen"
P13-1126,D10-1044,1,0.928725,"Missing"
P13-1126,D08-1089,0,0.0852581,"Missing"
P13-1126,2005.eamt-1.19,0,0.0733146,"different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the"
P13-1126,P90-1034,0,0.285011,"independent of the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =< w1 (f, e), ...wi (f, e), ..., wC (f, e) &gt;, (1) where wi (f, e) is a standard tf · idf weight, i.e. wi (f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci (f, e) in the corpus si by dividing"
P13-1126,C10-1056,0,0.0231682,"ve learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop o"
P13-1126,P10-1026,0,0.0348231,"Missing"
P13-1126,W07-0733,0,0.55098,"cently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add th"
P13-1126,P07-2045,0,0.0039338,"Missing"
P13-1126,W04-3250,0,0.184155,"nglish and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: λ was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with λ and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both COS and BC yield statistically significant improvements over the baseline, with BC performing better than COS by a further statistically significant margin. The Bhattacharyya coefficient"
P13-1126,E12-1055,0,0.385818,"in data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phras"
P13-1126,P07-1004,0,0.0242508,"in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2"
P13-1126,C04-1059,0,0.0646896,"Missing"
P13-1126,P98-2127,0,0.0289323,"think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =< w1 (f, e), ...wi (f, e), ..., wC (f, e) &gt;, (1) where wi (f, e) is a standard tf · idf weight, i.e. wi (f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci (f, e) in the corpus si by dividing by the maximum raw count of any phr"
P13-1126,D07-1036,0,0.352527,"Missing"
P13-1126,D09-1074,0,0.415681,"n the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might rec"
P13-1126,P10-2041,0,0.137255,"nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For examp"
P13-1126,P02-1040,0,0.103578,"For details, refer to (Foster and Kuhn, 2007). The value of λ and α (see Eq 4 and Section 2.1) are determined by the performance on the dev set of the Arabic-to-English system. For both Arabic-to-English and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: λ was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with λ and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both"
P13-1126,2011.mtsummit-papers.2,0,0.0163856,"em trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT"
P13-1126,2008.iwslt-papers.6,0,0.0772445,"ded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computat"
P13-1126,C98-2122,0,\N,Missing
W05-0822,P00-1056,0,0.225418,"Missing"
W05-0822,2003.mtsummit-papers.15,1,0.86682,"s generated by rules; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. (A fourth postprocessing phase was not needed for the shared task.) http://www.statmt.org/wpt05/mt-shared-task/ 129 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129–132, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 2.1 Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003). For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ’ an into l’ an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore’s algorithm (Moore, 2002), segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of"
W05-0822,P02-1040,0,0.0814507,"lies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. 130 To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered during the iterations of Och’s algorithm (FrenchEnglish), and a grid search (all other languages). To perform the actual translation, we used our decoder, Canoe, which implements a dynamicprogramming beam search algorithm based on that of Pharaoh (Koehn, 2004). Canoe is input-output compatible with Pharaoh, with the except"
W05-0822,moore-2002-fast,0,0.0342615,"s, 2005 2.1 Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003). For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ’ an into l’ an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore’s algorithm (Moore, 2002), segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of a word to efficiently train a translation system. In a language like German, new words can be formed by compounding (writing two or more words together without a space or a hyphen in between). Segmentation is a crucial step in preprocessing languages such as German and Finnish texts. In addition to these simple operations, we also developed a rule-based component to detect numbers and dates in t"
W05-0822,P02-1038,0,0.179126,"Missing"
W05-0822,P03-1021,0,0.0391459,"ain components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002). The phrase-based translation model is similar to the one described in (Koehn, 2004), and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. 130 To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered dur"
W05-0822,koen-2004-pharaoh,0,\N,Missing
W06-1607,J93-2003,0,0.0496053,"that the phrases s˜k specified by a are conditionally independent, and depend only on their aligned phrases t˜k . The “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 2 Phrase-based Statistical MT Given a source sentence s, our phrase-based SMT system tries to find the target sentence ˆt that is the most likely translation of s. To make search more efficient, we use the Viterbi approximation and seek the most likely combination of t and its alignment a with s, rather than just the most likely t: 3 Smoothing Techniques Smoothing involves some recipe for modifying conditional distributions away from pure relativefrequency estimates made from joint counts, in order to compensate for data sparsity. In the spirit of ((Hastie et al., 2001), figure 2.11, pg. 38)"
W06-1607,2005.mtsummit-posters.19,0,0.0161648,"zero probabilities during phrase induction is reported in (Marcu and Wong, 2002), but no details are given. As described above, (Zens and i∈Aj where Aj is a set of likely alignment connections for sj . In our implementation of this method, ˜ ie the set of we assumed that Aj = {1, . . . , I}, all connections, and used IBM1 probabilities for p(s|t). 57 Ney, 2004) and (Koehn et al., 2005) use two different variants of glass-box smoothing (which they call “lexical smoothing”) over the phrasetable, and combine the resulting estimates with pure relativefrequency ones in a loglinear model. Finally, (Cettollo et al., 2005) describes the use of Witten-Bell smoothing (a black-box technique) for phrasetable counts, but does not give a comparison to other methods. As Witten-Bell is reported by (Chen and Goodman, 1998) to be significantly worse than Kneser-Ney smoothing, we have not yet tested this method. sets for tuning loglinear parameters, and tested on the 3064-sentence test sets. Results are shown in table 1 for relativefrequency (RF), Good-Turing (GT), Kneser-Ney with 1 (KN1) and 3 (KN3) discount coefficients; and loglinear combinations of both RF and KN3 phrasetables with Zens-Ney-IBM1 (ZN-IBM1) smoothed phr"
W06-1607,W95-0103,0,0.0108567,"proach assumes that all source words are conditionally independent, so that: p(˜ s|t˜) = J˜ Y where: c∗i (˜ s, t˜) = X c(˜ s, t1 . . . ti . . . tI˜). ti One might also consider progressively replacing the least informative remaining word in the target phrase (using tf-idf or a similar measure). The same idea could be applied in reverse, by replacing particular source (conditioned) words with wildcards. We have not yet implemented this new glass-box smoothing technique, but it has considerable appeal. The idea is similar in spirit to Collins’ backoff method for prepositional phrase attachment (Collins and Brooks, 1995). p(sj |t˜) j=1 We implemented two variants for p(sj |t˜) that are described in previous work. (Zens and Ney, 2004) describe a “noisy-or” combination: p(sj |t˜) = 1 − p(¯ sj |t˜) ≈ 1− c∗ (˜ s, t˜) Pi ∗ /I˜ ˜ c (˜ s , t ) s˜ i ˜ X I˜ Y (1 − p(sj |ti )) i=1 4 Related Work where s¯j is the probability that sj is not in the translation of t˜, and p(sj |ti ) is a lexical probability. (Zens and Ney, 2004) obtain p(sj |ti ) from smoothed relative-frequency estimates in a wordaligned corpus. Our implementation simply uses IBM1 probabilities, which obviate further smoothing. The noisy-or combination st"
W06-1607,P98-2158,0,0.0668697,"Missing"
W06-1607,N03-1017,0,0.192975,"techniques in ngram LM smoothing is to combine estimates made using the previous n − 1 words with those using only the previous n−i words, for i = 2 . . . n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing. where each fi (s, t, a) is a feature function, and weights λi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model probabilities are features of the form: • The predicted objects are word sequences (in another language). This contrasts to LM smoothing where they are single words, and are thus less amenable to decomposition for smoothing purposes. log p(s|t, a) ≈ We propose various ways of dealing with these special features of the phrasetable smoothing problem, and give evaluations of their performance within a"
W06-1607,W05-0824,0,0.0192822,"Missing"
W06-1607,W02-1018,0,0.0414401,"r statistical MT. For the IBM models, alignment probabilities need to be smoothed for combinations of sentence lengths and positions not encountered in training data (Garc´ıa-Varea et al., 1998). Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. Langlais (2005) reports negative results for synonym-based smoothing of IBM2 lexical probabilities prior to extracting phrases for phrasebased SMT. For phrase-based SMT, the use of smoothing to avoid zero probabilities during phrase induction is reported in (Marcu and Wong, 2002), but no details are given. As described above, (Zens and i∈Aj where Aj is a set of likely alignment connections for sj . In our implementation of this method, ˜ ie the set of we assumed that Aj = {1, . . . , I}, all connections, and used IBM1 probabilities for p(s|t). 57 Ney, 2004) and (Koehn et al., 2005) use two different variants of glass-box smoothing (which they call “lexical smoothing”) over the phrasetable, and combine the resulting estimates with pure relativefrequency ones in a loglinear model. Finally, (Cettollo et al., 2005) describes the use of Witten-Bell smoothing (a black-box t"
W06-1607,P03-1021,0,0.228399,"mass is subtracted from the seen translations. To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp λi fi (s, t, a) i • There is no obvious lower-order distribution for backoff. One of the most important techniques in ngram LM smoothing is to combine estimates made using the previous n − 1 words with those using only the previous n−i words, for i = 2 . . . n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing. where each fi (s, t, a) is a feature function, and weights λi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model probabilities are features of the form: • The predicted objects are word sequences (in another language). This contrasts to LM smoothing where they are single wo"
W06-1607,2001.mtsummit-papers.68,0,0.0338788,"To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp λi fi (s, t, a) i • There is no obvious lower-order distribution for backoff. One of the most important techniques in ngram LM smoothing is to combine estimates made using the previous n − 1 words with those using only the previous n−i words, for i = 2 . . . n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing. where each fi (s, t, a) is a feature function, and weights λi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model probabilities are features of the form: • The predicted objects are word sequences (in another language). This contrasts to LM smoothing where they are single words, and are thus less amenable to decomposition for smoothi"
W06-1607,N04-1033,0,0.587067,"smoothing distribution to modify p˜(˜ s|t˜) for higher-frequency events, whereas the latter uses it only for low-frequency events (most often 0frequency events). Since for phrasetable smoothing, better prediction of unseen (zero-count) events has no direct impact—only seen events are represented in the phrasetable, and thus hypothesized during decoding—interpolation seemed a more suitable approach. For combining relative-frequency estimates with glass-box smoothing distributions, we employed loglinear interpolation. This is the traditional approach for glass-box smoothing (Koehn et al., 2003; Zens and Ney, 2004). To illustrate the difference between linear and loglinear interpolation, consider combining two Bernoulli distributions p1 (x) and p2 (x) using each method: plinear (x) = αp1 (x) + (1 − α)p2 (x) ploglin (x) = p1 (x)α p2 (x) p1 (x)α p2 (x) + q1 (x)α q2 (x) where qi (x) = 1 − pi (x). Setting p2 (x) = 0.5 to simulate uniform smoothing gives ploglin (x) = p1 (x)α /(p1 (x)α + q1 (x)α ). This is actually less smooth than the original distribution p1 (x): it preserves extreme values 0 and 1, and makes intermediate values more extreme. On the other hand, plinear (x) = αp1 (x) + (1 − α)/2, which has"
W06-1607,P02-1040,0,\N,Missing
W06-1607,P04-1066,0,\N,Missing
W06-3118,N06-1004,1,0.829818,"s c(s, t) where D = n1 /(n1 + 2n2 ), n1+ (∗, t) is the number of distinct phrases s with which t co-occurs, and P pk (s) = n1+ (s, ∗)/ s n1+ (s, ∗), with n1+ (s, ∗) analogous to n1+ (∗, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SCMs differ from the conventional penalty-based distortion, which always favours less rather than more distortion. We developed a particular kind of SCM based on decision trees (DTs) containing both questions of a positional type (e.g., questions about the distance of a given phrase from the beginning of the source sentence or from the previ"
W06-3118,W05-0822,1,0.831685,"le data set and explore the benefits of a number of recently added features. Section 2 describes the changes that have been made to Portage in the past year that affect the participation in the 2006 shared task. Section 3 outlines the methods employed for this task and extensions of it. In Section 4 the results are summarized in tabular form. Following these, there is a conclusions section that highlights what can be gleaned of value from these results. 2 Portage Because this is the second participation of Portage in such a shared task, a description of the base system can be found elsewhere (Sadat et al, 2005). Briefly, Portage is a research vehicle and development prototype system exploiting the state-of-the-art in statistical machine translation (SMT). It uses a custom Phrase-Table Smoothing Phrase-based SMT relies on conditional distributions p(s|t) and p(t|s) that are derived from the joint frequencies c(s, t) of source/target phrase pairs observed in an aligned parallel corpus. Traditionally, relative-frequency estimation is used to derive conP ditional distributions, ie p(s|t) = c(s, t)/ s c(s, t). However, relative-frequency estimation has the well-known problem of favouring rare events. For"
W06-3118,W05-0800,0,0.0869259,"Missing"
W06-3118,2006.jeptalnrecital-poster.23,1,0.726586,"n to the training resources used in WPT 2005 for the French-English task, i.e. Europarl and Hansard, we used a bilingual dictionary, Le Grand Dictionnaire Terminologique (GDT) 2 to train translation models and the English side of the UN parallel corpus (LDC2004E13) to train an English language model. Integrating terminological lexicons into a statistical machine translation engine is not a straightforward operation, since we cannot expect them to come with attached probabilities. The approach we took consists on viewing all translation candidates of each source term or phrase as equiprobable (Sadat et al, 2006). In total, the data used in this second part of our contribution to WMT 2006 is described as follows: (1) A set of 688,031 sentences in French and English extracted from the Europarl parallel corpus (2) A set of 6,056,014 sentences in French and English extracted from the Hansard parallel corpus, the official record of Canada’s parliamentary debates. (3) A set of 701,709 sentences in French and English extracted from the bilingual dictionary GDT. (4) Language models were trained on the French and English parts of the Europarl and Hansard. We used the provided Europarl corpus while omitting da"
W06-3118,N04-1033,0,0.0248659,"s phrase. The resulting estimates are: cg (s, t) , s cg (s, t) + p(t)n1 pg (s|t) = P P where p(t) = c(t)/ t c(t). The estimates for pg (t|s) are analogous. The second strategy is Kneser-Ney smoothing (Kneser and Ney, 1995), using the interpolated variant described in (Chen and Goodman., 1998):1 pk (s|t) = c(s, t) − D + D n1+ (∗, t) pk (s) P s c(s, t) where D = n1 /(n1 + 2n2 ), n1+ (∗, t) is the number of distinct phrases s with which t co-occurs, and P pk (s) = n1+ (s, ∗)/ s n1+ (s, ∗), with n1+ (s, ∗) analogous to n1+ (∗, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SC"
W07-0703,J93-2003,0,0.00932986,"Missing"
W07-0703,P06-1010,0,0.044303,"ts a correct transliteration that the decoder outputs correctly, but which fails to receive credit from the BLEU metric because this transliteration is not found in the references. As an example, for the name “”ﺳﻮﯾﺮﯾﻮس, four references came up with four different interpretations: swerios, swiriyus, severius, sweires. A quick query in Google showed us another four acceptable interpretations (severios, sewerios, sweirios, sawerios). Machine transliteration has been an active research field for quite a while (Al-Onaizan and Knight, 2002; AbdulJaleel and Larkey, 2003; Klementiev and Roth, 2006; Sproat et al, 2006) but to our knowledge there is little published work on evaluating transliteration within a real MT system. The closest work to ours is described in (Hassan and Sorensen, 2005) where they have a list of names in Arabic and feed this list as the input text to their MT system. They evaluate their system in three different cases: as a word-based NE translation, phrase-based NE translation and in presence of a transliteration module. Then, they report the BLEU score on the final output. Since their text is comprised of only NEs, the BLEU increase is quite high. Combining all three models, they get"
W07-0703,N06-1011,0,\N,Missing
W07-0703,W02-0505,0,\N,Missing
W07-0703,P02-1040,0,\N,Missing
W07-0703,W05-0712,0,\N,Missing
W07-0703,N03-1017,0,\N,Missing
W07-0703,D08-1076,0,\N,Missing
W07-0717,J93-2003,0,0.0101062,"p(˜ s|t˜): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Mixture-Model Adaptation 2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system (Koehn et al., 2003). Given a source sentence s, this tries to find the target sentence ˆt that is the most likely translation of s, using the Viterbi approximation: 1. Split the corpus into different components, according to some criterion. 2. Train a model on each corpus component. ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following general algorithm: t,a where alignment a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK );"
W07-0717,P99-1022,0,0.0257994,"slight advantage. granularity baseline file genre document dev nist04mix 31.9 32.4 32.5 32.9 nist05 30.4 30.8 31.1 30.9 test nist06nist 27.6 28.6 28.9 28.6 nist06gale 12.9 13.4 13.2 13.4 Table 8: The effects of source granularity on dynamic adaptation. 5 Related Work Mixture modeling is a standard technique in machine learning (Hastie et al., 2001). It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990). Most previous work on adaptive SMT focuses on the use of IR techniques to identify a relevant subset of the training corpus from which an adapted model can be learned. Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments. Hildebrand et al (1995) describe a similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation. They rely on a perplexit"
W07-0717,W06-1607,1,0.748963,"and loglinear mixing frameworks, with uniform weights used in the linear mixture. Both types of mixture model are better than the baseline, but the linear mixture is slightly better than the loglinear mixture. This is quite surprising, because these results are on the development set: the loglinear model tunes its component weights on this set, whereas the linear model only adjusts global LM and TM weights. We speculated that this may have been due to non-smooth component models, and tried various 132 smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components. None helped, however, and we conclude that the problem is most likely that Och’s algorithm is unable to find a good maximimum in this setting. Due to this result, all experiments we describe below involve linear mixtures only. combination baseline loglinear mixture uniform linear mixture adapted model LM TM LM+TM 30.2 30.2 30.2 30.9 31.2 31.4 31.2 31.1 31.8 Table 2: Linear versus loglinear combinations on NIST04-nw. 4.2 Distance Metrics for Weighting Table 3 compares the performance of all distance metrics descr"
W07-0717,P03-1021,0,0.053334,"model on each corpus component. ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following general algorithm: t,a where alignment a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK ); ˜ ˜ tk are target phrases such that t = t1 . . . t˜K ; s˜k are source phrases such that s = s˜j1 . . . s˜jK ; and s˜k is the translation of the kth target phrase t˜k . To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp αi fi (s, t, a) (1) i where each fi (s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and 129 3. Weight each model according to its fit with the test domain: • For cross-domain adaptation, set parameters using a development corpus drawn from the test domain, and use for all future documents. • For dynamic adaptation, set global parameters using a development corpus drawn from several different domain"
W07-0717,2001.mtsummit-papers.68,0,0.0190075,"s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following general algorithm: t,a where alignment a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK ); ˜ ˜ tk are target phrases such that t = t1 . . . t˜K ; s˜k are source phrases such that s = s˜j1 . . . s˜jK ; and s˜k is the translation of the kth target phrase t˜k . To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp αi fi (s, t, a) (1) i where each fi (s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and 129 3. Weight each model according to its fit with the test domain: • For cross-domain adaptation, set parameters using a development corpus drawn from the test domain, and use for all future documents. • For dynamic adaptation, set global parameters using a development corpus drawn from several different domains. Set mixture weights as a function of the distances from c"
W07-0717,P06-1091,0,0.00665594,"chnique of mixture modeling (Hastie et al., 2001). This involves dividing the training corpus into different components, training a model on each part, then weighting each model appropriately for the current context. Mixture modeling is a simple framework that encompasses many different variants, as described below. It is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases. This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (Tillmann and Zhang, 2006; Liang et al., 2006). Techniques for assigning mixture weights depend on the setting. In cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly. In dynamic adaptation, training poses a problem because no reference text is available. Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample). We do not learn mixture weights directly with this method, because there is little hope 128 Proceedings of th"
W07-0717,N04-1033,0,0.0400093,"r is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes. 4-gram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit. Phrase translation model probabilities PK are features of the form: log p(s|t, a) ≈ sk |t˜k ). k=1 log p(˜ We use two different estimates for the conditional probabilities p(t˜|˜ s) and p(˜ s|t˜): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Mixture-Model Adaptation 2 Phrase-based Statistical MT Our baseline is a s"
W07-0717,2006.iwslt-evaluation.12,0,0.025729,"language corpus. This approach has the advantage of not limiting LM adaptation to a parallel corpus, but the disadvantage of requiring two translation passes (one to generate the nbest lists, and another to translate with the adapted model). Ueffing (2006) describes a self-training approach that also uses a two-pass algorithm. A baseline system generates translations that, after confidence filtering, are used to construct a parallel corpus based on the test set. Standard phrase-extraction tech134 niques are then applied to extract an adapted phrase table from the system’s own output. Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters. Each source sentence is then decoded using the language model trained on the cluster that assigns highest likelihood to that sentence. The work we present here is complementary to both the IR approaches and Ueffing’s method because it provides a way of exploiting a preestablished corpus division. This has the potential to allow sentences having little surface similarity to the current source text to contribute"
W07-0717,C04-1059,0,0.84709,"Missing"
W07-0717,N03-1017,0,0.110712,"p(˜ We use two different estimates for the conditional probabilities p(t˜|˜ s) and p(˜ s|t˜): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Mixture-Model Adaptation 2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system (Koehn et al., 2003). Given a source sentence s, this tries to find the target sentence ˆt that is the most likely translation of s, using the Viterbi approximation: 1. Split the corpus into different components, according to some criterion. 2. Train a model on each corpus component. ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following gener"
W07-0717,P06-1096,0,0.0772821,"Missing"
W07-0717,P02-1040,0,\N,Missing
W07-0717,D08-1076,0,\N,Missing
W07-0717,2005.eamt-1.19,0,\N,Missing
W07-0728,W06-1607,1,0.620595,"tails of which can be found in (Ueffing et al., 2007). The main features of this configuration are: • The use of two distinct phrase tables, containing phrase pairs extracted from the Europarl and the News Commentary training corpora respectively. • Multiple phrase-probability feature functions in the log-linear models, including a joint prob1 A version of PORTAGE is made available by the NRC to Canadian universities for research and education purposes. 204 ability estimate, a standard frequency-based conditional probability estimate, and variants thereof based on different smoothing methods (Foster et al., 2006). • A 4-gram language model trained on the combined Europarl and News Commentary targetlanguage corpora. • A 3-gram adapted language model: this is trained on a mini-corpus of test-relevant targetlanguage sentences, extracted from the training material using standard information retrieval techniques. • A 5-gram truecasing model, trained on the combined Europarl and News Commentary target-language corpora. 2.3 Training data Ideally, the training material for the post-editing layer of our system should consist in a corpus of text in two parallel versions: on the one hand, raw machine translation"
W07-0728,W05-0822,1,0.476331,"e did not rely on this feature, and used the system in its basic “out-of-the-box” configuration. 2.2 Statistical Phrase-based Post-Editing The output of the rule-based MT system described above is fed into a post-editing layer that performs domain-specific corrections and adaptation. This operation is conceptually not very different from a “target-to-target” translation; for this task, we used the PORTAGE system, a state-of-the-art statistical phrase-based machine translation system developed at the National Research Council of Canada (NRC). 1 A general description of PORTAGE can be found in (Sadat et al., 2005). For our participation in this shared task, we decided to configure and train the PORTAGE system for post-editing in a manner as much as possible similar to the corresponding translation system, the details of which can be found in (Ueffing et al., 2007). The main features of this configuration are: • The use of two distinct phrase tables, containing phrase pairs extracted from the Europarl and the News Commentary training corpora respectively. • Multiple phrase-probability feature functions in the log-linear models, including a joint prob1 A version of PORTAGE is made available by the NRC to"
W07-0728,N07-1064,1,0.836481,", and as much as 5 BLEU points improvement over the direct SMT approach. This article describes a machine translation system based on an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based MT system, then automatically post-edit the output using a statistical phrase-based system. An implementation of this approach based on the SYSTRAN and PORTAGE MT systems was used in the shared task of the Second Workshop on Statistical Machine Translation. Experimental results on the test data of the previous campaign are presented. 1 Introduction Simard et al. (2007) have recently shown how a statistical phrase-based machine translation system can be used as an automatic post-editing (APE) layer, on top of a rule-based machine translation system. The motivation for their work is the repetitive nature of the errors typically made by rule-based systems. Given appropriate training material, a statistical MT system can be trained to correct these systematic errors, therefore reducing the post-editing effort. The statistical system views the output of the rule-based system as the source language, and reference human translations as the target language. Because"
W07-0728,W07-0732,0,\N,Missing
W07-0728,W07-0724,1,\N,Missing
W09-0439,N04-1033,0,0.0174667,"roach by between 0.6 and 2.5 BLEU points. The convergence behaviour of the lattice variant was also much smoother than that of the n-best variant. It would be interesting to apply some of the insights of the current paper to the lattice variant of Och’s procedure. Previous Work One possible approach to estimating log-linear weights on features is to dispense with the n-best lists employed by Och’s procedure and, instead, to optimize weights by directly accessing the decoder. The disadvantage of this approach is that far more iterations of decoding of the full development set are required. In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. However, each iteration is unusually fast, because only monotone decoding is permitted (i.e., the order of phrases in the target language mirrors that in the source language). Similarly, Cettolo and Federico (2004) apply the simplex method to optimize weights directly using the decoder. In their experiments on NIST 2003 Chinese-English data, they found about 100 iterations of decoding were required. Although they obtained consistent and stable performance gains for MT, the"
W09-0439,W08-0304,0,0.698034,"ve operation. Moreover, because this function is not differentiable, efficient gradient-based optimization algorithms cannot be used. Och’s procedure is the most widely-used version of MERT for SMT (Och, 2003). To reduce Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242–249, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 242 the test set. We analyze the causes of each of these problems, and propose solutions for improving the stability of the overall procedure. 2 pruning the set of n-best hypotheses at each iteration. Cer et al (2008) also aim at improving Och’s MERT. They focus on the search for the best set of weights for an n-best list that follows choice of a starting point. They propose a modified version of Powell’s in which “diagonal” directions are chosen at random. They also modify the objective function used by Powell’s to reflect the width of the optima found. They are able to show that their modified version of MERT outperforms both a version using Powell’s, and a more heuristic search algorithm devised by Philipp Koehn that they call Koehn Coordinate Descent, as measured on the development set and two test dat"
W09-0439,D07-1055,0,0.154387,"Missing"
W09-0439,2004.iwslt-papers.2,0,0.037137,"ghts on features is to dispense with the n-best lists employed by Och’s procedure and, instead, to optimize weights by directly accessing the decoder. The disadvantage of this approach is that far more iterations of decoding of the full development set are required. In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. However, each iteration is unusually fast, because only monotone decoding is permitted (i.e., the order of phrases in the target language mirrors that in the source language). Similarly, Cettolo and Federico (2004) apply the simplex method to optimize weights directly using the decoder. In their experiments on NIST 2003 Chinese-English data, they found about 100 iterations of decoding were required. Although they obtained consistent and stable performance gains for MT, these were inferior to the gains yielded by Och’s procedure in (Och, 2003). Taking Och’s MERT procedure as a baseline, (Zens et al., 2007) experiment with different training criteria for SMT and obtain the best results for a criterion they call “expected BLEU score”. Moore and Quirk (2008) share the goal underlying our own research: impro"
W09-0439,P05-1033,0,0.00472174,"ve to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och’s procedure by supplying different random seeds to a core component of the procedure (Powell’s algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data. 1 Introduction Most recent approaches in SMT, eg (Koehn et al., 2003; Chiang, 2005), use a log-linear model to combine probabilistic features. Minimum ErrorRate Training (MERT) aims to find the set of loglinear weights that yields the best translation performance on a development corpus according to some metric such as BLEU. This is an essential step in SMT training that can significantly improve performance on a test corpus compared to setting weights by hand. MERT is a difficult problem, however, because calculating BLEU as a function of log-linear weights requires decoding, which is an expensive operation. Moreover, because this function is not differentiable, efficient g"
W09-0439,P08-2010,0,0.061885,"at improving Och’s MERT. They focus on the search for the best set of weights for an n-best list that follows choice of a starting point. They propose a modified version of Powell’s in which “diagonal” directions are chosen at random. They also modify the objective function used by Powell’s to reflect the width of the optima found. They are able to show that their modified version of MERT outperforms both a version using Powell’s, and a more heuristic search algorithm devised by Philipp Koehn that they call Koehn Coordinate Descent, as measured on the development set and two test data sets. (Duh and Kirchhoff, 2008) ingeniously uses MERT as a weak learner in a boosting algorithm that is applied to the n-best reranking task, with good results (a gain of about 0.8 BLEU on the test set). Recently, some interesting work has been done on what might be considered a generalization of Och’s procedure (Macherey et al., 2008). In this generalization, candidate hypotheses in each iteration of the procedure are represented as lattices, rather than as n-best lists. This makes it possible for a far greater proportion of the search space to be represented: a graph density of 40 arcs per phrase was used, which correspon"
W09-0439,P07-1019,0,0.0113459,"this does not guarantee that Powell’s algorithm will find a global maximum, and so Powell’s is typically run with many different randomly-chosen initial weights in order to try to find a good maximum. 4 num sents 1506 2080 1788 1664 num Chinese toks 38,312 55,159 53,446 41,798 Table 1: Development and test corpora. et al., 2003) employing a log-linear combination of feature functions. HMM and IBM2 models were used to perform separate word alignments, which were symmetrized by the usual “diag-and” algorithm prior to phrase extraction. Decoding used beam search with the cube pruning algorithm (Huang and Chiang, 2007). We used two separate log-linear models for MERT: • large: 16 phrase-table features, 2 4-gram language model features, 1 distortion feature, and 1 word-count feature (20 features in total). • small: 2 phrase-table features, 1 4-gram language model feature, 1 distortion feature, and 1 word-count feature (5 features in total). The phrase-table features for the large model were derived as follows. Globally-trained HMM and IBM2 models were each used to extract phrases from UN and non-UN portions of the training corpora (see below). This produced four separate phrase tables, each of which was used"
W09-0439,N03-1017,0,0.00262285,"tends to be sensitive to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och’s procedure by supplying different random seeds to a core component of the procedure (Powell’s algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data. 1 Introduction Most recent approaches in SMT, eg (Koehn et al., 2003; Chiang, 2005), use a log-linear model to combine probabilistic features. Minimum ErrorRate Training (MERT) aims to find the set of loglinear weights that yields the best translation performance on a development corpus according to some metric such as BLEU. This is an essential step in SMT training that can significantly improve performance on a test corpus compared to setting weights by hand. MERT is a difficult problem, however, because calculating BLEU as a function of log-linear weights requires decoding, which is an expensive operation. Moreover, because this function is not differentiab"
W09-0439,D08-1076,0,0.0216455,"width of the optima found. They are able to show that their modified version of MERT outperforms both a version using Powell’s, and a more heuristic search algorithm devised by Philipp Koehn that they call Koehn Coordinate Descent, as measured on the development set and two test data sets. (Duh and Kirchhoff, 2008) ingeniously uses MERT as a weak learner in a boosting algorithm that is applied to the n-best reranking task, with good results (a gain of about 0.8 BLEU on the test set). Recently, some interesting work has been done on what might be considered a generalization of Och’s procedure (Macherey et al., 2008). In this generalization, candidate hypotheses in each iteration of the procedure are represented as lattices, rather than as n-best lists. This makes it possible for a far greater proportion of the search space to be represented: a graph density of 40 arcs per phrase was used, which corresponds to an n-best size of more than two octillion (2 ∗ 1027 ) entries. Experimental results for three NIST 2008 tasks were very encouraging: though BLEU scores for the lattice variant of Och’s procedure did not typically exceed those for the n-best variant on development data, on test data the lattice varia"
W09-0439,C08-1074,0,0.586396,"ors that in the source language). Similarly, Cettolo and Federico (2004) apply the simplex method to optimize weights directly using the decoder. In their experiments on NIST 2003 Chinese-English data, they found about 100 iterations of decoding were required. Although they obtained consistent and stable performance gains for MT, these were inferior to the gains yielded by Och’s procedure in (Och, 2003). Taking Och’s MERT procedure as a baseline, (Zens et al., 2007) experiment with different training criteria for SMT and obtain the best results for a criterion they call “expected BLEU score”. Moore and Quirk (2008) share the goal underlying our own research: improving, rather than replacing, Och’s MERT procedure. They focus on the step in the procedure where the set of feature weights optimizing BLEU (or some other MT metric) for an n-best list is estimated. Typically, several different starting points are tried for this set of weights; often, one of the starting points is the best set of weights found for the previous set of n-best hypotheses. The other starting points are often chosen randomly. In this paper, Moore and Quirk look at the best way of generating the random starting points; they find that"
W09-0439,P03-1021,0,0.167088,"oglinear weights that yields the best translation performance on a development corpus according to some metric such as BLEU. This is an essential step in SMT training that can significantly improve performance on a test corpus compared to setting weights by hand. MERT is a difficult problem, however, because calculating BLEU as a function of log-linear weights requires decoding, which is an expensive operation. Moreover, because this function is not differentiable, efficient gradient-based optimization algorithms cannot be used. Och’s procedure is the most widely-used version of MERT for SMT (Och, 2003). To reduce Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242–249, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 242 the test set. We analyze the causes of each of these problems, and propose solutions for improving the stability of the overall procedure. 2 pruning the set of n-best hypotheses at each iteration. Cer et al (2008) also aim at improving Och’s MERT. They focus on the search for the best set of weights for an n-best list that follows choice of a starting point. They propose a modified version of Powell’s i"
W10-1702,P07-2045,0,0.00952282,"sh. Chinese-to-English tasks are based on training data for the NIST 1 2009 evaluation Chinese-toEnglish track. All the allowed bilingual corpora have been used for estimating the translation model. We trained two language models: the first one is a 5-gram LM which is estimated on the target side of the parallel data. The second is a 5- 1 1,506 1,664 1,357 - Table 1: Statistics of training, dev, and test sets for Chinese-to-English task. We carried out experiments based on translation N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 |S| |W| |S| |S| |S| |S| Eng 10.1M 270.0M 279.1M 2 http://www.nist.gov/speech/tests/mt 14 http://www.st"
W10-1702,N04-1022,0,0.317546,"ments across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 P"
W10-1702,D08-1076,0,0.0202448,"N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 |S| |W| |S| |S| |S| |S| Eng 10.1M 270.0M 279.1M 2 http://www.nist.gov/speech/tests/mt 14 http://www.statmt.org/wmt06/ 20 additional features. In this experiment, the former is about 10 times faster than the latter in terms of processing time, as shown in Table 3. testset NIST’06 NIST’08 baseline 35.70 28.60 rescoring 36.01 28.97 three-pass 35.98 28.99 FCD 36.00 29.10 Fwd. 36.13 29.19 Bwd. 36.11 29.20 Bid. 36.20 29.28 In our second experiment, we set the size of N-best list N equal to 10,000 for both Chinese-toEnglish and German-to-English tasks. The results are reported in Table 4. The s"
W10-1702,P03-1021,0,0.0144611,"Missing"
W10-1702,N04-1021,0,0.030639,"performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Theref"
W10-1702,P02-1040,0,0.101853,"Missing"
W10-1702,D08-1065,0,0.239694,"of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 Proceedings of the Joint 5th Workshop on Statist"
W10-1702,P08-1025,0,0.021642,"airs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 Proceedings of the Joint"
W10-1702,2007.mtsummit-papers.15,1,0.906463,"am n-gram one week's expansion new partial hyp. one week's work about one week's work new about week's work hypotheses one weeks' work . one week's work . one week's work . Fast Consensus Hypothesis Regeneration Since the three hypothesis regeneration methods with n-gram expansion, confusion network decoding and re-decoding produce very similar performance (Chen et al., 2008), we consider only n-gram expansion method in this paper. N-gram expansion can (almost) fully exploit the search space of target strings which can be generated by an n-gram language model trained on the N-best hypotheses (Chen et al., 2007). Figure 1: Example of original hypotheses; bi-grams collected from them; backward expanding a partial hypothesis via an overlapped n-1-gram; and new hypotheses generated through backward n-gram expansion. 2.1 2.2 Hypothesis regeneration with bidirectional n-gram expansion Feature-based scoring functions To speed up the search, the partial hypotheses are pruned via beam-search in each expanding step. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothe"
W10-1702,C08-1014,1,0.891638,"-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7 {Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca Nero et al. (2009) proposed a fast consensus decoding (FCD) algorithm in which the similarity scores are computed based on the feature expectations over the translation N-best list or translation forest. It is equivalent to MBR decoding when using a linear similarity function, such as unigram precision. Re-ranking approaches improve performance on an N-best list whose contents are fixed. A complementary strategy is to augment the contents of an N-best list in order to broaden the search space. Chen et al (2008) have proposed a three-pass SMT process, in which a hypothesis regeneration pass is added between the decoding and rescoring passes. New hypotheses are generated based on the original N-best hypotheses through n-gram expansion, confusion-network decoding or re-decoding. All three hypothesis regeneration methods obtained decent and comparable improvements in conjunction with the same rescoring model. However, since the final translation candidates in this approach are produced from different methods, local feature functions (such as translation models and reordering models) of each hypothesis a"
W10-1702,P09-1064,0,\N,Missing
W10-1702,W07-0724,0,\N,Missing
W10-1702,P07-1019,0,\N,Missing
W10-1717,W08-0304,0,0.0612136,"Missing"
W10-1717,W09-0439,1,0.800854,"alf of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder runs. Though the algorithm is straightforward and is highly parallelizable, attention must be paid to space and time resource issues during implementation. Lattices output by our decoder were large and needed to be shrunk dramatically for the algorithm to function wel"
W10-1717,W07-0717,1,0.856161,"kward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, cont"
W10-1717,P07-1019,0,0.047578,"MM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, contiguous phrases can always be swapped. Out-of-vocabulary (OOV) source words are passed through unchanged to the ta"
W10-1717,2005.iwslt-1.8,0,\N,Missing
W10-1717,N04-1033,0,\N,Missing
W10-1717,D08-1076,0,\N,Missing
W10-1717,P03-1021,0,\N,Missing
W11-2105,W05-0909,0,0.163547,"tic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following fact"
W11-2105,W10-1749,0,0.0124594,"=1− ∑d i 2 i (n + 1)n(n − 1) (16) where di indicates the distance between the ranks of the i-th element. For example: T: Bob reading book likes R: Bob likes reading book The rank vector of the reference is [1, 2, 3, 4], while the translation rank vector is [1, 3, 4, 2]. The Spearman’s correlation score between these two 2 2 2 vectors is 1 − 0 + (3 − 2) + (4 − 3) + (2 − 4) =0.90. (4 + 1) ⋅ 4 ⋅ (4 − 1) In order to avoid negative values, we normalized the correlation score, obtaining the penalty NSCP: (17) NSCP = (1 + ρ) / 2 Normalized Kendall’s correlation penalty (NKCP): this is adopted from (Birch and Osborne, 2010) and (Isozaki et al., 2010). In the previous example, where the rank vector of the 73 translation is [1, 3, 4, 2], there are C42 = 6 pairs of integers. There are 4 increasing pairs: (1,3), (1,4), (1,2) and (3,4). Kendall’s correlation is defined by: τ = 2× # increasing pairs −1 # all pairs (18) Therefore, Kendall’s correlation for the translation “Bob reading book likes” is 2 × 4 / 6 − 1 =0.33. Again, to avoid negative values, we normalized the coefficient score, obtaining the penalty NKCP: (19) NKCP = (1 + τ ) / 2 2.3 Term weighting The original BLEU metric weights all n-grams equally; howeve"
W11-2105,W08-0309,0,0.0730212,"ng WMT 2008 all-to-English submissions as the dev set. Test sets include WMT 2009 all-to-English, WMT 2010 all-to-English and 2010 English-to-all submissions. Table 1 summarizes the dev and test set statistics. Default settings Table 2: Weight of each penalty 3.3 Evaluation metrics We used Spearman’s rank correlation coefficient to measure the correlation of AMBER with the human judgments of translation at the system level. The human judgment score we used is based on the “Rank” only, i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al., 2008). Thus, AMBER and the other metrics were evaluated on how well their rankings correlated with the human ones. For the sentence level, we use consistency rate, i.e., how consistent the ranking of sentence pairs is with the human judgments. 3.4 Results All test results shown in this section are averaged over all three tests described in 3.1. First, we compare AMBER with two of the most widely used metrics: original IBM BLEU and METEOR v1.0. Table 3 gives the results; it shows both the version of AMBER with basic preprocessing, AMBER(1) (with tokenization and lowercasing) and the default version"
W11-2105,E06-1032,0,0.1159,", we decided to make it a modified version of BLEU whose rankings of translations would (ideally) correlate even more highly with human rankings. Thus, our metric is called AMBER: “A Modified Bleu, Enhanced Ranking” metric. Some of the AMBER variants use an information source with a mild linguistic flavour – morphological knowledge about suffixes, roots and prefixes – but otherwise, the metric is based entirely on surface comparisons. 2 AMBER Like BLEU, AMBER is composed of two parts: a score and a penalty. AMBER = score × penalty (1) To address weaknesses of BLEU described in the literature (Callison-Burch et al., 2006; Lavie and Denkowski, 2009), we use more sophisticated formulae to compute the score and penalty. 2.1 Enhancing the score First, we enrich the score part with geometric average of n-gram precisions (AvgP), F-measure derived from the arithmetic averages of precision and recall (Fmean), and arithmetic average of Fmeasure of precision and recall for each n-gram (AvgF). Let us define n-gram precision and recall as follows: 71 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 71–77, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics"
W11-2105,N10-1080,0,0.0252719,"her level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a metric to tune an MT system. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). When we developed our own metric, we decided to make it a modified version of BLEU whose rankings of translations would (ideally) correlate even more highly with human rankings. Thus, our metric is called AMBER: “A Modified Bleu, Enhanced Ranking” metric. Some of the AMBER variants use an information source with a mild linguistic flavour – morphological knowledge about suffixes, roots and prefixes – but otherwise, the metric is based entirely on surface comparisons. 2 AMBER Like BLEU, AMBER is composed of two parts: a score and a penalty. AMBER = score × penalty (1) To address weaknesses of"
W11-2105,P08-1007,0,0.0929208,"th human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed"
W11-2105,D08-1064,0,0.0298414,"precision and recall for each n-gram (AvgF) is given by: 1 N p ( n) r ( n) (8) AvgF ( N , α ) = ∑ N n =1 αp (n ) + (1 − α ) r ( n) The score is the weighted average of the three values: AvgP, Fmean, and AvgF. score( N ) = θ1 × AvgP( N ) + θ 2 × Fmean( N , M ,α ) (9) + (1 − θ1 − θ 2 ) × AvgF ( N ,α ) The free parameters N, M, α , θ1 and θ 2 were manually tuned on a dev set. 2.2 Various penalties Instead of the original brevity penalty, we experimented with a product of various penalties: P penalty = ∏ peniwi (10) i =1 where wi is the weight of each penalty peni. Strict brevity penalty (SBP): (Chiang et al., 2008) proposed this penalty. Let ti be the transla72 tion of input sentence i, and let ri be its reference (or if there is more than one, the reference whose length in words |ri |is closest to length |ti |). Set  ∑i |ri | SBP = exp1 −  ∑ min{ |ti |, |ri |}  i   (11) Strict redundancy penalty (SRP): long sentences are preferred by recall. Since we rely on both recall and precision to compute the score, it is necessary to punish the sentences that are too long.  ∑ max{ |ti |, |ri |}   SRP = exp1 − i   | r | ∑i i   (12) Character-based strict brevity penalty (CSBP) and Character-based"
W11-2105,W10-1751,0,0.0211502,"s system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (exce"
W11-2105,W10-1753,0,0.0605815,"Missing"
W11-2105,D10-1092,0,0.035676,"s the reference. To make AMBER more portable across all Indo-European languages, we use short words (those with fewer than 4 characters) to approximate the stop words. SWDP = exp(− |a−b| ) # unigram(r ) (15) where a and b are the number of short words in the translation and reference respectively. Long word difference penalty (LWDP): is defined similarly to SWDP. LWDP = exp(− |c−d | ) # unigram(r ) (15) where c and d are the number of long words (those longer than 3 characters) in the translation and reference respectively. Normalized Spearman’s correlation penalty (NSCP): we adopt this from (Isozaki et al., 2010). This penalty evaluates similarity in word order between the translation and reference. We first determine word correspondences between the translation and reference; then, we rank words by their position in the sentences. Finally, we compute Spearman’s correlation between the ranks of the n words common to the translation and reference. ρ =1− ∑d i 2 i (n + 1)n(n − 1) (16) where di indicates the distance between the ranks of the i-th element. For example: T: Bob reading book likes R: Bob likes reading book The rank vector of the reference is [1, 2, 3, 4], while the translation rank vector is"
W11-2105,W04-1013,0,0.0117415,"weight each n-gram according to its information value. 2.4 Four matching strategies In the original BLEU metric, there is only one matching strategy: n-gram matching. In AMBER, we provide four matching strategies (the best AMBER variant used three of these): 1. N-gram matching: involved in computing precision and recall. 2. Fixed-gap n-gram: the size of the gap between words “word1 [] word2” is fixed; involved in computing precision only. 3. Flexible-gap n-gram: the size of the gap between words “word1 * word2” is flexible; involved in computing precision only. 4. Skip n-gram: as used ROUGE (Lin, 2004); involved in computing precision only. 2.5 Input preprocessing The AMBER score can be computed with different types of preprocessing. When using more than one type, we computed the final score as an average over runs, one run per type (our default AMBER variant used three of the preprocessing types): Final _ AMBER = 1 T ∑ AMBER(t ) T t =1 We provide 8 types of possible text input: 0. Original - true-cased and untokenized. 1. Normalized - tokenized and lower-cased. (All variants 2-7 below also tokenized and lower-cased.) 2. “Stemmed” - each word only keeps its first 4 letters. 3. “Suffixed” -"
W11-2105,W10-1754,0,0.490409,"shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important wh"
W11-2105,P09-1034,0,0.0239911,"development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a metric to tune an MT system. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from"
W11-2105,P02-1040,0,0.108716,"ation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several o"
W11-2105,2006.amta-papers.25,0,0.171115,"t incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment"
W11-2105,W09-0441,0,0.0621069,"ce-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisio"
W11-2105,W07-0734,0,\N,Missing
W12-3104,W08-0309,0,0.0671104,"Missing"
W12-3104,P12-1098,1,0.790951,"0 and p20 = 0 . Let Then we obtain nd +1+1+9 ) Its ρ = 1 - 6(14(16 −1) = −0.2 ; however, its th P1: 1 2 3 4 5 6 (the 2 word “the”, 4 word “of” and 6th word “,” in the reference are not aligned to any word in the hypothesis. Thus, their positions are not in P1, so the positions of the matching words “in winter 2010 I visited Paris” are normalized to 1 2 3 4 5 6) P2: 4 5 6 1 3 2 (the word “’s” was unaligned). 60 v2 = 1 − DIST2 ( P1 , P2 ) n2 −1 (7) As with v1, v2 is also from 0 to 1, and larger values indicate more similar permutations. The ordering measure vs is the harmonic mean of v1 and v2 (Chen et al., 2012): (8) vs = 2 / (1/v1 + 1/v2 ) . In (Chen et al., 2012) we found this to be slightly more effective than the geometric mean. vs in (8) is computed at segment level. We compute document level ordering vD with a weighted arithmetic mean: l vD ∑ v × len ( R) = ∑ len ( R) s =1 s l s =1 s (9) s where l is the number of segments of the document, and len(R) is the length of the reference after text preprocessing. vs is the segment-level ordering penalty. Recall that the penalty part of AMBER is the weighted product of several component penalties. In the original version of AMBER, there were 10 compone"
W12-3104,W11-2105,1,0.899965,"o AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 1 2 Introduction AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011). It is designed to have the advantages of BLEU (Papineni et al., 2002), such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment. According to the paper just cited: “It can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality”. Many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to BLEU, such as external lexical and syntactic resources. Unlike these and like BLEU, AMBER relies"
W12-3104,D10-1092,0,0.01535,"s. The first is absolute permutation distance: n DIST1 ( P1 , P2 ) = ∑ |p1i − p2i | (4) i =1 Let DIST1 ( P1 , P2 ) n( n + 1) / 2 ν1 = 1 − (5) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reordering too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this example: Ref: Recently , I visited Paris Hyp: I visited Paris recently P1: 1 2 3 4 P2: 2 3 4 1 3.1 Ordering penalty v We use a simple matching algorithm (Isozaki et al., 2010) to do 1-1 word alignment between the hypothesis and the reference. After word alignment, represent the reference by a list of normalized positions of those of its words that were aligned with words in the hypothesis, and represent the hypothesis by a list of positions for the corresponding words in the reference. For both lists, unaligned words are ignored. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Suppose we have +1+1+ 3 v1 = 1 - 14(4 +1)/2 = 0.4 . Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure"
W12-3104,P02-1040,0,0.0936783,"e second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 1 2 Introduction AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011). It is designed to have the advantages of BLEU (Papineni et al., 2002), such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment. According to the paper just cited: “It can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality”. Many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to BLEU, such as external lexical and syntactic resources. Unlike these and like BLEU, AMBER relies entirely on matching surface forms in tokens in the hypothesis and ref"
W12-3104,C96-2141,0,0.429494,"simple matching algorithm (Isozaki et al., 2010) to do 1-1 word alignment between the hypothesis and the reference. After word alignment, represent the reference by a list of normalized positions of those of its words that were aligned with words in the hypothesis, and represent the hypothesis by a list of positions for the corresponding words in the reference. For both lists, unaligned words are ignored. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Suppose we have +1+1+ 3 v1 = 1 - 14(4 +1)/2 = 0.4 . Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure is based on jump width. This punishes only once a sequence of words that moves a long distance with the internal word order conserved, rather than on every word. In the following, only two groups of words have moved, so the jump width punishment is light: Ref: In the winter of 2010, I visited Paris Hyp: I visited Paris in the winter of 2010 The second distance measure is n DIST2 ( P1 , P2 ) = ∑ |( p1i − p1i −1 ) − ( p2i − p2i −1 ) |(6) Ref: in the winter of 2010 , I visited Paris Hyp: I visited Paris in 2010 ’s winter i =1 0 1 where we set p = 0 and p20 = 0 . Let"
W12-3104,W10-1703,0,\N,Missing
W15-3044,2006.amta-papers.25,0,0.0917436,"ge of [0,1]. Syntax−based Fscore 1 Density Density Abstract Introduction The aims of automatic Machine Translation (MT) evaluation metrics, which measure the quality of translations against human references, are twofold. Firstly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translatio"
W15-3044,P15-2025,1,0.72825,"th multi-level and non-multi-level scoring frameworks. V =< w1 · Voh , w2 · Vwd , w3 · VgRAE , w4 · VtRAE > (10) where Voh is the one-hot representation, Vwd denotes the word representations, and VgRAE and VtRAE are representations learned with greedy RAE and tree-based RAE, respectively. The weights w1 ... w4 are tuned on development data. 3.2 Representation based metric Distributed representations for words and sentences have been shown to significantly boost the performance of a NLP system (Turian et al., 2010). A representation-based translation evaluation metric, DREEM, is introduced in (Anonymous, 2015). The metric has shown to be able to achieve state-of-the-art performance, compared to popular metrics such as BLEU and Meteor. Therefore, in this paper, we also adapt this metric for our experiments. In a nutshell, the DREEM metric evaluates translations by employing three different types of word and sentence representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with a recursive autoencoder (RAE). Two different RAEbased representations are used in this metric: one is based on a greedy u"
W15-3044,W05-0909,0,0.636809,"ly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Conside"
W15-3044,P10-1040,0,0.00916093,". Fmean = Score(t, r) = Cosα (t, r) × Plen (6) In our studies, we fine-tune all the parameters for both multi-level and non-multi-level scoring frameworks. V =< w1 · Voh , w2 · Vwd , w3 · VgRAE , w4 · VtRAE > (10) where Voh is the one-hot representation, Vwd denotes the word representations, and VgRAE and VtRAE are representations learned with greedy RAE and tree-based RAE, respectively. The weights w1 ... w4 are tuned on development data. 3.2 Representation based metric Distributed representations for words and sentences have been shown to significantly boost the performance of a NLP system (Turian et al., 2010). A representation-based translation evaluation metric, DREEM, is introduced in (Anonymous, 2015). The metric has shown to be able to achieve state-of-the-art performance, compared to popular metrics such as BLEU and Meteor. Therefore, in this paper, we also adapt this metric for our experiments. In a nutshell, the DREEM metric evaluates translations by employing three different types of word and sentence representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with a recursive autoencoder"
W15-3044,W14-3348,0,0.0318647,"f sentence-level scores, with the weights being the reference lengths, as follows. PD len(ri )Scorei Scored = i=1 (2) PD i=1 len(ri ) where Scorei is the score of sentence i, and D is the number of sentences in the document. 3 Evaluation metrics We apply our multi-level approach to two metrics. The first one is Meteor (Banerjee and Lavie, 2005), which has been widely used for machine translation evaluations. The second one is DREEM, a new metric based on distributed representations generated by deep neural networks. 3.1 Metric Meteor We use the latest version of Meteor, i.e. Meteor Universal (Denkowski and Lavie, 2014) in this paper. Meteor computes a one-to-one alignment between matching words in a translation and a reference. The space of possible alignments is constructed by exhaustively identifying all possible matches of the following types: exact word matches, word stem matches, synonym word matches, and matches between phrases listed as paraphrases. Alignment is then conducted as a beam search. From the final alignment, the translation’s Meteor score is calculated as follows. First, content DREEM with a similarity score computed with the Cosine function and a length penalty. Let the size of the vecto"
W15-3044,W13-2256,0,0.0142521,"r example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Figure 1 depicts the distributions of the two metrics’ evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstest2012.en-cs. As shown in F"
W15-3044,W05-0904,0,0.0418492,"introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Figure 1 depicts the distributions of the two metrics’ evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstes"
W15-3044,D11-1035,0,0.0155037,"systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLU"
W15-3044,P11-1023,0,0.0792917,"pineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Figure 1 depicts the distributions of the two metrics’ evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstest2012.en-cs. As shown in Figures 1, the variances of the crea"
W15-3044,W14-3336,0,0.0186478,"sk data. Development sets include WMT 2012 all-to-English, and English-to-all submissions. Test sets contain WMT 2013, and WMT 2014 all-to-English, plus 2013, 2014 Englishto-all submissions. The languages “all” include French, Spanish, German, Czech and Russian. For training the word embedding and recursive auto-encoder model, we used WMT 2014 training data 1 . We used the English, French, German and Czech sentences in “Europarl v7” and “News Commentary” for our experiments. To train the representations for Russian, we used the “Yandex 1M corpus”. 4.2 Results Following WMT 2014’s metric task (Machacek and Bojar, 2014), to measure the correlation with 1 363 http://www.statmt.org/wmt14/translation-task.html metric Original BLEU Sentence BLEU Original Meteor Sentence Meteor M ulti − levelw Meteor M ulti − levelwd Meteor DREEM M ulti − levelw DREEM M ulti − levelwd DREEM Into-English seg τ sys γ – 0.821 0.259 0.841 0.279 0.849 0.279 0.863 0.285 0.871 0.294⋆ 0.885⋆ 0.287 0.875 0.293 0.880 0.303⋆ 0.892⋆ metric Original BLEU Sentence BLEU Original Meteor Sentence Meteor M ulti − levelw Meteor DREEM M ulti − levelw DREEM Out-of-English seg τ sys γ – 0.843 0.221 0.846 0.228 0.845 0.228 0.853 0.234 0.861 0.236 0.904"
W15-3044,P02-1040,0,0.0963605,"Distributions of translation quality. Xaxis is in the range of [0,1]. Syntax−based Fscore 1 Density Density Abstract Introduction The aims of automatic Machine Translation (MT) evaluation metrics, which measure the quality of translations against human references, are twofold. Firstly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) met"
W16-2317,N12-1047,1,0.854573,"ierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a r"
W16-2317,N13-1003,1,0.836258,"). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translation"
W16-2317,P14-1129,0,0.161205,"Missing"
W16-2317,E14-4029,0,0.0478782,"Missing"
W16-2317,D11-1125,0,0.0353707,"mentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase l"
W16-2317,P07-1019,0,0.0247693,"ian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 8"
W16-2317,W10-1717,1,0.823355,"ces) and the monolingual English corpus available for the constrained news translation task, which is a combination of the Europarl v7 corpus, the NewsCommentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs a"
W16-2317,N04-1021,0,0.137306,"the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 82 features: 27 decoder features and 55 additional rescoring features. The rescoring model was tuned using nbest MIRA. Of the rescoring features, 51 consisted of various IBM features for word- and lemmaaligned IBM1, IBM2, IBM4 and HMM models, as well as various other standard length, n-gram, and n-best features. The final four features used NNJMs for rescoring, two Russian-word NNJM rescoring features and two Russian-lemma ones. Following Devlin et al. (2014), one NNJM feature rescored the 1000best list using a English-to-Russian NNJM, where the roles of th"
W16-2317,E99-1010,0,0.130019,"(Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural network joint model Distortion and sparse feature models Similar to the translation model, our hierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters o"
W16-2317,P12-1049,0,0.0701105,"Missing"
W16-2317,2014.amta-researchers.3,1,0.793782,"12 units for the single hidden layer. We train our models with mini-batch stochastic gradient descent, with a batch size of 128 words, and an initial learning rate of 0.3. We check our training objective on the development set every 20K batches, and if it fails to improve for two consecutive checks, the learning rate is halved. Training stops after 5 consecutive failed checks or after 90 checks. To enable efficient decoding, our models are self-normalized with a squared penalty on the Language models Our system consists of three n-gram language models (LMs) and two word class language models (Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural"
W16-2317,D13-1140,0,0.0283969,"xtracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. Our internal development experiments indicated that using lemma alignments improved the translation quality of a baseline phrase-based system by roughly 0.2 BLEU, and also benefited the perplexity of the bilingual neural language models described in Section 2.5 and 3.1. 2.3 2.5 We employ two neural network joint models, or NNJMs (Vaswani et al., 2013; Devlin et al., 2014). The NNJM is a feed-forward neural network language model that assumes access to a source sentence f and an aligned source index ai , which points to the most influential source word for the translation of the target word ei . The NNJM calculates the language modeling proba+m i−1 bility p(ei |ei−n+1 , faaii−m ), which accounts for the n−1 preceding target words, and for 2m+1 words of source context, centered around fai . Following Devlin et al. (2014), we use n = 4 and m = 5, resulting in 3 words of target context and 11 words of source context, effectively a 15-gram lan"
W17-4732,D11-1033,0,0.0310836,"omain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarity function for identifying sentence pairs from general-domain training corpora that are close to the target domain. We built four language models using the input and output sides of the training corpora and the development set respectively to select 3 million sentence pairs from the training corpora that are close to the news domain. However, the development set, which consists of only 1k sentence pairs, is too tiny to be a suitable corpus for building the in-domain language models that will enable the bilingual LM"
W17-4732,W16-2317,1,0.894593,"Missing"
W17-4732,W17-3205,1,0.837791,"cted by bilingual LM cross-entropy difference (xent), c) further trained with synthetic data, d) further trained with cost weighting, e) further trained with in-domain data selected by semi-supervised convolutional neural network classifier (sscnn), f) greedy model averaging and g) optimized against sentence-level BLEU on the intersection of the subsets of data selected by xent and sscnn using MRT. 3.2 Data selection and domain adaptation lion sentence pairs from the training corpora that are close to the news domain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarit"
W17-4732,K16-1031,1,0.92652,"-English (out of twenty participants) in WMT 2017 human evaluation. 1 George Foster∗ Work performed while at NRC. 330 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 330–337 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics the one described in Section 2.1,1 and then employed the machine-translated Russian and perfect English sentence pairs as additional data to train the Russian-English MT system. To select sentences for back-translation, we used a semi-supervised convolutional neural network classifier (Chen and Huang, 2016). We sampled two million sentences from the English monolingual News Crawl 2015 & 2016 corpora according to their classifier scores, which reflect their similarity to the the English half of our development set. formance (third place in both language pairs) in the preliminary automatic evaluation of WMT 2017. In this paper, we discuss the lessons learned in building large-scale state-of-the-art NMT systems. 2 Russian-English news translation We used all the Russian-English parallel corpora available for the constrained news translation task. They include the CommonCrawl corpus, the NewsComment"
W17-4732,N04-1021,0,0.0967458,"lt is rather disappointing by comparison with the exciting improvement reported in Sennrich et al. (2016a), i.e. 3-4 BLEU. Another disappointing result is that model averaging does not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take"
W17-4732,N12-1047,1,0.741262,"s not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take advantage of the rescoring framework to have our NNJMs view each candidate translation from 332 Figure 1: Russian-English learning curve on development set in cased BLEU of selected model"
W17-4732,P14-1129,0,0.0771307,"Missing"
W17-4732,E17-3017,0,0.0554434,"Missing"
W17-4732,P16-1009,0,0.450161,"inflections, since they play an important role in disambiguating the meaning of sentences. Chinese does not have clear word boundaries. The number of Chinese word types created by automatic word segmentation software is high, while naive character segmentation would result in a skewed Chinese to English sentence length ratio. These characteristics make it difficult for machine translation systems to learn the correct association between words in Chinese and English. Since this was the first time we deployed NMT models in an evaluation, we first tried to replicate the results of previous work (Sennrich et al., 2016a). Our NMT systems are based on Nematus (Sennrich et al., 2017). We used automatic back-translation (Sennrich et al., 2016b) of a subselected monolingual News corpus as additional training data, and all the training data is segmented into subword units using BPE (Sennrich et al., 2016c). We also experimented with pervasive dropout as implemented in Nematus. For Russian-English, our WMT16 PBMT system scored higher than all the NMT systems we built this year. We therefore experimented with using the NMT systems as features for rescoring the 1000-best output from our WMT16 PBMT system. This stra"
W17-4732,W16-2316,0,0.0118976,"and embedding layers to 0.15. For the hidden layers, we set the dropout probability to 0.3. NMT baseline system Our NMT baseline system is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A"
W17-4732,P16-1162,0,0.103844,"Missing"
W17-4732,P16-1159,0,0.0343062,"ystem is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A common practice for avoiding overfitting to the training data is ensembling the last few models saved as checkpoints. Rec"
W17-4732,W03-1730,0,0.0742719,"We used all the Chinese-English parallel corpora available for the constrained news translation task. They include the UN corpus, the NewsCommentary v12 corpus and the CWMT corpus. In total, 25 million parallel Chinese-English sentences were used to train the baseline system. We used half of the WMT 17 news translation development set as our development set and the other half as internal test set. The English texts in the training/development/test corpora were tokenized and lowercased while the Chinese texts in the training/development/test corpora were segmented using the ICTCLAS segmenter (Zhang et al., 2003). Then the Chinese and English text were combined to train a BPE model with vocabulary size of 90k. Although in figure 1 we see that none of the NMT systems manage to beat our WMT16 PBMT submission, the more interesting result is that there is more than 1.8 BLEU gain on the development set and 1.1 BLEU gain on the test set by rescoring the PBMT 1000-best list using just one of our NMT systems and no other features, as in line (g). The final rescoring with weighted collections of NMT systems, language model features, NNJM features and n-best features shows 1.8 BLEU improvement over the WMT 16 s"
