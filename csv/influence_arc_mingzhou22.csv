2003.mtsummit-papers.23,C00-2090,0,0.121507,"y example, they have the opportunity to be combined to one chunk. We express the procedure of getting ~ s &apos;1K &apos; → ~ s1K |E) . Pr(s1K |~ s &apos;1K &apos; , E) with function F (~ Consider the data sparseness problem, submit to losing some of linguistic constraints, we get a back off function L(~s &apos;1K&apos; → ~s1K |E) which is based on a weighted length to substitute the function F. We have: (2) example based chunk combination In base chunking, we follow the SL monolingual chunking model that is introduced by Wang et al. (2002), in which 13 chunk types were used for English, which is the same with Erik et al. (2000). ~ t1 K and its example set e with equation (3). The ~ s K and the best example set best translation tˆ K of ~ 1 1 ê for the translation should gain the highest Pr(e, ~ t1 K |~ s1K , e’). Pr(e, ~ t1 K |~ s1K , e &apos; ) = ∏ Pr(e~sk |~ sk , e &apos; ) ⋅ Pr(~ tk |~ s , e~sk ) 424 4 3 14 42k44 3 k 14 exampleselection model translation model (3) There are an example selection model and a phrase-based translation model in the example-based translation model. The distortion model is not adopted because our model is for a TMS and only partial translation will be provided to users. Example selection model Giv"
2003.mtsummit-papers.23,2002.tmi-tutorials.1,0,0.0310936,"Missing"
2011.mtsummit-papers.14,J96-1002,0,0.0526965,"Missing"
2011.mtsummit-papers.14,W09-0436,0,0.0226187,"Missing"
2011.mtsummit-papers.14,J07-2003,0,0.0233923,"reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase translation table, where all the transla"
2011.mtsummit-papers.14,D09-1024,0,0.0211603,"tion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a s"
2011.mtsummit-papers.14,W04-3250,0,0.0218324,"el data with the Xinhua portion of LDC English Gigaword, where no target function words are deleted. The LM is integrated into the SMT decoder rather than used in a post reranking step. In addition, a CRF-based POS tagger is trained over Penn Treebank to label the target portion of bilingual data. The development data is NIST 2003 data set and the test data comes from NIST 2005 and NIST 2006 evaluation data set. The caseinsensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric, where statistical signiﬁcance test is performed using the bootstrap re-sampling method proposed by (Koehn, 2004). 4.2 Words Accuracy of TFWIM Following (Setiawan et al., 2009), the selection of the target function words is based on their frequencies in the training corpus. Although our method can be applied to the generation of any number of distinct function words, in our experiments we mainly focus on ﬁve typical target function words contained in the set W ={”the”, ”of”, ”to”, ”in”, ”for”}. They are the most frequent target function words that are unaligned (i.e., aligned to NULL) in word alignment. Table 5 shows their statistical information. For convenience, each setting of TFWIM is de144 Table 5:"
2011.mtsummit-papers.14,N03-1017,0,0.017962,"odel for SMT, which can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase transl"
2011.mtsummit-papers.14,W08-0301,1,0.923541,"everaging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function"
2011.mtsummit-papers.14,D08-1077,0,0.0845194,"t improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function words are more ﬂexible than measure words, the generation of function words faces more challenges. In addition, (Menezes and Quirk, 2008) introduced an extension approach to the syntactic-based SMT system that allows structural word insertion and deletion. The effectiveness of these methods motivates us to address the generation of target function words in the phrasebased SMT which is a popular system in both academic and industrial areas. 141 3 Our Method Our method focuses on the processing of target function words, including the deletion and the insertion. The deletion takes place during the model training, where the target function words are removed from the training data before conducting translation modeling. The insertio"
2011.mtsummit-papers.14,P03-1021,0,0.0381847,"Missing"
2011.mtsummit-papers.14,J04-4002,0,0.0445212,"can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase translation table, where"
2011.mtsummit-papers.14,P02-1040,0,0.0820753,"alized reordering model comes from LDC2003E14, which contains 128K sentence pairs. A 5-gram language model (LM) is trained over the English portion of parallel data with the Xinhua portion of LDC English Gigaword, where no target function words are deleted. The LM is integrated into the SMT decoder rather than used in a post reranking step. In addition, a CRF-based POS tagger is trained over Penn Treebank to label the target portion of bilingual data. The development data is NIST 2003 data set and the test data comes from NIST 2005 and NIST 2006 evaluation data set. The caseinsensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric, where statistical signiﬁcance test is performed using the bootstrap re-sampling method proposed by (Koehn, 2004). 4.2 Words Accuracy of TFWIM Following (Setiawan et al., 2009), the selection of the target function words is based on their frequencies in the training corpus. Although our method can be applied to the generation of any number of distinct function words, in our experiments we mainly focus on ﬁve typical target function words contained in the set W ={”the”, ”of”, ”to”, ”in”, ”for”}. They are the most frequent target function words that are unaligne"
2011.mtsummit-papers.14,P07-1090,0,0.0177966,"anwhile, the second step is aim to recover those function words to make translation results more ﬂuent. Intuitively, it is expected that the correct insertion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English tr"
2011.mtsummit-papers.14,P09-1037,0,0.0521647,"p is aim to recover those function words to make translation results more ﬂuent. Intuitively, it is expected that the correct insertion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang e"
2011.mtsummit-papers.14,J97-3002,0,0.242662,"Missing"
2011.mtsummit-papers.14,P06-1066,0,0.0601275,"Missing"
2011.mtsummit-papers.14,P08-1011,1,0.912537,", 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function words are more ﬂexible than measure words, the generation of function words faces more challenges. In addition, (Menezes and Quirk, 2008) introduced an extension approach to the syntact"
2011.mtsummit-papers.19,P10-1147,0,0.0155607,"zed for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system performance. Huang and Xiang (2010) further proposed the use of annotated alignment results to extract annotated translation pairs which are used as training samples for discriminative model training. DeNero and Klein (2010) also used annotated alignment results in a similar way using MIRA training with precision and recall of the translation pairs as training objective (not BLEU-oriented). Such models may still suffer from over-fitting, as the features used may not be powerful enough to separate highly probable translation pairs from unlikely ones. yu bei han you bangjiao (a) have diplomatic relation with North Korea (b) (c) yu bei han have diplomatic relation you bangjiao with North Korea 0.05 yu X1 you X2 have X2 with X1 0.5 bei han North Korea 0.4 bangjiao diplomatic relation 0.1 0.02 Figure 1. An example for"
2011.mtsummit-papers.19,P08-1010,0,0.124702,"sed a phrase-based, joint probability model with EM training to do direct phrase alignment of training data. Since this method suffers from over-fitting, subsequent researchers introduced prior into the generative process: Blunsom et al. (2008) and DeNero et al. (2008) proposed to use dirichlet processing to do structure alignment with sampling training. However, it is still very difficult to integrate various 182 kinds of features, and the model cannot be optimized for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system performance. Huang and Xiang (2010) further proposed the use of annotated alignment results to extract annotated translation pairs which are used as training samples for discriminative model training. DeNero and Klein (2010) also used annotated alignment results in a similar way using MIRA training with precision and recall of the translation pairs a"
2011.mtsummit-papers.19,C10-1056,0,0.0166382,"to use dirichlet processing to do structure alignment with sampling training. However, it is still very difficult to integrate various 182 kinds of features, and the model cannot be optimized for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system performance. Huang and Xiang (2010) further proposed the use of annotated alignment results to extract annotated translation pairs which are used as training samples for discriminative model training. DeNero and Klein (2010) also used annotated alignment results in a similar way using MIRA training with precision and recall of the translation pairs as training objective (not BLEU-oriented). Such models may still suffer from over-fitting, as the features used may not be powerful enough to separate highly probable translation pairs from unlikely ones. yu bei han you bangjiao (a) have diplomatic relation with North Korea (b) (c) y"
2011.mtsummit-papers.19,W04-3250,0,0.0662409,"1O improves the performance on Nist’08 but not significantly on Nist’06. Since both UTMPERC and UTMMIRA get almost the same performance on training data, and UTMMIRA outperforms UTMPERC on the two test datasets a lot, it is confirmed that UTMMIRA is better in generalization capacity 4 (to be deeply analyzed in the next section). UTMOWL with logistic loss is not very good on Nist’06, while it significantly improves the performance on Nist’08. UTMMIRA gets larger improvement than other methods. Note that the BLEU differences between UTMMIRA and the three baselines are statistically significant (Koehn, 2004). Compared with PDM, UTMMIRA achieves a better improve- 7 Conclusion ment, and also the training time of our method (34 In order to handle the inconsistence between transhours, for UTMMIRA, UTMPERC and UTMPERC)5 is lation modeling and decoding, we propose a new 6 much shorter than that of PDM (119 hours) . discriminative translation sub-model, which is optimized with the same criterion as the translation 4 output is evaluated (BLEU). The new translation This observation is contrary to that in Arun and Koehn sub-model uses all translation pairs in the transla(2007), which shows perceptron and"
2011.mtsummit-papers.19,P06-1096,0,0.173483,"ation. Moreover, this method of translation modeling suffers from over-fitting which is critical especially when test data is not similar to training data. In this paper, we propose a new unified framework to add a discriminative translation sub-model into the conventional linear framework, and the sub-model is optimized with the same criterion as the translation output is evaluated (BLEU in our case). Similar to Blunsom et al. (2009), each translation pair is a feature in itself, and the training method can affect the pairs directly so as to handle over-fitting. Unlike any previous approach(Liang et al., 2006; Arun and Koehn, 2007; Chiang et al., 2008), in which the weights of translation pair features and those features of sub-models are tuned in the same process, we propose a new scalable submodel which integrates all the translation pairs, and then combine the new sub-model with other sub-models into the conventional framework. The over-fitting problem of the new scalable sub-model is well tackled by MIRA. The scalable training (MIRA) of the new sub-model and the standard training (MERT) of conventional framework are unified into an interactive training process. In the following, the previous a"
2011.mtsummit-papers.19,P05-1012,0,0.152347,"Missing"
2011.mtsummit-papers.19,D08-1023,0,0.0419415,"Missing"
2011.mtsummit-papers.19,J03-1002,0,0.00412002,"tion performance is evaluated by case-insensitive BLEU4. Our decoder is a state-of-the-art implementation of hierarchical phrase-based model (HPM) (Chiang, 2007) with standard features, including language sub-model, translation sub-model, etc. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. FBIS newswire corpus is our training data, which is used to extract translation pairs and train the scalable feature weights ݓ in equation (2). The translation pairs are extracted as in Chiang (2007) from word alignment matrixes, which are generated by running GIZA++ (Och and Ney, 2003) in two directions, and then symmetrized using the grow-diag-final method (Koehn et al., 2003). The idea of unification of MERT and MIRA in UTM is evaluated against PDM, which uses the same features as in UTM. The generalization capacity of UTM using MIRA (UTMMIRA) is evaluated against three baselines, viz. leaving-oneout (L1O), UTM using Perceptron (UTMPERC) and UTM using OWL-QN (UTMOWL). The NIST’05 test set is used as our development dataset to tune the sub-model weights ߣ in equation (1) and the NIST’06 and NIST’08 test sets are used as our test sets. 6.1 Explanatory Capacity As mentioned"
2011.mtsummit-papers.19,P09-1088,0,0.0136108,"the translation pairs are usually assigned by maximum likelihood estimation (MLE). That is, conventional translation modeling never takes translation evaluation metric into consideration. Moreover, this method of translation modeling suffers from over-fitting which is critical especially when test data is not similar to training data. In this paper, we propose a new unified framework to add a discriminative translation sub-model into the conventional linear framework, and the sub-model is optimized with the same criterion as the translation output is evaluated (BLEU in our case). Similar to Blunsom et al. (2009), each translation pair is a feature in itself, and the training method can affect the pairs directly so as to handle over-fitting. Unlike any previous approach(Liang et al., 2006; Arun and Koehn, 2007; Chiang et al., 2008), in which the weights of translation pair features and those features of sub-models are tuned in the same process, we propose a new scalable submodel which integrates all the translation pairs, and then combine the new sub-model with other sub-models into the conventional framework. The over-fitting problem of the new scalable sub-model is well tackled by MIRA. The scalable"
2011.mtsummit-papers.19,P03-1021,0,0.0181234,"pectively. Experiment result and analysis are given in Section 6. 2 Translation Modeling The task of translation modeling can be defined as, given a bilingual training corpus, the search of the optimal set of translation pairs with two goals: 1) Explanatory capacity: i.e. the training data can be fully represented by the translation pairs. 2) Generalization capacity: i.e. the translation pairs can also predict the correct translation given unseen source input. In other words, the translation pairs can avoid over-fitting. The conventional approach to translation modeling comprises three steps (Och, 2003): Firstly, the sentence pairs in training corpus are aligned at word level. Secondly, translation pairs are extracted using a heuristic method. Lastly, MLE is used to compute translation probabilities. There are a few shortcomings of this method: 1) Inconsistent format of translation knowledge: word alignment in training vs. translation pairs (phrase pairs) in decoding. 2) The training process is not oriented towards translation evaluation metric: BLEU is not considered in the scoring of translation pairs. 3) This method may cause over-fitting: it is not considered whether the phrases are extr"
2011.mtsummit-papers.19,J07-2003,0,0.0601155,"o ܰܧܩሺܨሻ, so that more feature weights could be updated. We adopt the oracle ranking method to rank ܰܧܩሺܨሻ, and the top half candidates in ܰܧܩሺܨሻare used as the positive samples and the left as the negative samples (line 5). At last, we use OWL-QN to optimize ݓusing the positive and negative samples (line 6). 6 Experiment The experiments evaluate the performance of our model and training methods in a Chinese-English setting. Translation performance is evaluated by case-insensitive BLEU4. Our decoder is a state-of-the-art implementation of hierarchical phrase-based model (HPM) (Chiang, 2007) with standard features, including language sub-model, translation sub-model, etc. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. FBIS newswire corpus is our training data, which is used to extract translation pairs and train the scalable feature weights ݓ in equation (2). The translation pairs are extracted as in Chiang (2007) from word alignment matrixes, which are generated by running GIZA++ (Och and Ney, 2003) in two directions, and then symmetrized using the grow-diag-final method (Koehn et al., 2003). The idea of unification of MERT and MIRA in UTM"
2011.mtsummit-papers.19,D08-1024,0,0.393931,"n modeling suffers from over-fitting which is critical especially when test data is not similar to training data. In this paper, we propose a new unified framework to add a discriminative translation sub-model into the conventional linear framework, and the sub-model is optimized with the same criterion as the translation output is evaluated (BLEU in our case). Similar to Blunsom et al. (2009), each translation pair is a feature in itself, and the training method can affect the pairs directly so as to handle over-fitting. Unlike any previous approach(Liang et al., 2006; Arun and Koehn, 2007; Chiang et al., 2008), in which the weights of translation pair features and those features of sub-models are tuned in the same process, we propose a new scalable submodel which integrates all the translation pairs, and then combine the new sub-model with other sub-models into the conventional framework. The over-fitting problem of the new scalable sub-model is well tackled by MIRA. The scalable training (MIRA) of the new sub-model and the standard training (MERT) of conventional framework are unified into an interactive training process. In the following, the previous approaches to translation modeling are review"
2011.mtsummit-papers.19,W02-1001,0,0.0483533,"ussed in section 5. MERT and scalable training are unified into an interactive training process, as shown in Figure 2. Algorithm Interactive Training INPUT: Training dataሺܨǡ ܧሻ; Develop dataሺܨௗ ǡ ܧௗ ሻ OUTPUT: ߣ and ݓ 1: while(BLEU onሺܨௗ ǡ ܧௗ ሻ is improved) 2: fix ߣ, and train  ݓusing scalable method onሺܨǡ ܧሻ 3: fix ݓ, and train ߣ using MERT on ሺܨௗ ǡ ܧௗ ሻ 4: Return ߣ and ݓ Perceptron (Rosenblatt, 1962) is an incremental training procedure (i.e. stochastic approximation) which optimizes a minimum square error (MSE) loss function. Here, we use the average perceptron (Collins, 2002) for our scalable training, which is shown to be more effective than the standard one. The update rule on an example ሺ ǡ  ሻ is:  ݓ՚  ݓ  ᇱ ሺ ǡ  ሻ െ  ᇱ ሺ ǡ  ሻ Figure 2. Training method for UTM Given initial ߣ and ݓ, we first fix ߣ and train ݓ using scalable method on training data ሺܨǡ ܧሻ, then we fix the new  ݓand update ߣ using MERT on development data ሺܨௗ ǡ ܧௗ ሻ . MERT and scalable training continue interactively until translation performance is not improved on development data ሺܨௗ ǡ ܧௗ ሻ. We use 0 as initial  ݓand the trained submodel weights (using"
2011.mtsummit-papers.19,D08-1033,0,0.0185821,"ess is not oriented towards translation evaluation metric: BLEU is not considered in the scoring of translation pairs. 3) This method may cause over-fitting: it is not considered whether the phrases are extracted from a highly probable phrase alignment or from an unlikely one. To unify the format of translation knowledge, Marcu and Wong (2002) proposed a phrase-based, joint probability model with EM training to do direct phrase alignment of training data. Since this method suffers from over-fitting, subsequent researchers introduced prior into the generative process: Blunsom et al. (2008) and DeNero et al. (2008) proposed to use dirichlet processing to do structure alignment with sampling training. However, it is still very difficult to integrate various 182 kinds of features, and the model cannot be optimized for SMT performance directly. To integrate various kinds of features and optimize translation sub-model regarding BLEU directly, Deng et al. (2008) proposed a discriminative phrase extraction method with datadriven features to capture the quality and confidence of phrases and phrase pairs. The feature weights are optimized jointly with the translation engine to maximize the end-to-end system per"
2011.mtsummit-papers.19,2005.mtsummit-papers.33,0,0.103624,"Missing"
2011.mtsummit-papers.19,P10-1049,0,0.0148023,"ion to over-fitting for nearly all discriminative translation modeling approaches is the length constraint to the source and/or target side of translation pairs. That is, the long translation pairs, which have weak generalization capacity, are simply filtered away. This solution, however, also discards long but useful phrases like &quot;I would like to have&quot;. Blunsom et al. (2008) used a discriminative latent variable model with each translation pair as a feature and a  ʹܮregularization to deal with overfitting, and in order to incorporate language model, sampling method is adopted for training. Wuebker et al. (2010) used leaving-one-out (L1O) to deal with over-fitting and forced alignment to deal with the errors introduced by incorrect word alignment. The basic idea is to use the trained SMT decoder to re-decode the training data, and then use the decoded result to re-calculate translation pair probabilities. Since the correct target sentence (i.e. the target side of training data) is not guaranteed to be generated by SMT decoder, forced alignment is used to generate the correct target sentence by discarding all phrase translation candidates which do not match any sequence in the correct target sentence."
2011.mtsummit-papers.19,P08-1012,0,0.033463,"Missing"
2011.mtsummit-papers.19,J93-2003,0,\N,Missing
2011.mtsummit-papers.20,P08-1023,0,0.0201031,"MBR definition, where  ܯis a constant large enough. We define ܩሺߩǡ ߩᇱ ሻ as the similarity measure between two hypotheses ߩ and ߩᇱ . In this sense, ܵ ሺߩሻ can be viewed as the expected similarity between ߩ and all hypotheses in ሺሻ. W formulate ܩሺߩǡ ߩᇱ ሻ as a weighted combination of a set of similarity features: Step 1: Baseline Phrase Extraction ܩሺߩǡ ߩᇱ ሻ ൌ  ߣ ߠ ሺߩǡ ߩᇱ ሻ In this step, all potential phrase pairs that are consistent with word alignments are extracted from a given training corpus ࣝ using the standard phrase extraction method. Furthermore, inspired by several studies (Mi et al., 2008; Dyer et al., ሺͳሻ ఘᇲ אሺሻ ሺʹሻ  where ߠ is the  th feature with its weight ߣ . x 1 Given a source phrase  and a target phrase , the phrase pair ሺǡ ሻ is said to be consistent with word alignment if and only if: (1) at least one word in one phrase is aligned to one word in the other phrase; (2) no words in one phrase can be aligned to a word outside the other phrase. ܲሺߩȁሻ is the hypothesis distribution over all hypotheses contained in ሺሻ: ܲሺߩȁሻ ൌ 190 σሺாǡிሻ ࣝאσࣛ ߜሺࣛǡாǡிሻ ሺߩሻܲሺࣛȁܧǡ ܨሻ σఘᇲ אுሺሻ σሺாǡிሻ ࣝאσࣛ ߜሺࣛǡாǡிሻ ሺߩᇱ ሻܲሺࣛȁܧǡ ܨሻ where ߜሺࣛǡாǡிሻ ሺߩሻ equals to 1 when ߩ"
2011.mtsummit-papers.20,P08-1010,0,0.0501013,"Missing"
2011.mtsummit-papers.20,W04-3243,0,0.205854,"Missing"
2011.mtsummit-papers.20,P08-1115,0,0.0374457,"Missing"
2011.mtsummit-papers.20,E99-1010,0,0.0253761,"BR model, and use them as extra phrasal features. In our experimental part, we will show that besides alignment pruning, using similarity scores as additional features can provide further improvements as well. When using n-best alignment results instead of 1-best ones, translation probabilities and lexical weights are estimated based on fractional counts instead of absolute frequencies of phrases. 3 alignment-based features 3) ߠௐଶ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s class) link pairs in ߩ co-occur in ߩᇱ . Word clusters are obtained by using mkcls toolkit (Och, 1999) that trains word classes based on the maximum-likelihood criterion. The total numbers of word classes are set to be 80 for both Chinese and English. One question may be asked is the reason that we remove all alignment links from phrase pairs of relative low MBR scores. In fact, although all alignment links are not necessarily bad in those low-quality phrase pairs, we just remove all of them from training corpus for convenience. By varying different values of ݐ, we can empirically find an optimal setting, where alignment pruning can bring benefits for final translation quality. 2.4 x Similar"
2011.mtsummit-papers.20,P06-1097,0,0.0229039,"rs. Because of using MBR-inspired techniques, we also investigate the impacts of our method on MBR decoding over translation hypergraphs of the baseline system. We re-implement an MBR decoder (Kumar et al., 2009) that uses the linear BLEU score as its loss function. 4.3 Word Aligner Although discriminative methods have already shown comparable word alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large scale corpus. As a result, we evaluate our approach based on two different word aligners. x Disc-Aligner. A discriminative word aligner (Fraser and Marcu, 2006) is re-implemented to predict alignments for the training corpus. A data set of 491 sentence pairs with human annotated word alignments is used to tune model parameters. Disc-Aligner can produce n-best alignment alternatives. x GIZA-Aligner: An unsupervised word aligner GIZA++ (Och and Ney, 2003) is used with the default parameters. In this paper, we only use its Viterbi (1-best) alignment outputs. 4.4 Baseline Phrase Extraction Method The standard phrase extraction method (Base-PE) proposed by Och and Ney (2004) is utilized to generate the baseline phrase table. The length limitations are set"
2011.mtsummit-papers.20,P03-1021,0,0.0600191,"re peak, while Ͳ  ߙ  ͳ makes the distribution more uniform. Due to the fact that varying ߙ to modify the entropy of the alignment distribution doesn’t have consistent impacts on translation quality (Venugopal et al., 2008), in this paper we just fix this value to be 1.0. ሼܵ ሺߩǡ ሺሻሽ maintained for each phase pair as additional phrasal features; then, we use this phrase table in our log-linear SMT system and optimize the weights of these similarity scores together with the weights of original SMT model features to maximize BLEU on development data set using the MERT algorithm proposed by Och (2003) 3 ; last, we collect the well-tuned feature weights ሼߣ ሽ and ሼߣ ሽ and compute ܵ ሺߩሻ and ܵ ሺߩሻ for each ߩ based on Equation (3) and (4). Algorithm 1: MBR Phrase Scoring 1: 2: 3: 4: 5: 6: 7: 8 We rewrite Equation (1) by replacing ܩሺߩǡ ߩᇱ ሻ using Equation (2) as: ܵ ሺߩሻ ൌ  ߣ ሼ  ߠ ሺߩǡ ߩᇱ ሻܲሺߩᇱ ȁሻሽ ఘᇲ אሺሻ  9: 10: 11: 12: 13: 14: 15: 16: 17: 18: ሺ͵ሻ ൌ  ߣ ܵ ሺߩǡ ሺሻሻ  ܵ ൫ߩǡ ሺሻ൯ ൌ σఘᇲ אሺሻ ߠ ሺߩǡ ߩᇱ ሻܲሺߩᇱ ȁሻ is defined as the expected value of the  th similarity feature ߠ for ߩ based on the entire ሺሻ. We then consider scoring phrase pairs based on their target phra"
2011.mtsummit-papers.20,J07-3002,0,0.0249927,"Missing"
2011.mtsummit-papers.20,J04-4002,0,0.157296,"in the parallel data; last, a new phrase table is learned from the link-pruned parallel data and used in SMT decoding. We evaluate our approach on the NIST Chinese-to-English MT tasks, and show significant improvements on parallel data sets of different scales. 1 ≒ᕩ ≒ᕩ hydrogen bomb bomb hydrogen bombs (a) (b) ≒ᕩ ≒ᕩ was (c) detonated against (d) Figure 1: Phrase pairs extracted from different bilingual sentence pairs with the same source phrases, in which dashed lines denote wrong alignment links. Introduction Bilingual phrases are the fundamental building blocks for phrase-based SMT systems (Och and Ney, 2004; Koehn et al., 2004a; Chiang, 2005), and their abilities to handle local reorderings and translation ambiguity as well as many-to-many word translations are key factors to the success of phrasal SMT models. The common practice of extracting bilingual phrases from the parallel data usually consists of three steps: first, words in bilingual sentence pairs are aligned using state-of-the-art automatic word alignment tools, such as GIZA++ (Och and Ney, 2003), in both directions; second, word alignment links are refined using heuristics, such as Grow-Diagonal-Final (GDF) method; third, bilingual ph"
2011.mtsummit-papers.20,C10-1056,0,0.218567,"Missing"
2011.mtsummit-papers.20,P02-1040,0,0.0824251,"investigate the impacts of different parameter settings on this corpus; the second data set includes the following data sets, LDC2003E07, LDC2003E14, LDC2005T06 and LDC2005T10 with 354K sentence pairs contained after pre-processing. We confirm the effectiveness of our method on it using the optimal parameter setting determined on the first data set. We train a 5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100"
2011.mtsummit-papers.20,P09-1104,0,0.0455259,"Missing"
2011.mtsummit-papers.20,W96-0213,0,0.212647,"ܽǡ ǡ ሽ do find ܵመ ሺߩሻ ൌ ݔܽܯఘᇲאሺሻ ሼܵ ሺߩᇱ ሻሽ from ሺሻ find ܵመ ሺߩሻ ൌ ݔܽܯఘᇲאሺሻ ሼܵ ሺߩᇱ ሻሽ from ሺሻ ܵ௫ ሺߩሻ ൌ ܵመ ሺߩሻ ܵ כመ ሺߩሻ if ܵ ሺߩሻ ܵ כ ሺߩሻൟ ൏ ሼܵ௫ ሺߩሻ  ݐ כሽ then prune all alignment links contained in ߩ from the positions they were extracted in ࣝ end if end for return ࣝ with link-pruned word alignments 1) ߠௐଶௐ ሺߩǡ ߩᇱ ሻ . A feature that counts how many (source word)-to-(target word) link pairs in ߩ co-occur in ߩᇱ . 2) ߠௐଶ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s POS) link pairs in ߩ co-occur in ߩᇱ . Two MaxEnt-based POS taggers (Ratnaparkhi, 1996) are used to tag Chinese and English words contained in the bilingual corpus respectively. 4) ߠௐଶௌ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s stem) link pairs in ߩ co-occur in ߩᇱ . A stem dictionary that contains 22,660 entries is used to convert English words into their stem forms. We consider the stem for each Chinese word as the Chinese word itself. 5) ߠி௧ ሺߩǡ ߩᇱ ሻ . A feature that reflects the agreement on word fertilities for ߩ and ߩᇱ : Step 4: Phrase Re-Extraction Last, we re-extract bilingual phrases based on the link-pruned training corpus to learn a new"
2011.mtsummit-papers.20,P09-2031,0,0.0430818,"Missing"
2011.mtsummit-papers.20,D08-1065,0,0.0163685,"ሺߩǡ ߩᇱ ሻ. A feature that counts how many ngrams in ߩ௧ co-occur in ߩ௧ ᇱ : 192 4.2 ߠ ሺߩǡ ߩᇱ ሻ ൌ  ఠ ሺߩ௧ ሻߜఘᇲ  ሺሻ ఠאఘ ఠ ሺߩ௧ ሻ is the number of times that  occurs in ߩ௧ , ߜఘᇲ  ሺሻ equals to 1 when  occurs in ߩ௧ᇱ , and 0 otherwise. In this paper, the order of n-gram considered varies from 1 to 4. 8) ߠ ሺߩǡ ߩᇱ ሻ . A feature that reflects the agreement on word lengths for ߩ௧ and ߩ௧ᇱ : ߠ ሺߩǡ ߩᇱ ሻ ൌ ȁߩ௧ ȁߜȁఘ ȁ ሺȁߩ௧ᇱ ȁሻ ߜȁఘ ȁ ሺȁߩ௧ᇱ ȁሻ equals to 1 when ȁߩ௧ ȁ ൌ ȁߩ௧ᇱ ȁ, and 0 otherwise. These features are motivated by the success of consensus-based techniques (Kumar and Byrne, 2004; Tromble et al., 2008; Kumar et al., 2009). To summarize, 6 features are contained in the first category and 5 features are contained in the second category. Because that source and target phrases are exchangeable for each phrase pair, there will be (2*11=22) similarity features in total for each bilingual phrase5. 4 4.1 Experiments Data and Metric We evaluate on the NIST Chinese-to-English MT tasks. The NIST 2003 (MT03) data set is used as the development set to tune model parameters, and evaluation results are reported on the NIST 2005 (MT05) and 2008 (MT08) data sets. Two parallel data sets with different scale"
2011.mtsummit-papers.20,J07-1003,0,0.011579,"al times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used i"
2011.mtsummit-papers.20,W04-3250,0,0.0256834,"g data sets, LDC2003E07, LDC2003E14, LDC2005T06 and LDC2005T10 with 354K sentence pairs contained after pre-processing. We confirm the effectiveness of our method on it using the optimal parameter setting determined on the first data set. We train a 5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100, and MERT algorithm (Och, 2003) is utilized to optimize model parameters. Because of using MBR-inspire"
2011.mtsummit-papers.20,W02-1019,0,0.0800667,"Because of appearing in training corpus several times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learne"
2011.mtsummit-papers.20,N04-1022,0,0.138498,"n training corpus several times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned p"
2011.mtsummit-papers.20,2008.amta-papers.18,0,0.0751871,"sing a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used in SMT decoding. We evaluate on a state-of-the-art phrase-based SMT decoder on the NIST Chinese-to-English MT tasks, and experiments show that our MBRbased approach outperforms the standard phrase extraction method by up to 1.45 BLEU points. 2008; Venugopal et al., 2008; Liu et al., 2009), in which n-best alternatives of annotations to SMT systems are leveraged to improve translation quality, we allow our proposed phrase extraction method to operate on n-best word alignments as well: given a sentence pair with n-best alignment candidates, we use alignments in the n-best list one at a time with the same sentence pair to form a new word-aligned sentence pair, and annotate it with the posterior probability of the alignment it used. These posterior probabilities will be used in the next step to compute MBR model scores. 2 The objective of this step is to score p"
2011.mtsummit-papers.20,P06-1066,0,0.0260477,"5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100, and MERT algorithm (Och, 2003) is utilized to optimize model parameters. Because of using MBR-inspired techniques, we also investigate the impacts of our method on MBR decoding over translation hypergraphs of the baseline system. We re-implement an MBR decoder (Kumar et al., 2009) that uses the linear BLEU score as its loss function. 4.3 Word Aligner Althou"
2011.mtsummit-papers.20,P09-2060,0,0.218602,"Missing"
2011.mtsummit-papers.20,W04-3227,0,0.0699735,"Missing"
2011.mtsummit-papers.20,P09-1019,0,0.105678,"ingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used in SMT decoding. We ev"
2011.mtsummit-papers.20,2006.amta-papers.11,0,0.0384085,"Missing"
2011.mtsummit-papers.6,W08-0301,1,0.906283,"skeletonenhanced re-ranking, which re-ranks the n-best output of a conventional SMT decoder with respect 73 to translated skeleton; another is skeletonenhanced decoding, which re-ranks the translation hypotheses of not only the entire sentence but any span of the sentence. In the following, we will elaborate the related works in Section 2, followed by the modules of skeleton-enhanced translation framework in Section 3, and the experiments results, which show significant improvements over a state-of-the-art phrase-based baseline system, in Section 4. Section 5 is our conclusion. 2 Related Work Li et al. (2008) proposed three source spurious word deletion models2 to calculate the translation probability for any source word to be translated into the special empty symbol, ε. The first model uses a uniform probability ሺߝሻ , calculated by MLE from the word-aligned training corpus. The second one is word-type sensitive probability ሺߝȁݓሻ, where  ݓis the type of a source word, estimated in similar way as the first model. The third one is a word-token sensitive model, which uses Conditional Random Fields (CRF) (Lafferty et al., 2001) to calculate how likely a word token should be spurious given it"
2011.mtsummit-papers.6,P09-1065,0,0.0164394,"quires a mapping between (nonspurious) source word positions during source word deletion. During each decoding step, where the translation candidates of a particular source sentence span are under consideration, SED fetches the corresponding source skeleton span and its best partial translated skeleton, and then calculates several kinds of similarity between the (partial) translation candidates and the partial translated skeleton. The scoring of the translation candidates is thus enhanced by the skeleton-related features. At a glance, SED is similar to collaborative decoding (Li et al., 2009, Liu et al., 2009). There is, however, a major difference. While every decoder involved in collaborative decoding selects its own best translation candidate by considering the candidates from other decoders, the SED decoder considers the candidates from skeleton translation, but not vice versa. 4 In this section, after elaborating the experiment settings in Section 4.1, we will explain how the thresholds for CIM and CSM are chosen empirically in Section 4.2 and Section 4.3 respectively. The improvement in SMT performance by the skeletonenhanced methods will be shown in Section 4.4, followed by a detailed featur"
2011.mtsummit-papers.6,D08-1077,0,0.0165919,"ranslation probability for any source word to be translated into the special empty symbol, ε. The first model uses a uniform probability ሺߝሻ , calculated by MLE from the word-aligned training corpus. The second one is word-type sensitive probability ሺߝȁݓሻ, where  ݓis the type of a source word, estimated in similar way as the first model. The third one is a word-token sensitive model, which uses Conditional Random Fields (CRF) (Lafferty et al., 2001) to calculate how likely a word token should be spurious given its context. All of them can improve a phrase-based system significantly. Menezes and Quirk (2008) improved both phrase-based and treelet systems by introducing structural word insertion and deletion without requiring lexical anchor. The insertion/deletion order templates are based on syntactic cues and two additional feature functions (a count of structurally inserted words and a count of structurally deleted words). The probabilities of the templates are estimated by MLE. 2 However, the problem of word insertion is discussed but not addressed. In these two approaches, spurious words are treated the same as non-spurious words; they are simply assigned with a special translation probabilit"
2011.mtsummit-papers.6,J03-1002,0,0.00511041,"rther improvement. 76 Experiment Setting We conduct our experiments on the test data from NIST 2006 and NIST 2008 Chinese-to-English machine translation tasks. To tune the model parameters, NIST 2005 test data is used as our development data. The bilingual training dataset is NIST 2008 training set excluding the Hong Kong Law and Hong Kong Hansard (contains 354k sentence pairs, 8M Chinese words and 10M English words). The translation pairs are extracted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performe"
2011.mtsummit-papers.6,P03-1021,0,0.0231374,"on skeleton which is computed by the language model trained with spurious-word-deleted language model training data (in section 3.2). The features of unigram/bigram recall/precision measure the similarity with respect to faithfulness while the feature of skeleton language model measures the similarity with respect to fluency 4 . The values of these five features are calculated on the fly during decoding. The five features are used alongside the conventional features in SMT, and the weights of all these features can be trained by any conventional method like Minimum Error Rate Training (MERT) (Och, 2003). In sum, SED requires a mapping between (nonspurious) source word positions during source word deletion. During each decoding step, where the translation candidates of a particular source sentence span are under consideration, SED fetches the corresponding source skeleton span and its best partial translated skeleton, and then calculates several kinds of similarity between the (partial) translation candidates and the partial translated skeleton. The scoring of the translation candidates is thus enhanced by the skeleton-related features. At a glance, SED is similar to collaborative decoding (L"
2011.mtsummit-papers.6,P02-1040,0,0.0813556,"y as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences are listed in Table 1. Skeletons TNS-1 TDS1-1 TNS-2 TDS1-2 TNS-1 TDS2-1 TNS-2 TDS2-2 TNS-12 TDS12-12 TNS-23 TDS23-23 TNS-123 TDS123-123 TNS-1234 TDS1234-1234 Sourc"
2011.mtsummit-papers.6,J07-2003,0,0.0472828,"common between ܶௌ and ܵ 4 Trigram and 4-gram features were attempted but found to give no further improvement. 76 Experiment Setting We conduct our experiments on the test data from NIST 2006 and NIST 2008 Chinese-to-English machine translation tasks. To tune the model parameters, NIST 2005 test data is used as our development data. The bilingual training dataset is NIST 2008 training set excluding the Hong Kong Law and Hong Kong Hansard (contains 354k sentence pairs, 8M Chinese words and 10M English words). The translation pairs are extracted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2"
2011.mtsummit-papers.6,J97-3002,0,0.0211816,"ontains 354k sentence pairs, 8M Chinese words and 10M English words). The translation pairs are extracted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences a"
2011.mtsummit-papers.6,D07-1006,0,0.0158878,"s, and conjunctions like &quot;because&quot;, &quot;therefore&quot;, etc. refer to objects, actions, events, or logical relationships. Their meaning is language-neutral and they usually have counterparts in another language. In contrast, some words serve to express (language-specific) grammatical relations only, and thus they may 72 To deal with the spurious words in sentence pairs, IBM models 3, 4 and 5 (Brown et al., 1993) introduce a special token null, which can align to a source/target word. Hence there are two types of words: spurious words (null-aligned) and nonspurious words. Similar with the IBM models, Fraser and Marcu (2007) proposed a new generative model called LEAF, in which words are classified into three types instead of two: spurious words, head words (which are the key words of a sentence) and non-head words (modifiers of head words). These two methods are generative models for word alignment, which cannot be directly used in the conventional log-linear model of statistic machine translation (SMT). The conventional phrasebased SMT captures spurious words within the phrase pairs in the translation table. For example, in the phrase pair (&quot;yong (spend) na (that) bi qian(money)&quot;, &quot;to spend that money&quot;), it imp"
2011.mtsummit-papers.6,N03-1017,0,0.108226,"Missing"
2011.mtsummit-papers.6,W04-3250,0,0.722002,"inhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences are listed in Table 1. Skeletons TNS-1 TDS1-1 TNS-2 TDS1-2 TNS-1 TDS2-1 TNS-2 TDS2-2 TNS-12 TDS12-12 TNS-23 TDS23-23 TNS-123 TDS123-123 TNS-1234 TDS1234-1234 Source Target Word Frequency Word Frequency de 127,226 the 237,160 le 11,991 of 128,443 zai 6,577 to 41,217 zhong 6,250 in 39,146 shang 4,287 for 35,570 Table 1"
2011.mtsummit-papers.6,P06-1066,0,0.0211885,"acted from word alignment matrices in the same way as Chiang (2007). The word alignment matrixes are generated in two directions by running GIZA++ (Och and Ney, 2003. Our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. There are two baseline SMT systems, one is a state-of-the-art implementation of hierarchical phrase-based model (Hiero) (Chiang, 2007) with conventional features; another is state-of-the-art implementation of Bracketing Transduction Grammar (Dekai Wu, 1997) (BTG) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The case-insensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric for SMT performance and statistical significance is performed using bootstrap re-sampling method proposed by Koehn (2004). 4.2 3 Experiments CIM of Spurious Words For the context insensitive model (CIM) of spurious word deletion, we calculate the percentage of unaligned tokens of each word type from wordaligned bilingual training corpus. The top 5 frequent unaligned words for source/target sentences are listed in Table 1. Skeletons TNS-1 TDS1-1 TNS-2 TDS1-2 TNS-1 TDS2-1 TNS-2 TDS2-2 TNS-12 TDS12-12 TNS-23 TDS"
2011.mtsummit-papers.6,J93-2003,0,\N,Missing
2011.mtsummit-papers.6,P09-1066,1,\N,Missing
2011.mtsummit-papers.6,J04-4002,0,\N,Missing
2011.mtsummit-papers.6,D07-1080,0,\N,Missing
2011.mtsummit-systems.2,J07-2003,0,0.040477,"Missing"
2011.mtsummit-systems.2,P06-1097,0,0.0570349,"Missing"
2011.mtsummit-systems.2,P06-1065,0,0.0513002,"Missing"
2020.acl-main.130,P17-1152,0,0.0695968,"Missing"
2020.acl-main.130,W19-4828,0,0.0695299,"Missing"
2020.acl-main.130,N19-1423,0,0.142582,"similarity of the final hidden state from both LSTMs. Sequential Matching Network (Wu et al., 2017): To avoid losing information in the context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. RoBERTa: Liu et al. (2019) re-establish BERT’s masked language model training objective by using more data and different hyper-parameters. We fine-tune RoBERTa in the same way as BERT. GPT-2 (Radford et al., 2019): Giv"
2020.acl-main.130,N19-1246,0,0.0402022,"Missing"
2020.acl-main.130,D19-1243,0,0.0342445,"Missing"
2020.acl-main.130,P19-1356,0,0.0569446,"Missing"
2020.acl-main.130,N18-1023,0,0.0279937,"alogue datasets (Lowe et al., 2015; Wu et al., 2017). Because MuTual is modified from listening tests of English as a foreign language, the complexity of morphology and grammar is much simpler than other datasets. For human-annotated datasets, there is always a trade-off between the number of instances being annotated and the quality of annotations (Kryciski et al., 2019). Our dataset is smaller than the previous crawling-based dialogue dataset (Lowe et al., 2015; Wu et al., 2017) due to the collection method. But it is comparable with high-quality reasoning based dataset (Clark et al., 2018; Khashabi et al., 2018; Talmor et al., 2019) and human-designed dialogue dataset (Zhang et al., 2018a). Moreover, around 10k is sufficient to train a discriminative model (Nivre et al., 2019) or fine-tuning the pretraining model (Wang et al., 2019). To assess the distribution of different reasoning types, we annotate the specific types of reasoning that are involved for instance, sampled from the test set and categorize them into six groups. The definition and ratio of each group are shown as follows. Attitude Reasoning: This type of instance tests if a model knows the speaker’s attitude towards an object. Algebrai"
2020.acl-main.130,D19-1051,0,0.0296336,"instance when inspectors doubt the uniqueness or correctness of the answer. 3.2 Analysis The detailed statistics of MuTual are summarized in Table 2. MuTual has an average of 4.73 turns. The vocabulary size is 11,343, which is smaller than other dialogue datasets (Lowe et al., 2015; Wu et al., 2017). Because MuTual is modified from listening tests of English as a foreign language, the complexity of morphology and grammar is much simpler than other datasets. For human-annotated datasets, there is always a trade-off between the number of instances being annotated and the quality of annotations (Kryciski et al., 2019). Our dataset is smaller than the previous crawling-based dialogue dataset (Lowe et al., 2015; Wu et al., 2017) due to the collection method. But it is comparable with high-quality reasoning based dataset (Clark et al., 2018; Khashabi et al., 2018; Talmor et al., 2019) and human-designed dialogue dataset (Zhang et al., 2018a). Moreover, around 10k is sufficient to train a discriminative model (Nivre et al., 2019) or fine-tuning the pretraining model (Wang et al., 2019). To assess the distribution of different reasoning types, we annotate the specific types of reasoning that are involved for in"
2020.acl-main.130,D17-1082,0,0.0723066,"Missing"
2020.acl-main.130,W15-4640,0,0.493103,"we will be late for the dinner. Figure 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly"
2020.acl-main.130,D19-1191,0,0.0373074,"Missing"
2020.acl-main.130,Q19-1016,0,0.113805,"sense knowledge are not captured sufficiently (Young et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $"
2020.acl-main.130,P15-1152,0,0.0210348,"et al., 2018a) considers consistent personality in dialogue. Crowd workers are required to act the part of a given provided persona, and chat naturally. Dialogue NLI (Welleck et al., 2019) is a natural language inference dataset modified from PERSONA-CHAT. It demonstrates that NLI can be used to improve the consistency of dialogue models. CoQA (Reddy et al., 2019) is collected by pairing two annotators to chat about a passage in the form of questions and answers. Each question is dependent on the conversation history. There are also several large-scale datasets in Chinese, such as Sina Weibo (Shang et al., 2015), Douban Conversation Corpus (Wu et al., 2017) and E-commerce Dialogue Corpus (Zhang et al., 2018b). As shown in Table 1, most of the existing conversation benchmarks do not focus on testing reasoning ability. One exception is CoQA, which considers pragmatic reasoning. The difference is that CoQA is a machine comprehension dataset, in which conversations are based on a given passage. Another related reading comprehension dataset is DREAM (Sun et al., 2019), which is designed specifically for challenging dialogue-based reading 1407 Listening Comprehension M Ma'am, you forgot your phone. Dialogu"
2020.acl-main.130,Q19-1014,0,0.172647,"et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Per"
2020.acl-main.130,N19-1421,0,0.465732,"in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Persona Diverse Open Open Open Movie Open Open Science Open Narrative Open Manually $ "" $ "" $ """
2020.acl-main.130,P19-1001,0,0.279625,"Missing"
2020.acl-main.130,voorhees-tice-2000-trec,0,0.41351,"Missing"
2020.acl-main.130,P19-1363,0,0.130911,"soning capability and commonsense knowledge are not captured sufficiently (Young et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances P"
2020.acl-main.130,J19-1005,1,0.900814,"Missing"
2020.acl-main.130,P17-1046,1,0.758434,"the dinner. Figure 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive resul"
2020.acl-main.130,N19-1242,0,0.0248812,"e context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. RoBERTa: Liu et al. (2019) re-establish BERT’s masked language model training objective by using more data and different hyper-parameters. We fine-tune RoBERTa in the same way as BERT. GPT-2 (Radford et al., 2019): Given a context, the positive response has a higher probability compared with negative responses. Motivated by this, we concatenate co"
2020.acl-main.130,D18-1009,0,0.269438,"ow we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Persona Diverse Open Open Open Movie Open Open Science Open"
2020.acl-main.130,P18-1205,0,0.49499,"re 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive results on these datasets"
2020.acl-main.130,C18-1317,0,0.432694,"re 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive results on these datasets"
2020.acl-main.130,P18-1103,0,0.0685413,"date response as the model output. The “IDF” is calculated only on the training set. Dual LSTM (Lowe et al., 2015): Two LSTMs are used to encode context and response, respectively. The relevance between context and response is calculated by the similarity of the final hidden state from both LSTMs. Sequential Matching Network (Wu et al., 2017): To avoid losing information in the context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representa"
2020.acl-main.318,D18-1214,0,0.10454,"Missing"
2020.acl-main.318,P17-1042,0,0.500515,"om which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embeddings by finding near"
2020.acl-main.318,P18-1073,0,0.0579027,"es clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial s"
2020.acl-main.318,D18-1399,0,0.0702327,"Missing"
2020.acl-main.318,P19-1019,0,0.0119731,"ut also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping"
2020.acl-main.318,Q17-1010,0,0.0911767,"the embeddings of the source language as X := WX. Divide {Cix }m 9 Initialize i=0 into K groups via clustering. {Wi }K i=0 with W. Fine-tune each Wi according to Eq. (6) and do the refinement. return {Wi }K i=0 4.2.3 we retrieve the translation of x by calculating the CSLS score of Ws x and each target embedding y, similar to Eq. (2) introduced in §2.2. 4 4.3.1 Dataset Bilingual lexicon induction (BLI) measures the word translation accuracy in comparison to a gold standard. We report results on the widely used MUSE dataset (Conneau et al., 2017). This dataset consists of monolingual fastText (Bojanowski et al., 2017) embeddings of many languages and dictionaries for many language pairs divided into training and test sets. The evaluation follows the setups of Conneau et al. (2017). 4.2 4.2.1 Implementation Details Pre-processing We choose the top 10,000 word embeddings to build word graph because the monolingual embeddings of low-frequency words may be trained insufficiently. The embeddings are normalized following Artetxe et al. (2018b). Specifically, we first apply length normalization to the embeddings, and then mean center each dimension. After that, we do length normalization again to ensure the word"
2020.acl-main.318,D18-1043,0,0.111745,"he performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embeddings by finding nearest neighbors in the target embedding space. With the new dictionary, the mapping fun"
2020.acl-main.318,Q19-1007,0,0.0634208,"e method proposed by Artetxe et al. (2018b). We use their public code to finish this step. We initialized W with a random orthogonal matrix. After building the seed dictionary, we first solve the Procrustes problem (Eq. (1)), followed by the refinement process. 4.3 Experiment 4.1 Clique Extraction Main Results Baselines We choose several supervised and unsupervised methods to be our baselines. The supervised baselines include: (1) The iterative Procrustes method proposed by Smith et al. (2017); (2) The multi-step framework proposed by Artetxe et al. (2018a); (3) a geometric method proposed by Jawanpuria et al. (2019). The unsupervised baselines include (1) MUSE proposed by Conneau et al. (2017), which is a GAN based method followed by a refinement process; (2) a Wasserstein GAN based method combined with distribution matching and back translation, proposed by Xu et al. (2018); (3) a method proposed by Alvarez-Melis and Jaakkola (2018) that views the mapping problem as optimal transportation and optimize the Gromov-Wasserstein distance between embedding spaces; (4) A robust self-learning method proposed by Artetxe et al. (2018b), which leverages the intra-linguistic word similarity information to infer ini"
2020.acl-main.318,D18-1330,0,0.0646003,"Missing"
2020.acl-main.318,D18-1549,0,0.0751542,"oach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary ("
2020.acl-main.318,W15-1521,0,0.0323991,"2 Background Unsupervised bilingual lexicon induction (BLI) is the task of inducing word translations from monolingual corpora of two languages. Recently proposed methods follow the same procedure, i.e., first learning cross-lingual embeddings in an unsupervised way (§2.1) and then inducing bilingual lexicons from the embedding spaces (§2.2). 2.1 Unsupervised Cross-lingual Embeddings Previous methods for learning cross-lingual embeddings can be roughly divided into two categories (Ormazabal et al., 2019), i.e., mapping methods and joint learning methods. As the second category, the skip-gram (Luong et al., 2015) for example, requires bilingual corpus during training, current methods for unsupervised cross-lingual embeddings mainly fall into the first category. Given pretrained monolingual embeddings of two languages, the mapping methods try to map the source and target embedding spaces through a linear transformation (Mikolov et al., 2013) W ∈ Md×d (R), where Md×d (R) is the space of d × d matrices of real numbers and d is the dimension of the embeddings. Based on that, Xing et al. (2015) propose to constrain W to be orthogonal, i.e., W&gt; W = I, and Conneau et al. (2017) find this is a Procrustes prob"
2020.acl-main.318,P19-1492,0,0.0998402,"ngs; (3) We improve the BLI performance on the MUSE dataset with our method, even compared with strong baselines. 2 Background Unsupervised bilingual lexicon induction (BLI) is the task of inducing word translations from monolingual corpora of two languages. Recently proposed methods follow the same procedure, i.e., first learning cross-lingual embeddings in an unsupervised way (§2.1) and then inducing bilingual lexicons from the embedding spaces (§2.2). 2.1 Unsupervised Cross-lingual Embeddings Previous methods for learning cross-lingual embeddings can be roughly divided into two categories (Ormazabal et al., 2019), i.e., mapping methods and joint learning methods. As the second category, the skip-gram (Luong et al., 2015) for example, requires bilingual corpus during training, current methods for unsupervised cross-lingual embeddings mainly fall into the first category. Given pretrained monolingual embeddings of two languages, the mapping methods try to map the source and target embedding spaces through a linear transformation (Mikolov et al., 2013) W ∈ Md×d (R), where Md×d (R) is the space of d × d matrices of real numbers and d is the dimension of the embeddings. Based on that, Xing et al. (2015) pro"
2020.acl-main.318,P18-1072,0,0.123342,"Missing"
2020.acl-main.318,N15-1104,0,0.134887,"Missing"
2020.acl-main.318,D18-1268,0,0.322829,"proach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embeddings by finding nearest neighbors in the target embedding space. With the new dict"
2020.acl-main.318,P17-1179,0,0.591623,"ngual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods. 1 Introduction Bilingual lexicon induction (BLI) is an important task of machine translation and becomes an essential part of recent unsupervised machine translation approaches (Lample et al., 2018; Artetxe et al., 2018c; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). Previous methods for BLI are ∗ Contribution during internship at MSRA. mostly based on unsupervised cross-lingual word embeddings (Zhang et al., 2017; Artetxe et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; AlvarezMelis and Jaakkola, 2018), the goal of which is to find a mapping function, typically a linear transformation (Mikolov et al., 2013), to map the source embeddings into the target embedding spaces. To do this, they first build a seed dictionary (known as the initial solution) with different methods and then learn the optimal mapping function that fits the seed dictionary. Based on the mapping function, a new dictionary of higher quality is inferred from the cross-lingual word embe"
2020.acl-main.320,D18-1549,0,0.236248,"mple et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) usually use the following n-gram embeddings based initialization. They first build phrase translation tables with the help of unsupervised cross-lingual n-gram embeddings (Conneau et al., 2017; Artetxe et al., 2018a), and then use them to build two initial Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) models with two language models. However, there are two problems with their initialization methods. (1) Some complex sentence structures of original training sentences are hard to be recovered with the"
2020.acl-main.320,P19-1019,0,0.019551,"e and taren as target 59.87 48.52 fr as target 58.71 47.63 Table 3: Test BLEU scores of the rewriting models. 4 Related Work Unsupervised machine translation becomes a hot research topic in recent years. The pioneering methods are based on NMT models (Transformer) (Artetxe et al., 2017; Lample et al., 2017; Yang et al., 2018) trained with denoising auto-encoder (Vincent et al., 2010) and iterative back-translation. The following work shows that SMT methods and the hybrid of NMT and SMT can be more effective (Artetxe et al., 2018b; Lample et al., 2018; Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). They build the initial PBSMT models with language models and phrase tables inferred from unsupervised cross-lingual n-gram embeddings. Recently, Lample and Conneau (2019) propose a pre-training method and achieve state-of-theart performance on unsupervised en-f r and en-de translation tasks. But they use much more monolingual data from Wikipedia than previous work and this paper. We must also mention the work of Wu et al. (2019). They similarly use retrieval and rewriting framework for unsupervised MT. However, ours is different from theirs in two aspects. First, we efficiently calculate the"
2020.acl-main.320,D18-1399,0,0.0828926,"o minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. 1 Introduction Recent work has shown successful practices of unsupervised machine translation (UMT) (Artetxe et al., 2017; Lample et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Re"
2020.acl-main.320,J03-1002,0,0.0445331,"eir initialization methods. (1) Some complex sentence structures of original training sentences are hard to be recovered with the n-gram translation tables. (2) The initial translation tables inevitably contain much noise, which will be amplified in the subsequent process. In this paper, we propose a novel retrieve-andrewrite initialization method for UMT. Specifically, we first retrieve semantically similar sentence pairs from monolingual corpora of two languages with the help of unsupervised cross-lingual sentence embeddings. Next, with those retrieved similar sentence pairs, we run GIZA++ (Och and Ney, 2003) to get word alignments which are used to delete unaligned words in the target side of the retrieved sentences. The modified target sentences are then rewritten with a designed sequence-to-sequence rewriting model to minimize the semantic gap between the source and target sides. Taking the pairs of the source sentences and corresponding rewritten targets as pseudo parallel data, we then build two initial PBSMT models (source-to-target and targetto-source), which are used to generate pseudo parallel data to warm up NMT models, followed by an iterative back-translation training process. Our code"
2020.acl-main.320,Q17-1010,0,0.0129359,"emantic gap between the source and retrieved targets, we do target sentences rewriting (§2.2) by deleting unaligned words in the target side, and generate complete and better-aligned targets via our rewriting model with the help of missing information provided by the source. After that, we treat the rewritten pairs as the pseudo parallel data for translation models initialization and training (§2.3). 2.1 Similar Sentences Retrieval Given two monolingual corpora Dx and Dy of two languages X and Y respectively, we first build unsupervised cross-lingual word embeddings of X and Y using fastText (Bojanowski et al., 2017) and vecmap (Artetxe et al., 2018a), and then we obtain cross-lingual sentence embeddings based on the cross-lingual word embeddings via SIF (Arora et al., 2017). After that, we use the marginal-based scoring (Artetxe and Schwenk, 2018) to retrieve Figure 2: Example of rewriting. The unaligned words, i.e., 250 and 建议(suggestion), proposed by GIZA++ have been removed in y 0 , which is then rewritten by the model to the right target yˆ (40 and 反馈(responses)). More examples of the sentences before and after rewriting are shown in Appendix B. similar sentences from two corpora1 . Examples retrieve"
2020.acl-main.320,P16-1009,0,0.0220579,"els, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. 1 Introduction Recent work has shown successful practices of unsupervised machine translation (UMT) (Artetxe et al., 2017; Lample et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) usually use the following n-gram embeddings based initialization. They first build phrase translation tables with the help of unsupervised cross-lingual n-gram embeddings (Conneau et al., 2017; Artetxe"
2020.acl-main.320,D07-1103,0,0.101406,"Missing"
2020.acl-main.320,P16-1162,0,0.00987909,"els, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. 1 Introduction Recent work has shown successful practices of unsupervised machine translation (UMT) (Artetxe et al., 2017; Lample et al., 2017, 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019). The common framework is to build two initial translation models (i.e., source to target and target to source) and then do iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with pseudo data generated by each other. The initialization stage is important because bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. ∗ Contribution during internship at MSRA. Previous methods for UMT (Lample et al., 2018; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) usually use the following n-gram embeddings based initialization. They first build phrase translation tables with the help of unsupervised cross-lingual n-gram embeddings (Conneau et al., 2017; Artetxe"
2020.acl-main.321,W05-0909,0,0.192678,"-Transformer. 3.1 Datasets Following the previous work (Maruf et al., 2019), we use three English-German datasets as the benchmark datasets, which are TED, News, and Europarl. The statistic of these datasets can be found in Table 1. We obtain the processed datasets from Maruf et al. (2019)2 , so that our results can be compared with theirs reported in Maruf et al. (2019). We use the scripts of Moses toolkit3 to tokenize the sentences. We also split the words into subword units (Sennrich et al., 2016) with 30K mergeoperations. The evaluation metrics are BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN (Werlen et al., 2018) SAN (Maruf et al., 2019) QCN (Yang et al., 2019) Transformer (Zhang et al., 2018) +BERT 24.58 24.62"
2020.acl-main.321,N18-1118,0,0.310147,"ncoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this c"
2020.acl-main.321,N19-1423,0,0.0488099,"sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately. Moreover, it cannot be directly adapted to the recent pre-training models (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019; Dong et al., 2019; Song et al., 2019; Lample and Conneau, 2019), which encodes multiple sentences with a single encoder. Different from the dual-encoder structure, the uni-encoder structure takes the concatenation of contexts and source sentences as the input (as shown in Figure 1b). Therefore, when modeling the contexts, it can make full use of the interaction between the source sentences and the contexts, while the dual-encoder model fails to exploit this information. Moreover, the uni-encoder structure is identical to the recent pre-training mode"
2020.acl-main.321,W19-5321,0,0.0606372,"the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully mo"
2020.acl-main.321,D18-1512,0,0.378057,"models of dual-encoder structures in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of the proposed structure. 2 Translation Encoder Context Formally, we denote X = {x1 , x2 , · · · , xN } as the source document with N sentences, and Y = {y1 , y2 , · · · , yM } as the target document with M sentences. We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms (Sennrich and Volk, 2011). Therefore, we can assume that (xi , yi ) is a parallel sentence pair. Following Zhang et al. (2018), y<i can be omitted because x<i and y<i conveys the same information. As a result, the probability can be approximated as: P (Y |X) ≈ N Y P (yi |xi ; x<i ; x&gt;i ) (1) i=1 where xi is the source sentence aligned to yi , and (x<i , x&gt;i ) is the document-level context used to translate yi . + Word Embedding + Segment Embedding Source Figure 2: The architecture of the proposed FlatTransformer model. 2.2 Segment Embedding The flat structure adopts a unified encoder that does not distinguish the context sentences and the source sentences. Therefore, we introduce the segment embedding to identify the"
2020.acl-main.321,P18-1118,0,0.155313,"Context Source (b) Uni-Encoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is"
2020.acl-main.321,N19-1313,0,0.747779,"velopment of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside ea"
2020.acl-main.321,W18-6307,0,0.0517618,"Missing"
2020.acl-main.321,P02-1040,0,0.106916,"e denote the proposed model as Flat-Transformer. 3.1 Datasets Following the previous work (Maruf et al., 2019), we use three English-German datasets as the benchmark datasets, which are TED, News, and Europarl. The statistic of these datasets can be found in Table 1. We obtain the processed datasets from Maruf et al. (2019)2 , so that our results can be compared with theirs reported in Maruf et al. (2019). We use the scripts of Moses toolkit3 to tokenize the sentences. We also split the words into subword units (Sennrich et al., 2016) with 30K mergeoperations. The evaluation metrics are BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN (Werlen et al., 2018) SAN (Maruf et al., 2019) QCN (Yang et al., 2019) Transformer"
2020.acl-main.321,N18-1202,0,0.0428805,"cument-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately. Moreover, it cannot be directly adapted to the recent pre-training models (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019; Dong et al., 2019; Song et al., 2019; Lample and Conneau, 2019), which encodes multiple sentences with a single encoder. Different from the dual-encoder structure, the uni-encoder structure takes the concatenation of contexts and source sentences as the input (as shown in Figure 1b). Therefore, when modeling the contexts, it can make full use of the interaction between the source sentences and the contexts, while the dual-encoder model fails to exploit this information. Moreover, the uni-encoder structure is identical to the recent pre-training models (e.g., 3505 Procee"
2020.acl-main.321,P16-1162,0,0.110385,"l stateof-the-art models on three document-level machine translation benchmarks. We denote the proposed model as Flat-Transformer. 3.1 Datasets Following the previous work (Maruf et al., 2019), we use three English-German datasets as the benchmark datasets, which are TED, News, and Europarl. The statistic of these datasets can be found in Table 1. We obtain the processed datasets from Maruf et al. (2019)2 , so that our results can be compared with theirs reported in Maruf et al. (2019). We use the scripts of Moses toolkit3 to tokenize the sentences. We also split the words into subword units (Sennrich et al., 2016) with 30K mergeoperations. The evaluation metrics are BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN ("
2020.acl-main.321,W11-4624,0,0.0203562,"document-level machine translation datasets. Experiments show that it can achieve better performance than the baseline models of dual-encoder structures in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of the proposed structure. 2 Translation Encoder Context Formally, we denote X = {x1 , x2 , · · · , xN } as the source document with N sentences, and Y = {y1 , y2 , · · · , yM } as the target document with M sentences. We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms (Sennrich and Volk, 2011). Therefore, we can assume that (xi , yi ) is a parallel sentence pair. Following Zhang et al. (2018), y<i can be omitted because x<i and y<i conveys the same information. As a result, the probability can be approximated as: P (Y |X) ≈ N Y P (yi |xi ; x<i ; x&gt;i ) (1) i=1 where xi is the source sentence aligned to yi , and (x<i , x&gt;i ) is the document-level context used to translate yi . + Word Embedding + Segment Embedding Source Figure 2: The architecture of the proposed FlatTransformer model. 2.2 Segment Embedding The flat structure adopts a unified encoder that does not distinguish the cont"
2020.acl-main.321,W17-4811,0,0.420201,"Encoder Transformer Decoder + Context Source (b) Uni-Encoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder"
2020.acl-main.321,D19-1081,0,0.742702,"Missing"
2020.acl-main.321,P19-1116,0,0.540392,"Missing"
2020.acl-main.321,P18-1117,0,0.125667,"ving the architectures of the document machine translation models. Tiedemann and Scherrer (2017) and Wang et al. (2017) explore possible solutions to exploit the crosssentence contexts for neural machine translation. Zhang et al. (2018) extends the Transformer model with a new context encoder to represent documentlevel context. Werlen et al. (2018) and (Maruf et al., 2019) propose two different hierarchical attention models to model the contexts. Yang et al. (2019) introduces a capsule network to improve these hierarchical structures. There are also some works analyzing the contextual errors (Voita et al., 2018, 2019b; Bawden et al., 2018) and providing the test suites (M¨uller et al., 2018). More recently, Voita et al. (2019a) explores the approaches to incorporate the mono-lingual data to augment the document-level bi-lingual dataset. Different from these works, this paper mainly discusses the comparison between dual-encoder models and uniencoder models and proposes a novel method to improve the uni-encoder structure. 5 Conclusions In this work, we explore the solutions to improve the uni-encoder structures for document-level machine translation. We propose a Flat-Transformer model with a unified"
2020.acl-main.321,D17-1301,0,0.43505,"slation Transformer Encoder Transformer Decoder + Context Source (b) Uni-Encoder Structure Figure 1: The overview of the dual-encoder structure and the uni-encoder structure for document-level machine translation. Introduction Thanks to the development of the deep learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new co"
2020.acl-main.321,D18-1325,0,0.505586,"p learning methods, the machine translation systems have achieved good performance that is even comparable with human translation in the news domain (Hassan et al., 2018). However, there are still some problems with machine translation in the documentlevel context (L¨aubli et al., 2018). Therefore, more recent work (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Voita et al., 2019a; Junczys-Dowmunt, 2019) is focusing on the document-level machine translation. Most of the existing models (Zhang et al., 2018; Maruf et al., 2019; Werlen et al., 2018) for document-level machine translation use two encoders to model the source sentences and the document-level contexts. Figure 1a illustrates the structure of these models. They extend the standard 1 In this work, document-level contexts denote the surrounding sentences of the current source sentence. Transformer model with a new context encoder, and the encoder for source sentences is conditioned on this context encoder. However, they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately."
2020.acl-main.321,D19-1164,0,0.7981,"re BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005). 3.2 Implementation Details The batch size is limited to 4, 000 tokens for all models. We set the hidden units of the multi-head component and the feed-forward layer as 512 and 1024. The embedding size is 512, the number of heads is 4, and the dropout rate (Srivastava et al., 2014) is 0.3. The number of Transformer blocks 3507 2 3 https://github.com/sameenmaruf/selective-attn https://github.com/moses-smt/mosesdecoder TED BLEU METR News BLEU METR Europarl BLEU METR Dual HAN (Werlen et al., 2018) SAN (Maruf et al., 2019) QCN (Yang et al., 2019) Transformer (Zhang et al., 2018) +BERT 24.58 24.62 25.19 24.01 23.19 45.48 45.32 45.91 45.30 45.25 25.03 24.84 22.37 22.42 22.06 44.02 44.27 41.88 42.30 42.25 29.58 29.90 29.82 29.93 30.72 46.91 47.11 47.86 48.16 48.62 Uni RNN (Bahdanau et al., 2015) Transformer (Vaswani et al., 2017) Our Flat-Transformer +BERT 19.24 23.28 24.87 26.61 40.81 44.17 47.05 48.53 16.51 22.78 23.55 24.52 36.79 42.19 43.97 45.40 26.26 28.72 30.09 31.99 44.14 46.22 48.56 49.76 Model Table 2: Results on three document-level machine translation benchmarks (“Dual” denotes dual-encoder, while “Uni” means uni-encoder). TE"
2020.acl-main.321,D18-1049,0,0.699692,"models of dual-encoder structures in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of the proposed structure. 2 Translation Encoder Context Formally, we denote X = {x1 , x2 , · · · , xN } as the source document with N sentences, and Y = {y1 , y2 , · · · , yM } as the target document with M sentences. We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms (Sennrich and Volk, 2011). Therefore, we can assume that (xi , yi ) is a parallel sentence pair. Following Zhang et al. (2018), y<i can be omitted because x<i and y<i conveys the same information. As a result, the probability can be approximated as: P (Y |X) ≈ N Y P (yi |xi ; x<i ; x&gt;i ) (1) i=1 where xi is the source sentence aligned to yi , and (x<i , x&gt;i ) is the document-level context used to translate yi . + Word Embedding + Segment Embedding Source Figure 2: The architecture of the proposed FlatTransformer model. 2.2 Segment Embedding The flat structure adopts a unified encoder that does not distinguish the context sentences and the source sentences. Therefore, we introduce the segment embedding to identify the"
2020.acl-main.331,P19-1033,1,0.648302,"//msnews.github.io. 1 Introduction Online news services such as Google News and Microsoft News have become important platforms for a large population of users to obtain news information (Das et al., 2007; Wu et al., 2019a). Massive news articles are generated and posted online every day, making it difficult for users to find interested news quickly (Okura et al., 2017). Personalized news recommendation can help users alleviate information overload and improve news reading experience (Wu et al., 2019b). Thus, it is widely used in many online news platforms (Li et al., 2011; Okura et al., 2017; An et al., 2019). In traditional recommender systems, users and items are usually represented using IDs, and their interactions such as rating scores are used to learn ID representations via methods like collaborative filtering (Koren, 2008). However, news recommendation has some special challenges. First, news articles on news websites update very quickly. New news articles are posted continuously, and existing news articles will expire in short time (Das et al., 2007). Thus, the cold-start problem is very severe in news recommendation. Second, news articles contain rich textual information such as title and"
2020.acl-main.331,N19-1423,0,0.0292461,"ove the performance of different neural text representation methods such as CNN and LSTM for news recommendation. It shows that selecting important words in news texts using attention can help learn more informative news representations. Another interesting finding is that the combination of LSTM and attention can achieve the best performance. However, to our best knowledge, it is not used in existing news recommendation methods. 5.3.2 Pre-trained Language Models Next, we explore whether the quality of news representation can be further improved by the pretrained language models such as BERT (Devlin et al., 2019), which have achieved huge success in different NLP tasks. We applied BERT to the news representation module of three state-of-theart news recommendation methods, i.e., NAML, LSTUR and NRMS. The results are summarized in Fig. 3. We find that by replacing the original word embedding module with the pre-trained BERT model, the performance of different news recommendation methods can be improved. It shows the BERT model pre-trained on large-scale corpus like Wikipedia can provide useful semantic information for news representation. We also find that fine-tuning the pre-trained BERT model with the"
2020.acl-main.331,D14-1162,0,0.0942926,"ify and compare the methods introduced in Section 4 on the MIND dataset. Since most of these news recommendation methods are based on news titles, for fair comparison, we only used news titles in experiments unless otherwise mentioned. We will explore the usefulness of different news texts such as body in Section 5.3.3. In order to simulate the practical news recommendation scenario where we always have unseen users not included in training data, we randomly sampled half of the users for training, and used all the users for test. For those methods that need word embeddings, we used the Glove (Pennington et al., 2014) as initialization. Adam was used as the optimizer. Since the non-clicked news are usually much more than the clicked news in each impression log, following (Wu et al., 2019b) we applied negative sampling technique to model training. All hyper-parameters were selected according to the results on the validation set. The metrics used in our experiments are AUC, MRR, nDCG@5 and nDCG@10, which are standard metrics for recommendation result evaluation. Each experiment was repeated 10 times. 3601 Overall LibFM DSSM Wide&Deep DeepFM DFM GRU DKN NPA NAML LSTUR NRMS Overlap Users Unseen Users AUC MRR n"
2020.acl-main.331,D16-1264,0,0.0417148,"and body. It is not appropriate to simply representing them using IDs, and it is important to understand their content from their texts (Kompan and Bielikov´a, 2010). Third, there is no explicit rating of news articles posted by users on news platforms. Thus, in news recommendation users’ interest in news is usually inferred from their click behaviors in an implicit way (Ilievski and Roy, 2013). A large-scale and high-quality dataset can significantly facilitate the research in an area, such as ImageNet for image classification (Deng et al., 2009) and SQuAD for machine reading comprehension (Rajpurkar et al., 2016). There are several public datasets for traditional recommendation tasks, such as Amazon dataset1 for product recommendation and MovieLens dataset2 for movie recommendation. Based on these datasets, many well-known recommendation methods have been developed. However, existing studies on news recommendation are much fewer, and many of them are conducted on proprietary datasets (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a). Although there are a few public datasets for news recommendation, they are usually in small size and most of them are not in English. Thus, a public 1 2 http://jm"
2020.acl-main.331,D19-1671,1,0.849233,"Missing"
2020.acl-main.331,N16-1174,0,0.052786,"dy (AMV) + Body + Cat. (AMV) + Body + Cat. + Ent. (AMV) AUC MRR nDCG@5 nDCG@10 66.22 64.17 66.32 67.07 67.09 67.23 67.38 67.50 67.60 31.92 30.49 31.88 32.34 32.40 32.41 32.37 32.43 32.51 34.53 32.81 34.42 34.98 35.03 35.04 35.12 35.21 35.24 40.23 38.57 40.22 40.74 40.80 40.83 40.79 40.96 41.03 Table 5: News representation with different news information. “Abs.”, “Cat.” and “Ent.” mean abstract, category and entity, respectively. Figure 3: BERT for news representation. ding (Avg-Emb), CNN, LSTM and multi-head selfattention (Self-Att). Since attention mechanism is an important technique in NLP (Yang et al., 2016), we also apply it to the aforementioned neural text representation methods. The results are in Table 4. We have several findings from the results. First, neural text representation methods such as CNN, Self-Att and LSTM can outperform traditional text representation methods like TF-IDF and LDA. This is because the neural text representation models can be learned with the news recommendation task, and they can capture the contexts of texts to generate better news representations. Second, Self-Att and LSTM outperform CNN in news representation. This is because multi-head self-attention and LSTM"
2020.acl-main.344,D16-1133,0,0.0207171,"ich makes the MT model access to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speec"
2020.acl-main.344,N19-1006,0,0.126859,"chained together, or an end-to-end approach, where a single model converts the source language audio sequence to the target language text sequence directly (Berard et al., 2016). Due to the alleviation of error propagation and lower latency, the end-to-end ST model has been a hot topic in recent years. However, large paired data of source audios and target sentences are required to train such a model, which is not easy to satisfy for most language pairs. To address this ∗ Works are done during internship at Microsoft issue, previous works resort to pre-training technique (Berard et al., 2018; Bansal et al., 2019), where they leverage the available ASR and MT data to pre-train an ASR model and an MT model respectively, and then initialize the ST model with the ASR encoder and the MT decoder. This strategy can bring faster convergence and better results. The end-to-end ST encoder has three essential roles: transcribe the speech, extract the syntactic and semantic knowledge of the source sentence and then map it to a semantic space, based on which the decoder can generate the correct target sentence. These pose a heavy burden to the encoder, which can be alleviated by pre-training. However, we argue that"
2020.acl-main.344,2004.iwslt-evaluation.13,0,0.0588685,"sing sentencepiece (Kudo and Richardson, 2018) with a fixed size of 5k tokens for all languages, and the punctuation is removed. For ST task, we normalize the punctuation using Moses and use the character-level vocabulary due to its better performance (Berard et al., 2018). Since there is no human-annotated segmentation provided in the IWSLT tst2013, we use two methods to segment the audios: 1) Following ESPnet, we segment each audio with the LIUM SpkDiarization tool (Meignier and Merlin, 2010). For evaluation, the hypotheses and references are aligned using the MWER method with RWTH toolkit (Bender et al., 2004). 4 3732 https://github.com/espnet/espnet 2) We perform sentence-level force-alignment between audio and transcription using aeneas5 tool and segment the audio according to alignment results. 4.2 Baselines Experiments are conducted in two settings: base setting and expanded setting. In base setting, only the corpus described in Section 4.1 is used for each task. In the expanded setting, additional ASR and/or MT data can be used. All results are reported on case-insensitive BLEU with the multibleu.perl script unless noted. 4.2.1 End-to-End ST Baselines We mainly compare our method with the conv"
2020.acl-main.344,N19-1423,0,0.0284924,"s the CTC loss Lctc and the cross-entropy loss LCE : LASR = αLCT C + (1 − α)LCE = −α log Pctc (y s |x) − (1 − α) log Ps2s (y s |x) (1) In this work, we set α to 0.3. The CTC loss works on the encoder output and it pushes the encoder to learn frame-wise alignment between speech with words. 3.3 Advanced Courses: Understanding and Word Mapping With the ability of transcription, we further propose two new tasks for the advanced courses. 3.3.1 Frame-based Masked Language Model The design of the Frame-based Masked Language Model task is inspired by the Masked Language Model (MLM) objective of BERT (Devlin et al., 2019) and semantic mask for ASR task (Wang et al., 2019a). This task enables the encoder to understand the inner meaning of a segment of speech. As shown in Figure 2, we first perform forcealignment between the speech and the transcript sentence to determine where in time particular words occur in the speech segment. For each word yis , we obtain its corresponding start position si and the end position ei in the sequence x according to force alignment results. At each training iteration, we randomly sample some percentage of the words in the y s and denote the selected word set as y ˜s . s s Next,"
2020.acl-main.344,N16-1109,0,0.241841,"to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speech translation, where"
2020.acl-main.344,L18-1001,0,0.23356,"Missing"
2020.acl-main.344,1998.amta-tutorials.1,0,0.75262,"Missing"
2020.acl-main.344,rousseau-etal-2014-enhancing,0,0.0514647,"Missing"
2020.acl-main.344,Q19-1020,0,0.126466,"2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Liu et al., 2019; Jia et al., 2019). However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, Wang et al. (2019b) propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. Kano et al. (2017) propose structured-based curriculum learning for English-Japanese speech translation, where they use a new decoder to replace the ASR decoder and to learn the output from the MT dec"
2020.acl-main.344,D18-2012,0,0.0163649,"filterbanks stacked with 3-dimensional pitch features extracted with a step size of 10ms and window size of 25ms. The features are normalized by the mean and the standard deviation for each training set. Utterances of more than 3000 frames are discarded. We perform speed perturbation with factors 0.9 and 1.1. The alignment results between speech and transcriptions are obtained by Montreal Forced Aligner (McAuliffe et al., 2017). For references pre-processing, we tokenize and lowercase all the text with the Moses scripts. For pre-training tasks, the vocabulary is generated using sentencepiece (Kudo and Richardson, 2018) with a fixed size of 5k tokens for all languages, and the punctuation is removed. For ST task, we normalize the punctuation using Moses and use the character-level vocabulary due to its better performance (Berard et al., 2018). Since there is no human-annotated segmentation provided in the IWSLT tst2013, we use two methods to segment the audios: 1) Following ESPnet, we segment each audio with the LIUM SpkDiarization tool (Meignier and Merlin, 2010). For evaluation, the hypotheses and references are aligned using the MWER method with RWTH toolkit (Bender et al., 2004). 4 3732 https://github.co"
2020.acl-main.344,D15-1166,0,0.0303917,"features, FMLM and FBLT, which explicitly teach the encoder to do source language understanding and target language meaning mapping. (3) Experiments show that both the proposed courses are helpful for speech translation, and our proposed curriculum pre-training leads to significant improvements. 2 2.1 Related Work Speech Translation Early work on speech translation used a cascade of an ASR model and an MT model (Ney, 1999; Matusov et al., 2005; Mathias and Byrne, 2006), which makes the MT model access to ASR errors. Recent successes of end-to-end models in the MT field (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and the ASR fields (Chan et al., 2016; Chiu et al., 2018) inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues. In this research line, Berard et al. (2016) give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance (Anastasopoulos et al., 2016; Duong et al., 2016; Berard et al., 2018; Weiss et al., 2017; Bansal et al., 2018, 2019; Sperber et al., 2019; Li"
2020.acl-main.392,P19-1033,1,0.583974,"sponding author:Chuan Shi(shichuan@bupt.edu.cn) https://news.google.com/ A core problem in news recommendation is how to learn better representations of users and news. Recently, many deep learning based methods have been proposed to automatically learn informative user and news representations (Okura et al., 2017; Wang et al., 2018). For instance, DKN (Wang et al., 2018) learns knowledge-aware news representation via multi-channel CNN and gets a representation of a user by aggregating her clicked news history with different weights. However, these methods (Wu et al., 2019b; Zhu et al., 2019; An et al., 2019) usually focus on news contents, and seldom consider the collaborative signal in the form of high-order connectivity underlying the user-news interactions. Capturing high-order connectivity among users and news could deeply exploit structure characteristics and alleviate the sparsity, thus improving the rec4255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4255–4264 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ommendation performance (Wang et al., 2019). For example, as shown in Figure 1, the high-order relationship u1 –d1"
2020.acl-main.392,D14-1181,0,0.00246914,"documents that have not previously existed in the user-news interaction graph G during training or testing. Our model takes these news documents as isolated nodes in the graph G. Their representations are based on only content feature hd without neighbor aggregation, and can also be disentangled via Eq. 3. 5.2 Performance Evaluation We evaluate the performance of our model GNUD by comparing it with the following state-of-the-art baseline methods: LibFM (Rendle, 2012), a feature-based matrix factorization method, with the concatenation of TF-IDF vectors of news title and profile as input. CNN (Kim, 2014), applying two parallel CNNs to word sequences in news titles and profiles respectively and concatenate them as news features. The user representation is learned from the user’s news history. DSSM (Huang et al., 2013), a deep structured semantic model. In our experiments, we model the user’s clicked news as the query and the candidate news as the documents. Wide & Deep (Cheng et al., 2016), a deep model for recommendation which combines a (Wide) linear model and (Deep) feed-forward neural network. We also use the concatenation of news title and profile embeddings as features. DeepFM (Guo et al"
2020.acl-main.392,D19-1671,1,0.648001,"Missing"
2020.acl-main.392,D18-1430,1,0.833329,"latent variable which can be inferred in an iterative process. The motivation of the iterative process is as follows. Given zu,k , the value of the latent variables {rd,k : 1 ≤ k ≤ K, (u, d) ∈ E} can be obtained by measuring the similarity between user u and her clicked news d under the k-th subspace, which is computed as Eq. 4. Initially, we set zu,k = su,k . On the other hand, after obtaining the latent variables {rd,k }, we can find an estimate of zu,k by aggregating information from the clicked news, which is computed as Eq. 5: 4258 Algorithm 1 Neighborhood Routing Algorithm According to (Yang et al., 2018), the mutual information maximization can be converted to the following form. Given the representation of a user u in k-th (1 ≤ k ≤ K) latent subspace, the preference regularizer P (k|zu,k ) estimates the probability of the k-th subspace (w.r.t. the k-th preference) that zu,k belongs to: Require: S si,k , i ∈ {u} {d : (u, d) ∈ E}, 1 ≤ k ≤ K; Ensure: zu,k , 1 ≤ k ≤ K; 1: ∀k = 1, ...K, zu,k ← su,k 2: for T iterations do 3: for d that satisfies (u, d) ∈ E do 4: ∀k = 1, · · · , K : rd,k ← z> u,k sd,k 5: ∀k = 1, · · · , K : rd,k ← softmax(rd,k ) 6: end for 7: for factor k = 1, P 2, ...K do 8: zu,k"
2020.acl-main.531,P17-2021,0,0.0218203,"oleft (R2L) and left-to-right (L2R) to improve the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source text. On top of soft templa"
2020.acl-main.531,P19-1122,0,0.0955751,"outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman"
2020.acl-main.531,Q18-1031,0,0.0459095,"ion Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of tags and target words. The templates are soft because no explicit paradigms are inaugurated to build new translation from them, and the target tokens could be modified. In order to effectively use the templates, we introduce soft template-based neural machine translation (ST-NMT), which can use source text and soft templates to predict the final translation. Our approach can be split"
2020.acl-main.531,I17-1013,0,0.0171418,"ces, we adopt the constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding auth"
2020.acl-main.531,P18-1015,0,0.04749,"Missing"
2020.acl-main.531,P07-2045,0,0.0108457,"validation set is devtest2014, and the test set is newstest2014. ASPEC Japanese-Chinese We use 0.67M sentence pairs from ASPEC Japanese-Chinese corpus (Nakazawa et al., 2016) 2 . We use the devtest as the development data, which contains 2090 sentences, and the test data contains 2107 sentences with a single reference per source sentence. 3.2 Preprocessing and Training Details LDC Chinese-English The base Transformer model is used for this task, which includes 6 layers, each layer of which has the hidden dimensions of 512, feedforward dimensions of 2048 , and 8 attention heads. We use Moses (Koehn et al., 2007) to tokenize English sentences and our in-house tool to tokenize Chinese sentences. We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to encode 1 LDC2002E17, LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E17, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T06, LDC2004T08, LDC2005T10 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ sentences using a shared vocabulary of 40K symbols. IWSLT14 German-English We adopt the small setup of the Transformer model. The model has 6 layers with the embedding size of 512, a feedforward size of 1024, and 4 attention he"
2020.acl-main.531,N16-1046,0,0.0389191,"Missing"
2020.acl-main.531,P18-1008,0,0.301326,"as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt the constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 20"
2020.acl-main.531,P14-5010,0,0.00243224,"et al., 2017), we set the learning rate as 0.1. Chinese and Japanese sentences are tokenized with our in-house tools and encoded by BPE with a shared vocabulary of 10K symbols. 3.3 Evaluation We evaluate the performance of the translation results. The evaluation metric is BLEU (Papineni et al., 2002). For the Chinese-English and GermanEnglish translation tasks, we use case-insensitive tokenized BLEU scores. For the English-German translation task, we use case-sensitive tokenized BLEU scores for evaluation. All the experiments last for 150 epochs and use Stanford parser to generate templates (Manning et al., 2014). For all translation tasks, we use the checkpoint, which has the best valid performance on the valid set. For different test sets, we adapt the beam size and the length penalty to get better performance. In order to avoid the difference of the tokenizer for Chinese translation result evaluation, we adopt the character-level BLEU for testing. Checkpoint averaging is not used, except notification. 5983 Zh → En MT06 MT03 MT05 MT08 MT12 Avg. ConvS2S (Gehring et al., 2017) GNMT (Wu et al., 2016) 39.98 40.53 42.25 42.88 41.22 42.73 33.43 33.97 32.21 32.55 37.28 38.03 Transformer (our implementation"
2020.acl-main.531,P18-1068,0,0.115432,"t al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituency"
2020.acl-main.531,D17-1090,1,0.803851,"nced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of tags and target words. The templates are"
2020.acl-main.531,P16-1078,0,0.0162146,"019b,a) use the right-toleft (R2L) and left-to-right (L2R) to improve the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source"
2020.acl-main.531,D18-1048,0,0.0726954,"se tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing ba"
2020.acl-main.531,C16-1172,0,0.0577942,"Missing"
2020.acl-main.531,W18-6301,0,0.0136486,"a label smoothing of 0.1. We use BPE to encode sentences with a shared vocabulary of 10K symbols. WMT14 English-German We use the big setting of Transformer (Vaswani et al., 2017), in which both the encoder and the decoder have 6 layers, with the embedding size of 1024, feedforward size of 4096, and 16 attention heads. The dropout rate is fixed as 0.3. We adopt Adam (Kingma and Ba, 2015) optimizer with a learning rate 0.1 of the similar learning rate schedule as Transformer (Vaswani et al., 2017). We set the batch size as 6000 and the update frequency as 16 on 8 GPUs for updating parameters (Ott et al., 2018) to imitate 128 GPUs. The datasets are encoded by BPE with a shared vocabulary (Sennrich et al., 2016) of 40K symbols. ASPEC Japanese-Chinese We use the base setting of Transformer the same to the ChineseEnglish translation task. Following the similar learning rate schedule (Vaswani et al., 2017), we set the learning rate as 0.1. Chinese and Japanese sentences are tokenized with our in-house tools and encoded by BPE with a shared vocabulary of 10K symbols. 3.3 Evaluation We evaluate the performance of the translation results. The evaluation metric is BLEU (Papineni et al., 2002). For the Chine"
2020.acl-main.531,P18-1123,0,0.0676718,"19), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of tags and target words. The templates are soft because no explicit paradigms are inaugurated to build new translation from them, and the target tokens could be modified. In order to effectively use the templates, we introduce soft template-based neural machine translation (ST-NMT), which can use source text and"
2020.acl-main.531,P02-1040,0,0.10654,"pdating parameters (Ott et al., 2018) to imitate 128 GPUs. The datasets are encoded by BPE with a shared vocabulary (Sennrich et al., 2016) of 40K symbols. ASPEC Japanese-Chinese We use the base setting of Transformer the same to the ChineseEnglish translation task. Following the similar learning rate schedule (Vaswani et al., 2017), we set the learning rate as 0.1. Chinese and Japanese sentences are tokenized with our in-house tools and encoded by BPE with a shared vocabulary of 10K symbols. 3.3 Evaluation We evaluate the performance of the translation results. The evaluation metric is BLEU (Papineni et al., 2002). For the Chinese-English and GermanEnglish translation tasks, we use case-insensitive tokenized BLEU scores. For the English-German translation task, we use case-sensitive tokenized BLEU scores for evaluation. All the experiments last for 150 epochs and use Stanford parser to generate templates (Manning et al., 2014). For all translation tasks, we use the checkpoint, which has the best valid performance on the valid set. For different test sets, we adapt the beam size and the length penalty to get better performance. In order to avoid the difference of the tokenizer for Chinese translation re"
2020.acl-main.531,P16-1162,0,0.428682,"-Chinese corpus (Nakazawa et al., 2016) 2 . We use the devtest as the development data, which contains 2090 sentences, and the test data contains 2107 sentences with a single reference per source sentence. 3.2 Preprocessing and Training Details LDC Chinese-English The base Transformer model is used for this task, which includes 6 layers, each layer of which has the hidden dimensions of 512, feedforward dimensions of 2048 , and 8 attention heads. We use Moses (Koehn et al., 2007) to tokenize English sentences and our in-house tool to tokenize Chinese sentences. We use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to encode 1 LDC2002E17, LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E17, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T06, LDC2004T08, LDC2005T10 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ sentences using a shared vocabulary of 40K symbols. IWSLT14 German-English We adopt the small setup of the Transformer model. The model has 6 layers with the embedding size of 512, a feedforward size of 1024, and 4 attention heads. In order to prevent overfitting, we use a dropout of 0.3, a l2 weight decay of 10−4 , and a label smoothing of 0.1. We use BPE to enco"
2020.acl-main.531,D19-1633,0,0.0843568,"tes and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrate the effectiveness of soft target templates. 1 Target Recently, neural machine translation (NMT) (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018) has achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pan"
2020.acl-main.531,D18-1037,0,0.0372152,"Missing"
2020.acl-main.531,P19-1207,0,0.347103,"achieved significant progress. Some advanced models (Chatterjee et al., 2016; Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Geng et al., 2018; Zhou et al., 2019a) predict the ultimate translation by multi-pass generation conditioned on the previous text such as CMLMs (Ghazvininejad et al., 2019), ABD-NMT (Zhang et al., 2018), SynST (Akoury et al., 2019), and Deliberation Network (Xia et al., 2017). Inspired by these works and the successful application of templates for other intriguing tasks, including semantic parsing (Dong and Lapata, 2018), summarization (Cao et al., 2018; Wang et al., 2019a), question answering (Duan et al., 2017; Contribution during internship at Microsoft Research Asia. † Corresponding author. like playing basketball Figure 1: An example of template guided translation results. S denotes subject and VP denotes verb phrase. Introduction ∗ I Template S like VP Pandey et al., 2018), and other text generation tasks (Wiseman et al., 2018; Guu et al., 2018), we assume the candidate templates of the target sentences can guide the sentence translation process. We denote these templates extracted from the constituencybased parse tree as soft templates, which consist of"
2020.acl-main.531,D18-1509,0,0.018984,"e the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source text. On top of soft templates and source text, we incorporate"
2020.acl-main.531,D18-1356,0,0.0718155,"Missing"
2020.acl-main.531,P17-1065,1,0.691077,"t (L2R) to improve the quality of machine translation. Non-Autoregressive decoding (Ghazvininejad et al., 2019) first predicts the target tokens and masked tokens, which will be filled in the next iterations. Then, the model predicts the unmasked tokens on top of the source text and a mixed translation consisting of the masked and unmasked tokens. Semi-autoregressive also (Akoury et al., 2019) predicts chunked fragments or the unmasked tokens based on the tree structure before the final translation. In addition, there are many existing works (Eriguchi et al., 2016; Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018; Dong and Lapata, 2018; Wang et al., 2018; Gu et al., 2018) which incorporate syntax information or the tree structure into NMT to improve the quality of translation results. 5 Conclusion In this work, we propose a novel approach that utilizes source text and additional soft templates. More specifically, our approach can extract the templates from the sub-tree, which derives from the specific depth of the constituency-based parse tree. Then, we use a Transformer model to predict the soft target templates conditioned on the source text. On top of soft templates and source te"
2020.acl-main.539,D13-1160,0,0.0230982,"-module is generated, which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50."
2020.acl-main.539,P18-1071,0,0.0989351,"Missing"
2020.acl-main.539,D18-1192,0,0.0336961,"ate). In LPA settings, (Weighted) Voting means assigning each program with (score-weighted) equal weight to vote for the final result. Ranking means using the result generated by the top program ranked by the discriminator. As shown in Figure 3, a program in TABFACT is structural and follows a grammar with over 50 functions. To effectively capture the structure of the program and also generate legitimate programs following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig"
2020.acl-main.539,P17-1003,0,0.0467597,"which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50.5 50.4 56.2 57.0 65."
2020.acl-main.539,D19-1603,0,0.11077,"Missing"
2020.acl-main.539,D18-2002,0,0.014426,"grams following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Foll"
2020.acl-main.539,D18-1010,0,0.0610233,"used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fa"
2020.acl-main.539,D14-1162,0,0.0848898,"-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Following Chen et al. (2019), we employ the label of veracity to guide the learning process of the semantic parser. We also employ programs produced by LPA (Latent Program Algorithm) for comparison, which is provided by Chen et al. (2019). In the training process, we train the semantic parser and the claim verification model separately. The training of semantic parser includes two steps: candidate search and sequence-to-action learning. For candidate search, we closely follow LPA by first collecting"
2020.acl-main.539,D17-1317,0,0.036168,"ost influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural abstractive summarization systems (Goodrich et al., 2019; Kry´sci´nski et al., 2019), as well as the use of this factual accuracy as a reward"
2020.acl-main.539,P13-1045,0,0.0675626,"gical form, in a semantic parsing manner (Liang, 2016). Then, our system builds a heterogeneous graph to capture the connections among the statement, the table and the program. Such connections reflect the related context of each token in the graph, which are used to define attention masks in a Transformer-based (Vaswani et al., 2017) framework. The attention masks are used to learn graph-enhanced contextual representations of tokens1 . We further develop a program-guided neural module network to capture the structural and compositional semantics of the program for semantic compositionality. (Socher et al., 2013; Andreas et al., 2015). Graph nodes, whose representations are computed using the contextual representations of their constituents, are considered as arguments, and logical operations are considered as modules to recursively produce representations of higher level nodes along the program. Experiments show that our system outperforms previous systems and achieves the state-of-the-art verification accuracy. The contributions of this paper can be summarized as follows: • We propose LogicalFactChecker, a graphbased neural module network, which utilizes logical operations for fact-checking. 1 Here"
2020.acl-main.539,D19-1216,0,0.0218259,"nction of difference time, which is not covered by the current set. 5 Related Work There is a growing interest in fact checking in NLP with the rising importance of assessing the truthfulness of texts, especially when pre-trained language models (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019) are more and more powerful in generating fluent and coherent texts. Previous studies in the field of fact checking differ in the genres of supporting evidence used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al.,"
2020.acl-main.539,N18-1074,0,0.15998,"Missing"
2020.acl-main.539,D19-6601,0,0.0280888,"a et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural"
2020.acl-main.539,D19-5316,0,0.0844217,": An example of table-based fact checking. Given a statement and a table as the input, the task is to predict the label. Program reflects the underlying meaning of the statement, which should be considered for fact checking. Introduction Fact checking for textual statements has emerged as an essential research topic recently because of the unprecedented amount of false news and rumors spreading through the internet (Thorne et al., 2018; ∗ Work done while this author was an intern at Microsoft Research. Chen et al., 2019; Goodrich et al., 2019; Nakamura et al., 2019; Kry´sci´nski et al., 2019; Vaibhav et al., 2019). Online misinformation may manipulate people’s opinions and lead to significant influence on essential social events like political elections (Faris et al., 2017). In this work, we study fact checking, with the goal of automatically assessing the truthfulness of a textual statement. The majority of previous studies in fact checking mainly focused on making better use of the meaning of words, while rarely considered symbolic reasoning about logical operations (such as “count”, “superlative”, “aggregation”). However, modeling logical operations is an essential step towards the modeling of compl"
2020.acl-main.539,P17-1041,0,\N,Missing
2020.acl-main.539,D18-1266,0,\N,Missing
2020.acl-main.544,2020.acl-main.23,0,0.0747168,"Missing"
2020.acl-main.544,2021.ccl-1.108,0,0.0750586,"Missing"
2020.acl-main.544,P18-2119,0,0.0142462,"at focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recentl"
2020.acl-main.544,D16-1031,0,0.027836,"and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-cen"
2020.acl-main.544,N16-1098,0,0.0228979,"ated text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP com"
2020.acl-main.544,P18-1043,0,0.387556,"two kinds of reasonable inferences for the event under different background knowledge that is absent in the dataset. Introduction Inferential text generation aims to understand dailylife events and generate texts about their underlying causes, effects, and mental states of event participants, which is crucial for automated commonsense reasoning. Taking Figure 1 as an example, given an event “PersonX reads PersonY’s diary”, the cause of the participant “PersonX” is to “obtain Person Y’s secrets” and the mental state of “PersonX” is “guilty”. Standard approaches for inferential text generation (Rashkin et al., 2018; Sap et al., 2019; Bosselut et al., 2019; Du et al., 2019) typically only ∗ Work done while this author was an intern at Microsoft Research. take the event as the input, while ignoring the background knowledge that provides crucial evidence to generate reasonable inferences. For example, if the background knowledge of this example is “PersonY invites PersonX to read his diary”, the outputs should be different. In this paper, we present an evidence-aware generative model, which ﬁrst retrieves relevant evidence from a large text corpus and then leverages retrieved evidence to guide the generati"
2020.acl-main.544,L16-1233,0,0.0270969,"y using better semantic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and"
2020.acl-main.544,D17-1006,0,0.0182539,"ic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Gr"
2020.acl-main.544,D18-1009,0,0.022466,"different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 20"
2020.acl-main.544,D16-1050,0,0.0283868,"erential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse"
2020.acl-main.544,P17-1061,0,0.0188185,"of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-centered If-Then reasoning is the most related to our work, which introduces an additio"
2020.acl-main.549,D14-1059,0,0.0802266,"ly assessing the truthfulness of a textual claim by looking for textual evidence. Introduction Internet provides an efficient way for individuals and organizations to quickly spread information to massive audiences. However, malicious people spread false news, which may have significant influence on public opinions, stock prices, even presidential elections (Faris et al., 2017). Vosoughi et al. (2018) show that false news reaches more people ∗ Work done while this author was an intern at Microsoft Research. Previous works are dominated by natural language inference models (Dagan et al., 2013; Angeli and Manning, 2014) because the task requires reasoning of the claim and retrieved evidence sentences. They typically either concatenate evidence sentences into a single string, which is used in top systems in the FEVER challenge (Thorne et al., 2018b), or use feature fusion to aggregate the features of isolated evidence sentences (Zhou et al., 2019). However, both methods fail to capture rich semantic-level structures among multiple evidence, which also prevents the use of deeper reasoning model for fact checking. In Figure 1, we give a motivating example. Making the correct prediction requires a model to reaso"
2020.acl-main.549,W04-2412,0,0.194303,"Missing"
2020.acl-main.549,N16-1138,0,0.0557513,") Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of evidence from Wikipedia for making the conclusion. FEVER contains 185,445 annotated instances, which to the best of our knowledge is the largest benchmark dataset in this area. The majority of participating teams in the FEVER chall"
2020.acl-main.549,W18-5516,0,0.192352,"by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning module have model innovations of taking the graph information into consideration. Instead of trai"
2020.acl-main.549,2021.ccl-1.108,0,0.0443279,"Missing"
2020.acl-main.549,P14-1095,0,0.05237,"ce Medal of Honor , an astronaut must perform feats of extraordinary accomplishment while participating in space flight under the authority of NASA . Tuples: ('awarded', 'the Congressional Space Medal Evidence #2 of Honor’) ('To be awarded the Congressional Space Medal of Honor',’an astronaut','perform','feats of extraordinary accomplishment’) ('an astronaut', 'participating','in space flight','under the authority of NASA' ) Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Ra"
2020.acl-main.549,W18-5527,0,0.0209756,"EVER challenge (Thorne et al., 2018b) use the same pipeline consisting of three components, namely document selection, evidence sentence selection, and claim verification. In document selection phase, participants typically extract named entities from a claim as the query and use Wikipedia search API. In the evidence selection phase, participants measure the similarity between the claim and an evidence sentence candidate by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the informati"
2020.acl-main.549,D17-1317,0,0.0495327,"4). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of evidence from Wikipedia for making the conclusion. FEVER contains 185,445 annotated instances, which to the best of our knowledge is the largest benchmark dataset in this area. The majority of participating teams in the FEVER challenge (Thorne et al., 2018b) use the same pipeline consisting of three components, namely document selection, evidence sentence selection, and claim verification. In document selection phase, par"
2020.acl-main.549,N18-2074,0,0.154527,"rd w5j from s5 are connected on the graph, simply concatenating evidence sentences as a single string fails to capture their semantic-level structure, and would give a large distance to w1i and w5j , which is the number of words between them across other three sentences (i.e., s2 , s3 , and s4 ). An intuitive way to achieve our goal is to define an N × N matrix of distances of words along the graph, where N is the total number of words in the evidence. However, this is unacceptable in practice because the representation learning procedure will take huge memory space, which is also observed by Shaw et al. (2018). In this work, we adopt pre-trained model XLNet (Yang et al., 2019) as the backbone of our approach because it naturally involves the concept of relative position5 . Pre-trained models capture rich contextual representations of words, which is helpful for our task which requires sentence-level reasoning. Considering the aforementioned issues, we implement an approximate solution to trade off between the efficiency of implementation and the informativeness of the graph. Specifically, we reorder evidence sentences with a topology sort algorithm with the intuition that closely linked nodes shoul"
2020.acl-main.549,D18-1209,0,0.108334,"Missing"
2020.acl-main.549,N18-1074,0,0.293102,"Missing"
2020.acl-main.549,W18-5501,0,0.137843,"Missing"
2020.acl-main.549,W14-2508,0,0.231712,"al Evidence #2 of Honor’) ('To be awarded the Congressional Space Medal of Honor',’an astronaut','perform','feats of extraordinary accomplishment’) ('an astronaut', 'participating','in space flight','under the authority of NASA' ) Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of"
2020.acl-main.549,D18-1010,0,0.18542,"st to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning module have model innovations of taking the graph information into consideration. Instead of training each component separately, Yin and Roth (2018) show that joint learning could improve both claim verification and evidence selection. 7 Conclusion In this work, we present a graph-based approach for fact checking. When assessing the veracity of a claim giving multiple evidence sentences, our approach is built upon an automatically constructed graph, which is derived based on semantic role labeling. To better exploit the graph information, we propose two graph-based modules, one for calculating contextual word embeddings using graph-based distance in XLNet, and the other for learning representations of graph components and reasoning over t"
2020.acl-main.549,W18-5515,0,0.222915,", participants measure the similarity between the claim and an evidence sentence candidate by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning m"
2020.acl-main.549,P19-1085,0,0.40704,"sidential elections (Faris et al., 2017). Vosoughi et al. (2018) show that false news reaches more people ∗ Work done while this author was an intern at Microsoft Research. Previous works are dominated by natural language inference models (Dagan et al., 2013; Angeli and Manning, 2014) because the task requires reasoning of the claim and retrieved evidence sentences. They typically either concatenate evidence sentences into a single string, which is used in top systems in the FEVER challenge (Thorne et al., 2018b), or use feature fusion to aggregate the features of isolated evidence sentences (Zhou et al., 2019). However, both methods fail to capture rich semantic-level structures among multiple evidence, which also prevents the use of deeper reasoning model for fact checking. In Figure 1, we give a motivating example. Making the correct prediction requires a model to reason based on the understanding that “Rodney King riots” is occurred in “Los Angeles County” from the first evidence, and that “Los Angeles County” is “the most populous county 6170 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6170–6180 c July 5 - 10, 2020. 2020 Association for Computa"
2020.acl-main.599,P19-1620,0,0.156639,"nswer” should be given if there is no suitable short answer. 2.2 • We achieve state-of-the-art performance on both long and short answer leaderboard of NQ at the time of submission (Jun. 25th, 2019), and our model surpasses single human performance on the development dataset at both long and short answer criteria. Preliminary Data Preprocessing Since the average length of the documents in NQ is too long to be considered as one training instance, we first split each document into a list of document fragments with overlapping windows of tokens, like in the original BERT model for the MRC tasks (Alberti et al., 2019b; Devlin et al., 2019). Then we generate an instance from a document fragment by concatenating a “[CLS]” token, tokenized question, a “[SEP]” token, tokens from the content of the doc6709 Output Layer Document Fragment Add & Norm Paragraph Feed-Forward Sentence Add & Norm Concatenate Token Graph Integration Token-Level Self-Attention Sentence-Level Self-Attention N× Paragraph-Level Self-Attention Figure 4: The graph on the left is an illustration of the graph integration layer. The graph on the right shows the incoming information when updating a paragraph node. The solid lines represent the"
2020.acl-main.599,P17-1171,0,0.270549,"al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches, they treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark, while we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results. According to Kwiatkowski et al. (2019), a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer. For instance,"
2020.acl-main.599,P16-1046,0,0.238693,"agraphs 6708 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained joi"
2020.acl-main.599,P17-1147,0,0.0527221,"dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al"
2020.acl-main.599,N18-2075,0,0.0309702,"normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani e"
2020.acl-main.599,Q19-1026,0,0.165659,"ar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches,"
2020.acl-main.599,N18-2078,0,0.0307711,"k et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different"
2020.acl-main.599,P18-1078,0,0.295567,"ides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answe"
2020.acl-main.599,P17-1055,1,0.862058,"ns that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extrac"
2020.acl-main.599,N18-1158,0,0.0177026,"ion for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote each other. At inference time, we use a pipeline s"
2020.acl-main.599,D16-1244,0,0.153285,"Missing"
2020.acl-main.599,N16-1174,0,0.432339,"of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote eac"
2020.acl-main.599,D16-1264,0,0.0512332,"ong and short answer criteria. 1 Figure 1: An example from NQ dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to selec"
2020.acl-main.599,K19-1074,0,0.0605159,"Missing"
2020.acl-main.599,D16-1103,0,0.0241031,"Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have s"
2020.acl-main.599,N18-2074,0,0.0339776,"graph integration layer and pass it to the feedforward layer. 6711 3.3.4 Feed-Forward Layer Following the inner structure of the transformer (Vaswani et al., 2017), we also utilize an additional fully connected feed-forward network at the end of our graph encoder. It consists of two linear transformations with a GELU activation in between. GELU is Gaussian Error Linear Unit activation (Hendrycks and Gimpel, 2016), and we use GELU as the non-linear activation, which is consistent with BERT. 3.3.5 Inspired by positional encoding in Vaswani et al. (2017) and relative position representations in Shaw et al. (2018), we introduce a novel relational embedding on our constructed graph, which aims at modeling the relative position information between nodes on the multi-granularity document structure. We make the edges in our document modeling graph to embed relative positional information. We modify equation 1 and 2 for eij and z i to introduce our relational embedding as follows: eij = zi = X αij  h0i = j∈Ni ,oj +1=oi  h0j + aij + boi , Output Layer The objective function is defined as the negative sum of the log probabilities of the predicted distributions, averaged over all the training instances. The"
2020.acl-main.599,D18-1246,0,0.0199332,"evel encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different 6715 types of edges in the graph. 6 Jaco"
2020.acl-main.599,P18-1030,0,0.0200399,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D18-1244,0,0.031553,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D15-1167,1,0.734635,"Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibil"
2020.acl-main.599,P18-1158,0,0.0352041,"l., 2017; Lai et al., 2017; Trischler et al., 2017; Yang et al., 2018). Lots of work has begun to build end-to-end deep learning models and has achieved good results (Seo et al., 2017; Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network m"
2020.acl-main.599,W17-2623,0,\N,Missing
2020.acl-main.599,D17-1082,0,\N,Missing
2020.acl-main.599,D18-1259,0,\N,Missing
2020.acl-main.599,N19-1423,0,\N,Missing
2020.coling-main.482,P18-1031,0,0.0207804,"unsupervised settings, such as text clustering. In this paper, we propose a novel method to fine-tune pre-trained models unsupervisedly for text clustering, which simultaneously learns text representations and cluster assignments using a clustering oriented loss. Experiments on three text clustering datasets (namely TREC-6, Yelp, and DBpedia) show that our model outperforms the baseline methods and achieves stateof-the-art results. 1 Introduction Pre-trained language models have shown remarkable progress in many natural language understanding tasks (Radford et al., 2018; Peters et al., 2018; Howard and Ruder, 2018). Especially, BERT (Devlin et al., 2018) applies the fine-tuning approach to achieve ground-breaking performance in a set of NLP tasks. BERT, a deep bidirectional transformer model (Vaswani et al., 2017), utilizes a huge unlabeled data to learn complex features and representations and then fine-tunes its pre-trained model on the downstream tasks with labeled data. Although BERT has achieved great success in many natural language understanding tasks under supervised fine-tuning approaches, relatively little work has been focused on applying pre-trained models in unsupervised settings. In this p"
2020.coling-main.482,N18-1202,0,0.0409681,"pre-trained models in unsupervised settings, such as text clustering. In this paper, we propose a novel method to fine-tune pre-trained models unsupervisedly for text clustering, which simultaneously learns text representations and cluster assignments using a clustering oriented loss. Experiments on three text clustering datasets (namely TREC-6, Yelp, and DBpedia) show that our model outperforms the baseline methods and achieves stateof-the-art results. 1 Introduction Pre-trained language models have shown remarkable progress in many natural language understanding tasks (Radford et al., 2018; Peters et al., 2018; Howard and Ruder, 2018). Especially, BERT (Devlin et al., 2018) applies the fine-tuning approach to achieve ground-breaking performance in a set of NLP tasks. BERT, a deep bidirectional transformer model (Vaswani et al., 2017), utilizes a huge unlabeled data to learn complex features and representations and then fine-tunes its pre-trained model on the downstream tasks with labeled data. Although BERT has achieved great success in many natural language understanding tasks under supervised fine-tuning approaches, relatively little work has been focused on applying pre-trained models in unsuper"
2020.coling-main.492,J05-3002,0,0.11297,"her extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granularity so that the important and unimportant contents could be separated. Therefore, we argue that extracting * Contribution done during internship at Microsoft Research Asia while pursuing PhD at Harbin Institute of Technology"
2020.coling-main.492,D19-1307,0,0.023521,"Missing"
2020.coling-main.492,P18-1063,0,0.147971,"Finally, extracted sentences are factually faithful to the input document, compared with abstractive methods (Cao et al., 2018). Despite the success of extractive systems, from previous works, it is still not clear whether extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granular"
2020.coling-main.492,P16-1046,0,0.162019,"aluation of both automatic and human evaluations. Hopefully, our work could provide some inspiration of the basic extraction units in extractive summarization for future research. 1 Introduction Automatic text summarization aims to produce a brief piece of text which can preserve the most important information in it. The important contents are identified and then extracted to form the output summary (Nenkova and McKeown, 2011). In recent decades, extractive methods have proven effective in many systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015; Cheng and Lapata, 2016; Zhou et al., 2018; Nallapati et al., 2017). In previous works, extractive summarization systems perform extraction on the sentence level (Mihalcea and Tarau, 2004; Cheng and Lapata, 2016; Nallapati et al., 2017). As the extraction unit, a sentence is a grammatical unit of one or more words that express a statement, question, request, etc. There are several advantages of extracting sentences. First, extractive systems are simpler, easier to develop, and faster during run-time in real application scenarios, compared with abstractive systems. Moreover, original sentences in the input are natura"
2020.coling-main.492,D18-1409,0,0.118128,"Missing"
2020.coling-main.492,D18-1443,0,0.0291087,"2019), we randomly sampled 50 documents from the CNN/Daily Mail test set, which is the same as in §3. 1 https://github.com/abisee/cnn-dailymail 5623 6 6.1 Results Automatic Evaluation Table 3 shows the ROUGE evaluation results. We compare the SSE with the following systems: Abstractive Systems Pointer-Generator Network (PGN) (See et al., 2017) ia a sequence-to-sequence model with copy and coverage mechanisms. FastRewrite (Chen and Bansal, 2018) conducts extraction first then generation. JECS (Xu and Durrett, 2019) first extracts sentences then compresses them to reduce redundancy. Bottom-Up (Gehrmann et al., 2018) applies constrains on the copying probability. Extractive Systems LEAD3 is a commonly used baseline which simply extracts the first three sentences. T EXT R ANK (Mihalcea and Tarau, 2004) is a popular graph-based unsupervised system. S UM MA RU NN ER and NN-SE (Nallapati et al., 2017; Cheng and Lapata, 2016) use hierarchical structure for document encoding and predict sentence extraction probabilities. N EU S UM (Zhou et al., 2018) jointly model the sentence scoring and selection steps. B ERT S UM E XT, B ERT S UM E XT + T RI B LK (with trigram blocking) (Liu, 2019), S ELF -S UPERVISED (Wang"
2020.coling-main.492,N18-1065,0,0.0136492,"/TAC, CNN/Daily Mail, New York Times, etc. In this paper, we take the most commonly used dataset in recent research works (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018; Xu and Durrett, 2019; Lebanoff et al., 2019), CNN/Daily Mail, as our testbed. The statistics of it can be found in Table 2. One of the most distinguishable features of this dataset is that the output summary is in the form of highlights written by the news editors. As shown in the example in Figure 1, the summary (highlights) is a list of bullets. Therefore, extractive methods perform well on this dataset (Grusky et al., 2018). Figure 1: A screenshot example of the document-summary pair in the CNN/Daily Mail dataset. 3.2 The Drawbacks There are two main potential drawbacks of extracting sentences. First, unnecessary information is smuggled with the extracted sentences. Second, duplicate content may appear when extracting multiple sentences. To analyze whether the issues exist, we conduct experiments and analyses with both count-based statistics and human judgments. We consider two different settings to reach our final conclusion, i.e., the extractive oracle and a real extractive system. First, we check the quality"
2020.coling-main.492,P18-1249,0,0.0153127,"622 where σ(·) is the sigmoid function. The training objective of the model is the binary cross-entropy loss 00 ). given the extractive oracle label yi,j and the predicted probability p(Ci,j 5 Experiment 5.1 Dataset Following previous extractive works (Zhou et al., 2018; Xu and Durrett, 2019; Lebanoff et al., 2019; Zhang et al., 2019; Dong et al., 2018), we conduct data preprocessing using the same method1 in See et al. (2017), including sentence splitting and word tokenization. we preprocess the data as same as See et al. (2017). We then use a state-of-the-art BERT-based constituency parser (Kitaev and Klein, 2018) to process the input document whose performance is 95.17 F1 on WSJ test set. The statistic of the original CNN/Daily Mail dataset and the sub-sentential version are listed in Table 2. CNN/Daily Mail Training Dev Test #(Document) 287,227 13,368 11,490 #(Ref / Document) 1 1 1 Doc Len (Sentence) 31.58 26.72 27.05 Doc Len (Word) 791.36 769.26 778.24 Ref Len (Sentence) 3.79 4.11 3.88 Ref Len (Word) 55.17 61.43 58.31 Doc Len (Sub-Sentence) 52.84 51.37 52.02 Table 2: Data statistics of CNN/Daily Mail dataset. 5.2 Implementation Details We found that the tokenizer used in the constituency parser is d"
2020.coling-main.492,P19-1209,0,0.157846,"evel is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granularity so that the important and unimportant contents could be separated. Therefore, we argue that extracting * Contribution done during internship at Microsoft Research Asia while pursuing PhD at Harbin Institute of Technology. This work is licensed"
2020.coling-main.492,W04-1013,0,0.0329308,"Transformer layers are set to 0.1. We train the model for 4 epochs which takes about 6 hours. The final model is picked according to the performance on the development set among the 4 model checkpoints. 00 ) and select the top ones. Since the During inference, we rank the extraction units according to p(Ci,j extraction unit in this paper is shorter than full sentence, we repeatedly select next sub-sentential unit until the summary length reaches the limit. The length limit is set to 60 words according to the statistics on the development set in Table 2. 5.3 Evaluation Metric We employ ROUGE (Lin, 2004) as our evaluation metric. Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-L (LCS) as the evaluation metrics in the experimental results. Additionally, we also conduct human evaluation on the output summaries. Following previous works (Cheng and Lapata, 2016; Nallapati et al., 2017; Liu, 2019; Zhang et al., 2019), we randomly sampled 50 documents from the CNN/Daily Mail test set, which is the same as in §3. 1 https://github.com/abisee/cnn-dailymail 5623 6 6.1 Results Automatic Evaluation Table 3 shows the ROUGE evaluation results. We compare the SSE with the follo"
2020.coling-main.492,J93-2004,0,0.0696658,"a relatively complete meaning and be human-readable. Therefore, the clause nodes, such as S and SBAR, become a good choice. In this section, we introduce how to perform extraction on the sub-sentential units, and present a BERT-based model for it. 4.1 The Sub-Sentential Units In order to perform extraction on the sub-sentential units, we need to determine what units can be extracted. The proposed method is based on the constituency parsing tree. The basic idea is based on the sub-sentential clauses in the tree. In our experiments, we adopt the syntactic tagset used in the Penn Treebank (PTB) (Marcus et al., 1993). There are two main types in the PTB tagset, phrase and clause. We use the clause tag since the information in a clause is more complete than a phrase. S S ADVP NP VBD CC VP NP PP VP S , NP VP CC S VP VP …. S SBAR WHNP S VP S Figure 2: Two simplified constituency parsing trees. The nodes in circles are candidates. The final selected node is the on in red solid-lined circle. Given the parsing tree ti of sentence si , we traverse it to determine the boundary of extraction units. Specifically, every clause is treated as the extraction unit candidates. If one of its ancestors is a clause node, we"
2020.coling-main.492,W09-1801,0,0.0464396,"d grammatically correct. Finally, extracted sentences are factually faithful to the input document, compared with abstractive methods (Cao et al., 2018). Despite the success of extractive systems, from previous works, it is still not clear whether extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extract"
2020.coling-main.492,W04-3252,0,0.758707,"etitively comparing to full sentence extraction under the evaluation of both automatic and human evaluations. Hopefully, our work could provide some inspiration of the basic extraction units in extractive summarization for future research. 1 Introduction Automatic text summarization aims to produce a brief piece of text which can preserve the most important information in it. The important contents are identified and then extracted to form the output summary (Nenkova and McKeown, 2011). In recent decades, extractive methods have proven effective in many systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015; Cheng and Lapata, 2016; Zhou et al., 2018; Nallapati et al., 2017). In previous works, extractive summarization systems perform extraction on the sentence level (Mihalcea and Tarau, 2004; Cheng and Lapata, 2016; Nallapati et al., 2017). As the extraction unit, a sentence is a grammatical unit of one or more words that express a statement, question, request, etc. There are several advantages of extracting sentences. First, extractive systems are simpler, easier to develop, and faster during run-time in real application scenarios, compared with abstractive sys"
2020.coling-main.492,P17-1099,0,0.554056,"= V . 4.2.3 Training Objective With the chunk level representation vectors C 00 , the model predict the output probability of each chunk 00 : Ci,j 00 00 p(Ci,j ) = σ(Wo Ci,j + bo ) (5) 5622 where σ(·) is the sigmoid function. The training objective of the model is the binary cross-entropy loss 00 ). given the extractive oracle label yi,j and the predicted probability p(Ci,j 5 Experiment 5.1 Dataset Following previous extractive works (Zhou et al., 2018; Xu and Durrett, 2019; Lebanoff et al., 2019; Zhang et al., 2019; Dong et al., 2018), we conduct data preprocessing using the same method1 in See et al. (2017), including sentence splitting and word tokenization. we preprocess the data as same as See et al. (2017). We then use a state-of-the-art BERT-based constituency parser (Kitaev and Klein, 2018) to process the input document whose performance is 95.17 F1 on WSJ test set. The statistic of the original CNN/Daily Mail dataset and the sub-sentential version are listed in Table 2. CNN/Daily Mail Training Dev Test #(Document) 287,227 13,368 11,490 #(Ref / Document) 1 1 1 Doc Len (Sentence) 31.58 26.72 27.05 Doc Len (Word) 791.36 769.26 778.24 Ref Len (Sentence) 3.79 4.11 3.88 Ref Len (Word) 55.17 61."
2020.coling-main.492,N06-2046,0,0.385291,"categorized from different perspectives. From the perspective of having supervision or not, there are two major types: unsupervised methods and supervised methods. One of the difficulties in training an extractive system is the lack of extraction labels. The reason is that most of the reference summary is written by human experts, therefore, it is hard to find the exact appearance in the input document. Without natural training labels, unsupervised and supervised methods treat extractive summarization as different problems. Graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006) are very useful unsupervised methods. In these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Supervised methods for extractive summarization create training labels manually. (Cao et al., 2015; Ren et al., 2017) directly train regression models using ROUGE scores as the supervision. (Cheng and Lapata, 2016; Nallapati et al.,"
2020.coling-main.492,P19-1214,0,0.0131439,"2018) applies constrains on the copying probability. Extractive Systems LEAD3 is a commonly used baseline which simply extracts the first three sentences. T EXT R ANK (Mihalcea and Tarau, 2004) is a popular graph-based unsupervised system. S UM MA RU NN ER and NN-SE (Nallapati et al., 2017; Cheng and Lapata, 2016) use hierarchical structure for document encoding and predict sentence extraction probabilities. N EU S UM (Zhou et al., 2018) jointly model the sentence scoring and selection steps. B ERT S UM E XT, B ERT S UM E XT + T RI B LK (with trigram blocking) (Liu, 2019), S ELF -S UPERVISED (Wang et al., 2019) and HIBERT (Zhang et al., 2019) use pre-training techniques in extractive document summarization. BERT-SENT is the sentence-level extractive baseline described in section 3.2 . Model PGN FastRewrite JECS Bottom-Up LEAD3 T EXT R ANK ROUGE -1 ROUGE -2 ROUGE -L S UMMA RU NN ER NN-SE N EU S UM B ERT S UM E XT B ERT S UM E XT +T RI B LK S ELF -S UPERVISED HIBERT BERT-SENT SSE 39.53 40.88 41.70 41.22 40.24 40.20 39.60 41.13 41.59 42.61 43.25 41.36 42.10 42.13 42.72 17.28 17.80 18.50 18.68 17.70 17.56 16.20 18.59 19.01 19.99 20.24 19.20 19.70 19.73 20.29 36.38 38.54 37.90 38.34 36.45 36.44 35.30 37."
2020.coling-main.492,P10-1058,0,0.0396597,"cate content may appear when extracting multiple sentences. To analyze whether the issues exist, we conduct experiments and analyses with both count-based statistics and human judgments. We consider two different settings to reach our final conclusion, i.e., the extractive oracle and a real extractive system. First, we check the quality of the sentence level extractive oracle, since it is the upper bound of any extraction system. Two different methods are used in recent extractive summarization research for building the oracle training label. The first one is based on semantic correspondence (Woodsend and Lapata, 2010) of document sentences and reference summary, used in (Cheng and Lapata, 2016). The second one is heuristic, which maximizes the ROUGE score with respect to gold summaries. This one is more broadly used in many recent extractive systems (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2019; Liu, 2019). We adopt the second method since it is more widely used and easy to implement. The extractive oracle is computed with the metric of ROUGE-2 F1 score, which is also the metric used in the final automatic evaluation in these systems. Second, we check the output of a BERT-based sentence le"
2020.coling-main.492,D19-1324,0,0.334292,"ences are factually faithful to the input document, compared with abstractive methods (Cao et al., 2018). Despite the success of extractive systems, from previous works, it is still not clear whether extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granularity so that the importa"
2020.coling-main.492,P19-1499,1,0.512227,"sed methods. In these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Supervised methods for extractive summarization create training labels manually. (Cao et al., 2015; Ren et al., 2017) directly train regression models using ROUGE scores as the supervision. (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2019) search the oracle extracted sentences as the training labels. Cheng and Lapata (2016) propose treating document summarization as a sequence labeling task. They first encode the sentences in the document and then classify each sentence into two classes, i.e., extraction or not. Nallapati et al. (2017) propose a system called SummaRuNNer with more features, which also treat extractive document summarization as a sequence labeling task. Zhou et al. (2018) propose using pointer networks (Vinyals et al., 2015) to repeatedly extract sentences. Recently, Reinforcement Learning (RL) is also introduce"
2020.coling-main.492,P18-1061,1,0.620465,"ic and human evaluations. Hopefully, our work could provide some inspiration of the basic extraction units in extractive summarization for future research. 1 Introduction Automatic text summarization aims to produce a brief piece of text which can preserve the most important information in it. The important contents are identified and then extracted to form the output summary (Nenkova and McKeown, 2011). In recent decades, extractive methods have proven effective in many systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015; Cheng and Lapata, 2016; Zhou et al., 2018; Nallapati et al., 2017). In previous works, extractive summarization systems perform extraction on the sentence level (Mihalcea and Tarau, 2004; Cheng and Lapata, 2016; Nallapati et al., 2017). As the extraction unit, a sentence is a grammatical unit of one or more words that express a statement, question, request, etc. There are several advantages of extracting sentences. First, extractive systems are simpler, easier to develop, and faster during run-time in real application scenarios, compared with abstractive systems. Moreover, original sentences in the input are naturally fluent and gram"
2020.coling-main.492,D15-1042,0,\N,Missing
2020.coling-main.492,W01-0100,0,\N,Missing
2020.coling-main.492,D18-1088,1,\N,Missing
2020.coling-main.82,N18-1202,0,0.0316223,"layout variations. Recently, the rapid development of deep learning in computer vision has significantly boosted the data-driven image-based approaches for document layout analysis. Although these approaches have been widely adopted and made significant progress, they usually leverage visual features while neglecting textual features from the documents. Therefore, it is inevitable to explore how to leverage the visual and textual information in a unified way for document layout analysis. Nowadays, the state-of-the-art computer vision and NLP models are often built upon the pre-trained models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Raffel et al., 2019; Xu et al., 2019) followed by fine-tuning on specific downstream tasks, which achieves very promising results. However, pre-trained models not only require large-scale unlabeled data for self-supervised learning, but also need high quality labeled data for task-specific fine-tuning to achieve good performance. For document layout analysis tasks, there have been some image-based document layout datasets, while most of them are built for computer vision approaches and"
2020.coling-main.82,D19-1348,0,0.0586205,"Missing"
2020.emnlp-main.193,I08-2115,0,0.0513539,"ies in the field of DeepFake detection of generated text are dominated by deep-learning based document classification models and studies about discriminating features of generated text. GROVER (Zellers et al., 2019) detects generated text by a fine-tuned model of the generative model itself. Ippolito et al. (2019) fine-tune the BERT model for discrimination and explore how sampling strategies and text excerpt length affect the detection. GLTR (Gehrmann et al., 2019) develops a statistical method of computing per-token likelihoods and visualizes histograms over them to help deepfake detection. Badaskar et al. (2008) and P´erez-Rosas et al. (2017) study language distributional features including n-gram frequencies, text coherence and syntax features. Vijayaraghavan et al. (2020) study the effectiveness of different numeric representations (e.g., TFIDF and Word2Vec) and different neural networks (e.g., ANNs, LSTMs) for detection. Bakhtin et al. (2019) tackle the problem as a ranking task and study about the cross-architecture and cross-corpus generalization of their scoring functions. Schuster et al. (2019) indicate that simple provenance-based detection methods are insufficient for solving the problem and"
2020.emnlp-main.193,2021.ccl-1.108,0,0.0824142,"Missing"
2020.emnlp-main.193,P17-1161,0,0.0592401,"Missing"
2020.emnlp-main.193,N18-1074,0,0.060846,"Missing"
2020.emnlp-main.193,P19-3019,0,0.0188039,"evaluate on datasets produced by both GPT-2 and GROVER. Advances in generative models have promoted the development of detection methods. Previous studies in the field of DeepFake detection of generated text are dominated by deep-learning based document classification models and studies about discriminating features of generated text. GROVER (Zellers et al., 2019) detects generated text by a fine-tuned model of the generative model itself. Ippolito et al. (2019) fine-tune the BERT model for discrimination and explore how sampling strategies and text excerpt length affect the detection. GLTR (Gehrmann et al., 2019) develops a statistical method of computing per-token likelihoods and visualizes histograms over them to help deepfake detection. Badaskar et al. (2008) and P´erez-Rosas et al. (2017) study language distributional features including n-gram frequencies, text coherence and syntax features. Vijayaraghavan et al. (2020) study the effectiveness of different numeric representations (e.g., TFIDF and Word2Vec) and different neural networks (e.g., ANNs, LSTMs) for detection. Bakhtin et al. (2019) tackle the problem as a ranking task and study about the cross-architecture and cross-corpus generalization"
2020.emnlp-main.297,N18-1150,0,0.0196461,"d tuned the minimum summary length on the validation set in the range of [30, 80]. The search range of minimum summary length was empirically set according to the summaries of training split of CNNDM, where the average and medium minimum lengths are both around 55. We used step size of 5 to get quick feedback. Similar 3651 to the pre-training process, the datasets with less instances were fine-tuned with smaller batch sizes (i.e., 64 for NYT and 768 for CNNDM). 5 Model Lead3 BERTExt (Liu and Lapata, 2019) PTGen (See et al., 2017) DRM (Paulus et al., 2018) BottomUp (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) BERTAbs (Liu and Lapata, 2019) UniLM (Dong et al., 2019) T RANSFORMER-S2S RO BERTABASE -S2S RO BERTA-S2S RO BERTACONT -S2S Automatic Evaluation We used ROUGE (Lin, 2004) to measure the quality of different summarization model outputs. We reported full-length F1 based ROUGE1, ROUGE-2 and ROUGE-L scores on CNNDM, while we used the limited-length recall based ROUGE-1, ROUGE-2 and ROUGEL on NYT, following Durrett et al. (2016). The ROUGE scores are computed using the ROUGE-1.5.5.pl script6 . Models in Comparison Lead3 is a baseline which simply takes the first three sentences of a document as its"
2020.emnlp-main.297,P18-1063,0,0.0208635,"rage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) proposed a permutation language modeling objective that r"
2020.emnlp-main.297,D18-1443,0,0.178386,") that have been extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang e"
2020.emnlp-main.297,P16-1154,0,0.0129739,") that lead to another huge performance gain. However, extractive models have their own limitations. For example, the extracted sentences might be too long and redundant. Besides, manually written summaries in their nature are abstractive. Therefore, we focus on abstractive summarization in this paper. Abstractive Summarization This task aims to generate a summary by rewriting a document, which is a SEQ 2 SEQ learning problem. SEQ 2 SEQ attentive LSTMs (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015) are employed in Nallapati et al. (2016) that have been extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement lear"
2020.emnlp-main.297,P18-1013,0,0.0277906,"ed with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) propos"
2020.emnlp-main.297,N19-4009,0,0.0673284,"Missing"
2020.emnlp-main.320,2020.acl-main.499,0,0.0209775,"rounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification"
2020.emnlp-main.320,D19-1565,0,0.0399722,"Missing"
2020.emnlp-main.320,D16-1146,0,0.0382263,"Missing"
2020.emnlp-main.320,N19-1423,0,0.173838,"ency between sentencelevel and token-level predictions (§3.2) and textual knowledge from literal definitions of propaganda techniques (§3.3). At last, we describe the training and inference procedures (§3.4). 3.1 Base Model To better exploit the sentence-level information and further benefit token-level prediction, we develop a fine-grained multi-task method as our base model, which makes predictions for 18 propaganda techniques at both sentence level and token level. Inspired by the success of pre-trained language models on various natural language processing downstream tasks, we adopt BERT (Devlin et al., 2019) as the backbone model here. For each input sentence, the sequence is modified as “[CLS]sentence tokens[SEP ]”. Specifically, on top of BERT, we add 19 binary classifiers for finegrained sentence-level predictions, and one 19-way classifier for token-level predictions, where all classifiers are implemented as linear layers. At sentence level, we perform multiple binary classifications and this can further support leveraging declarative knowledge. The last representation of the special token [CLS] which is regarded as a summary of the semantic content of the input, is adopted to perform multipl"
2020.emnlp-main.320,Q15-1027,0,0.0123863,"ta. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural networ"
2020.emnlp-main.320,D19-1405,0,0.0184536,"h first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need a lot of training data and are not interpretable. On the other hand, logicbased expert systems are interpretable and require less or no training data. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number o"
2020.emnlp-main.320,P19-1028,0,0.154388,"ugh creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification modules to recursively construct model similar to the backward chaining algorithm of Prolog. Evans and Grefenstette (2018) develop a differentiable model of forward chaining inference, where weights represent a probability distribution over clauses. Li and Srikumar (2019) inject logic-driven neurons to existing neural networks by measuring the degree of the head being true measured by probabilistic soft logic (Kimmig et al., 2012). Our approach belongs to the first direction, and to the best of knowledge our work is the first one that augments neural network with logical knowledge for propaganda detection. 6 Conclusion In this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles. Specifically, the declarative knowledge is expressed in both first-order logic and natu"
2020.emnlp-main.320,D17-1317,0,0.0842897,"fluence an audience. 4. Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, h"
2020.emnlp-main.320,N15-1118,0,0.421682,"Missing"
2020.emnlp-main.320,N18-1074,0,0.0442108,"njection of first-order logic into neural networks. We will describe related studies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of conven"
2020.emnlp-main.320,D18-1215,0,0.0256681,"tions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second gr"
2020.emnlp-main.320,P17-2067,0,0.193758,". Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, however, they"
2020.emnlp-main.320,D19-1216,0,0.0257362,"tudies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need"
2020.emnlp-main.320,P16-1228,0,\N,Missing
2020.emnlp-main.467,P19-1620,0,0.165147,"tream model performance. Question data augmentation (QDA) aims to automatically generate context-relevant questions to further improve the model performance for the above tasks (Yang et al., 2019; Dong et al., 2019). Existing QDA methods mainly employ the round-trip 2 It can also be a document span or a passage. For notational simplicity, we use the “paragraph” to refer to it in the rest of this paper. 5798 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5798–5810, c November 16–20, 2020. 2020 Association for Computational Linguistics consistency (Alberti et al., 2019; Dong et al., 2019) to synthesize answerable questions. However, the round-trip consistency method is not able to generate context-relevant unanswerable questions, where MRC with unanswerable questions is a challenging task (Rajpurkar et al., 2018; Kwiatkowski et al., 2019). Zhu et al. (2019) firstly study unanswerable question DA, which relies on annotated plausible answer to constructs a small pseudo parallel corpus of answerable-to-unanswerable questions for unanswerable question generation. Unfortunately, most question answering (QA) and MRC datasets do not provide such annotated plausibl"
2020.emnlp-main.467,W05-0909,0,0.14492,"Missing"
2020.emnlp-main.467,K16-1002,0,0.0209034,"We compare our CRQDA against the following baselines: (1) EDA (Wei and Zou, 2019): it augments question data by performing synonym replacement, random insertion, random swap, or random deletion operation. We implement EDA with their source code5 to synthesize a new question data for each question of SQuAD 2.0; (2) Back-Translation (Yu et al., 2018; Prabhumoye et al., 2018): it uses machine translation model to translate questions into French and back into English. We implement Back-Translation based on the source code6 to generate a new question data for each original question; (3) Text-VAE (Bowman et al., 2016; Liu et al., 2019a): it uses RNNbased VAE to generate a new question data for each question of SQuAD 2.0. The implementation is based on the source code7 ; (4) AE with Noise: it uses the same autoencoder of CRQDA for question data rewriting. The only difference is that the autoencoder cannot utilize the MRC gradient but only uses a noise (sampled from Gaussian distribution) to revise the question embedding. This experiment is designed to show necessity of the pre-trained MRC. (5) 3M synth (Alberti et al., 2019): it employs round-trip consistency technique to synthesize 3M questions on SQuAD 2"
2020.emnlp-main.467,P17-1123,0,0.283174,"nd machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to the question. Because the above tasks require the model to reason about the question-paragraph pair, existing textual DA methods th"
2020.emnlp-main.467,P17-2090,0,0.0212487,"e processing (NLP) tasks remains a challenging problem. Unlike the general image DA techniques such as rotation and cropping, it is more difficult to synthesize new high-quality and diverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in N"
2020.emnlp-main.467,N10-1086,0,0.0461113,"te and Sefara, 2019) and paraphrasing (Kumar et al., 2019) are proposed to generate new textual samples. All the methods mentioned above usually generate individual sentences separately. For QDA of MRC, QG, and QNLI tasks, these DA approaches cannot guarantee the generating question are relevant to the given paragraph. In order to generate contextrelevant answerable and unanswerable questions, our CRQDA method utilizes a pre-trained MRC as guidance to revise the question in continuous embedding space, which can be seen as a special constrained paraphrasing method for QDA. Question generation (Heilman and Smith, 2010; Du et al., 2017; Zhao et al., 2018; Zhang and 5799 Bansal, 2019) is attracting attention in the field of natural language generation (NLG). However, most previous works are not designed for QDA. That is, they do not aim to generate context-relevant questions for improving downstream model performance. Compared to QG, QDA is relatively unexplored. Recently, some works (Alberti et al., 2019; Dong et al., 2019) utilize round-trip consistency technique to synthesize answerable questions. They first use a generative model to generate the question with the paragraph and answer as model input, and"
2020.emnlp-main.467,P17-1147,0,0.0344156,"Missing"
2020.emnlp-main.467,N18-2072,0,0.11471,"sks remains a challenging problem. Unlike the general image DA techniques such as rotation and cropping, it is more difficult to synthesize new high-quality and diverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC"
2020.emnlp-main.467,N19-1363,0,0.0219264,"les, including using variational autoencodes (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014), generative adversarial networks (GANs) (Tanaka and Aranha, 2019), and pretrained language generation models (Radford et al., 2019; Kumar et al., 2020; Anaby-Tavor et al., 2020). Back-translation (Sennrich et al., 2016; Yu et al., 2018) is also a major way for textual DA, which uses machine translation model to translate English sentences into another language (e.g., French), and back into English. Besides, data noising techniques (Xie et al., 2017; Marivate and Sefara, 2019) and paraphrasing (Kumar et al., 2019) are proposed to generate new textual samples. All the methods mentioned above usually generate individual sentences separately. For QDA of MRC, QG, and QNLI tasks, these DA approaches cannot guarantee the generating question are relevant to the given paragraph. In order to generate contextrelevant answerable and unanswerable questions, our CRQDA method utilizes a pre-trained MRC as guidance to revise the question in continuous embedding space, which can be seen as a special constrained paraphrasing method for QDA. Question generation (Heilman and Smith, 2010; Du et al., 2017; Zhao et al., 201"
2020.emnlp-main.467,2020.lifelongnlp-1.3,0,0.0763259,"hub.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to"
2020.emnlp-main.467,Q19-1026,0,0.0194933,"t can also be a document span or a passage. For notational simplicity, we use the “paragraph” to refer to it in the rest of this paper. 5798 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5798–5810, c November 16–20, 2020. 2020 Association for Computational Linguistics consistency (Alberti et al., 2019; Dong et al., 2019) to synthesize answerable questions. However, the round-trip consistency method is not able to generate context-relevant unanswerable questions, where MRC with unanswerable questions is a challenging task (Rajpurkar et al., 2018; Kwiatkowski et al., 2019). Zhu et al. (2019) firstly study unanswerable question DA, which relies on annotated plausible answer to constructs a small pseudo parallel corpus of answerable-to-unanswerable questions for unanswerable question generation. Unfortunately, most question answering (QA) and MRC datasets do not provide such annotated plausible answers. Inspired by the recent progress in controllable text revision and text attribute transfer (Wang et al., 2019; Liu et al., 2020), we propose a new QDA method called Controllable Rewriting based Question Data Augmentation (CRQDA), which can generate both new context"
2020.emnlp-main.467,2021.ccl-1.108,0,0.0566718,"Missing"
2020.emnlp-main.467,P16-1009,0,0.253175,"iverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. G"
2020.emnlp-main.467,P02-1040,0,0.106571,"Missing"
2020.emnlp-main.467,W18-5446,0,0.032856,"Missing"
2020.emnlp-main.467,P18-1080,0,0.0249486,"data, resulting in about 220K data samples (including the original data samples). The hyperparameter of βs , βt , βa , βb , and max-step are set to 0.9, 0.5, 0.5, 0.99, and 5, respectively. 4 https://github.com/huggingface/ transformers. Baselines We compare our CRQDA against the following baselines: (1) EDA (Wei and Zou, 2019): it augments question data by performing synonym replacement, random insertion, random swap, or random deletion operation. We implement EDA with their source code5 to synthesize a new question data for each question of SQuAD 2.0; (2) Back-Translation (Yu et al., 2018; Prabhumoye et al., 2018): it uses machine translation model to translate questions into French and back into English. We implement Back-Translation based on the source code6 to generate a new question data for each original question; (3) Text-VAE (Bowman et al., 2016; Liu et al., 2019a): it uses RNNbased VAE to generate a new question data for each question of SQuAD 2.0. The implementation is based on the source code7 ; (4) AE with Noise: it uses the same autoencoder of CRQDA for question data rewriting. The only difference is that the autoencoder cannot utilize the MRC gradient but only uses a noise (sampled from Ga"
2020.emnlp-main.467,D19-1670,0,0.254005,"llenging problem. Unlike the general image DA techniques such as rotation and cropping, it is more difficult to synthesize new high-quality and diverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model"
2020.emnlp-main.467,P18-2124,0,0.269168,"or NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to the question. Because the above tasks require the model to reason about the question-par"
2020.emnlp-main.467,D16-1264,0,0.482031,"s computed by: J (q, qˆ0 ) = count(wq ∩ wqˆ) , count(wq ∪ wqˆ) (11) here wq is the word in q and wqˆ is the word in qˆ0 . The whole question rewriting procedure is summarized in Algorithm 1. 4 Experiments In this section, we describe the experimental details and results. We first conduct the experiment on the SQuAD 2.0 dataset (Rajpurkar et al., 2018) to compare CRQDA with other strong baselines, which is 5802 reported in § 4.1. The ablation study and further analysis are provided in § 4.2. Then we evaluate our method on additional two tasks including question generation on SQuAD 1.1 dataset (Rajpurkar et al., 2016) in § 4.3, and question-answering language inference on QNLI dataset (Wang et al., 2018) in § 4.4. Methods BERTlarge (Devlin et al., 2018) (original) + EDA (Wei and Zou, 2019) + Back-Translation (Yu et al., 2018) + Text-VAE (Liu et al., 2019a) + AE with Noise + 3M synth (Alberti et al., 2019) + UNANSQ (Zhu et al., 2019) + CRQDA (ours) EM 78.7 78.3 77.9 75.3 76.7 80.1 80.0 80.6 F1 81.9 81.6 81.2 78.6 79.8 82.8 83.0 83.3 Table 1: Comparison results on SQuAD 2.0. 4.1 SQuAD The extractive MRC benchmark SQuAD 2.0 dataset contains about 100,000 answerable questions and over 50,000 unanswerable quest"
2020.emnlp-main.467,Q19-1016,0,0.0495117,"Missing"
2020.emnlp-main.467,P18-1156,0,0.0298439,"Missing"
2020.emnlp-main.467,D18-1259,0,0.0378266,"Missing"
2020.emnlp-main.467,D19-1253,0,0.0920265,"Missing"
2020.emnlp-main.467,D18-1424,0,0.253689,"ation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to the question. Because the above tasks require the model to reason about the question-paragraph pair, existing textual DA methods that directly augment"
2020.emnlp-main.467,P19-1415,0,0.500464,"an or a passage. For notational simplicity, we use the “paragraph” to refer to it in the rest of this paper. 5798 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5798–5810, c November 16–20, 2020. 2020 Association for Computational Linguistics consistency (Alberti et al., 2019; Dong et al., 2019) to synthesize answerable questions. However, the round-trip consistency method is not able to generate context-relevant unanswerable questions, where MRC with unanswerable questions is a challenging task (Rajpurkar et al., 2018; Kwiatkowski et al., 2019). Zhu et al. (2019) firstly study unanswerable question DA, which relies on annotated plausible answer to constructs a small pseudo parallel corpus of answerable-to-unanswerable questions for unanswerable question generation. Unfortunately, most question answering (QA) and MRC datasets do not provide such annotated plausible answers. Inspired by the recent progress in controllable text revision and text attribute transfer (Wang et al., 2019; Liu et al., 2020), we propose a new QDA method called Controllable Rewriting based Question Data Augmentation (CRQDA), which can generate both new context-relevant answerabl"
2020.emnlp-main.484,D19-1252,1,0.908102,"LUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu"
2020.emnlp-main.484,D17-1302,0,0.176422,"e select 11 cross-lingual tasks in XGLUE, which are categorized into 3 groups: single-input understanding tasks, pair-input understanding tasks, and generation tasks. For each task, training set is only available in English. In order to obtain a good performance on XGLUE, a model should be able to learn how to do a task well using its English training set, and then transfer this ability to test sets in other languages. Table 2 gives the dataset statistics and Table 3 lists languages covered by all tasks. 2 https://commoncrawl.org/. Single-input Understanding Tasks POS Tagging (POS) Following (Kim et al., 2017), we select a subset of Universal Dependencies (UD) Treebanks (v2.5) (Zeman et al., 2019), which covers 18 languages. Accuracy (ACC) of the predicted POS tags is used as the metric. News Classification (NC) This task aims to predict the category given a news article. It covers 5 languages, including English, Spanish, French, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tas"
2020.emnlp-main.484,2020.acl-main.703,0,0.0930192,"Missing"
2020.emnlp-main.484,P19-4007,0,0.0660349,"Missing"
2020.emnlp-main.484,2021.ccl-1.108,0,0.046651,"Missing"
2020.emnlp-main.484,D18-1269,0,0.125055,"rench, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tasks MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric. XNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE. PAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric. Query-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad titl"
2020.emnlp-main.484,N19-1423,0,0.68133,"oder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b; Dong et al., 2019; Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019; Conneau and Lample, 2019; Huang et al., 2019; Conneau et al., 2019) and multimodal pre-trained models (Lu et al., 2019; Li et al., 2020; Chen et al., 2019; Zhou et al., 2020). In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, 1 The dataset is available at https://microsoft. github.io/XGLUE/, The code and model is available at https://github.com/microso"
2020.emnlp-main.484,W03-0419,0,0.651554,"Missing"
2020.emnlp-main.484,D19-1382,0,0.266447,"understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b; Dong et al., 2019; Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019; Conneau and Lample, 2019; Huang et al., 2019; Conneau et al., 2019) and multimodal pre-trained models (Lu et al., 2019; Li et al., 2020; Chen et al., 2019; Zhou et al., 2020). In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, 1 The dataset is available at https://microsoft. github.io/XGLUE/, The code and model is available at https://github.com/microsoft/Unicoder where an NLP task often h"
2020.emnlp-main.484,N19-1131,0,0.0455297,"t News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tasks MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric. XNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE. PAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric. Query-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad title, ad description, label>. The label indicates whether the ad is relevant to the query (Good), or not (Bad). We con6009 Task # of Languages |Train|en |Dev|avg"
2020.emnlp-main.484,2020.findings-emnlp.217,1,\N,Missing
2020.emnlp-main.581,2020.bea-1.16,0,0.122574,"Missing"
2020.emnlp-main.633,L18-1269,0,0.0207178,"is the equivalent learning rate considering all successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development"
2020.emnlp-main.633,N19-1423,0,0.383234,"ge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.1 1 Introduction With the prevalence of deep learning, many huge neural models have been proposed and achieve state-of-the-art performance in various fields (He et al., 2016; Vaswani et al., 2017). Specifically, in Natural Language Processing (NLP), pretraining and fine-tuning have become the new norm of most tasks. Transformer-based pretrained models (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Song et al., 2019; Dong et al., 2019) have dominated the field of both Natural Language Understanding (NLU) and Natural Language Generation (NLG). These models benefit from their “overparameterized” nature (Nakkiran et al., 2020) and contain millions or even billions of parameters, making it computationally expensive and inefficient considering both memory consumption and ∗ Equal contribution. Work done during these two authors’ internship at Microsoft Research Asia. 1 The code and pretrained model are available at https: //github.com/JetRunner/BERT-of-T"
2020.emnlp-main.633,I05-5002,0,0.0126639,"r the compression and lr0 is the equivalent learning rate considering all successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2"
2020.emnlp-main.633,P84-1044,0,0.289138,"Missing"
2020.emnlp-main.633,2021.ccl-1.108,0,0.236556,"Missing"
2020.emnlp-main.633,2020.acl-main.467,0,0.0449486,"Missing"
2020.emnlp-main.633,D13-1170,0,0.0197042,"g rate considering all successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development set of GLUE, the resu"
2020.emnlp-main.633,2020.findings-emnlp.178,1,0.902857,"onding successor module scci by the probability of p. (b) During successor fine-tuning and inference, all successor modules scc1...3 are combined for calculation. tion (Turc et al., 2019) pretrains the student model with a self-supervised masked LM objective on a large corpus first, then performs a standard KD on supervised tasks. TinyBERT (Jiao et al., 2019) conducts the Knowledge Distillation twice with data augmentation. MobileBERT (Sun et al., 2020) devises a more computationally efficient architecture and applies knowledge distillation with a bottom-totop layer training procedure. PABEE (Zhou et al., 2020b) exploits early exiting to dynamically accelerate the inference of BERT. 3 BERT-of-Theseus In this section, we introduce module replacing, the technique proposed for BERT-of-Theseus. Further, we introduce a Curriculum Learning driven scheduler to obtain better performance. The workflow is shown in Figure 1. 3.1 specific loss function (e.g., Cross Entropy), which closely resembles a fine-tuning procedure. Inspired by Dropout (Srivastava et al., 2014), we propose module replacing, a novel technique for model compression. We call the original model and the target model predecessor and successor"
2020.emnlp-main.633,D19-1441,0,0.294124,"Quantization (Gong et al., 2014), Weights Pruning (Han et al., 2016) and Knowledge Distillation (KD) (Hinton et al., 2015). Among them, KD has received much attention for compressing pretrained language models. KD exploits a large teacher model to “teach” a compact student model to mimic the teacher’s behavior. In this way, the knowledge embedded in the teacher model can be transferred into the smaller model. However, the retained performance of the student model relies on a welldesigned distillation loss function which forces the student model to behave as the teacher. Recent studies on KD (Sun et al., 2019; Jiao et al., 2019) even leverage more sophisticated model-specific distillation loss functions for better performance. Different from previous KD studies which explicitly exploit a distillation loss to minimize the distance between the teacher model and the student model, we propose a new genre of model compression. Inspired by the famous thought experiment “Ship of Theseus”2 in Philosophy, where all components of a ship are gradually replaced by new ones until no original component exists, we propose Theseus Compression for BERT (BERT-ofTheseus), which progressively substitutes modules of B"
2020.emnlp-main.633,2020.acl-main.195,0,0.530066,". . , scc3 }. prdi and scci contain two and one layer, respectively. (a) During module replacing training, each predecessor module prdi is replaced with corresponding successor module scci by the probability of p. (b) During successor fine-tuning and inference, all successor modules scc1...3 are combined for calculation. tion (Turc et al., 2019) pretrains the student model with a self-supervised masked LM objective on a large corpus first, then performs a standard KD on supervised tasks. TinyBERT (Jiao et al., 2019) conducts the Knowledge Distillation twice with data augmentation. MobileBERT (Sun et al., 2020) devises a more computationally efficient architecture and applies knowledge distillation with a bottom-totop layer training procedure. PABEE (Zhou et al., 2020b) exploits early exiting to dynamically accelerate the inference of BERT. 3 BERT-of-Theseus In this section, we introduce module replacing, the technique proposed for BERT-of-Theseus. Further, we introduce a Curriculum Learning driven scheduler to obtain better performance. The workflow is shown in Figure 1. 3.1 specific loss function (e.g., Cross Entropy), which closely resembles a fine-tuning procedure. Inspired by Dropout (Srivastav"
2020.emnlp-main.633,Q19-1040,0,0.0431157,"acement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development set of GLUE, the result of MNLI is an average on MNLI-m and MNLI-mm; the results on MRPC and"
2020.emnlp-main.633,N18-1101,0,0.0249639,"l successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development set of GLUE, the result of MNLI is an averag"
2020.emnlp-main.633,D16-1264,0,\N,Missing
2020.emnlp-tutorials.1,2020.acl-tutorials.8,0,0.152569,"ns, which are interpretable to developers and users at concept level. The design of such symbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty i"
2020.emnlp-tutorials.1,P18-1043,0,0.0282424,"y conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies between them. We will use Bayesian Network (Pearl, 198"
2020.emnlp-tutorials.1,N16-3020,0,0.0292639,"natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural language (Dua et al., 2019), reasoning over rules in natural language (Clark et al., 2020), and logical reasoning (Yu et al., 2020). Afterwards, we will review model interpretation methods, including post-hoc ones and intrinsic ones. Post-hoc methods aim to interpret what an existing model learned without making changes to the original model. We will cover saliency maps (Simonyan et al., 2013), local interpretable model-agnostic explanations (LIME) (Ribeiro et al., 2016), testing with concept activation vectors (TCAV) (Kim et al., 2018), and visual explanation generation (Hendricks et al., 2016). Intrinsic methods are that inherently interpretable (to some extent). We will cover attention (Bahdanau et al., 2014), interpretable CNN (Zhang et al., 2018), and neural 2 Dilemma: Interpretability vs. Performance (30 min.) will review post-hoc models and intrinsic models for interpretation, and discuss the dilemma of “interpretability versus performance”. module network (Andreas et al., 2016). We last summarize the content of this tutorial and discuss possible futur"
2020.emnlp-tutorials.1,N15-1118,0,0.0314425,"ng (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science e"
2020.emnlp-tutorials.1,2020.acl-tutorials.7,0,0.0323856,"ce; Outline Opening (15 min.) will describe the motivation and outline of this tutorial and give our definition on machine reasoning. • Du et al. (2020) - a survey on interpretable machine learning techniques; Symbolic Reasoning (20 min.) will review typical methods based on propositional logic and first order logic, respectively. • Chen and Yih (2020) - a tutorial on opendomain question answering, in which many work can be categorized as neural-evidence reasoning; Probabilistic Reasoning (20 min.) will review typical methods based on Bayesian Network and Markov Logic Network, respectively. • Sap et al. (2020) - a tutorial on commonsense reasoning for natural language processing. Neural-Symbolic Reasoning (40 min.) will review typical methods including knowledge graph reasoning, neural semantic parsing, neural module network and symbolic knowledge as constraints. 6 Tutorial Abstract Machine reasoning research aims to build interpretable AI systems that can solve problems or draw conclusions from what they are told (i.e. facts and observations) and already know (i.e. models, common sense and knowledge) under certain constraints. In this tutorial, we will (1) describe the Neural-Evidence Reasoning (4"
2020.emnlp-tutorials.1,P19-1028,0,0.0227561,"; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural"
2020.emnlp-tutorials.1,P18-1034,1,0.817959,"erentiable modules, where each module represents a program with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box"
2020.emnlp-tutorials.1,N19-1421,0,0.0253981,"opers and users at concept level. The design of such symbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represe"
2020.emnlp-tutorials.1,N18-1074,0,0.0259547,"ymbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies bet"
2020.emnlp-tutorials.1,2020.emnlp-main.320,1,0.824035,"orot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural language (Dua et al"
2020.emnlp-tutorials.1,D17-1060,0,0.0200163,"bolic reasoning system (1) integrates existing reasoning technologies with symbolic knowledge based on neural networks and (2) implements inference as a chain of differentiable modules, where each module represents a program with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned m"
2020.emnlp-tutorials.1,D18-1259,0,0.0285158,"o broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies between them. We will use Bayesian Network (Pearl, 1988) and Markov Logic Network (Richardson and Domingos, 2006) as two repre"
2020.emnlp-tutorials.1,2020.emnlp-main.558,0,0.0147533,"gram with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretabilit"
2020.findings-emnlp.139,D14-1181,0,0.0124414,"Missing"
2020.findings-emnlp.139,P07-2045,0,0.00976133,"Missing"
2020.findings-emnlp.139,2020.acl-main.703,0,0.111381,"Missing"
2020.findings-emnlp.139,P16-1195,0,0.0204779,"eration, evaluated with smoothed BLEU-4 score. language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance8 . 4.4 Generalization to Programming Languages NOT in Pre-training We would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016)9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al. (2016). M ODEL BLEU MOSES (KOEHN ET AL ., 2007) IR SUM-NN (RUSH ET AL ., 2015) 2- LAYER B I LSTM T RANSFORMER (VASWANI ET AL ., 2017) T REE LSTM (TAI ET AL ., 2015) C ODE NN (I YER ET AL ., 2016) CODE 2 SEQ (A LON ET AL ., 2019) RO BERTA PRE - TRAIN W / CODE ONLY C ODE BERT (RTD) C OD"
2020.findings-emnlp.139,C04-1072,0,0.0190066,"ly applied. Predicted probabilities of RoBERTa and CodeBERT are given. Code Documentation Generation Although the pre-training objective of CodeBERT does not include generation-based objectives (Lewis et al., 2019), we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004). 7 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 Model Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4"
2020.findings-emnlp.139,2021.ccl-1.108,0,0.209986,"Missing"
2020.findings-emnlp.139,N18-1202,0,0.370575,"lps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.1 1 Introduction Large pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) ∗ Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa (Liu et al., 2019) have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artifici"
2020.findings-emnlp.139,D19-1250,0,0.0585014,"Missing"
2020.findings-emnlp.139,P19-1493,0,0.0463596,"Missing"
2020.findings-emnlp.139,D15-1044,0,0.118151,"Missing"
2020.findings-emnlp.139,P15-1150,0,0.0976114,"Missing"
2020.findings-emnlp.139,2020.tacl-1.48,0,0.0794978,"Missing"
2020.findings-emnlp.139,P02-1040,0,\N,Missing
2020.findings-emnlp.161,N19-1071,0,0.0751448,"ns are not explicitly modeled in our model and therefore our model is less dependent on sentence positions (as shown in experiments). There are also an interesting line of work on unsupervised abstractive summarization. Yang et al. 1785 (2020) pre-trains a seq2seq Transformer by predicting the first three sentences of news documents and then further tunes the model with semantic classification and denoising auto-encoding objectives. The model described in Wang and Lee (2018) utilizes seq2seq auto-encoding coupled with adversarial training and reinforcement learning. Fevry and Phang (2018) and Baziotis et al. (2019) focus on sentence summarization (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked l"
2020.findings-emnlp.161,Q17-1010,0,0.0323688,"ed with adversarial training and reinforcement learning. Fevry and Phang (2018) and Baziotis et al. (2019) focus on sentence summarization (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive pre-training objective to p"
2020.findings-emnlp.161,P16-1046,0,0.0745449,"documents are paired with human written summaries. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin"
2020.findings-emnlp.161,N19-1423,0,0.461437,"n et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised"
2020.findings-emnlp.161,K18-1040,0,0.294146,"nd weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c November 16 - 20, 2020. 2020 A"
2020.findings-emnlp.161,W04-1017,0,0.0293505,"d summarization and pre-training. Supervised Summarization Most summarization models require supervision from labeled datasets, where documents are paired with human written summaries. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et a"
2020.findings-emnlp.161,D18-1443,0,0.0434639,"loglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervised Summarization Compared to supervised models, unsupervised models only need unlabeled documents during training. Most unsupervised ext"
2020.findings-emnlp.161,P16-1154,0,0.0227368,"of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervis"
2020.findings-emnlp.161,D13-1158,0,0.160864,"rst author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured wit"
2020.findings-emnlp.161,2020.acl-main.439,0,0.0205955,"olov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive pre-training objective to predict relative distances of surrounding sentences to the anchor sentence, while our sentence shuffling task predicts original positions of sentences from a shuffled docuemt. Besides, pre-training methods mentioned above focus on learning good word, sentence or document representations for downstream tasks, while our method focuses on learning sentence level attention distributions (i.e., sentence associations), which is shown in our experiments to be very helpful for unsupervised summarization. 3 Model In this section, we describe our unsuperv"
2020.findings-emnlp.161,2020.acl-main.703,0,0.0754839,"Missing"
2020.findings-emnlp.161,W04-1013,0,0.0376574,"ds as in (Zheng and Lapata, 2019) and finally retain 36,745 for training, 5,531 for validation and 4,375 for test. We segment sentences using the Stanford CoreNLP toolkit (Manning et al., 2014). Sentences are then tokenized with the UTF-8 based BPE tokenizer used in RoBERTa and GPT-2 (Radford et al., 2019) and the resulting vocabulary contains 50,265 subwords. During training, we only leverage articles in CNN/DM or NYT; while we do use both articles and summaries in validation sets to tune hyper-parameters of our models. We evaluated the quality of summaries from different models using ROUGE (Lin, 2004). We report the full length F1 based ROUGE-1, ROUGE-2, ROUGE-L on both CNN/DM and NYT datasets. These ROUGE scores are computed using the ROUGE-1.5.5.pl script4 . 4.2 Implementation Details The main building blocks of S TAS are Transformers (Vaswani et al., 2017). In the following, we describe the sizes of them using the number of layers L, the number of attention heads A, and the hidden size N . As in (Vaswani et al., 2017; Devlin et al., 2019), the hidden size of the feed-forward sublayer is always 4H. S TAS contains one hierarchical encoder (see Section 3.1) and two decoders, where they are"
2020.findings-emnlp.161,P02-1058,0,0.512317,"ely, human labeling for summarization task is expensive and therefore high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page"
2020.findings-emnlp.161,D19-1387,0,0.473379,"r hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervised Summarization Compared to supervised models, unsupervised models only need unlabeled documents during training. Most unsupervised extractive models are graph based (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). For example,"
2020.findings-emnlp.161,2021.ccl-1.108,0,0.104583,"Missing"
2020.findings-emnlp.161,P14-5010,0,0.00302378,"ticles for training, 13,368 for validation and 11,490 for test. Following Zheng and Lapata (2019), we adopted the splits widely used in abstractive summarization (Paulus et al., 2018) for the NYT dataset, which ranks articles by their publication date and used the first 589,284 for training, the next 32,736 for validation and the remaining 32,739 for test. Then, we filter out documents whose summaries are shorter than 50 words as in (Zheng and Lapata, 2019) and finally retain 36,745 for training, 5,531 for validation and 4,375 for test. We segment sentences using the Stanford CoreNLP toolkit (Manning et al., 2014). Sentences are then tokenized with the UTF-8 based BPE tokenizer used in RoBERTa and GPT-2 (Radford et al., 2019) and the resulting vocabulary contains 50,265 subwords. During training, we only leverage articles in CNN/DM or NYT; while we do use both articles and summaries in validation sets to tune hyper-parameters of our models. We evaluated the quality of summaries from different models using ROUGE (Lin, 2004). We report the full length F1 based ROUGE-1, ROUGE-2, ROUGE-L on both CNN/DM and NYT datasets. These ROUGE scores are computed using the ROUGE-1.5.5.pl script4 . 4.2 Implementation D"
2020.findings-emnlp.161,W04-3252,0,0.634116,"for summarization task is expensive and therefore high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sen"
2020.findings-emnlp.161,N18-1158,0,0.387507,"s. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained"
2020.findings-emnlp.161,N19-4009,0,0.0146229,"ock. LEAD-3 simply selects the first three sentences as the summary for each document. TEXTRANK (Mihalcea and Tarau, 2004) views a document as a graph with sentences as nodes and edge weights using the sentence similarities. It selects top sentences as summary w.r.t. PageRank (Page et al., 1999) scores. PACSUM (Zheng and Lapata, 2019) is yet another graphbased extractive model using BERT as sentence features. Sentences are ranked using centralities (sum of all out edge weights). They made the ranking criterion positional sensitive by forcing negative 6 We used gradient accumulation technique (Ott et al., 2019) to increase the actual batch size. edge weights for edges between the current sentence and its preceding sentences. Adv-RF (Wang and Lee, 2018) and TED (Yang et al., 2020) are all based on unsupervised seq2seq auto-encoding with additional objectives of adversarial training, reinforcement learning and seq2seq pre-training to predict leading sentences. PACSUM is based on the BERT (Devlin et al., 2019) initialization. RoBERTa (Liu et al., 2019), which extends BERT with better training strategies and more training data, outperforms BERT on many tasks. We therefore re-implemented PACSUM and exten"
2020.findings-emnlp.161,D15-1226,0,0.130693,"hip at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al.,"
2020.findings-emnlp.161,D14-1162,0,0.109516,"q2seq auto-encoding coupled with adversarial training and reinforcement learning. Fevry and Phang (2018) and Baziotis et al. (2019) focus on sentence summarization (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive p"
2020.findings-emnlp.161,N18-1202,0,0.0116715,"ation (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive pre-training objective to predict relative distances of surrounding sentences to the anchor sentence, while our sentence shuffling task predicts original p"
2020.findings-emnlp.161,radev-etal-2004-mead,0,0.0947782,"k on supervised summarization, unsupervised summarization and pre-training. Supervised Summarization Most summarization models require supervision from labeled datasets, where documents are paired with human written summaries. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus"
2020.findings-emnlp.161,W00-0403,0,0.939861,"ummaries. Unfortunately, human labeling for summarization task is expensive and therefore high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then em"
2020.findings-emnlp.161,P17-1099,0,0.748564,"hen human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervised Summarization Compared to superv"
2020.findings-emnlp.161,2020.findings-emnlp.168,0,0.657384,"ce similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c November 16 - 20, 2020. 2020 Association for Computational Linguistic"
2020.findings-emnlp.161,D18-1088,1,0.882361,"r, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized fo"
2020.findings-emnlp.161,P19-1499,1,0.831509,"re measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c November 16 - 20, 2020. 2020 Association for Computational Linguistics hierarchical transformer has a token-level transformer to learn sentence representations and a sentence-level transformer to learn interactions between sentences with self-attention. In Zhang et al. (2019), HIBERT is applied to supervised extractive summarization. However, we believe that after pre-training HIBERT"
2020.findings-emnlp.161,P19-1628,0,0.34507,"; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents"
2020.findings-emnlp.161,D08-1079,0,0.902499,"high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in ("
2020.findings-emnlp.161,D18-1451,0,0.397504,"node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c Novembe"
2020.findings-emnlp.178,P11-1015,0,\N,Missing
2020.findings-emnlp.178,D17-1070,0,\N,Missing
2020.findings-emnlp.178,P17-1052,0,\N,Missing
2020.findings-emnlp.178,P19-1580,0,\N,Missing
2020.findings-emnlp.178,P16-1162,0,\N,Missing
2020.findings-emnlp.217,P18-1015,0,0.0541138,"ch 2405 Method LEAD-3 (Nallapati et al., 2017) PTGEN (See et al., 2017) PTGEN+Coverage (See et al., 2017) S2S-ELMo (Edunov et al., 2019) Bottom-Up (Gehrmann et al., 2018) BERTSUMABS (Liu and Lapata, 2019) BERTSUMEXTABS (Liu and Lapata, 2019) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet ROUGE-1 ROUGE-2 ROUGE-L 40.42 36.44 39.53 41.56 41.22 41.72 42.13 42.12 43.33 43.68 17.62 15.66 17.28 18.94 18.68 19.39 19.60 19.50 20.21 20.64 36.67 33.42 36.38 38.47 38.34 38.76 39.18 39.01 40.51 40.72 Table 1: Results on the CNN/DailyMail test set. Method OpenNMT (Klein et al., 2017) Re3Sum (Cao et al., 2018) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet R-1 R-2 R-L 36.73 37.04 38.73 38.45 39.55 17.86 19.03 19.71 19.45 20.27 33.68 34.46 35.96 35.75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet out"
2020.findings-emnlp.217,P18-1177,0,0.142803,"tNet R-1 R-2 R-L 36.73 37.04 38.73 38.45 39.55 17.86 19.03 19.71 19.45 20.27 33.68 34.46 35.96 35.75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet outperforms previous models on all metrics. Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. F"
2020.findings-emnlp.217,P17-1123,0,0.322918,"Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with future bigram prediction training. During inference, we set the length penalty to 1.0 and beam size to 4. We set the hyper-parameters according to the performance on the dev set. We compare our ProphetNet against following baselines: OpenNMT (Klein et al., 2017) which 2405 Method LEAD-3 (Nallapati et al.,"
2020.findings-emnlp.217,N19-1409,0,0.0121283,"utput to between 45 and 110 tokens with a 1.2 length penalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the bes"
2020.findings-emnlp.217,D18-1443,0,0.145819,"rmup steps, and the total fine-tune epoch are set to 512, 1000, and 10. We limit the length of the output to between 45 and 110 tokens with a 1.2 length penalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The r"
2020.findings-emnlp.217,P17-4012,0,0.0209716,"res of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with future bigram prediction training. During inference, we set the length penalty to 1.0 and beam size to 4. We set the hyper-parameters according to the performance on the dev set. We compare our ProphetNet against following baselines: OpenNMT (Klein et al., 2017) which 2405 Method LEAD-3 (Nallapati et al., 2017) PTGEN (See et al., 2017) PTGEN+Coverage (See et al., 2017) S2S-ELMo (Edunov et al., 2019) Bottom-Up (Gehrmann et al., 2018) BERTSUMABS (Liu and Lapata, 2019) BERTSUMEXTABS (Liu and Lapata, 2019) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet ROUGE-1 ROUGE-2 ROUGE-L 40.42 36.44 39.53 41.56 41.22 41.72 42.13 42.12 43.33 43.68 17.62 15.66 17.28 18.94 18.68 19.39 19.60 19.50 20.21 20.64 36.67 33.42 36.38 38.47 38.34 38.76 39.18 39.01 40.51 40.72 Table 1: Results on the CNN/DailyMail test set. Method OpenNMT (Klein et al., 2017) Re3S"
2020.findings-emnlp.217,D19-1001,0,0.0248556,"RT (Lewis et al., 2019) uses the encoder-decoder structure to generate the original sentence with its spoiled input to denoise. In the BART decoder, the undamaged language model is learned thus brings improvement to NLG tasks. Natural language generation methods are typically based on the left-to-right or right-to-left language models and generate one token in each time step. These methods can not capture the information of future tokens. Recently, incorporating future information into language generation tasks has attracted the attention of researchers (Li et al., 2017; Serdyuk et al., 2018; Lawrence et al., 2019; Oord et al., 2018). Li et al. (2017) propose an actor-critic model which designs a value function as a critic to estimate the future success. In their method, they not only consider the MLE-based learning but also incorporate an RL-based value function into the decoder process. (Oord et al., 2018) do not predict future tokens directly but tried to model a density ratio to preserve the mutual information between context and future token. Serdyuk et al. (2018) point out traditional Recurrent Neural Networks (RNNs) may prefer to generate each token based on the recent tokens, it is hard to lear"
2020.findings-emnlp.217,2020.acl-main.703,0,0.276307,"Missing"
2020.findings-emnlp.217,D19-1387,0,0.114656,"hetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with fu"
2020.findings-emnlp.217,K16-1028,0,0.101321,"ext summarization datasets: (a) the non-anonymized version of the CNN/DailyMail dataset (See et al., 2017), and (b) Gigaword corpus (Rush et al., 2015). CNN/DailyMail We use Adam optimizer (Kingma and Ba, 2015) with a peak learning rate 1 × 10−4 . The batch size, warmup steps, and the total fine-tune epoch are set to 512, 1000, and 10. We limit the length of the output to between 45 and 110 tokens with a 1.2 length penalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et"
2020.findings-emnlp.217,N18-1202,0,0.0221085,"enalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam op"
2020.findings-emnlp.217,D16-1264,0,0.076299,"2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we split the SQuAD 1.1 (Rajpurkar et al., 2016) dataset into training, development and test sets. We also report the results on the data split as did in Zhao et al. (2018), which reverses the development set and test set. The question generation task is typically formulated as a Seq2Seq problem. The input passage and the answer are packed as “answer [SEP] input passage” as input, and the question is used as the target output sequence. We fine-tune the ProphetNet model 10 epochs in the training set and report the results of the two kinds of data splits as mentioned above. The first 512 tokens of the passage are fed to the model. The peak le"
2020.findings-emnlp.217,D15-1044,0,0.367465,"Missing"
2020.findings-emnlp.217,P17-1099,0,0.120286,"N+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with future bigram prediction training. During inference, we set the length penalty to 1.0 and beam size to 4. We set the hyper-parameters according to the performance on the dev set. We compare our ProphetNet against followin"
2020.findings-emnlp.217,D19-1253,0,0.226997,"38.73 38.45 39.55 17.86 19.03 19.71 19.45 20.27 33.68 34.46 35.96 35.75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet outperforms previous models on all metrics. Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we s"
2020.findings-emnlp.217,D18-1424,0,0.262924,".75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet outperforms previous models on all metrics. Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we split the SQuAD 1.1 (Rajpurkar et al., 2016) dataset into trainin"
2020.findings-emnlp.217,P02-1040,0,\N,Missing
2020.findings-emnlp.217,W05-0909,0,\N,Missing
2020.findings-emnlp.217,W04-1013,0,\N,Missing
2020.findings-emnlp.217,2020.tacl-1.5,0,\N,Missing
2020.lrec-1.236,P17-4012,0,0.0342952,"Missing"
2021.acl-long.157,W05-0909,0,0.120487,"Missing"
2021.acl-long.157,D19-1155,0,0.0110693,"(Pont-Tuset et al., 2020), and CIDEr-D (Vedantam et al., 2015). 4.4 Loss Dataset Results Baseline and +Trace methods The Baseline and +Trace methods are our re-implementations following (Pont-Tuset et al., 2020)’s method description. The Baseline method only takes image feature as input while the +Trace model take image feature 2 https://github.com/tylin/coco-caption We add an additional id to every trace-image-caption triplet and adjust some code of the standard evaluation tool to meet the ”1 trace-vs-1 caption” evaluation need. 2018 3 and trace both as input. They employ the architecture in Changpinyo et al. (2019) with a few minor differences. First, they set the number of Transformers’ layers for both the encoder and the decoder to 2 instead of 6. Second, their projection layers also consist of layer normalization(Ba et al., 2016). Third, they set the maximum number of iterations to 150k. Finally, they allow the maximum number of target captions to be as long as 225 to account for the narration’s longer nature. LoopCAG methods Our model comprises of four components: 1) the transformer encoderdecoder framework; 2) the trace input; 3) Attention Guidance(+AG for short) grounding loss; 4) Contrastive cons"
2021.acl-long.157,P04-1077,0,0.0495775,"Missing"
2021.acl-long.157,2021.naacl-main.280,1,0.748297,"Missing"
2021.acl-long.157,P02-1040,0,0.113144,"Missing"
2021.acl-long.348,P18-1073,0,0.162705,"g steps of pre-training encoder and decoder are separated, therefore the training samples of them are not necesarrily the same. (In the figure, the training sample for pre-training the encoder is x1 = x11 x21 ..x61 ) and the training sample for pre-training the decoder is x2 = x12 x22 ..x62 ). For MT fine-tuning, we use the parallel training sample {x1 , y1 } from the parallel corpus or generated from back-translation. connected for MT fine-tuning. We propose two types of semantic interfaces, namely CL-SemFace and VQ-SemFace. The former takes the trained unsupervised cross-lingual embeddings (Artetxe et al., 2018) as the interface for encoder and decoder pretraining. Inspired by the success of neural discrete representation learning (Van Den Oord et al., 2017), the latter uses language-independent vector quantized (VQ) embeddings (semantic unites) as the interface to map encoder outputs and decoder inputs into the shared VQ space. Experiments conducted on both supervised and unsupervised translation tasks demonstrate that SemFace effectively connects the pre-trained encoder and decoder, and achieves a significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively. Our contributions ar"
2021.acl-long.348,N19-1423,0,0.0204389,"the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-training decoder benefits little in their results. The potential reason is that the cross-attention between the encoder and decoder is not pre-trained, which is randomly initialized when they are connected for fine-tuning, resulting in a lack of semantic interfaces between the pre-trained encoder and decoder. Another line of work attempts to pre-train a sequence-to-sequence model directly, e.g., MASS (Song et al., 2019) and BART (Lewis et al., 2020). But these methods usually"
2021.acl-long.348,2020.acl-main.703,0,0.419451,"d decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-trainin"
2021.acl-long.348,2020.emnlp-main.210,0,0.0184456,"wis et al., 2020) adopted a similar framework and trained the model as a denoising auto-encoder. mBART (Liu et al., 2020) trained BART model on large-scale monolingual corpora in many languages. Although the above work can pre-train the cross-attention of decoder, they are learned on monolingual denoising auto-encoding and cannot learn the corss-lingual transformation between source and target languages. There is also some work trying to explicitly introduce cross-lingual information in a code-switch way during the sequence-to-sequence pre-training, such as CSP (Yang et al., 2020b) and mRASP (Lin et al., 2020). However, their methods need a lexicon or phrase translation table, which is inferred from unsupervised cross-lingual embeddings. Therefore, they depend on the quality of the dictionary. The most similar work to ours is probably the one of DALL·E and CLIP (Radford et al., 2020). DALL·E is a transformer language model that receives both the text and the image as a single stream of data. The core idea is to define the cross-modality interface of image and text, which can generate images from text descriptions. In this paper, to address the above limitations of pretraining methods for NMT, we at"
2021.acl-long.348,2020.tacl-1.47,0,0.349601,"s significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-training decoder benefits li"
2021.acl-long.348,N18-1202,0,0.11426,"eddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages:"
2021.acl-long.348,D19-1071,1,0.849776,"(Devlin et al., 2018). The early pre-training techniques mainly focused on the natural language understanding tasks such as the GLUE benchmark (Wang et al., 2018) , and later it was gradually extended to the natural language generation tasks, e.g., NMT. Recently, a prominent line of work has been proposed to improve NMT with pre-training. These techniques can be broadly classified into two categories. The first category usually uses pre-trained models as feature extractors of a source language, or initializes the encoder and decoder with pretrained models separately (Lample and Conneau, 2019; Ren et al., 2019; Yang et al., 2020a; Zhu et al., 2020). For example, Lample and Conneau (2019) proposed a cross-lingual language model with a supervised translation language modeling objective, and used MLM or CLM to pre-train the encoder and decoder of NMT. However, the combined encoder-decoder model, where the crossattention is randomly initialized, often does not work well because of the lack of semantic interfaces between the pre-trained encoder and decoder. There is also some work trying to leverage BERTlike pre-trained models for MT with an adapter (Guo et al., 2020) or an APT framework (Weng et al., 2"
2021.acl-long.348,W18-5446,0,0.0462228,"Missing"
2021.acl-long.348,2020.emnlp-main.208,0,0.0575371,"18). The early pre-training techniques mainly focused on the natural language understanding tasks such as the GLUE benchmark (Wang et al., 2018) , and later it was gradually extended to the natural language generation tasks, e.g., NMT. Recently, a prominent line of work has been proposed to improve NMT with pre-training. These techniques can be broadly classified into two categories. The first category usually uses pre-trained models as feature extractors of a source language, or initializes the encoder and decoder with pretrained models separately (Lample and Conneau, 2019; Ren et al., 2019; Yang et al., 2020a; Zhu et al., 2020). For example, Lample and Conneau (2019) proposed a cross-lingual language model with a supervised translation language modeling objective, and used MLM or CLM to pre-train the encoder and decoder of NMT. However, the combined encoder-decoder model, where the crossattention is randomly initialized, often does not work well because of the lack of semantic interfaces between the pre-trained encoder and decoder. There is also some work trying to leverage BERTlike pre-trained models for MT with an adapter (Guo et al., 2020) or an APT framework (Weng et al., 2020). The former de"
2021.acl-long.438,2020.acl-main.21,0,0.0316391,"t al., 2017) and conversational question ranking (Aliannejadi et al., 2019). Corresponding authors. anaphora MLE Was anyone opposed to Introduction ∗ Was anyone opposed to Ira Hayes revealing the truth about Harlon and the Rosenthal photograph? The former directly generates conversational questions based on the dialogue context. However, the generated questions may be irrelevant and meaningless (Rosset et al., 2020). A lack of explicit semantic guidance makes it difficult to produce each question token from scratch while preserving relevancy and usefulness at the same time (Wang et al., 2018; Chai and Wan, 2020). Instead, the latter proposes to retrieve questions from a collection for the given dialogue context, which can usually guarantee that the questions are relevant and useful (Shen et al., 2018; Rosset et al., 2020). However, question ranking methods do not lead to a natural communication between human and machine (Pulman, 1995), as they neglect important characteristics in conversations, e.g., anaphora and ellipsis. As shown in Fig. 1, the self-contained question (SQ4) lacks these characteristics, which makes it look unnatural. In this work, we study the task of Conversa5638 Proceedings of the"
2021.acl-long.438,D17-1090,1,0.825326,"xtensive attention. It introduces a new way to connect people to information through conversations (Qu et al., 2020; Gao et al., 2021; Ren et al., 2020). One of the key features of CIS is mixed initiative behavior, where a system can improve user satisfaction by proactively asking clarification questions (Zhang et al., 2018; Aliannejadi et al., 2019; Xu et al., 2019), besides passively providing answers (Croft et al., 2010; Radlinski and Craswell, 2017; Lei et al., 2020). Previous studies on asking clarification questions can be grouped into two categories: conversational question generation (Duan et al., 2017) and conversational question ranking (Aliannejadi et al., 2019). Corresponding authors. anaphora MLE Was anyone opposed to Introduction ∗ Was anyone opposed to Ira Hayes revealing the truth about Harlon and the Rosenthal photograph? The former directly generates conversational questions based on the dialogue context. However, the generated questions may be irrelevant and meaningless (Rosset et al., 2020). A lack of explicit semantic guidance makes it difficult to produce each question token from scratch while preserving relevancy and usefulness at the same time (Wang et al., 2018; Chai and Wan"
2021.acl-long.438,D19-1605,0,0.369577,"ust 1–6, 2021. ©2021 Association for Computational Linguistics tional Question Simplification (CQS). Given a dialogue context and self-contained question as input, CQS aims to transform the self-contained question into a conversational one by simulating conversational characteristics, such as anaphora and ellipsis. For example, in Fig. 1, four simplification operations are applied to obtain the conversational question (CQ4), which is context-dependent and superior to its origin one (SQ4) in terms of naturalness and conveying. The reverse process, i.e., Conversational Question Rewriting (CQR) (Elgohary et al., 2019; Voskarides et al., 2020) which rewrites CQ4 into SQ4, has been widely explored in the literature (Vakulenko et al., 2020; Yu et al., 2020). Although the proposed methods for CQR can be easily adopted for CQS, they do not always generate satisfactory results as they are all trained to optimize a maximum likelihood estimation (MLE) objective, which gives equal attention to generate each question token. Therefore, they often get stuck in easily learned tokens, i.e., tokens appearing in input, ignoring conversational tokens, e.g., him, which is a small but important portion of output. To address"
2021.acl-long.438,2020.acl-main.545,0,0.415851,"uit?’ We follow the CANARD splits for training and evaluation. In addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing. 4.2 Evaluation metrics Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We compare with several"
2021.acl-long.438,2020.acl-main.90,0,0.528286,"uit?’ We follow the CANARD splits for training and evaluation. In addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing. 4.2 Evaluation metrics Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We compare with several"
2021.acl-long.438,W04-1013,0,0.0138516,"valuate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing. 4.2 Evaluation metrics Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We compare with several recent state-of-the-art methods for this task or closely related tasks: • Origin us"
2021.acl-long.438,2021.ccl-1.108,0,0.081705,"Missing"
2021.acl-long.438,D19-1510,0,0.0178337,"they do not work on this task for various reasons. For example, due to the lack of labels needed for training, we cannot compare with the methods proposed by Rosset et al. (2020) and Xu et al. (2020). Su et al. (2019) propose a model that can only copy tokens from input; it works well on the reverse task (i.e., CQR), but not on CQS. 4.4 Implementation details We use BERT2BERT for the modeling of the editing and phrasing parts (Rothe et al., 2020), as other pretrained models like GPT-2 (Radford et al., 2019) cannot work for both. The hidden size is 768 and phrase vocabulary is 3461 following (Malmi et al., 2019). We use the BERT vocabulary (30,522 tokens) for all BERT-based or BERT2BERT-based models. We use the Adam optimizer (learning rate 5e-5) (Kingma and Ba, 2015) to train all models. In particular, we train all models for 20,000 warm-up steps, 5 epochs with pretrained model parameters frozen, and 20 epochs for all parameters. For RISE, the maximum editing iteration times is set to 3. We use gradient clipping with a maximum gradient norm of 1.0. We select the best models based on the performance on the validation set. During inference, we use greedy decoding for all models. 4.5 Results We list th"
2021.acl-long.438,D19-5809,0,0.0370357,"ch as 5645 possible, where the result may be uncontrollable. In future work, we will add a discriminator to check the necessary information. 6 Related work Studies on asking conversational question can be divided into two categories: conversational question generation and conversational question ranking. Conversational question generation aims to directly generate conversational questions conditioned on the dialogue context (Sultan et al., 2020; Ren et al., 2021a). Zamani et al. (2020) and Qi et al. (2020) define a question utility function to guide the generation of conversational questions. Nakanishi et al. (2019); Jia et al. (2020) incorporate knowledge with auxiliary tasks. These methods may generate irrelevant questions due to their pure generation nature. Conversational question ranking (Aliannejadi et al., 2019) retrieves questions from a collection based on the given context, so the questions are mostly relevant to the context. Kundu et al. (2020) propose a pair-wise matching network between context and question to do question ranking. Some studies also use auxiliary tasks to improve ranking performance, such as Natural Language Inference (Kumar et al., 2020) and relevance classification (Rosset"
2021.acl-long.438,P02-1040,0,0.109838,"and evaluation. In addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing. 4.2 Evaluation metrics Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We compare with several recent state-of-the-art methods for this task or closely relat"
2021.acl-long.438,2020.findings-emnlp.3,0,0.0184692,"out necessary information in Example 2. Here, RISE tries to simulate conversational characteristics as much as 5645 possible, where the result may be uncontrollable. In future work, we will add a discriminator to check the necessary information. 6 Related work Studies on asking conversational question can be divided into two categories: conversational question generation and conversational question ranking. Conversational question generation aims to directly generate conversational questions conditioned on the dialogue context (Sultan et al., 2020; Ren et al., 2021a). Zamani et al. (2020) and Qi et al. (2020) define a question utility function to guide the generation of conversational questions. Nakanishi et al. (2019); Jia et al. (2020) incorporate knowledge with auxiliary tasks. These methods may generate irrelevant questions due to their pure generation nature. Conversational question ranking (Aliannejadi et al., 2019) retrieves questions from a collection based on the given context, so the questions are mostly relevant to the context. Kundu et al. (2020) propose a pair-wise matching network between context and question to do question ranking. Some studies also use auxiliary tasks to improve ra"
2021.acl-long.438,P18-1204,0,0.0214397,"generation (Duan et al., 2017) and conversational question ranking (Aliannejadi et al., 2019). Corresponding authors. anaphora MLE Was anyone opposed to Introduction ∗ Was anyone opposed to Ira Hayes revealing the truth about Harlon and the Rosenthal photograph? The former directly generates conversational questions based on the dialogue context. However, the generated questions may be irrelevant and meaningless (Rosset et al., 2020). A lack of explicit semantic guidance makes it difficult to produce each question token from scratch while preserving relevancy and usefulness at the same time (Wang et al., 2018; Chai and Wan, 2020). Instead, the latter proposes to retrieve questions from a collection for the given dialogue context, which can usually guarantee that the questions are relevant and useful (Shen et al., 2018; Rosset et al., 2020). However, question ranking methods do not lead to a natural communication between human and machine (Pulman, 1995), as they neglect important characteristics in conversations, e.g., anaphora and ellipsis. As shown in Fig. 1, the self-contained question (SQ4) lacks these characteristics, which makes it look unnatural. In this work, we study the task of Conversa56"
2021.acl-long.438,2020.tacl-1.18,0,0.104717,"e next state spt+1 by applying the predicted actions from the phrasing MDP to the current state spt . rt ∈ R is the shared reward function. π p is the phrasing policy network. RISE tries to maximize the expected reward: J(θ) = Eaet ∼πe ,apt ∼πp [rt ], (3) where θ is the model parameter which is optimized with the policy gradient: ∇J(θ) = Eaet ∼πe ,apt ∼πp [rt (∇ log π e (aet |set ) + ∇ log π p (apt |spt ))], (4) Next, we will show how to model π e (aet |set ), π p (apt |spt ), and rt . 3.2 Policy networks We implement the editing and phrasing policy networks (π e and π p ) based on BERT2BERT (Rothe et al., 2020) as shown in Fig. 2. The editing policy network is implemented by the encoder to predict combinatorial edits, and the phrasing policy network is implemented by the decoder to predict phrases. 3.2.1 Editing policy network We unfold all tokens of the utterances in the context into a sequence C = (w1 , . . . , wc ), where wi denotes a token and we add “[SEP]” to separate different utterances. Then the context and input question in t-th iteration are concatenated with “[SEP]” as the separator. Finally, we feed them into the encoder of BERT2BERT to obtain hidden representations for tokens in questi"
2021.acl-long.438,P19-1003,0,0.140694,", e.g., ‘Did he win the lawsuit?’ We follow the CANARD splits for training and evaluation. In addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing. 4.2 Evaluation metrics Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We co"
2021.acl-long.438,2020.acl-main.500,0,0.207259,"which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We compare with several recent state-of-the-art methods for this task or closely related tasks: • Origin uses the original self-contained question as output. • Rule (Yu et al., 2020) employs two simple rules to mimic two conversational characteristics: anaphora and ellipsis. • QGDiv (Sultan et al., 2020) uses RoBERTa (Liu et al., 2019) with beam search (Wiseman and Rush, 2016) for generation. • Trans++ (Vakulenko et al., 2020) predicts several word distributions, and combines them to obtain the final word distribution when generating each token. • QuerySim (Yu et al., 2020) adopts a GPT2 (Radford et al., 2019) model to generate conversational question. We also found some methods from related tasks. But they do not work on this task for various reasons. For example, due to the lack of labels needed for training, we cannot compare with the methods proposed by Rosset et al. (2020) and Xu et al."
2021.acl-long.438,D16-1137,0,0.0700395,"Missing"
2021.acl-long.438,D19-1172,1,0.83484,"its reverse, Conversational Question Rewriting. Q1–A3 is the context, SQ4 is the selfcontained question, and CQ4 is the conversational question. Conversational information seeking (CIS) (Zamani and Craswell, 2020; Ren et al., 2021b) has received extensive attention. It introduces a new way to connect people to information through conversations (Qu et al., 2020; Gao et al., 2021; Ren et al., 2020). One of the key features of CIS is mixed initiative behavior, where a system can improve user satisfaction by proactively asking clarification questions (Zhang et al., 2018; Aliannejadi et al., 2019; Xu et al., 2019), besides passively providing answers (Croft et al., 2010; Radlinski and Craswell, 2017; Lei et al., 2020). Previous studies on asking clarification questions can be grouped into two categories: conversational question generation (Duan et al., 2017) and conversational question ranking (Aliannejadi et al., 2019). Corresponding authors. anaphora MLE Was anyone opposed to Introduction ∗ Was anyone opposed to Ira Hayes revealing the truth about Harlon and the Rosenthal photograph? The former directly generates conversational questions based on the dialogue context. However, the generated questions"
2021.acl-long.438,2020.emnlp-main.537,0,0.380746,"uit?’ We follow the CANARD splits for training and evaluation. In addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing. 4.2 Evaluation metrics Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper. 1 2 http://canard.qanta.org http://www.treccast.ai 4.3 Baselines We compare with several"
2021.acl-long.442,P17-1152,0,0.0331223,"enforcing similar objects to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score"
2021.acl-long.442,D17-1070,0,0.0259629,"logy In this section, we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The r"
2021.acl-long.442,2020.findings-emnlp.139,1,0.917023,"x in fragments[1:]) Label: With the growing population of software developers, natural language code search, which improves the productivity of the development process via retrieving semantically relevant code given natural language queries, is increasingly important in both communities of software engineering and natural language processing (Allamanis et al., 2018; Liu et al., 2020a). The key challenge is how to effectively measure the semantic similarity between a natural language query and a code. There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space. However, these models are ∗ 0 Figure 1: Two examples in CoSQA. A pair of a web query and a Python function with documentation is annotated with “1” or “0”, representing whether the code answers the query or not. Introduction Work done during internship at Microsoft Research Asia. The CoSQA data and leaderboard are available at https://github.com/microsoft/CodeXGLUE/tree/main/TextCode/NL-code-search-WebQuery. The code is available at https://github.com/Jun-jie-Huang/CoCLR 1 1 mostly trained on"
2021.acl-long.442,2020.acl-main.758,0,0.0222518,"ring. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague queries and (7) others. Basically, queries in (2)-(7) categories are not likely to be answered only by a code function, since they may nee"
2021.acl-long.442,2021.ccl-1.108,0,0.0812067,"Missing"
2021.acl-long.442,I17-2053,0,0.0462943,"Missing"
2021.acl-long.442,P16-2022,0,0.016286,"ts to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score s(i,i) can be viewed"
2021.acl-long.442,D19-1410,0,0.0129035,"he model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed lines denote the augmented e"
2021.acl-long.442,W18-3022,0,0.0178495,"we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed"
2021.acl-long.442,2020.findings-emnlp.361,0,0.0315257,"Gu et al., 2018; Cambronero Related Work In this part, we describe existing datasets and methods on code search and code question answering. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague que"
2021.acl-long.442,2020.emnlp-main.36,0,0.0559045,"Missing"
2021.acl-long.62,N19-1423,0,0.011306,"r model optimization, we adopt the gradient descent algorithm. 4 Experiments We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics. Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report the micro-averaged (Precision = Recall = F1) and macro-averaged scores (Precision, Recall, F1) in all the settings including 2-way"
2021.acl-long.62,D19-1488,1,0.846914,"ntity e, we build a one-way directed edge from a sentence to the entity e, in order to allow only information propagation from sentences to entities. In this way, we can avoid integrating true entity knowledge directly into news representation, which may mislead the detection of fake news. 3.2 Heterogeneous Graph Convolution Based on the above directed heterogeneous document graph G, we develop a heterogeneous graph attention network for learning the news representation as well as the contextual entity representations. It considers not only the weights of different nodes with different types (Hu et al., 2019) but also the edge directions in the heterogeneous graph. Formally, we have three types T = {τ1 , τ2 , τ3 } of nodes: sentences S, topics T and entities E with different feature spaces. We apply LSTM to encode a sentence s = {w1 , · · ·, wm } and get its feature vector xs ∈ RM . The entity e ∈ E is initialized with the entity representations eKB ∈ RM learned from the external KB (see Subsection 3.3.1). The topic t ∈ T is initialized with one-hot vector xt ∈ RK . Next, consider the graph G = (V, E) where V and E represent the set of nodes and edges respectively. Let X ∈ R|V|×M be a matrix conta"
2021.acl-long.62,D14-1181,0,0.00516264,"training, Y is the corresponding label indicator matrix, Θ is the model parameters, and η is regularization factor. For model optimization, we adopt the gradient descent algorithm. 4 Experiments We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics. Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report th"
2021.acl-long.62,2020.lrec-1.747,0,0.141975,"past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality"
2021.acl-long.62,P18-1022,0,0.0202518,"representation for improving fake news detection. Some works (Wang, 2017; Khattar et al., 2019; Wang et al., 2020) also consider incorporating multi-modal features such as images for improving fake news detection. Content-based Fake News Detection On the other hand, news contents contain the clues to differentiate fake and trusted news. A lot of existing works extract specific writing styles such as lexical and syntactic features (Conroy et al., 2015; Rubin et al., 2016; Khurana and Intelligentie, 2017; Rashkin et al., 2017; Shu et al., 2020; Oshikawa et al., 2020) and sensational headlines (Potthast et al., 2018; Sitaula et al., 2019) for fake news classifier. To avoid hand-crafted feature engineering, neural models have been proposed (Wang, 2017; Rodr´ıguez and Iglesias, 2019). For example, Ibrain et al. applied deep neural networks, such as BiLSTM and convolutional neural networks (CNN) for fake news detection (Rodr´ıguez and Iglesias, 2019). However, these works fail to consider different sentence interaction patterns between trusted and fake news documents. Vaibhav et al. proposed to model a document as a sentence graph capturing the sentence interactions and applied graph attention networks for"
2021.acl-long.62,D17-1317,0,0.247703,"2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing ap"
2021.acl-long.62,W16-0802,0,0.658737,"llcott and Gentzkow, 2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Alth"
2021.acl-long.62,D18-1209,0,0.0282805,"ural information from the triplets and textual information from the entity descriptions in the KB. 3.3.2 Entity Comparison We then perform entity-to-entity comparison between the news document and the KB, to capture the semantic consistency between the news content and the KB. We calculate a comparison vector ai between each contextual entity representation ec ∈ RN and its corresponding KB-based entity embedding eKB ∈ RM . ai = fcmp (ec , We · eKB ) , (6) where fcmp () denotes the comparison function, and We ∈ RN ×M is a transformation matrix. To measure the embedding closeness and relevance (Shen et al., 2018), we design our comparison function as: fcmp (x, y) = Wa [x − y, x y], (7) where Wa ∈ RN ×2N is a transformation matrix and is hadamard product, i.e., element-wise product. The final output comparison feature vector C ∈ RN is obtained by the max pooling over the alignment vectors A = [a1 , a2 , ..., an ] of all the entities E = {e1 , e2 , ..., en } in the news document. 3.4 Model Training After obtaining the comparison vector C ∈ RN and the final news document representation vector Hd ∈ RN , we concatenate and feed them into a Softmax layer for fake news classification. Formally, Z = Softmax(W"
2021.acl-long.62,N18-1074,0,0.0329643,"Missing"
2021.acl-long.62,D19-5316,0,0.253593,"rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality structured subjectpredicate-object triplets and unstructured entity descriptions, which could serve as evidence for detecting fake news. As shown in Figure 4, the news 754 Proceedings of the 59th Annual Meeting of the Association for Computation"
2021.acl-long.62,P17-2067,0,0.177697,"ial elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality structured s"
2021.acl-long.62,D19-1471,0,0.0120727,"ou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. constructed a bipartite network of user and posts with ‘like’ stance information, and proposed a semisupervised probabilistic model to predict the likelihood of posts being hoaxes (Tacchini et al., 2017). Propagation-based approaches for fake news detection are based on the basic assumption that the credibility of a news event is highly related to the credibilities of relevant social media posts. Both 755 Figure 1: An example of directed heterogeneous document graph incorporating topics and entities. homogeneous (Jin et al., 2016) and heterogeneous credibility networks (Gupta"
2021.acl-long.62,2020.acl-main.549,1,0.77437,"he KB. 3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information. 2 Related Work Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. co"
2021.acl-long.62,P19-1085,0,0.0180519,"mpare the news to the KB. 3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information. 2 Related Work Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019"
2021.emnlp-main.771,P17-1176,0,0.0166798,"ode. To facilitate the study of this task, we create a dataset with multiple programming languages. The dataset is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the b"
2021.emnlp-main.771,2020.emnlp-main.728,0,0.0161259,"an appropriate message, while the cascaded model fails may due to the error propagation. More examples on multilingual dataset are shown in Appendix D. 6 Related Work Our work is enlightened from two research lines of studies, which are automated code repair and commit message generation. We discuss these topics in the following. projects with numerous buggy-fixes pairs (Tufano et al., 2018; Chen et al., 2019; Vasic et al., 2019; Yasunaga and Liang, 2020). Tufano et al. (2018) first proposed using end-to-end neural machine translation model for learning bug-fixing patches. Besides, Guo et al. (2020) demonstrated that appropriately incorporating the natural language descriptions into the pre-train model could further improve the performance of code repair. Commit Message Generation Early work on automatic commit message generation translates source code changes (such as feature additions and bug repairs) into natural language based on predefined rules and templates (Buse and Weimer, 2010; Cortés-Coy et al., 2014). To overcome the limitation of high complexity and difficult extensibility, some researchers employ information retrieval methods to generate commit messages, which attempts to r"
2021.emnlp-main.771,D17-1158,0,0.0168867,"a dataset with multiple programming languages. The dataset is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method"
2021.emnlp-main.771,P84-1044,0,0.302985,"Missing"
2021.emnlp-main.771,Q17-1024,0,0.0151993,"ination between 9787 cφ = sof tmax zc zTφ √ H ! zφ (9) the output state of commit message decoder and the gated fusion of context representations, which can be calculated as:  T oc = zc + Wo gδ δ + gζ ζ T  Joint Training We jointly train our model in an end-to-end manner, the overall loss is defined as (13) where LR (θ), LC (θ)and LT (θ) are used to optimize the repaired code generation, commit message generation, and binary sequence classification, respectively. When training multilingual model of fixing code and predicting commit message, following multilingual neural machine translation (Johnson et al., 2017), we mix the training corpus and add a special token (e.g., <java>) at the beginning of each input sequence to distinguish from different programming languages. 4 Train Valid Test Total Multi. Python Java Javascript C-sharp Cpp 36682 11129 21446 5424 8510 4585 1391 2680 678 1063 4586 1392 2681 678 1064 45853 13912 26807 6780 10637 Mono. Java 47775 3000 3000 53775 (12) where Wo ∈ RH×H is the learnable weights. denotes the element-wise product. LJ (θ) = LR (θ) + LC (θ) + LT (θ) Languages Data In this section, we describe the creation of the dataset in detail. We first describe how we collect the"
2021.emnlp-main.771,P17-2045,0,0.116157,"ly reduce debugging costs in software development and helps 1 Introduction programmers to understand the high-level rationale Deep learning has been demonstrated remarkably of changes, a lot of great work has been proposed to adept at numerous natural language processing deal with automated program repair (Tufano et al., (NLP) tasks, such as machine translation (Bah- 2018; Chen et al., 2019; Dinella et al., 2020; Yadanau et al., 2014), relation extraction (Zhang et al., sunaga and Liang, 2020; Tang et al., 2021) and 2017), grammar error correction (Ge et al., 2018), commit message generation (Loyola et al., 2017; and so on. The success of deep learning in NLP Liu et al., 2020; Nie et al., 2020), respectively. also promotes the development of which in pro- However, existing work tackles the two tasks ingramming languages (Clement et al., 2020; Lu dependently, ignoring the underlying relationship et al., 2021). Recently, researchers have exploited between these two closely related tasks, e.g., afdeep learning to programming-language related ter fixing the bug, commit message can record the tasks, such as code completion (Svyatkovskiy et al., process of code repair. Therefore it is crucial to 2020), aut"
2021.emnlp-main.771,P16-1162,0,0.561635,"is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method behaves better than the cascaded method on commit messag"
2021.emnlp-main.771,2021.findings-acl.111,1,0.811582,"Missing"
2021.emnlp-main.771,P02-1040,0,0.109484,"e, and commit message. The statistics of the dataset used in this paper are summarized in Table 1. More processing details and statistics can be found in Appendix A and Appendix B. We release the datasets at https: //github.com/jqbemnlp/BFCsData. 5 5.1 Experiments Experimental Settings Evaluation Metrics We conduct evaluations on both code repair and commit message generation. For the code repair, we use exact match accuracy (Chen et al., 2018) to measure the percentage of the predicted fixed code that are exactly matching the truth fixed code. In addition, we also introduce the BLEU-4 score (Papineni et al., 2002) as a supplementary metric to evaluate their partial match. For the commit message generation, we use BLEU-4 and Rouge-L (Lin, 2004) to evaluate our model. 3 https://www.githubarchive.org https://docs.github.com/en/ free-pro-team@latest/rest 4 5 https://sites.google.com/view/ learning-fixes/data 9788 Models Naive Method Oracle Method Cascaded Model + Teacher-student + Multitask + Back-translation Joint Model Automated Code Repair BLEU-4 xMatch 87.45 0.00 85.07 3.21 88.23 6.16 87.94 8.33 87.73 5.26 87.61 8.01 Commit Message Generation BLEU-4 ROUGE-L 8.40 7.98 12.64 11.59 9.69 9.41 10.58 10.19 1"
2021.emnlp-main.771,D17-1182,0,0.0279627,"Missing"
2021.emnlp-main.771,W16-2323,0,0.311742,"is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method behaves better than the cascaded method on commit messag"
2021.findings-acl.121,P19-1279,0,0.0206704,"dopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-A DAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge in"
2021.findings-acl.121,P18-1009,0,0.0216531,"d analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-A DAPTER (F+L), K-A DAPTER (F), and K-A DAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix. 4.1 Entity Typing We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token “@” before and after a certain entity, then the first “@” special token representation is adopted to perform classification. As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works. Baselin"
2021.findings-acl.121,P19-1285,0,0.0618623,"Missing"
2021.findings-acl.121,N19-1423,0,0.0429486,"texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-A DAPTER captures versatile knowledge than RoBERTa. 1 1 Introduction Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al., ∗ Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/microsoft/K-Adapter 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) sugges"
2021.findings-acl.121,L18-1544,0,0.0249766,"ayer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-attention heads as AA , the hidden dimension of down-projection and up-projection layers as Hd and Hu . In detail, we have the following adapter size: N = 2, HA = 768, AA = 12, Hu = 1024 and Hd = 768. The RoBERTa lay2 3.3 Factual Adapter Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input repres"
2021.findings-acl.121,2020.findings-emnlp.71,0,0.198207,"Missing"
2021.findings-acl.121,W16-1313,0,0.118964,"-the-shell dependency parser from Stanford Parser3 on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer 3 https://github.com/huggingface/transformers 1408 http://nlp.stanford.edu/software/lex-parser.html OpenEntity Model FIGER P R Mi-F1 Acc Ma-F1 Mi-F1 NFGEC (Shimaoka et al., 2016) BERT-base (Zhang et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2021) WKLM (Xiong et al., 2020) 68.80 76.37 78.42 78.60 77.20 - 53.30 70.96 72.90 73.70 74.20 - 60.10 73.56 75.56 76.10 75.70 - 55.60 52.04 57.19 60.21 75.15 75.16 75.61 81.99 71.73 71.63 73.39 77.00 RoBERTa RoBERTa + multitask K-A DAPTER (w/o knowledge) K-A DAPTER (F) K-A DAPTER (L) K-A DAPTER (F+L) 77.55 77.96 74.47 79.30 80.01 78.99 74.95 76.00 74.91 75.84 74.00 76.27 76.23 76.97 76.17 77.53 76.89 77.61 56.31 59.86 56.93 59.50 61.10 61.81 82.43 84.45 82.56 84.52 83.61 84.87 77.83 7"
2021.findings-acl.121,D18-1244,0,0.0240646,"ion classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In part"
2021.findings-acl.121,D17-1004,0,0.069357,"Missing"
2021.findings-acl.121,P19-1139,0,0.231713,"s suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Sch¨utze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa. Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fad"
2021.findings-acl.121,P17-1018,1,0.80079,"elected. We report accuracy scores obtained from the leaderboard. Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets. To fine-tune our models for this task, the input token sequence is modified as “<SEP>question </SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose F1 (Ling and Weld, 2012) scores to evaluate our models. Baselines BERT-FTRACE+SW AG (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) propo"
2021.findings-acl.36,2020.emnlp-main.751,0,0.0340526,"the average number of words in source inputs. R-L: ROUGE-L. B-4: BLUE-4. MTR: METEOR. D-2: Distinct-2. 2.2 Tasks and Datasets GLGE contains eight English NLG tasks, covering text summarization, question generation, generative question answering, and dialogue. Descriptions and statistics of these tasks are shown in Table 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword co"
2021.findings-acl.36,W05-0909,0,0.337371,"urkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K passages from a real world search engine. Each passage contains a highlight span and a related query, we regard the queries as questions in this dataset. After the pre-processing, there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target output is a user question. ROUGE-L, BLEU-4,"
2021.findings-acl.36,2020.acl-main.9,0,0.366674,"Missing"
2021.findings-acl.36,L18-1269,0,0.017035,"the pretrained model. For these tasks, the non-pretrained model tends to generate universal responses (Li et al., 2016) or outputs. Moreover, there is still a huge gap between the bigram diversity of pretrained models and real samples (golden) on the task of XSUM, MSQG, and PersonaChat. Obviously, there exists great room for future improvement of the pretrained models in terms of output diversity. 4 Related Works Benchmarks Recently, the development of general natural language understanding (NLU) evaluation benchmarks has helped drive the progress of pretraining and transfer learning in NLP. Conneau and Kiela (2018) propose a toolkit, SentEval, for evaluating the quality of universal sentence representations. DecaNLP (McCann et al., 2018) casts ten diversified NLP tasks as a general question-answering format for evaluation. Wang et al. (2019b) propose a widely-used multi-task benchmark, GLUE, for NLU in the English language. There are nine NLU tasks in GLUE, including two single-sentence tasks, three similarity and paraphrase tasks, and four natural language inference tasks. After that, SuperGLUE (Wang et al., 2019a) is proposed as a harder counterpart of GLUE. Besides sentence- and sentence-pair classif"
2021.findings-acl.36,N19-1423,0,0.169557,"eration Evaluation (GLGE), a new multi-task benchmark for evaluating the generalization capabilities of NLG models across eight language generation tasks. For each task, we continue to design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and GLGE-Hard). This introduces 24 subtasks to comprehensively compare model performance. To encourage research on pretraining and transfer learning on NLG models, we make GLGE publicly available and build a leaderboard with strong baselines including MASS, BART, and Prophet1 Net . 1 Introduction Pretrained language models, such as BERT (Devlin et al., 2019) and other advanced pretrained models (Raffel et al., 2020; Yang et al., 2019; Liu et al., 2019; Alberti et al., 2019; Brown et al., 2020; Clark et al., 2020) have made great progress in a host of Natural Language Understanding (NLU) tasks. Meanwhile, the development of general evaluation benchmarks has also helped drive the progress of these models. These benchmarks usually use an overall score to evaluate the performance of models across a wide range of NLU tasks. In addition to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) which are general ∗ Work is done during internship at"
2021.findings-acl.36,P17-1123,0,0.0119354,". Compared with answer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MS"
2021.findings-acl.36,W18-2706,0,0.0130688,"he same pretraining data as Liu et al. (2019), consisting of 160GB of news, books, stories, and web text. For each task in GLGE, we fine-tune BARTlarge with the same hyper-parameters used in their source 10 code for a maximum of 20000 iterations. Except BART, all the baselines adopt BERTuncased tokenizer. We fine-tune all baselines on each individual task with 4 × 16GB NVIDIA V100 GPUs. We evaluate the best model checkpoint based on the loss on the development set. During inference, we use beam search (Och and Ney, 2004) with beam size 4 or 5 and remove the duplicated trigrams in beam search (Fan et al., 2018) to obtain 9 https://github.com/pytorch/fairseq/ tree/master/examples/bart 10 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md the generated results. 3.3 Results and Analysis Overall results. The main results are presented in Table 2. From the overall scores (highlighted in color), we can observe the fairly consistent gains moving from LSTM to Transformer, and then to pretrained base models and pretrained large models, such as ProphetNetbase and ProphetNetlarge . The performance gap between the pretrained model and non-pretrained model is obvious. The diff"
2021.findings-acl.36,2020.lrec-1.302,0,0.085243,"Missing"
2021.findings-acl.36,2020.acl-main.703,0,0.270812,"l language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for eva"
2021.findings-acl.36,N16-1014,0,0.162483,"mation as an additional condition to facilitate specific response generation. PersonaChat (Zhang et al., 2018) dataset consists of about 160K utterances, which requires the model to generate responses according to given multiturn conversations and persona profile. After preprocessing, there are 151,157 ⟨persona profile description text, conversation history, response⟩ data triples, where the source input is a sequence of conversation history along with several sentences of persona profile description information, and the target output is a response. BLEU-1, BLEU-2, Distinct-1, and Distinct-2 (Li et al., 2016) are used as the evaluation metrics. 2.3 Overall Score Similar to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), we seek to give an overall system performance over all GLGE tasks by aggregating the scores of all tasks. We follow GLUE to adopt a simple approach that weighs each task equally. For the tasks with multiple metrics, we firstly average those metrics to get a task score. Besides, because the values of the original Distinct1 (D-1) and Distinct-2 (D-2) (Li et al., 2016) scores which are used as the metrics for dialogue task are usually quite small (less than 0.01), we re-"
2021.findings-acl.36,W04-1013,0,0.0599605,"4: BLUE-4. MTR: METEOR. D-2: Distinct-2. 2.2 Tasks and Datasets GLGE contains eight English NLG tasks, covering text summarization, question generation, generative question answering, and dialogue. Descriptions and statistics of these tasks are shown in Table 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processin"
2021.findings-acl.36,2021.ccl-1.108,0,0.0321684,"Missing"
2021.findings-acl.36,D18-1206,0,0.0116702,"summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the British Broadcasting Corporation (BBC), which contains professionally written single-sentence summaries. After the preprocessing, there are 226,677 ⟨article, summary⟩ data pairs, where the source input is the news article, and the target output is a single-sentence summary. MSNews MicroSoft News headline generation (MSNews) is a new News headline generation dataset we collected for GLGE. We random select 151K online news articles from 2012-01-01 to 2020-09-01 from a real-world news search engine. Each article contains a professionally written single-s"
2021.findings-acl.36,J04-4002,0,0.0753216,"embedding/hidden size and 4096 feed-forward filter size. The pretraining of BARTlarge uses the same pretraining data as Liu et al. (2019), consisting of 160GB of news, books, stories, and web text. For each task in GLGE, we fine-tune BARTlarge with the same hyper-parameters used in their source 10 code for a maximum of 20000 iterations. Except BART, all the baselines adopt BERTuncased tokenizer. We fine-tune all baselines on each individual task with 4 × 16GB NVIDIA V100 GPUs. We evaluate the best model checkpoint based on the loss on the development set. During inference, we use beam search (Och and Ney, 2004) with beam size 4 or 5 and remove the duplicated trigrams in beam search (Fan et al., 2018) to obtain 9 https://github.com/pytorch/fairseq/ tree/master/examples/bart 10 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md the generated results. 3.3 Results and Analysis Overall results. The main results are presented in Table 2. From the overall scores (highlighted in color), we can observe the fairly consistent gains moving from LSTM to Transformer, and then to pretrained base models and pretrained large models, such as ProphetNetbase and ProphetNetlarge . The"
2021.findings-acl.36,P02-1040,0,0.109921,"on generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K passages from a real world search engine. Each passage contains a highlight span and a related query, we regard the queries as questions in this dataset. After the pre-processing, there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target outpu"
2021.findings-acl.36,2020.findings-emnlp.217,1,0.68917,"on benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for evaluating the generalization cap"
2021.findings-acl.36,D16-1264,0,0.287036,"cle, and the target output is a news headline. 2.2.2 Answer-aware Question Generation The question generation task is another typical NLG task, which aims to generate a question based on a given text passage or document. Compared with answer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie,"
2021.findings-acl.36,Q19-1016,0,0.0115463,"there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target output is a user question. ROUGE-L, BLEU-4, and METEOR are used as the metrics. 2.2.3 Conversational Question Answering Conversational question answering is a classic and popular generative question answering task. Compared with the extractive question answering, such as SQuAD (Rajpurkar et al., 2016), conversational question answering requires the model to answer the question based on a running conversation history and the given passage. CoQA (Reddy et al., 2019) dataset contains 127K questions with answers, obtained from 8K conversations about text passages from seven diverse domains. After the pre-processing, there are 116,630 ⟨conversation history, passage, question, answer⟩ data 4-tuples, where the source input is a sequence of conversation history along with a given question and a given passage, and the target output is a freeform answer text. F1-Score (Rajpurkar et al., 2016) is used as the metric. 2.2.4 Personalized Dialogue Conversational AI is an important topic in NLG. Compared with text summarization, the responses of single-turn conversati"
2021.findings-acl.36,2020.tacl-1.18,0,0.177338,"for English, several general language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new m"
2021.findings-acl.36,D15-1044,0,0.0308312,"contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the British Broadcasting Corporation (BBC), which contains professionally written single-sentence summaries. After the preprocessing, there are 226,677 ⟨article, summary⟩ data pairs, where the source input is the news"
2021.findings-acl.36,P17-1099,0,0.0378114,"ble 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the"
2021.findings-acl.36,2020.acl-main.704,0,0.0397801,"Missing"
2021.findings-acl.36,2020.aacl-main.85,0,0.247778,"chmarks usually use an overall score to evaluate the performance of models across a wide range of NLU tasks. In addition to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) which are general ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset are publicly available at https://github.com/microsoft/glge. language understanding evaluation benchmarks for English, several general language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated"
2021.findings-acl.36,P18-1205,0,0.0264513,"text. F1-Score (Rajpurkar et al., 2016) is used as the metric. 2.2.4 Personalized Dialogue Conversational AI is an important topic in NLG. Compared with text summarization, the responses of single-turn conversations are diverse and might lack of specification, and thus it is hard to use the single ground-truth for automatic evaluation. We select the personalizing dialogue task, which is a challenging multi-turn conversation task. In addition to the conversation history, this task gives the profile information as an additional condition to facilitate specific response generation. PersonaChat (Zhang et al., 2018) dataset consists of about 160K utterances, which requires the model to generate responses according to given multiturn conversations and persona profile. After preprocessing, there are 151,157 ⟨persona profile description text, conversation history, response⟩ data triples, where the source input is a sequence of conversation history along with several sentences of persona profile description information, and the target output is a response. BLEU-1, BLEU-2, Distinct-1, and Distinct-2 (Li et al., 2016) are used as the evaluation metrics. 2.3 Overall Score Similar to GLUE (Wang et al., 2019b) an"
2021.findings-acl.36,2020.acl-demos.30,0,0.075506,"Missing"
2021.findings-acl.36,D18-1424,0,0.0119763,"nswer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K"
2021.findings-emnlp.75,2020.acl-main.421,0,0.0284757,"age, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc pt ro scn sco sq sv tl ur war ar arz bg bs cy fa hi hr id is mg mk ms ps ru sh sl so #2 sr su sw yi am as be ckb cs et eu fi he hu ja jv km la lo lt lv mr my"
2021.findings-emnlp.75,Q19-1038,0,0.0232089,"ar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc pt ro scn sco sq sv tl ur war ar arz bg bs cy fa hi hr id is mg mk ms ps ru sh sl so #2 sr su sw yi am as be ckb cs et eu fi he hu ja jv km la lo lt lv mr my #3 ne or pa pl sa sd sk th uk wuu zh #4 az bn gu hy ka kk kn ko k"
2021.findings-emnlp.75,2020.emnlp-main.367,0,0.0200812,"ag added to the input of encoder. Their approach focuses on 23 relatively high resource languages. Fan et al. (2020) clusters languages into several groups according to language family, cultural connection and geographical proximity. They do not obtain any language representation and their language groups are human 2 Related Work annotated. Kudugunta et al. (2019) reveals the connection between language SVCCA similarity from Our approach presents a new pipeline of multilinNMT models and language family. Their evaluagual pre-training. Our representation sprachbund tion relies on parallel data. Chung et al. (2020) clasis inspired by linguistic language clustering. Our sifies languages into groups based on their token analysis is closely related to methodology in linoverlap. Only lexical information of languages is guistics. used in their approach. Yu et al. (2021) uses multiMultilingual Pre-Training Multilingual pre- lingual denoising autoencoder to generate language training was proposed to pre-train a single model embeddings and analyze the clusters derived from with hundreds of languages. Many works use a the embeddings. There are also a few earlier works large amount of multilingual data (e.g., mC4"
2021.findings-emnlp.75,2020.tacl-1.30,0,0.0214491,"ical feature for each language, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc pt ro scn sco sq sv tl ur war ar arz bg bs cy fa hi hr id is mg mk ms ps ru sh sl so #2 sr su sw yi am as be ckb cs et eu fi he h"
2021.findings-emnlp.75,2020.acl-main.747,0,0.243341,"ge representation from multilingual pre-trained models and massive multilingual corpora. ii) We conduct extensive analysis to show language representation and representation sprachbunds can reflect linguistic language similarity and relatedness from multiple perspectives, therefore they can be considered as new paradigm for clustering similar languages in linguistics. iii) We use representation sprachbunds in multilingual pre-training to alleviate the cross-lingual contradiction and differences, and obtain significant improvements compared with strong baselines. ple and Conneau, 2019), XLM-R (Conneau et al., 2020), Unicoder (Huang et al., 2019) and mT5 (Xue et al., 2020). Several benchmarks are proposed to evaluate the cross-lingual ability of multilingual pre-trained models, including XGLUE (Liang et al., 2020), XTREME (Hu et al., 2020) and XTREME-R (Ruder et al., 2021) Language Clustering in Linguistics The linguists propose to classify languages in several ways from different perspectives. There are two main kinds of language clustering: genealogical clustering and typological clustering. In genealogical clustering, languages are clustered into language families (Durbin, 1985; Marcantonio, 2002) by"
2021.findings-emnlp.75,D18-1269,0,0.0294155,"tures of languages, therefore the similarity between language representation can be a good metric for clustering languages. With a 768-dimension vector as numerical feature for each language, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl"
2021.findings-emnlp.75,Y17-1038,0,0.0627517,"Missing"
2021.findings-emnlp.75,N19-1423,0,0.0694387,"Missing"
2021.findings-emnlp.75,E17-2002,0,0.0205942,"ical cognates between languages (Hymes, 1960), which is very time-consuming. Our language representation can even further help linguists infer lexical similarity more easily (e.g. linear regression between representation similarity and lexical similarity). The similarity data is shown in Appendix B. Relationship with Language Syntax Languages have diverse syntactic features defined by linguists and can be classified through these features. We show that the distribution of our language representation implies the syntactic features of corresponding languages. We use the lang2vec Python package (Littell et al., 2017) to query the URIEL database7 . We choose three representative syntactic features: (subject, object, verbal) word order, adjective position and adposition position. As shown in Figure 3, we find that languages with the same syntactic features approximately have similar language representation. 7 http://www.cs.cmu.edu/~dmortens/uriel. html 885 Surprise: Help for Exploring Linguistic Mystery Coincidentally, we find that our language representation connect with an existing underexplored linguistic mystery. In Figure 2, Uralic and Austronesian languages (jv, id, ms) have similar language represent"
2021.findings-emnlp.75,D19-1089,0,0.039809,"Missing"
2021.findings-emnlp.75,2020.lrec-1.494,0,0.0191531,"to groups based on their token analysis is closely related to methodology in linoverlap. Only lexical information of languages is guistics. used in their approach. Yu et al. (2021) uses multiMultilingual Pre-Training Multilingual pre- lingual denoising autoencoder to generate language training was proposed to pre-train a single model embeddings and analyze the clusters derived from with hundreds of languages. Many works use a the embeddings. There are also a few earlier works large amount of multilingual data (e.g., mC4 (Xue on generating and analyzing language representaet al., 2020), CCNet (Wenzek et al., 2020)) to pre- tion (Tiedemann, 2018; Östling and Tiedemann, train large multilingual models like XLM (Lam- 2017). 882 Figure 1: The pipeline of our approach. We first generate language representation for each language with multilingual pre-trained models (XLM-R) and multilingual corpora. We cluster languages into several representation sprachbunds composed of languages with similar representation. We pre-train one model for each representation sprachbund with corpora from that representation sprachbund. Best viewed in color. Compared to existing works, our approach enjoys the following advantages."
2021.findings-emnlp.75,D19-1382,0,0.0170242,"he similarity between language representation can be a good metric for clustering languages. With a 768-dimension vector as numerical feature for each language, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc"
2021.findings-emnlp.75,2021.acl-long.560,0,0.0340023,"anguage representation and their language groups are human 2 Related Work annotated. Kudugunta et al. (2019) reveals the connection between language SVCCA similarity from Our approach presents a new pipeline of multilinNMT models and language family. Their evaluagual pre-training. Our representation sprachbund tion relies on parallel data. Chung et al. (2020) clasis inspired by linguistic language clustering. Our sifies languages into groups based on their token analysis is closely related to methodology in linoverlap. Only lexical information of languages is guistics. used in their approach. Yu et al. (2021) uses multiMultilingual Pre-Training Multilingual pre- lingual denoising autoencoder to generate language training was proposed to pre-train a single model embeddings and analyze the clusters derived from with hundreds of languages. Many works use a the embeddings. There are also a few earlier works large amount of multilingual data (e.g., mC4 (Xue on generating and analyzing language representaet al., 2020), CCNet (Wenzek et al., 2020)) to pre- tion (Tiedemann, 2018; Östling and Tiedemann, train large multilingual models like XLM (Lam- 2017). 882 Figure 1: The pipeline of our approach. We fir"
2021.naacl-main.280,D19-1252,1,0.753364,"Missing"
2021.naacl-main.280,L18-1548,0,0.0642253,"oss-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018"
2021.naacl-main.280,2020.acl-main.653,0,0.715277,"h monolingual and parallel corpora. Contribution during internship at Microsoft Research. Contact person: Li Dong and Furu Wei. We jointly train I NFOXLM with MMLM, TLM 3576 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576–3588 June 6–11, 2021. ©2021 Association for Computational Linguistics and X L C O. We conduct extensive experiments on several cross-lingual understanding tasks, including cross-lingual natural language inference (Conneau et al., 2018), cross-lingual question answering (Lewis et al., 2020), and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019). Experimental results show that I NFOXLM outperforms strong baselines on all the benchmarks. Moreover, the analysis indicates that I NFOXLM achieves better cross-lingual transferability. 2 2.1 Related Work between the sampled positive and negative pairs. In addition to the estimators, various view pairs are employed in these methods. The view pair can be the local and global features of an image (Hjelm et al., 2019; Bachman et al., 2019), the random data augmentations of the same image (Tian et al., 2019; He et al., 2020; Chen"
2021.naacl-main.280,L16-1561,0,0.0179259,"OXLM with previous work on three cross-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inf"
2021.naacl-main.280,2020.findings-emnlp.147,0,0.110105,"Missing"
2021.naacl-main.280,tiedemann-2012-parallel,0,0.0390072,"e also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018) is a widely used cross"
2021.naacl-main.312,R13-1027,0,0.0105799,"tence X of length m and target sentence Y of length n, we can construct n intermediate sentences Zk = (yn−k+1 , . . . , yn , [m], y1 , . . . , yn−k )(k ∈ [1, n]). Because the target sentence length n can be too long, we randomly sample S intermediate sentences from n intermediate sentences to construct the subset SY , where S is the number of sampled start positions. We apply scores calculated by the hard or soft Smart-Start methods to the loss of different intermediate samples to teach model which start position is better. This procedure can be described by the weighted log-likelihood (WML) (Dimitroff et al., 2013) reward function L over the dataset D as below:   wk log Pθ (Zk |X) L= (2) X,Y ∈D Zk ∈SY where SY is the subset containing S samples. wk is calculated by the hard or soft Smart-Start methods. For the hard Smart-Start method, we use the median training loss of intermediate samples as threshold to select appropriate samples to update model parameters. We calculate wk by comparing the training loss generated by the current model of each Zk from SY with the threshold as below: Our Smart-Start method is extremely interested in breaking up the limitation of this decoding order. Different from the"
2021.naacl-main.312,Q19-1042,0,0.033751,"Missing"
2021.naacl-main.312,W18-6301,0,0.0135741,"4000 warming-up steps. We set the number of sampled start positions S = 8 described as Equation 2. For the LDC Zh→En translation task, we use the Transformer_base setting with the embedding size as 512 and feed-forward network (FFN) size as 2048. For the IWSLT14 De→En translation task, we use the Transformer_small setting with embedding size as 512 and FFN size as 1024. The dropout is set as 0.3 and weight decay as 0.0001 to prevent overﬁtting. For the WMT14 En→De translation task, we use the Transformer_big setting with embedding size as 1024 and FFN size as 4096. Following the previos work (Ott et al., 2018), we accumulate the gradient for 16 iterations to simulate a 128-GPU environment. 3.2 3.3 In this section, we evaluate our method on three popular benchmarks. 3.1 Dataset Training Details Baselines and Results We conduct experiments on 8 NVIDIA 32G V100 We compare our method with the other baseGPUs and set batch size as 1024 tokens. In lines, including Transformer (Vaswani et al., 2017), RP Transformer (Shaw et al., 2018), Lightthe training stage, we adopt the Adam optimizer 3984 Zh → En MT06 MT02 MT03 MT05 MT08 MT12 Avg LightConv (Wu et al., 2019) DynamicConv (Wu et al., 2019) 43.41 43.65 42."
2021.naacl-main.312,P02-1040,0,0.109436,"Missing"
2021.naacl-main.312,W16-2323,0,0.114321,"he dataset D as below:   wk log Pθ (Zk |X) L= (2) X,Y ∈D Zk ∈SY where SY is the subset containing S samples. wk is calculated by the hard or soft Smart-Start methods. For the hard Smart-Start method, we use the median training loss of intermediate samples as threshold to select appropriate samples to update model parameters. We calculate wk by comparing the training loss generated by the current model of each Zk from SY with the threshold as below: Our Smart-Start method is extremely interested in breaking up the limitation of this decoding order. Different from the traditional L2R and R2L (Sennrich et al., 2016a), our Smart-Start method wk = δLk ≥Lmed (3) predicts median word yn−k+1 over the source sentence. Furthermore, we predict the right part of tarwhere δLk ≥Lmed equals to 1 if Lk ≥ Lmed else 0. get sentence (yn−k+1 , . . . , yn ) sequentially which Lmed is the median loss of the sample in SY . For is on the right part of this word. Finally, we gener- each intermediate sentence Zk ∈ SY , the objective ate the rest words (y1 , . . . , yn−k ) on the left part of of Zk is denoted as Lk = log Pθ (Zk |X). 3983 Final Translation ܻ : ݕଵ ݕ୬ି୩ାଵ 濷 濷 ݕ୬ି୩ Left-to-Right ܼ ՜ ܻ Intermediate Translati"
2021.naacl-main.312,P16-1162,0,0.430013,"he dataset D as below:   wk log Pθ (Zk |X) L= (2) X,Y ∈D Zk ∈SY where SY is the subset containing S samples. wk is calculated by the hard or soft Smart-Start methods. For the hard Smart-Start method, we use the median training loss of intermediate samples as threshold to select appropriate samples to update model parameters. We calculate wk by comparing the training loss generated by the current model of each Zk from SY with the threshold as below: Our Smart-Start method is extremely interested in breaking up the limitation of this decoding order. Different from the traditional L2R and R2L (Sennrich et al., 2016a), our Smart-Start method wk = δLk ≥Lmed (3) predicts median word yn−k+1 over the source sentence. Furthermore, we predict the right part of tarwhere δLk ≥Lmed equals to 1 if Lk ≥ Lmed else 0. get sentence (yn−k+1 , . . . , yn ) sequentially which Lmed is the median loss of the sample in SY . For is on the right part of this word. Finally, we gener- each intermediate sentence Zk ∈ SY , the objective ate the rest words (y1 , . . . , yn−k ) on the left part of of Zk is denoted as Lk = log Pθ (Zk |X). 3983 Final Translation ܻ : ݕଵ ݕ୬ି୩ାଵ 濷 濷 ݕ୬ି୩ Left-to-Right ܼ ՜ ܻ Intermediate Translati"
2021.naacl-main.312,N18-2074,0,0.0279932,"Missing"
2021.naacl-main.312,P18-2054,0,0.0189744,"models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; He et al., 2018). Asynchronous and synchronous Bidirectional decoding Model (Zhang et al., 2018; Zhou et al., 2019b) exploits the contexts generated in the R2L manner to help the L2R translation. Previous non-monotonic methods (Serdyuk et al., 2018; Zhang et al., 2018; Zhou et al., 2019a,b; Zhang et al., 2019; Welleck et al., 2019) jointly leverage L2R and R2L information. Non-monotonic methods are also widely used in many tasks (Huang et al., 2018; Shu and Nakayama, 2018), such as parsing (Goldberg and Elhadad, 2010), image caption (Mehri and Sigal, 2018), and dependency parsing (Kiperwasser and Goldberg, 2016; Li et al., 2019). Similarly, insertion-based method (Gu et al., 2019; Stern et al., 2019) predicts the next token and its position to be inserted. 5 Conclusion In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. Our method predicts a median word and then generates the words on the right part. Finally, it generates words on the left. Experimental results show that our Smart-Start me"
2021.naacl-main.312,Q16-1032,0,0.0174688,"al., 2017; He et al., 2018). Asynchronous and synchronous Bidirectional decoding Model (Zhang et al., 2018; Zhou et al., 2019b) exploits the contexts generated in the R2L manner to help the L2R translation. Previous non-monotonic methods (Serdyuk et al., 2018; Zhang et al., 2018; Zhou et al., 2019a,b; Zhang et al., 2019; Welleck et al., 2019) jointly leverage L2R and R2L information. Non-monotonic methods are also widely used in many tasks (Huang et al., 2018; Shu and Nakayama, 2018), such as parsing (Goldberg and Elhadad, 2010), image caption (Mehri and Sigal, 2018), and dependency parsing (Kiperwasser and Goldberg, 2016; Li et al., 2019). Similarly, insertion-based method (Gu et al., 2019; Stern et al., 2019) predicts the next token and its position to be inserted. 5 Conclusion In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. Our method predicts a median word and then generates the words on the right part. Finally, it generates words on the left. Experimental results show that our Smart-Start method signiﬁcantly improves the quality of translation. Training Time The Transformer baseline costs Acknowledgments nearly 0.9 hours and our"
2021.naacl-main.312,P07-2045,0,0.00927966,"riments Number of Sampled Start Positions Figure 3: Results of different values of the number of sampled start positions on IWSLT14 De→En test set. IWSLT14 De-En corpus contains 16K training sequence pairs. The valid and test set both contain 7K sentence pairs. LDC Zh-En corpus is from the LDC corpus. The training data contains 1.4M sentence pairs. NIST 2006 is used as the valid set. NIST 2002, 2003, 2005, 2008, and 2012 are used as test sets. WMT14 En-De corpus has 4.5M sentence pairs. The newstest2013 and the newstest2014 are used as valid the test set. All languages are tokenized by Moses (Koehn et al., 2007) and our Chinese tokenizer, and then encoded using byte pair encoding (BPE) (Sennrich et al., 2016b) with 40K merge operations. The evaluation metric is BLEU (Papineni et al., 2002). (β1 = 0.9, β2 = 0.98) (Kingma and Ba, 2015) using the inverse sqrt learning rate schedule (Vaswani et al., 2017) with a learning rate of 0.1 and 4000 warming-up steps. We set the number of sampled start positions S = 8 described as Equation 2. For the LDC Zh→En translation task, we use the Transformer_base setting with the embedding size as 512 and feed-forward network (FFN) size as 2048. For the IWSLT14 De→En tra"
2021.naacl-main.312,D15-1166,0,0.0539853,"affecting the training time is the number of sampled start positions. We also investigate the proper value of the number of sampled start positions. In practice, smaller value such as 4 or 6 can also bring signiﬁcant improvements. Therefore, we choose a smaller value of the sampled start positions and use multiple GPUs to keep the training time in a reasonable range. 4 Related Work Neural Machine Translation (NMT) has attracted a lot of attention recently. The architecture of NMT models has evolved quickly so that there are many different models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; He et al., 2018). Asynchronous and synchronous Bidirectional decoding Model (Zhang et al., 2018; Zhou et al., 2019b) exploits the contexts generated in the R2L manner to help the L2R translation. Previous non-monotonic methods (Serdyuk et al., 2018; Zhang et al., 2018; Zhou et al., 2019a,b; Zhang et al., 2019; Welleck et al., 2019) jointly leverage L2R and R2L information. Non-monotonic methods are also widely used in many tasks (Huang et al., 2018; Shu and Nakayama, 2018), such as parsing (Goldberg and Elhadad, 2010), im"
2021.naacl-main.312,W19-3620,0,0.0789923,"ersonality 晝ⴷ ⷿ㘖 . very 濿濼瀉濸濿瀌 Happy to talk with people , Yang Sen has a lively personality (a) Left-to-Right Translation: Le talks with people , Yang Sen is very lively . Smart-Start: Yang Sen has a lively personality . [m] Chatting with people , (b) Translation: Chatting with people , Yang Sen has a lively personality . Figure 1: Example of baseline method (a) and our Smart-Start method (b). “[m]” is designed to indicate the termination of the right part generation. “[m]” is an abbreviation of “[middle]”. There are some related works on non-monotonic text generation (Mehri and Sigal, 2018; Welleck et al., 2019; Gu et al., 2019; Zhou et al., 2019b,a). 1 Introduction Inspired by these works, we are extremely interNeural machine translation (NMT) has made re- ested in considering choosing one proper position to start decoding instead of L2R or R2L order. We markable progress in recent years. There has been much progress in encoder-decoder framework, in- propose a novel method called the Smart-Start decluding recurrent neural models (Wu et al., 2016), coding method. Speciﬁcally, our method starts the generation of target words from the right part of convolutional models (Gehring et al., 2017) and self-"
C02-1010,P00-1050,1,0.887476,"Missing"
C02-1010,C00-2131,0,0.0314432,"Missing"
C02-1010,C92-2101,0,\N,Missing
C02-1010,W00-0726,0,\N,Missing
C02-1010,P93-1004,0,\N,Missing
C02-1010,C96-1078,0,\N,Missing
C02-1010,P00-1015,1,\N,Missing
C02-1010,J97-3002,0,\N,Missing
C02-1010,P98-2221,0,\N,Missing
C02-1010,C98-2216,0,\N,Missing
C02-1012,W98-1120,0,\N,Missing
C02-1012,M98-1004,0,\N,Missing
C02-1012,M98-1012,0,\N,Missing
C02-1012,M98-1014,0,\N,Missing
C02-1012,M98-1021,0,\N,Missing
C02-1012,J92-4003,0,\N,Missing
C02-1012,M98-1017,0,\N,Missing
C02-1057,2001.mtsummit-papers.25,0,0.293303,"Missing"
C02-1057,2001.mtsummit-papers.67,0,0.0972107,"-occurrence and so on as indicators of translation quality. Brew C (1994) compares human rankings and automatic measures to decide the translation quality, whose criteria involve word frequency, POS tagging distribution and other text features. Another type of evaluation method involves comparison of the translation result with human translations. Yokoyama (2001) proposed a two-way MT based evaluation method, which compares output Japanese sentences with the original Japanese sentence for the word identification, the correctness of the modification, the syntactic dependency and the parataxis. Yasuda (2001) evaluates the translation output by measuring the similarity between the translation output and translation answer candidates from a parallel corpus. Akiba (2001) uses multiple edit distances to automatically rank machine translation output by translation examples. Another path of machine translation evaluation is based on test suites. Yu (1993) designs a test suite consisting of sentences with various test points. Guessoum (2001) proposes a semi-automatic evaluation method of the grammatical coverage machine translation systems via a database of unfolded grammatical structures. Koh (2001) de"
C02-1057,2001.mtsummit-papers.68,0,0.060773,"Missing"
C02-1057,2001.mtsummit-papers.3,0,\N,Missing
C02-1057,bohan-etal-2000-evaluating,0,\N,Missing
C02-1057,2001.mtsummit-papers.35,0,\N,Missing
C02-1057,H94-1019,0,\N,Missing
C02-1057,C00-1055,0,\N,Missing
C02-1060,P94-1038,0,\N,Missing
C02-1060,P93-1022,0,\N,Missing
C02-1060,P90-1034,0,\N,Missing
C02-1060,P97-1009,0,\N,Missing
C02-1060,P98-2148,0,\N,Missing
C02-1060,C98-2143,0,\N,Missing
C08-1048,J93-2003,0,0.0546689,"Missing"
C08-1048,P05-1033,0,0.110291,"Missing"
C08-1048,P03-1035,0,0.0433035,"s a linear regression of the human evaluation scores as a function of the BLEU score for our 6 systems which generate SSs given FSs. Among the 6 systems, three are implemented using a word-based SMT model with 100K, 400K, and 970K couplets for training, respectively, while the other three are implemented using a phrase-based SMT model with 100K, 400K, and 970K couplets for training, respectively. The word-based SMT model contains only two features: word translation model and language model. The word translation model is trained on the corpus segmented by a Chinese word breaker implemented by (Gao et al., 2003). We selected 100 FSs from the testing data set; for each of them, the best SS candidate was generated using each system. Then we computed the BLEU score and the human score of each system. The human score is the average score of all SS candidates. Each candidate is scored 1 if it is acceptable, and 0 if not. The correlation of 0.92 indicates that BLEU tracks human judgment well. http://svmlight.joachims.org/ 382 0.276 of BLEU score. When we add more features incrementally, the BLEU score is improved consistently. Furthermore, with the Ranking SVM model, the score is improved by 0.13 percent,"
C08-1048,N03-1017,0,0.0889196,"corpus: 379 p( fi |si )  3.2 count ( fi, si ) m (2)  count ( f , s ) r i r 1 where m is the number of distinct phrases that can be mapped to the phrase si and count ( fi, si ) is the number of occurrences that fi and si appear at the corresponding positions in a couplet. The inverted phrase translation model p( si |fi ) has been proven useful in previous SMT research work (Och and Ney, 2002); so we also include it in our phrase-based SMT model. Lexical weight (LW) Previous research work on phrase-based SMT has found that it is important to validate the quality of a phrase translation pair (Koehn et al., 2003). A good way to do this is to check its lexical weight pw( fi |si ) , which indicates how well its words translate to each other: Ni pw( fi |si )   p( fj |sj ) j 1 (3) where Ni is the number of characters in fi or si , fj and sj are characters in fi and si respectively, and p( fj |sj ) is the character translation probability of sj into fj . Like in phrase translation probability estimation, p( fj |sj ) can be computed by relative frequency: p( f j |s j )  count ( f j , s j ) m  count ( f r 1 r ,sj) (4) where m is the number of distinct characters that can be mapped to the character sj a"
C08-1048,P03-1021,0,0.0700523,"are usually reversible, to alleviate the data sparseness, we reverse the FS and SS in the training couplets and merge them with original training data for estimating translation probabilities. For the language people use in Chinese couplets is same as that in Chinese poetry, for the purpose of smoothing the language model we add about 1,600,000 sentences from ancient Chinese poetry to train language model, which are not necessarily couplets. To estimate the weights λi in formula (1), we use Minimum Error Rate Training (MERT) algorithm, which is widely used for phrase-based SMT model training (Och, 2003). The training data and criteria (BLEU) for MERT will be explained in Subsection 5.1. 380 In this section, we will detail each step of the generation of the second sentence. Phonetic harmony filter We filter out the SSs with improper tones at the end character position according to the Chinese character pronunciation dictionary. 4.1 4.3 4 Couplet Generation Decoding for N-best Candidates First, we use a phrase-based decoder similar to the one by (Koehn et al., 2003) to generate an Nbest list of SS candidates. Because there is no word reordering operation in the SS generation, our decoder is a"
C08-1048,P02-1038,0,0.070095,"style make Chinese couplets have more regular form. Given the FS, writing a good SS to match it is a difficult task because the SS must conform to constraints on syntax, rhyme and semantics, as described above. It also requires the writer to innovatively use extensive knowledge in different disciplines. Some of the difficulties can be seen from the following example: 378 有 have 女 daughter 有 have 子 son 方 so 称 call 好 good | | | | | | | 缺 lack 鱼 fish 缺 lack 羊 mutton 敢 dare 叫 call 鲜 delicious S  {s1 , s2 ,..., sn } , where fi and si are Chinese characters, so that p(S|F) is maximized. Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model. Then the S* that maximizes p(S|F) can be expressed as follows: Figure 3. An Example of a Complicated Couplet. Figure 3 shows a complicated couplet of “有 女 有 子 方 称 好; 缺 鱼 缺 羊 敢 叫 鲜” (Once one has a daughter and son, one’s life is complete; who would dare call a meal without fish and mutton delicious? In China, there is an old saying that courses made of fish and mutton are most delicious). The FS contains a repeated character “有” (have), and a character decomposition: “好” (good) and its “component c"
C08-1048,J04-4002,0,0.0644237,"gmented into phrases s1...sI and f1... f I , respectively. We assume a uniform distribution over all possible segmentations. I h1( S , F )   p( fi |si ) Couplet Generation Model Phrase-based SMT Model Given a FS denoted as F  { f1 , f 2 ,..., f n } , our objective is to seek a SS denoted as (1) i 1 S i 1 I In this paper, a multi-phase SMT approach is designed, where an SMT system generates an Nbest list of candidates and then a ranking model is used to determine the new ranking of the Nbest results using additional features. This approach is similar to recent reranking approaches of SMT (Och and Ney, 2004). In the SMT system, a phrase-based log-linear model is applied where two phrase translation models, two lexical weights and a language model are used to score the output sentences, and a monotone phrasebased decoder is employed to get the N-best results. Then a set of filters based on linguistic constraints of Chinese couplets are used to remove candidates of low quality. Finally a Ranking SVM model is used to rerank the candidates using additional features like word associations, etc. 3.1 S *  arg max p ( S |F ) h2( S , F )   p( si |fi ) i 1 Phrase translation model Inverted phrase trans"
C08-1048,P02-1040,0,0.0847275,"190 FSs with 9,500 labeled SS candidates (negative: 6,728; positive: 2,772) to train the Ranking SVM model. 5 5.1 Experimental Results Evaluation Method Automatic evaluation is very important for parameter estimation and system tuning. An automatic evaluation needs a standard answer data set and a metric to show for a given input sentence the closeness of the system output to the standard answers. Since generating the SS given the FS can be viewed as a kind of machine translation process, the widely accepted automatic SMT evaluation methods may be applied to evaluate the generated SSs. BLEU (Papineni, et al, 2002) is widely used for automatic evaluation of machine translation systems. It measures the similarity between the MT system output and human-made reference translations. The BLEU metric ranges from 0 to 2 1 and a higher BLEU score stands for better translation quality. N BLEU  BP  exp(  wn log pn) (8) n1 Some adaptation is necessary to use BLEU for evaluation of our couplet generator. First, pn, the n-gram precision, should be position-sensitive in the evaluation of SSs. Second, BP, the brevity penalty, should be removed, because all system outputs have the same length and it has no effect i"
C08-1141,E06-1032,0,0.273045,"phenomena to provide diagnostic evaluation. The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems. 1 Introduction Automatic MT evaluation is a crucial issue for MT system developers. The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU (Papineni et al., 2002) and its variants. Ever since its invention, the BLEU score has been a widely accepted benchmark for MT system evaluation. Nevertheless, the research community has been aware of the deficiencies of the BLEU metric (Callison-Burch et al., 2006). For instance, BLEU fails to sufficiently capture the vitality of natural languages: all grams of a sentence are © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. treated equally ignoring their linguistic significance; only consecutive grams are considered ignoring the skipped grams of certain linguistic relations; candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference ignoring the variation in lexical choice. Furthermore, BL"
C08-1141,A00-2019,0,0.0155641,"Missing"
C08-1141,W04-3250,0,0.149378,"Missing"
C08-1141,P04-1077,0,0.0226721,"uding 3,200 pairs of sentences containing 6 classes of check-points. Their check-points were manually constructed by human experts, therefore it will be costly to build new test corpus while the check-points in our approach are constructed automatically. Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching. There are many recent work motivated by ngram based approach. (Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level. (Lin and Och, 2004) used longest common subsequence and skipbigram statistics. (Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses. (Liu et al., 2005) used syntactic features and unlabeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment. (Gimenez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings. (Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greate"
C08-1141,W05-0904,0,0.0285694,"Missing"
C08-1141,J03-1002,0,0.00842467,"Missing"
C08-1141,P02-1040,0,0.0750984,"Missing"
C08-1141,W00-1212,1,0.735942,"Missing"
C08-1141,P03-1054,0,0.00524886,"Missing"
C08-1141,W07-0738,0,\N,Missing
C08-1141,W05-0909,0,\N,Missing
C10-1035,W09-0437,0,0.0415685,"Missing"
C10-1035,J07-2003,0,0.0991921,"Missing"
C10-1035,N10-1141,0,0.0470614,"Missing"
C10-1035,D07-1079,0,0.0330617,"Missing"
C10-1035,W06-1607,0,0.0862014,"Missing"
C10-1035,W07-0717,0,0.1147,"Missing"
C10-1035,P06-1121,0,0.170188,"Missing"
C10-1035,P08-1067,0,0.0606721,"Missing"
C10-1035,D08-1011,0,0.0336369,"Missing"
C10-1035,P09-1107,0,0.034877,"Missing"
C10-1035,P09-1066,1,0.872421,"Missing"
C10-1035,P09-1065,0,0.0623191,"Missing"
C10-1035,D08-1066,0,0.0418409,"Missing"
C10-1035,E06-1005,0,0.0843704,"Missing"
C10-1035,P00-1056,0,0.249082,"Missing"
C10-1035,P03-1021,0,0.0605587,"Missing"
C10-1035,J04-4002,0,0.109408,"Missing"
C10-1035,P08-1066,0,0.0397231,"Missing"
C10-1035,J97-3002,0,0.0995427,"Missing"
C10-1035,P06-1066,0,0.0484801,"Missing"
C10-1035,C08-1144,0,0.033509,"Missing"
C10-1036,J07-2003,0,0.117997,"Missing"
C10-1036,P09-1107,0,0.113563,"Missing"
C10-1036,P08-1023,0,0.055569,"Missing"
C10-1036,W05-1506,0,0.103769,"Missing"
C10-1036,P08-1067,0,0.0669287,"Missing"
C10-1036,W04-3250,0,0.0749687,"Missing"
C10-1036,N04-1022,0,0.722962,"Missing"
C10-1036,P09-1019,0,0.462334,"Missing"
C10-1036,P03-1021,0,0.202425,"Missing"
C10-1036,J04-4002,0,0.151503,"Missing"
C10-1036,P07-1040,0,0.0636909,"Missing"
C10-1036,D08-1065,0,0.787903,"n model’s distribution. Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. Their work has shown that MBR decoding performs better than Maximum a Posteriori (MAP) decoding for different evaluation criteria. After that, many dedi1 This work has been done while the author was visiting Microsoft Research Asia. Mu Li, Dongdong Zhang, Ming Zhou Microsoft Research Asia muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com cated efforts have been made to improve the performances of SMT systems by utilizing MBRinspired methods. Tromble et al. (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al. (2009) presented more efficient algorithms for MBR decoding on both lattices and hypergraphs to alleviate the high computational cost problem in Tromble et al.’s work. DeNero et al. (2009) proposed a fast consensus decoding algorithm for MBR for both linear and non-linear similarity measures. All work mentioned above share a common setting: an MBR decoder is built based on one and only one MAP decoder. On the other hand, recent researc"
C10-1036,W02-1021,0,0.0878161,"Missing"
C10-1036,P09-1065,0,0.171644,"Missing"
C10-1036,D08-1022,0,\N,Missing
C10-1036,D07-1105,0,\N,Missing
C10-1036,P09-1066,1,\N,Missing
C10-1036,P06-1066,0,\N,Missing
C10-1036,P09-1064,0,\N,Missing
C10-1036,J97-3002,0,\N,Missing
C10-1036,2008.amta-srw.3,0,\N,Missing
C10-1054,W09-3109,0,0.0116525,"esearch Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. Each TL document pairs up with the SL document to form a cand"
C10-1054,J93-2003,0,0.0386299,"ains some extra phrase or clause, or even conveys different meaning than the other. It is doubtful if the document pairs from Phase 1 are too noisy to be processed by the sentence pair classifier. An alternative way for sentence pair extraction is to further filter the document pairs and discard any pairs that do not look like parallel. It is hypothesized that the parallel relationship between two documents can be assimilated by the word alignment between them. The document pair filter produces the Viterbi alignment, with the associated probability, of each document pair based on IBM Model 1 (Brown et al., 1993). The word alignment model (i.e. the statistical dictionary used by IBM Model 1) is trained on the NIST SMT training dataset. The probability of the Viterbi alignment of a document pair is the sole basis on which we decide whether the pair is genuinely parallel. That is, an empirically determined threshold is used to distinguish parallel pairs from non-parallel ones. In our experiment, a very strict threshold is selected so as to boost up the precision at the expense of recall. There are a few important details that enable the document pair filter succeed in identifying parallel text: 1) Funct"
C10-1054,A00-1004,0,0.0204635,"has received much attention. Hansards, or parliamentary proceedings in more than one language, are obvious source of bilingual corpora, yet they are about a particular domain and therefore of limited use. Many researchers then explore the Web. Some approach attempts to locate bilingual text within a web page (Jiang et al., 2009); some others attempt to collect web pages in different languages and decide the parallel relationship between the web pages by means of structural cues, like existence of a common ancestor web page, similarity between URLs, and similarity between the HTML structures (Chen and Nie, 2000; Resnik 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel,"
C10-1054,J07-2003,0,0.333413,"e ranked by TF-IDF and the top-N words are selected. Each keyword is then translated into a few TL words by a statistically learned dictionary. In our experiments the dictionary is learned from NIST SMT training data. 2) Query of TF-IDF-ranked machine translated keywords (QTL-TFIDF). It is assumed that a machine translation (MT) system is better at handling lexical ambiguity than simple dictionary translation. Thus we propose to first translate the SL document into TL and extract the top-N TF-IDF-ranked words as query. In our experiments the MT system used is hierarchical phrase-based system (Chiang, 2007).2 3) Query of named entities (QNE). Another way to tackle the drawback of QSL-TFIDF is to focus on named entities (NEs) only, since NEs often provide strong clue for identifying correspondence between two languages. All NEs in a SL document are ranked by TF-IDF, and the top-N NEs are then translated (word by word) by dictionary. In our experiments we identify SL (Chinese) NEs 2 We also try online Google translation service, and the performance was roughly the same. 476 implicitly found by the word segmentation algorithm stated in Gao et al. (2003), and the dictionaries for translating NEs inc"
C10-1054,W04-3208,0,0.0253305,"e while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. Each"
C10-1054,P03-1035,0,0.0107406,"system used is hierarchical phrase-based system (Chiang, 2007).2 3) Query of named entities (QNE). Another way to tackle the drawback of QSL-TFIDF is to focus on named entities (NEs) only, since NEs often provide strong clue for identifying correspondence between two languages. All NEs in a SL document are ranked by TF-IDF, and the top-N NEs are then translated (word by word) by dictionary. In our experiments we identify SL (Chinese) NEs 2 We also try online Google translation service, and the performance was roughly the same. 476 implicitly found by the word segmentation algorithm stated in Gao et al. (2003), and the dictionaries for translating NEs include the same one used for QSL-TFIDF, and the LDC Chinese/English NE dictionary. For the NEs not covered by our dictionary, we use Google translation service as a back-up. A small-scale experiment is run to evaluate the merits of these queries. 300 Chinese news web pages in three different periods (each 100) are collected. For each Chinese text, each query (containing 10 keywords) is constructed and submitted to both Google and Yahoo Search, and top-40 returned English web pages for each search are kept. Note that the Chinese news articles are not"
C10-1054,P09-1098,1,0.83971,"y improves the performance of statistical machine translation. 1 Introduction Bilingual corpora are very valuable resources in NLP. They can be used in statistical machine translation (SMT), cross language information retrieval, and paraphrasing. Thus the acquisition of bilingual corpora has received much attention. Hansards, or parliamentary proceedings in more than one language, are obvious source of bilingual corpora, yet they are about a particular domain and therefore of limited use. Many researchers then explore the Web. Some approach attempts to locate bilingual text within a web page (Jiang et al., 2009); some others attempt to collect web pages in different languages and decide the parallel relationship between the web pages by means of structural cues, like existence of a common ancestor web page, similarity between URLs, and similarity between the HTML structures (Chen and Nie, 2000; Resnik 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cu"
C10-1054,N03-1017,0,0.00862123,"ore and better training data for SMT, we evaluate the parallel and comparable corpora with respect to improvement in Bleu score (Papineni et al., 2002). 5.1 Experiment Setup Our experiment starts with the 11,000 Chinese documents as described in Section 2. We use various combinations of queries in document pair retrieval (Section 3). Based on the candidate document pairs, we produce both comparable corpora and parallel corpora using sentence pair extraction (Section 4). The corpora are then given to our SMT systems as training data. The SMT systems are our implementations of phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). The two systems employ a 5-gram language model trained from the Xinhua section of the Gigaword corpus. There are many variations of the bilingual training dataset. The B1 section of the NIST SMT training set is selected as the baseline bilingual dataset; its size is of the same order of magnitude as most of the mined corpora so that the comparison is fair. Each of the mined bilingual corpora is compared to that baseline dataset, and we also evaluate the performance of the combination of each mined bilingual corpus with the baseline set. Phrase"
C10-1054,moore-2002-fast,0,0.0329541,"o work on data without common words. Therefore, all words on a comprehensive stopword list must be removed from a document pair before word alignment. 2) The alignment probability must be normalized with respect to sentence length, so that the threshold applies to all documents regardless of document length. Subjective evaluation on selected samples shows that most of the document pairs kept by the filter are genuinely parallel. Thus the document pairs can be broken down into sentence pairs simply by a sentence alignment method. For the sentence alignment, our experiments use the algorithm in Moore (2002). 5 Experiments It is a difficult task to evaluate the quality of automatically acquired bilingual corpora. As our ultimate purpose of mining bilingual corpora is to provide more and better training data for SMT, we evaluate the parallel and comparable corpora with respect to improvement in Bleu score (Papineni et al., 2002). 5.1 Experiment Setup Our experiment starts with the 11,000 Chinese documents as described in Section 2. We use various combinations of queries in document pair retrieval (Section 3). Based on the candidate document pairs, we produce both comparable corpora and parallel co"
C10-1054,J05-4003,0,0.366315,"r was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. Each TL document pairs up with"
C10-1054,P03-1021,0,0.0309187,".76(+1.68) 23.41(+1.75) 34.54(+1.69) 23.59(+2.41) B1+parallel(all query) 35.40(+2.32) 23.47(+1.81) 35.27(+2.42) 23.61(+2.43) Table 4: Evaluation of translation quality improvement by mined corpora. The figures inside brackets refer to the improvement over baseline. The bold figures indicate the highest Bleu score in each column for comparable corpora and parallel corpora, respectively. Bilingual Training Corpus The SMT systems learn translation knowledge (phrase table and rule table) in standard way. The parameters in the underlying log-linear model are trained by Minimum Error Rate Training (Och, 2003) on the development set of NIST 2003 test set. The quality of translation output is evaluated by case-insensitive BLEU4 on NIST 2005 and NIST 2008 test sets4. 5.2 Experimental result Table 3 lists the size of various mined parallel and comparable corpora against the baseline B1 bilingual dataset. It is obvious that for a specific type of query in document pair retrieval, the parallel corpus is significantly smaller than the corresponding comparable corpus. The apparent explanation is that a lot of document pairs are discarded due to the document Queries SP #SP #SL #TL extraction words words Ba"
C10-1054,P02-1040,0,0.0847863,"tive evaluation on selected samples shows that most of the document pairs kept by the filter are genuinely parallel. Thus the document pairs can be broken down into sentence pairs simply by a sentence alignment method. For the sentence alignment, our experiments use the algorithm in Moore (2002). 5 Experiments It is a difficult task to evaluate the quality of automatically acquired bilingual corpora. As our ultimate purpose of mining bilingual corpora is to provide more and better training data for SMT, we evaluate the parallel and comparable corpora with respect to improvement in Bleu score (Papineni et al., 2002). 5.1 Experiment Setup Our experiment starts with the 11,000 Chinese documents as described in Section 2. We use various combinations of queries in document pair retrieval (Section 3). Based on the candidate document pairs, we produce both comparable corpora and parallel corpora using sentence pair extraction (Section 4). The corpora are then given to our SMT systems as training data. The SMT systems are our implementations of phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). The two systems employ a 5-gram language model trained from the Xinhua section of"
C10-1054,J03-3002,0,0.280125,"Missing"
C10-1054,P06-1062,1,0.890945,"about a particular domain and therefore of limited use. Many researchers then explore the Web. Some approach attempts to locate bilingual text within a web page (Jiang et al., 2009); some others attempt to collect web pages in different languages and decide the parallel relationship between the web pages by means of structural cues, like existence of a common ancestor web page, similarity between URLs, and similarity between the HTML structures (Chen and Nie, 2000; Resnik 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Pha"
C10-1054,P03-1010,0,0.239581,"k 1 This work has been done while the first author was visiting Microsoft Research Asia. and Smith, 2003; Yang and Li, 2003; Shi et al., 2006). The corpora thus obtained are generally of high quality and wide variety in domain, but the amount is still limited, as web pages that exhibit those structural cues are not abundant. Some other effort is to mine bilingual corpora by textual means only. That is, two pieces of text are decided to be parallel merely from the linguistic perspective, without considering any hint from HTML markup or website structure. These approaches (Zhao and Vogel, 2002; Utiyama and Isahara 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009) share roughly the same framework: Phase 1: Document Pair Retrieval 1) documents in some target language (TL) are stored in some database; 2) each document in some source language (SL) is represented by some TL keywords; 3) the TL keywords in (2) are used to assign some TL documents to a particular SL document, using some information retrieval (IR) technique. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use"
C10-1054,E03-1050,0,0.0481981,"Missing"
C10-1075,P05-1033,0,0.132848,"llel corpora available to the constrained track of NIST 2008 Chinese-English MT evaluation task were used for translation model training, which consist of around 5.1M bilingual sentence pairs. GIZA++ was used for word alignment in both directions, which was further refined with the intersec-diaggrow heuristics. We used a 5-gram language model which was trained from the Xinhua portion of English Gigaword corpus version 3.0 from LDC and the English part of parallel corpora. 4.2 Machine Translation System We used an in-house implementation of the hierarchical phrase-based decoder as described in Chiang (2005). In addtion to the standard features used in Chiang (2005), we also used a lexicon feature indicating how many word paris in the translation found in a conventional Chinese-English lexicon. Phrasal rules were extracted from all the parallel data, but hierarchical rules were only extracted from the FBIS part of the parallel data which contains around 128,000 sentence pairs. For all the development data, feature weights of the decoder were tuned using the MERT algorithm (Och, 2003). 4.3 Results of Development Data Pre-construction In the following we first present some overall results using the"
C10-1075,C04-1059,0,0.0924552,"olingual and bilingual) to the existing training corpora has been shown to be very effective in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes"
C10-1075,W07-0717,0,0.0553139,"n shown to be very effective in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes these two kinds of methods complementary to each other — it is possi"
C10-1075,N04-1035,0,0.0759513,"Missing"
C10-1075,N03-1017,0,0.00907053,"Missing"
C10-1075,2007.mtsummit-tutorials.1,0,0.030423,"Missing"
C10-1075,D07-1036,0,0.303405,"Missing"
C10-1075,D09-1074,0,0.0531116,"ctive in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes these two kinds of methods complementary to each other — it is possible to make further impro"
C10-1075,P02-1038,0,0.0329115,"l methods that use a fixed model parameter setting across different data sets. 1 e∗ = argmax Pr(e|f ) e and the posterior probability distribution Pr(e|f ) is directly approximated by a log-linear formulation: Pr(e|f ) = pλ (e|f ) P exp( M m=1 λm hm (e, f )) =P PM 0 e0 exp( m=1 λm hm (e , f )) (1) in which hm ’s are feature functions and λ = (λ1 , . . . , λM ) are model parameters (feature weights). For a successful practical log-linear SMT model, it is usually a combined result of the several efforts: • Construction of well-motivated SMT models Introduction In recent years, log-linear model (Och and Ney, 2002) has been a mainstream method to formulate statistical models for machine translation. Using this formulation, various kinds of relevant properties and data statistics used in the translation process, either on the monolingual-side or on the bilingual-side, are encoded and used as realvalued feature functions, thus it provides an effective mathematical framework to accommodate a large variety of SMT formalisms with different computational linguistic motivations. ∗ This work was done while the author was visiting Microsoft Research Asia. • Accurate estimation of feature functions • Appropriate"
C10-1075,P03-1021,0,0.403672,"such λ∗ that is optimal for multiple data sets at the same time. Table 1 shows some empirical evidences when two data sets are mutually used as development and test data. In this setting, we used a hierarchical phrase based decoder and 2 years’ evaluation data of NIST Chineseto-English machine translation task (for the year 2008 only the newswire subset was used because we want to limit both data sets within the same domain to show that data mismatch also exists even if there is no domain difference), and report results using BLEU scores. Model parameters were tuned using the MERT algorithm (Och, 2003) optimized for BLEU metric. Dev data MT05 MT08-nw MT05 0.402 0.372 MT08-nw 0.306 0.343 Table 1: Translation performance of cross development/test on two NIST evaluation data sets. In our work, we present a solution to this problem by using test data dependent model parameters for test data translation. As discussed above, since model parameters are solely determined by development data D, selection of log-linear model parameters is basically equivalent to selecting a set of development data D. However, automatic development data selection in current SMT research remains a relatively open issue"
C10-1075,P07-1004,0,0.0590762,"e task. To our knowledge, there is no dedicated discussion on principled methods to perform development data selection in previous research. In L¨u et al. (2007), log-linear model parameters can also be adjusted at decoding time. But in their approach, the adjustment was based on heuristic rules and re-weighted training data distribution. In addition, compared with training data selection, the computational cost of development data selection is much smaller. From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al., 2007). But our methods do not rely on surface similarities between training and training/development sentences, and development/test sentences are not used to re-train SMT sub-models. 6 Conclusions and Future Work In this paper, we addressed the data mismatch issue between training and decoding time of loglinear SMT models, and presented principled methods for dynamically inferring test data dependent model parameters with development set selection. We describe two algorithms for this task, development set pre-construction and dynamic construction, and evaluated our method on the NIST data sets for"
C10-1075,P06-1077,0,\N,Missing
C10-1075,2005.eamt-1.19,0,\N,Missing
C10-1079,W05-0622,0,0.0163437,"n the next section, we review related work. In Section 3 we detail key components of our approach. In Section 4, we setup experiments and evaluate the effectiveness of our method. Finally, Section 5 concludes and presents the future work. 2 Related Work Our related work falls into two categories: SRL on news and domain adaption. As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single"
C10-1079,W05-0625,0,0.0276125,"ganized as follows: In the next section, we review related work. In Section 3 we detail key components of our approach. In Section 4, we setup experiments and evaluate the effectiveness of our method. Finally, Section 5 concludes and presents the future work. 2 Related Work Our related work falls into two categories: SRL on news and domain adaption. As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsi"
C10-1079,P09-5003,0,0.0223964,"Missing"
C10-1079,W08-2101,0,0.0134703,"ova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo, 2006; Merlo and Musillo, 2008), or by using Markov Logic Networks (MLN, Richardson and Domingos, 2005) as the learning framework (Riedel and Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009). All the above approaches focus on sentences from news articles or other formal documents, and depend on human annotated corpus for training. To our knowledge, little study has been carried out on SRL for news tweets. As for domain adaption, some researchers regarded the out-of-GRPDLQ GDWD DV SULRU NQRZOHGJH´DQGestimated the model parameters by maximizing the posterior under this prior distribution, and successfully applied their app"
C10-1079,N09-1018,0,0.0696354,"their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo, 2006; Merlo and Musillo, 2008), or by using Markov Logic Networks (MLN, Richardson and Domingos, 2005) as the learning framework (Riedel and Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009). All the above approaches focus on sentences from news articles or other formal documents, and depend on human annotated corpus for training. To our knowledge, little study has been carried out on SRL for news tweets. As for domain adaption, some researchers regarded the out-of-GRPDLQ GDWD DV SULRU NQRZOHGJH´DQGestimated the model parameters by maximizing the posterior under this prior distribution, and successfully applied their approach to language modeling (Bacchiani and Roark, 2003) and parsing (Roark and Bacchiani, 2003). Daumé III and Marcu (2006) presented a QRYHO IUDPHZRUN E"
C10-1079,N06-2026,0,0.0220547,"anok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo, 2006; Merlo and Musillo, 2008), or by using Markov Logic Networks (MLN, Richardson and Domingos, 2005) as the learning framework (Riedel and Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009). All the above approaches focus on sentences from news articles or other formal documents, and depend on human annotated corpus for training. To our knowledge, little study has been carried out on SRL for news tweets. As for domain adaption, some researchers regarded the out-of-GRPDLQ GDWD DV SULRU NQRZOHGJH´DQGestimated the model parameters by maximizing the posterior under this prior distribution, and succ"
C10-1079,J08-2005,0,0.0221116,"eview related work. In Section 3 we detail key components of our approach. In Section 4, we setup experiments and evaluate the effectiveness of our method. Finally, Section 5 concludes and presents the future work. 2 Related Work Our related work falls into two categories: SRL on news and domain adaption. As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo"
C10-1079,W08-2125,0,0.0121536,"beled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo, 2006; Merlo and Musillo, 2008), or by using Markov Logic Networks (MLN, Richardson and Domingos, 2005) as the learning framework (Riedel and Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009). All the above approaches focus on sentences from news articles or other formal documents, and depend on human annotated corpus for training. To our knowledge, little study has been carried out on SRL for news tweets. As for domain adaption, some researchers regarded the out-of-GRPDLQ GDWD DV SULRU NQRZOHGJH´DQGestimated the model parameters by maximizing the posterior under this prior distribution, and successfully applied their approach to language modeling (Bacchiani and Roark, 2003) and parsing (Roark and Bacchiani, 2003). Daumé III and Marcu (2006) pre"
C10-1079,N03-1027,0,0.0162277,"005) as the learning framework (Riedel and Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009). All the above approaches focus on sentences from news articles or other formal documents, and depend on human annotated corpus for training. To our knowledge, little study has been carried out on SRL for news tweets. As for domain adaption, some researchers regarded the out-of-GRPDLQ GDWD DV SULRU NQRZOHGJH´DQGestimated the model parameters by maximizing the posterior under this prior distribution, and successfully applied their approach to language modeling (Bacchiani and Roark, 2003) and parsing (Roark and Bacchiani, 2003). Daumé III and Marcu (2006) presented a QRYHO IUDPHZRUN E GHILQLQJ D JHQHUDO GoPDLQ´EHWZHHQWKHWUXOLQ-GRPDLQ´DQGWUXO out-of-GRPDLQ´ Unlike existing domain adaption approaches, our method is about adapting SRL system on news domain to the news tweets domain, two domains that differ in writing style but are linked through content similarity. 700 3 Our Method Our method of SRL for news tweets is to train a domain specific SRL on automatically annotated training data as briefed in Section 1. In this section we present details of the five crucial components of our method, i.e.,"
C10-1079,P03-1002,0,0.0814673,"Missing"
C10-1079,W08-2121,0,0.0354669,"Missing"
C10-1079,P05-1073,0,0.0154372,"ction 3 we detail key components of our approach. In Section 4, we setup experiments and evaluate the effectiveness of our method. Finally, Section 5 concludes and presents the future work. 2 Related Work Our related work falls into two categories: SRL on news and domain adaption. As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo, 2006; Merlo and Musill"
C10-1079,J08-2002,0,0.0173197,"mponents of our approach. In Section 4, we setup experiments and evaluate the effectiveness of our method. Finally, Section 5 concludes and presents the future work. 2 Related Work Our related work falls into two categories: SRL on news and domain adaption. As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). Exceptions to the pipelined approach exist. Màrquez et al. (2005) sequentially labeled the words according to their positions relative to an argument (i.e., inside, outside or at the beginning of it). Carreras et al. (2004) and Surdeanu et al. (2007) jointly labeled all the predicates. Vickrey and Koller(2008) simplified the input sentence by hand-written and machine learnt rules before conducting SRL. Some other approaches simultaneously resolved all the sub-tasks by integrating syntactic parsing and SRL into a single model (Musillo and Merlo, 2006; Merlo and Musillo, 2008), or by using Mar"
C10-1079,W08-2140,0,0.0220689,"Missing"
C10-1079,W04-2415,0,\N,Missing
C10-1079,W05-0628,0,\N,Missing
C10-1079,W04-3212,0,\N,Missing
C10-1079,P00-1056,0,\N,Missing
C10-1079,W06-2303,0,\N,Missing
C10-2025,C08-1005,0,0.0879333,"rne, 2004) decoding over n-best list finds a translation that has lowest expected loss with all the other hypotheses, and it shows that improvement over the Maximum a Posteriori (MAP) decoding. Several word-based methods (Rosti et al., 2007a; Sim et al., 2007) have also been proposed. Usually, these methods take n-best list from different SMT systems as inputs, and construct a confusion network for second-pass decoding. There are also a lot of research work to advance the confusion network construction by finding better alignment between the skeleton and the other hypotheses (He et al., 2008; Ayan et al., 2008). Typically, all the approaches above only use full hypotheses but have no access to the PHS information. In this paper, we present hybrid decoding — a novel statistical machine translation (SMT) decoding paradigm using multiple SMT systems. In our work, in addition to component SMT systems, system combination method is also employed in generating partial translation hypotheses throughout the decoding process, in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses. Experimental results on"
C10-2025,P06-1121,0,0.0535726,"es word alignment information. However, in hybrid decoding, it is quite time-consuming and impractical to conduct word alignment like GIZA++ for each span. Fortunately, unit hypotheses word alignment can be obtained from the model training process, which is shown in Figure 2. We devise a heuristic approach for PHS alignment that leverages the translation derivations from the sub-phrases. The derivation information ultimately comes from the phrase table in phrase-based systems (Koehn et al., 2003; Xiong et al., 2006) or the rule table in syntactic-based systems (Chiang, 2007; Liu et al., 2007; Galley et al., 2006). The derivation is built in a phrase-based system as follows. For example, we have two phrase where string “m-n”means the mth word in the source phrase is aligned to the nth word in the target phrase. When combining the two phrases for generating “我们 的 经济 利益”, we obtain the translation hypothesis as “our economic interests”and also integrate the alignment fragment to get “0-0 1-0 2-1 3-2”. The case is similar in syntactic-based system for non-terminal substitution, which we will not discuss further here. Next, we introduce the skeleton-to-hypothesis word alignment algorithm in detail. With th"
C10-2025,N03-1017,0,0.0544342,"Missing"
C10-2025,P02-1040,0,0.0893694,"Suppose we have n individual decoders. The ranking function Fn of the nth decoder can be written as: Fn (e) = m X λn,i hn,i (f, e) (2) i=1 Figure 3: Algorithm for skeleton-to-hypothesis alignment Subroutines UNION(A,B) GETALIGN(S,align) tial in confusion network construction. The simplest way is to use the top-1 PHS from any individual decoder with the best performance under some criteria. However, this cannot always lead to better performance on some evaluation metrics (Rosti et al., 2007a). An alternative would be MBR method with some loss function such as TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). We show the experimental results of two skeleton selection methods for PHS combination in Section 3. Description the union of set A and set B get the words aligned to S based on align similarity between w1 and w2 , we use edit distance here align w1 with w2 Table 1: Description for subroutines Due to the variety of the word order in nbest outputs, skeleton selection becomes essenwhere each hn,i (f, e) is a feature function of the nth decoder, and λn,i is the corresponding feature weight. m is the number of features in each decoder. The final result of hybrid decoder is the top1 translation f"
C10-2025,P00-1056,0,\N,Missing
C10-2025,E06-1005,0,\N,Missing
C10-2025,P07-1089,0,\N,Missing
C10-2025,P09-1066,1,\N,Missing
C10-2025,P06-1066,0,\N,Missing
C10-2025,P07-1040,0,\N,Missing
C10-2025,N04-1022,0,\N,Missing
C10-2025,D07-1056,1,\N,Missing
C10-2025,P09-1065,0,\N,Missing
C10-2025,P03-1021,0,\N,Missing
C10-2025,P05-1033,0,\N,Missing
C10-2025,N07-1029,0,\N,Missing
C10-2025,J97-3002,0,\N,Missing
C10-2025,D08-1011,0,\N,Missing
C10-2025,W04-3250,0,\N,Missing
C10-2025,J07-2003,0,\N,Missing
C10-2083,W05-0622,0,0.0215668,"Missing"
C10-2083,W08-2121,0,0.0319283,"Missing"
C10-2083,P05-1073,0,0.0647414,"Missing"
C10-2083,J02-3001,0,0.0430742,"Missing"
C10-2083,J08-2002,0,0.0471153,"Missing"
C10-2083,W05-0625,0,0.0458444,"Missing"
C10-2083,W04-3212,0,0.0223085,"Missing"
C10-2083,P09-5003,0,0.0227933,"Missing"
C10-2083,W08-2101,0,0.029267,"Missing"
C10-2083,N09-1018,0,0.0166878,"ancy to SRL, we use no parsers in our approach. We built a baseline SRL, which depends on no parsers, and use the MLN framework to exploit redundancy. Our intuition is that SRL on one sentence can help that on other differently phrased sentences with similar meaning. For example, consider the following sentence from a news article: A suicide bomber blew himself up Sunday in market in Pakistan&apos;s northwest crowded with shoppers ahead of a Muslim holiday, killing 12 people, including a mayor who once supported but had turned against the Taliban, officials said. The state-of-art MLN-based system (Meza-Ruiz and Riedel, 2009), hereafter referred to as MLNBS for brevity, incorrectly labels northwest instead of bomber as A0 of killing. Now consider another sentence from another news article: Police in northwestern Pakistan say that a suicide bomber has killed at least 13 people and wounded dozens of others. Here MLNBS correctly identify bomber as A0 of killing. When more sentences are observed where bomber as A0 of killing is correctly identified, we will be more confident that bomber should be labeled as A0 of killing, and that northwest should not be the A0 of killing according to the constraint that one predicate"
C10-2083,N06-2026,0,0.0537119,"Missing"
C10-2083,J08-2005,0,0.0520469,"Missing"
C10-2083,W08-2125,0,0.0620331,"=&gt;final_role (s1,p1,a1,+r) (2) Authorities said Ida could make landfall as early as Tuesday morning, although it was forecast to weaken by then. … Collective Inference Based on MLN Our method includes two core components: a baseline system that conducts SRL on every sentence; and a collective inference system that accepts as input a group of sentences with preliminary SRL information provided by the baseline. We build the baseline by removing formulas involving syntactic parsing information from MLNBS (while keeping other rules) and retraining the system using the tool and scripts provided by Riedel and Meza-Ruiz (2008) on the manually annotated news corpus described in Section 4. A collective inference system is constructed to leverage redundancy in the SRL information from the baseline. We first redefine the predicate role and treat it as observed: predicate role: Int x Int x Int x Role; role has four parameters: the first one stands for the number of sentence in the input, which is necessary to distinguish the sentences in a group; the other three are taken from the arguments of the role predicate defined by Riedel and MezaRuiz (2008), which denote the positions of the predicate and the argument in the se"
C10-2083,W06-2303,0,\N,Missing
C10-2084,W06-3123,0,0.0189229,"n phrase. As an example, if there is a simple phrase pair &lt;white house, 白 宫&gt;, then it is transformed into the ITG rule  white house 白宫 . An important question is how these phrase pairs can be formulated. Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. This method suffers from computational complexity because it considers all possible phrases and all their possible alignments. Birch et al. (2006) propose a better and more efficient method of constraining the search space which does not contradict a given high confidence word alignment for each sentence. Our PITG collects all phrase pairs which are consistent with a word alignment matrix produced by a simpler word alignment model. 2.3 HP-ITG : P-ITG with H-Phrase pairs P-ITG is the first enhancement of ITG to capture the linguistic phenomenon that more than one word of a language may function as a single unit, so that these words should be aligned to a single unit of another language. But P-ITG can only treat contiguous words as a sing"
C10-2084,P06-2014,0,0.0363234,"Missing"
C10-2084,W07-0403,0,0.0690132,"ng compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like ""although"", ""because"") but in Chinese we need two connectives (e.g. There is a sentence pattern ""虽然 但是  although "", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-ba"
C10-2084,J07-2003,0,0.819709,"i et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like ""although"", ""because"") but in Chinese we need two connectives (e.g. There is a sentence pattern ""虽然 但是  although "", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-based SMT (Chiang, 2007) is proved to be superior to simple phrase-based SMT, it is natural to ask, why don‟t we further incorporate hierarchical phrase pairs (henceforth h-phrase pairs) into ITG? In this paper we propose a ITG formalism and parsing algorithm using h-phrase pairs. The ITG model involves much more parameters. On the one hand, each phrase/h-phrase pair has its own probability or score. It is not feasible to learn these parameters through discriminative/supervised learning since the repertoire of phrase pairs is much larger than the size of human-annotated alignment set. On the other hand, there are als"
C10-2084,P08-1010,0,0.0439096,"Missing"
C10-2084,P05-1057,0,0.196987,"Missing"
C10-2084,W02-1018,0,0.0261841,"irs of phrases (or blocks). That is, a sequence of source language word can be aligned, as a whole, to one (or a sequence of more than one) target language word. These methods can be subsumed under the term phrase-based ITG (P-ITG), which enhances W-ITG by altering the definition of a terminal production to include phrases:  (c.f. Figure 1(c)). stands for English phrase and stands for foreign phrase. As an example, if there is a simple phrase pair &lt;white house, 白 宫&gt;, then it is transformed into the ITG rule  white house 白宫 . An important question is how these phrase pairs can be formulated. Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. This method suffers from computational complexity because it considers all possible phrases and all their possible alignments. Birch et al. (2006) propose a better and more efficient method of constraining the search space which does not contradict a given high confidence word alignment for each sentence. Our PITG collects all phrase pairs which are c"
C10-2084,P06-1065,0,0.0766653,"n initial alignment matrix by some simpler word alignment model) and an initial estimation of , the discriminative training process and the approx735 imate EM learning process are alternatively iterated until there is no more improvement. The sketch of the semi-supervised training is shown in Figure 5. 4.5 Features for word pairs The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel et al., 1996) and IBM model 4 (Brown et al., 1993). 2) Conditional link probability (Moore, 2006). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 4.6 Features for phrase/h-phrase pairs For our HP-DITG model, the rule probabilities in both English-to-foreign and foreign-toEnglish directions are estimated and taken as features, in addition to those features in WDITG, in the discriminative model of alignment hypothesis selection: 1) : The conditional probability of English phrase/h-phrase given foreign phrase/h-phrase. 2) : The conditional probability of foreign phrase/h-phrase given English phrase/h-phrase. The features are calculated as described in section 4.3. 5 Evaluatio"
C10-2084,P03-1021,0,0.00775187,"rs in (1) to be learned. The first is the values of the features Ψ. Most features are indeed about the probabilities of the phrase/h-phrase pairs and there are too many of them to be trained from a labeled data set of limited size. Thus the feature values are trained by approximate EM. The other kind of parameters is feature weights λ, which are 734 trained by an error minimization method. The discriminative training of λ and the approximate EM training of Ψ are integrated into a semisupervised training framework similar to EMD3 (Fraser and Marcu, 2006). 4.2 Discriminative Training of λ MERT (Och, 2003) is used to train feature weights λ. MERT estimates model parameters with the objective of minimizing certain measure of translation errors (or maximizing certain performance measure of translation quality) for a development corpus. Given an SMT system which produces, with model parameters , the K-best candidate translations for a source sentence , and an error measure of a particular candidate with respect to the reference translation , the optimal parameter values will be: MERT for DITG applies the same equation for parameter tuning, with different interpretation of the components in the equ"
C10-2084,J04-4002,0,0.405349,"Missing"
C10-2084,C96-2141,0,0.40965,"roximate EM learning of feature values are integrated in a single semi-supervised framework. Given an initial estimation of (estimated from an initial alignment matrix by some simpler word alignment model) and an initial estimation of , the discriminative training process and the approx735 imate EM learning process are alternatively iterated until there is no more improvement. The sketch of the semi-supervised training is shown in Figure 5. 4.5 Features for word pairs The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel et al., 1996) and IBM model 4 (Brown et al., 1993). 2) Conditional link probability (Moore, 2006). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 4.6 Features for phrase/h-phrase pairs For our HP-DITG model, the rule probabilities in both English-to-foreign and foreign-toEnglish directions are estimated and taken as features, in addition to those features in WDITG, in the discriminative model of alignment hypothesis selection: 1) : The conditional probability of English phrase/h-phrase given foreign phrase/h-phrase. 2) : The condition"
C10-2084,J97-3002,0,0.87575,"properties for word alignment, it still suffers from the limitation of one-to-one matching. While existing approaches relax this limitation using phrase pairs, we propose a ITG formalism, which even handles units of non-contiguous words, using both simple and hierarchical phrase pairs. We also propose a parameter estimation method, which combines the merits of both supervised and unsupervised learning, for the ITG formalism. The ITG alignment system achieves significant improvement in both word alignment quality and translation performance. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of CFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. One of the merits of ITG is that it is less biased towards short-distance reordering compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns"
C10-2084,P06-1066,0,0.0698234,"ese word segmentation standard. 250 sentence pairs are used as training data and the other 241 are test data. The large, un-annotated bilingual corpus for approximate EM learning of feature values is FBIS, which is also the training set for our SMT systems. In SMT experiments, our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. The NIST‟03 test set is used as our development corpus and the NIST‟05 and NIST‟08 test sets are our test sets. We use two kinds of state-of-the-art SMT systems. One is a phrase-based decoder (PBSMT) with a MaxEntbased distortion model (Xiong, et al., 2006), and the other is an implementation of hierarchical phrase-based model (HPBSMT) (Chiang, 2007). The phrase/rule table for these two systems is not generated from the terminal node of HPDITG tree directly, but extracted from word alignment matrix (HP-DITG generated) using the same criterion as most phrase-based systems (Chiang, 2007). 5.2 roughly the same in all cases, while W-ITG has the lowest recall, due to the limitation of one-toone matching. The improvement by (simple) phrase pairs is roughly the same as that by hphrase pairs. And it is not surprising that the combination of both kinds o"
C10-2084,J07-3002,0,0.0771218,"0 24.90 34.77 25.25 25.43 35.04 34.82 25.56 Appendix A. The Normal Form Grammar Table 4 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null. 1 2 3 Table 3. Semi-supervised Training Task on BLEU. It can be observed that EMD improves SMT performance in most iterations in most cases. EMD does not always improve BLEU score because the objective function of the discriminative training in EMD is about alignment FMeasure rather than BLEU. And it is well known that the correlation between F-Measure and BLEU (Fraser and Marcu, 2007) is itself an intriguing problem. The best HP-DITG leads to more than 1 BLEU point gain compared with GIZA++ on all datasets/MT models. Compared with BERK, EMD3 improves SMT performance significantly on NIST05 and slightly on NIST08. 6 ity but also machine translation performance significantly. Combining the formalism and semi-supervised training, we obtain better alignment and translation than the baselines of GIZA++ and BERK. A fundamental problem of our current framework is that we fail to obtain monotonic increment of BLEU score during the course of semisupervised training. In the future,"
C10-2084,P05-1059,0,0.113012,"ption. Figure 4(c) shows how the span pair [e1,e3]/[f1,f3] can be generated in two ways: one is combining a phrase pair and a word pair directly, and the other way is replacing the X in the h-phrase pair with a word pair. Here we only show how hphrase pairs with one variable be used during the parsing, and h-phrase pairs with more than one variable can be used in a similar way. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. Zhang and Gildea (2005) show that Model 1 (Brown et al., 1993) probabilities of the word pairs inside and outside a span pair are useful. Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O(n4). Tic-tac-toe pruning method is adopted in this paper. 4 Semi-supervised Training The original formulation of ITG (W-ITG) is a generative model in which the ITG tree of a sentence pair is produced by a set of rules. The parameters of these rules are trained by EM. Certainly it is difficult to add more non-independent features in such a genera"
C10-2084,P09-1104,0,0.47586,"ITG alignment system achieves significant improvement in both word alignment quality and translation performance. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of CFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. One of the merits of ITG is that it is less biased towards short-distance reordering compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clau"
C10-2084,P08-1012,0,0.0176898,"such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009)1. The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like ""although"", ""because"") but in Chinese we need two connectives (e.g. There is a sentence pattern ""虽然 但是  although "", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-based SMT (Chiang, 2007) is proved to be super"
C10-2084,W01-1812,0,0.084175,"Missing"
C10-2084,J93-2003,0,\N,Missing
C10-2084,2006.amta-papers.2,0,\N,Missing
C10-2084,P06-1097,0,\N,Missing
C12-1047,P11-5003,0,0.0437679,"Missing"
C12-1047,P07-2015,0,0.024705,"tion 4 shows the experiments and results, and we conclude this work with directions for future study in section 5. 2 Related work 2.1 Extractive document summarization A substantial amount of work has been done on extractive text summarization (Lloret and Palomar, 2012). Many text features, such as term frequency, sentence position, query relevance and sentence dependency structure, have been investigated for sentence salience estimation. They are usually weighted automatically by applying certain learning-based mechanisms or tuned experimentally to build a feature-based summarization system (Fuentes et al., 2007) (Wong et al., 2008). Previous research shows that a combination of sentence position, fixedphrase and sentence length give the best results in learning-based sentence selection (Ani Nenkova, 2011). Meanwhile, feature-based approaches have been widely used in the top five participating systems in DUC3 2005-2007. In addition, different types of links among sentences and documents are employed by graphbased approaches to measure sentence salience, such as LexRank (Erkan and Radev, 2004), TextRank (Mihalcea, 2004), and Mutual Reinforcement Chain(MRC) (Wei et al., 2008). LexRank and TextRank make"
C12-1047,N03-1020,0,0.0933039,"del, the contribution of social influence and content quality to the model, the correctness of sub-topic segmentation, and the accuracy of the content quality estimation model. The performance of the social influence of users has not been evaluated due to the difficulty of data acquisition. It is hard to label the influence of users, or to collect such information automatically. Therefore, in this paper, we evaluate the end-to-end contribution of social influence under the whole summarization framework. 4.2 Evaluation metric We evaluate the performance of our summarization system using ROUGE (Lin and Hovy, 2003), which is widely-used in summarization evaluation. It measures the overlap of N-grams between the predicted summary and the reference, which is defined as, ROU GE = Σ t∈Sr e f Σ g r amn ∈t C ount mat ch (g r amn ) Σ t∈Sr e f Σ g r amn ∈t C ount(g r amn ) where n is the word length of n-gram, S r e f denotes the reference summary, C ount(g r amn ) is the number of n-grams comprising sentences in the reference summary and C ount mat ch (g r amn ) computes the maximum number of n-grams appearing both in the summary generated by our system and the reference summary. 772 4.3 Evaluation of the rein"
C12-1047,W11-0709,0,0.206308,"mary. Both feature-based and graph-based approaches are exploited to measure the salience of posts under an extractive summarization framework. Taking into consideration 3 http://www-nlpir.nist.gov/projects/duc/data.html 765 the evolutionary characteristic of topics along time line, researchers have also started to explore the evolutionary summarization of events in micro-blog. In feature-based approaches, a variety of statistical and linguistic features have been extensively investigated, such as, language model (O’Connor et al., 2010), tweet frequency (Shiells et al., 2010), term frequency (Liu et al., 2011) (Takamura et al., 2011) (Parthasarathy, 2012), TF-IDF (Frederking, 2011) (Chakrabarti and Punera, 2011), hybrid TF-IDF (Sharifi et al., 2010b), KLdivergence (Zubiaga et al., 2012), time delay (Takamura et al., 2011), and topic relevance (Long et al., 2011). Among them, simple term frequency has proven to be extremely extraordinary for topic-sensitive micro-blog summarization because of the unstructured and short characteristics of micro-blog posts according to Inouye and Kalita (2011). As for micro-blog summarization, some micro-blog specific features such as text normalization, the content o"
C12-1047,P04-3020,0,0.0390607,"anisms or tuned experimentally to build a feature-based summarization system (Fuentes et al., 2007) (Wong et al., 2008). Previous research shows that a combination of sentence position, fixedphrase and sentence length give the best results in learning-based sentence selection (Ani Nenkova, 2011). Meanwhile, feature-based approaches have been widely used in the top five participating systems in DUC3 2005-2007. In addition, different types of links among sentences and documents are employed by graphbased approaches to measure sentence salience, such as LexRank (Erkan and Radev, 2004), TextRank (Mihalcea, 2004), and Mutual Reinforcement Chain(MRC) (Wei et al., 2008). LexRank and TextRank make use of pairwise similarity between sentences, hypothesizing that the sentences similar to most of the other sentences in a cluster are more salient. In contrast to the single level PageRank in LexRank and TextRank, MRC considers both internal and external constraints on three different levels, document, sentence, and term and achieves promising improvement. 2.2 Micro-blog summarization Recently, researchers have conducted a number of investigations on micro-blog(e.g. Twitter) summarization. Instead of ranking s"
C12-1047,N10-1100,0,0.569622,"richness, as a measure of the regularity of written language and the pointless degree of the content. The above information is jointly employed in a graph-based ranking algorithm. In order to avoid redundancy in the result, the final summary is generated by selecting tweets from the previous ranking results with the traditional Maximal Marginal Relevance(MMR) algorithm (Carbonell and Goldstein, 1998). We conduct experiments on a real data set containing 3.9 million tweets. Compared with two popular graph-based summarization approaches, namely Lexrank (Erkan and Radev, 2004) and phrase graph (Sharifi et al., 2010a), the experimental results show that: • The reinforcement summarization model integrating social influence and content quality achieves a considerable performance and outperforms the standard LexRank and the phrase graph summarization approaches. • The social influence of users and the content quality of tweets help to more effectively measure the salience of tweets. The rest of this paper is organized as follows. Related work is introduced in Section 2. Next, we present a detailed introduction of our approach in Section 3. Section 4 shows the experiments and results, and we conclude this wo"
C12-1047,P07-1070,0,0.0122818,"y to dominate the topic. The last is set against the informal writing style of Twitter. Inspired by Wei et al. (2008), we propose a unified mutual reinforcement summarization model taking advantage of relations among tweets, words, and users for tweet salience measurement. Figure 1 shows the overview of the proposed model. The similarity of tweets to the sub-topic benefits from both the content similarity among tweets and the word coverage in the sub-topic cluster. In document summarization, the contribution of relationships among sentences to the performance improvements has been recognized (Wan et al., 2007). Sharifi et al. (2010a) found that sequences of words that encompassed the topic phrase highly overlapped when considering a large number of tweets for a single topic. The social influence of users contributes to salience measurement via author relation. And the content quality of tweets is incorporated at the tweet level. Figure 1: The Unified Mutual Reinforcement Graph Model The mutual reinforcement model is formed with three PageRank-like models for word, tweet, and user respectively, but in a unified and interrelated way. The ranking of one of them is derived not only from the relationshi"
C12-1047,C08-1124,0,0.0269199,"ments and results, and we conclude this work with directions for future study in section 5. 2 Related work 2.1 Extractive document summarization A substantial amount of work has been done on extractive text summarization (Lloret and Palomar, 2012). Many text features, such as term frequency, sentence position, query relevance and sentence dependency structure, have been investigated for sentence salience estimation. They are usually weighted automatically by applying certain learning-based mechanisms or tuned experimentally to build a feature-based summarization system (Fuentes et al., 2007) (Wong et al., 2008). Previous research shows that a combination of sentence position, fixedphrase and sentence length give the best results in learning-based sentence selection (Ani Nenkova, 2011). Meanwhile, feature-based approaches have been widely used in the top five participating systems in DUC3 2005-2007. In addition, different types of links among sentences and documents are employed by graphbased approaches to measure sentence salience, such as LexRank (Erkan and Radev, 2004), TextRank (Mihalcea, 2004), and Mutual Reinforcement Chain(MRC) (Wei et al., 2008). LexRank and TextRank make use of pairwise simi"
C12-1104,H01-1065,0,0.0106409,"ls our method and Section 6 evaluates our method. Section 7 demonstrates the application of the proposed method to the Twitter search. Finally, Section 8 concludes with a discussion of future work. 2 Related Work Two categories of research are highly related to our work: multi-document summarization and recent studies of tweets. 2.1 Multi-document Summarization Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008), sentence compression (Knight and Marcu, 2002), and reformulation (Barzilay et al., 2001; Saggion, 2011); while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. NewsBlaster3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring 3 http://newsblaster.cs.columbia.edu 1701 language generation to produce a grammatical and coherent summary, and better suites the scenario of tweet summarization. Note that our method considers each tweet as the unit for summarizatio"
C12-1104,C10-1034,1,0.723844,"dies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (2010) propose TwitterRank, an extension of the PageRank algorithm to identify influential twitter accounts; O’Connor et al. (2010) present TweetMotif which groups tweets by frequent significant terms; Inouye and Kalita (2011) compare several tweet summarization algorithms that use text features like TFIDF to compute the similarity between any two tweet; Sharifi et al. (2010) exploit the Phrase Reinforcement Algorithm to find the most commonly used phrases that encompass the given topic phrase, based on which salient sentences"
C12-1104,W10-0713,0,0.031121,"es Congress (Golbeck et al., 2010), by city police departments in large U.S. cities (Heverin and Zach, 2010), and by scholars (Priem and Costello, 2010); Jansen et al. (2009) report research results investigating microblogging as a form of electronic word-of-mouth for sharing consumer opinions concerning brands; Heverin and Zach (2010) give insights into why particular events resonate with the population. All the above studies indicate the critical role of tweets as a dynamic information source. There is another line of studies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (201"
C12-1104,C10-1079,1,0.787012,"(2010) give insights into why particular events resonate with the population. All the above studies indicate the critical role of tweets as a dynamic information source. There is another line of studies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (2010) propose TwitterRank, an extension of the PageRank algorithm to identify influential twitter accounts; O’Connor et al. (2010) present TweetMotif which groups tweets by frequent significant terms; Inouye and Kalita (2011) compare several tweet summarization algorithms that use text features like TFIDF to compute the similarity"
C12-1104,P11-1037,1,0.779704,"lars (Priem and Costello, 2010); Jansen et al. (2009) report research results investigating microblogging as a form of electronic word-of-mouth for sharing consumer opinions concerning brands; Heverin and Zach (2010) give insights into why particular events resonate with the population. All the above studies indicate the critical role of tweets as a dynamic information source. There is another line of studies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (2010) propose TwitterRank, an extension of the PageRank algorithm to identify influential twitter accounts; O’Connor et al"
C12-1104,C08-1087,0,0.0587079,"Missing"
C12-1104,P10-1057,0,0.0172924,"ssifier, is learnt statistically from the training data. There are methods between those categories. For example, Wan and Yang (2008) consider cluster level information, i.e., the importance of the cluster and the relevance of sentence to the cluster, for computing sentence salience score. Motivated by LexRank (Erkan and Radev, 2004), we adopt graph based methods. Differently, our system incorporates rich social network features and considers readability to compute salience score of every tweet. Most existing studies focus on formal texts such as news. However, exceptions exist. For instance, Qazvinian and Radev (2010, 2008) study the problem of summarizing a scientific paper. They propose a clustering approach where communities in the citation summary’s lexical network are formed and sentences are extracted from separate clusters. Sharifi et al. (2010) use the Phrase Reinforcement algorithm to generate one-line summary for a collection of tweets related to a topic. Though our method is also designed for tweets, there are several significant differences. Firstly, our method does not assume that the input tweets are about a topic. Secondly, our method selects representative tweets by exploiting social netwo"
C12-1104,N10-1100,0,0.456337,"argely attributed to the short and noise prone nature of tweets, which causes a single tweet to be insufficient to provide reliable information to compute its salience score. We develop a graph-based summarization system that aggregates social signals, i.e., re-tweeted times and follower numbers to handle this challenge. More specifically, the translation probability from one tweet to the other depends on both the similarity between the two tweets and the social network features associated with the second tweet. This largely differentiates our system from existing studies, such as the work of Sharifi et al. (2010), which uses only tweet-level content features (e.g., keywords) to select representative sentences. 1 2 http://www.twitter.com Noise in tweets means ill-formed words or sentences in tweets. 1700 Besides utilizing social signals, our system has two additional features. Firstly, the readability feature is introduced to the graph model to reduce the chance of tweets hard to read to appear in the summarization. Several factors are considered while computing a tweet’s readability, including: 1) The number of out-of-vocabulary (OOV) words; 2) the number of words; and 3) the number of abnormal symbol"
C12-1104,xie-etal-2008-extracting,0,0.0150853,"d work. Section 3 defines the task. Section 4 describes the baseline. Section 5 details our method and Section 6 evaluates our method. Section 7 demonstrates the application of the proposed method to the Twitter search. Finally, Section 8 concludes with a discussion of future work. 2 Related Work Two categories of research are highly related to our work: multi-document summarization and recent studies of tweets. 2.1 Multi-document Summarization Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008), sentence compression (Knight and Marcu, 2002), and reformulation (Barzilay et al., 2001; Saggion, 2011); while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. NewsBlaster3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring 3 http://newsblaster.cs.columbia.edu 1701 language generation to produce a grammatical and coherent summary, and better suites the scenario of tw"
C12-1104,P99-1071,0,\N,Missing
C12-2081,esuli-sebastiani-2006-sentiwordnet,0,0.177866,"Missing"
C12-2081,P11-2104,0,0.169438,"Missing"
C12-2081,N10-1119,0,0.0289978,"Missing"
C12-2081,P07-1123,0,0.374643,"Missing"
C12-2081,E09-1077,0,0.249258,"Missing"
C12-2081,D08-1058,0,0.169823,"oreover, in many cases, two or more English sentiment words often are translated to the same foreign word. Both factors lead to smaller translated sentiment lexicons than the original ones. (Mihalcea et al., 2007) study the efectiveness of translating English sentiment lexicon to Romanian using two bilingual dictionaries. The original English sentiment lexicon contains 6,856 entries; after translation, only 4,983 entries are left in the Romanian sentiment lexicon. About 2000 entries are lost or conlated into other entries during the translation process. The translation method is also used in (Wan, 2008, 2011). On the other hand, though bootstrapping methods don't use bilingual dictionaries and hence are not subject to the limitation of the translation methods, they have relatively high demands for semantic resources such as WordNet (Fellbaum, 1998). Bootstrapping methods enlarge the sentiment lexicons from English sentiment seed words. (Hassan et al., 2011) present a method to identify the sentiment polarity of foreign words by using WordNet (or similar semantic resources) in the target foreign language. (Ku and Chen, 2007) create a Chinese Lexicon by translating the General Inquirer, combi"
C12-2081,J11-3005,0,0.0428357,"Missing"
C12-2081,N10-2012,0,0.014788,"eriod or question mark, at the end of the English word. We use this simple rule to limit the possible parts-of-speech of the translations. For example, “efusive.” is translated to “热情洋溢”, while “efusive” is translated to “感情奔放的” ; after adding punctuation context, “efusive” is translated to words that have diferent parts-of-speech. We can also combine this technique with the coordinated phrase technique. Concretely, We use a bi-gram language model for generating possible collocations. Instead of creating our own language model from large corpora, we leverage the Microsoft Web N-gram Services (Wang et al., 2010)3 , an online N-gram corpus that built from Web documents. We choose the bi-gram language model trained on document titles. Given each English polarity word w1 , we use the language model to generate up to the 1000 most frequent bi-grams w1 w2 . To create coordinated phrases, we irst translate all sentiment words using Google Translate. And then we create coordinated phrases for the English sentiment words which are translated into the same Chinese word. We select those English words and join them with the word “and”. The punctuation context are generated by appending a period after the given"
C12-2081,H05-1044,0,0.0366647,"ent dictionaries in other languages as well. Depending on the target language, we might need to make some small modiications. Word segmentation is unnecessary for most European languages. And in some languages, we need to consider the word order issues when extracting the sentiment words from the translation results, since translation engine might reorder the queries. For example, in Arabic, the modifying adjectives are placed before the nouns, which is diferent from English; and also in Arabic, the words are written from right to left. 3 Experimental Study We use the MPQA subjective lexicon (Wilson et al., 2005) as the English lexicon. We only keep the strong subjective entries, which include 1,481 positive and 3,080 negative entries. For the purpose of comparison, we implemented the following baseline approaches. The irst three baselines rely on a bilingual dictionary. We use the LDC (Linguistic Data Consortium) English-Chinese bilingual wordlists6 , which is also used in (Wan, 2008). This dictionary contains 18,195 entries. Each English entry is mapped to a list of Chinese words or expressions. As shown in Table 2, the irst baseline (DICT) looks up the English entry in the bilingual dictionary and"
C12-2081,C10-1136,1,0.894878,"Missing"
C14-1018,baccianella-etal-2010-sentiwordnet,0,0.227345,"or negative score reflecting its sentiment polarity and strength. Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale se"
C14-1018,J81-4005,0,0.756294,"Missing"
C14-1018,P07-1054,0,0.0130165,"component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym"
C14-1018,P12-1092,0,0.137724,"ons between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment"
C14-1018,C04-1200,0,0.143042,"ng the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are"
C14-1018,P13-2087,0,0.0738275,"hrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words an"
C14-1018,P12-1043,0,0.0177108,"with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item."
C14-1018,C94-1079,0,0.0290494,"nt lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentim"
C14-1018,P11-1015,0,0.623804,"ntation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase em"
C14-1018,S13-2053,0,0.722061,"(2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and :(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expressions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of word"
C14-1018,S13-2052,0,0.0305225,"Missing"
C14-1018,W02-1011,0,0.0230923,"m tweets, leveraging massive tweets containing positive and negative emoticons as training set without any manual annotation. To obtain more training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban Dictionary 2 , which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in the sentiment lexicon. We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. T"
C14-1018,J11-1002,0,0.0117166,"timent information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level cla"
C14-1018,E09-1077,0,0.0168758,"uilt manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resource"
C14-1018,D11-1014,0,0.0867976,"an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodology In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode the sentimen"
C14-1018,D13-1170,0,0.0317488,"Missing"
C14-1018,P14-1146,1,0.621391,"d in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodolo"
C14-1018,P02-1053,0,0.0288544,"eets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment sco"
C14-1018,N10-1119,0,0.0358758,"entiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approac"
C14-1018,H05-1044,0,0.940027,"eness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. The main contributions of this work are as follows: • To our best knowledge, this is the first work that leverages the continuous representation of phrases for building large-scale sentiment lexicon from Twitter; • We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework fo"
C14-1018,P13-1173,0,0.0128539,"negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this directi"
C14-1018,P10-1040,0,\N,Missing
C14-1108,P06-1067,0,0.0317284,"problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both te"
C14-1108,J07-2003,0,0.649224,"first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the rule. For This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1144 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1144–1153, Dublin, Ireland, August 23-29 2014. example"
C14-1108,D08-1089,0,0.57593,"ic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the"
C14-1108,C10-1050,0,0.44024,"nsistency in Moses between training and decoding. Here we would like to note that phrase based orientation depends on phrase segmentation. For example, in Figure 1, the orientation of phrase “this is” with respect to next phrase could be either:  D, if we think the next phrase is “the lower reach of ” which is what Figure 1 shows.  or S, if the next phrase is “the lower reach of the yellow river” which can compose a legal phrase pair with “huanghe xiayou” according to the standard phrase pair extraction algorithm. 1150 The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who proposed a word-based reordering model for HPB system. The difference between our work and Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model whic"
C14-1108,C08-1041,0,0.126231,"29 2014. example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub phrase X before “xiayou” should be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reorderi"
C14-1108,W05-1507,0,0.0354703,"ine translation, the problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous g"
C14-1108,D10-1014,0,0.0157829,"hould be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reordering model for hierarchical phrase-based translation and achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a str"
C14-1108,W13-2258,0,0.237941,"r internal structure. For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientation probability of X1 will condition on “zhe, xiayou, this, river”. In this paper, we will consider how the words covered by the nonterminal X1 are reordered. Rather than using “xiayou” as a feature to determine the orientation of X1 with respect to the next phrase, we think the immediately translated source word “huanghe” could be more informative through it is not on the boundary of X1 , since “huanghe” is the exact starting point from where we search for the next phrase to translate. Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned training data for use with hierarchical phrase inventories, and scored orientations in hierarchical decoding. 2.2 Path-based lexicalized reordering model The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discontinuous phrase-based translation path in the following two steps: 1) Represent each rule as a sequence of phrase pairs and non-terminals. 2) The rules’ sequences are used"
C14-1108,P07-2045,0,0.00826616,"central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonter"
C14-1108,P06-1090,0,0.0287036,"anguage into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and n"
C14-1108,P13-1156,0,0.275902,"the immediately translated source word “huanghe” could be more informative through it is not on the boundary of X1 , since “huanghe” is the exact starting point from where we search for the next phrase to translate. Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned training data for use with hierarchical phrase inventories, and scored orientations in hierarchical decoding. 2.2 Path-based lexicalized reordering model The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discontinuous phrase-based translation path in the following two steps: 1) Represent each rule as a sequence of phrase pairs and non-terminals. 2) The rules’ sequences are used to find the corresponding phrase-based path of a HPB derivation and calculate the phrase-based reordering features. Figure 2. The phrase-based path of the derivation in Figure 1. 1145 A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentences and whose target sides generated the target sentences from left to right. For example, the phrase-based"
C14-1108,P00-1056,0,0.144249,"ectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to tune feature weights for translation systems. We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002). 5.2 Experimental results on FBIS corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in table 1. Chinese English #sentences 128832 128832 #wor"
C14-1108,J04-4002,0,0.219684,"rectly on synchronous rules. For each target phrase contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model on both small and large scale data. On NIST machine translation test sets, our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline hierarchical phrase-based system. 1 Introduction In statistical machine translation, the problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reord"
C14-1108,P03-1021,0,0.111575,"ner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model which are M, S and D with respect to the previous and next phrase respectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to tune feature weights for translation systems. We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua"
C14-1108,P02-1040,0,0.0916478,"s used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002). 5.2 Experimental results on FBIS corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in table 1. Chinese English #sentences 128832 128832 #words 3016570 3922816 Table 1. The statistics of FBIS corpus Table 2 summarizes the translation performance. The first row shows the results of baseline HPB system, and the second row shows the results when we integrated our lexicalized reordering model (LRM). We get 1.2, 0.8 and 0.7 BLEU point improv"
C14-1108,N04-4026,0,0.184029,"Missing"
C14-1108,D09-1105,0,0.113438,"ase “this is” with respect to next phrase could be either:  D, if we think the next phrase is “the lower reach of ” which is what Figure 1 shows.  or S, if the next phrase is “the lower reach of the yellow river” which can compose a legal phrase pair with “huanghe xiayou” according to the standard phrase pair extraction algorithm. 1150 The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who proposed a word-based reordering model for HPB system. The difference between our work and Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model which are M, S and D with respect to the previous and next phrase respectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003)"
C14-1108,2011.mtsummit-papers.43,0,0.115615,"has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reordering model for hierarchical phrase-based translation and achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPB baseline system. 2 Related work In this section, we briefly review two types of related work which are a nonterminal-based lexicalized reordering models and a path-based lexicalized reordering model. Both of them calculate the orientation for HPB translation. 2.1 Nonterminal-based lexicalized reordering models Xiao et al. (2011) proposed an orientation model for HPB translation. The orientation probability of a derivation is calculated as the product of orientation probabilities of all nonterminals except the root. In order to define the relative orders of nonterminals and their adjacent phrase, they expand the alignment in a rule to include both terminals and nonterminals. There may be multiple ways to segment a rule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008). They significantly outperformed the HPB system on both Chinese-English and German-English translation. Xiao et al"
C14-1108,P06-1066,0,0.0289271,"order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phr"
C14-1108,W06-3108,0,0.131425,"language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of"
C14-1108,W06-3119,0,0.0423035,"al Linguistics: Technical Papers, pages 1144–1153, Dublin, Ireland, August 23-29 2014. example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub phrase X before “xiayou” should be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based"
C14-1108,W12-3125,0,\N,Missing
C14-1210,W09-2307,0,0.162431,"gaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations plus a default relation representing unknown cases. The detailed descriptions about dependency parsing are explained in Chang et al. (2009). 5.2 Experimental Results on FBIS Corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the preprocessing. 2233 Chinese English #sentences 128,832 128,832 #words 3,016,570 3,922,816 Table 2. The statistics of FBIS corpus The evaluation results over FBIS corpus are reported in Table 3. The first row shows the results of baseline, the next three rows show the effect of three features respectively and the last row gives the result when all features are"
C14-1210,P08-1009,0,0.0373898,"Missing"
C14-1210,J07-2003,0,0.881897,"translate a sentence, the dependency knowledge is used to compute the syntactic structural consistency of the rule against the dependency tree of the sentence. We characterize the structure consistency by three features and integrate them into the standard SMT log-linear model to guide the translation process. Our method is evaluated on multiple Chinese-to-English machine translation test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points improvements over a strong baseline of an in-house implemented HPB translation system. 1 Introduction HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical trans"
C14-1210,D11-1079,0,0.0167224,"and that between the rule and its context. Both above related work and our work need parse the source sentence to get syntactic context before decoding. There are also some methods incorporating syntax information without the need of online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). They parse the training data to label the non-terminals with syntactic tags. During the bottom-up decoding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006). Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. They focus on the relative order of each dependent word and its head word after translation, while our method models whether the dependency information of a rule matches the context or not. Our work utilizes contextual information around translation rules. In this sense, it is similar to He et al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they leverage lexical context for rule selection while we focus on the syntactic contextual information. 3 Hierarchical Phrase"
C14-1210,C08-1041,0,0.017156,"with syntactic tags. During the bottom-up decoding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006). Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. They focus on the relative order of each dependent word and its head word after translation, while our method models whether the dependency information of a rule matches the context or not. Our work utilizes contextual information around translation rules. In this sense, it is similar to He et al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they leverage lexical context for rule selection while we focus on the syntactic contextual information. 3 Hierarchical Phrase based Machine Translation Our model proposed in this paper is an extension of the HPB model (Chiang, 2007). Formally, HPB model is a weighted synchronous context free grammar. It employs a generalization of the standard plain phrase extraction approach in order to acquire the synchronous rules of the grammar directly from word-aligned parallel text. Rules have the form of: where X is a"
C14-1210,2006.amta-papers.8,0,0.0356325,"phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been muc"
C14-1210,D10-1014,0,0.688855,"Missing"
C14-1210,D13-1053,0,0.358716,"ecial efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borrowed X2 X1> (2) <借 了 X, lent X> (4) <X1 借 了 X2, X1 borrowed X2> (5) <借了 NP, borrowed X> (7) <PP 借了 NP, borrow X2 X1> (6) <借了 NP, lent X> (8) <NP 借了 NP, X1 lent X2> Although augmenting the non-terminals with syntactic tags in these methods achieved better results for HPB model, they have limitations that the syntax information on the non-terminals are not discrimThis work is licenced under a Creative Commons Attrib"
C14-1210,P00-1056,0,0.147529,"PB rule is extended into the form of and the score is calculated by: ∑ where the additional three features are defined in Section 4.3, , and are corresponding feature weights. We test our soft dependency matching model on a Chinese-English translation task. The NIST06 evaluation data was used as our development set to tune the feature weights, and NIST04, NIST05 and NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel corpus, and then further test the performance of our method on a large scale training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations"
C14-1210,P03-1021,0,0.120898,"rd-aligned parallel text. Rules have the form of: where X is a nonterminal, and are both strings of terminals and non-terminals from source and target side respectively, and ∼ is a one-to-one correspondence between nonterminal occurrences in and . Associated with each rule is a set of feature functions with the form . These feature functions are combined into a log-linear model. When a rule is applied during SMT decoding, its score is calculated as: ∑ where is the weight associated with feature function . The feature weights are typically optimized using minimum error rate training algorithm (Och, 2003). 4 Soft Dependency Matching Model In order to incorporate syntactic knowledge to refine both the word ordering and word sense disambiguation for HPB model, we propose a soft dependency matching model (SDMM). It extends HPB rule into a form which is named as SDMM rule: where RDT(rule’s dependency triples) is a set of dependency triples defined on source string . Each element in RDT is a triple representing dependency knowledge in the form: {m-h-l} where m and h are the dependent and head respectively, l is the label of the dependency relation type. m and h could be any of terminals, non-termin"
C14-1210,N03-1017,0,0.0567749,"dency tree of the sentence. We characterize the structure consistency by three features and integrate them into the standard SMT log-linear model to guide the translation process. Our method is evaluated on multiple Chinese-to-English machine translation test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points improvements over a strong baseline of an in-house implemented HPB translation system. 1 Introduction HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008),"
C14-1210,W12-3128,0,0.0242719,"Missing"
C14-1210,P06-1077,0,0.057462,"model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010"
C14-1210,P08-1114,0,0.0791107,"(2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borrowed X2 X1> (2) <借 了 X, lent X> (4) <X1 借 了 X2, X1 borrowed X2> (5) <借了 NP, borrowed X> (7) <PP 借了 NP, borrow X2 X1>"
C14-1210,D08-1022,0,0.0554861,"Missing"
C14-1210,P02-1040,0,0.0917171,"nd NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel corpus, and then further test the performance of our method on a large scale training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations plus a default relation representing unknown cases. The detailed descriptions about dependency parsing are explained in Chang et al. (2009). 5.2 Experimental Results on FBIS Corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus af"
C14-1210,P08-1066,0,0.026789,"by using the formal synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by"
C14-1210,D09-1008,0,0.366391,"resent the syntactic variation of translation rules in the form of distribution. The main difference is that they annotate non-terminals with head POS tags while we use dependency triples (over both terminals and non-terminals) to explicitly represent both the dependency relations inside the rule, and that between the rule and its context. Both above related work and our work need parse the source sentence to get syntactic context before decoding. There are also some methods incorporating syntax information without the need of online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). They parse the training data to label the non-terminals with syntactic tags. During the bottom-up decoding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006). Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. They focus on the relative order of each dependent word and its head word after translation, while our method models whether the dependency information of a rule matches the context or not. Our work utilizes contextu"
C14-1210,2010.amta-papers.8,0,0.029154,"Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borr"
C14-1210,J97-3002,0,0.019268,"eved promising results on various language pairs. One problem of these methods is that exactly matching syntactic constraints cannot always guarantee a good translation, and violating syntactic structure does not always induce a poor translation. It could be more reasonable if the credit and penalty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge directly from the syntactic structures over the training corpus. Xiong et al. (2009) present a method that automatically learns syntactic constraints from training data for the ITG based translation (Wu, 1997; Xiong et al., 2006). They utilize the syntactic constraints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on the ITG based model, the method is also applicable to the HPB model. The main difference between Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 2228 against the source syntax tree. For rules which are same in the source side but different in the target side, our method will distinguish the inconsistency degree for different rules. While, for such rules, Xiong et al. (2009) will give a sam"
C14-1210,D11-1020,0,0.0180229,"synchronous grammar to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating synta"
C14-1210,P06-1066,0,0.0356965,"sing results on various language pairs. One problem of these methods is that exactly matching syntactic constraints cannot always guarantee a good translation, and violating syntactic structure does not always induce a poor translation. It could be more reasonable if the credit and penalty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge directly from the syntactic structures over the training corpus. Xiong et al. (2009) present a method that automatically learns syntactic constraints from training data for the ITG based translation (Wu, 1997; Xiong et al., 2006). They utilize the syntactic constraints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on the ITG based model, the method is also applicable to the HPB model. The main difference between Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 2228 against the source syntax tree. For rules which are same in the source side but different in the target side, our method will distinguish the inconsistency degree for different rules. While, for such rules, Xiong et al. (2009) will give a same score which will be"
C14-1210,P09-1036,0,0.0164889,"credit if it respects the parse tree but may incur a cost if it violates a constituent boundary. The soft constrain based methods achieved promising results on various language pairs. One problem of these methods is that exactly matching syntactic constraints cannot always guarantee a good translation, and violating syntactic structure does not always induce a poor translation. It could be more reasonable if the credit and penalty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge directly from the syntactic structures over the training corpus. Xiong et al. (2009) present a method that automatically learns syntactic constraints from training data for the ITG based translation (Wu, 1997; Xiong et al., 2006). They utilize the syntactic constraints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on the ITG based model, the method is also applicable to the HPB model. The main difference between Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 2228 against the source syntax tree. For rules which are same in the source side but different in the target side, our meth"
C14-1210,W06-3119,0,0.609439,"ference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntactic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X could be refined into NP or PP as shown in rules (5-8) respectively. (1) <借 了 X, borrowed X> (3) <X1 借 了 X2, borrowed X2 X1> (2) <借 了 X, lent X> (4) <X1 借 了 X2, X1 borrowed X2> (5) <借了 NP, borrowed X> (7) <PP 借了 NP, borrow X2 X1> (6) <借了 NP, lent X> (8) <NP 借了 NP, X1 lent X2> Although augmenting the non-terminals with syntactic tags in these methods achieved better results for HPB model, they have limitations that the syntax information on the non-terminals are not discrimT"
C14-1210,P08-1064,0,0.0171028,"r to well capture the recursiveness of language during translation. In a formal synchronous grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. To generate grammatical translations, lots of syntax-based models have been proposed by Galley et al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic structure of either the source sentence or the target sentence. These approaches can generate more grammatical translations by capturing the structural difference between language pairs. However, these models need special efforts to capture non-syntactic translation knowledge to improve the translation performance. It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 2010). There has been much work trying to improve HPB model by incorporating syntax information. Marton"
C14-1210,P11-2033,0,0.0160391,"corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Feature weights are tuned with the minimum error rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 score (Papineni et al., 2002). All the Chinese sentences in the training set, development set and test set are parsed by an in-house developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 named grammatical relations plus a default relation representing unknown cases. The detailed descriptions about dependency parsing are explained in Chang et al. (2009). 5.2 Experimental Results on FBIS Corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the preprocessing. 2233 Chinese English #sentences 128,832 128,832 #words 3,016,570 3,922,816 Table 2. The statistics of FBIS corpus The evaluation results over FBIS corpus are report"
C16-1004,D10-1047,0,0.0521323,"Missing"
C16-1004,P15-2136,1,0.701611,"a set of selected sentences S. Specifically, we present a new framework to conduct regression with respect to the relative gain of s given S calculated by the ROUGE metric. Besides the single sentence features, additional features derived from the sentence relations are incorporated. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that the proposed method outperforms state-of-the-art extractive summarization approaches. 1 Introduction Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ra"
C16-1004,C12-1056,0,0.0164822,"Missing"
C16-1004,W06-1643,0,0.0131876,"ion Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relativ"
C16-1004,W09-1802,0,0.0313306,"kova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relative importance f (s|S) with a regression model where additional features involving the sentence relations are incorporated. Then we generate the summary by greedily selecting the next sentence which maximizes f (s|S) with respect to the current selected sentences S. Our"
C16-1004,W00-0405,0,0.0341864,"Missing"
C16-1004,P07-2049,0,0.0156763,"Missing"
C16-1004,E14-1075,0,0.0755947,"ive gain of s given S calculated by the ROUGE metric. Besides the single sentence features, additional features derived from the sentence relations are incorporated. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that the proposed method outperforms state-of-the-art extractive summarization approaches. 1 Introduction Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and"
C16-1004,D15-1011,0,0.037119,"Missing"
C16-1004,P13-1099,0,0.0831969,"tence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relative importance f (s|S) with a regression model where additional features involving the sentence relations are incorporated. Then we generate the summary by greedily selecting the next sentence which maximizes f (s|S) with respect to the current selected sentences S. Our method improves th"
C16-1004,N10-1134,0,0.0287194,"Missing"
C16-1004,P11-1052,0,0.0967887,"Missing"
C16-1004,C00-1072,0,0.02429,"013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relative importance f (s|S) with a regression model where additional features involving the sentence relations are incorporated. Then we generate the summary by greedily selecting the next sentence which maximizes f (s|S) with respec"
C16-1004,W04-1013,0,0.0300013,"Missing"
C16-1004,P04-3020,0,0.0219873,"Missing"
C16-1004,W02-0401,0,0.0259134,"ummarization approaches. 1 Introduction Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Sp"
C16-1004,W12-2601,0,0.0356117,"Missing"
C16-1004,W00-0403,0,0.208521,"Missing"
C16-1004,P13-2024,0,0.0184591,"Missing"
C16-1004,D08-1079,0,0.0155828,"Missing"
C16-1004,P11-1155,0,0.0294679,"Missing"
C16-1004,W04-3252,0,\N,Missing
C16-1187,R13-1026,0,0.0312198,"need the information. Therefore, our effort is complementary to the existing work on response generation. It can keep the existing generation algorithms context-aware and improve their efficiency and robustness to noise. The task is challenging, as messages in a conversational environment are usually short and informal, and evidence that can indicate a message is context dependent is scarce. For example, on 3 million post-response pairs crawled from Weibo, the average length of messages is 4.65. On such short texts, classic NLP tools such as POS Tagger and Parser suffer from bad performance (Derczynski et al., 2013; Foster et al., 2011) and it is difficult to explicitly extract features that are discriminative on the two types of messages. More seriously, there are no large scale annotations available for building a supervised learning procedure. We consider leveraging the large amount of human-human conversation data available on the web to learn a message classifier. Our intuition is that a context dependent message has different linguistic context in different conversation sessions, therefore its responses could be more diverse on content than responses of a context independent message. To verify thi"
C16-1187,W11-2017,0,0.0295448,"proposal of learning weak supervision signals from responses of messages using large scale conversation data; 3) proposal of using an LSTM architecture to learn a message classifier; 4) empirical verification of the proposed method on human annotated data. 1991 2 Related Work Our work lies in the path of building chatbot systems with data-driven approaches. Differing from traditional dialogue systems (cf., (Young et al., 2013)) which rely on hand-crafted features and rules to generate reply sentences for specific applications such as voice dialling (Williams, 2008) and appointment scheduling (Janarthanam et al., 2011) etc., recent effort focuses on exploiting an end-to-end approach to learn a response generator from social conversation data for open domain dialogue (Koshinda et al., 2015; Higashinaka et al., 2016). For example, Ritter et al. (Ritter et al., 2011) employed a phrase-based machine translation model for response generation. In (Shang et al., 2015; Vinyals and Le, 2015), neural network architectures were proposed to learning response generators from one-round conversation data. Based on these work, Sordoni et al. (Sordoni et al., 2015b) incorporated linguistic context into the learning of respo"
C16-1187,D11-1054,0,0.254837,"ed method can significantly outperform baseline methods on accuracy of classification. 1 Introduction Together with the rapid growth of social media such as Twitter and Weibo, the amount of conversation data on the web has tremendously increased. This makes building open domain chatbot systems with data-driven approaches possible. To carry on reasonable conversations with humans, a chatbot system needs to generate proper response with regard to users’ messages. Recently, with the large amount of conversation data available, learning a response generator from data has drawn a lot of attention (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). A key step to coherent response generation is determining when to consider linguistic context of messages. Existing work on response generation, however, has overlooked this step. They either totally ignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al., 2015b; Serban et al., 2015). The former case is easy to lead to irrelevant responses when users’ input messages rely on the context information in previous conversation turns, while the latter case is"
C16-1187,P15-1152,0,0.290145,"cantly outperform baseline methods on accuracy of classification. 1 Introduction Together with the rapid growth of social media such as Twitter and Weibo, the amount of conversation data on the web has tremendously increased. This makes building open domain chatbot systems with data-driven approaches possible. To carry on reasonable conversations with humans, a chatbot system needs to generate proper response with regard to users’ messages. Recently, with the large amount of conversation data available, learning a response generator from data has drawn a lot of attention (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). A key step to coherent response generation is determining when to consider linguistic context of messages. Existing work on response generation, however, has overlooked this step. They either totally ignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al., 2015b; Serban et al., 2015). The former case is easy to lead to irrelevant responses when users’ input messages rely on the context information in previous conversation turns, while the latter case is costly (e.g., on mem"
C16-1187,N15-1020,0,0.139745,"eds to generate proper response with regard to users’ messages. Recently, with the large amount of conversation data available, learning a response generator from data has drawn a lot of attention (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). A key step to coherent response generation is determining when to consider linguistic context of messages. Existing work on response generation, however, has overlooked this step. They either totally ignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al., 2015b; Serban et al., 2015). The former case is easy to lead to irrelevant responses when users’ input messages rely on the context information in previous conversation turns, while the latter case is costly (e.g., on memory and responding time) for building a real chatbot system and has the risk of bringing in noise to response generation especially when users want to end the current conversation topic and start a new one. According to our observation, there are two types of messages in a conversational environment. The first type is context dependent message, which means to reply to the message,"
C16-1187,P08-4001,0,0.0201772,"essages in a conversational environment; 2) proposal of learning weak supervision signals from responses of messages using large scale conversation data; 3) proposal of using an LSTM architecture to learn a message classifier; 4) empirical verification of the proposed method on human annotated data. 1991 2 Related Work Our work lies in the path of building chatbot systems with data-driven approaches. Differing from traditional dialogue systems (cf., (Young et al., 2013)) which rely on hand-crafted features and rules to generate reply sentences for specific applications such as voice dialling (Williams, 2008) and appointment scheduling (Janarthanam et al., 2011) etc., recent effort focuses on exploiting an end-to-end approach to learn a response generator from social conversation data for open domain dialogue (Koshinda et al., 2015; Higashinaka et al., 2016). For example, Ritter et al. (Ritter et al., 2011) employed a phrase-based machine translation model for response generation. In (Shang et al., 2015; Vinyals and Le, 2015), neural network architectures were proposed to learning response generators from one-round conversation data. Based on these work, Sordoni et al. (Sordoni et al., 2015b) inco"
C16-1187,P15-2041,0,0.0179695,"eration. In this paper, instead of studying how to incorporate context into response generation, we consider the problem that when we need context in the process. Our work can keep the existing generation algorithms context-aware and at the same time improve their efficiency and robustness. We employ a Recurrent Neural Network (RNN) architecture to learn a message classifier. RNN models (Elman, 1990), due to their capability of modeling sequences with arbitrary length, have been widely used in many natural language processing tasks such as language modeling (Mikolov et al., 2010) and tagging (Xu et al., 2015) etc. Recently, it is reported that Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as two special RNN models which can capture long term dependencies in sequences outperform state of the art methods on tasks like machine translation (Sutskever et al., 2014) and response generation (Shang et al., 2015). In this paper, we apply the LSTM architecture to the task of context dependent message detection. We append LSTM with a two-layer feed-forward neural network, thus feature learning and model learning can be carried out simultane"
C16-1236,P14-1133,0,0.0265941,"Missing"
C16-1236,Q15-1039,0,0.0714069,"Missing"
C16-1236,D13-1160,0,0.40406,": Sum of CNN scores between context pattern and binding Path for each entity constraint Po : Sum of embedding similarities between superlative phrase and binding Path for each ordinal constraint Table 3: Features and their description. 5 Experiment We introduce experiment part on these aspects. We first introduce the settings of our experiments, especially the three data sets containing question/answer (QA) pairs. On these data sets, the results of our method are given, and based on the results we analyze drawbacks. 5.1 Set Up System Components We use the entire Freebase dump which is same as Berant et al. (2013) and host it with Virtuoso engine11 . Besides, an entity linker (Yang and Chang, 2015), the Stanford NER (Finkel et al., 2005), and an in-house implementation of shift-reduce dependency parser (Zhang and Nivre, 2011) with Stanford dependency (De Marneffe et al., 2006) which is used in detecting temporal clause are adopted in this work. Data Sets We evaluate our approach on three data sets. (i) ComplexQuestions (CompQ): It is a new data set which includes 2100 QA pairs released by this work with the details in Section 2. (ii) WebQuestions (WebQ): It contains 3778 QA pairs on training set and 20"
C16-1236,D14-1067,0,0.0389445,"Missing"
C16-1236,de-marneffe-etal-2006-generating,0,0.0293992,"Missing"
C16-1236,P15-1026,1,0.872928,"Missing"
C16-1236,P05-1045,0,0.460005,"contain any word in this word set, we simply remove it. This is intuitive, as WebQuestions and SimpleQuestions are our training data, and we only consider queries that can be covered by the training data as query candidates. Last, we classify the remaining queries based on the following rules: (1) If a question contains at least two non-overlap entities, then it belongs in the Multi-Entity category; (2) If a question contains a type phrase that comes from Freebase, then it belongs in the Type category; (3) If a question contains a time expression detected by an Named Entity Recognizer (NER) (Finkel et al., 2005), then it belongs in the Explicit Temporal category; (4) If a question contains keywords “when”, “before”, “after” and “during” in the middle, then it belongs in the Implicit Temporal category; (5) If a question contains ordinal number or superlative phrase from WordNet (Miller, 1995), then it belongs in the Ordinal category; (6) If a question starts with “how many”, or includes “number of” or “count of”, then it belongs in the Aggregation category. Note, a multi-constraint question may contain multiple types of constraints. We show constraint types, examples, and distributions in Table 1. Ten"
C16-1236,Q14-1030,0,0.032321,"Missing"
C16-1236,Q16-1010,0,0.0299673,"Missing"
C16-1236,P16-1220,0,0.269382,"Missing"
C16-1236,P15-1049,0,0.115534,"of x in ascending order, return the nth one Return the number of entity set x. Table 2: Functional predicates defined in this work. query set, which contains 20,999,951 distinct 5W1H questions2 that satisfy the following two rules: (i) each query should not contain pronouns (e.g., ‘you’, ‘my’, etc.), as questions with such words are usually non-factual questions, and (ii) each query’s length is between 7 and 20, as short queries seldom contain multi-constraints, and long queries are usually difficult to answer. Then, we further sample 10 percent of questions, and use an entity linking method (Yang and Chang, 2015) to detect entities. If no entity can be detected from a query, we simply remove it. Next, both WebQuestions and SimpleQuestions are used to extract a set of words, without considering stop words and entity words. If a query does not contain any word in this word set, we simply remove it. This is intuitive, as WebQuestions and SimpleQuestions are our training data, and we only consider queries that can be covered by the training data as query candidates. Last, we classify the remaining queries based on the following rules: (1) If a question contains at least two non-overlap entities, then it b"
C16-1236,D14-1071,1,0.685243,"Missing"
C16-1236,P14-1090,0,0.15918,"Missing"
C16-1236,N15-3014,0,0.14959,"Missing"
C16-1236,P14-2105,0,0.0197143,"vertices, and x denotes the answer. For example, the basic query graph B in Figure 2 can be represented as hUnited States, officials-y0 -holder, xi. To measure the quality of each basic query graph constructed, we leverage a convolutional neural network (CNN)-based model that is similar to (Gao et al., 2015; Shen et al., 2014b; Shen et al., 2014a; Yih et al., 2015) to calculate the similarity between question and the path of the basic query graph. We will describe the training resource in Section 4.4. 4.2 Constraint Detection and Binding Basic query graph is fit for single relation questions (Yih et al., 2014; Bordes et al., 2015), but not suffices to express a question with multiple constraints, such as the question in Figure 2. Hence, we propose to use constraints to restrict the answer set by adding them into the basic query graph.Adding a constraint contains two steps: Constraint Detection and Constraint Binding. We explain how to add each of the six kinds of constraints respectively in the following parts. Entity Constraint Entity constraint is designed to understand entities and relations which are often expressed by noun phrases and verb phrases. A constraint with an entity as its constant"
C16-1236,P15-1128,0,0.706817,"s, variable &lt; Equal vertices y0 and x, and two edges officials and holder. {C1 , C2 , C3 } is an ordered con?3 ?2 ?1 1 MaxAtN straint sequence detected based on the question, where C1 = hPresident,Equal,y1 i, C2 = h2000,&lt;,y2 i, C3 = h1,MaxAtN,y2 i. By adding ? C1 , C2 , C3 in order, we can construct the MulCG United States officials ?0 holder ? in Figure 2. Note, different constraint order can result in different MulCGs. We will introduce Figure 2: MulCG for question “Who was the first how to generate a MulCG in Section 4. president of United States after 2000?” Compared to the stage graph in Yih et al. (2015), our MulCG has the following two differences: (1) Entity constraints can be added beyond single KB fact, while stage graph only considers entities that connect to the CVT node of a single KB fact as constraints. (2) Non-entity constraints are defined and handled in a systematic way, while stage graph only considers limited non-entity constraints, i.e., type and gender. 4 Our Approach Problem Formalization Given a MulCQ Q and a KB K, the question is parsed into a set of MulCGs H(Q). For each MulCG G ∈ H(Q), a feature vector F(Q, G) is extracted and the one with the highest 5 If p contains two"
C16-1236,P11-2033,0,0.0142789,"nd their description. 5 Experiment We introduce experiment part on these aspects. We first introduce the settings of our experiments, especially the three data sets containing question/answer (QA) pairs. On these data sets, the results of our method are given, and based on the results we analyze drawbacks. 5.1 Set Up System Components We use the entire Freebase dump which is same as Berant et al. (2013) and host it with Virtuoso engine11 . Besides, an entity linker (Yang and Chang, 2015), the Stanford NER (Finkel et al., 2005), and an in-house implementation of shift-reduce dependency parser (Zhang and Nivre, 2011) with Stanford dependency (De Marneffe et al., 2006) which is used in detecting temporal clause are adopted in this work. Data Sets We evaluate our approach on three data sets. (i) ComplexQuestions (CompQ): It is a new data set which includes 2100 QA pairs released by this work with the details in Section 2. (ii) WebQuestions (WebQ): It contains 3778 QA pairs on training set and 2032 on test set which is released by Berant et al. (2013). The questions are collected from query log and the answers are manually labeled based on Freebase. (iii) SimpleQuestions (SimpQ): Each question in SimpleQuest"
C16-1236,P14-1091,1,\N,Missing
C16-1290,J93-2003,0,0.0754472,"a more flexible way, the attention mechanism (Bahdanau et al., 2014) was introduced into the encoderdecoder model. In an attention-based encoder-decoder model, matching scores between the source and target words are calculated based on their corresponding encoder and decoder states. These scores are then normalized and used as weights for the source words given each target word. This can be seen as a soft alignment and the attention mechanism here plays similar role to that of a traditional alignment model. In alignment models used in traditional machine translation models such as IBM Models (Brown et al., 1993), distortion and fertility are modeled explicitly. By comparison, in the attention mechanism, alignment is computed by matching the previous decoder hidden state with all the encoder hidden states, without modeling distortion and fertility. Since the translation of target words is guided by the attention † Work done while author was an undergraduate student of Shanghai Jiao Tong University and intern at Microsoft Research. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3082 Proceedings of COLIN"
C16-1290,J07-2003,0,0.0314165,"Missing"
C16-1290,W14-4012,0,0.00869674,"Missing"
C16-1290,D14-1179,0,0.0670752,"Missing"
C16-1290,P14-1129,0,0.0219979,"Missing"
C16-1290,P09-1104,0,0.0314005,"+ F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test set. Incorrect Reordering In Figure 2 we can see, R EC ATT generated the correct alignment on the example sentence shown"
C16-1290,D13-1176,0,0.25225,"Missing"
C16-1290,koen-2004-pharaoh,0,0.0197588,"ss the missing and repeated translation problems, we introduce fertility-conditioned decoder F ERT D EC. F ERT D EC uses a coverage vector 1 to represent the informationPof the source sentence that has not been translated. Initialized by the sum of source word embeddings Jj=1 xj and updated along the translation dynamically, our trainable coverage vector is different from the predefined condition vector used in (Wen et al., 2015). In order to leverage the coverage vector in decoding, we change the 1 The coverage vector in our work plays a similar role with the one used in beam search decoder (Koehn, 2004).There are two major two differences between them: 1. our coverage vector is used as a soft constraint instead of a hard constraint. 2. we tract untranslated words instead of translated words. 3086 decoding recurrent unit as follows: di = ei−1 di−1 ri = σ(W r yi−1 + U r hi−1 + V r di ) zi = σ(W z yi−1 + U z hi−1 + V z di ) ei = σ(W e yi−1 + U e hi−1 + V e di ) h0i = tanh(U (ri hi−1 ) + W yi−1 + V di ) hi = (1 − zi ) h0i + zi hi−1 + tanh(V h di ) where di is the coverage vector, ei is the new added extract gate, which is used to update di based on the words that has been translated. di is desig"
C16-1290,D15-1166,0,0.635888,"t the election to be held on January 30th next year would not be an end to serious violence in Iraq.” Figure 2. Our proposed model R EC ATT produced the correct reordering of the source words, and based on that generated a better translation. At position i in the target sentence, the attention model computes a matching score eij with match function α, for the previous decoder state hi−1 and each encoder state sj . eij = v > tanh (α(hi−1 , sj )) exp(eij ) wij = P k exp(eik ) We wrap this computation of weights as ALIGN: wi = ALIGN(hi−1 , sJ1 ) There are various match functions, as analyzed in (Luong et al., 2015). In our paper we use the sum match function α(hi−1 , sj ) = P W α hi−1 + U α sj . The weighted average of the encoder states sJ1 is calculated as the context ci = j wij sj . It is added to the input of each gate in the decoder, together with previous state hi−1 and previous target word embedding yi−1 : hi = RNN(hi−1 , yi−1 , ci ) 3 Problems of the Attention Mechanism Although attention modeling works well in finding translation correspondence between source and target words, there are still some issues that can be systematically identified, which fall into three categories: incorrect reorderi"
C16-1290,J03-1002,0,0.0421655,"e 1. BLEU scores w/o UNK replacement and the improvement from UNK replacement. RNNS EARCH R EC ATT F ERT D EC R EC ATT + F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test"
C16-1290,P02-1040,0,0.0972389,"Missing"
C16-1290,D15-1044,0,0.0274475,"Missing"
C16-1290,P16-5005,0,0.00482736,"and the improvement from UNK replacement. RNNS EARCH R EC ATT F ERT D EC R EC ATT + F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test set. Incorrect Reordering In Figur"
C16-1290,D15-1199,0,0.0707399,"always have full information about the previous alignments even when a longdistance jump happens, which makes the implicit distortion model much more flexible. 4.2 F ERT D EC To address the missing and repeated translation problems, we introduce fertility-conditioned decoder F ERT D EC. F ERT D EC uses a coverage vector 1 to represent the informationPof the source sentence that has not been translated. Initialized by the sum of source word embeddings Jj=1 xj and updated along the translation dynamically, our trainable coverage vector is different from the predefined condition vector used in (Wen et al., 2015). In order to leverage the coverage vector in decoding, we change the 1 The coverage vector in our work plays a similar role with the one used in beam search decoder (Koehn, 2004).There are two major two differences between them: 1. our coverage vector is used as a soft constraint instead of a hard constraint. 2. we tract untranslated words instead of translated words. 3086 decoding recurrent unit as follows: di = ei−1 di−1 ri = σ(W r yi−1 + U r hi−1 + V r di ) zi = σ(W z yi−1 + U z hi−1 + V z di ) ei = σ(W e yi−1 + U e hi−1 + V e di ) h0i = tanh(U (ri hi−1 ) + W yi−1 + V di ) hi = (1 − zi ) h"
C16-1309,P12-1056,0,0.0309786,"and has been extensively studied for the decades (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; Fung et al., 2005; He et al., 2007; Sayyadi et al., 2009; Zhao et al., 2012; Sayyadi and Raschid, 2013; Ge et al., 2015). They are based on either document- or keyword-based clustering, which usually suffer from either unawareness of time, high expensive computation cost or deviation of cluster centroids. In contrast, our approach is time-aware, centroid-aware and so efficient that it can be run on a large text stream. In addition, there is much work (Sakaki et al., 2010; Lee et al., 2011; Diao et al., 2012; Aggarwal and Subbian, 2012; Wang et al., 2013; Dong et al., 2015) studying event detection problem in social media. They usually use more or less social media features such as spatio-temporal information, which are not in the same setting with our task. 6 Conclusion and Future Work This paper proposes to use a novel text stream representation – Burst Information Networks to address the retrospective event detection challenge. Based on the BINet, we propose two fast centroid-aware event 3284 detection models that can effectively overcome the limitations of the previous event detection models"
C16-1309,P15-1056,1,0.851895,"(320.27s) 304.56s 716s 9.25s 3591.98s (1350.08s) BINet-ADM 2,562.17s (320.27s) 304.56s 716s 27.3s 3610.03s (1368.13s) Table 6: The running time of 4 parts of our BINet-based event detection approaches. The number in the round bracket is the running time of the model when it is run in 8-way parallel. 5 Related Work Event detection is one of the most popular research topics in recent years and has been extensively studied for the decades (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; Fung et al., 2005; He et al., 2007; Sayyadi et al., 2009; Zhao et al., 2012; Sayyadi and Raschid, 2013; Ge et al., 2015). They are based on either document- or keyword-based clustering, which usually suffer from either unawareness of time, high expensive computation cost or deviation of cluster centroids. In contrast, our approach is time-aware, centroid-aware and so efficient that it can be run on a large text stream. In addition, there is much work (Sakaki et al., 2010; Lee et al., 2011; Diao et al., 2012; Aggarwal and Subbian, 2012; Wang et al., 2013; Dong et al., 2015) studying event detection problem in social media. They usually use more or less social media features such as spatio-temporal information, w"
C16-1309,D16-1075,1,0.770827,"it is likely that the clusters obtained by the methods are not eventcentric, which has an adverse effect on the result, as illustrated in Figure 1. Figure 1: Deviation of cluster centroids: If clusters are not constructed around the centroid of the events (e.g., the dashline cluster is constructed around non-centroids such as people, kill and injure instead of earthquake or bombing), the performance will be adversely affected. To offer a better solution to event detection without the above limitations, we propose to use a novel text stream representation: Burst Information Networks (BINets) (Ge et al., 2016a; Ge et al., 2016b). In contrast to the keyword graph which is based on word co-occurrence, a BINet is constructed based on burst co-occurrence. In a BINet (Fig. 2), a node is a burst of one word, which can be represented by the word with one of its burst periods, and an edge between two nodes indicates how strongly they are related (i.e., how frequently they co-occur). Since the nodes in a BINet contains temporal information (e.g., burst period), a BINet is time-aware in which nodes in a community are both topically and temporally coherent. Hence, we can say each community in a BINet corresp"
C16-1309,P14-5010,0,0.00286061,"event communities: C = [E1 , E2 , ..., Ek ] 3: while kLk > 0 do 4: A ← L[0] (the first element in L) 5: E ← {A} ∪ {A0 |f (A, A0 ) > σA } 6: L←L−E 7: C.append(E) 8: end while 3281 4 Experiments and Evaluation We conduct experiments to evaluate the performance of our approach. We first evaluate our approach on the TDT4 dataset to compare other event detection approaches. Then, we apply our approach on a larger corpus (2009 – 2010 news corpus) to test its scalability and performance. For preprocessing, we remove stopwords and conduct lemmatization and name tagging using Stanford CoreNLP toolkit (Manning et al., 2014) before the construction of a BINet. 4.1 Evaluation on TDT4 The TDT4 collection is a well known dataset for comparing methods for event detection. The English part of the dataset includes approximately 29,000 news documents from news agencies such as CNN and BBC from October 2000 to Janurary 2001 (spanning 4 months), while only 1,884 documents1 are annotated to be related to 71 human identified events (topics). As the setting adopted by previous work (Li et al., 2005; Sayyadi and Raschid, 2013), we use the annotated subset as gold standard for evaluating the performance of our models. As most"
C16-1309,P12-2009,0,0.190351,"based on the BINet representation, which not only solve the centroid deviation problem but also are more efficient than traditional approaches. • We construct and release a dataset for evaluating event detection models on a large text stream during a long time span. 2 2.1 Burst Information Networks Burst Detection A word’s burst refers to a sharp increase of word frequency during a period. It usually indicates key information, important events or trending topics in a text stream as Figure 3 shows and is useful for many applications. In this paper, we detect a word’s burst using the method of Zhao et al. (2012) which is a variant of (Kleinberg, 2003) and models burst detection as a burst state sequence decoding problem where a word w’s burst state st (w) at time t could be 1 or 0 to indicate if the word bursts or not at t. Specially, if a word w bursts at every time epoch during a period, we call this period a burst period of w and w has a burst during this period. In Figure 3, earthquake has 2 burst periods (i.e., Jan 12 - Jan 31, and Feb 27 - Mar 7), which correspond to two famous earthquake events (i.e., 2010 Haiti earthquake and 2010 Chile earthquake). 3277 … government police aid donation Haiti"
C94-2153,2002.tmi-tutorials.1,0,0.0520831,"Missing"
C94-2153,1993.tmi-1.17,0,0.0256132,"Missing"
C94-2153,1983.tc-1.13,0,0.0639107,"Missing"
C94-2153,1993.tmi-1.28,0,0.0429899,"Missing"
C94-2153,J92-4001,0,0.0381426,"Missing"
C94-2153,W93-0312,1,0.773869,"Missing"
C94-2153,1993.tmi-1.25,0,0.0915548,"Missing"
C94-2153,1993.tmi-1.5,0,\N,Missing
D07-1019,P00-1037,0,0.721507,"ve been applied to deal with non-word errors and real-word errors respectively. Non-word error spelling correction is focused on the task of generating and ranking a list of possible spelling corrections for each word not existing in a spelling lexicon. Traditionally candidate ranking is based on manually tuned scores such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of natural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon, but recently (Ahmad and Kondrak, 2005) demonstrated that it is also possible to learn such models automatically from query logs wi"
D07-1019,P06-1129,1,0.813583,"also referred to be context sensitive spelling correction (CSSC), which tries to detect incorrect usage of valid words in certain contexts. Using a pre-defined confusion set is a common strategy for this task, such as in the work of (Golding and Roth, 1996) and (Mangu and Brill, 1997). Opposite to non-word spelling correction, in this direction only contextual evidences were taken into account for modeling by assuming all spelling similarities are equal. The complexity of query spelling correction task requires the combination of these types of evidence, as done in (Cucerzan and Brill, 2004; Li et al., 2006). One important contribution of our work is that we use web search results as extended contextual information beyond query strings by taking advantage of application specific knowledge. Although the information used in our methods can all be accessed in a search engine’s web archive, such a strategy involves web-scale data processing which is a big engineering challenge, while our method is a light-weight solution to this issue. 3 Motivation When a spelling correction model tries to make a decision whether to make a suggestion c to a query q, it generally needs to leverage two types of evidenc"
D07-1019,C04-1120,0,0.0310935,"Missing"
D07-1019,P02-1019,0,0.37831,"res such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of natural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon, but recently (Ahmad and Kondrak, 2005) demonstrated that it is also possible to learn such models automatically from query logs with the EM algorithm, which is similar to work of (Martin, 2004), learning from a very large corpus of raw text for removing non-word spelling errors in large corpus. All the work for non-word spelling correction focused on the current word itself without taking into account contextual information. Real-word spelling co"
D07-1019,C90-2036,0,\N,Missing
D07-1019,J96-1002,0,\N,Missing
D07-1019,H05-1120,0,\N,Missing
D07-1019,W04-3238,0,\N,Missing
D07-1035,W06-1650,0,0.871179,"l to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews. Some shopping sites already provide a function of assessing the quality of reviews. For example, Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. However, according to our survey in Section 3, users’ votes at Amazon have three kinds of biases as follows: (1) imbalance vote bias, (2) winner circle bias, and (3) early bird bias. Existing studies (Kim et al, 2006; Zhang and Varadarajan, 2006) used these users’ votes for training ranking models to assess the quality of reviews, which therefore are subject to these biases. In this paper, we demonstrate the aforementioned biases and define a standard specification to measure the quality of product reviews. We then manually annotate a set of ground-truth with real world product review data conforming to the specification. To automatically detect low-quality product reviews, we propose a classification-based approach learned from the annotated ground-truth. The proposed approach explores three aspects of p"
D07-1035,P98-2127,0,0.02747,"Missing"
D07-1035,P04-1035,0,0.0233245,"developing a specification on the quality of reviews and building a ground-truth according to the specification. 2.2 Mining Opinions from Reviews One area of research on opinion mining from product reviews is to judge whether a review expresses a positive or a negative opinion. For example, Turney (2006) presented a simple unsupervised learning algorithm in judging reviews as “thumbs up” (recommended) or “thumbs down” (not recommended). Pang et al (2002) considered the same problem and presented a set of supervised machine learning approaches to it. For other work see also Dave et al. (2003), Pang and Lee (2004, 2005). Another area of research on opinion mining is to extract and summarize users’ opinions from product reviews (Hu and Liu, 2004; Liu et al., 2005; Popescu and Etzioni, 2005). Typically, a sentence or a text segment in the reviews is treated as the basic unit. The polarity of users’ sentiments on a product feature in each unit is extracted. Then the aggregation of the polarities of individual senti335 Quality of Product Reviews In this section, we will first show three biases of users’ votes observed on Amazon, and then present our specification on the quality of product reviews. 3.1 Ama"
D07-1035,P05-1015,0,0.0925453,"Missing"
D07-1035,W02-1011,0,0.0260208,"oduct reviews, a set of specifications for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews. 1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews (Pang, et al, 2002; Liu, et al, 2004; Popescu and Etzioni, 2005). However, due to the lack of editorial and quality control, reviews on products vary greatly in quality. Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews. Some shopping sites already provide a function of assessing the quality of reviews. For example, Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. However, according to our surve"
D07-1035,H05-1043,0,0.613796,"s for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews. 1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews (Pang, et al, 2002; Liu, et al, 2004; Popescu and Etzioni, 2005). However, due to the lack of editorial and quality control, reviews on products vary greatly in quality. Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews. Some shopping sites already provide a function of assessing the quality of reviews. For example, Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. However, according to our survey in Section 3, users’ votes at Amazon have th"
D07-1035,H05-2017,0,\N,Missing
D07-1035,P02-1053,0,\N,Missing
D07-1035,C98-2122,0,\N,Missing
D07-1056,J04-4002,0,0.328585,"ordering and the latter with the longer distance reordering as global reordering. Phrase-based SMT system can effectively capture the local word reordering information which is common enough to be observed in training data. But it is hard to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to"
D07-1056,N03-1017,0,0.22019,"s can compensate for the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 1 IP NP VP DNP ADVP VP NP DEG AD VV 大幅 升值 NN 的 欧元 the significant appreciation of the Euro Figure 1: A reordering example Introduction In the last decade, statistical machine translation (SMT) has been widely studied and achieved good translation results. Two kinds of SMT system have been developed, one is phrase-based SMT and the other is syntax-based SMT. In phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words. Then source phrases are translated into target phrases respectively according to knowledge usually learned from bilingual parallel corpus. FiAs studied in previous SMT projects, language model, translation model and reordering model are the three major components in current SMT systems. Due to the difference between the source and target languages, the order of target phrases in the target sentence may differ from the order of source phrases in the source sentence. To make the translation res"
D07-1056,koen-2004-pharaoh,0,0.243773,"the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 1 IP NP VP DNP ADVP VP NP DEG AD VV 大幅 升值 NN 的 欧元 the significant appreciation of the Euro Figure 1: A reordering example Introduction In the last decade, statistical machine translation (SMT) has been widely studied and achieved good translation results. Two kinds of SMT system have been developed, one is phrase-based SMT and the other is syntax-based SMT. In phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words. Then source phrases are translated into target phrases respectively according to knowledge usually learned from bilingual parallel corpus. FiAs studied in previous SMT projects, language model, translation model and reordering model are the three major components in current SMT systems. Due to the difference between the source and target languages, the order of target phrases in the target sentence may differ from the order of source phrases in the source sentence. To make the translation results be closer"
D07-1056,H05-1021,0,0.313844,"Missing"
D07-1056,P06-1077,0,0.143706,"T systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relation is incorporated into the SMT system to strengthen the ability to handle global phrase reordering. Our method is different from previous syntax-based SMT systems in which the translation process was modeled based on specific syntactic structures, either phrase structures or dependency relatio"
D07-1056,P05-1034,0,0.375787,"e between the root node of T and the root node of the maximum sub-tree which exactly covers fi. For example, in Figure 1 the phrase “大幅” has the maximum sub-tree rooting at ADJP and its height is 3. The height of phrase “ 的 ” is 4 since its maximum sub-tree roots at 535 ADBP instead of AD. If two adjacent phrases have the same height, we regard them as peer phrases. In our model, we make use of bilingual phrases as well, which refer to source-target aligned phrase pairs extracted using the same criterion as most phrase-based systems (Och and Ney, 2004). 3.2 Model Similar to the work in Chiang (2005), our translation model can be formulated as a weighted synchronous context free grammar derivation process. Let D be a derivation that generates a bilingual sentence pair f, e, in which f is the given source sentence, the statistical model that is used to predict the translation probability p(e|f) is defined over Ds as follows: ? ? ? ∝ ? ? ∝ ??? ? ? ?? ?? ? → ?, ? × ? ?→?,?∈? ?? where plm(e) is the language model, i(X ,) is a feature function defined over the derivation rule X,, and i is its weight. Although theoretically it is ideal for translation reorder modeling by const"
D07-1056,J03-1002,0,0.0108535,"Missing"
D07-1056,P96-1021,0,0.649154,"global reordering. Phrase-based SMT system can effectively capture the local word reordering information which is common enough to be observed in training data. But it is hard to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we pr"
D07-1056,P06-1066,0,0.799337,"used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relation is incorporated into the SMT system to strengthen the ability to hand"
D07-1056,P01-1067,0,0.4941,"Missing"
D07-1056,P05-1033,0,0.853395,"rd to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relati"
D07-1056,C04-1030,0,0.0617,"eign linguistic other models such as language model. In our system, we combine the parse trees gen- phrases can also be formed into two valid adjacent erated respectively by Stanford parser (Klein, 2003) phrase according to constraints proposed in the and a dependency parser developed by (Zhou, phrase extraction algorithm by Och (2003a), they 2000). Compared with the Stanford parser, the de- will be extracted as a reordering training sample. pendency parser only conducts shallow syntactic Finally, the ME modeling toolkit developed by analysis. It is powerful to identify the base NPs and Zhang (2004) is used to train the reordering model base VPs and their dependencies. Additionally, over the extracted samples. dependency parser runs much faster. For example, it took about three minutes for the dependency parser to parse one thousand sentences with aver4.2 Combination of Parse Trees 538 7 Experimental Results and Analysis We conducted our experiments on Chinese-toEnglish translation task of NIST MT-05 on a 3.0GHz system with 4G RAM memory. The bilingual training data comes from the FBIS corpus. The Xinhua news in GIGAWORD corpus is used to train a four-gram language model. The development"
D07-1056,P03-1021,0,0.134599,"tally twelve categories of features used to train the ME model. In fact, the probability of Rule (1) is just equal to the supplementary probability of Rule (2), and vice versa. For Rule (3)~(9), according to the syntactic structures, their application is determined since there is only one choice to complete reordering, which is similar to the “glue rules” in Chiang (2005). Due to the appearance of non-linguistic phrases, non-monotone phrase reordering is not allowed in these rules. We just assign these rules a constant score trained using our implementation of 537 Minimum Error Rate Training (Och, 2003b), which is 0.7 in our system. For Rule (10)~(12), they are also determined rules since there is no other optional rules competing with them. Constant score is simply assigned to them as well, which is 1.0 in our system. Fea. Description LS1 First word of first foreign phrase LS2 First word of second foreign phrase RS1 Last word of first foreign phrase RS2 Last word of second foreign phrase LT1 First word of first target phrase LT2 First word of second target phrase RT1 Last word of first target phrase RT2 Last word of second target phrase LPos POS of the node covering first foreign phrase RP"
D07-1056,W00-1212,1,\N,Missing
D07-1056,P03-1054,0,\N,Missing
D08-1053,P91-1022,0,0.435917,"Missing"
D08-1053,J93-2003,0,0.0436605,"s represented as a tree, and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment. Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as “Hansard”. With improved sentence alignment performance, web mining systems are able to acquire parallel sentences of higher quality from the web. 1 Introduction Sentence-aligned parallel bilingual corpora have been essential resources for statistical machine translation (Brown et al. 1993), and many other multi-lingual natural language processing applications. The task of aligning parallel sentences has received considerable attention since the renaissance of data driven machine translation in late 1980s. During the past decades, a number of methods have been proposed to address the sentence alignment problem. Although excellent performance was reported on clean corpora, they are less robust with presence of noise. A recent study by (Singh and Husain 2005) completed a systematic evaluation on different sentence aligners under various conditions. Their experiments showed that th"
D08-1053,W96-0201,0,0.0975502,"Missing"
D08-1053,1992.tmi-1.7,0,0.595243,"el 1 in a unified framework under a maximum likelihood criterion. To make it more robust on noisy text, they developed a background model to handle text deletions. To further improve sentence alignment accuracy and robustness, methods that make use of additional language or corpus specific information were developed. In Brown and Church’s lengthbased aligner, they assume prior alignment on some corpus specific anchor points to constrain and keep the Viterbi search on track. (Wu 1994) implemented a length-based model for ChineseEnglish with language specific lexical clues to improve accuracy. (Simard et al. 1992) used cognates, which only exists in closely related language pairs. (Chuang and Yeh 2005) exploited the statistically ordered matching of punctuation marks in two languages to achieve high accuracy sentence alignment. In their web parallel data mining system, (Chen and Nie 2000) used HTML tags in the same way as cognates in (Simard et al. 1992) for aligning Chinese-English parallel sentences. Tree based 507 alignment models have been successfully applied in machine translation (Wu 1997, Yamada & Knight 2001, Gildea 2003). 3 T he Stochastic T ree A lignment Model The structure of the HTML docu"
D08-1053,W05-0816,0,0.021938,". 1 Introduction Sentence-aligned parallel bilingual corpora have been essential resources for statistical machine translation (Brown et al. 1993), and many other multi-lingual natural language processing applications. The task of aligning parallel sentences has received considerable attention since the renaissance of data driven machine translation in late 1980s. During the past decades, a number of methods have been proposed to address the sentence alignment problem. Although excellent performance was reported on clean corpora, they are less robust with presence of noise. A recent study by (Singh and Husain 2005) completed a systematic evaluation on different sentence aligners under various conditions. Their experiments showed that the performance of sentence aligners are sensitive to properties of the text, such as format complexity (presence of elements other than text), structural distance (a scale from literal to free translation), the amount of noise (text deletions or preprocessing errors) and typological distance between languages. Their performance varies on different type of texts and they all demonstrate marked performance degradation over noisy data. The results suggest that there is curren"
D08-1053,P94-1012,0,0.163907,"Missing"
D08-1053,J93-1004,0,\N,Missing
D08-1053,moore-2002-fast,0,\N,Missing
D08-1053,A00-1004,0,\N,Missing
D08-1053,J03-3002,0,\N,Missing
D08-1053,P01-1067,0,\N,Missing
D08-1053,P03-1011,0,\N,Missing
D08-1053,O05-2005,0,\N,Missing
D08-1053,J97-3002,0,\N,Missing
D08-1053,P93-1002,0,\N,Missing
D08-1053,1999.mtsummit-1.79,0,\N,Missing
D09-1038,W98-1115,0,0.0439035,"or the SCFG learnt automatically from the training corpus. It can work with an efficient CKY-style binarizer to search for the lowest-cost binarization. We apply our method into a state-of-the-art string-to-tree SMT system. The experimental results show that our method outperforms the synchronous binarization method (Zhang et al., 2006) with over 0.8 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tr"
D09-1038,P05-1033,0,0.0773653,"owever, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtai"
D09-1038,J07-2003,0,0.862926,"method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004"
D09-1038,P06-1121,0,0.465965,"SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using th"
D09-1038,W07-0405,0,0.0851011,"oblem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP"
D09-1038,W04-3250,0,0.186026,"Missing"
D09-1038,W06-1606,0,0.331083,"2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP will be JJR R2 : S → NP 会 VP , NP will VP binarization G’ (R1) v11 : VP → V12 JJR , V12 JJR v12 : V12 → VB V13 , VB V13 v13 : V13 → NP 会 , NP will be (R2) v21 : S → V22 VP , v22 : V22 → NP 会 , rule bucket v11 3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1, binarizing an SCFG in a fixed (left-heavy) way (Zhang et al., 2006) may lead to a large n"
D09-1038,H05-1101,0,0.0456339,"h it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using the synchronous binarization method (Zhang et al., 2006) as follows: VP → V1 JJR , V1 → VB V2 , V2 → NP 会 , V1 JJR VB V2 NP will be This binarization is shown with the solid lines as binarization (a) in Figure 1."
D09-1038,D08-1018,0,0.0770688,"NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2"
D09-1038,D07-1078,0,0.038172,"r development data set comes from NIST2003 evaluation data in which the sentences of more than 20 words are excluded to speed up the Minimum Error Rate Training (MERT). The test data sets are the NIST evaluation sets of 2005 and 2008. Our string-to-tree SMT system is built based on the work of (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the training corpus, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Before the rule extraction, we also binarize the parse trees on the English side using Wang et al. (2007) „s method to increase the coverage of GHKM and SPMT rules. There are totally 4.26M rules after the low frequency rules are filtered out. The pruning strategy is similar to the cube pruning described in (Chiang, 2007). To achieve acceptable translation speed, the beam size is set to 50 by default. The baseline system is based on the synchronous binarization (Zhang et al., 2006). 4.2 Binarization Schemes Besides the baseline (Zhang et al., 2006) and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison. In this paper, the"
D09-1038,N06-1033,0,0.250451,"ing polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a leftheavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decod"
D09-1038,N04-1035,0,\N,Missing
D09-1038,N06-1022,0,\N,Missing
D09-1114,D08-1011,0,0.0122079,"mbination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translat"
D09-1114,2008.amta-srw.3,0,0.173771,". As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment betw"
D09-1114,P05-1033,0,0.863722,"e principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level co"
D09-1114,C08-1005,0,0.0133609,"el or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diver"
D09-1114,N07-2015,0,0.183044,"Missing"
D09-1114,P05-3026,0,0.0288038,"pts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models a"
D09-1114,W04-3250,0,0.482397,"typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, di"
D09-1114,P09-1066,1,0.820368,"re to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version"
D09-1114,2006.amta-papers.11,0,0.0160705,"ization project settles in Zhoushan China 's largest desalination project in Zhoushan China 's largest sea water desalination project in Zhoushan Chinese 海水 淡化 海水 淡化 English desalination sea water desalination ? ?? 0.4000 0.1748 0.0923 Table 2: Parameters of related phrases for examples in Table 1. The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word 海水 (sea water) is missing in the output. This can be explained with the data shown in Table 2. Because of noises and word alignment error"
D09-1114,P06-1077,0,0.0298112,"e sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypot"
D09-1114,W06-1606,0,0.0444251,"systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original o"
D09-1114,D07-1105,0,0.0609944,"multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination. In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard l"
D09-1114,E06-1005,0,0.0510437,"multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand a"
D09-1114,P02-1038,0,0.183819,", for a specific sentence some individual systems could generate better translations. It is expected that employing an ensemble of subspace-based systems and making use of consensus between them will outperform the baseline system. 3 Feature Subspace Method for SMT System Ensemble Construction In this section, we will present in detail the method for systematically deriving SMT systems from a standard linear SMT model based on feature subspaces for system combination. 3.1 SMT System Ensemble Generation Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). Let ? (?, ?) be a feature function, and ?? be its weight, an SMT model ? can be formally written as: ? ∗ = argmax ? ?? ? (?, ?) (1) ? Noticing that Equation (1) is a general formulation independent of any specific features, technically for any subset of features used in ?, a new SMT system can be constructed based on it, which we call a sub-system. Next we will use Ω to denote the full feature space defined by the entire set of features used in ?, and ? ⊆ Ω is a feature subset that belongs to ?(Ω), the power set of Ω. The derived subsystem based on subset ? ⊆ Ω is denoted by ?? . Although"
D09-1114,P03-1021,0,0.0402856,"occurrences of n-grams of ? in ? ′ : ?? ?, ? ′ = ? −?+1 ?=1 ?(???+?−1 , ? ′ ) ?− (?, ℋ ? ) = ( ? − ? + 1 − ?? (?, ? ′ )) ? ′ ∈ℋ ? ,? ′ ≠? (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2? + 2 if ? orders of n-gram are to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. B"
D09-1114,J03-1002,0,0.00495516,"∙,∙) is the indicator function ? ???+?−1 , ? ′ is 1 when the n-gram ???+?−1 appears in ? ′ , otherwise it is 0. In order to give the combination model an opportunity to penalize long but inaccurate transla1099 Data set #Sentences MT03 (dev) 919 MT04 (test) 1,788 MT05 (test) 1,082 #Words 23,782 47,762 29,258 Table 3: Data set statistics. We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement. The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Gigaword corpus version 3. In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5gram one without retraining. 4.2 System Description Theoretically our method is applicable to all linear model –based SMT systems. In our experimen"
D09-1114,J04-4002,0,0.0458893,"he baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combinatio"
D09-1114,N07-1029,0,0.0865499,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,P07-1040,0,0.17902,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,W08-0329,0,0.0169325,"ranslations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an"
D09-1114,P06-1066,0,0.0451678,"veloped systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). All bilingual data are used to extract phrases up to length 3 on the source side. In following experiments, we only consider removing common features shared by both baseline systems for feature subspace generation. Rule penalty feature and lexicalized reordering feature, which are particular to SYS1 and SYS2, are not used. We list the features in consideration as follows:  PEF and PFE: phrase translation probabilities ? ? ? and ? ? ?  PEFLEX and PFELEX: lexical weights ???? ? ? and ???? ? ?  PP: phrase penalty  WP: word penalty  BLP: bi-lexicon pair counting how many entries of a conven"
D09-1114,2005.eamt-1.20,0,\N,Missing
D10-1104,C94-1002,0,0.0461183,"Missing"
D10-1104,P06-1032,0,0.663855,"n complete “I have __ a telephone call.”, where the usage context can be represented as “made/received a telephone call”; however, in “I have __ a telephone call from my boss”, the prepositional phrase “from my boss” becomes a critical part of the context, which now cannot be described by n-gram features, resulting in only “received” being suitable. Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. Yi et al. (2008) introduced an unsupervised web-based proofing method for correcting verb-noun collocation errors. Brockett et al. (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. None of their methods incorporated semantic information. 1068 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics Unlike the other papers, we derive features from the output of an SRL (Màrquez, 2009) system to explicitly model verb usage context. SRL is generally understood as the task of identifying the arguments of a given verb and assigning them sema"
D10-1104,W02-1001,0,0.148863,"”, which is unrelated to the usage of “watch” in this case, can be easily excluded from the usage context. (iii) Research on SRL has made great achievements, including humanannotated training corpora and state-of-the-art systems, which can be directly leveraged. Taking an English sentence as input, our method first generates correction candidates by replacing each verb with verbs in its pre-defined confusion set; then for every candidate, it extracts SRLderived features; finally our method scores every candidate using a linear function trained by the generalized perceptron learning algorithm (Collins, 2002) and selects the best candidate as output. Experimental results show that SRL-derived features are effective in verb selection, but we also observe that noise in SRL output adversely increases feature space dimensions and the number of false suggestions. To alleviate this issue, we use local features, e.g., n-gram-related features, and 1069 achieve state-of-the-art performance when all features are integrated. Our contributions can be summarized as follows: 1. We propose to exploit SRL-derived features to explicitly model verb usage context. 2. We propose to use the generalized perceptron fram"
D10-1104,I08-1059,0,0.0738982,"Missing"
D10-1104,P98-1085,0,0.765249,"Missing"
D10-1104,P08-1021,0,0.0694838,"Missing"
D10-1104,P09-5003,0,0.0153128,"to represent verb usage context. Yi et al. (2008) introduced an unsupervised web-based proofing method for correcting verb-noun collocation errors. Brockett et al. (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. None of their methods incorporated semantic information. 1068 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics Unlike the other papers, we derive features from the output of an SRL (Màrquez, 2009) system to explicitly model verb usage context. SRL is generally understood as the task of identifying the arguments of a given verb and assigning them semantic labels describing the roles they play. For example, given a sentence “I want to watch TV tonight” and the target predicate “watch”, the output of SRL will be something like “I [A0] want to watch [target predicate] TV [A1] tonight [AM-TMP].”, meaning that the action “watch” is conducted by the agent “I”, on the patient “TV”, and the action happens “tonight”. We believe that SRL results are excellent features for characterizing verb usag"
D10-1104,W04-2609,0,0.0344703,"features to explicitly model verb usage context. 2. We propose to use the generalized perceptron framework to integrate SRL-derived (and other) features and achieve state-ofthe-art performance on both in-domain and out-of-domain test sets. Our paper is organized as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL er"
D10-1104,C04-1100,0,0.0186396,"-art performance on both in-domain and out-of-domain test sets. Our paper is organized as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SM"
D10-1104,J04-4002,0,0.0467734,"may get interference from their mother tongue (Liu et al., 2000). For example, some Chinese people mistakenly say “see newspaper”, partially because the translation of “see” co-occurs with “newspaper” in Chinese. Therefore English verbs in the dictionary sharing more than two Chinese meanings are collected. For example, “see” and “read” are in a confusion set because they share the meanings of both “看” (“to see”, “to read”) and “领会” (“to grasp”) in Chinese. 3. An SMT translation table. We extract paraphrasing verb expressions from a phrasal SMT translation table learnt from parallel corpora (Och and Ney, 2004). This may help us use the implicit semantics of verbs that SMT can capture but a dictionary cannot, such as the fact that the verb Note that verbs in any confusion set that we are not interested in are dropped, and that the verb itself is included in its own confusion set. We leave it to our future work to automatically construct verb confusions. The verb usage context1 refers to its surrounding text, which influences the way one understands the expression. Intuitively, verb usage context can take the form of a collocation, e.g., “watch … TV” in “I saw [watched] TV yesterday.” ; it can also s"
D10-1104,W08-2125,0,0.170307,"of correction candidates, and Scores  is the linear model trained by the perceptron learning algorithm, which will be discussed in section 3.4. ' We call every target verb in s a checkpoint. For example, “sees” is a checkpoint in “Jane sees TV every day.”. Correction candidates are generated by replacing each checkpoint with its confusions. Table 1 shows a sentence with one checkpoint and the corresponding correction candidates. Input Candidates Jane sees TV every day. Jane watches TV every day. Jane looks TV every day. … Table 1. Correction candidate list. One state-of-the-art SRL system (Riedel and Meza-Ruiz, 2008) is then utilized to extract predicate-argument structures for each verb in the input, as illustrated in Table 2. Semantic features are generated by combining the predicate with each of its arguments; e.g., “watches_A0_Jane”, “sees_A0_Jane”, “watches_A1_TV” and “sees_A1_TV” are semantic fea1070 tures derived from the semantic roles listed in Table 2. Sentence Jane sees TV every day Semantic roles Predicate: sees; A0: Jane; A1: TV; Jane watches TV every Predicate: watches; day A0: Jane; A1: TV; Table 2. Examples of SRL outputs. At the training stage, each sentence is labeled by the SRL system."
D10-1104,P03-1002,0,0.038669,"ed as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SMT training. The SMT approach on the artificial data set achieves encouraging"
D10-1104,C08-1109,0,0.126146,"Missing"
D10-1104,U06-1020,0,0.0298304,"d perceptron framework to integrate SRL-derived (and other) features and achieve state-ofthe-art performance on both in-domain and out-of-domain test sets. Our paper is organized as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-qua"
D10-1104,I08-2082,0,0.10304,"ormation extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SMT training. The SMT approach on the artificial data set achieves encouraging results for correcting countability errors. Yi et al. (2008) use web frequency counts to identify and correct determiner and verb-noun collocation errors. Compared with these methods, our approach explicitly models verb usage context by leveraging the SRL output. The SRL-based semantic features are integrated, along with the local features, into the generalized perceptron model. 3 Our Approach Our method can be regarded as a pipeline consisting of three steps. Given as input an English sentence written by ESL learners, the system first checks every verb and generates correction candidates by replacing each verb with its confusion set. Then a feature ve"
D10-1104,P00-1067,1,\N,Missing
D10-1104,C98-1082,0,\N,Missing
D12-1041,W05-0823,0,0.0512393,"Missing"
D12-1041,P11-1103,0,0.0197714,"research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) t"
D12-1041,2003.mtsummit-papers.6,0,0.0464169,"gnments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be all"
D12-1041,P05-1066,0,0.0424994,"Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are"
D12-1041,D11-1018,0,0.0476346,"03) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason o"
D12-1041,2010.amta-papers.22,0,0.0348025,"source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules fo"
D12-1041,P06-1097,0,0.0289981,",e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based"
D12-1041,C10-1043,0,0.0217631,". It can provide an averaged 1.2 BLEU improvements on these three evaluation data sets. BLEU BTG FDT-TM FDT-DM FDT-SLM FDT-RSM FDT-ALL PRO dev 20.60 21.21 21.13 20.84 21.07 21.83 21.89 test-1 20.27 20.71(+0.44) 20.79(+0.52) 20.50(+0.23) 20.75(+0.48) 21.34(+1.07) 21.81 test-2 13.15 13.98(+0.83) 14.25(+1.10) 13.36(+0.21) 13.59(+0.44) 14.46(+1.31) 14.69 Table 1: FDT-based model training on E-J task. Pre-reordering (PRO) is often used on language pairs, e.g. English and Japanese, with very different word orders. So we compare our method with PRO as well. We re-implement the PRO method proposed by Genzel (2010) and show its results in Table 1. On dev and test-2, FDT-ALL performs comparable to PRO, with no syntactic information needed at all. 5.4 Translation Quality on C-E Task We then evaluate the effectiveness of our FDT-based model training approach on C-E translation task, and present evaluation results in Table 2, from which we can see significant improvements as well. BLEU BTG FDT-TM FDT-DM FDT-SLM FDT-RSM FDT-ALL MT03 38.73 39.14 39.27 38.97 39.06 39.59 MT05 38.01 38.31(+0.30) 38.56(+0.55) 38.22(+0.21) 38.33(+0.32) 38.72(+0.71) MT08 23.78 24.30(+0.52) 24.50(+0.72) 24.04(+0.26) 24.13(+0.35) 24."
D12-1041,W05-1506,0,0.0391167,"se-based SMT system in Section 3. 2.1 Generation We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps: 1. Train component models needed for a specific SMT paradigm M based on training corpus C; 2. Perform MERT on the development data set to obtain a set of optimized feature weights; 3. For each {f, e} ∈ C, translate f into accurate e based on M, component models trained in step 1, and feature weights optimized in step 2; 4. For each {f, e} ∈ C, output the hypergraph (Huang and Chiang, 2005) H(f, e) generated in step 3 as its FDT space. In step 3: (1) all partial hypotheses that do not match any sequence in e will be discarded; (2) derivations covering identical source and target words but with different alignments will be kept as different partial candidates, as they can produce different FDTs for 446 the same sentence pair. For each {f, e}, the probability of each G ∈ H(f, e) is computed as: p(G|H(f, e)) = ∑ exp{ψ(G)} ′ G ′ ∈H(f,e) exp{ψ(G )} (1) where ψ(G) is the FD model score assigned to G. For each sentence pair, different alignment candidates can be induced from its differ"
D12-1041,C10-1071,0,0.0183537,"ced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlangua"
D12-1041,D09-1106,0,0.0143278,"d [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. 1 2 448 Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1 , ..., An } from the top n FDTs Ω = {G1 , ..., Gn } ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: ∑ exp{ i λi hi (Gk )} ∑ (6) p(Ak |Gk ) = ∑ Gk′ ∈Ω exp{ i λi hi (Gk′ )} ∑ where i λi hi (Gk ) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, instead of the sum o"
D12-1041,J10-3002,0,0.0188773,")} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Bi"
D12-1041,H05-1011,0,0.0316801,"n H(f, e) respectively. e), target-to-source translation probability • h(f¯|¯ of a translation rule r = {f¯, e¯}. ∑ ¯ ¯) {f,e}∈C fracH(f,e) (f , e ∑ h(f¯|¯ e) = ∑ ¯′ ¯) (4) {f,e}∈C f¯′ fracH(f,e) (f , e • h# (r), smoothed usage count for translation rule r = {f¯, e¯} in the whole generation step. h# (r) = 1+e {− ∑ 1 {f,e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more"
D12-1041,P06-1065,0,0.0272001,"(r) = 1+e {− ∑ 1 {f,e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions,"
D12-1041,P08-1023,0,0.0541771,"Missing"
D12-1041,D08-1089,0,0.0525339,"Missing"
D12-1041,P03-1021,0,0.0690948,"s of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeN"
D12-1041,J04-4002,0,0.169793,"r approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality. 1 Ming Zhou Microsoft Research Asia Introduction Most of today’s SMT systems depends heavily on parallel corpora aligned at the word-level to train their different component models. However, such annotations do have their drawbacks in training. On one hand, word links predicted by automatic aligners such as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. T"
D12-1041,P02-1040,0,0.0855811,"lity, including LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. A 5gram LM is trained on the Xinhua portion of LDC English Gigaword Version 3.0. NIST 2004 (MT04) data set is used as dev set, and evaluation results are measured on NIST 2005 (MT05) and NIST 2008 (MT08) data sets. In all evaluation data sets, each source sentence has four reference translations. Default word alignments for both SMT tasks are performed by GIZA++ with the intersect-diag-grow refinement. Translation quality is measured in terms of case-insensitive BLEU (Papineni et al., 2002) and reported in percentage numbers. 5.2 Baseline System The phrase-based SMT system proposed by Xiong et al. (2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. The maximum lengths for the source and target phrases are 5 and 7 on E-J task, and 3 and 5 on C-E task. The beam size is set to 20. 5.3 Translation Quality on E-J Task We first evaluate the effectiveness of our FDT-based model training approach on E-J translation task, and present evaluation results in Table 1, in which BTG denot"
D12-1041,N06-1002,0,0.0199934,"igned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side o"
D12-1041,2008.amta-srw.6,0,0.0202855,"dels (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al"
D12-1041,W11-2102,0,0.0123739,"mmon practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction"
D12-1041,P11-1086,0,0.020727,"e words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side of bilingual data is used to train a 4-gram LM. The development set (dev) which contains 2,000 sentences is used to optimize"
D12-1041,2008.amta-papers.18,0,0.0141752,"e source spans [i, m) and [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. 1 2 448 Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1 , ..., An } from the top n FDTs Ω = {G1 , ..., Gn } ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: ∑ exp{ i λi hi (Gk )} ∑ (6) p(Ak |Gk ) = ∑ Gk′ ∈Ω exp{ i λi hi (Gk′ )} ∑ where i λi hi (Gk ) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, i"
D12-1041,P10-1049,0,0.0638577,"word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic informatio"
D12-1041,J97-3002,0,0.0295005,"orne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h# (r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, hr (G) ad"
D12-1041,P06-1066,0,0.416938,"sed loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h# (r) is used to penalize those long translation rules which tend to occur in only"
D12-1041,N09-1028,0,0.0155643,"ing/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-re"
D12-1041,W06-3108,0,0.0225538,"DTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future. because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training. 4.3 Distortion Models Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r2 ⇒ r3 ⇒ r1 . 4 Lexicalized distortion models (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to"
D12-1041,D08-1022,0,\N,Missing
D12-1041,J93-2003,0,\N,Missing
D12-1078,2008.amta-srw.1,0,0.406236,"LT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability. Different from the previous work of modifying tree structures with post-processing methods, our m"
D12-1078,D08-1092,0,0.169702,"pending on the order of application. Table 7 and 8 show the experiment results of combining FA-PR with IDSG. It can be seen that either way of the combination is better than using FA-PR or IDSG alone. Yet there is no significant difference between the two kinds of combination. The best result is a gain of more than 3 Bleu points on IWSLT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. Th"
D12-1078,P11-1003,0,0.0136755,"SMT models, and directly produce trees which are consistent with word alignment matrices. Instead of modifying the parse tree to improve machine translation performance, many methods were proposed to modify word alignment by taking syntactic tree into consideration, including deleting incorrect word alignment links by a discriminative model (Fossum et al., 2008), re-aligning sentence pairs using EM method with the rules extracted with initial alignment (Wang et al., 2010), and removing ambiguous alignment of functional words with constraint from chunk-level information during rule extraction (Wu et al., 2011). Unlike all these pursuits, to generate a consistent word alignment, our method modifies the popularly used IDG symmetrization method to make it suitable for string-to-tree rule extraction, and our method is much simpler and faster than the previous works. 6 Conclusion In this paper we have attempted to improve SSMT by reducing the errors introduced by the mutual independence between monolingual parser and word aligner. Our major contribution is the strategies of re-training parser with the bilingual information in alignment matrices. Either of our proposals of targeted self-training with fro"
D12-1078,P03-1021,0,0.185049,"Missing"
D12-1078,W08-0306,0,0.0913119,"ent and syntactic tree for the target sentence. All the nodes in gray are frontier nodes. Example (a) contains two error links (in dash line), and the syntactic tree for the target sentence of example (b) is wrong. structure by incorrect alignment links, as shown by the two dashed links in Figure 1(a). These two incorrect links hinder the extraction of a good minimal rule “毡房 ” and that of a good composed rule “牧民, 的 NP(DT(the), NN(herdsmen), POS(&apos;s)) ”. By and large, incorrect alignment links lead to translation rules that are large in size, few in number, and poor in generalization ability (Fossum et al, 2008). The second problem is parsing error, as shown in Figure 1(b). The incorrect POS tagging of the word “lectures"" causes a series of parsing errors, including the absence of the noun phrase “NP(NN(propaganda), NN(lectures))”. These parsing errors hinder the extraction of good rules, such as “ 宣 讲 NP(NN(propaganda), NN(lectures)) ”. Note that in Figure 1(a), the parse tree is correct, and the incorrect alignment links might be fixed if the aligner takes the parse tree into consideration. Similarly, in Figure 1(b) some parsing errors might be fixed if the parser takes into consideration the corre"
D12-1078,P06-1121,0,0.207786,"aligner does not consider the syntax information of both languages, and the output links may violate syntactic correspondence. That is, some SL words yielded by a SL parse tree node may not be traced to, via alignment links, some TL words with legitimate syntactic structure. On the other hand, parser design is a monolingual activity and its impact on MT is not well studied (Ambati, 2008). Many good translation rules may thus be filtered by a good monolingual parser. In this paper we will focus on the translation task from Chinese to English, and the string-to-tree SSMT model as elaborated in (Galley et al., 2006). There are two kinds of translation rules in this model, minimal rules, and composed rules, which are composition of minimal rules. The minimal rules are extracted from a special kind of nodes, known as frontier nodes, on TL parse tree. The concept of frontier node can be illustrated by Figure 1, which shows two partial bilingual sentences with the corresponding TL sub-trees and word alignment links. The TL words yielded by a TL parse node can be traced to the corresponding SL words through alignment links. In the diagram, each parse node is represented by a rectangle, showing the phrase labe"
D12-1078,N06-1031,0,0.168264,"there is no significant difference between the two kinds of combination. The best result is a gain of more than 3 Bleu points on IWSLT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as"
D12-1078,W04-3250,0,0.317444,"Missing"
D12-1078,J10-2004,0,0.205983,"; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability. Different from the previous work of modifying tree structures with post-processing methods, our methods try to learn a suitable grammar for string-to-tree SMT models, and directly produce trees which are consistent with word alignment matrices. Instead of modifying the parse tree to"
D12-1078,J03-1002,0,0.00402198,"p2: Select the one which can generate the biggest frontier set:  step3: Add to , and repeat step 1, until no new link can be added. Like IDG, IDSG starts with all the links in and its main task is to add links selected from . IDSG is also subject to the constraints of IDG. The new criterion in link selection in IDSG is specified in Step 2. Given a parse tree of the TL side of the bilingual sentence, in each iteration IDSG considers the change of frontier set size caused by 858 PP 1 2-6 PP 3-6 1-2 Word Alignment Symmetrization The most widely used word aligners in MT, like HMM and IBM Models (Och and Ney, 2003), are directional aligners. Such aligner produces one set of alignment matrices for the SL-to-TL direction and another set for the TL-to-SL direction. Symmetrization refers to the combination of these two sets of alignment matrices. The most popular method of symmetrization is intersect-diag-grow (IDG). Given a bilingual sentence and its two alignment matrices and IDG starts with all the links in . Then IDG considers each link in in turn. A link is added if its addition does not make some phrase pairs overlap. Although IDG is simple and efficient, and has been shown to be effective in phrase-b"
D12-1078,P10-1049,0,0.118082,"ht be fixed if word aligner and parser are not mutually independent. In this paper, we emphasize more on the correction of parsing errors by exploiting alignment information. The general approach is to re-train a parser with parse trees which are the most consistent with alignment matrices. Our first strategy is to apply the idea of targeted selftraining (Katz-Brown et al., 2011) with the simple evaluation function of frontier set size. That is to re-train the parser with the parse trees which give rise to the largest number of frontier nodes. The second strategy is to apply forced alignment (Wuebker et al., 2010) to bilingual data and select the parse trees generated by our SSMT system for re-training the parser. Besides, although we do not invent a new word aligner exploiting syntactic information, we propose a new method to symmetrize the alignment matrices of two directions by taking parse tree into consideration. 2 Parser Re-training Strategies Most monolingual parsers used in SSMT are trained upon certain tree bank. That is, a parser is trained with the target of maximizing the agreement between its decision on syntactic structure and that decision in the human-annotated parse trees. As mentioned"
D12-1078,P02-1040,0,0.0863916,"Missing"
D12-1078,N07-1051,0,0.0726623,"Missing"
D12-1078,D11-1017,0,\N,Missing
D12-1078,P07-1003,0,\N,Missing
D12-1078,P06-2014,0,\N,Missing
D12-1078,D09-1024,0,\N,Missing
D13-1041,E06-1002,0,0.851221,"tion,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, t"
D13-1041,D07-1074,0,0.971336,"e false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives the model enough flexibility and expressivity. It can include any features that describe the similarity or dissimilarity of context d and candidate entity e. They often perform well even on small training set, with carefullydesigned features. This category falls into the local approach as the decision processes for each mention are made independently (Ratinov et al., 2011). (Cucerzan, 2007) first suggests to optimize an objective function that is similar to the collective ap427 Wikipedians annotate entries in Wikipedia with category network. This valuable information generalizes entity-context correlation to category-context correlation. (Bunescu and Pasca, 2006) utilize category-word as features in their ranking model. (Kataria et al., 2011) employ a hierarchical topic model where each inner node in the hierarchy is a category. Both approaches must rely on pruned categories because the large number of noisy categories. We try to address this problem with recent advances of repr"
D13-1041,D11-1072,0,0.338593,"Missing"
D13-1041,D08-1017,0,0.0165384,"global ranker. The differences are that we use stacking to train the local ranker to handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009;"
D13-1041,P11-1138,0,0.060973,"machine readable format, ambiguous names must be resolved in order to tell which realworld entity the name refers to. The task of linking names to knowledge base is known as entity linking or disambiguation (Ji et al., 2011). The resulting text is populated with semantic rich links to knowledge base like Wikipedia, and ready for various downstream NLP applications. ∗ Corresponding author Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously (Han et al., 2011; Hoffart et al., 2011; Kulkarni et al., 2009; Ratinov et al., 2011). Collective approaches can improve performance when local evidence is not confident enough. They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches (Ratinov et al., 2011). However, collective inference processes are often expensive and involve an exponential search space. We propose a collective entity linking method based on stacking. Stacked generalization (Wolpert, 1992) is a powerful meta learning algorithm that uses two levels of learners. The predictions of the first learner are taken as"
D13-1041,P05-1044,0,0.0476092,"the mention “Romney” as an examFinally, the semantic relatedness measure of two entities ei ,ej is defined as the common in-links of ei and ej in Wikipedia (Milne and Witten, 2008; Han et al., 2011): 430 where W is learned with supervision like clickthrough data. Given training data {(qi , di )}, training is done by randomly sampling a negative target d− . The model optimizes W such that f (q, d+ ) > f (q, d− ). Thus, the training objective is to minimize the following margin-based loss function: ∑ max(0, 1 − f (q, d+ ) + f (q, d− )) (7) q,d+ ,d− which is also known as contrastive estimation (Smith and Eisner, 2005). W can become very large and inefficient when we have a big vocabulary size. This is addressed by replacing W with its low rank approximation: W = UT V + I (8) here, the identity term I is a trade-off between the latent space model and a vector space model. The gradient step is performed with Stochastic Gradient Descent (SGD): U ←U + λV (d+ − d− )q T , if 1 − f (q, d+ ) + f (q, d− ) > 0 (9) − T V ←V + λU q(d − d ) , + if 1 − f (q, d+ ) + f (q, d− ) > 0. (10) where λ is the learning rate. The query and document are not necessary real query and document. In our case, we treat our problem as: gi"
D13-1041,P11-1139,0,0.022453,"handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives"
D13-1041,N10-1072,0,0.661752,"aboratory of Computational Linguistics (Peking University) Ministry of Education,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning"
D13-1107,D11-1033,0,0.0801174,"Missing"
D13-1107,W06-1615,0,0.0632491,"ation problem over mixture models for SMT systems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each 1063 task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking o"
D13-1107,J07-2003,0,0.0972138,"nces for testing in each domain. The details are shown in Table 2. Domain Business Ent. Health Sci&Tech Sports Politics Train En Ch 30M 28M 25M 22M 23M 20M 28M 26M 19M 16M 28M 24M Dev En Ch 36K 35K 21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation"
D13-1107,P13-2061,1,0.817113,"Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled monolingual data, in numbers of documents and words (main content). ”M” refers to million and"
D13-1107,D08-1072,0,0.0445467,"omain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods"
D13-1107,W10-1757,0,0.0555944,"e Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general"
D13-1107,eck-etal-2004-language,0,0.18163,"g domains are similar. However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply the"
D13-1107,W07-0717,0,0.372942,"dels may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To levera"
D13-1107,W06-1607,0,0.0717034,"efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 R"
D13-1107,D10-1044,0,0.132836,"Missing"
D13-1107,P09-1098,1,0.83532,"addition, the target-side LMs were re-used in the SMT systems as features. As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled mono"
D13-1107,W07-0733,0,0.520345,"riety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we"
D13-1107,W04-3250,0,0.0374174,"++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison."
D13-1107,P06-1096,0,0.264664,"Missing"
D13-1107,D07-1036,0,0.13075,"Missing"
D13-1107,P10-2041,0,0.30299,"hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult t"
D13-1107,J03-1002,0,0.00587202,"21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We h"
D13-1107,P03-1021,0,0.0335982,"ser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 Results The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to G"
D13-1107,P02-1040,0,0.0869864,"rithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-t"
D13-1107,P12-1099,0,0.144964,"s often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptati"
D13-1107,E12-1055,0,0.187281,"lation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise"
D13-1107,P12-1002,0,0.446041,"ased on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM). Meanwhile, all the systems share a same general-domain TM and LM. These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework. With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well. By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time. Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way. Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline. Moreover, the MTL-based adaptation also outperforms the conventional individual 1056 adaptation approach towards each domain. The rest of the paper is organized as follows: The proposed approach is explain"
D13-1107,P07-1004,0,0.348402,"nd second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adapt"
D14-1054,D08-1083,0,0.0209071,"Sentiment Analysis Duyu Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contai"
D14-1054,P14-2009,1,0.766966,"hou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does"
D14-1054,P13-2087,0,0.0212728,"using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is"
D14-1054,P11-2008,0,0.131527,"Missing"
D14-1054,P11-1015,0,0.3622,"cial Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic ch"
D14-1054,D13-1171,0,0.046832,"Missing"
D14-1054,J11-2001,0,0.0176143,"notated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like e"
D14-1054,S13-2053,0,0.183839,"such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information. The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence. However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (Liu, 2012; Mohammad et al., 2013a), which are splitted by syntactic chunkers. Besides, a better approach would be to utilize the sentiment information to improve the segmentor. Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn. In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification. The framework is illustrated in FigIn this paper, we propose a joint segmentation and classification framework for sentiment analysis. Existin"
D14-1054,C14-1018,1,0.854768,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,N10-1120,0,0.161612,"Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, b"
D14-1054,P14-1146,1,0.670131,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,pak-paroubek-2010-twitter,0,0.0427636,"pically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like emoticons (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Zhao et al., 2012). Majority of existing approaches follow Pang et al. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learnin"
D14-1054,P10-1141,0,0.148379,"Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-w"
D14-1054,P02-1053,0,0.0058165,"the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corp"
D14-1054,W02-1011,0,0.0553792,"fication performance. The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations. Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods. 1 Introduction Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Majority of existing approaches follow Pang et al. (2002) and treat sen∗ This work was partly done when the first and fourth authors were visiting Microsoft Research. 477 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477–487, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics CG SC SEG SEG SC that is not bad -1 <+1,-1> NO 0.6 0.6 that is not bad that is not bad -1 <+1,-1> NO 0.4 0.4 Polarity: +1 that is not bad +1 <+1,+1> YES 2.3 2.3 that is not bad +1 <+1,+1> YES 1.6 1.6 Segmentations Polarity Update Rank Update Input Top K Figure 1: The joint segmentation and c"
D14-1054,P12-2018,0,0.141338,"l. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learning effective features to obtain better classification performance. On movie or product reviews, Wang and Manning (2012) present NBSVM, which trades-off • To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework. • We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013. 478 overview of the proposed joint segmentation and classification model (JSC) for sentiment analysis. The segmentation candidate generation model and the segmentation ranking model are described in Section 4. The details of the sentiment classific"
D14-1054,H05-1044,0,0.0828802,"Missing"
D14-1054,D11-1014,0,0.256554,"ytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a sepa"
D14-1054,D13-1016,0,0.0546336,"Missing"
D14-1054,D12-1110,0,0.0455462,"Missing"
D14-1054,D11-1016,0,0.0130106,"the document feature. On Twitter, Mohammad et al. (2013b) develop a state-of-the-art Twitter sentiment classifier in SemEval 2013, using a variety of sentiment lexicons and hand-crafted features. With the revival of deep learning (representation learning (Hinton and Salakhutdinov, 2006; Bengio et al., 2013; Jones, 2014)), more recent studies focus on learning the low-dimensional, dense and real-valued vector as text features for sentiment classification. Glorot et al. (2011) investigate Stacked Denoising Autoencoders to learn document vector for domain adaptation in sentiment classification. Yessenalina and Cardie (2011) represent each word as a matrix and compose words using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabili"
D14-1054,P14-1011,1,0.830375,"ords or phrases of variable length. Under this scenario, phrase embedding is highly suitable as it is capable to represent phrases with different length into a consistent distributed vector space (Mikolov et al., 2013). For each phrase, phrase embedding is a dense, real-valued and continuous vector. After the phrase embedding is trained, the nearest neighbors in the embedding space are favored to have similar grammatical usages and semantic meanings. The effectiveness of phrase embedding has been verified for building large-scale sentiment lexicon (Tang et al., 2014a) and machine translation (Zhang et al., 2014). We learn phrase embedding with Skip-Gram model (Mikolov et al., 2013), which is the state-of(2) k where φij is the segmentation score of Ωij ; sf eijk is the k-th segmentation feature of Ωij ; w and b are the parameters of the segmentation ranking model. During training, given a sentence si and its gold sentiment polarity polig , the optimization objec2 j∈Hi Segmentation-Specific Feature We empirically design four segmentation-specific features to reflect the information of each segmentation, as listed in Table 3. The objective of the segmentation ranking model is to assign a scalar to each"
D14-1054,D13-1170,0,0.203308,"ng Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal"
D14-1054,J13-3004,0,\N,Missing
D14-1054,P11-1016,1,\N,Missing
D14-1071,P14-1091,1,0.898601,"that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on th"
D14-1071,P11-1060,0,0.26728,"sentations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle"
D14-1071,P14-1133,0,0.0783746,"(Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c). Based on the components defined above, the paired relationships are described as foll"
D14-1071,D13-1160,0,0.426708,"results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007)"
D14-1071,D12-1104,0,0.00652345,"y types to the given context, which may solve the entity disambiguation problem in KB-QA; C-P can leverage the semantic overlap between question contexts (n-gram features) and logical predicates, which is important for mapping NL-questions to their corresponding predicates. 3.2 This section describes how we extract the semantic associated pairs of NLE-entries and KB-triples to learn the relational embeddings (Section 4.1). &lt;Relation Mention, Predicate> Pair (MP) Each relation mention denotes a lexical phrase of an existing KB-predicate. Following information extraction methods, such as PATTY (Nakashole et al., 2012), we extracted the &lt;relation mention, logical predicate> pairs from English W IKIPEDIA3 , which is closely connected to our KB, as follows: Given a KB-triple &lt;entitysubj , logical predicate, entityobj >, we extracted NLEentries &lt;entitysubj , relation mention, entityobj > where relation mention is the shortest path between entitysubj and entityobj in the dependency tree of sentences. The assumption is that any relation mention (m) in the NLE-entry containing such entity pairs that occurred in the KB-triple is likely to express the predicate (p) of that triple. With obtaining high-quality MP pai"
D14-1071,P13-1042,0,0.520324,"perties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zet"
D14-1071,D13-1136,0,0.100595,"igger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Y"
D14-1071,J90-1003,0,0.164555,"Missing"
D14-1071,P13-1158,0,0.395373,"space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins"
D14-1071,P14-1090,0,0.205332,"Missing"
D14-1071,D12-1069,0,0.140714,"Missing"
D14-1071,D13-1161,0,0.0295005,"th KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c). Based on the components defined above, the"
D15-1106,J08-1001,0,0.0939993,"Missing"
D15-1106,J97-3002,0,0.0402534,"of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we"
D15-1106,D14-1218,0,0.181084,"the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural network"
D15-1106,P06-1066,0,0.0156601,"3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we can find that the rerank system improves SMT performance consistently. For a single sentence without the"
D15-1106,P14-1140,1,0.746216,"Missing"
D15-1106,P13-1017,1,0.460566,"olov et al., 2010) uses a hidden layer which employs a real-valued vector recurrently as network’s input to keep as many history as possible. This makes RNNLM be able to extend for capturing history beyond a sentence. To prevent the potential exponential decay of the history, the history length in RNN can not be too long. Here we approximate the history information of previous sentences, p(Sk |S1 , S2 , ..., Sk−1 ), by the following: For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2 NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying"
D15-1106,P03-1021,0,0.0435395,"two Chinese sentence in one document together with their correct translation: 我 拍摄 过 的 冰山, 有些 冰 是 非常 年 轻 - - 几千 年 年龄 Some of the ice in the icebergs that I photograph is very young - - a couple thousand years old. 有些 冰 超过 十万 年 And some of the ice is over 100,000 years old. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT. For our reranking system, to score the translation of one sentence we need the translation results of all the previous sentences in the document. Our SMT decoder generates 10-best results of all the sentences of the documents and the rerank905 Chinese word “ 有 些” means “some” in English. But when it is used in parallelism sentences, it means “some of” instead of “some”. The traditional SMT system translates the italics part without considering the context. The translation result for this kind"
D15-1106,P02-1040,0,0.10845,"Missing"
D15-1106,D13-1170,0,0.00405147,"dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. Related work An attempt of introducing RNN into convolutional neural networ"
D15-1106,2012.eamt-1.60,0,\N,Missing
D16-1075,P15-1056,1,0.182234,"challenges for summarizing a text stream. First, a stream summarization model should be able to be aware of redundant information in the stream for avoiding generating redundant content in the summary; second, a stream summarization algorithm should be capable of analyzing text content on the stream level for identifying the most important information in the stream; third, a stream summarization model should be efficient, scalable and able to run in an online fashion because data size of a text stream is usually huge, and it is dynamic and updated every second. The previous approaches (e.g., (Ge et al., 2015b)) tend to cluster similar documents as event detection to avoid redundancy, rank the clusters based on their sizes and topical relevance to the reference summaries, and select one document from each cluster as representative documents. Due to the high time complexity of clustering models, their approaches usually run slowly and are not scalable. 785 To overcome the limitations, we propose Burst Information Networks (BINet) as a novel representation of a text stream. In a BINet (Figure 2), a node is a burst word (including entities) with the time span of one of its burst periods, and an edge"
D16-1075,P15-1155,0,0.0215125,"xt stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem in the big data age a"
D16-1075,P13-2099,1,0.841386,"on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem"
D16-1075,W04-1013,0,0.022973,"Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated by various approaches, annotated each entry based on the reference summary and the manually edited event chronicles on the web, and used precision@K to evaluate the quality of top K event entries in a stream summary instead of using ROUGE (Lin, 2004) because news stream summaries are eventcentric. In this paper, we adopt the same evaluation setting and use the same reference summaries and the annotations with our previous work (Ge et al., 2015b) to evaluate our summaries’ quality. For the event entries that are not in Ge et al. (2015b)’s annotations, we have 3 human judges annotate them according to the previous annotation guideline and consider an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseli"
D16-1075,P14-5010,0,0.00387579,"APW and XIN news stories in English Gigaword (Graff et al., 2003)) as a news stream. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which models word burst detection as a burst state decoding problem. In total, there are 140,557 documents in the dataset. Topic Disaster Sports Politics Military Comprehensive #Entry 35 19 8 14 85 #Entry in corpus 28 12 5 13 64 Table 2: The number of event entries in the reference summaries. The third column is the number of event entries excluding those events that do not appear in the corpus. We removed stopwords and used Stanford CoreNLP (Manning et al., 2014) to do lemmatization and named tagging, and built BINets on the news stream during 2009 and 2010 separately. On the 2009 news stream, there are 31,888 nodes and 833,313 edges while there are 32,997 nodes and 825,976 edges on the 2010 stream. Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated"
D16-1075,Q14-1015,0,0.0274957,"marization challenge. However, they studied the problem on a static timestamped corpus instead of on a dynamic text stream and their proposed pipeline-style approach cannot be applied on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select importan"
D16-1075,P12-2009,0,0.164471,"r an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseline randomly selects documents in the dataset as event entries. • N B: this baseline uses Naive Bayes to cluster documents for event detection and ranks the clusters based on the combination score of topical relevance and the event impact (i.e., event cluster size). The earliest documents in the topranked clusters are selected as entries. • B-H AC: similar to N B except that BurstVSM representation (Zhao et al., 2012) is used for event detection using Hierarchical Agglomerative Clustering algorithm. • TA HBM: similar to N B except that the stateof-the-art event detection model (TaHBM) proposed by Ge et al. (2015b) is used for event detection. • Ge et al. (2015b): the state-of-the-art stream summarization approach which used TaHBM to detect events and L2R model to rank events. Note that we did not compare with previous multidocument summarization models because the goal and setting of stream summarization are different from multi-document summarization, as Section 1 https://en.wikipedia.org/wiki/2009 Random"
D16-1075,N16-3015,0,\N,Missing
D16-1081,W14-1605,0,0.0311354,"lack ones are associated with clues. Compared with our riddle task, the clues in the CPs are derived from each question where the radicals in solution are derived from the metaphors in the riddles. Proverb (Littman et al., 2002) is the first system for the automatic resolution of CPs. Ernandes et al. (2005) utilize a web-search module to find sensible candidates to questions expressed in natural language and get the final answer by ranking the candidates. And the rule-based module and the dictionary module are mentioned in his work. The tree kernel is used to rerank the candidates proposed by Barlacchi et al. (2014) for automatic resolution of crossword puzzles. From another perspective, there are a few projects Offline Learning Riddle/Solution Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine tran"
D16-1081,C92-4176,0,0.132808,"Missing"
D16-1081,C08-1048,1,0.756656,"on Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine translation (SMT) framework is proposed to generate Chinese couplets and classic Chinese poetry (He et al., 2012; Zhou et al., 2009; Jiang and Zhou, 2008). Jiang and Zhou (2008) use a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several works using neural networ"
D16-1081,J03-1002,0,0.00555444,"d to extract the metaphors based on the phrase-radical alignments and rules. We exploit the phrase-radical alignments as to de848 scribe the simple metaphors, e.g. “千 里” aligns “马”, which aligns the phrase and the radical by the meaning. We employ a statistical framework with a word alignment algorithm to automatically mine phrase-radical metaphors from riddle dataset. Considering the alignment is often represented as the matching between successive words in the riddle and a radical in the solution, we propose two methods specifically to extract alignments. The first method in according with (Och and Ney, 2003) is described as follows. With a riddle description q and corresponding solution s, we tokenize the input riddle q to character as (w1 , w2 , . . . , wn ) and decompose the solution s into radicals as (r1 , r2 , . . . , rm ). We count all ([wi , wj ], rk )(i, j ∈ [1, n], k ∈ [1, m]) as alignments. The second method takes into account more structural information of characters. Let (w1 , w2 ) denote two successive characters in the riddle q. If w1 is a radical of w2 and the rest parts of w2 as r appear in the solution q, we strongly support that ((w1 , w2 ), r) is a alignment. It is identical if"
D16-1081,D14-1074,0,0.0239932,"a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several works using neural network to generate Chinese poems(Zhang and Lapata, 2014; Yi et al., 2016). Due to the limited data and strict rules, it is hard to transfer to the riddle generation. 3 Phrase-Radical Alignments and Rules The metaphor is one of the key components in both solving and generation. On the one hand we need to identify these metaphors since each of them aligns a radical in the final solution. On the other hand, we need to integrate these metaphors into the riddle descriptions to generate riddles. Thus, how to extract the metaphors of riddles becomes a big challenge in our task. Below we introduce our method to extract the metaphors based on the phrase-ra"
D16-1081,Y09-1006,1,0.90926,"rning Riddle/Solution Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine translation (SMT) framework is proposed to generate Chinese couplets and classic Chinese poetry (He et al., 2012; Zhou et al., 2009; Jiang and Zhou, 2008). Jiang and Zhou (2008) use a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several wo"
D16-1081,J92-1011,0,\N,Missing
D17-1007,D11-1072,0,0.100881,"Missing"
D17-1007,Q15-1023,0,0.0938631,"Missing"
D17-1007,P14-5010,0,0.00445278,"tities based on the sentence search instead of the common method using entity search. There are some issues in the original annotations because of the annotation regulation. First, entities in their own pages are usually not annotated. Thus we annotate these entities with matching between the text and the page title. Second, entities are usually annotated only in their first appearance. We annotate these entities if they are annotated in previous sentences in the page. Moreover, pronouns are widely used in Wikipedia sentences and are usually not annotated. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to do the coreference resolution. In addition, we use the content in the disambiguation page and the infobox. Although these two kinds of information may have incomplete grammatical structure, it contains enough context information for the sentence search in our task. We use the Wikipedia snapshot of May 1, 2016, which contains 4.45 million pages and 120 million sentences. We extract sentences that contain at least one anchor in the Wikipedia articles, and Our work is different from using search engines to generate candidates. We firstly propose to search Wikipedia sentences and take advantag"
D17-1007,D07-1074,0,0.211261,"lake Shelton]]. 2 Related Work Recognizing entity mentions in text and linking them to the corresponding entries helps to understand documents and queries. Most work uses the knowledge base including Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011) and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages. It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates. They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates. Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages. Then they disambiguate candidates by calculating the similarity between the contextual information and the document as well as category tags on Wikipedia pages. Milne and Witten (2008) generate candidates by gathering all n-grams in the document, and retaining those whose probability exceeds a low threshold. Then they define commonness and relatedness on the hyper-link structure of Wikipedia to disambiguate candidates. The work on linking entities in queries has been Table"
D17-1090,N16-1152,0,0.0232147,"shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then compute the question-to-generated question matching score QQ(Q, Qgen"
D17-1090,D13-1160,0,0.0746789,"sing recurrent neural network (RNN); Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation ("
D17-1090,P16-1049,1,0.835962,"assage, and [Gen] denotes the generated question of the passage. WikiQA QA QA+QG ly wrong question topic, or a partially right question topic. In the future, we plan to develop an independent question topic selection model for the question generation task. 9.3 As described in Section 7, we combine question generation into QA system for answer sentence selection task, and do evaluation on SQuAD, MS MARCO, and WikiQA. Evaluation results are shown in Table 5, 6, and 7, where QA denotes the result of our in-house implementation of a retrieval-based answer selection approach (DocChat) proposed by (Yan et al., 2016), QA+QG denotes result by combining question-to-generated question matching score with DocChat score. MAP 0.8843 0.8887 MRR 0.8915 0.8963 MAP 0.5131 0.5230 MRR 0.5195 0.5291 ACC@1 0.6540 0.6624 for CQA websites; while questions from the other datasets are labeled by crowd-sourcing. In order to explain these improvements, two datasets, WikiQG+ and WikiQG-, are built from WikiQA test set: given each document and its labeled question, we pair the question with its CORRECT answer sentence as a QA pair and add it to WikiQG+; we also pair the same question with a randomly selected WRONG answer sente"
D17-1090,J15-1001,0,0.0853699,"e input sentence into its corresponding Minimal Recursion Semantics (MRS) representation, and then generates a question guided by the English Resource Grammar that includes a large scale handcrafted lexicon and grammar rules. Labutov et al. (2015) proposed an ’ontologycrowd-relevance’ method for question generation. First, Freebase types and Wikipedia session names are used as semantic tags to understand texts. Question are then generated based on question templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate 30M QA pairs, but their inputs are knowledge triples, instead of passages. Song and Zhao (2016) proposed a question generation method using question template seeds and used search engine to do question expansion. Du et al. (2017) proposed a neural question generation method using a vanilla sequence-tosequence RNN model, which is most-related to our work. But this method i"
D17-1090,D15-1237,0,0.0201733,"is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics where each generated Qi can be answered by S. There are four components in our QG engine: query “what is the population of nyc” is issued to YahooAnswers2 , the returned page contains a list of related questions"
D17-1090,P17-1123,0,0.430684,"templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate 30M QA pairs, but their inputs are knowledge triples, instead of passages. Song and Zhao (2016) proposed a question generation method using question template seeds and used search engine to do question expansion. Du et al. (2017) proposed a neural question generation method using a vanilla sequence-tosequence RNN model, which is most-related to our work. But this method is still based on labeled dataset, and tried RNN only. Comparing to all these related work mentioned above, our question generation approach has two uniqueness: (1) all question patterns, that are used as training data for question generation, are automatically extracted from a large scale CQA question set without any crowdsourcing effort. Such question patterns reflect the most common user intentions, and therefore are useful to search, QA, and chatbo"
D17-1090,E17-1066,0,0.0152031,"ction task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874 c Copenhagen, Denmark, September 7–11, 2017. 2017 Associatio"
D17-1090,P03-1054,0,0.0495266,"ct question topic candidates from S, including: 6 Given a predicted question pattern Qp and a selected question topic Qt of an input passage S, a complete question Q can be simply generated by replacing # in Qp with Qt . We use a set of features to rank generated question candidates: • question pattern prediction score, which is the prediction score by either retrieval-based approach or generation-based approach; • Entities as question topic candidates, which are detected based on Freebase4 entities; • Noun phrases as question topic candidates, which are detected based on the Stanford parser (Klein and Manning, 2003). • question topic selection score, for retrievalbased approach, this score is computed as s(Qt , Qp ), while for generation-based approach, this score is the attention score; Once a question topic candidate Qt is extracted from S, we then measure how Qt can fit Qp by: • QA matching score, which measures relevance between generated question Q and S. 1 X s(Qt , Qp ) = · #(Qtpk ) · dist(vQt , vQtk ) p N k • word overlap between Q and S, which counts number of words that co-occur in Q and S; s(Qt , Qp ) denotes the confidence that Qt can be filled into Qp to generate a reasonable question. Qtpk d"
D17-1090,Q16-1019,0,0.11248,"egrated and evaluated in an end-to-end QA task directly, and shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then"
D17-1090,P15-1086,0,0.265209,"ched by Qp , and 0 otherwise. All features are combined by a linear model as: p(Q|S) = https://developers.google.com/freebase/ X i 870 λi · hi (Q, S, Qp , Qt ) 8 where hi (Q, S, Qp , Qt ) is one of the features described above, and λi is the corresponding weight. 7 Related Work Yao et al. (2012) proposed a semantic-based question generation approach, which first parses the input sentence into its corresponding Minimal Recursion Semantics (MRS) representation, and then generates a question guided by the English Resource Grammar that includes a large scale handcrafted lexicon and grammar rules. Labutov et al. (2015) proposed an ’ontologycrowd-relevance’ method for question generation. First, Freebase types and Wikipedia session names are used as semantic tags to understand texts. Question are then generated based on question templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate"
D17-1090,D16-1147,0,0.0176953,"QA task directly, and shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then compute the question-to-generated question"
D17-1090,P02-1040,0,0.114478,"QA) website. The motivation of using CQA website for training data collection is that, such websites (e.g., YahooAnswers, Quora, etc.) contain large scale QA pairs generated by real users, and these questions reflect the most common user intentions, and therefore are useful to search, QA, and chatbot scenarios. To achieve the 2nd goal, we explore two ways to generate questions for a given passage, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN). We evaluate the generation quality by BLEU score (Papineni et al., 2002) and human annotations, and discuss their pros and cons in Section 9. To achieve the 3rd goal, we integrate our question generation approach into an end-to-end QA task, i.e., answer sentence selection, and evaluate its impact on three popular benchmark datasets, SQuAD, MS MARCO, and WikiQA. Experimental results show that, the generated questions can improve the QA quality on all these three datasets. This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as t"
D17-1090,D16-1264,0,0.0988632,"questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference"
D17-1090,P14-1133,0,\N,Missing
D17-1175,P16-1231,0,0.218022,"ies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsi"
D17-1175,D14-1082,0,0.83352,"ntroduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq mo"
D17-1175,D14-1179,0,0.0403433,"Missing"
D17-1175,P16-2006,0,0.024372,". We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve compa"
D17-1175,D16-1137,0,0.0605328,"Missing"
D17-1175,P81-1022,0,0.754027,"Missing"
D17-1175,P15-1033,0,0.472757,"arning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information,"
D17-1175,E17-1063,0,0.0894503,"been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key r"
D17-1175,P11-2033,0,0.106709,"LAS 93.00 90.95 92.20 89.70 91.80 89.60 91.57 87.26 93.20 90.90 93.10 90.90 93.99 92.05 93.99 91.90 94.30 91.95 94.10 91.90 92.02 89.10 91.84 88.84 93.65 91.52 93.71 91.60 94.24 92.01 94.16 92.13 CTB Dev Test UAS LAS UAS LAS 86.00 84.40 84.00 82.40 83.90 82.40 87.20 85.90 87.20 85.70 87.60 86.10 87.35 85.85 87.84 86.15 86.21 83.80 85.80 83.53 87.28 85.30 87.41 85.40 88.06 86.30 87.97 86.18 Table 1: Results of various state-of-the-art parsing systems on English dataset (PTB with Stanford Dependencies) and Chinese dataset (CTB). The numbers reported from different systems are taken from: Z&N11 (Zhang and Nivre, 2011); C&M14 (Chen and Manning, 2014); ConBSO (Wiseman and Rush, 2016); Dyer15 (Dyer et al., 2015); Weiss15 (Weiss et al., 2015); K&G16 (Kiperwasser and Goldberg, 2016); DENSE (Zhang et al., 2017). p 6/(drow + dcol ), where drow and dcol are the number of rows and columns (Glorot and Bengio, 2010). Our models are trained on a Tesla K40m GPU and optimized with vanilla SGD algorithm with mini-batch size 64 for English dataset and 32 for Chinese dataset. The initial learning rate is set to 2 and will be halved when unlabeled attachment scores (UAS) on the development set do not increase for 900 batche"
D17-1175,Q16-1023,0,0.291671,"s on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-"
D17-1175,J93-2004,0,0.0581776,"Sc < 2 I(yi ) =  1 otherwise (8) where Sc represents the number of words in the stack and Wc is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each target symbol yi can be rewritten as exp (gi ) ∗ I(yi ) p(yi |y<i , h) = P k exp (gk ) ∗ I(yk ) (9) where gi is the ith element of g(yi−1 , zi , ci ). 4 Experiments In this section, we evaluate our parsing model on the English and Chinese datasets. Following Dyer et al. (2015), Stanford Dependencies (de Marneffe and Manning, 2008) conversion of the Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency parsing. In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk&gt; token. 4.1 Setup For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embedding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following Chen and Manning (2014); Dyer et al. (2015), we used 300-dimensional pretrained GloVe vector"
D17-1175,W04-0308,0,0.0771375,"Ua ht + Sa st ) i,t = va tanh(Wa [zi−1 ; ci (7) where W m is the weight matrix. With this network structure, we obtain different context vectors (c1i , c2i , . . . , cli ), and the final context vector ci , which is considered as complex context information, is replaced by the concatenation of those vectors: ci = [c1i ; c2i ; . . . ; cli ]. Decoder: Unlike machine translation and text summarization in which seq2seq model is widely applied, a sequence of action in dependency parsing must satisfy some constraints so that they can generate a dependency tree. Following the arcstandard algorithm (Nivre, 2004), the precondition can be categorized as 1) SHIFT(SH): There exists at least one word that is not pushed into the stack; 2) LEFT-ARC(LR(d)) and RIGHT-ARC(RR(d)): There are at least two words in the stack. These two constraints can be defined as indicator functions  yi = SH, Wc ≤ 0  0 0 yi = LR(d) or RR(d), Sc < 2 I(yi ) =  1 otherwise (8) where Sc represents the number of words in the stack and Wc is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each target symbol yi can be rewritten as exp (gi ) ∗ I(yi ) p(yi |"
D17-1175,D14-1162,0,0.087559,"d Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency parsing. In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk&gt; token. 4.1 Setup For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embedding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following Chen and Manning (2014); Dyer et al. (2015), we used 300-dimensional pretrained GloVe vectors (Pennington et al., 2014) to initialize our word embedding matrix. Other model parameters are initialized using a normal distribution with a mean of 0 and a variance of 1679 Parser Z&N11 C&M14 ConBSO Dyer15 Weiss15 K&G16 DENSE seq2seq Our model Ensemble PTB-SD Dev Test UAS LAS UAS LAS 93.00 90.95 92.20 89.70 91.80 89.60 91.57 87.26 93.20 90.90 93.10 90.90 93.99 92.05 93.99 91.90 94.30 91.95 94.10 91.90 92.02 89.10 91.84 88.84 93.65 91.52 93.71 91.60 94.24 92.01 94.16 92.13 CTB Dev Test UAS LAS UAS LAS 86.00 84.40 84.00 82.40 83.90 82.40 87.20 85.90 87.20 85.70 87.60 86.10 87.35 85.85 87.84 86.15 86.21 83.80 85.80 83.5"
D17-1175,D15-1044,0,0.0506612,"ributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key role in classic transition-based or graph-based dependency parsing models, cannot be explicitly employed. For example, classic transition-based parsing algorithm utilizes a stack to manage the heads of partial s"
D17-1175,P15-1032,0,0.458119,"tiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-ba"
D18-1088,N18-1150,0,0.059276,"017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. Extractive summari"
D18-1088,P16-1046,1,0.940453,"Zhou† † Microsoft Research Asia, Beijing, China ‡ Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh, UK {xizhang,fuwei,mingzhou}@microsoft.com,mlap@inf.ed.ac.uk Abstract (Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010). The successful application of neural network models to a variety of NLP tasks and the availability of large scale summarization datasets (Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 201"
D18-1088,W04-1017,0,0.0245229,"001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document. A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Filatova and Hatzivassiloglou, 2004) coupled with binary classifiers (Kupiec et al., 1995), hidden Markov models (Conroy and O’leary, 2001), graph based methods 779 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779–784 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sum sent1 Latent Variables 1 0 sent1 sent2 sum sent2 1 0 sent3 sent4 ment are given (methods for obtaining these labels are discussed in Section 3). As shown in the lower part of Figure 1, our extractive model has three parts: a sentence encoder to convert each sentence"
D18-1088,P15-1162,0,0.072593,"Missing"
D18-1088,E17-2068,0,0.0115852,"3.30 17.28 15.75 15.82 16.20 17.52 18.20 18.45 18.77 15.43 R-L 36.57 35.50 32.65 36.38 39.08 36.90 35.30 36.39 36.60 37.14 37.54 34.33 Table 1: Results of different models on the CNN/Dailymail test set using full-length F1 ROUGE -1 (R-1), ROUGE -2 (R-2), and ROUGE -L (R-L). regularized all LSTMs with a dropout rate of 0.3 (Srivastava et al., 2014; Zaremba et al., 2014). We also applied word dropout (Iyyer et al., 2015) at rate 0.2. We set the hidden unit size d = 300 for both word-level and sentence-level LSTMs and all LSTMs had one layer. We used 300 dimensional pre-trained FastText vectors (Joulin et al., 2017) to initialize our word embeddings. The latent model was initialized from the extractive model (thus both models have the same size) and we set the weight in Equation (7) to α = 0.5. The latent model was trained with SGD, with learning rate 0.01 for 5 epochs. During inference, for both extractive and latent models, we rank sentences with p(yi = True|y1:i−1 , D) and select the top three as summary (see also Equation (3)). Experiments Dataset and Evaluation We conducted experiments on the CNN/Dailymail dataset (Hermann et al., 2015; See et al., 2017). We followed the same pre-processing steps as"
D18-1088,D15-1044,0,0.0753765,"to learn sentence representations, while they use convolutional neural network coupled with max pooling (Kim et al., 2016). 2.2 Sentence Compression We train a sentence compression model to map a sentence selected by the extractive model to a sentence in the summary. The model can be used to evaluate the quality of a selected sentence with respect to the summary (i.e., the degree to which it is similar) or rewrite an extracted sentence according to the style of the summary. For our compression model we adopt a standard attention-based sequence-to-sequence architecture (Bahdanau et al., 2015; Rush et al., 2015). The training set for this model is generated from the same summarization dataset used to train the exractive model. Let D = (S1 , S2 , . . . , S|D |) denote a document and H = (H1 , H2 , . . . , H|H |) its summary. We view each sentence Hi in the summary as a target sentence and assume that its corresponding source is a sentence in D most similar to it. We measure the similarity between source sentences and candidate targets using ROUGE, i.e., Sj = argmaxSj ROUGE(Sj , Hi ) and hSj , Hi i is a training instance for the compresˆi besion model. The probability of a sentence H ˆ ˆ ˆ ing the comp"
D18-1088,W04-1013,0,0.496949,"ver et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to i"
D18-1088,P17-1099,0,0.866109,"esentations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-b"
D18-1088,N10-1134,0,0.0358833,"nd H = (H1 , H2 , . . . , H|H |) its human summary (Hk is a sentence in H). We assume that there is a latent variable zi ∈ {0, 1} for each sentence Si indicating whether Si should be selected, and zi = 1 entails it should. We use the extractive model from Section 2.1 to produce probability distributions for latent variables (see Equation (3)) and obtain them by sampling zi ∼ p(zi |z1:i−1 , hD i−1 ) (see R(C, H) = α Rp (C, H) + (1 − α) Rr (C, H) (7) Our use of the terms “precision” and “recall” is reminiscent of relevance and coverage in other summarization work (Carbonell and Goldstein, 1998; Lin and Bilmes, 2010; See et al., 2017). We train the model by minimizing the negative expected R(C, H): L(θ) = −E(z1 ,...,z|D |)∼p(·|D) [R(C, H)] (8) where p(·|D) is the distribution produced by the neural extractive model (see Equation (3)). Unfortunately, computing the expectation term is prohibitive, since the possible latent variable combinations are exponential. In practice, we approximate this expectation with a single sample from 1 We also experimented with unnormalized probabilities (i.e., excluding the exp in Equation (4)), however we obtained inferior results. 781 Model L EAD 3 L EAD 3 (Nallapati et al"
D18-1088,W01-0100,0,0.709066,"are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes directly from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models. 1 Introduction Document summarization aims to automatically rewrite a document into a shorter version while retaining its most important content. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document. A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Fil"
D18-1088,P05-3013,0,0.0543716,"Missing"
D18-1088,P10-1058,1,0.908096,"Missing"
D18-1088,K16-1028,0,0.373973,"of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually"
D18-1088,N18-1158,1,0.825962,"(Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which"
D18-1188,D13-1160,0,0.082852,"without knowing that cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table"
D18-1188,E17-2029,0,0.0206229,"ence. ψg (yi ) = viT Wg st ψc (yi ) = tanh(hi T Wc )st Encoder-Decoder Encoder: A bidirectional RNN with gated recurrent unit (GRU) (Cho et al., 2014) is used as the encoder to read a SQL query x = (x1 , ..., xT ). The forward RNN reads a SQL query (5) 4.3 (7) Incorporating Latent Variable Increasing the diversity of generated questions is very important to improve accuracy, generalization, and stability of the semantic parser, since this 1599 increases the mount of training data and produces more diverse questions for the same intent. In this work, we incorporate stochastic latent variables (Cao and Clark, 2017; Serban et al., 2017) to the sequence-to-sequence model in order to increase question diversity. Specifically, we introduce a latent variable z ∼ p(z), which is a standard Gaussian distribution N (0, In ) in our case, and calculate the likelihood of a target sentence y as follows: Z (8) p(y|x) = p(y|z, x)p(z) dz z We maximize the evidence lower bound (ELBO), which decomposes the loss into two parts, including (1) the KL divergence between the posterior distribution and the prior distribution, and (2) a cross-entropy loss between the generated question and the ground truth. logp(y|x) ≥ −DKL (Q"
D18-1188,D17-1090,1,0.851999,"uage questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2"
D18-1188,N13-1092,0,0.044976,"Missing"
D18-1188,D17-1087,0,0.0434135,"the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that"
D18-1188,P16-1154,0,0.0599355,"ding of the previously predicted word yt−1 , and the last hidden state st−1 is fed to the next step. st = GRU (st−1 , yt−1 , ct ) After obtaining hidden states st , we adopt the copying mechanism that predicts a word from the target vocabulary or from the source sentence (detailed in Subsection 4.2). 4.2 Incorporating Copying Mechanism In our task, the generated question utterances typically include informative yet low-frequency words such as named entities or numbers. Usually, these words are not included in the target vocabulary but come from SQL queries. To address this, we follow CopyNet (Gu et al., 2016) and incorporate a copying mechanism to select whether to generate from the vocabulary or copy from SQL queries. The probability distribution of generating the t-th word is calculated as Equation 6, where ψg (·) and ψc (·) are scoring functions for generating from the vocabulary ν and copying from the source sentence x, respectively. eψg (yt ) + eψc (yt ) P ψg (v) + ψc (w) v∈ν e w∈x e p(yt |y<t , x) = P (6) The two scoring functions are calculated as follows, where Wg and Wc are model parameters, vi is the one-hot indicator vector for yi and hi is the hidden state of word yi in the source sent"
D18-1188,P17-1097,0,0.0762159,"Missing"
D18-1188,P16-1004,0,0.187589,"Missing"
D18-1188,P17-1089,0,0.15759,"s, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is critical for accessing relational databases w"
D18-1188,D17-1091,0,0.0405482,"ns from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2017); and (3) using the QG results as additional constraints in the training objectives (Tang et al., 2017). This work belongs to the first direct"
D18-1188,P18-1069,0,0.0366913,"Missing"
D18-1188,P17-1123,0,0.0222525,"also relates to the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates t"
D18-1188,P16-1002,0,0.0909215,"Missing"
D18-1188,P17-1014,0,0.0279021,"r et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2017); and (3) using the"
D18-1188,D17-1160,0,0.0464092,"xtensional experiment on WikiTableQuestions2 (Pasupat and Liang, 2015) in a transfer learning scenario to verify the effectiveness of our approach. WikiTableQuestions contains 22,033 complex questions on 2,108 Wikipedia tables. Each instance consists of a natural language question, a table and an answer. Following Pasupat and Liang (2015), we report development accuracy which is averaged over the first three 80-20 training data splits. Test accuracy is reported on the train-test data. In this experiment, we apply the QG model learnt from WikiSQL to improve the state-of-theart semantic parser (Krishnamurthy et al., 2017) on this dataset. Different from WikiSQL, this dataset requires question-answer pairs for training. Thus, we generate question-answer pairs by follow steps. We first sample SQL queries on the tables from WikiTableQuestions, and then use our QG model to generate question-SQL pairs. After2 https://nlp.stanford.edu/software/ sempre/wikitable/ wards, we obtain question-answer pairs by executing SQL queries. The generated question-answer pairs will be combined with the original WikiTableQuestions training data to train the model. Pasupat and Liang (2015) Neelakantan et al. (2016) Haug et al. (2017)"
D18-1188,Q13-1016,0,0.0298687,"cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is"
D18-1188,P11-1060,0,0.0403622,"ions. For instance, without knowing that cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work,"
D18-1188,P16-1170,0,0.035962,"e QA model and train the QG model, and incorporate more generated question-logical form pairs to further improve the QA model. Question Generation Our work also relates to the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority o"
D18-1188,P02-1040,0,0.10339,"Missing"
D18-1188,P15-1142,0,0.0229864,"imit-supervision scenarios. This is consistent with our intuition that the performance of the QG model is improved by incorporating the copying mechanism, since rare words of great importance mainly come from the input sequence. To better understand the impact of incorporating a latent variable, we show examples generated by different QG variations in Table 7. We can see that incorporating a latent variable empowers the model to generate diverse questions for the same intent. 5.5 Transfer Learning on WikiTableQuestions In this part, we conduct an extensional experiment on WikiTableQuestions2 (Pasupat and Liang, 2015) in a transfer learning scenario to verify the effectiveness of our approach. WikiTableQuestions contains 22,033 complex questions on 2,108 Wikipedia tables. Each instance consists of a natural language question, a table and an answer. Following Pasupat and Liang (2015), we report development accuracy which is averaged over the first three 80-20 training data splits. Test accuracy is reported on the train-test data. In this experiment, we apply the QG model learnt from WikiSQL to improve the state-of-theart semantic parser (Krishnamurthy et al., 2017) on this dataset. Different from WikiSQL, t"
D18-1188,P16-1003,0,0.0372546,"lt” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is critical for accessing re"
D18-1188,D14-1162,0,0.0812641,"pectively. Similar to hx , hy is obtained by encoding the target sentence. µ = Wµ [hx ; hy ] + bµ log(σ 2 ) = Wσ [hx ; hy ] + bσ 4.4 the initial state of the decoder, and then decoding deterministically for each sample. Here, we list our training details. We set the dimension of the encoder hidden state as 300, and the dimension of the latent variable z as 64. We use dropout with a rate of 0.5, which is applied to the inputs of RNN. Model parameters are initialized with uniform distribution, and updated with stochastic gradient decent. Word embedding values are initialized with Glove vectors (Pennington et al., 2014). We set the learning rate as 0.1 and the batch size as 32. We tune hyper parameters on the development, and use beam search in the inference process. 5 Experiment We conduct experiments on the WikiSQL dataset1 (Zhong et al., 2017). WikiSQL is the largest handannotated semantic parsing dataset which is an order of magnitude larger than other datasets in terms of both the number of logical forms and the number of schemata (tables). WikiSQL is built by crowd-sourcing on Amazon Mechanical Turk, including 61,297 examples for training, and 9,145/17,284 examples for development/testing. Each instanc"
D18-1188,P18-1034,1,0.877859,"ll-scale supervised training data that consists of SQL-question pairs. Lastly, the generated question-SQL pairs are viewed as the pseudo-labeled data, which are combined with the supervised training data to train the semantic parser. Since we conduct the experiment on WikiSQL dataset, we follow Zhong et al. (2017) and use the same template-based SQL sampler, as summarized in Table 1. The details about the semantic parser and the question generation model will be introduced in Sections 3 and Section 4, respectively. 3 Semantic Parsing Model We use a state-of-the-art end-to-end semantic parser (Sun et al., 2018) that takes a natural language question as the input and outputs a SQL query, which is executed on a table to obtain the answer. To make the paper self-contained, we briefly describe the approach in this section. The semantic parser is abbreviated as STAMP, which is short for Syntax- and Table- Aware seMantic Parser. Based on the encoder-decoder framework, STAMP takes a question as the input and generates a SQL query. It extends pointer networks (Zhong et al., 2017; Vinyals et al., 2015) by incorporating three “channels” in the decoder, in which the column channel predicts column names, the va"
D18-1188,P15-1129,0,0.431991,"Missing"
D18-1188,P07-1121,0,0.419448,"Missing"
D18-1188,P16-1127,0,0.207322,"Missing"
D18-1188,P17-1096,0,0.0226953,"to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2"
D18-1188,D07-1071,0,0.232712,"Missing"
D18-1188,D17-1125,0,0.0685018,"Missing"
D18-1232,D15-1237,0,\N,Missing
D18-1232,D16-1264,0,\N,Missing
D18-1232,D16-1180,0,\N,Missing
D18-1232,P17-1147,0,\N,Missing
D18-1232,D17-1215,0,\N,Missing
D18-1232,P17-1018,1,\N,Missing
D18-1232,D17-1264,0,\N,Missing
D18-1232,P18-1158,0,\N,Missing
D18-1232,P18-1078,0,\N,Missing
D18-1271,C16-1171,0,0.0159595,"nu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates,"
D18-1271,D14-1092,1,0.890876,"Missing"
D18-1271,D12-1025,1,0.84361,"s languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsuperv"
D18-1271,D14-1061,1,0.825032,"ast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsupervised, effective, in"
D18-1271,N07-2008,0,0.0207658,"0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and"
D18-1271,P98-1069,0,0.239043,"slation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel co"
D18-1271,D16-1075,1,0.854091,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,C16-1309,1,0.941152,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,N13-1056,0,0.0349868,"Missing"
D18-1271,W09-3107,1,0.81454,"ed alignment, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of doc"
D18-1271,D15-1015,0,0.0135832,"10; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods,"
D18-1271,P06-1103,0,0.0425456,"i and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast"
D18-1271,W02-0902,0,0.0611221,"tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are"
D18-1271,W11-2125,0,0.0196081,"guage knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Ro"
D18-1271,N16-1132,0,0.031503,"Missing"
D18-1271,P08-1088,0,0.0159272,"d (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of t"
D18-1271,D14-1198,1,0.826094,"nt, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of documents where these"
D18-1271,W11-2206,1,0.807265,"y for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and"
D18-1271,D11-1006,0,0.0252557,", 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By"
D18-1271,J05-4003,0,0.0814633,"ments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016;"
D18-1271,D14-1162,0,0.0810082,"ge. Given that our approach is unsupervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been suppo"
D18-1271,E09-1091,0,0.0212751,"lignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the"
D18-1271,C10-1124,0,0.0211684,"eans it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining"
D18-1271,P15-2118,0,0.0147857,"s and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narro"
D18-1271,N18-1202,0,0.0218069,"pervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been supported by the U.S. DARPA AIDA Pro"
D18-1271,P99-1067,0,0.288414,"ing and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. Howev"
D18-1271,N04-1033,0,0.054922,"37 documents. We removed stopwords, conducted lemmatization and name tagging for the English stream, and did word segmentation and name tagging for the Chinese stream using the Stanford CoreNLP toolkit (Manning et al., 2014). 4 Due to the upper bound of Conf (Gc , Ge ), the algorithm must terminate after several iterations. We detected bursts and constructed the BINets5 for the Chinese and English stream based on (Ge et al., 2016a). The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English one has 8,852 nodes and 85,125 edges. Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation. Among the 7,360 nodes in the Chinese BINet, 2,281 nodes need to be deciphered since their words are not in the bi-lingual lexicon. 4.1.2 Evaluation Setting We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e∗ which has the highest score as c’s counterpart in the English BINet: e∗ = arg max Score(c, e) e∈Cand(c) We rank the aligned node pairs by the score and manually evaluate the quality of the top K pairs. A pair hc,ei is annotated as correct if e is a cor"
D18-1271,P11-1002,0,0.0330427,"n (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that ou"
D18-1271,P10-1115,0,0.0274153,"rger than that used in our experiment and they are endlessly updated. 2503 10 The alignments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Ki"
D18-1271,W02-2026,0,0.132146,"Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for genera"
D18-1271,P17-1179,0,0.0253021,"Missing"
D18-1271,D17-1207,0,0.0444246,"Missing"
D18-1271,C04-1089,0,0.0807467,"ingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpor"
D18-1271,N10-1063,0,0.0216071,"endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et a"
D18-1271,P06-1010,0,0.229533,"Chinese BINet, its candidate nodes in the English BINet can be derived as: Cand(c) = {e|P(e) ∩ P(c) 6= ∅} where e ∈ Ve , and P(c) and P(e) are the burst periods of c and e respectively. 3.3 Candidate Verification For the candidate list for c (i.e., Cand(c)), we need to verify each node e ∈ Cand(c) and choose the most probable one as c’s counterpart. Formally, we define Score(c, e) as the credibility score of e being the correct counterpart of c and propose the following novel clues for verification. Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: 1 Sp ∈ [0, 1] ∝ LD where LD is the normalized (by e’s length) Levenshtein edit distance between c’s pinyin3 string and e’s word string. Translation For a node e ∈ Cand(c), it is possible that e’s word exists or partially exists in the bi-lingual lexicon. We can exploit the translation clue to verify if e is c’s counterpar"
D18-1271,D12-1003,0,0.0225396,"De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across la"
D19-1071,P18-1073,0,0.146796,"e translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are not the case, the above pre-training method may not provide much useful cross-lingual information. In this paper, by incorporating explicit crosslingual training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. Then, we propose a new pr"
D19-1071,D18-1399,0,0.0526068,"e translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are not the case, the above pre-training method may not provide much useful cross-lingual information. In this paper, by incorporating explicit crosslingual training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. Then, we propose a new pr"
D19-1071,D18-1549,0,0.210576,"l unsupervised machine translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are not the case, the above pre-training method may not provide much useful cross-lingual information. In this paper, by incorporating explicit crosslingual training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. The"
D19-1071,W03-0301,0,0.0610666,"model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with Lample and Conneau (2019) (with the significance level of p<0.01). 4.3 4.4 Evaluation of Cross-lingual Pre-trained Encoder Word Alignment To evaluate the cross-lingual modeling capacity of our pre-trained models, we first conduct experiments on the English-French (en-fr) dataset of the HLT/NAACL 2003 alignment shared task (Mihalcea and Pedersen, 2003). Given two parallel sentences in English and French respectively, we feed each sentence into the pre-trained cross-lingual encoder and get its respective outputs. Then, we calculate the similarities between the outputs of the two sentences and choose target words with max similarity scores as the alignments of correUsage of Pre-trained Decoder As mentioned in Section 3.4, it is interesting to explore the different usage of pre-trained decoders in the MT task. According to our intuition, directly using the pre-trained model as the decoder may not work well because parameters of the decoder nee"
D19-1071,Q17-1010,0,0.0324462,"ge Model (CMLM) which is to predict the translation candidates of randomly masked n-grams. The last step is to leverage the pre-trained cross-lingual language models as the encoder and decoder of a neural machine translation model and fine-tune the translation model iteratively. 3 3.1 Method 3.2 N-gram Translation Table Inferring Following previous work of unsupervised machine translation (Artetxe et al., 2018b; Lample et al., 2018; Ren et al., 2019), given two languages X and Y , we build our n-gram translation tables as follows. First, we obtain monolingual n-gram embeddings using fastText (Bojanowski et al., 2017) and then get cross-lingual n-gram embeddings using vecmap (Artetxe et al., 2018a) in a fully unsupervised way. Based on that, we calculate the similarity score of n-grams x and y in two languages respectively with the marginal-based scoring method (Conneau et al., 2017; Artetxe and Schwenk, 2018). Specifically, given the crosslingual embeddings of x and y, denoted as ex and ey , the similarity score is calculated as: Overview Our method can be divided into three steps as shown in Figure 2. Given two languages X and Y , we first get unsupervised cross-lingual n-gram embeddings of them, from wh"
D19-1071,P12-1017,0,0.0214834,"ed BPE space during their pre-training method, which is inexplicit and limited. Therefore, we figure out a new pre-training method that gives the model more explicit and stronger cross-lingual information. our model to learn both monolingual and crosslingual information during pre-training. Besides, we find the two modifications(translation prediction and n-gram mask) made by CMLM have nearly equal contributions to the translation performance, except for en2de, where the “n-gram mask” has little influence. 5 Related Work Unsupervised machine translation dates back to Klementiev et al. (2012); Nuhn et al. (2012), but becomes a hot research topic in recent years. As the pioneering methods, Artetxe et al. (2017); Lample et al. (2017); Yang et al. (2018) are mainly the modifications of the encoder-decoder structure. The core idea is to constrain outputs of encoders of two languages into a same latent space with a weight sharing mechanism such as using a shared encoder. Denoising auto-encoder (Vincent et al., 2010) and adversarial training methods are also leveraged. Besides, they apply iterative backtranslation to generated pseudo data for crosslingual training. In addition to NMT methods for unsupervis"
D19-1071,J93-2003,0,0.135968,"training signals, we propose a novel crosslingual pre-training method based on BERT (Devlin et al., 2018) for unsupervised machine translation. Our method starts from unsupervised crosslingual n-gram embeddings, from which we infer n-gram translation pairs. Then, we propose a new pre-training objective called Cross-lingual Masked Language Model (CMLM), which masks the input n-grams randomly and predicts their corresponding n-gram translation candidates inferred above. To solve the mismatch between different lengths of the masked source and predicted target n-grams, IBM models are introduced (Brown et al., 1993) to derive the training loss at each Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the crosslingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pa"
D19-1071,J03-1002,0,0.0130581,"Unsupervised Machine Translation Taking two cross-lingual language models pretrained with the above method as the encoder and decoder, we build an initial unsupervised neural machine translation model. Then, we train the model with monolingual data until convergence via denoising auto-encoder and iterative backtranslation, as described in Artetxe et al. (2017); Lample et al. (2017, 2018). Different from them, we step further and make another iteration with updated n-gram translation tables. Specifically, we translate the monolingual sentences with our latest translation model and run GIZA++ (Och and Ney, 2003) on the generated pseudo parallel data to extract updated n-gram translation pairs, which are used to tune the encoder as Section 3.3, together with the back-translation within a multi-task learning framework. Experimental results show that running another iteration can further improve the translation performance. It is also interesting to explore the usage of pre-trained decoders in the translation model. It seems that pre-training decoders has a smaller effect on the final performance than pre-training encoders (Lample and Conneau, 2019), one reason for which could be that the encoder-to-dec"
D19-1071,D18-1269,0,0.343657,"that our method can produce better cross-lingual representations and significantly improve the performance of unsupervised machine translation. Our contributions are listed as follows. 2.2 • We propose a novel cross-lingual pre-training method to incorporate explicit cross-lingual information into pre-trained models, which significantly improves the performance of unsupervised machine translation. Based on BERT, Lample and Conneau (2019) propose a cross-lingual version called XLM and reach the state-of-the-art performance on some crosslingual NLP tasks including cross-lingual classification (Conneau et al., 2018), machine translation, and unsupervised cross-lingual word embedding. The basic points of XLM are mainly two folds. The first one is to use a shared vocabulary of BPE (Sennrich et al., 2016b) to provide potential crosslingual information between two languages just as mentioned in Lample et al. (2018), in an inexplicit way though. The second point is a newly proposed training task called Translation Language Modeling (TLM), which extends MLM by concatenating parallel sentences into a single training text stream. In this way, the model can leverage the cross-lingual information provided by paral"
D19-1071,P16-1009,0,0.717545,"ting, China § Microsoft Research Asia, Beijing, China † {shuoren,mashuai}@buaa.edu.cn § {Wu.Yu,shujliu,mingzhou}@microsoft.com Abstract performance as pointed in Lample et al. (2018), Artetxe et al. (2018b) and Ren et al. (2019). Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are"
D19-1071,P16-1162,0,0.79878,"ting, China § Microsoft Research Asia, Beijing, China † {shuoren,mashuai}@buaa.edu.cn § {Wu.Yu,shujliu,mingzhou}@microsoft.com Abstract performance as pointed in Lample et al. (2018), Artetxe et al. (2018b) and Ren et al. (2019). Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models (Lample and Conneau, 2019). However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited. Firstly, although the same BPE pieces from different languages may share the same semantic space, the semantic information of n-grams or sentences in different languages may not be shared properly. However, cross-lingual information based on n-gram level is crucial to model the initialization of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018b), which is absent in the current pretraining method. Secondly, BPE sharing is limited to languages that share much of their alphabet. For language pairs that are"
D19-1071,P18-1005,0,0.0186214,"the model more explicit and stronger cross-lingual information. our model to learn both monolingual and crosslingual information during pre-training. Besides, we find the two modifications(translation prediction and n-gram mask) made by CMLM have nearly equal contributions to the translation performance, except for en2de, where the “n-gram mask” has little influence. 5 Related Work Unsupervised machine translation dates back to Klementiev et al. (2012); Nuhn et al. (2012), but becomes a hot research topic in recent years. As the pioneering methods, Artetxe et al. (2017); Lample et al. (2017); Yang et al. (2018) are mainly the modifications of the encoder-decoder structure. The core idea is to constrain outputs of encoders of two languages into a same latent space with a weight sharing mechanism such as using a shared encoder. Denoising auto-encoder (Vincent et al., 2010) and adversarial training methods are also leveraged. Besides, they apply iterative backtranslation to generated pseudo data for crosslingual training. In addition to NMT methods for unsupervised machine translation, some following work shows that SMT methods and the hybrid of NMT and SMT can be more effective (Artetxe et al., 2018b;"
D19-1172,D14-1181,0,0.00514974,"For simplification, we use a special symbol, [S], to concatenate all the entity information together. These two parts can be regarded as different source information. Based on whether the inter-relation between different source information is explicitly explored, the classification models can be classified into two categories, unstructured models and structured models. Unstructured models concatenate different source inputs into a long sequence with a special separation symbol [SEL]. We implement several widely-used sequence classification models, including Convolutional Neural Network (CNN) (Kim, 2014), Long-Short Term Memory Network (LSTM) (Schuster and Paliwal, 1997), and Recurrent Convolutional Neural Network (RCNN) (Lai et al., 2015), Transformer. The details of the models are shown in Supplementary Materials. 1622 [A] web browser Midori operating system Midori Decoder Decoder Encoder Entity Info [B] Entity Rendering Module Ambiguous Context Template What are the languages used to create the source code of Midori? Encoder Template Generating Module Decoder When you say the source code language used in the program Midori, are you referring to [A] or [B]? Hidden vector of [A] Hidden vecto"
D19-1172,J81-4005,0,0.674542,"Missing"
D19-1172,N03-1007,0,0.0346386,"Missing"
D19-1172,D16-1076,0,0.0350947,"Missing"
D19-1172,P17-1045,0,0.0494573,"Missing"
D19-1172,P18-1068,0,0.018531,"es (e.g., “web browser Midori ”) and pattern phrases ( e.g. “When you say the source code language used in the program Midori, are you referring to [A] or [B]?” ). The entity phrase is summarized from the given entity information for distinguishing between two entities. The pattern phrase is used to locate the position of ambiguity, which is closely related with the context. In summary, two kinds of phrases refer to different source information. Based on this feature, we propose a new coarse-to-fine model, as shown in Figure 6. Similar ideas have been successfully applied to semantic parsing (Dong and Lapata, 2018). The proposed model consists of a template generating module Tθ and an entity rendering module Rφ . Tθ first generates a template containing pattern phrases and the symbolic representation of the entity phrases. Then, the symbolized entities contained in the generated template are further properly rendered by the entity rendering module Rφ to reconstruct complete entity information. Since the annotated clarification questions explicitly separate entity phrases and pattern phrases, we can easily build training data for these two modules. For clarity, the template is constructed by replacing en"
D19-1172,D18-1188,1,0.829182,"ll dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification-based question answering. We implement representative neural networks as baselines for three tasks and propose a new generation model. The detailed analysis shows that our dataset brings new challenges. More powerful models and reasonable evaluation metrics need further explored. In the future, we plan to"
D19-1172,D18-1361,0,0.0212818,"rification Question in Other Tasks There are several studies on asking clarification questions. Stoyanchev et al. (2014) randomly drop one phrase from a question and require annotators to ask a clarification question toward the dropped information, e.g.“Do you know the birth data of XXX”. However, the small dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification"
D19-1172,P18-1255,0,0.125162,"Missing"
D19-1172,N16-1174,0,0.0245396,"late generating module and an entity rendering module. The former is used to generate a clarification template based on ambiguous question, e.g., “When you say the source code language used in the program Midori, are you referring to [A] or [B]?”. The latter is used to fill up the generated template with detailed entity information. Structured models use separate structures to encode different source information and adopt an additional structure to model the inter-relation of the source information. Specifically, we use two representative neural networks, Hierarchical Attention Network (HAN) (Yang et al., 2016) and Dynamic Memory Network (DMN) (Kumar et al., 2016), as our structured baselines. The details of the models are shown in Supplementary Materials. 4.2 Clarification Question Generation Models The input of the generation model is the ambiguous context and entity information. In single-turn cases, the ambiguous context is current question Qa . In multi-cases, the input is current question and the previous conversation turn {Qp , Rp , Qa }. We use [S] to concatenate all entity information together, and use [SEL] to concatenate entity information and context information into a long sequence. We"
D19-1252,W18-6317,0,0.0174688,"sentences as a sequence and input it to Unicoder. The representation of the first token in the final layer will be used for the paraphrase classification task. This procedure is illustrated in Figure 1.b. We created the cross-lingual paraphrase classification dataset from machine translation dataset. Each bilingual sentence pair (X, Y ) servers as a positive sample. For negative samples, the most straight forward method is to replace Y to a ran2487 dom sampled sentence from target language. But this will make the classification task too easy. So we introduce the hard negative samples followed Guo et al. (2018). First, we train a light-weight paraphrase model with random negative samples. Then we use this model to select sentence with high similarity score to X but doesn’t equal to Y as hard negative samples. We choose DAN (Iyyer et al., 2015) as the light model. We create positive and negative samples in 1:1. Cross-lingual Masked Language Model Previous successful pre-training language model (Devlin et al., 2018; Radford et al., 2018) is conducted on document-level corpus rather than sentence-level corpus. The language model perplexity on document also is much lower than sentence (Peters et al., 20"
D19-1252,P15-1162,0,0.0938242,"Missing"
D19-1252,P07-2045,0,0.0085558,"inese(zh), Hindi(hi), Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M 15.5M 0.6M 12.6M 0.2M 0.8M 1.8M 0.5M 3.8M 5.5M 9.8M 0.6M 9.3M 4.0M 11.4M 13.2M 1.6M 11.7M 0.2M 3.3M 0.5M 0.7M 3.5M 9.6M Table 1: Sentence number we used in pre-training. Training Details Model Structure Our Unicoder is a 12-layer transformer with 1024 hidden units, 16 heads, GELU activation (Hendrycks and Gimpel, 2017). we set dropout to 0.1. The vocabulary size is 95,0"
D19-1252,W08-0336,0,0.0235743,"Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M 15.5M 0.6M 12.6M 0.2M 0.8M 1.8M 0.5M 3.8M 5.5M 9.8M 0.6M 9.3M 4.0M 11.4M 13.2M 1.6M 11.7M 0.2M 3.3M 0.5M 0.7M 3.5M 9.6M Table 1: Sentence number we used in pre-training. Training Details Model Structure Our Unicoder is a 12-layer transformer with 1024 hidden units, 16 heads, GELU activation (Hendrycks and Gimpel, 2017). we set dropout to 0.1. The vocabulary size is 95,000. Pre-training deta"
D19-1252,D18-1269,0,0.21585,"corpus in which each sentence and its translation is well aligned. We construct cross-lingual document by replacing the sentences with even index to its translation as illustrated in Figure 1.c. We truncate the cross-lingual document by 256 sequence length and feed it to Unicoder for masked language modeling. Multi-language Fine-tuning A typical setting of cross-lingual language understanding is only one language has training data, but the test is conducted on other languages. We denote the language has training data as source language, and other languages as target languages. A scalable way (Conneau et al., 2018) to address this problem is through Cross-lingual TEST, in which a pre-trained encoder is trained on data in source language and directly evaluated on data in target languages. There are two other machine translation methods that make training and test belong to the same language. TRANSLATE-TRAIN translates the source language training data to a target language and fine-tunes on this pseudo training data. TRANSLATE-TEST fine-tunes on source language training data, but translates the target language test data to source language and test on it. Inspired by multi-task learning (Liu et al., 2018,"
D19-1252,eisele-chen-2010-multiun,0,0.0200481,"ion, we describe the data processing and training details. Then we compare the Unicoder with the current state of the art approaches on two tasks: XNLI and XQA. 2488 Data Processing Our model is pre-trained on 15 languages, including English(en), French(fr), Spanish(es), German(de), Greek(el), Bulgarian(bg), Russian(ru), Turkish(tr), Arabic(ar), Vietnamese(vi), Thai(th), Chinese(zh), Hindi(hi), Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M"
D19-1252,N18-1202,0,0.232254,"rman) is obtained. In short, our contributions are 4-fold. First, 3 new cross-lingual pre-training tasks are proposed, which can help to learn a better languageindependent encoder. Second, a cross-lingual question answering (XQA) dataset is built, which can be used as a new cross-lingual benchmark dataset. Third, we verify that by fine-tuning multiple languages together, significant improvements can be obtained. Fourth, on the XNLI dataset, new state-of-the-art results are achieved. Related work Monolingual Pre-training Recently, pretraining an encoder by language model (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2018) and machine translation (McCann et al., 2017) have shown significant improvement on various natural language understanding (NLU) tasks, like tasks in GLUE (Wang et al., 2018). The application scheme is to fine-tune the pre-trained encoder on single sentence classification task or sequential labeling task. If the tasks have multiple inputs, just concatenate them into one sentence. This approach enables one model to be generalized to different language understanding tasks. Our approach also is contextual pre-training so it could been applied to various NLU tasks. Cross-lin"
D19-1252,P16-1162,0,0.0211415,"Keeping the same setting, XLM proposed a new task TLM, which uses a concatenation of the parallel sentences into one sample for masked language modeling. Besides these two tasks, we proposed three new cross-lingual pre-training tasks for building a better language-independent encoder. Approach This section will describe details of Unicoder, including tasks used in the pre-training procedure and its fine-tuning strategy. Model Structure Unicoder follows the network structure of XLM (Lample and Conneau, 2019). A shared vocabulary is constructed by running the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016) on corpus of all languages. We also down sample the rich-resource languages corpus, to prevent words of target languages from being split too much at the character level. Pre-training Tasks in Unicoder Both masked language model and translation language model are used in Unicoder by default, as they have shown strong performance in XLM. Motivated by Liu et al. (2019), which shows that pre-trained models can be further improved by involving more tasks in pre-training, we introduce three new cross-lingual tasks in Unicoder. All training data for these three tasks are acquired from the existing"
D19-1252,W18-5446,0,0.0675618,"Missing"
D19-1252,D19-1077,0,0.089503,"Missing"
D19-1252,N15-1104,0,0.121749,"Missing"
D19-5802,D14-1179,0,0.0154774,"Missing"
D19-5802,N18-2074,0,0.0275226,"toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly mask some words in the passage to <UNK> with 0.2 probability while training. The dimension of character embedding and segment embedding is 64 and 128, respectively. The number of Transformer layers used in our model is 12. For each Transformer layer, we set the hidden size to 128, and use relative position embedding (Shaw et al., 2018) whose clipping distance is 16. The number of the attention heads is 8. During training, the batch size is 32 and the number of the max training epochs is 80. We use 3.2 Results Exact match (EM) and F1 scores are two evaluation metrics for SQuAD. EM measures the percentage of the prediction that matches the groundtruth answer exactly, while F1 measures the overlap between the predicted answer answer and the ground-truth answer. The scores on the development set are evaluated by the official script. As shown in Table 1, the unified model outperforms previous state-of-the-art models and the base"
D19-5802,P82-1020,0,0.673204,"Missing"
D19-5802,P17-1147,0,0.024587,"er for the MRC task. Experimental results on SQuAD show that the unified model outperforms previous networks that separately treat encoding and matching. We also introduce a metric to inspect whether a Transformer layer tends to perform encoding or matching. The analysis results show that the unified model learns different modeling strategies compared with previous manually-designed models. 1 Introduction In spite of different neural network structures, encoding and matching components are two basic building blocks for many NLP tasks like machine reading comprehension (Rajpurkar et al., 2016; Joshi et al., 2017). A widely-used paradigm is that the input texts are encoded into vectors, and then these vectors are aggregated to model interactions between them by matching layers. Figure 1(a) shows a typical machine reading comprehension model, encoding components separately encode question and passage to vector representations. Then, we obtain context-sensitive representations for input words by considering the interactions between question and passage. Finally, an output layer is used to predict the prob∗ Contribution during internship at Microsoft Research 14 Proceedings of the Second Workshop on Machi"
D19-5802,P17-1018,1,0.922662,"soft Research hangbobao@gmail.com,piaosh@hit.edu.cn lidong1,fuwei,wenwan,nanya,lecu,mingzhou@microsoft.com Abstract ability of each token being the start or end position of the answer span. The encoding layers are usually built upon recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), and self-attention networks (Yu et al., 2018). For the matching component, various model components have been developed to fuse question and passage vector representations, such as match-LSTM (Wang and Jiang, 2016), coattention (Seo et al., 2016; Xiong et al., 2016), and self-matching (Wang et al., 2017). Recently, Devlin et al. (2018) employ Transformer networks to pretrain a bidirectional language model (called BERT), and then fine-tune the layers on specific tasks, which obtains state-of-the-art results on MRC. A research question is: apart from the benefits of pretraining, how many performance gain comes from the unified network architecture. In this paper, we evaluate and analyze unifying encoding and matching components with Transformer layers (Vaswani et al., 2017), using MRC as a case study. As shown in Figure 1(b), compared with previous specially-designed MRC networks, we do not exp"
D19-5802,D14-1162,0,0.0867023,"re Transformer layers used to compute the question-sensitive passage representations. To make a fair comparison, we only compare with the models that do not rely on pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Experiments Experimental Setup Dataset Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is composed of over 100,000 instances created by crowdworkers. Every answer is constrained to be a continuous sub-span of the passage. Settings We employ the spaCy toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly mask some words in the passage to <UNK> with 0.2 probability while training. The dimension of character embedding and segment embedding is 64 and 128, respectively. The number of Transformer layers used in our model is 12. For each Transformer layer, we set the hidden size to 128, and use relative position embedding (Shaw et al., 2018) whose clipping distance is 16. The number of the attention heads is 8. Duri"
D19-5802,N18-1202,0,0.0240129,"Apart from comparing with previous state-of-the-art models (Seo et al., 2016; Wang et al., 2017; Yu et al., 2018), we implement a baseline model that separately perform encoding and matching. The same settings as above are used. The first three Transformer layers are utilized to encode passage and question separately. Then we add a passage-question matching layer following Yu et al. (2018), with nine more Transformer layers used to compute the question-sensitive passage representations. To make a fair comparison, we only compare with the models that do not rely on pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Experiments Experimental Setup Dataset Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is composed of over 100,000 instances created by crowdworkers. Every answer is constrained to be a continuous sub-span of the passage. Settings We employ the spaCy toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly ma"
E17-1059,P12-1039,1,0.752484,"our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product infor"
E17-1059,P16-1004,1,0.841718,"model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. The attention model boosts performance for various tasks (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 201"
E17-1059,D15-1166,0,0.0304329,"Missing"
E17-1059,W15-2922,0,0.026596,"ries. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content"
E17-1059,P82-1020,0,0.85057,"Missing"
E17-1059,D13-1176,0,0.0263763,"valuable for sentiment analysis, but not well studied previously. • We propose an attention-enhanced attributeto-sequence model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. T"
E17-1059,N16-1086,0,0.0159003,"nd hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product information implicitly indicates the style of generated reviews, which makes the results extremely diverse. Another line of related work is the encoderdecoder model with neural networks. Specifically, an encoder is employed to encode"
E17-1059,P02-1040,0,0.117645,"employs distributed representations to avoid using sparse indicator features. Then, we evaluate the NN methods that use different attributes to retrieve reviews, which is a strong baseline for the generation task. The results show that our method outperforms the baseline methods. Moreover, the improvements of the attention mechanism are significant with p < 0.05 according to the bootstrap resampling test (Koehn, 2004). We further show some examples to analyze the attention model in Section 4.4. we use the greedy search algorithm to generate reviews. 4.3 MELM 59.00 Evaluation Results The BLEU (Papineni et al., 2002) score is used for automatic evaluation, which has been shown to correlate well with human judgment on many generation tasks. The BLEU score measures the precision of n-gram matching by comparing the generated results with references, and penalizes length using a brevity penalty term. We compute BLEU-1 (unigram) and BLEU-4 (up to 4 grams) in experiments. 4.3.1 Comparison with Baseline Methods We describe the comparison methods as follows: Rand. The predicted results are randomly sampled from all the reviews in the T RAIN set. This baseline method suggests the expected lower bound for this task"
E17-1059,D14-1181,0,0.00361927,"nce against baseline methods. Moreover, we demonstrate that the attention mechanism significantly improves the performance of our model. The contributions of this work are three-fold: 2 Related Work Sentiment analysis and opinion mining aim to identify and extract subjective content in text (Liu, 2015). Most previous work focuses on using rulebased methods or machine learning techniques for sentiment classification, which classifies reviews into different sentiment categories. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However"
J15-1002,baccianella-etal-2010-sentiwordnet,0,0.0149854,"e only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words in WordNet. They select seven positive words and seven negative words and expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentimen"
J15-1002,C10-1004,0,0.0343341,"Missing"
J15-1002,D10-1005,0,0.149484,"a. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Without sufficient features, it is difficult for these approaches to perform well in learning. Another line of cross-lingual sentiment classification uses Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants, like Boyd-Graber and Resnik (2010) or He, Alani, and Zhou (2010). These studies assume that each review is a mixture of sentiments and each sentiment is a probability over words. Then they apply the LDA-like approach to model the sentiment polarity of each review. Nonetheless, this assumption may not be applicable in sentiment lexicon learning because a single word can be regarded as the minimal semantic unit, and it is difficult, if not impossible, to infer the latent topics from a single word. Recall that different from the sentiment classification of product reviews where the instances are normally independent, words in sen"
J15-1002,P11-1061,0,0.0255781,"essary because it is relatively easy to collect from the Web. Consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment l"
J15-1002,P11-2075,0,0.0525109,"Missing"
J15-1002,esuli-sebastiani-2006-sentiwordnet,0,0.0560392,"Missing"
J15-1002,P11-2104,0,0.0862085,"ords to the words in the target language. The few existing approaches first build word relations between English and the target language. Then, based on the word relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph,"
J15-1002,P97-1023,0,0.0418632,"ely leverages the inter-language relations and both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the p"
J15-1002,W10-4116,0,0.0466903,"Missing"
J15-1002,C04-1200,0,0.955319,"mputing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Oku"
J15-1002,N06-1014,0,0.0196801,"Missing"
J15-1002,P11-1033,0,0.014246,"gle Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Wit"
J15-1002,P12-1060,1,0.855189,"They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task"
J15-1002,C12-2081,1,0.923795,"They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task"
J15-1002,P07-1123,0,0.137069,"expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentiment Lexicon Learning The work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translating the English sentiment words into Romanian through bilingual English–Romanian dictionaries. When confronting multiword translations, they translate the multiwords word by word. Then the validated translations must occur at least three times on the Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the"
J15-1002,J05-4003,0,0.0143604,"garded as negative. 4. Experiment 4.1 Data Sets We conduct experiments on Chinese sentiment lexicon learning. As in previous work (Baccianella, Esuli, and Sebastiani 2010), the sentiment words in General Inquirer lexicon are selected as the English seeds (Stone 1997). From the GI lexicon we collect 2,005 positive words and 1,635 negative words. To build the bilingual word graph, we adopt the Chinese–English parallel corpus, which is obtained from the news articles published by Xinhua News Agency in Chinese and English collections, using the automatic parallel sentence identification approach (Munteanu and Marcu 2005). Altogether, we collect more than 25M parallel sentence pairs in English and (of) and am) Chinese. We remove all the stopwords in Chinese and English (e.g., together with the low-frequency words that occur fewer than 5 times. After preprocessing, we finally have more than 174,000 English words, among which 3,519 words have sentiment labels and more than 146,000 Chinese words for which we need to predict the sentiment labels. To transfer sentiment information to Chinese unlabeled words more efficiently, we remove the unlabeled English words in the word graph (i.e., XU E = Φ). The unsupervised"
J15-1002,J11-1002,0,0.101363,"both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. Fo"
J15-1002,E09-1077,0,0.0937027,"n summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied. In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-English language (hereafter referred to as “the target language”) with the help of the available English sentiment lexicons. The underlying motivation of this task is to leverage the existing English sentiment lexicons and substantial linguistic resources to label the sentiment polarities of"
J15-1002,W03-0404,0,0.0971793,"ds or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. Their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) define several pattern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the"
J15-1002,W11-1704,0,0.104368,"ord relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph. The intra-language subgraphs are used to model the semantic relations among the w"
J15-1002,P05-1017,0,0.0288596,"gative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the"
J15-1002,P09-1027,0,0.0759673,"ian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for"
J15-1002,W03-1017,0,0.230886,"are from the Department of Computing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Tak"
J15-1002,D12-1127,0,\N,Missing
J15-1002,kamps-etal-2004-using,0,\N,Missing
J15-2004,W11-0705,0,0.0607269,"structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Na¨ıve Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Na¨ıve Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to t"
J15-2004,Q13-1005,0,0.00541006,"al of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the"
J15-2004,P14-1091,1,0.77232,"h iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the CYK algorithm that parses sentences in a bottom–up fashion. We use the log-linear model to score candidates generated by beam search. Instead of using question-answ"
J15-2004,P05-1022,0,0.023269,"ssification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The work presented in this article is close to traditional statistical parsing, as we borrow some algorithms to build the sentiment parser. Syntactic parsers are learned from the Treebank corpora, and find the most likely parse tree with the largest probability. In this article, we borrow some well-known techniques from syntactic parsing methods (Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005; ¨ Kubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free Grammar. These techniques are used to build the sentiment grammar and parsing model. They provide a natural way of defining the structure of sentiment trees and parse sentences to trees. The key difference lies in that our task is to calculate the polarity label of a sentence, instead of obtaining the parse tree. We only have sentencepolarity pairs as our training instances instead of annotated tree structures. Moreover, in the decoding process, our goal is to compute correct"
J15-2004,J07-2003,0,0.0181793,"represent the form of an inference rule as: (r) H1 . . . [i, X, j] HK (8) where, if all the terms r and Hk are true, then we can infer [i, X, j] as true. Here, r denotes a sentiment rule, and Hk denotes an item. When we refer to both rules and items, we employ the word terms. Theoretically, we can convert the sentiment rules to CNF versions, and then use the CYK algorithm to conduct parsing. Because the maximum number of non-terminal symbols in a rule is already restricted to two, we formulate the statistical sentiment parsing based on a customized CYK algorithm that is similar to the work of Chiang (2007). Let X, X1 , X2 represent the non-terminals N or P; the inference rules for the statistical sentiment parsing are summarized in Figure 3. 3.3 Ranking Model The parsing model generates many candidate parse trees T(s) for a sentence s. The goal of the ranking model is to score and rank these parse trees. The sentiment tree with the highest score is treated as the best representation for sentence s. We extract a feature vector φ(s, t) ∈ Rd for the specific sentence-tree pair (s, t), where t ∈ T(s) is the parse tree. Let ψ ∈ Rd be the parameter vector for the features. We use the log-linear model"
J15-2004,D08-1083,0,0.669511,"contrast] The negation expressions, intensification modifiers, and the contrastive conjunction can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the P"
J15-2004,D09-1062,0,0.0171966,"Missing"
J15-2004,P10-2050,0,0.0145972,"tect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) proposed using delicate written linguistic patterns as heuristic decision rules when computing the sentiment from individual words to phrases and finally to the sentence. The manually compiled rules were powerful enough to discriminate between the different sentiments in effective remedies (positive) / effective torture (negative), and in too colorful (negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a conditional random field model to calculate the sentiment of all the parsed elements in the depe"
J15-2004,W10-2903,0,0.0205801,"parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without a"
J15-2004,W10-3110,0,0.0535975,"Missing"
J15-2004,C10-2028,0,0.0707075,"Missing"
J15-2004,P10-1018,0,0.0204095,"Missing"
J15-2004,J99-4004,0,0.0826633,"In order to tackle the OOV problem, we treat a text span that consists of OOV words as empty text span, and derive them to E. The OOV text spans are combined with other text spans without considering their sentiment information. Finally, each sentence is derived to the symbol S using the start rules that are the beginnings of derivations. We can use the sentiment grammar to compactly describe the derivation process of a sentence. 3.2 Parsing Model We present the formal description of the statistical sentiment parsing model following deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999) as used in traditional syntactic parsing. For a concrete example, (A → BC) 274 [i, B, k] [i, A, j] [k, C, j] (6) Dong et al. A Statistical Parsing Framework for Sentiment Classification ∗ ∗ j ∗ which represents if we have the rule A → BC and B ⇒ wki and C ⇒ wk (⇒ is used to represent the reflexive and transitive closure of immediate derivation), then we can ∗ j obtain A ⇒ wi . By adding a unary rule j (A → w i ) [i, A, j] (7) with the binary rule in Equation (6), we can express the standard CYK algorithm for CFG in Chomsky Normal Form (CNF). And the goal is [0, S, n], in which S is the start"
J15-2004,P14-1022,0,0.0365465,"Missing"
J15-2004,P97-1023,0,0.045005,"rage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically. The sentiment dictionaries used for lexicon-based sentiment analysis can be created manually, or automatically using seed words to expand the list of words. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polar"
J15-2004,D07-1115,0,0.0357653,"akamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity stren"
J15-2004,kamps-etal-2004-using,0,0.0896409,"Missing"
J15-2004,W06-1642,0,0.0313257,"adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas"
J15-2004,P06-1115,0,0.0399416,"latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) pr"
J15-2004,P03-1054,0,0.0210265,"ey regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most previous methods are either rigid in terms of handcrafted rules, or sensitive to the performance of existing synt"
J15-2004,R09-1034,0,0.0605772,"Missing"
J15-2004,D12-1069,0,0.0126019,"ount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a n"
J15-2004,J13-2005,0,0.0371933,"Missing"
J15-2004,D09-1017,0,0.0285956,"Missing"
J15-2004,C12-2072,0,0.0397133,"Missing"
J15-2004,P11-1015,0,0.111172,"en widely recognized that sentiment expressions are colloquial and evolve over time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classifier. However, it is usually not easy to design effective features to build the classifier. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classification. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensification, contrast, and th"
J15-2004,J93-2004,0,0.0523085,"Missing"
J15-2004,P05-1012,0,0.100479,"Missing"
J15-2004,P07-1055,0,0.014092,"ated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually cra"
J15-2004,N10-1120,0,0.417582,"Missing"
J15-2004,P04-1035,0,0.1445,"r of classes in sentiment classification is smaller than that in topic-based text classification (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three machine learning methods to produce automated classifiers to generate class labels for movie reviews. They tested them on Na¨ıve Bayes, Maximum Entropy, and Support Vector Machine (SVM), and evaluated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina,"
J15-2004,P05-1015,0,0.976359,"over, the Stanford Sentiment Treebank5 contains polarity labels of all syntactically plausible phrases. In addition, we use the MPQA6 data set for the phrase-level task. We describe these data sets as follows. RT-C: 436,000 critic reviews from Rotten Tomatoes. It consists of 218,000 negative and 218,000 positive critic reviews. The average review length is 23.2 words. Critic reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to indicate the polarity, which we use directly as the polarity label of corresponding reviews. PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331 positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data set is widely used as the benchmark data set in the sentence-level polarity classification task. The data source is the same as RT-C. SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C. The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are extracted and annotated by workers from Amazon Mechanical Turk. The experimental settings of positive/negative classification for sentences are the same as in Socher et al. (2013). RT-U: 737,"
J15-2004,W02-1011,0,0.0223195,"Missing"
J15-2004,P06-2034,0,0.0277218,"s. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration,"
J15-2004,D12-1110,0,0.0833857,"Missing"
J15-2004,D11-1014,0,0.2693,"Missing"
J15-2004,D13-1170,0,0.308991,"egation for sentiment analysis. Some other methods model sentiment compositionality in the vector space. They regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most pre"
J15-2004,J11-2001,0,0.865765,"28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created manually by experts. I"
J15-2004,P11-2100,0,0.0312767,"Missing"
J15-2004,P05-1017,0,0.0398978,"Missing"
J15-2004,P12-2066,0,0.204703,"Missing"
J15-2004,P02-1053,0,0.0173161,"publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created"
J15-2004,J09-3003,0,0.0514213,"Missing"
J15-2004,D10-1102,0,0.041237,"Missing"
J15-2004,W03-1017,0,0.152178,"n this article. Regardless of what granularity the task is performed on, existing approaches deriving sentiment polarity from text fall into two major categories, namely, lexicon-based and learning-based approaches. The lexicon-based sentiment analysis uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incorporating intensification and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classif"
J15-2004,D07-1071,0,0.0408997,"t, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the"
J15-2004,P09-1110,0,0.026688,"Missing"
J15-2004,N10-1119,0,\N,Missing
J15-2004,P12-2018,0,\N,Missing
J15-2004,P11-1060,0,\N,Missing
J19-1005,D14-1179,0,0.00774847,"Missing"
J19-1005,N16-1108,0,0.0585039,"Missing"
J19-1005,C14-1088,0,0.024067,"ry and recent progress of chatbots, and application of text matching techniques in other tasks. Together with the review of existing work, we clarify the connection and difference between these works and our work in this article. 2.1 Chatbots Research on chatbots goes back to the 1960s when ELIZA (Weizenbaum 1966), an early chatbot, was designed with a large number of handcrafted templates and heuristic rules. 167 Computational Linguistics Volume 45, Number 1 ELIZA needs huge human effort but can only return limited responses. To remedy this, researchers have developed data-driven approaches (Higashinaka et al. 2014). The idea behind data-driven approaches is to build a chatbot with the large amount of conversation data available on social media such as forums and microblogging services. Methods along this line can be categorized into retrieval-based and generation-based ones. Generation-based chatbots reply to a message with natural language generation techniques. Early work (Ritter, Cherry, and Dolan 2011) regards messages and responses as source language and target language, respectively, and learn a phrase-based statistical machine translation model to translate a message to a response. Recently, toge"
J19-1005,D14-1181,0,0.00313027,"tenates the complement of each utterance with the last input; and in “combined,” s0 is the union of the other heuristics. Let vo = un in all heuristics, then the matching model of DL2R can be reformulated as mdl2r (s, r) = o X 0 (r)) MLP( fdl2r (vi )  fdl2r (vo )) · MLP( fdl2r (vi )  fdl2r (6) i=1 ~ v,1 , . . . , where MLP(· ) is a multi-layer perceptron. ∀v ∈ {v1 , . . . , vo }, suppose that {w ~ v,nv } represent embedding vectors of the words in v, then fdl2r (v) is given by w ~ v,1 , . . . , w ~ v,nv ) fdl2r (v) = CNN Bi-LSTM(w  (7) where CNN(· ) is a convolutional neural network (CNN) (Kim 2014) and Bi-LSTM(· ) is a bi-directional recurrent neural network with LSTM units (Bi-LSTM) (Graves, Mohamed, and Hinton 2013). The output of Bi-LSTM(· ) is all the hidden states of the Bi-LSTM 0 model. fdl2r (· ) is defined in the same way with fdl2r (· ). In DL2R, hdl2r (· ) can be viewed as an identity function on {fdl2r (v1 ), . . . , fdl2r (vo )}. Note that in the paper of Yan, Song, and Wu (2016), the authors also assume that each response candidate is associated with an antecedent posting p. This assumption does not always hold in multi-turn 1 We borrow the operator from MATLAB. 171 Computa"
J19-1005,N16-1014,0,0.190493,"such as flight booking, bus route enquiry, restaurant recommendation, and so forth; chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter, Cherry, and Dolan 2011). Building an open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage o"
J19-1005,P16-1094,0,0.129481,"such as flight booking, bus route enquiry, restaurant recommendation, and so forth; chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter, Cherry, and Dolan 2011). Building an open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage o"
J19-1005,D16-1127,0,0.163683,"such as flight booking, bus route enquiry, restaurant recommendation, and so forth; chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter, Cherry, and Dolan 2011). Building an open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage o"
J19-1005,D17-1230,0,0.0612794,"Missing"
J19-1005,P16-1098,0,0.0602627,"Missing"
J19-1005,D16-1176,0,0.045887,"Missing"
J19-1005,W15-4640,0,0.0590687,"he responses from the chatbot may drift to the topic of “Shanghai” if the chatbot pays significant attention to these words. Therefore, it is crucial yet non-trivial to let the chatbot understand the important points in the context and leverage them in matching and at the same time circumvent noise. Second, there is a clear dependency between Turn-5 and Turn-2 in the context, and the order of utterances matters in response selection because there will be different proper responses if we exchange Turn-3 and Turn-5. Existing work, including the recurrent neural network architectures proposed by Lowe et al. (2015), the deep learning to respond architecture proposed by Yan, Song, and Wu (2016), and the multi-view architecture proposed by Zhou et al. (2016), may lose important information in context-response matching because they follow the same paradigm to perform matching, which suffers clear drawbacks. In fact, although these models have different structures, they can be interpreted with a unified framework: A context and a response are first individually represented as vectors, and then their matching score is computed with the vectors. The context representation includes two layers. The first layer"
J19-1005,C16-1316,0,0.0795782,"ooking, bus route enquiry, restaurant recommendation, and so forth; chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter, Cherry, and Dolan 2011). Building an open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage of returning inform"
J19-1005,D16-1244,0,0.0684532,"Missing"
J19-1005,D14-1162,0,0.0842034,"Missing"
J19-1005,D11-1054,0,0.223411,"Missing"
J19-1005,P15-1152,0,0.148006,"Missing"
J19-1005,N15-1020,0,0.0241211,"to response generation; Li et al. (2016a) proposed a maximum mutual information objective to improve diversity of generated responses; Xing et al. (2017) and Mou et al. (2016) introduced external knowledge into the sequence-to-sequence model; Wu et al. (2018b) proposed decoding a response from a dynamic vocabulary; Li et al. (2016b) incorporated persona information into the sequence-to-sequence model to enhance response consistency with speakers; and Zhou et al. (2018) explored how to generate emotional responses with a memory augmented sequence-to-sequence model. In multi-turn conversation, Sordoni et al. (2015) compressed a context to a vector with a multi-layer perceptron in response generation; Serban et al. (2016) extended the sequence-to-sequence model to a hierarchical encoder-decoder structure; and under this structure, they further proposed two variants including VHRED (Serban et al. 2017b) and MrRNN (Serban et al. 2017a) to introduce latent and explicit variables into the generation process. Xing et al. (2018) exploited a hierarchical attention mechanism to highlight the effect of important words and utterances in generation. Upon these methods, reinforcement learning technique (Li et al. 20"
J19-1005,voorhees-tice-2000-trec,0,0.489297,"n candidates (Rn @k) as evaluation metrics. Here the matching models are required to return k most likely responses, and Rn @k = 1 if the true response is among the k candidates. Rn @k will become larger when k gets larger or n gets smaller. Rn @k has bias when there are multiple true candidates for a context. Hence, on the Douban corpus, apart from Rn @ks, we also followed the convention of information retrieval and used mean average precision (MAP) (Baeza-Yates, Ribeiro-Neto et al. 182 Wu et al. A Sequential Matching Framework for Retrieval-Based Chatbots 1999), mean reciprocal rank (MRR) (Voorhees and Tice 2000), and precision at position 1 (P@1) as evaluation metrics, which are defined as follows PNr MAP = 1 |S| X MRR = 1 |S| X P@1 = 1 |S| X AP(si ) , where AP(si ) = j=0 s i ∈S rel(rtop1 , si ) k=0 PNr rel(rk ,si ) j · rel(rj , si ) j=0 rel(rj , si ) s i ∈S RR(si ) , where RR(si ) = Pj 1 ranki (35) (36) (37) s i ∈S where ranki refers to the position of the first relevant response to context si in the ranking list; rj refers to the response ranked at the j-th position; rel(rj , si ) = 1 if rj is an appropriate response to context si , otherwise rel(rj , si ) = 0; rtop1 is the response ranked at the t"
J19-1005,P16-1122,0,0.065619,"Missing"
J19-1005,D13-1096,0,0.0265043,"naturally and meaningfully converse with humans on open domain topics (Ritter, Cherry, and Dolan 2011). Building an open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage of returning informative and fluent responses. Although most existing work on retrieval-based chatbots s"
J19-1005,N16-1170,0,0.0400905,"Missing"
J19-1005,P18-2067,1,0.863206,"open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage of returning informative and fluent responses. Although most existing work on retrieval-based chatbots studies response selection for single-turn conversation (Wang et al. 2013) in which conversation history is ignore"
J19-1005,P17-1046,1,0.592189,"and how they calculate the matching score with the two representations. The framework view unifies the existing models and indicates the common drawbacks they have: everything in the context is compressed to one or more fixed-length vectors before matching is conducted; and there is no interaction between the context and the response in the formation of their representations. The context is represented without enough supervision from the response, and so is the response. To overcome the drawbacks, we propose a sequential matching network (SMN) for context-response matching in our early work (Wu et al. 2017) where we construct Table 1 An example of multi-turn conversation. Turn-1 Turn-2 Turn-3 Turn-4 Turn-5 Context Human: How are you doing? ChatBot: I am going to hold a drum class in Shanghai. Anyone wants to join? The location is near Lujiazui. Human: Interesting! Do you have coaches who can help me practice drum? ChatBot: Of course. Human: Can I have a free first lesson? Response Candidates Response 1: Sure. Have you ever played drum before? X Response 2: What lessons do you want? 7 165 Computational Linguistics Volume 45, Number 1 a matching vector for each utterance–response pair through conv"
J19-1005,P15-1007,0,0.0606354,"Missing"
J19-1005,Q16-1019,0,0.0823372,"Missing"
J19-1005,D16-1036,0,0.133839,"2011). Building an open domain chatbot is challenging, because it requires the conversational engine to be capable of responding to any input from humans that covers a wide range of topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the Internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Vinyals and Le 2015; Li et al. 2016b; Mou et al. 2016; Serban et al. 2016; Xing et al. 2017) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Yan, Song, and Wu 2016; Zhou et al. 2016; Wu et al. 2018a). Generation-based methods generate responses with natural language generation models learned from conversation data, while retrieval-based methods re-use the existing responses by selecting proper ones from an index of the conversation data. In this work, we study the problem of response selection in retrieval-based chatbots, because retrieval-based chatbots have the advantage of returning informative and fluent responses. Although most existing work on retrieval-based chatbots studies response selection for single-turn conversation (Wang et al. 2013) in which conversation h"
J19-1005,P16-1044,0,\N,Missing
K18-1019,P16-1185,0,0.0373358,"Missing"
K18-1019,J07-2003,0,0.136275,"e whole training process. It confirms that our proposed approach not only stabilizes GAN training but also achieves better results. 31 30 29 BLEU 28 27 26 25 24 23 22 10000 20000 30000 40000 50000 60000 70000 80000 90000 Mini-Batches Figure 2: The BLEU score changes on IWSLT2014 German-English validation set for RNNSearch, Adversarial-NMT* and BGAN-NMT as training progresses. 4.3 We also conduct experiments on Chinese-English translation task with strong SMT and NMT baselines: HPSMT, RNNSearch and AdversarialNMT*. HPSMT is an in-house implementation of the hierarchical phrase-based MT system (Chiang, 2007), where a 4-gram language model is trained using the modified Kneser-Ney smoothing algorithm over the target data from bilingual data. Table 2 shows the evaluation results of different models on NIST datasets. All the results are reported based on case-insensitive BLEU. We can observe that RNNSearch significantly outperforms HPSMT by 4.78 BLEU points on average, and BGAN-NMT can further improve the performances, with 2.33 BLEU points on average. Additionally, our BGAN-NMT gains better performances than Adversarial-NMT* with 1.03 BLEU points on average. These experimental results confirm the ef"
K18-1019,N03-1017,0,0.124311,"Missing"
K18-1019,J82-2005,0,0.733933,"Missing"
K18-1019,D17-1230,0,0.0285585,"ndidates are used as positive and negative examples of discriminator training respectively. Due to the computation cost, we cannot generate many negative examples, so that the discriminator is easy to overfit. The overfitted discriminator will give biased signals to the generator and make it update incorrectly, leading to the instability of the generator training. Wu et al. (2017) found that combining adversarial training objective with MLE can significantly improve the stability of generator training, which is also reported in language model and neural dialogue generation (Lamb et al., 2016; Li et al., 2017). Actually, although this method leverages real translation signal to guide the generator and alleviate the effect of overfitted discriminator, it cannot deal with the inadequate training problem of the discriminator, which essentially plays a more important role in GAN training. cients α1 , α2 , ... , αT calculated with exp (a(ht , zi−1 )) αt = P k exp (a(hk , zi−1 )) (3) where a is a feed-forward neural network with a single hidden layer. 2.1.2 MLE Training NMT systems are usually trained to maximize the conditional log-probability of the correct translation given a source sentence with resp"
K18-1019,P15-1002,0,0.0274615,"optimized using the vanilla SGD algorithm with mini-batch 32 for De-En and 128 for Zh-En. We re-normalize gradients if their norm exceeds 2.0. The initial learning rate is set as 0.2 for De-En and 0.02 for Zh-En, and it is halved when BLEU scores on the validation set do not increase for 20,000 batches. To generate the synthetic bilingual data, beam search strategy with beam size 4 is adopted for both De-En and Zh-En. At test time, beam search is employed to find the best translation with beam size 8 and translation probabilities normalized by the length of the candidate translations. Follow Luong et al. (2015), <unk&gt; is replaced with the corresponding target word in a post processing step. Model Architecture RNNSearch model proposed by Bahdanau et al. (2014) is leveraged to be the translation model, but it should be noted that our BGAN-NMT is independent of the NMT network structure. We use a single layer GRU for encoder and decoder. For Zh-En, the size of word embedding (for both source and target words) is 256 and the size of hidden layer is set to 1024. For De-En, in order to compare with previous work (Ranzato et al., 2015; Bahdanau et al., 2016), the size of word embedding and GRU hidden state"
K18-1019,P02-1040,0,0.103721,"ess: the generator G can be improved with the discriminator D in GAN 1, and then the enhanced G serves as a better discriminator to guide 4 4.1 Experiments Setup To examine the effectiveness of our proposed approach, we conduct experiments on translation 194 Methods MIXER (Ranzato et al., 2015) MRT (Shen et al., 2016) BSO (Wiseman and Rush, 2016) Adversarial-NMT (Wu et al., 2017) A-C (Bahdanau et al., 2016) Softmax-Q (Ma et al., 2017) Adversarial-NMT* BGAN-NMT tasks with two language pairs: German-English (De-En for in short) and Chinese-English (Zh-En for in short). In all experiments, BLEU (Papineni et al., 2002) is adopted as the automatic metric for translation quality evaluation and computed using Moses multi-bleu.perl script. 4.1.1 Dataset Model 21.81 25.84 26.36 27.94 28.53 28.77 28.03 29.17 Table 1: Comparison with previous work on IWSLT2014 German-English translation task. The “Baseline” means the performance of pretrained model used to warmly start training. For German-English translation task, following previous work (Ranzato et al., 2015; Bahdanau et al., 2016), we select data from German-English machine translation track of IWSLT2014 evaluation tasks, which consists of sentence-aligned subt"
K18-1019,P16-1009,0,0.10016,"Missing"
K18-1019,P16-1159,0,0.384953,"the conditional log-probability of the correct translation given the source sentence. However, as argued in Bengio et al. (2015), the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that may be never observed in the training data. To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores (Bengio et al., 2015; Ranzato et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016). Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (Yang et al., 2017; Wu et al., 2017). Formally, GAN consists of two ”adversarial” models: a generator and a discriminator. In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence p"
K18-1019,D16-1137,0,0.33289,"g-probability of the correct translation given the source sentence. However, as argued in Bengio et al. (2015), the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that may be never observed in the training data. To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores (Bengio et al., 2015; Ranzato et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016). Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (Yang et al., 2017; Wu et al., 2017). Formally, GAN consists of two ”adversarial” models: a generator and a discriminator. In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence pair is real or generated."
K18-1019,N18-1122,0,0.04082,"Missing"
L18-1079,reschke-etal-2014-event,0,0.0969942,"n 2011 Tohoku earthquake and 869 Sanriku earthquake is expressed by the following text in 2011 Tohoku earthquake: infobox generation problem can be divided into three subtasks: event classification, event schema extraction and slot filling. As mentioned before, EventWiki can provide rich information for event classification and event schema extraction. Moreover, it is extremely useful for training a slot filling model for event extraction. Slot and value pairs in the infobox (intra-event information) in EventWiki can be used as weak (distant) supervision for training a slot filling model, as (Reschke et al., 2014) did. For example, for the slot value pair “magnitude: 9.0” in 2011 Tohoku earthquake, we first find out the sentences which “9.0” appears in. The context information of “9.0” can be used as features and “magnitude” is used as the label of “9.0” for training a slot filling model. 2.3.2. Event-event relation extraction and inference As slot filling for event extraction, we can also use interevent information in EventWiki to train an event-event relation extraction model using distant supervision strategy. For an event relation triple, we can find out the sentences that mention both events in th"
N07-2024,P97-1003,0,0.0241485,"ence translations, of Chinese and Arabic newspaper articles. Ent Entropy3 from a trigram language model trained on 4.4 million English sentences with the SRILM toolkit (Stolcke, 2002). The trigrams are intended to detect local mistakes. JLE (Japanese Learners of English Corpus) Transcripts of Japanese examinees in the Standard Speaking Test. False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al., 2003). The speaking style is more formal than spontaneous English, due to the examination setting. Parse Parse score from Model 2 of the statistical parser (Collins, 1997), normalized by the number of words. We hypothesize that nonnative sentences are more likely to receive lower scores. 3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task. For the ranking task, we utilize the ranking mode of SVM-Light. In this mode, the SVM algorithm is adapted for learning ranking functions, originally used for ranking web pages with respect to a 1 Except spelling mistakes, which we consider to be a separate problem that should be dealt with in a pre-processing step. 2 The nature of"
N07-2024,P01-1020,0,0.0307522,"eir writing. Classifying a sentence into discrete categories can be difficult: a sentence that seems fluent to one judge might not be good enough to another. An alternative is to rank sentences by their relative fluency. This would be useful when a non-native speaker is unsure which one of several possible ways of writing a sentence is the best. Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Com"
N07-2024,2005.eamt-1.15,0,0.0481903,"e into discrete categories can be difficult: a sentence that seems fluent to one judge might not be good enough to another. An alternative is to rank sentences by their relative fluency. This would be useful when a non-native speaker is unsure which one of several possible ways of writing a sentence is the best. Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Companion Volume, pages 93–9"
N07-2024,P06-1030,0,0.0136139,"s the best. Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Companion Volume, pages 93–96, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al.,"
N07-2024,P06-1031,0,0.0208375,"has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of er93 Proceedings of NAACL HLT 2007, Companion Volume, pages 93–96, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1 , and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 query (Joachims, 2002). In our context, given a set of English sent"
N07-2024,N01-1031,0,0.0319142,"07, Companion Volume, pages 93–96, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1 , and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 query (Joachims, 2002). In our context, given a set of English sentences with similar semantic content, say s1 , . . . , sn , and a ranking based on their fluency, the learning algorithm estimates the weights w ~ to satisfy the inequalities: w ~ · Φ(sj ) &gt; w ~ · Φ(sk ) where sj is more fluent than sk , and where Φ maps a sentence to a feature vector. This is in contrast to standard SVMs, which learn a hyperplane boundary between native and non-native sentences from the inequalities: Experimen"
N07-2024,P03-2026,0,\N,Missing
N07-2024,P06-1032,0,\N,Missing
N18-1141,D13-1160,0,0.066233,"ning when to regard generated questions as positive instances (collaborative) could improve the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks"
N18-1141,D17-1091,0,0.0227649,", question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algor"
N18-1141,P17-1123,0,0.0582539,"arning of question answering and question generation. Question answering (QA) and question generation (QG) are closely related natural language processing tasks. The goal of QA is to obtain an answer given a question. The goal of QG is almost reverse which is to generate a question from the answer. In this work, we consider answer selection (Yang et al., 2015; Balakrishnan et al., 2015) as the QA task, which assigns a numeric score to each candidate answer, and selects the top ranked one as the answer. We consider QG as a generation problem and exploit sequence-to-sequence learning (Seq2Seq) (Du et al., 2017; Zhou et al., 2017) as the backbone of the QG model. The key idea of this work is that QA and QG are two closely tasks and we seek to leverage the connection between these two tasks to improve both QA and QG. Our primary motivations are twofolds. On one hand, the Seq2Seq based QG model is trained by maximizing the literal similarity between the generated sentence and the ground truth sentence with maximum-likelihood estimation objective function (Du et al., 2017). However, there is no signal indicating whether or not the generated sentence could be correctly answered by the input. This proble"
N18-1141,D17-1090,1,0.740288,"g (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algorithm learns when to"
N18-1141,P16-1154,0,0.0765019,"Missing"
N18-1141,P16-1014,0,0.026262,"vector, header vector, and the cell vector. The backbone of the decoder is an attention based GRU RNN, which generates a word at each time step and repeats the process until generating the end-of-sentence symbol. We made two modifications to adapt the decoder to the table structure. The first modification is that the attention model is calculated over the headers, cells and the caption of a table. Ideally, the decoder should learn to focus on a region of the table when generating a word. The second modification is a table based copying mechanism. It has been proven that the copying mechanism (Gulcehre et al., 2016; Gu 1568 et al., 2016) is an effective way to replicate lowfrequent words from the source to the target sequence in sequence-to-sequence learning. In the decoding process, a word is generated either from the target vocabulary via standard sof tmax or from a table via the copy mechanism. A neural gate gt is used to trade-off between generating from the target vocabulary and copying from the table. The probability of generating a word y calculated as follows, where αt (y) is the attention probability of the word y from the table at time step t and βt (y) is the probability of predicting the wor"
N18-1141,P17-1019,0,0.0168286,"ion-answer pairs are correct and some are wrong. However, this kind of dataset is hard to obtain in most situations because of the lack of manual annotation efforts. From this perspective, the QA model could exactly benefit from the QG model through incorporating additional questionanswer pairs whose questions are automatically generated by the QG model1 . To achieve this goal, we present a training algorithm that improves the QA model and the 1 An alternative way is to automatically generate answers for each question. Solving the problem in this condition requires an answer generation model (He et al., 2017), which is out of the focus of this work. Our algorithm could also be adapted to this scenario. 1564 Proceedings of NAACL-HLT 2018, pages 1564–1574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics QG model in a loop. The QA model improves QG through introducing an additional QA-specific loss function, the objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating"
N18-1141,N03-1017,0,0.0990867,"A and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25 algorithm. WordCnt uses the number of co-occurred words in query-caption pair, query-header pair, and query-cell pair, respectively. MT based PP is a phrase-level feature. The features come from a phrase table which is extracted from bilingual corpus via statistical machine translation approach (Koehn et al., 2003). LambdaMART (Burges, 2010) is used to train the ranker. CNN uses convolutional neural network to measure the similarity between the query and table caption, table headers, and table cells, respectively. TQNN is the table-based QA model implemented in this work, which is regard as the baseline for the joint learning algorithm. Results of single systems are given in Table 1. We can see that BM25 is a simple yet very effective baseline method. Our basic model performs better than all the single models in terms of MAP. Method BM25 WordCnt MT based PP CNN TQNN (baseline) Seq2SeqPara GCN (competiti"
N18-1141,D16-1127,0,0.264999,"rks on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary tas"
N18-1141,P17-1103,0,0.0185062,"49M query-table pairs. An example of the data is given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity be"
N18-1141,P16-1170,0,0.0518732,"simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when t"
N18-1141,P02-1040,0,0.10243,"given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25"
N18-1141,D16-1264,0,0.0851505,"e the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from differe"
N18-1141,P17-1096,0,0.474587,"objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating additional training instances. Here the key problem is how to label the generated question-answer pair. The application of Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 2017) in this scenario regards every generated question-answer pair as a negative instance. On the contrary, Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) regards every generated question-answer pair appended with special domain tag as a positive instance. However, it is non-trivial to label the generated question-answer pairs because some of which are good paraphrases of the ground truth yet some might be negative instances with similar utterances. To address this, we bring in a collaboration detector, which takes two question-answer pairs as the input and determines their relation as collaborative or competitive. The output of the collaboration detector is regarded as the label of the generated questionanswer pair. We conduct experiments on b"
N18-1154,W12-3018,0,0.100628,"Missing"
N18-1154,P16-1185,0,0.0153548,"tion models have achieved the state-of-the-art performance. The typical training algorithm for sequence prediction is Maximum Likelihood Estimation ✓⇤ = argmax ✓ E (X,Y ⇤ )⇠D log p✓ (Y ⇤ |X) (1) Despite the popularity of MLE or teacher forcing (Doya, 1992) in neural sequence prediction tasks, two general issues are always haunting: 1). data sparsity and 2). tendency for overfitting, with which can both harm model generalization. To combat data sparsity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both"
N18-1154,P07-2045,0,0.00570763,"follows: 3.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 32.2 32.1 32 BLEU S(Y, Y ⇤ ) r log p⌘ (Y |Y ⇤ ) ⌧ X s(yt |y1:t 1 , Y ⇤ ) = · r log p⌘ (yt |y1:t ⌧ t 0 31.9 31.8 31.7 31.6 31.5 0 1, Y ⇤ 2 3 4 5 6 7 8 Epoch (Generator) ) (17) 1 Figure 5: Coaching GBN’s learning curve on IWSLT German-English Dev set. Machine Translation Dataset We follow Ranzato et al. (2015); Bahdanau et al. (2016) and select German-English machine translation track of the IWSLT 2014 evaluation campaign. The corpus contains sentencewise aligned subtitles of TED and TEDx talks. We use Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as lowercasing. The evaluation metric is BLEU (Papineni et al., 2002) computed via the multi-bleu.perl. System Setting We use a unified GRU-based RNN (Chung et al., 2014) for both the generator and the coaching bridge. In order to compare with existing papers, we use a similar system setting with 512 RNN hidden units and 256 as embedding size. We use attentive encoder-decoder to build our system (Bahdanau et al., 2014). During training, we apply ADADELTA (Zeiler, 2012) with ✏ = 10 6 and ⇢ = 0.95 to optimize parameters of the generator and the"
N18-1154,W04-1013,0,0.0147779,"h (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems and our proposed GBN can yield a further improvement. We also observe that LM GBN and coaching GBN have both achieved better performance than Uniform GBN, which confirms that better regularization effects are achieved, and the generators become more robust and generalize better. We draw the learning curve of both the bridge and the generator in Figure 5 to demonstrate how they coo"
N18-1154,P02-1040,0,0.103934,"(Y |Y ⇤ ) ⌧ X s(yt |y1:t 1 , Y ⇤ ) = · r log p⌘ (yt |y1:t ⌧ t 0 31.9 31.8 31.7 31.6 31.5 0 1, Y ⇤ 2 3 4 5 6 7 8 Epoch (Generator) ) (17) 1 Figure 5: Coaching GBN’s learning curve on IWSLT German-English Dev set. Machine Translation Dataset We follow Ranzato et al. (2015); Bahdanau et al. (2016) and select German-English machine translation track of the IWSLT 2014 evaluation campaign. The corpus contains sentencewise aligned subtitles of TED and TEDx talks. We use Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as lowercasing. The evaluation metric is BLEU (Papineni et al., 2002) computed via the multi-bleu.perl. System Setting We use a unified GRU-based RNN (Chung et al., 2014) for both the generator and the coaching bridge. In order to compare with existing papers, we use a similar system setting with 512 RNN hidden units and 256 as embedding size. We use attentive encoder-decoder to build our system (Bahdanau et al., 2014). During training, we apply ADADELTA (Zeiler, 2012) with ✏ = 10 6 and ⇢ = 0.95 to optimize parameters of the generator and the coaching bridge. During decoding, a beam size of 8 is used to approximate the full search space. An important hyper-para"
N18-1154,D15-1044,0,0.198506,"rized in Table 1. We can observe that our fine-tuned MLE baseline (29.10) is already over1711 RG-2 11.32 11.88 14.45 17.54 15.95 RG-L 26.42 26.96 30.71 33.63 31.68 34.10 16.70 31.75 34.32 16.88 31.89 34.49 16.70 31.95 34.83 16.83 32.25 35.26 17.22 32.67 Coaching GBN Learning Curve 83.5 83 ROUGE-2 RG-1 29.55 29.76 33.10 36.15 34.04 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems an"
N18-1154,P16-1009,0,0.0691058,"Missing"
N18-1154,D16-1137,0,0.0544207,"Missing"
N18-1154,P17-1139,0,0.0205442,"ity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both of the benefits from synthetic data and regularization. Within the architecture, the bridge module (bridge) first transforms the point-wise ground truth into a bridge distribution, which can be viewed as a target proposer from whom more target examples are drawn to train the generator. By introducing different constraints, the bridge can be set or trained to possess specific property, with which the drawn samples can augment target-side data (allevi"
N18-1154,D16-1160,0,0.0716227,"neural sequence prediction models have achieved the state-of-the-art performance. The typical training algorithm for sequence prediction is Maximum Likelihood Estimation ✓⇤ = argmax ✓ E (X,Y ⇤ )⇠D log p✓ (Y ⇤ |X) (1) Despite the popularity of MLE or teacher forcing (Doya, 1992) in neural sequence prediction tasks, two general issues are always haunting: 1). data sparsity and 2). tendency for overfitting, with which can both harm model generalization. To combat data sparsity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (G"
N18-1154,P17-1101,1,0.942479,"can observe that our fine-tuned MLE baseline (29.10) is already over1711 RG-2 11.32 11.88 14.45 17.54 15.95 RG-L 26.42 26.96 30.71 33.63 31.68 34.10 16.70 31.75 34.32 16.88 31.89 34.49 16.70 31.95 34.83 16.83 32.25 35.26 17.22 32.67 Coaching GBN Learning Curve 83.5 83 ROUGE-2 RG-1 29.55 29.76 33.10 36.15 34.04 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems and our proposed GBN c"
O01-2001,C96-2098,0,0.0606194,"Missing"
O01-2001,A94-1006,0,0.0478751,"Missing"
O01-2001,J94-4003,0,0.476138,"Missing"
O01-2001,P95-1032,0,0.0641052,"Missing"
O01-2001,P98-1069,0,0.0613736,"Missing"
O01-2001,J96-1001,0,\N,Missing
O01-2001,1994.amta-1.26,0,\N,Missing
O01-2001,C96-1040,0,\N,Missing
O01-2001,W97-0119,0,\N,Missing
O01-2001,J94-1002,0,\N,Missing
O01-2001,J93-2004,0,\N,Missing
O01-2001,W96-0208,0,\N,Missing
O01-2001,C90-2036,0,\N,Missing
O01-2001,J93-2003,0,\N,Missing
O01-2001,1995.tmi-1.28,0,\N,Missing
O01-2001,C94-2119,0,\N,Missing
O01-2001,C98-1066,0,\N,Missing
O01-2001,W00-1211,0,\N,Missing
O01-2001,C92-2101,0,\N,Missing
O01-2001,A88-1019,0,\N,Missing
O01-2001,C94-1079,0,\N,Missing
O01-2001,C00-2131,0,\N,Missing
O01-2001,W00-1212,1,\N,Missing
O01-2001,P98-2212,0,\N,Missing
O01-2001,C98-2207,0,\N,Missing
O01-2001,J90-2002,0,\N,Missing
O01-2001,P95-1026,0,\N,Missing
O01-2001,P95-1033,0,\N,Missing
O01-2001,P93-1001,0,\N,Missing
O01-2001,P91-1022,0,\N,Missing
O01-2001,W99-0617,0,\N,Missing
O01-2001,J92-4003,0,\N,Missing
O01-2001,P91-1037,0,\N,Missing
O01-2001,J97-2004,0,\N,Missing
O01-2001,J97-3002,0,\N,Missing
O01-2001,P00-1050,0,\N,Missing
O01-2001,P98-2127,0,\N,Missing
O01-2001,C98-2122,0,\N,Missing
O01-2001,P98-2139,0,\N,Missing
O01-2001,C98-2134,0,\N,Missing
O01-2001,W95-0106,0,\N,Missing
O01-2001,P93-1024,0,\N,Missing
O01-2004,P91-1022,0,0.082527,"Missing"
O01-2004,P93-1001,0,0.0434082,"Missing"
O01-2004,C94-2119,0,0.0380266,"Missing"
O01-2004,W09-4205,0,0.062288,"Missing"
O01-2004,C92-2101,0,0.061944,"Missing"
O01-2004,C00-1075,0,0.0265163,"Missing"
O01-2004,J93-2004,0,0.0354613,"Missing"
O01-2004,P98-2139,0,0.0592392,"Missing"
O01-2004,1997.tmi-1.13,0,0.107408,"Missing"
O01-2004,C96-1040,0,0.0598061,"Missing"
O01-2004,P98-2212,0,0.0382456,"Missing"
O01-2004,1993.tmi-1.25,0,0.14908,"Missing"
O01-2004,C00-2131,0,0.0446209,"Missing"
O01-2004,P95-1033,0,0.55593,"Missing"
O01-2004,J97-3002,0,0.330471,"Missing"
O01-2004,W00-1211,1,0.841686,"Missing"
O01-2004,W97-0119,0,\N,Missing
O01-2004,J93-2003,0,\N,Missing
O01-2004,1995.tmi-1.28,0,\N,Missing
O01-2004,J97-2004,0,\N,Missing
O01-2004,P00-1050,0,\N,Missing
O01-2004,W95-0106,0,\N,Missing
O03-5001,M95-1012,0,0.019977,"the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, howe"
O03-5001,M98-1014,0,0.0748286,"s and NE abbreviations. The rest of this paper is organized as follows: Section 2 briefly discusses related work. Section 3 presents in detail the class-based LM for Chinese NE identification. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 19"
O03-5001,W02-2002,0,0.0703239,"et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems s"
O03-5001,J92-4003,0,0.0859177,"Missing"
O03-5001,J95-4004,0,0.0190207,"sman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the"
O03-5001,W02-2004,0,0.0435105,"[Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Som"
O03-5001,M98-1017,0,0.55844,"3 presents in detail the class-based LM for Chinese NE identification. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collin"
O03-5001,W99-0613,0,0.130951,"Missing"
O03-5001,W02-2010,0,0.018642,"ikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly introduce two systems, namely, the rule-based NTU system for Chinese [Chen et al., 1998] and the machine learning"
O03-5001,M98-1020,0,0.0290977,"n. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002]"
O03-5001,O01-2002,1,0.856911,"Missing"
O03-5001,M95-1014,0,0.0121619,"g NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], c"
O03-5001,M98-1007,0,0.0931121,"Missing"
O03-5001,W02-2013,0,0.0298652,"d approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al.,"
O03-5001,M98-1015,0,0.0421973,"Missing"
O03-5001,W02-2020,0,0.0215295,"able. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly"
O03-5001,M98-1021,0,0.039939,"riefly discusses related work. Section 3 presents in detail the class-based LM for Chinese NE identification. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vas"
O03-5001,A97-1028,0,0.0754129,"Missing"
O03-5001,W02-2025,0,0.0140736,"proaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly introduce two systems, namely, the rule-based NTU system for Chi"
O03-5001,W98-1120,0,0.0327242,"lack et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to"
O03-5001,C02-1012,1,0.613407,"same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, ha"
O03-5001,W02-2035,0,0.0900186,"Missing"
O03-5001,M98-1016,0,0.0156729,"nsche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly introduce two systems, namely, the rule-based NTU system for Chinese [Chen et al., 1998] and the machine learning based BBN system [Bikel et al., 1999], because these are representative of the two different approaches. Generally speaking, the NTU system employs the rule-based method. It utilizes different types of information and models, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache model and n-gram model. Different kinds of NEs employ di"
O03-5001,M98-1004,0,\N,Missing
O03-5001,M98-1012,0,\N,Missing
O03-5001,dorr-etal-2000-chinese,0,\N,Missing
O03-5001,W98-0708,0,\N,Missing
O03-5001,chang-etal-1998-taxonomy,0,\N,Missing
O03-5001,J98-1005,0,\N,Missing
O03-5001,W98-0718,0,\N,Missing
O03-5001,W98-0709,0,\N,Missing
O03-5001,W98-0705,0,\N,Missing
O03-5001,W98-0704,0,\N,Missing
O03-5001,J93-1007,0,\N,Missing
O03-5001,W02-1817,0,\N,Missing
O03-5001,J01-1001,0,\N,Missing
O03-5001,C94-1101,0,\N,Missing
O03-5001,C92-2070,0,\N,Missing
O03-5001,P00-1064,0,\N,Missing
O03-5001,W00-1202,0,\N,Missing
O03-5001,W02-2029,0,\N,Missing
O03-5001,W02-2031,0,\N,Missing
O03-5001,C02-1080,0,\N,Missing
O03-5001,P97-1061,0,\N,Missing
O03-5001,P95-1026,0,\N,Missing
O03-5001,P99-1020,0,\N,Missing
O03-5001,W00-1318,0,\N,Missing
O03-5001,J94-4003,0,\N,Missing
O03-5001,P91-1019,0,\N,Missing
O03-5001,P02-1060,0,\N,Missing
O03-5001,C02-1044,0,\N,Missing
O03-5001,1996.amta-1.13,0,\N,Missing
O03-5001,P96-1006,0,\N,Missing
O03-5001,P95-1025,0,\N,Missing
O03-5001,C96-1089,0,\N,Missing
O03-5001,P98-1117,0,\N,Missing
O03-5001,C98-1113,0,\N,Missing
O03-5001,W95-0111,0,\N,Missing
O03-5001,M98-1009,0,\N,Missing
O03-5001,P02-1062,0,\N,Missing
P00-1015,J93-2004,0,0.0517215,"Missing"
P00-1015,A88-1019,0,\N,Missing
P00-1015,P99-1009,0,\N,Missing
P00-1015,P98-1034,0,\N,Missing
P00-1015,C98-1034,0,\N,Missing
P00-1015,P98-1010,0,\N,Missing
P00-1015,C98-1010,0,\N,Missing
P00-1032,J96-1003,0,\N,Missing
P00-1067,P93-1002,0,0.0604293,"Missing"
P00-1067,P91-1022,0,0.0208785,"Missing"
P00-1067,W93-0301,0,0.0648142,"Missing"
P00-1067,J93-2003,0,0.00789834,"Missing"
P00-1067,P93-1001,0,\N,Missing
P03-1016,P01-1008,0,0.168805,"Missing"
P03-1016,J93-2003,0,0.00566892,"Missing"
P03-1016,P98-2127,0,0.28292,"ap between the query space and the document space. For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents. Besides, the synonymous expressions are also important in language generation (Langkilde and Knight, 1998) and computer assisted authoring to produce vivid texts. Up to now, there have been few researches which directly address the problem of extracting synonymous collocations. However, a number of studies investigate the extraction of synonymous words from monolingual corpora (Carolyn et al., 1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al., 2001). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. In addition, some studies investigate the extraction of synonymous words and/or patterns from bilingual corpora (Barzilay and Mckeown, 2001; Shimohata and Sumita, 2002). However, these methods can only extract synonymous expressions which occur in the bilingual corpus. Due to the limited size of the bi"
P03-1016,shimohata-sumita-2002-automatic,0,0.0239679,"Missing"
P03-1016,O01-2001,1,0.862821,"Missing"
P03-1016,J96-1001,0,\N,Missing
P03-1016,J93-1007,0,\N,Missing
P03-1016,J96-1002,0,\N,Missing
P03-1016,P04-1022,1,\N,Missing
P03-1016,P98-1116,0,\N,Missing
P03-1016,C98-1112,0,\N,Missing
P03-1016,O04-2001,0,\N,Missing
P03-1016,O04-1027,0,\N,Missing
P03-1016,C02-1084,0,\N,Missing
P03-1016,E99-1005,0,\N,Missing
P03-1016,C98-2122,0,\N,Missing
P04-1022,2003.mtsummit-papers.14,0,0.104868,"Missing"
P04-1022,W02-0902,0,0.0181043,"thod exhibits promising results in selecting the right translation among several options provided by bilingual dictionary. Zhou et al.(2001) proposes a method to simulate translation probability with a cross language similarity score, which is estimated from monolingual corpora based on mutual information. The method achieves good results in word translation selection. In addition, (Dagan and Itai, 1994) and (Li, 2002) propose using two monolingual corpora for word sense disambiguation. (Fung, 1998) uses an IR approach to induce new word translations from comparable corpora. (Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. (Cao and Li, 2002) acquire noun phrase translations by making use of web data. (Wu and Zhou, 2003) also make full use of large scale monolingual corpora and limited bilingual corpora for synonymous collocation extraction. 3 Training a triple translation model from monolingual corpora In this section, we first describe the dependency correspondence assumption underlying our approach. Then a dependency triple translation model and the monolingual corpus based training algorithm are proposed. The obtained triple translation model will be us"
P04-1022,J96-1001,0,\N,Missing
P04-1022,W97-0311,0,\N,Missing
P04-1022,J90-1003,0,\N,Missing
P04-1022,J93-1007,0,\N,Missing
P04-1022,C00-2135,0,\N,Missing
P04-1022,J93-2003,0,\N,Missing
P04-1022,C02-1011,0,\N,Missing
P04-1022,P98-1069,0,\N,Missing
P04-1022,C98-1066,0,\N,Missing
P04-1022,P99-1067,0,\N,Missing
P04-1022,J94-4003,0,\N,Missing
P04-1022,J04-1001,0,\N,Missing
P04-1022,P03-1016,1,\N,Missing
P04-1022,P02-1044,0,\N,Missing
P04-1022,P93-1003,0,\N,Missing
P04-1022,thanopoulos-etal-2002-comparative,0,\N,Missing
P04-1022,P99-1041,0,\N,Missing
P04-1022,O01-2001,1,\N,Missing
P05-1062,P03-1035,0,0.0509336,"Missing"
P05-1062,N04-1042,0,0.03101,"Missing"
P05-1062,N04-4010,0,0.0222912,"Missing"
P05-1062,lavelli-etal-2004-critical,0,\N,Missing
P06-1062,J00-1004,0,0.02177,"Missing"
P06-1062,P91-1022,0,0.541924,"Missing"
P06-1062,J93-2003,0,0.0132601,"des in T F and T E ; K i and K j are the degrees of N iF and is combined with T[iE+1, j ] to be aligned with N Ej . The time complexity of the decoding algoF ( F E ) ( F Pr T [ m ,n ] T [i , j ] , A = Pr T [ m ,n ] N iE .TC[1, K ]T [iE+1, j ] , rithm is O( |T |× |T |×(degree(T ) + degree(T )) ) , where the degree of a tree is defined as the largest degree of its nodes. ) F F A where K is the degree of N iE . Finally, the node translation probability is modeled as Pr(NlF NjE ) ≈ Pr(NlF .l NiE .l)Pr(NlF .t NiE .t) . And ( ) the text translation probability Pr t F t E is model using IBM model I (Brown et al 1993). 4.2 Parameter Estimation Using Expectation-Maximization Our tree alignment model involves three categories of parameters: the text translation probability Pr t F t E , tag mapping probability Pr l l ' , and ( ( ) ) node deletion probability pd . Conventional parallel data released by LDC are used to train IBM model I for estimating the text translation probability Pr t F t E . One way to estimate ( ) Pr (l l ) and ' 5 E 2 E Aligning Sentences Using Tree Alignment Model To exploit the HTML structure similarities between parallel web documents, a cascaded approach is used in our sentence align"
P06-1062,P05-1074,0,0.00854072,"Missing"
P06-1062,P93-1002,0,0.101428,"Missing"
P06-1062,P93-1001,0,0.136988,"Missing"
P06-1062,1994.amta-1.11,0,0.0989515,"Missing"
P06-1062,1999.mtsummit-1.79,0,0.716737,"Missing"
P06-1062,P03-1058,0,0.0247213,"Missing"
P06-1062,P03-1010,0,0.107258,"Missing"
P06-1062,P94-1012,0,0.100897,"Missing"
P06-1062,J97-3002,0,0.0913856,"Missing"
P06-1062,P01-1067,0,0.0614498,"Missing"
P06-1062,moore-2002-fast,0,0.0195621,"Missing"
P06-1062,N04-1034,0,0.0584508,"Missing"
P06-1062,J03-3002,0,0.67772,"parallel sentences. 6 pd is to manually align nodes between parallel DOM trees, and use them as training corpora for maximum likelihood estimation. However, this is a very time-consuming and error-prone procedure. In this paper, the inside outside algorithm presented in (Lari and Young, 1990) is extended 1, K i pute the best alignment between N iF and N Ej . F T[m ,n ] , then Dynamic Programming for Decoding Web Document Model Pair Verification To verify whether a candidate web document pair is truly parallel, a binary maximum entropy based classifier is used. Following (Nie et al 1999) and (Resnik and Smith, 2003), three features are used: (i) file length ratio; (ii) HTML tag similarity; (iii) sentence alignment score. 493 The HTML tag similarity feature is computed as follows: all of the HTML tags of a given web page are extracted, and concatenated as a string. Then, a minimum edit distance between the two tag strings associated with the candidate pair is computed, and the HMTL tag similarity score is defined as the ratio of match operation number to the total operation number. The sentence alignment score is defined as the ratio of the number of aligned sentences and the total number of sentences in"
P06-1062,W90-0102,0,\N,Missing
P06-1062,C90-3045,0,\N,Missing
P06-1062,P91-1023,0,\N,Missing
P06-1062,J93-1006,0,\N,Missing
P06-1129,P98-2127,0,0.270785,"Missing"
P06-1129,P02-1038,0,0.0135806,"raining algorithm. We use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to learn the model parameter λs of the maximum entropy model. GIS training requires normalization over all possible prediction classes as shown in the denominator in equation (6). Since the potential number of correction candidates may be huge for multi-term queries, it would not be practical to perform the normalization over the entire search space. Instead, we use a method to approximate the sum over the n-best list (a list of most probable correction candidates). This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. 3.4.1 Features Features used in our maximum entropy model are classified into two categories I) baseline features and II) features supported by distributional similarity evidence. Below we list the feature templates. Category I: 1. Language model probability feature. This is the only real-valued feature with feature value set to the logarithm of source model probability: query term and its correction candidate are above certain thresholds; 4. Lexicon-based features, which are generated by checking whether a query t"
P06-1129,C90-2036,0,\N,Missing
P06-1129,P00-1037,0,\N,Missing
P06-1129,J96-1002,0,\N,Missing
P06-1129,P97-1008,0,\N,Missing
P06-1129,P02-1019,0,\N,Missing
P06-1129,H05-1120,0,\N,Missing
P06-1129,C98-2122,0,\N,Missing
P06-1129,W04-3238,0,\N,Missing
P06-1129,P99-1004,0,\N,Missing
P06-1136,P96-1041,0,0.0198835,"Missing"
P06-1136,C02-1026,0,0.148048,"Missing"
P06-1136,P02-1054,0,0.0778108,"Missing"
P06-1136,2001.mtsummit-papers.68,0,0.0126165,"likelihood of all training instances, given the bigram or biterm model: |INS | i =2 where OC stands for the language model of the ordered centroid and λ is the mixture weight combining the unigram and bigram (or biterm) probabilities. After taking logarithm and exponential for Equation (4), we get Equation (5).  log P(t1 |OC ) +    Score( A) = exp n (5)  ∑ log[λP(t i |OC ) + (1 − λ ) P(t i |t i −1 , OC )]  i=2  We observe that this formula penalizes verbose candidate answers. This can be alleviated by adding a brevity penalty, BP, which is inspired by machine translation evaluation (Papineni et al., 2001). λ∗ = arg max ∑ P (t1( j ) ...t l((jj)) |OC ) λ (11) j =1 |INS | l j  = arg max ∑ ∑ log λP (t i( j ) ) + (1 − λ ) P (t i( j ) |t i(−j1) )  λ j =1  i = 2  [ ] BP and P(t1) are ignored because they do not affect λ . λ can be estimated using EM iterative procedure: 1) Initialize λ to a random estimate between 0 and 1, i.e., 0.5; 2) Update λ using: λ ( r +1) = l |INS | λ ( r ) P (t i( j ) ) 1 1 j ×∑ (12) ∑ |INS |j =1 l j − 1 i = 2 λ ( r ) P (t i( j ) ) + (1 − λ ( r ) ) P (t i( j ) |t i(−j1) ) where INS denotes all training instances and |INS |gives the number of training instances which is"
P06-1136,P02-1006,0,0.00748848,"anguage model from the web; 2) adopting the language model to rerank candidate answers; 3) removing redundancies. Figure 1 shows five main modules. Learning ordered centroid: 1) Query expansion. Definitional questions are normally short (i.e., who is Bill Gates?). Query expansion is used to refine the query intention. First, reformulate query via simply adding clue words to the questions. i.e., for “Who is ...?” question, we add the word “biography”; and for “What is ...?” question, we add the word “is usually”, “refers to”, etc. We learn these clue words using the similar method proposed in (Ravichandran and Hovy, 2002). Second, query a web search engine (i.e., Google 4 ) with reformulated query and learn top-R (we empirically set R=5) most frequent co-occurring terms with the target from returned snippets as query expansion terms; 2) Learning centroid vector (profile). We query Google again with the target and expanded terms learned in the previous step, download top-N (we empirically set N=500 based on the tradeoff between the snippet number and the time complexity) snippets, and split snippets into sentences. Then, we retain the generated sentences that contain the target, denoted as W. Finally, learn top"
P06-1136,H01-1006,0,0.059265,"Missing"
P06-1136,P02-1040,0,\N,Missing
P07-1011,P04-1022,1,0.832643,"Missing"
P07-1011,W05-0904,0,0.0123293,"e ratio of the number of erroneous LCs to the number of collocations in each sentence. Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references"
P07-1011,P06-1032,0,0.446282,"high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which requires expert help to be recruited and is ti"
P07-1011,P06-1031,0,0.0698495,"al errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003;"
P07-1011,J93-2003,0,0.00558954,"pose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising. 1 Gao Cong Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. ∗ Work done while the author was a visiting student at MSRA † Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mis"
P07-1011,P98-1032,0,0.105684,"oyed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) to evaluate the well-formedness of machine translation outputs. The writers of ESL and MT normally make different mistakes:"
P07-1011,A00-2019,0,0.804432,"Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which req"
P07-1011,P97-1003,0,0.0417279,"rom Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references and machine outputs. In this paper, we calculate the densities of seven kinds of function w"
P07-1011,P01-1020,0,0.163624,"nd is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) to evaluate the well-formedness of machine translation outputs. The writers of ESL and MT normally make different mistakes: in general, ESL writers can write overall grammatically correct sentences with some local mistakes while MT outputs normally produce locally well-formed phrases with overall grammatically wrong sentences. Hence, the manual features designed for MT evaluation are not applicable to detect erroneous sentences from ESL learners. LSPs differ from the traditional sequential patterns, e.g., (Agrawal and Srikant, 1995; Pei et al., 2001) in that LSPs are atta"
P07-1011,2005.eamt-1.15,0,0.196679,"Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. ∗ Work done while the author was a visiting student at MSRA † Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), art"
P07-1011,H05-1006,0,0.137165,"use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contr"
P07-1011,C98-1032,0,\N,Missing
P07-1011,P03-2026,0,\N,Missing
P07-1091,koen-2004-pharaoh,0,0.0571564,"ssible reorderings of its children. The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases. The form of reordering rules, and the calculation of reordering probability for a particular node, can also be generalized easily.6 The only problem for the generalized reordering knowledge is that, as there are more classes, data sparseness becomes more severe. 6 The Decoder The last three sections explain how the S → n×S 0 part of formula 2 is done. The S 0 → T part is simply done by our re-implementation of PHARAOH (Koehn, 2004). Note that nonmonotonous translation is used here since the distance-based model is needed for local reordering. For the n×T → Tˆ part, the factors in consideration include the score of T returned by the decoder, and the reordering probability Pr (S → S 0 ). In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: exp(λr logPr (S → S 0 ) + X λi Fi (S 0 → T )) (3) i The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder. All t"
P07-1091,P03-1021,0,0.0573639,"r local reordering. For the n×T → Tˆ part, the factors in consideration include the score of T returned by the decoder, and the reordering probability Pr (S → S 0 ). In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: exp(λr logPr (S → S 0 ) + X λi Fi (S 0 → T )) (3) i The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder. All the feature weights (λs) were trained using our implementation of Minimum Error Rate Training (Och, 2003). The final translation Tˆ is the T with the highest total score. 5 Namely, N1 N2 N3 , N1 N3 N2 , N2 N1 N3 , N2 N3 N1 , N3 N1 N2 , and N3 N2 N1 , if the child nodes in the original order are N1 , N2 , and N3 . 6 For example, the reordering probability of a phrase p = p1 p2 p3 generated by a 3-ary node N is Pr (r)×Pr (pi1 )×Pr (pj2 )×Pr (pk3 ) where r is one of the six reordering patterns for 3-ary nodes. 724 It is observed in pilot experiments that, for a lot of long sentences containing several clauses, only one of the clauses is reordered. That is, our greedy reordering algorithm (c.f. secti"
P07-1091,P00-1056,0,0.055284,"set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translation table is the one for NIST MT-2005. The GIGAWORD corpus is used for training language model. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al., 2003). 7.2 The Preprocessing Module As mentioned in section 3, the preprocessing module for reordering needs a parser of the SL, a word alignment tool, and a Maximum Entropy training tool. We use the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar, the GIZA++ (Och and Ney, 2000) alignment package with its default settings, and the ME tool developed by (Zhang, 2004). Section 5 mentions that our reordering model can apply to nodes of any branching factor. It is interesting to know how many branching factors should be included. The distribution of parse tree nodes as shown in table 1 is based on the result of parsing the Chinese side of NIST MT-2002 test set by the Stanford parser. It is easily seen that the majority of parse tree nodes are binary ones. Nodes with more than 3 children seem to be negligible. The 3ary nodes occupy a certain proportion of the distribution,"
P07-1091,P02-1040,0,0.107922,") is trivial, but there is a subtle point about the calculation of language model score: the language model score of a translated clause is not independent from other clauses; it should take into account the last few words of the previous translated 0 clause. The best translated clause Tˆ(Ci ) is selected in step 3(a)(iii) by equation 3. In step 4 the best translation Tˆj is X arg max exp(λr logPr (S → Sj )+ Tj 0 score(T (Ci ))). i 7 Experiments 7.1 Corpora Our experiments are about Chinese-to-English translation. The NIST MT-2005 test data set is used for evaluation. (Case-sensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. The 7 IP stands for inflectional phrase and CP for complementizer phrase. These two types of phrases are clauses in terms of the Government and Binding Theory. Branching Factor Count Percentage 2 12294 73.41 3 3173 18.95 &gt;3 1280 7.64 Table 1: Distribution of Parse Tree Nodes with Different Branching Factors Note that nodes with only one child are excluded from the survey as reordering does not apply to such nodes. test set and development set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translatio"
P07-1091,P05-1034,0,0.0665473,"ot monotonous, since the distance-based model is needed for local reordering. Our second contribution is our definition of the best translation: arg max exp(λr logPr (S → S 0 )+ T X λi Fi (S 0 → T )) i 721 where Fi are the features in the standard phrasebased model and Pr (S → S 0 ) is our new feature, viz. the probability of reordering S as S 0 . The details of this model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to reordering. The most notab"
P07-1091,P06-1067,0,0.209315,"spect to English word order. Instead of relying on manual rules, (Xia and McCord, 2004) propose a method in learning patterns of rewriting SL sentences. This method parses training data and uses some heuristics to align SL phrases with TL ones. From such alignment it can extract rewriting patterns, of which the units are words and POSs. The learned rewriting rules are then applied to rewrite SL sentences before monotonous translation. Despite the encouraging results reported in these papers, the two attempts share the same shortcoming that their reordering is deterministic. As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. That is, the choice of reordering is independent from other translation factors, and once a reordering mistake is made, it cannot be corrected by the subsequent decoding. To overcome this weakness, we suggest a method to ‘soften’ the hard decisions in preprocessing. The essence is that our preprocessing module generates n-best S 0 s rather than merely one S 0 . A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reorder"
P07-1091,P05-1066,0,0.78591,"op half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks found by our decoder. and it achieves better MT performance on the basis of the standard phrase-based model. To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering. Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: 0 S→S →T (1) where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S 0 is translated as a TL sentence T by monotonous translation. Our first contribution is a new translation model as represented by formula 2: S → n × S 0 → n × T → Tˆ (2) where an n-best list of S 0 , instead of only one S 0 , is generated. The reason of such change will be given in section 2. Note also that the translation process S 0 →"
P07-1091,P03-1035,1,0.537454,"Missing"
P07-1091,P03-1054,0,0.0115851,"ey as reordering does not apply to such nodes. test set and development set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translation table is the one for NIST MT-2005. The GIGAWORD corpus is used for training language model. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al., 2003). 7.2 The Preprocessing Module As mentioned in section 3, the preprocessing module for reordering needs a parser of the SL, a word alignment tool, and a Maximum Entropy training tool. We use the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar, the GIZA++ (Och and Ney, 2000) alignment package with its default settings, and the ME tool developed by (Zhang, 2004). Section 5 mentions that our reordering model can apply to nodes of any branching factor. It is interesting to know how many branching factors should be included. The distribution of parse tree nodes as shown in table 1 is based on the result of parsing the Chinese side of NIST MT-2002 test set by the Stanford parser. It is easily seen that the majority of parse tree nodes are binary ones. Nodes with more than 3 children seem to be negligible"
P07-1091,N03-1017,0,0.0335242,"Missing"
P07-1091,N04-4026,0,0.135932,"his model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to reordering. The most notable ones are (Xia and McCord, 2004) and (Collins et al., 2005), both of which make use of linguistic syntax in the preprocessing stage. (Collins et al., 2005) analyze German clause structure and propose six types of rules for transforming German parse trees with respect to English word order. Instead of relying on manual rules, (Xia and McCord, 2004) propose a"
P07-1091,C04-1073,0,0.804395,"lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks found by our decoder. and it achieves better MT performance on the basis of the standard phrase-based model. To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering. Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: 0 S→S →T (1) where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S 0 is translated as a TL sentence T by monotonous translation. Our first contribution is a new translation model as represented by formula 2: S → n × S 0 → n × T → Tˆ (2) where an n-best list of S 0 , instead of only one S 0 , is generated. The reason of such change will be given in section 2. Note also that the tr"
P07-1091,P01-1067,0,0.351717,"anslation process S 0 → T is not monotonous, since the distance-based model is needed for local reordering. Our second contribution is our definition of the best translation: arg max exp(λr logPr (S → S 0 )+ T X λi Fi (S 0 → T )) i 721 where Fi are the features in the standard phrasebased model and Pr (S → S 0 ) is our new feature, viz. the probability of reordering S as S 0 . The details of this model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to r"
P07-1091,W06-1609,0,\N,Missing
P07-1091,W02-1039,0,\N,Missing
P07-1091,2005.iwslt-1.8,0,\N,Missing
P08-1011,C02-1126,0,0.0131409,"ed in a similar way as does with target features. The source head word feature is defined to be a function f4 to indicate whether a word ei is the source head word in English according to a parse tree of the source sentence. Similar to the definition of lexical features, we also use a set of features based on POS tags of source language. 3 3.1 Model Training and Application Training We parsed English and Chinese sentences to get training samples for measure word generation model. Based on the source syntax parse tree, for each measure word, we identified its head word by using a toolkit from (Chiang and Bikel, 2002) which can heuristically identify head words for sub-trees. For the bilingual corpus, we also perform word alignment to get correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we ne"
P08-1011,P05-1033,0,0.134325,"ls and indefinite articles are directly followed by countable nouns to denote the quantity of objects. Therefore, in the English-to-Chinese machine translation task we need to take additional efforts to generate the missing measure words in Chinese. For example, when translating the English phrase three books into the Chinese phrases “三本书”, where three corresponds to the numeral “三” and books corresponds to the noun “书”, the Chinese measure word “本” should be generated between the numeral and the noun. In most statistical machine translation (SMT) models (Och et al., 2004; Koehn et al., 2003; Chiang, 2005), some of measure words can be generated without modification or additional processing. For example, in above translation, the phrase translation table may suggest the word three be translated into “三”, “三本”, “三只”, etc, and the word books into “书”, “书本”, “名册” (scroll), etc. Then the SMT model selects the most likely combination “三本书” as the final translation result. In this example, a measure word candidate set consisting of “本” and “只” can be generated by bilingual phrases (or synchronous translation rules), and the best measure word “本” from the measure 2 1 There are some exceptional cases,"
P08-1011,N03-1017,0,0.0412262,"Missing"
P08-1011,P07-1017,0,0.0485184,"Missing"
P08-1011,P00-1056,0,0.0252377,"s and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in previous sections, we apply our measure word generation module into SMT output as a post-processing ste"
P08-1011,J04-4002,0,0.307392,"Missing"
P08-1011,P02-1040,0,0.0737285,"Missing"
P08-1011,N07-1051,0,0.0125043,"et correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in"
P08-1116,P05-1074,0,0.349233,"paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (20"
P08-1116,N03-1003,0,0.929129,"es that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using biling"
P08-1116,P01-1008,0,0.869425,"e more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphras"
P08-1116,J93-2003,0,0.00852245,"raphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T ∗ of a sentence S by finding: T ∗ = arg max{P (T |S)} model feature. λT M i and λLM are the weights of the feature functions. hT M i (T, S) is defined as: hT M i (T, S) = log Ki Y Scorei (Tk , Sk ) (3) k=1 where Ki is the number of phrase substitutes from S to T based on P Ti . Tk in T and Sk in S are phrasal paraphrases in P Ti . Scorei (Tk , Sk ) is the paraphrase likelihood according to P Ti 2 . A 5-gram language model is used, therefore: hLM (T, S) = log J Y p(tj |tj−4 , ..., tj−1 ) (4) j=1 where J is the length of T , tj is the j-th word of T ."
P08-1116,N06-1003,0,0.198695,"f the method varies greatly on different test sets and it performs best on the test set of news sentences, which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (19"
P08-1116,I05-5003,0,0.0339384,", 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a pars"
P08-1116,N06-1058,0,0.367526,", which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resour"
P08-1116,koen-2004-pharaoh,0,0.038836,"ights of the feature functions. hT M i (T, S) is defined as: hT M i (T, S) = log Ki Y Scorei (Tk , Sk ) (3) k=1 where Ki is the number of phrase substitutes from S to T based on P Ti . Tk in T and Sk in S are phrasal paraphrases in P Ti . Scorei (Tk , Sk ) is the paraphrase likelihood according to P Ti 2 . A 5-gram language model is used, therefore: hLM (T, S) = log J Y p(tj |tj−4 , ..., tj−1 ) (4) j=1 where J is the length of T , tj is the j-th word of T . 4 Exploiting Multiple Resources This section describes the extraction of phrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: T = arg max{P (S|T )P (T )} T (1) In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be eas"
P08-1116,P98-2127,0,0.511715,"ts before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especial"
P08-1116,W07-0716,0,0.260551,"rns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T ∗ of a sentence S by finding: T ∗ = arg max{P (T |S)} model feature. λ"
P08-1116,P00-1056,0,0.124901,"m the comparable corpus, parallel sentences are extracted. Let s1 and s2 be two sentences from comparable documents d1 and d2 , if their similarity based on word overlapping rate is above a threshold T h3 , s1 and s2 are identified as parallel sentences. In this way, 872,330 parallel sentence pairs are extracted. 5 http://people.csail.mit.edu/mcollins/code.html The context of a chunk is made up of 6 words around the chunk, 3 to the left and 3 to the right. 7 The similarity of two documents is computed using the vector space model and the word weights are based on tf·idf. 6 1024 We run Giza++ (Och and Ney, 2000) on the parallel sentences and then extract aligned phrases as described in (Koehn, 2004). The generated paraphrase table is pruned by keeping the top 20 paraphrases for each phrase. After pruning, 100,621 pairs of paraphrases are extracted. Given phrase p1 and its paraphrase p2 , we compute Score3 (p1 , p2 ) by relative frequency (Koehn et al., 2003): count(p2 , p1 ) Score3 (p1 , p2 ) = p(p2 |p1 ) = P 0 p0 count(p , p1 ) (7) People may wonder why we do not use the same method on the monolingual parallel and comparable corpora. This is mainly because the volumes of the two corpora differ a lot"
P08-1116,P02-1038,0,0.0245662,"hrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: T = arg max{P (S|T )P (T )} T (1) In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be easily combined in the loglinear model. Specifically, feature functions are derived from each paraphrase resource and then combined with the language model feature1 : ∗ T = arg max { T N X λT M i hT M i (T, S)+ i=1 λLM hLM (T, S)} (2) where N is the number of paraphrase tables. hT M i (T, S) is the feature function based on the ith paraphrase table P Ti . hLM (T, S) is the language 1 The reordering model is not considered in our model. 1023 Sim(e1 , e2 ) P = P (r,e)∈Tr (e1 )∩Tr (e2 ) (I(e1 , r, e) (r,e)∈Tr (e1 ) I(e1 , r, e) + P + I(e2 , r"
P08-1116,N03-1024,0,0.164374,"2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in paraphrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level"
P08-1116,I05-1011,0,0.0283694,"allison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in paraphrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang"
P08-1116,W04-3219,0,0.790425,"r contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora."
P08-1116,P02-1006,0,0.0452147,"ction 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been wide"
P08-1116,P03-1016,1,0.890346,"eanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation"
P08-1116,W97-0703,0,\N,Missing
P08-1116,N03-1017,0,\N,Missing
P08-1116,C98-2122,0,\N,Missing
P09-1066,C08-1005,0,0.0599862,"ngalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Stati"
P09-1066,P05-1033,0,0.476399,"e rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a loglinear model aims to"
P09-1066,D08-1011,0,0.382928,"sensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire sea"
P09-1066,2008.amta-srw.3,0,0.391125,"y, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Statistical Machine Translation (SMT) model while a majority of the space, within which there are many potentially good translations, is pruned away in decoding."
P09-1066,N04-1022,0,0.0804613,"explored in decoding. Experimental results on data sets for NIST Chinese-to-English machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders, and the outputs from co-decoding can be used to further improve the result of system combination. 1 Introduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated"
P09-1066,P06-1077,0,0.0243484,"m step 2 to step 4 until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search sp"
P09-1066,E06-1005,0,0.0485259,"oduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better a"
P09-1066,P02-1038,0,0.124274,"Missing"
P09-1066,P03-1021,0,0.0111584,"egation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences in ℋ? ? of all n-grams in e: ? −?+1 ?? + ?, ?′ = ?=1 ?? − ?, ? ′ is the n-gram disagreement measure function which is complementary to ?? + ?, ? ′ : ? −?+1 ?=1 Model Training We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. Let ?? be the feature weight vector for member decoder ?? , the training procedure proceeds as follows: 1. Choose initial values for ?1 , … , ?? 2. Perform co-decoding using all member decoders on a development set D with ?1 , … , ?? . For each decoder ?? , find a new feature weight vector ?′? which optimizes the specified evaluation criterion L on D using the MERT algorithm based on the n-best list ℋ? generated by ?? : ?′? = argmax? ? (?|?, ℋ? , ?)) where T denotes the translations selected by re-ranking the translations"
P09-1066,J04-4002,0,0.162557,"forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline model, the only change is hypothesis scoring. By rerunning a complete decoding process, member model can be applied to re-score all hypotheses explored by a decoder. Therefore step 3 can be viewed as full-scale hypothesis re-ranking because the re-ranking scope is beyond the limited n-best hypotheses currently cached in ℋ?"
P09-1066,W04-3250,0,0.67335,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,P08-1066,0,0.047141,"cal phrase-based decoder. Phrasal rules are extracted from all bilingual sentence pairs, while rules with variables are extracted only from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a BTG decoder with lexicalized reordering model based on maximum entropy principle as proposed by Xiong et al. (2006). We use all the bilingual data to extract phrases up to length 3. The third one (SYS3) is a string-to-dependency tree –based decoder as proposed by Shen et al. (2008). For rule extraction we use the same setting as in SYS1. We parsed the language model training data with Berkeley parser, and then trained a dependency language model based on the parsing output. All baseline decoders are extended with n-gram consensus –based co-decoding features to construct member decoders. By default, the beam size of 20 is used for all decoders in the experiments. We run two iterations of decoding for each member decoder, and hold the value of ? in Equation 5 as a constant 0.05, which is tuned on the test data of NIST 2004 Chinese-toEnglish machine translation task. 3.3 D"
P09-1066,koen-2004-pharaoh,0,0.617964,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,D08-1065,0,0.341537,"?′ ?? ?? (?, ?′) (4) ?′ ∈ℋ? ? where e is a translation of f by decoder ?? (? ≠ ?), ? ′ is a translation in ℋ? ? and ? ?′ ?? is the posterior probability of translation ? ′ determined by decoder ?? given source sentence f. ?? (?, ?′) is a consensus measure defined on e and ?′, by varying which different feature functions can be obtained. 587 Referring to the log-linear model formulation, the translation posterior ? ?′ ?? can be computed as: ? ?′ ?? = exp ??? ?′ ?′′ ∈ℋ? ? exp ??? ?′′ (5) 2.5 where ?? (∙) is the score function given in Equation 2, and ? is a scaling factor following the work of Tromble et al. (2008) To compute the consensus measures, we further decompose each ?? ?, ?′ into n-gram matching statistics between e and ?′. Here we do not discriminate among different lexical n-grams and are only concerned with statistics aggregation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences"
P09-1066,P07-2045,0,0.00862222,"Missing"
P09-1066,P06-1066,0,0.614687,"until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline"
P09-1066,W08-0329,0,\N,Missing
P09-1066,N07-1029,0,\N,Missing
P09-1098,J93-2003,0,0.0176455,"Missing"
P09-1098,2007.mtsummit-papers.9,0,0.543499,"pages are very common in non-English web sites, mining bilingual data from them should be an important task. However, as far as we know, there is no publication available on mining bilingual sentences directly from bilingual web pages. Most existing methods for mining bilingual sentences from the Web, such as (Nie et al., 1999; Resnik and Smith, 2003; Shi et al., 2006), try to mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment methods. As to mining term translations from bilingual web pages, Cao et al. (2007) and Lin et al. (2008) proposed two different methods to extract term translations based on the observation that authors of many bilingual web pages, especially those whose primary language is Chinese, Japanese or Korean, sometimes annotate terms with their English translations inside a pair of parentheses, like “c1c2...cn(e1 e2 ... em)” (c1c2...cn is a primary language term and e1 e2 ... em is its English translation). Actually, in addition to the parenthesis pattern, there is another interesting phenomenon that in many bilingual web pages bilingual data appear collectively and follow similar"
P09-1098,C92-1019,0,0.102668,"t pair being a TSP, we preprocess it via the following steps: a) b) c) d) Isolating the English and Chinese contents from their contexts in the bilingual snippet pair. Here, we use a very simple rule: in the English snippet, we regard all characters within (and including) the first and the last English letter in the snippet as the English content; similarly, in the Chinese snippet we regard all characters within (and including) the first and the last Chinese character in the snippet as the Chinese content; Word segmentation of the Chinese content. Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; Stop words filtering. We compiled a small list of stop words manually (for example, “of”, “to”, “的”, etc.) and remove them from the English and Chinese content; Stemming of the English content. We use an in-house stemming tool to get the uninflected form of all English words. After preprocessing, all English words form a collection E={e1,e2,…,em } and all Chinese words constitute a collection C={c1,c2,…,cn}, where ei is an English word, and ci is a Chinese word. We then use a linking algorithm which takes both translation and transliteration into consideratio"
P09-1098,C92-2082,0,0.00874367,"slations difficult. Moreover, based on the assumption that anchor texts in different languages referring to the same web page are possibly translations of each other, (Lu et al., 2004) propose a novel approach to construct a multilingual lexicon by making use of web anchor texts and their linking structure. However, since only famous web pages may have inner links from other pages in multiple languages, the number of translations that can be obtained with this method is limited. Pattern-based Relation Extraction Pattern-based relation extraction has also been studied for years. For instance, (Hearst, 1992; Finkelstein-Landau and Morin, 1999) proposed an iterative pattern learning method for extracting semantic relationships between terms. (Brin, 1998) proposed a method called DIPRE (Dual Iterative Pattern Relation Expansion) to extract a relation of books (author, title) pairs from the Web. Since translation can be regarded as a kind of relation, those ideas can be leveraged for extracting translation pairs. 3 Overview of the Proposed Approach Web pages Transliteration model Bilingual dictionary input depend Preprocessing Seed mining depend Pattern learning Translation pairs output Pattern-bas"
P09-1098,H05-1061,0,0.0180252,"ese, Japanese or Korean sometimes annotate terms with their English translations inside a pair of parentheses. Their methods are tested on a large set of web pages and achieve promising results. However, since not all translations in bilingual web pages follow the parenthesis pattern, these methods may miss a lot of translations appearing on the Web. Apart from mining term translations directly from bilingual web pages, more approaches have been proposed to mine term translations from text snippets returned by a web search engine (Jiang et al., 2007; Zhang and Vines, 2004; Cheng et al., 2004; Huang et al., 2005). In their methods the source language term is usually given and the goal is to find the target language translations from the Web. To obtain web pages containing the target translations, they submit the source term to the web search engine and collect returned snippets. Various techniques have been proposed to extract the target translations from the snippets. Though these methods achieve high accuracy, they are not suitable for compiling a large-scale bilingual dictionary for the following reasons: 1) they need a list of predefined source terms which is not easy to obtain; 2) the relevance r"
P09-1098,P08-1113,0,0.349364,"in non-English web sites, mining bilingual data from them should be an important task. However, as far as we know, there is no publication available on mining bilingual sentences directly from bilingual web pages. Most existing methods for mining bilingual sentences from the Web, such as (Nie et al., 1999; Resnik and Smith, 2003; Shi et al., 2006), try to mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment methods. As to mining term translations from bilingual web pages, Cao et al. (2007) and Lin et al. (2008) proposed two different methods to extract term translations based on the observation that authors of many bilingual web pages, especially those whose primary language is Chinese, Japanese or Korean, sometimes annotate terms with their English translations inside a pair of parentheses, like “c1c2...cn(e1 e2 ... em)” (c1c2...cn is a primary language term and e1 e2 ... em is its English translation). Actually, in addition to the parenthesis pattern, there is another interesting phenomenon that in many bilingual web pages bilingual data appear collectively and follow similar surface patterns. Fig"
P09-1098,J05-4003,0,0.117484,"tly from bilingual web pages. Most existing methods of mining bilingual sentences from the Web, such as (Nie et al., 1999; Resnik and Smith, 2003; Shi et al., 2006), mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment methods. However, since the number of bilingual web sites is quite small, these methods can not yield a large number of bilingual sentences. (Shi et al., 2006), mined a total of 1,069,423 pairs of English-Chinese parallel sentences. In addition to mining from parallel documents, (Munteanu and Marcu, 2005) proposed a method for discovering bilingual sentences in comparable corpora. As to the term translation extraction from bilingual web pages, (Cao et al., 2007) and (Lin et al., 2008) proposed two different methods utilizing the parenthesis pattern. The primary insight is that authors of many bilingual web pages, especially those whose primary language is Chinese, Japanese or Korean sometimes annotate terms with their English translations inside a pair of parentheses. Their methods are tested on a large set of web pages and achieve promising results. However, since not all translations in bili"
P09-1098,J03-3002,0,0.877903,"ist in non-English web sites. Most of them have a primary language (usually a non-English language) and a secondary language (usually English). The content in the secondary language is often the translation of some primary language text in the page. Since bilingual web pages are very common in non-English web sites, mining bilingual data from them should be an important task. However, as far as we know, there is no publication available on mining bilingual sentences directly from bilingual web pages. Most existing methods for mining bilingual sentences from the Web, such as (Nie et al., 1999; Resnik and Smith, 2003; Shi et al., 2006), try to mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment methods. As to mining term translations from bilingual web pages, Cao et al. (2007) and Lin et al. (2008) proposed two different methods to extract term translations based on the observation that authors of many bilingual web pages, especially those whose primary language is Chinese, Japanese or Korean, sometimes annotate terms with their English translations inside a pair of parentheses, like “c1c2...cn(e1 e2 ... e"
P09-1098,C04-1089,0,0.0713717,"Missing"
P09-1098,P06-1062,1,0.577386,"Missing"
P09-1098,P05-3010,0,\N,Missing
P09-1121,D08-1092,0,0.113058,"t the same time, ranking search results for the query Han Feizi (d dd) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these"
P09-1121,N09-1034,0,0.0268404,"y to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, machine translation can provide important predic"
P09-1121,W04-3207,0,0.0322795,"ng function constant. At the same time, ranking search results for the query Han Feizi (d dd) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can explo"
P09-1121,P08-1084,0,0.0325253,"search results for the query Han Feizi (d dd) is likely to be harder in English than in Chinese. A large portion of web queries have such properties that they are originated in a language different from the one they are searched. This variance in problem difficulty across languages is not unique to web search; it appears in a wide range of natural language processing problems. Much recent work on bilingual data has focused on exploiting these variations in difficulty to improve a variety of monolingual tasks, including parsing (Hwa et al., 2005; Smith and Smith, 2004; Burkett and Klein, 2008; Snyder and Barzilay, 2008), named entity recognition (Chang et al., 2009), and topic clustering (Wu and Oard, 2008). In this work, we exploit a similar intuition to improve monolingual web search. Our problem setting differs from cross-lingual web search, where the goal is to return machinetranslated results from one language in response to a query from another (Lavrenko et al., 2002). We operate under the assumption that for many monolingual English queries (e.g., Han Feizi), there exist good documents in English. If we have Chinese information as well, we can exploit it to help find these documents. As we will see, m"
P10-1033,H05-1011,0,0.0925595,"econd stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel,"
P10-1033,P06-1065,0,0.0535147,"ntence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posi320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by includin"
P10-1033,P06-2014,0,0.0182417,"that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the principles behind these tec"
P10-1033,W07-0403,0,0.0529252,"features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posi320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al. (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. It is interesting to see if DPDI can benefit the parsing of a more realistic ITG. HP-DITG extends Cherry and Lin‟s approach by not only employing simple phrase pairs but also hierarchical phrase pairs (Chiang, 2007). The grammar is enriched with rules of the format: ?  ?? /?? where ?? and ?? refer to the English and foreign side of the i-th (simple/hierarchical) phrase pair respectively. As example, if there is a simple phrase p"
P10-1033,C96-2141,0,0.52026,"for HP-DITG ID 8 Table 5 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null. 1 ? → ?|?|? 2 ? → ?? |?? |?? 3 ? → ?? |?? |?? ? → ?? |?? 4 ? → ?? |??? |??? 5 ? → ??? ??? 6 ?? → ?/? 7 ?? → ?/?; ?? → ?/? 8 ??? → ?? |??? ?? ; ??? 9 ??? → ??? ?? ; ??? → Table 4: Evaluation of DPDI against HMM, Giza++ and BITG Table 3 lists the word alignment time cost and SMT performance of different pruning methods. HP-DITG using DPDI achieves the best Bleu score with acceptable time cost. Table 4 compares HP-DITG to HMM (Vogel, et al., 1996), GIZA++ (Och and Ney, 2000) and BITG (Haghighi et al., 2009). It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score. Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant (Koehn, 2004). An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI. On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning"
P10-1033,2005.mtsummit-papers.33,0,0.0984013,"Missing"
P10-1033,J97-3002,0,0.682813,"ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et"
P10-1033,J07-2003,0,0.825163,"istent link ratio with the sum of the link‟s posterior probability. The DITG Models The discriminative ITG alignment can be conceived as a two-staged process. In the first stage DPDI selects good span pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which de"
P10-1033,P09-1104,0,0.474924,"all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the principles behind these techniques have certain con"
P10-1033,W05-1506,0,0.0332463,"y span pairs. It is empirically found to be alignment is done in a similar way to chart pars- highly harmful to alignment performance and ing (Wu, 1997). The base step applies all relevant therefore not adopted in this paper. The third type of pruning is equivalent to miterminal unary rules to establish the links of word pairs. The word pairs are then combined into nimizing the beam size of alignment hypotheses span pairs in all possible ways. Larger and larger in each hypernode. It is found to be well handled span pairs are recursively built until the sentence by the K-Best parsing method in Huang and Chiang (2005). That is, during the bottom-up pair is built. Figure 1(a) shows one possible derivation for a construction of the span pair repertoire, each span toy example sentence pair with three words in pair keeps only the best alignment hypothesis. each sentence. Each node (rectangle) represents a Once the complete parse tree is built, the k-best pair, marked with certain phrase category, of for- list of the topmost span is obtained by minimally eign span (F-span) and English span (E-span) expanding the list of alignment hypotheses of (the upper half of the rectangle) and the asso- minimal number of sp"
P10-1033,P00-1056,0,0.439865,"the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posi320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic e"
P10-1033,W01-1812,0,0.0499944,"Missing"
P10-1033,P05-1057,0,0.0363119,"an pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM"
P10-1033,J93-2003,0,\N,Missing
P10-1033,P05-1059,0,\N,Missing
P10-1033,P06-1097,0,\N,Missing
P10-1033,N09-1026,0,\N,Missing
P10-1033,P03-1021,0,\N,Missing
P10-1033,P08-1012,0,\N,Missing
P10-1033,W04-3250,0,\N,Missing
P10-2002,C08-1041,0,0.496023,"selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structu"
P10-2002,N03-1017,0,0.0859175,"ngzhou}@microsoft.com Abstract proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic infor"
P10-2002,D08-1010,0,0.286273,"othesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish"
P10-2002,P07-1089,0,0.0312973,"Missing"
P10-2002,P06-1077,0,0.112862,"ctical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model"
P10-2002,W06-1606,0,0.0274999,"with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into fou"
P10-2002,P08-1114,0,0.322519,"del in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the"
P10-2002,P08-1023,0,0.023425,"model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into four sub-models that"
P10-2002,P02-1040,0,0.0820023,"e instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to θ = 0.75 and ϕ = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 NIST 2006 0.3025 0.3061 0.3089 0.3141 As shown in Table 1, all the methods outperform the baseline because they have extra models to guide the hierarchical rule selection in some ways which might lead to better translation. Apparently, our method also performs better than the other two approaches, indicating that our method is more effective in the hierarchical rule selection as both source-side and target-side rules are selected together. 4.3 Effect of sub-models Due to the space"
P10-2002,N07-1051,0,0.0342946,"d 5. Length features, which are the length of sub-phrases covered by source nonterminals. 4 4.1 Experiments NIST 2008 0.2200 0.2254 0.2253 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p < 0.01) Experiment setting We implement a hierarchical phrase-based system similar to the Hiero (Chiang, 2005) and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser (Petrov and Klein, 2007). The ME training toolkit, developed by (Zhang, 2006), is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to θ = 0.75 and ϕ = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evalu"
P10-2002,J96-1002,0,0.0681035,"e the system performance significantly. 2 the monolingual source side of the training corpus. CFSM is used to capture how likely the sourceside rule is linguistically motivated or has the corresponding target-side counterpart. For CBSM, it can be naturally viewed as a classification problem where each distinct source-side rule is a single class. However, considering the huge number of classes may cause serious data sparseness problem and thereby degrade the classification accuracy, we approximate CBSM by a binary classification problem which can be solved by the maximum entropy (ME) approach (Berger et al., 1996) as follows: Ps (α|C) ≈ Ps (υ|α, C) P exp[ i λi hi (υ, α, C)] P =P 0 υ 0 exp[ i λi hi (υ , α, C)] Hierarchical Rule Selection Model Following (Chiang, 2005), hα, γi is used to represent a synchronous context free grammar (SCFG) rule extracted from the training corpus, where α and γ are the source-side and target-side rule respectively. Let C be the context of hα, γi. Formally, our joint probability model of hierarchical rule selection is described as follows: P (α, γ|C) = P (α|C)P (γ|α, C) where υ ∈ {0, 1} is the indicator whether the source-side rule is applied during decoding, υ = 1 when the"
P10-2002,P05-1034,0,0.0418459,"d can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods"
P10-2002,P05-1033,0,0.339463,"stitute of Technology, Harbin, China {cuilei,tjzhao}@mtlab.hit.edu.cn ‡ Microsoft Research Asia, Beijing, China {dozhang,muli,mingzhou}@microsoft.com Abstract proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more r"
P10-2002,N09-1025,0,0.0381803,"ilingual training corpus, among which we assume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table"
P10-2002,P08-1066,0,0.0351936,"The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into four sub-models that can be further clas"
P10-2002,P06-1121,0,0.115919,"rporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint"
P10-2002,D09-1008,0,0.0917229,"ume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marto"
P10-2002,W08-0302,0,0.0163984,"ed from multiple distinct sentence pairs in the bilingual training corpus, among which we assume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baselin"
P10-2002,P09-1037,0,0.121053,"didate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens"
P10-2002,J97-3002,0,0.020514,"edicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This feature interacts between source and target components since it sh"
P10-2002,P06-1066,0,0.0365012,"e selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research A"
P10-2002,P09-1036,0,0.177282,"-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the search in a larger sp"
P10-2002,P01-1067,0,0.112783,"s is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Lingu"
P10-2002,W04-3250,0,\N,Missing
P11-1016,H05-1045,0,0.0392263,"Missing"
P11-1016,C10-2028,0,0.835958,"Missing"
P11-1016,W06-0301,0,0.119207,"Missing"
P11-1016,H05-1066,0,0.0223227,"Missing"
P11-1016,P04-1035,0,0.0542744,"sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned above, these approaches work in a target-independent way, and so need to be adapted for target-dependent sentiment classification. 3 Approach Overview The problem we address in this paper is targetdependent sentiment classification of tweets. So the input of our task is a collection of tweets containing the target and the output is labels assigned to each of the tweets. Inspired by (Barbosa and Feng, 2010; Pang and Lee, 2004), we design a three-step approach in this paper: 1. Subjectivity classification as the first step to decide if the tweet is subjective or neutral about the target; 2. Polarity classification as the second step to decide if the tweet is positive or negative about the target if it is classified as subjective in Step 1; 3. Graph-based optimization as the third step to further boost the performance by taking the related tweets into consideration. In each of the first two steps, a binary SVM classifier is built to perform the classification. To train the classifiers, we use SVM-Light 6 with a linea"
P11-1016,W02-1011,0,0.0397246,"timent target as a query, and search for tweets containing positive or negative sentiments towards the target. The problem needing to be addressed can be formally named as Target-dependent Sentiment Classification of Tweets; namely, given a query, classifying the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem, such as (Go et al., 2009 5 ; Barbosa and Feng, 2010), basically follow (Pang et al., 2002), who utilize machine learning based classifiers for the sentiment classification of texts. However, their classifiers actually work in a target-independent way: all the features used in the classifiers are independent of the target, so the sentiment is decided no matter what the target is. Since (Pang et al., 2002) (or later research on sentiment classification Abstract Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positiv"
P11-1016,J01-4004,0,0.0118864,"ependent sentiment classification. In addition to the noun phrases including the target, we further expand the extended target set with the following three methods: 1. Adding mentions co-referring to the target as new extended targets. It is common that people use definite or demonstrative noun phrases or pronouns referring to the target in a tweet and express sentiments directly on them. For instance, in “Oh, Jon Stewart. How I love you so.”, the author expresses a positive sentiment to “you” which actually refers to “Jon Stewart”. By using a simple co-reference resolution tool adapted from (Soon et al., 2001), we add all the mentions referring to the target into the extended target set. 2. Identifying the top K nouns and noun phrases which have the strongest association with the target. Here, we use Pointwise Mutual Information (PMI) to measure the association. PMI ( w, t )  log   Target-dependent Features Target-dependent sentiment classification needs to distinguish the expressions describing the target from other expressions. In this paper, we rely on the syntactic parse tree to satisfy this need. Specifically, for any word stem wi in a tweet which has one of the following relations with the"
P11-1016,P02-1053,0,0.026769,"e Lakers, we can confidently classify this tweet as positive. The remainder of this paper is structured as follows. In Section 2, we briefly summarize related work. Section 3 gives an overview of our approach. We explain the target-dependent and contextaware approaches in detail in Sections 4 and 5 respectively. Experimental results are reported in Section 6 and Section 7 concludes our work. 2 Related Work In recent years, sentiment analysis (SA) has become a hot topic in the NLP research community. A lot of papers have been published on this topic. 2.1 2.3 Target-independent SA Specifically, Turney (2002) proposes an unsupervised method for classifying product or movie reviews as positive or negative. In this method, sentimental phrases are first selected from the reviews according to predefined part-of-speech patterns. Then the semantic orientation score of each phrase is calculated according to the mutual information values between the phrase and two predefined seed words. Finally, a review is classified based on the average semantic orientation of the sentimental phrases in the review. In contrast, (Pang et al., 2002) treat the sentiment classification of movie reviews simply as a special c"
P11-1016,H05-1044,0,0.209257,"Missing"
P11-1016,D12-1110,0,\N,Missing
P11-1016,J11-2001,0,\N,Missing
P11-1016,P03-1054,0,\N,Missing
P11-1016,C10-2005,0,\N,Missing
P11-1016,N10-1120,0,\N,Missing
P11-1016,P11-2008,0,\N,Missing
P11-1016,P97-1023,0,\N,Missing
P11-1037,J92-4003,0,0.149212,"d on instance re-weighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning a robust representation of the input from unlabeled data. Miller et al. (2004) use word clusters (Brown et al., 1992) learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. (2009) introduce Latent Semantic Association (LSA) for NER. In our pilot study of NER for tweets, we adopt bag-of-words models to represent a word in tweet, to concentrate our efforts on combining global evidence with local information and semi-supervised learning. We leave it to our future work to explore which is the best input representation for our task. 3 Task Definition We first introduce some background about tweets, then give a formal definition of the task. 3.1 The Tweets A tweet is a short text me"
P11-1037,D10-1098,0,0.0778136,"Missing"
P11-1037,W02-1001,0,0.23149,"Missing"
P11-1037,W10-0713,0,0.546486,"elines and that both the combination with KNN and the semi-supervised learning strategy are effective. The rest of our paper is organized as follows. In the next section, we introduce related work. In Section 3, we formally define the task and present the challenges. In Section 4, we detail our method. In Section 5, we evaluate our method. Finally, Section 6 concludes our work. 2 Related Work Related work can be roughly divided into three categories: NER on tweets, NER on non-tweets (e.g., news, bio-logical medicine, and clinical notes), and semi-supervised learning for NER. 2.1 NER on Tweets Finin et al. (2010) use Amazons Mechanical Turk service 2 and CrowdFlower 3 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausma"
P11-1037,D09-1015,0,0.00692716,"Missing"
P11-1037,P05-1045,0,0.0670203,"argmaxc (w ′ ,c′ )∈nb δ(c, c ) · cos(w,  w  ). 3: Calculate the labeling confidence cf : cf ∑ ′ (w  ,c ∑ ′ ′ ′ δ(c,c )·cos(w,  w  ) )∈nb ′ ′ (w  ,c )∈nb cos(w,  w ′) = = = . 4: return The predicted label c∗ and its confidence cf . model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN’s incapability to capture fine-grained evidence involving multiple decision points. The Linear CRF model is used as the fine model, with the following considerations: 1) It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. Algorithm 2 KNN Training. Require: Training tweets ts. 1: Initialize the classifier lk :lk = ∅. 2: for Each tweet t ∈ ts do 3: for Each word,label pair (w, c) ∈ t do 4: Get the feature vector w:  w  5: 6: 7: 8: = reprw"
P11-1037,N09-1032,0,0.0124944,"ave an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning a robust representation of the input from unlabeled data. Miller et al. (2004) use word clusters (Brown et al., 1992) learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. (2009) introduce Latent Semantic Association (LSA) for NER. In our pilot study of NER for tweets, we adopt bag-of-words models to represent a word in tweet, to concentrate our efforts on combining global evidence with local information and semi-supervised learning. We leave it to our future work to explore which is the best input representation for our task. 3 Task Definition We first introduce some background about tweets, then give a formal definition of the task. 3.1 The Tweets A tweet is a short text message containing no more than 140 characters in Twitter, the biggest micro-blog service. Here"
P11-1037,W02-1041,0,0.128934,"Missing"
P11-1037,P07-1034,0,0.0228947,"Inside and Outside of a chunk). In contrast to the above work, our study focuses on NER for tweets, a new genre of texts, which are short, noise prone and ungrammatical. 2.3 Semi-supervised Learning for NER Semi-supervised learning exploits both labeled and un-labeled data. It proves useful when labeled data is scarce and hard to construct while unlabeled data is abundant and easy to access. Bootstrapping is a typical semi-supervised learning method. It iteratively adds data that has been 361 confidently labeled but is also informative to its training set, which is used to re-train its model. Jiang and Zhai (2007) propose a balanced bootstrapping algorithm and successfully apply it to NER. Their method is based on instance re-weighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning"
P11-1037,P03-1054,0,0.00537236,"Missing"
P11-1037,P06-1141,0,0.201162,"Missing"
P11-1037,M98-1015,0,0.0194458,"nin et al. (2010) use Amazons Mechanical Turk service 2 and CrowdFlower 3 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausman (1998) use manual rules to extract entities of predefined types; Zhou and Ju (2002) adopt Hidden Markov Models (HMM) while Finkel et al. (2005) use CRF to train a sequential NE labeler, in which the BIO (meaning Beginning, the Inside and the Outside of 2 1 3 http://sourceforge.net/projects/opennlp/ 360 https://www.mturk.com/mturk/ http://crowdflower.com/ an entity, respectively) schema is applied. Other methods, such as classification based on Maximum Entropy models and sequential application of Perceptron or Winnow (Collins, 2002), are also practiced. The state-of-the-art system, e.g., the Stanford"
P11-1037,H05-1056,0,0.123381,"Missing"
P11-1037,W09-1119,0,0.899478,"ike an <PRODUCT >iphone</PRODUCT>without apps, <PERSON>Justin Bieber</PERSON>without his hair,<PERSON>Lady gaga</PERSON> without her telephone, it just wouldn...”, meaning that “iphone” is a product, while “Justin Bieber” and “Lady gaga” are persons. 4 Our Method NER task can be naturally divided into two subtasks, i.e., boundary detection and type classification. Following the common practice , we adopt a sequential labeling approach to jointly resolve these sub-tasks, i.e., for each word in the input tweet, a label is assigned to it, indicating both the boundary and entity type. Inspired by Ratinov and Roth (2009), we use the BILOU schema. Algorithm 1 outlines our method, where: trains and traink denote two machine learning processes to get the CRF labeler and the KNN classifier, respectively; reprw converts a word in a tweet into a bag-of-words vector; the reprt function transforms a tweet into a feature matrix that is later fed into the CRF model; the knn function predicts the class of a word; the update function applies the predicted class by KNN to the inputted tweet; the crf function conducts word level NE labeling;τ and γ represent the minimum labeling confidence of KNN and CRF, respectively, whi"
P11-1037,N10-1009,0,0.0111834,"Missing"
P11-1037,P08-1076,0,0.0926995,"Missing"
P11-1037,P09-3003,0,0.0814909,"b δ(c, c ) · cos(w,  w  ). 3: Calculate the labeling confidence cf : cf ∑ ′ (w  ,c ∑ ′ ′ ′ δ(c,c )·cos(w,  w  ) )∈nb ′ ′ (w  ,c )∈nb cos(w,  w ′) = = = . 4: return The predicted label c∗ and its confidence cf . model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN’s incapability to capture fine-grained evidence involving multiple decision points. The Linear CRF model is used as the fine model, with the following considerations: 1) It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. Algorithm 2 KNN Training. Require: Training tweets ts. 1: Initialize the classifier lk :lk = ∅. 2: for Each tweet t ∈ ts do 3: for Each word,label pair (w, c) ∈ t do 4: Get the feature vector w:  w  5: 6: 7: 8: = reprw (w, t). Add"
P11-1037,D09-1158,0,0.00983723,"t proves useful when labeled data is scarce and hard to construct while unlabeled data is abundant and easy to access. Bootstrapping is a typical semi-supervised learning method. It iteratively adds data that has been 361 confidently labeled but is also informative to its training set, which is used to re-train its model. Jiang and Zhai (2007) propose a balanced bootstrapping algorithm and successfully apply it to NER. Their method is based on instance re-weighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning a robust representation of the input from unlabeled data. Miller et al. (2004) use word clusters (Brown et al., 1992) learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. (2009) introduce Latent Semantic Association (LSA) for NER. In our"
P11-1037,W07-1033,0,0.00577536,"Missing"
P11-1037,W03-0434,0,0.0545152,"inear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. Algorithm 2 KNN Training. Require: Training tweets ts. 1: Initialize the classifier lk :lk = ∅. 2: for Each tweet t ∈ ts do 3: for Each word,label pair (w, c) ∈ t do 4: Get the feature vector w:  w  5: 6: 7: 8: = reprw (w, t). Add the w  and c pair to the classifier: lk = lk ∪ {(w,  c)}. end for end for return KNN classifier lk . 4.3 Features Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle (Zhang and Johnson, 2003), and extracts bag-ofword features from the window as features. For each word, our CRF model extracts similar features as Wang (2009) and Ratinov and Roth (2009), namely, orthographic features, lexical features and gazetteers related features. In our work, we use the gazetteers provided by Ratinov and Roth (2009). Two points are worth noting here. One is that before feature extraction for either the KNN or the CRF, stop words are removed. The stop words used here are mainly from a set of frequently-used words 6 . The other is that tweet meta data is normalized, that is, every link becomes *LIN"
P11-1037,P02-1060,0,0.065803,"Missing"
P11-1037,N04-1043,0,\N,Missing
P11-1037,W03-0430,0,\N,Missing
P11-1037,W03-0419,0,\N,Missing
P11-1037,M98-1004,0,\N,Missing
P11-1126,P05-1033,0,0.734658,"y the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest"
P11-1126,P10-1146,0,0.0227603,"ndent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus dec"
P11-1126,C10-2025,1,0.718798,"ures from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction brings high computational complexity. What’s more, partial hypotheses generated by confusion network decoding cannot be assigned exact feature values for future use in higher level decoding, and they only use feature values of 1-best hypothesis as an approximation. HM decoding, on the other hand, leverages a set of enriched features, which 1263 are computable for all the hypotheses gene"
P11-1126,P09-1064,0,0.0171445,"ms as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consi"
P11-1126,N10-1141,0,0.352458,"ft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from d"
P11-1126,C10-1036,1,0.617604,"oring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from different SMT systems"
P11-1126,P06-1121,0,0.0316772,"er itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the"
P11-1126,D08-1011,0,0.0195808,"demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram co"
P11-1126,W04-3250,0,0.0206599,"ontain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle.  DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dep"
P11-1126,N04-1022,0,0.0272922,"between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although the"
P11-1126,P09-1019,0,0.184738,"mbination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements ov"
P11-1126,P09-1066,1,0.802998,"ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hyp"
P11-1126,P09-1107,0,0.280871,"ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hyp"
P11-1126,P09-1065,0,0.0268883,"ty to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction b"
P11-1126,P03-1021,0,0.0152193,"ience: : the word count feature. 3) : the n-gram posterior feature of computed based on the mixture search space generated by the HM decoder: is the posterior probability of an n-gram in , is the posterior probability of one translation given based on . 4) 1) is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by , and 0 otherwise. 6) : the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces: not exist in equals to 1 when does , and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:  Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order.  Inverted rule . It combines translations of two consecuti"
P11-1126,J04-4002,0,0.0494598,"of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.co"
P11-1126,P02-1040,0,0.0823569,"newswire portions of the NIST 2006 (MT06) and 2008 (MT08) data sets. All bilingual corpora available for the NIST 2008 constrained data track of Chinese-to-English MT task are used as training data, which contain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 20"
P11-1126,P08-1066,0,0.154677,"set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations."
P11-1126,P07-1040,0,0.0207307,"ated techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or e"
P11-1126,D08-1065,0,0.0189733,"one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approa"
P11-1126,J97-3002,0,0.018326,"s the posterior probability of one translation given based on . 4) 1) is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by , and 0 otherwise. 6) : the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces: not exist in equals to 1 when does , and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:  Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order.  Inverted rule . It combines translations of two consecutive blocks into a single larger block in an inverted order. These two rules are used bottom-up until the whole source sentence is fully covered. We use two reordering rule penalty features,"
P11-1126,P06-1066,0,0.0212564,"ni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle.  DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dependency trees. A target dependency language model is used as an additional feature. Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same selected data set as well. A trigram dependency language model used in DHPB is trained with the outputs from"
P11-4008,P09-1098,1,0.919544,"s include iCiba 4 and Lingoes 5 . The second depends mainly on mined bilingual term/sentence pairs, e.g., Youdao 6 . In contrast to those services, our system has a higher recall and fresher results, unique search functions (e.g., fuzzy POS-based search, classifier filtering), and an integrated language learning experience (e.g., translation with interactive word alignment, and photorealistic lip-synced video tutors). Bilingual Corpus Mining and Postprocessing. Shi et al. (2006) uses document object model (DOM) tree mapping to extract bilingual sentence pairs from aligned bilingual web pages. Jiang et al. (2009b) exploits collective patterns to extract bilingual term/sentence pairs from one web page. Liu et al. (2010) proposes training a SVM-based classifier with multiple linguistic features to evaluate the quality of mined corpora. Some methods are proposed to detect/correct errors in English (Liu et al., 2010; Sun et al., 2007). Following this line of work, Engkoo implements its mining pipeline with a focus on robustness and speed, and is designed to work on a very large volume of web pages. 3 System Description In this section, we first present the architecture followed by a discussion of the bas"
P11-4008,P09-1066,1,0.752002,"Missing"
P11-4008,D10-1104,1,0.876218,"ao 6 . In contrast to those services, our system has a higher recall and fresher results, unique search functions (e.g., fuzzy POS-based search, classifier filtering), and an integrated language learning experience (e.g., translation with interactive word alignment, and photorealistic lip-synced video tutors). Bilingual Corpus Mining and Postprocessing. Shi et al. (2006) uses document object model (DOM) tree mapping to extract bilingual sentence pairs from aligned bilingual web pages. Jiang et al. (2009b) exploits collective patterns to extract bilingual term/sentence pairs from one web page. Liu et al. (2010) proposes training a SVM-based classifier with multiple linguistic features to evaluate the quality of mined corpora. Some methods are proposed to detect/correct errors in English (Liu et al., 2010; Sun et al., 2007). Following this line of work, Engkoo implements its mining pipeline with a focus on robustness and speed, and is designed to work on a very large volume of web pages. 3 System Description In this section, we first present the architecture followed by a discussion of the basic components; we 2 http://oxforddictionaries.com http://www.ldoceonline.com/ 4 http://dict.en.iciba.com/ 5 h"
P11-4008,P00-1056,0,0.0697466,"Missing"
P11-4008,P06-1062,1,0.931296,"ries edited by experts, e.g., Oxford dictionaries 2 and Longman contemporary English dictionary 3 . Examples of these kinds of services include iCiba 4 and Lingoes 5 . The second depends mainly on mined bilingual term/sentence pairs, e.g., Youdao 6 . In contrast to those services, our system has a higher recall and fresher results, unique search functions (e.g., fuzzy POS-based search, classifier filtering), and an integrated language learning experience (e.g., translation with interactive word alignment, and photorealistic lip-synced video tutors). Bilingual Corpus Mining and Postprocessing. Shi et al. (2006) uses document object model (DOM) tree mapping to extract bilingual sentence pairs from aligned bilingual web pages. Jiang et al. (2009b) exploits collective patterns to extract bilingual term/sentence pairs from one web page. Liu et al. (2010) proposes training a SVM-based classifier with multiple linguistic features to evaluate the quality of mined corpora. Some methods are proposed to detect/correct errors in English (Liu et al., 2010; Sun et al., 2007). Following this line of work, Engkoo implements its mining pipeline with a focus on robustness and speed, and is designed to work on a very"
P11-4008,P07-1011,1,0.908757,"e.g., translation with interactive word alignment, and photorealistic lip-synced video tutors). Bilingual Corpus Mining and Postprocessing. Shi et al. (2006) uses document object model (DOM) tree mapping to extract bilingual sentence pairs from aligned bilingual web pages. Jiang et al. (2009b) exploits collective patterns to extract bilingual term/sentence pairs from one web page. Liu et al. (2010) proposes training a SVM-based classifier with multiple linguistic features to evaluate the quality of mined corpora. Some methods are proposed to detect/correct errors in English (Liu et al., 2010; Sun et al., 2007). Following this line of work, Engkoo implements its mining pipeline with a focus on robustness and speed, and is designed to work on a very large volume of web pages. 3 System Description In this section, we first present the architecture followed by a discussion of the basic components; we 2 http://oxforddictionaries.com http://www.ldoceonline.com/ 4 http://dict.en.iciba.com/ 5 http://www.lingoes.cn/ 6 http://dict.youdao.com 3 45 Figure 1: System architecture of Engkoo. then demonstrate the main scenarios. 3.1 System Overview Figure 1 presents the architecture of Engkoo. It can be seen that"
P12-1032,N09-1014,0,0.194275,"ve hundred dollars tea ? Src 我 想要 五百 元 以下 的 茶 . Ref I would like some tea under five hundred dollars . Best1 I would like tea under five hundred dollars . Figure 1. Two sentences from IWSLT (Chinese to English) data set. ""Src"" stands for the source sentence, and ""Ref"" means the reference sentence. ""Best1"" is the final output of the decoder. the most similar translation examples from translation memory (TM) systems (Ma et al., 2011). A classifier is applied to re-rank the n-best output of a decoder, taking as features the information about the agreement with those similar translation examples. Alexandrescu and Kirchhoff (2009) proposed a graph-based semi-supervised model to re-rank n-best translation output. Note that these two attempts are about translation consensus for similar sentences, and about reranking of n-best output. It is still an open question whether translation consensus for similar sentences/spans can be applied to the decoding process. Moreover, the method in Alexandrescu and Kirchhoff (2009) is formulated as a typical and simple label propagation, which leads to very large graph, thus making learning and search inefficient. (c.f. Section 3.) In this paper, we attempt to leverage translation consen"
P12-1032,P09-1064,0,0.0405537,"Missing"
P12-1032,N10-1141,0,0.0239482,"Missing"
P12-1032,C10-1036,1,0.814223,"Missing"
P12-1032,W04-3250,0,0.137364,"Missing"
P12-1032,N04-1022,0,0.0389813,"just a span of it, whether the candidate is the same as or similar to the supporting candidates, and whether the supporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multip"
P12-1032,P09-1019,0,0.0161674,"upporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for the same (span of) source"
P12-1032,P09-1066,1,0.837826,"Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for the same (span of) source sentence. It should be noted that consensus among translations of similar source sentences/spans is also helpful for good candidate selection. Consider the examples in Figure 1. For the source (Chine"
P12-1032,P06-1096,0,0.0822772,"Missing"
P12-1032,P03-1021,0,0.104259,"eatures and feature weights in the log-linear model. Algorithm 1 Semi-Supervised Learning 0; λ = , , ; while not converged do , , , . λ , , end while return last ( , λ ) e1 a1 m n Algorithm 1 outlines our semi-supervised method for such alternative training. The entire process starts with a decoder without consensus features. Then a graph is constructed out of all training, dev, and test data. The subsequent structured label propagation provides feature values to the MT decoder. The decoder then adds the new features and re-trains all the feature weights by Minimum Error Rate Training (MERT) (Och, 2003). The decoder with new feature weights then provides new n-best candidates and their posteriors for constructing another consensus graph, which in turn gives rise to next round of 306 0.75 1, f1 b c d1 2, f1 d1 b c 3, f2 d1 b c 0.5 e1 a1 b n e1 d 1 b n Figure 2. A toy graph constructed for re-ranking. MERT. This alternation of structured label propagation and MERT stops when the BLEU score on dev data converges, or a pre-set limit (10 rounds) is reached. 5 Graph Construction A technical detail is still needed to complete the description of graph-based consensus, namely, how the actual consensu"
P12-1032,P02-1040,0,0.0822517,"Missing"
P12-1032,D08-1065,0,0.0157186,"didates, and whether the supporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for th"
P12-1032,J97-3002,0,0.317878,"Missing"
P12-1032,P10-1049,0,0.0144735,"urce span. It is not difficult to handle test nodes, since the purpose of MT decoder is to get all possible segmentations of a source sentence in dev/test data, search for the translation candidates of each source span, and calculate the probabilities of the candidates. Therefore, the cells in the search space of a decoder can be directly mapped as test nodes in the graph. Training nodes can be handled similarly, by applying forced alignment. Forced alignment performs phrase segmentation and alignment of each sentence pair of the training data using the full translation system as in decoding (Wuebker et al., 2010). In simpler term, for each sentence pair in training data, a decoder is applied to the source side, and all the translation candidates that do not match any substring of the target side are deleted. The cells of in such a reduced search space of the decoder can be directly mapped as training nodes in the graph, just as in the case of test nodes. Note that, due to pruning in both decoding and translation model training, forced alignment may fail, i.e. the decoder may not be able to produce target side of a sentence pair. In such case we still map the cells in the search space as training nodes"
P12-1032,P06-1066,0,0.0697401,"Missing"
P12-1032,P11-1124,0,\N,Missing
P12-1055,D10-1098,0,0.0503916,"Missing"
P12-1055,W05-1303,0,0.0825989,"ons 2 . As an illustrative example, we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets within five days. 526 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics by recognition errors. Another challenge of NEN is the dearth of information in a single t"
P12-1055,D07-1074,0,0.0414343,"y kappa is 0.72. Any inconsistent case is discussed by the two annotators till a consensus is reached. 2, 245 tweets are used for development, and the remainder are used for 5-fold cross validation. 5.2 Evaluation Metrics We adopt the widely-used Precision, Recall and F1 to measure the performance of NER for a particular type of entity, and the average Precision, Recall and F1 to measure the overall performance of NER (Liu et al., 2011; Ritter et al., 2011). As for NEN, we adopt the widely-used Accuracy, i.e., to what percentage the outputted canonical forms are correct (Jijkoun et al., 2008; Cucerzan, 2007; Li et al., 2002). 12 Two native English speakers. 532 5.3 Baseline Tables 1- 2 show the overall performance of the baseline and ours (denoted by SRN ). It can be seen that, our method yields a significantly higher F1 (with p < 0.01) than SBR , and a moderate improvement of accuracy as compared with SBN (with p < 0.05). As a case study, we show that our system successfully identified “jaxon11 ” as a PERSON in the tweet “· · · come to see jaxon11 someday· · · ”, which is mistakenly labeled as a LOCATION by SBR . This is largely owing to the fact that our system aligns “jaxon11 ” with “Jaxson12"
P12-1055,I11-1095,0,0.0222717,"we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets within five days. 526 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics by recognition errors. Another challenge of NEN is the dearth of information in a single tweet, due to the short and noise-prone na"
P12-1055,W10-0713,0,0.104261,"Missing"
P12-1055,D09-1015,0,0.00911895,"otated data set, and show that our method compares favorably with the baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding sc"
P12-1055,W06-1643,0,0.0711415,"Missing"
P12-1055,P11-1038,0,0.0565987,"Missing"
P12-1055,W02-1041,0,0.0298758,"baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside a"
P12-1055,M98-1015,0,0.0119543,"m each other. 2. We evaluate our method on a human annotated data set, and show that our method compares favorably with the baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findin"
P12-1055,C02-1127,0,0.0487485,"ge of 3.3 variations 2 . As an illustrative example, we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets within five days. 526 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics by recognition errors. Another challenge of NEN is the dearth of information"
P12-1055,P11-1037,1,0.398518,"ss than 140 characters shared through the Twitter service 1 , have become an important source of fresh information. As a result, the task of named entity recognition (NER) for tweets, which aims to identify mentions of rigid designators from tweets belonging to named-entity types such as persons, organizations and locations (2007), has attracted increasing research interest. For example, Ritter et al. (2011) develop a system that exploits a CRF model to segment named 1 http://www.twitter.com entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. Liu et al. (2011) combine a classifier based on the k-nearest neighbors algorithm with a CRFbased model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. However, named entity normalization (NEN) for tweets, which transforms named entities mentioned in tweets to their unambiguous canonical forms, has not been well studied. Owing to the informal nature of tweets, there are rich variations of named entities in them. According to our investigation on the data set provided by Liu et al. (2011), every named entity in tweets has an average of 3.3 variations 2"
P12-1055,W07-0804,0,0.0222233,"earest neighbors algorithm with a CRF-based model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. Our method leverages redundance in similar tweets, using a factor graph rather than a two-stage labeling strategy. One advantage of our method is that local and global information can interact with each other. 2.2 NEN There is a large body of studies into normalizing various types of entities for formally written texts. For instance, Cohen (2005) normalizes gene/protein names using dictionaries automatically extracted from gene databases; Magdy et al. (2007) address cross-document Arabic name normalization using a machine learning approach, a dictionary of person names and frequency information for names in a collection; Cucerzan (2007) demostrates a largescale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results; Dai et al. (2011) employ a Markov logic network to model interweaved con3 4 https://www.mturk.com/mturk/ http://crowdflower.com/ straints in a setting of gene mention normalization. Jijkoun et al. (2008) study NEN for UGC. They"
P12-1055,W03-0430,0,0.0793044,"in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk). A handful of work on other genres of texts exists. For example, Yoshida and Tsujii build a biomedical NER system (2007) using lexical features, orthographic features, semantic features and syntactic features, such as part-of-speech (POS) and shallow par"
P12-1055,H05-1056,0,0.0223589,"next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk). A handful of work on other genres of texts exists. For example, Yosh"
P12-1055,W09-1119,0,0.712807,"ER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk). A handful of work on other genres of texts exists. For example, Yoshida and Tsujii build a biomedical NER system (2007) using lexical features, orthographic features, semantic features and syntactic features, such as part-of-speech (POS) and shallow parsing; Downey et al. (2007) employ capitalization cues and n-gram statistics to lo"
P12-1055,D11-1141,0,0.172463,"Yn / m ∩ j+1 ∩ j−1 i+1 i−1 Ym Yn / Ym Yn is empty; whether the dominating label/entity type in Ymi is the same as that in Ynj . We develop a cascaded system as the baseline, which conducts NER and NEN sequentially. Its NER module, denoted by SBR , is based on the stateof-the-art method introduced by Liu et al. (2011); and its NEN model , denoted by SBN , follows the NEN system for user-generated news comments proposed by Jijkoun et al. (2008), which uses handcrafted rules to improve a typical NEN system that normalizes surface forms to Wikipedia page titles. We use the POS tagger developed by Ritter et al. (2011) to extract POS related features, and the OpenNLP toolkit to get lemma related features. 5 5.4 Results Experiments We manually annotate a data set to evaluate our method. We show that our method outperforms the baseline, a cascaded system that conducts NER and NEN individually. 5.1 Data Preparation We use the data set provided by Liu et al. (2011), which consists of 12,245 tweets with four types of entities annotated: PERSON, LOCATION, ORGANIZATION and PRODUCT. We enrich this data set by adding entity normalization information. Two annotators 12 are involved. For any entity mention, two annota"
P12-1055,N10-1009,0,0.0195022,"that our method compares favorably with the baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly ou"
P12-1055,P09-3003,0,0.00719362,"same entity; 2) set zmn to 1, if the similarity between tm and tn is above a threshold (0.8 in our work), or tm and tn share one hash tag; and 3)zmn ij = −1, if the similarity between tm and tn is below a threshold (0.3 in work). To compute 4.3 Features (1) 1 A feature in {ϕk }K k=1 involves a pair of neighbori−1 and y i , while a feaing NE-type labels, i.e., ym m (2) K2 ture in {ϕk }k=1 concerns a pair of distant NE-type labels and its associated normalization label, i.e., i ,y j and z ij . Details are given below. ym n mn (1) 1 4.3.1 Feature Set One: {ϕk }K k=1 We adopts features similar to Wang (2009), and Ratinov and Roth (2009), i.e., orthographic features, lexical features and gazetteer-related features. These features are defined on the observation. Combining i−1 and y i constitutes {ϕ(1) }K1 . them with ym m k k=1 Orthographic features: Whether tim is capitalized or upper case; whether it is alphanumeric or contains any slashes; wether it is a stop word; word prefixes and suffixes. Lexical features: Lemma of tim , ti−1 and ti+1 m m , i respectively; whether tm is an out-of-vocabulary i−1 and ti+1 , respec(OOV) word 11 ; POS of tim , tm m i tively; whether tm is a hash tag, a link, or"
P12-1055,W07-1033,0,0.0618195,"Missing"
P12-1055,W03-0419,0,\N,Missing
P12-1055,M98-1004,0,\N,Missing
P12-1060,C10-2028,0,0.00911262,"studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creatin"
P12-1060,P11-2075,0,0.118846,"t approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1 http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics et al. (2011) show that vocabulary coverage has a strong correlation with sentiment classification accuracy. Second, machine translation may change the sentiment polarity of the original"
P12-1060,C04-1121,0,0.3955,"sifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the words “excellent” and “poor”, and then uses the average scores of the phrases in a document as the sentiment of the document. Corpus-based methods are often built upon machine learning models. Pang et al. (2002) compare the performance of three commonly used machine learning models (Naive Bayes, Maximum Entropy and SVM). Gamon (2004) shows that introducing deeper linguistic features into SVM can help to improve the performance. The interested readers are referred to (Pang and Lee, 2008) for a comprehensive review of sentiment classification. 2.2 Cross-Lingual Sentiment Classification Cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) with labeled data in the source language (e.g. English), has been extensively studied in the very recent years. The basic idea is to explore the abundant labeled sentiment data in source language to alleviate the shorta"
P12-1060,P09-1028,0,0.0398786,"then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e"
P12-1060,N06-1014,0,0.01986,"third term on the right hand side (L(θ|Dt )) is optional. 2 For simplicity, we assume the prior distribution P (C) is uniform and drop it from the formulas. 3.3 Parameter Estimation Instead of estimating word projection probability (P (ws |wt ) and P (wt |ws )) and conditional probability of word to class (P (wt |c) and P (ws |c)) simultaneously in the training procedure, we estimate them separately since the word projection probability stays invariant when estimating other parameters. We estimate word projection probability using word alignment probability generated by the Berkeley aligner (Liang et al., 2006). The word alignment probabilities serves two purposes. First, they connect the corresponding words between the source language and the target language. Second, they adjust the strength of influences between the corresponding words. Figure 2 gives an example of word alignment probability. As is shown, the three words “tour de force” altogether express a positive meaning, while in Chinese the same meaning is expressed with only one word “杰作” (masterpiece). CLMM use word alignment probability to decrease the influences from “杰作” (masterpiece) to “tour”, “de” and “force” individually, using the w"
P12-1060,P11-1033,0,0.531802,". (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e.g. English and Chinese) . the authors use an unlabeled parallel corpus instead of machine translation engines. They assume parallel sentences in the corpus should have the same sentiment polarity. Besides, they assume labeled data in both language are available. They propose a method of training two classifiers based on maximum entropy formulation to maximize their prediction agreement on the parallel corpus. However, this method requires labeled data in both the source language and the targ"
P12-1060,J05-4003,0,0.0182661,"Missing"
P12-1060,W02-1011,0,0.0302846,"d data in the target language are also available. 1 Introduction Sentiment Analysis (also known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is ∗ Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is des"
P12-1060,J11-2001,0,0.018128,"w of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the"
P12-1060,P02-1053,0,0.015491,"abeled data in the target language. The paper is organized as follows. We review related work in Section 2, and present the cross-lingual mixture model in Section 3. Then we present the ex573 perimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment or"
P12-1060,D08-1058,0,0.154858,"directly adapt labeled data from the source language to target language. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual"
P12-1060,P09-1027,0,0.805218,"(Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1 http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguisti"
P12-1060,C08-1135,0,0.125358,"lso known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is ∗ Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other"
P12-1060,W06-1615,0,\N,Missing
P12-2008,P05-1074,0,0.0918368,"tic equivalency and surface dissimilarity. 1 Ming Zhou Microsoft Research Asia Introduction Paraphrasing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the original text. Paraphrasing technology has been applied in many NLP applications, such as machine translation (MT), question answering (QA), and natural language generation (NLG). 1 This work has been done while the author was visiting Microsoft Research Asia. In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase (Bannard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language. For question expansion, Dubou´e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segm"
P12-2008,P11-1020,0,0.036059,"e0 (es , {f }, λ1 , λ2 )) s=1 (2) where G is the automatic evaluation metric for paraphrasing. S is the development set for training the parameters and for each source sentence several human translations rs are listed as references. 2.2 Paraphrase Evaluation Metrics The joint inference method with MERT enables the dual SMT system to be optimized towards the quality of paraphrasing results. Different application scenarios of paraphrase have different demands on the paraphrasing results and up to now, the widely mentioned criteria include (Zhao et al., 2009; Zhao et al., 2010; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011): Semantic adequacy, fluency and dissimilarity. However, as pointed out by (Chen and Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. Two issues are also raised in (Zhao and Wang, 2010) about using automatic metrics: paraphrase changes less gets larger BLEU score and the evaluations of paraphrase quality and rate tend to be incompatible. To address the above problems, we propose a metric for tuning parameters and evaluating the quality of each candidate paraphrase c : iBLEU (s, rs , c) = αBLEU"
P12-2008,J07-2003,0,0.0612078,"aining 919 sentences) for MERT and test the performance on NIST 2008 data set (containing 1357 sentences). NIST Chinese-to-English evaluation data offers four English human translations for every Chinese sentence. For each sentence pair, we choose one English sentence e1 as source and use the three left sentences e2 , e3 and e4 as references. The English-Chinese and Chinese-English systems are built on bilingual parallel corpus contain40 ing 497,862 sentences. Language model is trained on 2,007,955 sentences for Chinese and 8,681,899 sentences for English. We adopt a phrase based MT system of Chiang (2007). 10-best lists are used in both of the translation processes. 3.2 Paraphrase Evaluation Results The results of paraphrasing are illustrated in Table 1. We show the BLEU score (computed against references) to measure the adequacy and self-BLEU (computed against source sentence) to evaluate the dissimilarity (lower is better). By “No Joint”, it means two independently trained SMT systems are employed in translating sentences from English to Chinese and then back into English. This result is listed to indicate the performance when we do not involve joint learning to control the quality of paraph"
P12-2008,N06-2009,0,0.0786247,"Missing"
P12-2008,I05-5003,0,0.032118,"2 53.51 48.08 35.64 26.30 iBLEU / 30.75 20.64 14.78 8.39 Table 1: iBLEU Score Results(NIST 2008) No Joint α=1 α = 0.9 α = 0.8 α = 0.7 Adequacy (0/1/2) 30/82/88 33/53/114 31/77/92 31/78/91 35/105/60 Fluency (0/1/2) 22/83/95 15/80/105 16/93/91 19/91/90 32/101/67 Variety (0/1/2) 25/117/58 62/127/11 23/157/20 20/123/57 9/108/83 Overall (0/1/2) 23/127/50 16/128/56 20/119/61 19/121/60 35/107/58 Table 2: Human Evaluation Label Distribution where s is the input sentence, rs represents the reference paraphrases. BLEU (c, rs ) captures the semantic equivalency between the candidates and the references (Finch et al. (2005) have shown the capability for measuring semantic equivalency using BLEU score); BLEU (c, s) is the BLEU score computed between the candidate and the source sentence to measure the dissimilarity. α is a parameter taking balance between adequacy and dissimilarity, smaller α value indicates larger punishment on selfparaphrase. Fluency is not explicitly presented because there is high correlation between fluency and adequacy (Zhao et al., 2010) and SMT has already taken this into consideration. By using iBLEU , we aim at adapting paraphrasing performance to different application needs by adjustin"
P12-2008,D11-1108,0,0.0491576,"Missing"
P12-2008,N10-1017,0,0.0233367,"ia Introduction Paraphrasing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the original text. Paraphrasing technology has been applied in many NLP applications, such as machine translation (MT), question answering (QA), and natural language generation (NLG). 1 This work has been done while the author was visiting Microsoft Research Asia. In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase (Bannard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language. For question expansion, Dubou´e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segment being translated independently in both of the translation proc"
P12-2008,C10-1069,0,0.0145529,"sing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the original text. Paraphrasing technology has been applied in many NLP applications, such as machine translation (MT), question answering (QA), and natural language generation (NLG). 1 This work has been done while the author was visiting Microsoft Research Asia. In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase (Bannard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language. For question expansion, Dubou´e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segment being translated independently in both of the translation processes. Context feat"
P12-2008,N04-1022,0,0.0438422,"hrase for a given segment by forcing the segment being translated independently in both of the translation processes. Context features are added into the SMT system to improve translation correctness against polysemous. To reduce 38 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 38–42, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics the noise introduced by machine translation, Zhao et al. (2010) propose combining the results of multiple machine translation engines’ by performing MBR (Minimum Bayes Risk) (Kumar and Byrne, 2004) decoding on the N-best translation candidates. The work presented in this paper belongs to the pivot language method for paraphrase generation. Previous work employs two separately trained SMT systems the parameters of which are tuned for SMT scheme and therefore cannot directly optimize the paraphrase purposes, for example, optimize the diversity against the input. Another problem comes from the contradiction between two criteria in paraphrase generation: adequacy measuring the semantic equivalency and paraphrase rate measuring the surface dissimilarity. As they are incompatible (Zhao and Wa"
P12-2008,D10-1090,0,0.0260176,"λ2 ) S X ∗ G(rs , e0 (es , {f }, λ1 , λ2 )) s=1 (2) where G is the automatic evaluation metric for paraphrasing. S is the development set for training the parameters and for each source sentence several human translations rs are listed as references. 2.2 Paraphrase Evaluation Metrics The joint inference method with MERT enables the dual SMT system to be optimized towards the quality of paraphrasing results. Different application scenarios of paraphrase have different demands on the paraphrasing results and up to now, the widely mentioned criteria include (Zhao et al., 2009; Zhao et al., 2010; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011): Semantic adequacy, fluency and dissimilarity. However, as pointed out by (Chen and Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. Two issues are also raised in (Zhao and Wang, 2010) about using automatic metrics: paraphrase changes less gets larger BLEU score and the evaluations of paraphrase quality and rate tend to be incompatible. To address the above problems, we propose a metric for tuning parameters and evaluating the quality of each candidate paraphrase c : iBL"
P12-2008,W09-2503,0,0.0184721,"as been used in extracting paraphrase (Bannard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language. For question expansion, Dubou´e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segment being translated independently in both of the translation processes. Context features are added into the SMT system to improve translation correctness against polysemous. To reduce 38 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 38–42, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics the noise introduced by machine translation, Zhao et al. (2010) propose combining the results of multiple machine translation engines’ by performing MBR (Minim"
P12-2008,P11-2096,0,0.0146904,")) s=1 (2) where G is the automatic evaluation metric for paraphrasing. S is the development set for training the parameters and for each source sentence several human translations rs are listed as references. 2.2 Paraphrase Evaluation Metrics The joint inference method with MERT enables the dual SMT system to be optimized towards the quality of paraphrasing results. Different application scenarios of paraphrase have different demands on the paraphrasing results and up to now, the widely mentioned criteria include (Zhao et al., 2009; Zhao et al., 2010; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011): Semantic adequacy, fluency and dissimilarity. However, as pointed out by (Chen and Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. Two issues are also raised in (Zhao and Wang, 2010) about using automatic metrics: paraphrase changes less gets larger BLEU score and the evaluations of paraphrase quality and rate tend to be incompatible. To address the above problems, we propose a metric for tuning parameters and evaluating the quality of each candidate paraphrase c : iBLEU (s, rs , c) = αBLEU (c, rs ) − (1 − α)BLEU"
P12-2008,P03-1021,0,0.0343597,"erent SMT features such as translation model and language model. When generating the paraphrase results for each source sentence es , the selection of the best paraphrase candidate e0 ∗ from e0 ∈ C is performed by: e0 ∗ (es , {f }, λM ) = arg maxe0 ∈C,f ∈ {f } M X λm hm (e0 |f )t(e0 , f )(1) m=1 where {f } is the set of sentences in pivot language translated from es , hm is the mth feature value and λm is the corresponding weight. t is an indicator function equals to 1 when e0 is translated from f and 0 otherwise. The parameter weight vector λ is trained by MERT (Minimum Error Rate Training) (Och, 2003). MERT integrates the automatic evaluation metrics into the training process to achieve optimal end-toend performance. In the joint inference method, the feature vector of each e0 comes from two parts: vector of translating es to {f } and vector of translating {f } to e0 , the two vectors are jointly learned at the same time: (λ∗1 , λ∗2 ) = arg max (λ1 ,λ2 ) S X ∗ G(rs , e0 (es , {f }, λ1 , λ2 )) s=1 (2) where G is the automatic evaluation metric for paraphrasing. S is the development set for training the parameters and for each source sentence several human translations rs are listed as refer"
P12-2008,W04-3219,0,0.10368,"Missing"
P12-2008,C10-4001,0,0.0993906,"yrne, 2004) decoding on the N-best translation candidates. The work presented in this paper belongs to the pivot language method for paraphrase generation. Previous work employs two separately trained SMT systems the parameters of which are tuned for SMT scheme and therefore cannot directly optimize the paraphrase purposes, for example, optimize the diversity against the input. Another problem comes from the contradiction between two criteria in paraphrase generation: adequacy measuring the semantic equivalency and paraphrase rate measuring the surface dissimilarity. As they are incompatible (Zhao and Wang, 2010), the question arises how to adapt between them to fit different application scenarios. To address these issues, in this paper, we propose a joint learning method of two SMT systems for paraphrase generation. The jointly-learned dual SMT system: (1) Adapts the SMT systems so that they are tuned specifically for paraphrase generation purposes, e.g., to increase the dissimilarity; (2) Employs a revised BLEU score (named iBLEU , as it’s an input-aware BLEU metric) that measures adequacy and dissimilarity of the paraphrase results at the same time. We test our method on NIST 2008 testing data. Wit"
P12-2008,P08-1116,1,0.834616,"ilarity. 1 Ming Zhou Microsoft Research Asia Introduction Paraphrasing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the original text. Paraphrasing technology has been applied in many NLP applications, such as machine translation (MT), question answering (QA), and natural language generation (NLG). 1 This work has been done while the author was visiting Microsoft Research Asia. In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase (Bannard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language. For question expansion, Dubou´e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segment being translate"
P12-2008,P08-1089,0,0.0207918,"ilarity. 1 Ming Zhou Microsoft Research Asia Introduction Paraphrasing (at word, phrase, and sentence levels) is a procedure for generating alternative expressions with an identical or similar meaning to the original text. Paraphrasing technology has been applied in many NLP applications, such as machine translation (MT), question answering (QA), and natural language generation (NLG). 1 This work has been done while the author was visiting Microsoft Research Asia. In contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase (Bannard and Callison-Burch, 2005; Zhao et al., 2008b; Callison-Burch, 2008; Kok and Brockett, 2010; Kuhn et al., 2010; Ganitkevitch et al., 2011). Thus researchers leverage bilingual parallel data for this task and apply two SMT systems (dual SMT system) to translate the original sentences into another pivot language and then translate them back into the original language. For question expansion, Dubou´e and Chu-Carroll (2006) paraphrase the questions with multiple MT engines and select the best paraphrase result considering cosine distance, length, etc. Max (2009) generates paraphrase for a given segment by forcing the segment being translate"
P12-2008,P09-1094,0,0.0723806,"tju.edu.cn Abstract mingzhou@microsoft.com As paraphrasing can be viewed as a translation process between the original expression (as input) and the paraphrase results (as output), both in the same language, statistical machine translation (SMT) has been used for this task. Quirk et al. (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. Zhao et al. (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al., 2009). Performance of the monolingual MT-based method in paraphrase generation is limited by the large-scale paraphrase corpus it relies on as the corpus is not readily available (Zhao et al., 2010). SMT has been used in paraphrase generation by translating a source sentence into another (pivot) language and then back into the source. The resulting sentences can be used as candidate paraphrases of the source sentence. Existing work that uses two independently trained SMT systems cannot directly optimize the paraphrase results. Paraphrase criteria especially the paraphrase rate is not able to be ens"
P12-2008,C10-1149,0,0.251261,"same language, statistical machine translation (SMT) has been used for this task. Quirk et al. (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. Zhao et al. (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al., 2009). Performance of the monolingual MT-based method in paraphrase generation is limited by the large-scale paraphrase corpus it relies on as the corpus is not readily available (Zhao et al., 2010). SMT has been used in paraphrase generation by translating a source sentence into another (pivot) language and then back into the source. The resulting sentences can be used as candidate paraphrases of the source sentence. Existing work that uses two independently trained SMT systems cannot directly optimize the paraphrase results. Paraphrase criteria especially the paraphrase rate is not able to be ensured in that way. In this paper, we propose a joint learning method of two SMT systems to optimize the process of paraphrase generation. In addition, a revised BLEU score (called iBLEU ) which"
P12-2008,D08-1021,0,\N,Missing
P12-2057,P10-1088,0,0.0457426,"Missing"
P12-2057,J93-2003,0,0.0390093,"Missing"
P12-2057,P05-1033,0,0.0608879,"phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods a"
P12-2057,J07-2003,0,0.0340551,"nd hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examine"
P12-2057,C10-2025,1,0.846579,"duction is needed to be performed for both the phrase table and the hierarchical rule table simultaneously, namely joint reduction. Similar to phrase reduction and hierarchical rule reduction, it selects the best N entries of the mixture of phrase and hierarchical rules. This method results in safer pruning; once a phrase is determined to be pruned, the hierarchical rules, which are related to this phrase, are likely to be kept, and vice versa. 3 Experiment We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task. The training data, as same as Cui et al. (2010), consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC. NIST 2003 set is used as a development set. NIST 2004, 2005, 2006, and 2008 sets are used for evaluation purpose. For word alignment, we use GIZA++1 , an implementation of IBM models (Brown et al., 1993). We have implemented a hierarchical phrase-based SMT model similar to Chiang (2005). The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003). Sampled 10,000 sentences from Chinese Gigaword corpus (Graff, 2007) was used for sou"
P12-2057,C10-1056,0,0.258016,"del (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores"
P12-2057,D07-1103,0,0.114556,"rs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2"
P12-2057,N03-1017,0,0.0239505,"the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010)."
P12-2057,N03-2016,0,0.031157,"ranslation model, T M , our goal is to find the optimally reduced translation model, T M ∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is"
P12-2057,P03-1021,0,0.0219527,". The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both. Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely. Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously. In minimum error rate training (MERT) stages, a development set, which consists of bilingual sentences, is used to find out the best weights of features (Och, 2003). One characteristic of our method is that it isolates feature weights of the translation model from SMT log-linear model, trying to minimize the impact of search path during decoding. The reduction procedure consists of three stages: translation scoring, redundancy estimation, and redundancy-based reduction. Our reduction method starts with measuring the translation scores of the individual phrase and the hierarchical rule. Similar to the decoder, the scoring scheme is based on the log-linear framework: X P S(p) = λi hi (p) (3) i 2 Proposed Model Given an original translation model, T M , our"
P12-2057,P02-1040,0,0.0852558,"∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is its weight. As the conventional hierarchical phrase-based SMT model, our features are co"
P12-2057,2009.mtsummit-papers.17,0,0.505698,"various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phra"
P12-2057,P09-2060,0,0.449057,"s mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2 &gt; where si and ti are"
P12-2057,C08-1144,0,0.100555,"amework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estima"
P12-3003,D09-1015,0,0.0176951,"R. NER is the task of identifying mentions of rigid designators from text belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggre"
P12-3003,P05-1045,0,0.0131915,"Missing"
P12-3003,W02-1041,0,0.0318638,"ext belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggregation methods (Krishnan and Manning, 2006), such pre-labeled results,"
P12-3003,P11-1016,1,0.74586,"alization model identifies and corrects ill-formed words. For example, after normalization, “loooove” in “· · · I loooove my icon· · · ” will be transformed to “love”. A phrase-based translation system without re-ordering is used to implement this model. The translation table includes manually compiled ill/good form pairs, and the language model is a trigram trained on LDC data 4 using SRILM (Stolcke, 2002). The OpenNLP 5 toolkit is directly used to implement the parsing model. In future, the parsing model will be re-trained using annotated tweets. The SA component is implemented according to Jiang et al. (2011), which incorporates target-dependent features and considers related tweets by utilizing a graph-based optimization. The classification model is a KNN-based classifier that caches confidently labeled results to re-train itself, which also recognizes and drops noisy tweets. 4 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?catalogId=LDC2005T12 5 http://sourceforge.net/projects/opennlp/ 16 Each processed tweet, if not identified as noise, is put into a shared buffer for indexing. The third part is responsible for indexing and querying. It constantly takes from the indexing buffer a processed"
P12-3003,W05-0625,0,0.0116131,"poral expressions, etc. These gazetteers represent general knowledge across domains, and help to make up for the lack of training data. SRL. Given a sentence, the SRL component identifies every predicate, and for each predicate further identifies its arguments. This task has been extensively studied on well-written corpora like news, and a couple of solutions exist. Examples include: 1) the pipelined approach, i.e., dividing the task into several successive components such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue, 2004; Koomen et al., 2005); 2) sequentially labeling 17 based approach (M`arquez et al., 2005), i.e., labeling the words according to their positions relative to an argument (i.e., inside, outside, or at the beginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the performance of the state-of-the-art SRL system (MezaRuiz and Riedel, 2009) drops sharply when applied to tweets. The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that are similar to the cur"
P12-3003,P06-1141,0,0.0293496,"2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggregation methods (Krishnan and Manning, 2006), such pre-labeled results, together with other conventional features used by the state-of-the-art NER systems, are fed into a linear CRF models, which conducts fine-grained tweet level NER. Secondly, the KNN and CRF model are repeatedly retrained with an incrementally augmented training set, into which highly confidently labeled tweets are added. Finally, following Lev Ratinov and Dan Roth (2009), 30 gazetteers are used, which cover common names, countries, locations, temporal expressions, etc. These gazetteers represent general knowledge across domains, and help to make up for the lack of tr"
P12-3003,M98-1015,0,0.0808344,"discuss two core components of QuickView: NER and SRL. NER. NER is the task of identifying mentions of rigid designators from text belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level"
P12-3003,W05-0628,0,0.0454103,"Missing"
P12-3003,N09-1018,0,0.0129393,"s task has been extensively studied on well-written corpora like news, and a couple of solutions exist. Examples include: 1) the pipelined approach, i.e., dividing the task into several successive components such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue, 2004; Koomen et al., 2005); 2) sequentially labeling 17 based approach (M`arquez et al., 2005), i.e., labeling the words according to their positions relative to an argument (i.e., inside, outside, or at the beginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the performance of the state-of-the-art SRL system (MezaRuiz and Riedel, 2009) drops sharply when applied to tweets. The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that are similar to the current tweet as the broader context. Algorithm 1 outlines its implementation, where: train denotes a machine learning process to get a labeler l, which in our work is a linear CRF model; the cluster function puts the new tweet into a cluster; the label function generates pr"
P12-3003,W09-1119,0,0.0692759,"Missing"
P12-3003,H05-1088,0,0.0416791,"Missing"
P12-3003,N10-1009,0,0.0313196,"tifying mentions of rigid designators from text belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggregation methods (Krish"
P12-3003,W04-3212,0,0.0482433,"ations, temporal expressions, etc. These gazetteers represent general knowledge across domains, and help to make up for the lack of training data. SRL. Given a sentence, the SRL component identifies every predicate, and for each predicate further identifies its arguments. This task has been extensively studied on well-written corpora like news, and a couple of solutions exist. Examples include: 1) the pipelined approach, i.e., dividing the task into several successive components such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue, 2004; Koomen et al., 2005); 2) sequentially labeling 17 based approach (M`arquez et al., 2005), i.e., labeling the words according to their positions relative to an argument (i.e., inside, outside, or at the beginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the performance of the state-of-the-art SRL system (MezaRuiz and Riedel, 2009) drops sharply when applied to tweets. The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that"
P12-3003,C10-1079,1,\N,Missing
P12-3003,M98-1004,0,\N,Missing
P13-1017,N03-1017,0,0.0812093,"ify the word embeddings in subsequent steps according to bilingual data. our model from raw sentence pairs, they are too computational demanding as the lexical translation probabilities must be computed from neural networks. Hence, we opt for a simpler supervised approach, which learns the model from sentence pairs with word alignment. As we do not have a large manually word aligned corpus, we use traditional word alignment models such as HMM and IBM model 4 to generate word alignment on a large parallel corpus. We obtain bidirectional alignment by running the usual growdiag-final heuristics (Koehn et al., 2003) on unidirectional results from both directions, and use the results as our training data. Similar approach has been taken in speech recognition task (Dahl et al., 2012), where training data for neural network model is generated by forced decoding with traditional Gaussian mixture models. Tunable parameters in neural network alignment model include: word embeddings in lookup table LT , parameters W l , bl for linear transformations in the hidden layers of the neural network, and distortion parameters sd of jump distance. We take the following ranking loss with margin as our training criteria:"
P13-1017,J96-1002,0,0.112467,"and target side respectively. Words are converted to embeddings using the lookup table LT , and the catenation of embeddings are fed to a classic neural network with two hidden-layers, and the output of the network is the our lexical translation score: 1 The ability to model context is not unique to our model. In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural network. (Berger et al., 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. tlex (ei , fj |e, f) = f3 ◦ f2 ◦ f1 ◦ LT (window(ei ), window(fj )) (6) f1 and f2 layers use htanh as activation functions, while f3 is only a linear transformation with no activation function. For the distortion td , we could use a lexicalized distortion model: td (ai , ai−1 |e, f) = td (ai − ai−1 |window(fai−1 )) (7) which can be computed by a neural network similar to the one used to compu"
P13-1017,J93-2003,0,0.0874815,"where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the simi"
P13-1017,P10-1033,1,0.843062,"proved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the similar meaning with “big”, or “huge”, it would be easier to find the corresponding word in the Chinese sentence. As we mentioned in the last paragraph, word embeddi"
P13-1017,P11-1043,0,0.018762,"test set. We adapt the segmentation on the Chinese side to fit our word segmentation standard. 171 archical phrase model (Chiang, 2007) from different word alignments. Despite different alignment scores, we do not obtain significant difference in translation performance. In our C-E experiment, we tuned on NIST-03, and tested on NIST08. Case-insensitive BLEU-4 scores on NIST-08 test are 0.305 and 0.307 for models trained from IBM-4 and NN alignment results. The result is not surprising considering our parallel corpus is quite large, and similar observations have been made in previous work as (DeNero and Macherey, 2011) that better alignment quality does not necessarily lead to better end-to-end result. 6.4 0.86 0.84 0.82 0.8 0.78 0.76 0.74 1 3 5 7 9 11 13 Figure 3: Effect of different window sizes on word alignment F-score. fitting problem. This is not surprising considering that larger window size only requires slightly more parameters in the linear layers. Lastly, it is worth noticing that our model with no context (window size 1) performs much worse than settings with larger window size and baseline IBM4. Our explanation is as follows. Our model uses the simple jump distance based distortion, which is we"
P13-1017,P11-1042,0,0.0684704,") (7) which can be computed by a neural network similar to the one used to compute lexical translation scores. If we map jump distance (ai − ai−1 ) to B buckets, we can change the length of the output layer to B, where each dimension in the output stands for a different bucket of jump distances. But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. So we drop the lexicalized 5 Training Although unsupervised training technique such as Contrastive Estimation as in (Smith and Eisner, 2005), (Dyer et al., 2011) can be adapted to train 1 In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated as unknown words. We"
P13-1017,H05-1011,0,0.0188895,"For word pair (ei , fj ), we take fixed length windows surrounding both ei and fj , . . . , ei+ sw , fj− tw , . . . , fj+ tw ), as input: (ei− sw 2 2 2 2 where sw, tw stand window sizes on source and target side respectively. Words are converted to embeddings using the lookup table LT , and the catenation of embeddings are fed to a classic neural network with two hidden-layers, and the output of the network is the our lexical translation score: 1 The ability to model context is not unique to our model. In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural network. (Berger et al., 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. tlex (ei , fj |e, f) = f3 ◦ f2 ◦ f1 ◦ LT (window(ei ), window(fj )) (6) f1 and f2 layers use htanh as activation functions, while f3 is only a linear transformation with no activation functio"
P13-1017,2012.iwslt-papers.3,0,0.0275577,"dent Deep Neural Network with HMM (CD-DNN-HMM) to speech recognition task, which significantly outperforms traditional models. Most methods using DNN in NLP start with a word embedding phase, which maps words into a fixed length, real valued vectors. (Bengio et al., 2006) proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted"
P13-1017,P09-1104,0,0.0091929,"obtained by classic HMM and IBM4 model. The second row and fourth row show results of the proposed model trained from HMM and IBM4 respectively. Experiments and Results results of our model also depends on the quality of baseline results, which is used as training data of our model. In future we would like to explore whether our method can improve other word alignment models. We also conduct experiment to see the effect on end-to-end SMT performance. We train hierWe conduct our experiment on Chinese-to-English word alignment task. We use the manually aligned Chinese-English alignment corpus (Haghighi et al., 2009) which contains 491 sentence pairs as test set. We adapt the segmentation on the Chinese side to fit our word segmentation standard. 171 archical phrase model (Chiang, 2007) from different word alignments. Despite different alignment scores, we do not obtain significant difference in translation performance. In our C-E experiment, we tuned on NIST-03, and tested on NIST08. Case-insensitive BLEU-4 scores on NIST-08 test are 0.305 and 0.307 for models trained from IBM-4 and NN alignment results. The result is not surprising considering our parallel corpus is quite large, and similar observations"
P13-1017,P05-1044,0,0.0167083,"(ai − ai−1 |window(fai−1 )) (7) which can be computed by a neural network similar to the one used to compute lexical translation scores. If we map jump distance (ai − ai−1 ) to B buckets, we can change the length of the output layer to B, where each dimension in the output stands for a different bucket of jump distances. But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. So we drop the lexicalized 5 Training Although unsupervised training technique such as Contrastive Estimation as in (Smith and Eisner, 2005), (Dyer et al., 2011) can be adapted to train 1 In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated"
P13-1017,D12-1110,0,0.0460558,"Missing"
P13-1017,N12-1005,0,0.0718261,"n NLP start with a word embedding phase, which maps words into a fixed length, real valued vectors. (Bengio et al., 2006) proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a"
P13-1017,C12-1089,0,0.0108582,") proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features,"
P13-1017,P10-1040,0,0.0173855,"ch smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated as unknown words. We set word embedding length to 20, window size to 5, and the length of the only hidden layer to 40. Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. Note that embedding for null word in either Ve and Vf cannot be trained from monolingual corpus, and we simply leave them at the initial value untouched. Word embeddings from monolingual corpus learn strong syntactic knowledge of each word, which is not always desirable for word alignment between some language pairs like English and Chinese. For example, many Chinese words can act as a verb, noun and adjective without any change, while their"
P13-1017,C96-2141,0,0.954535,"n better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the similar meaning with “big”, or “h"
P13-1017,J97-3002,0,0.1441,"l data is used to pre-train wordembeddings. Experiments on large-scale Chineseto-English task show that the proposed method produces better word alignment results, compared with both classic HMM model and IBM model 4. For future work, we will investigate more settings of different hyper-parameters in our model. Secondly, we want to explore the possibility of unsupervised training of our neural word alignment model, without reliance of alignment result of other models. Furthermore, our current model use rather simple distortions; it might be helpful to use more sophisticated model such as ITG (Wu, 1997), which can be modeled by Recursive Neural Networks (Socher et al., 2011). improvement. Due to time constraint, we have not tuned the hyper-parameters such as length of hidden layers in 1 and 3-hidden-layer settings, nor have we tested settings with more hidden-layers. It would be wise to test more settings to verify whether more layers would help. 6.4.4 Word Embedding Following (Collobert et al., 2011), we show some words together with its nearest neighbors using the Euclidean distance between their embeddings. As we can see from Table 2, after bilingual training, “bad” is no longer in the ne"
P13-1017,J07-2003,0,\N,Missing
P13-1128,P12-1055,1,0.377711,", and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 1 Introduction Twitter is a widely used social networking service. With millions of active users and hundreds of millions of new published tweets every day1 , it has become a popular platform to capture and transmit the human experiences of the moment. Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1 http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly"
P13-1128,P11-1080,0,0.0359615,"are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1 http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2 . To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea an"
P13-1128,P10-1006,0,0.0140507,"inking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2 . To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3 , and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2 Entity variation lists can be extracted from the entity resolution pages of Wikipedia. For example, the link “http://en.wikipedia.org/wiki/Svm” will lead us to a resolution page, where “Svm” are li"
P13-1157,W11-2104,0,0.0289613,"d. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Leacock et al., 2010). We us"
P13-1157,P11-1131,0,0.0623375,"constrained. For example, the same preposition rarely appears many times in a humangenerated sentence, while it does in a machinetranslated sentence due to the phrase salad. Similar to the POS LM, we first analyze sentences generated by human or SMT by a POS tagger, extract sequences of function words, and finally train LMs with the sequences. We use these LMs to obtain scores that are used as features ff w,H and ff w,M T . 3.4 Gappy-Phrase Feature There are a lot of common non-contiguous phrases that consist of sub-phrases (contiguous word string) and gaps, which we refer to as gappyphrases (Bansal et al., 2011). We specifically use gappy-phrases of 2-tuple, i.e., phrases consisting of two sub-phrases and one gap in the middle. Let us take an English example “not only ? but 1599 Sequences World population not only grows , but grows old . A press release not only informs but also teases . Hazelnuts are not only for food , but also fuel . The coalition must not only listen but also act . Table 1: Example of a sequence database also.” When a sentence contains the phrase “not only,” the phrase “but also” is likely to appear in human-generated setences. Such a gappy-phrase is difficult for SMT systems to"
P13-1157,2009.mtsummit-papers.9,0,0.0692921,"a. They target specific error types commonly made by ESL learners, such as errors in prepositions and subject-verb agreement. In contrast, our method does not specify error types and aims to detect machine-translated sentences focusing on the phrase salad phenomenon produced by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify humantranslated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target domains. While the conte"
P13-1157,P05-1033,0,0.011786,"fference. As a dataset that meets these requirements, we use parallel text in public websites (this is for fair evaluation and our method can be trained using nonparallel text on an actual deployment). Eight popular sites having Japanese and English parallel pages are crawled, whose text is manually verified to be human-generated. The main textual content of these 131K parallel pages are extracted, and the sentences are aligned using (Ma, 2006). As illustrated in Fig. 2, the text in one language is fed to the Bing translator, Google Translate, and an in-house SMT system4 implemented based on (Chiang, 2005) by ourselves for obtaining sentences translated by SMT systems. Due to a severe limitation on the number of requests to the APIs, we randomly subsample sentences before sending them to these SMT systems. We use text in the other language as human-generated sentences5 . In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sentences as an English dataset. We split each of them into two even datasets and use one for development and the other for evaluation. 4.2 Experiment Setting For the f"
P13-1157,P01-1020,0,0.0751766,"Missing"
P13-1157,D11-1142,0,0.0216873,"Missing"
P13-1157,2005.eamt-1.15,0,0.0307713,"results are detected. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Lea"
P13-1157,2009.mtsummit-posters.10,0,0.0928655,"Missing"
P13-1157,P09-1098,1,0.900461,"Missing"
P13-1157,W04-3230,0,0.015861,"Missing"
P13-1157,ma-2006-champollion,0,0.0359118,"hinetranslated, and the human-generated and machinetranslated sentences express the same content for fairness of evaluation to avoid effects due to vocabulary difference. As a dataset that meets these requirements, we use parallel text in public websites (this is for fair evaluation and our method can be trained using nonparallel text on an actual deployment). Eight popular sites having Japanese and English parallel pages are crawled, whose text is manually verified to be human-generated. The main textual content of these 131K parallel pages are extracted, and the sentences are aligned using (Ma, 2006). As illustrated in Fig. 2, the text in one language is fed to the Bing translator, Google Translate, and an in-house SMT system4 implemented based on (Chiang, 2005) by ourselves for obtaining sentences translated by SMT systems. Due to a severe limitation on the number of requests to the APIs, we randomly subsample sentences before sending them to these SMT systems. We use text in the other language as human-generated sentences5 . In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sen"
P13-1157,P10-2041,0,0.077424,"by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify humantranslated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target domains. While the context is different, our work uses a similar idea of data selection for the purpose of detecting low-quality sentences translated by SMT systems. 3 Proposed Method When APIs of SMT services are used for machinetranslating an Web page, they typically insert specific tags into the"
P13-1157,D12-1104,0,0.021276,"Missing"
P13-1157,P02-1040,0,0.0917361,"Missing"
P13-1157,W11-2111,0,0.0149911,"tures, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Leacock et al., 2010). We use common features that are also used in this a"
P13-1157,2011.mtsummit-papers.48,0,0.0739366,"d decides that the sentence is machine-translated when the score is lower than a predefined threshold. The classification is performed by 10-fold cross validation. We find the best performing threshold on a training set and evaluate the accuracy with a test set using the determined threshold. Additionally, we compare our method to a method that uses a feature indicating presence or absence of unigrams, which we call Lexical Feature. This feature is commonly used for translationese detection and shows the best performance as a single feature in (Baroni and Bernardini, 2005). It is also used by Rarrick et al. (2011) and shows the best performance by itself in detecting machine-translated sentences in English-Japanese translation in the setting of bilingual input. We implement the feature and use it against a monolingual input to fit our problem setting. 5 Results and Discussions In this section, we analyze and discuss the experiment results in detail. 5.1 Accuracy on Japanese Dataset We evaluate the sentence-level and documentlevel accuracy of our method using the Japanese dataset. Specifically, we evaluate effects of individual features and their combinations, compare with human annotations, and assess"
P13-1157,P06-1062,1,0.868949,"Missing"
P13-1157,N03-1033,0,0.00796679,"Missing"
P13-1157,W11-1218,0,\N,Missing
P13-2006,P11-1138,0,0.511698,"muli,mingzhou}@microsoft.com zhlongk@qq.com wanghf@pku.edu.cn Abstract d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the"
P13-2006,E06-1002,0,0.0494954,"ity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straight"
P13-2006,D07-1074,0,0.185206,"Missing"
P13-2006,P05-1044,0,0.00959837,"es are f (d) and f (e). In Figure 3, each row shares forward path of f (d) while each column shares forward path of f (e). At backpropagation stage, gradient is summed over each row of score nodes for f (d) and over each column for f (e). Till now, our input simply consists of bag-ofwords binary vector. We can incorporate any handcrafted feature f (d, e) as: exp sim(d, e) (3) ei ∈C(m) exp sim(d, ei ) L(d, e) = − log P Finally, we seek to minimize the following training objective across all training instances: X L= L(d, e) (4) d,e The loss function is closely related to contrastive estimation (Smith and Eisner, 2005), which defines where the positive example takes probability mass from. We find that by penalizing more negative examples, convergence speed can be greatly accelerated. In our experiments, the sof tmax loss function consistently outperforms pairwise ranking loss function, which is taken as our default setting. sim(d, e) = Dot(f (d), f (e)) + ~λf~(d, e) (5) In fact, we find that with only Dot(f (d), f (e)) as ranking score, the performance is sufficiently good. So we leave this as our future work. 32 3 Experiments and Analysis different decisions. To our surprise, our method with only local evi"
P13-2006,D11-1072,0,0.84346,"Missing"
P13-2006,N10-1072,0,0.0590816,"Missing"
P13-2006,P11-1115,0,0.0101432,"d pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness"
P13-2006,C10-1142,0,\N,Missing
P13-2006,P14-2013,0,\N,Missing
P13-2006,P14-1062,0,\N,Missing
P13-2006,Q14-1019,0,\N,Missing
P13-2006,W12-6324,0,\N,Missing
P13-2006,P14-1146,1,\N,Missing
P13-2006,W12-6322,0,\N,Missing
P13-2006,W12-6325,0,\N,Missing
P13-2006,W12-6323,0,\N,Missing
P13-2008,N03-1017,0,0.0365144,"Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 • Word Reorder feature hW R (Q, Q0 ), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al., 2003). • Length Difference feature hLD (Q, Q0 ), which is defined as |Q0 |− |Q|. • Edit Distance feature hED (Q, Q0 ), which is defined as the character-level edit distance between Q and Q0 . Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Besides, a set of traditional SMT features (Koehn et al., 2003) are also used in our paraphrasing model, including translation probability, lexical weight, word count, paraphrase rule count3 , and language model feature. ˆ"
P13-2008,P05-1074,0,0.105085,"ided by the paraphrase candidates of the original queries. 2.1 • Word Deletion feature hW DEL (Q, Q0 ), which is defined as the number of words in the original query Q without being aligned to any word in the paraphrase candidate Q0 . • Word Overlap feature hW O (Q, Q0 ), which is defined as the number of word pairs that align identical words between Q and Q0 . Paraphrase Extraction • Word Alteration feature hW A (Q, Q0 ), which is defined as the number of word pairs that align different words between Q and Q0 . Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 • Word Reorder feature hW R (Q, Q0 ), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al."
P13-2008,P00-1037,0,0.159318,"Missing"
P13-2008,P00-1056,0,0.131701,"er of word pairs that align different words between Q and Q0 . Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 • Word Reorder feature hW R (Q, Q0 ), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al., 2003). • Length Difference feature hLD (Q, Q0 ), which is defined as |Q0 |− |Q|. • Edit Distance feature hED (Q, Q0 ), which is defined as the character-level edit distance between Q and Q0 . Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Besides, a set of traditional SMT features (Koehn et al"
P13-2008,P03-1021,0,0.0343509,"Ming Zhou Microsoft Research Asia mingzhou@microsoft.com Ming Zhang School of EECS Peking University mzhang@net.pku.edu.cn Abstract an in-depth study on adapting paraphrasing to web search. First, we propose a search-oriented paraphrasing model, which includes specifically designed features for web queries that can enable a paraphrasing engine to learn preferences on different paraphrasing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitu"
P13-2008,P02-1040,0,0.0855744,"s are computed based on Q0n and DQ , instead of Q and DQ . In this way, the paraphrase candidates act as hidden variables and expanded matching features between queries and documents, making our ranking model more tunable and flexible for web search. Q 43 3.2 Baseline Systems score only, without considering characteristics of mismatches in search. The baselines of the paraphrasing and the ranking model are described as follows: The paraphrasing baseline is denoted as BLPara, which only uses traditional SMT features described at the end of Section 2.2. Weights are optimized by MERT using BLEU (Papineni et al., 2002) as the error criterion. Development data are generated based on the English references of NIST 2008 constrained track of Chinese-to-English machine translation task. We use the first reference as the source, and the rest as its paraphrases. The ranking model baseline (Liu et al., 2007) is denoted as BL-Rank, which only uses matching features computed based on original queries and different meta-streams of web pages, including URL, page title, page body, meta-keywords, metadescription and anchor texts. The feature functions we use include unigram/bigram/trigram BM25 and original/normalized Per"
P13-2008,P05-1033,0,0.0605837,"otators. MERT is used to optimize feature weights of our linear-formed paraphrasing model. For H(Q) is the hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight λm , Q0 denotes one candidate. In order to enable our paraphrasing model to learn the preferences on different paraphrasing strategies according to the characteristics of web queries, we design search-oriented features2 based on word alignments within Q and Q0 , which can be described as follows: 1 We apply CYK algorithm (Chappelier and Rajman, 1998), which is most commonly used in SMT (Chiang, 2005), to generating paraphrase candidates. 2 Similar features have been demonstrated effective in (Jones et al., 2006). But we use SMT-like model to generate query reformulations. 3 Paraphrase rule count is the number of rules that are used to generate paraphrase candidates. 4 The ranking model R (Liu et al., 2007) uses matching features computed based on original queries and documents. 42 each query Qi in {Qi }Si=1 , we first generate Nbest paraphrase candidates {Qji }N j=1 , and compute NDCG score for each paraphrase based on documents ranked by the ranker R and labeled documents DiLabel . We th"
P13-2008,W04-3219,0,0.10147,"Missing"
P13-2008,P09-1094,0,0.0324077,"Missing"
P13-2061,W06-1607,0,0.246555,"methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance s"
P13-2061,2012.amta-papers.7,0,0.0835577,"Missing"
P13-2061,P09-1098,1,0.833027,"test web data web data web data Table 1: Development and testing data used in the experiments. are also randomly partitioned and summed across different machines. Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length. The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation. 2.5 Setup We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009)1 , as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. based on the iterative computation in the Section 2.3. Before the iterative computation starts, the sum of the outlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , r"
P13-2061,N03-1017,0,0.122322,"ften lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Senten"
P13-2061,W04-3250,0,0.270408,"Missing"
P13-2061,J05-4003,0,0.126612,"Missing"
P13-2061,J04-4002,0,0.0798821,"phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Sentence Pair Vertices Gra"
P13-2061,P03-1021,0,0.159372,"Missing"
P13-2061,P02-1040,0,0.0949147,"Missing"
P13-2061,J03-3002,0,0.192071,"Missing"
P13-2061,P06-1062,1,0.888295,"Missing"
P13-2061,P07-1070,0,0.162098,"s are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. The quality of bilingual data is a key factor in"
P13-2061,J97-3002,0,0.279256,"utlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , rij i and hpj , rij i. The pairs with the same key are summed locally and accumulated across different machines. Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights. The key-value pairs are generr ated in the format hsi , P ij rkj · v(pj )i and hpj , P Experiments A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model: Integration into translation modeling After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(si )) are obtained. Instead of simple filtering, we use the 1 Although supervised data cleaning has been done in the post-processing, the corpus still contains a fair amount of noisy data based on our random sampling. 342 baseline (Wuebker et al., 2010) -0.25M -"
P13-2061,P10-1049,0,0.0134939,"me low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence p"
P13-2061,P06-1066,0,0.0731442,"Missing"
P13-2061,W04-3252,0,\N,Missing
P13-2061,J03-1002,0,\N,Missing
P14-1011,D13-1106,0,0.00699973,"ks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic mea"
P14-1011,W04-3250,0,0.0534067,"sh translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). In addition, we pre-train the word embedding with toolkit Word2Vec on large-scale monolingual data including the aforementioned data for SMT. The monolingual data contains 1.06B words for Chinese and 1.12B words for English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on trans"
P14-1011,D13-1054,0,0.266933,"ccessfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the"
P14-1011,D12-1088,0,0.0249865,"Missing"
P14-1011,P13-1078,0,0.0249237,"ts translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase emb"
P14-1011,P13-2119,0,0.0122932,"resentation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase e"
P14-1011,2007.mtsummit-papers.22,0,0.123108,"Missing"
P14-1011,D11-1014,0,0.55051,"sing word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or"
P14-1011,D07-1103,0,0.030171,"Missing"
P14-1011,P13-1045,0,0.574395,"syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully"
P14-1011,D13-1176,0,0.788727,"ecoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase. Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that"
P14-1011,2009.mtsummit-papers.17,0,0.0492238,"Missing"
P14-1011,D13-1140,0,0.00759691,"(multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, n"
P14-1011,J97-3002,0,0.185251,"Missing"
P14-1011,P06-1066,0,0.0963857,"Missing"
P14-1011,P13-1017,1,0.245382,"ties) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilingu"
P14-1011,D12-1089,0,0.0382796,"Missing"
P14-1011,D13-1141,0,0.206555,"didates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core i"
P14-1011,D13-1170,0,\N,Missing
P14-1013,W06-1607,0,0.0295023,"The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. We i"
P14-1013,P05-1048,0,0.0187263,"vel. However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Balt"
P14-1013,C08-1041,0,0.130559,"ent level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the"
P14-1013,2007.mtsummit-papers.11,0,0.0175661,"uation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, J"
P14-1013,P13-1126,0,0.0442344,"Missing"
P14-1013,W04-3250,0,0.0266605,"language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method"
P14-1013,J07-2003,0,0.732936,"tributions while topic-specific rules have sharper distributions. A standard entropy metric is used to measure the sensitivity of the source-side of hα, γi as: The model minimizes the pairwise ranking loss across all training instances: X L(f, e) (6) L= Sen(α) = − hf,ei We incorporate the learned topic similarity scores into the standard log-linear framework for SMT. When a synchronous rule hα, γi is extracted from a sentence pair hf, ei, a triple instance I = (hα, γi, hf, ei, c) is collected for inferring the topic representation of hα, γi, where c is the count of rule occurrence. Following (Chiang, 2007), we give a count of one for each phrase pair occurrence and a fractional count for each hierarchical phrase pair. The topic representation of hα, γi is then calculated as the weighted average: P (hα,γi,hf,ei,c)∈T {c × zf } zα = P (7) (hα,γi,hf,ei,c)∈T {c} zγ = (hα,γi,hf,ei,c)∈T P {c × ze } (hα,γi,hf,ei,c)∈T {c} eˆ = arg max P (e|f ) where the translation probability is given by: X P (e|f ) ∝ wi · log φi (f, e) i = Sim(zs , zγ ) = cos(zs , zγ ) (10) X | wj · log φj (f, e) + j {z Standard } X | k wk · log φk (f, e) {z Topic related (13) where φj (f, e) is the standard feature function and wj is"
P14-1013,D13-1107,1,0.837725,"hieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Related Work Generally, most previous research has leveraged conventional topic modeling techniques such as LDA or HTMM. In our work, a novel neural network based approach is proposed to infer topic representations for parallel data. The advantage of Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topic140 Sim(Src) Sim(Trg) Sim(Src+Trg) Sim(Src+Trg)+Sen(Src) Sim(Src+Trg)+Sen(Trg) Sim(Src+Trg)+Sen(Src+Trg) 42.51 42.43 42"
P14-1013,D08-1010,0,0.211625,"the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This ma"
P14-1013,P08-1114,0,0.0702404,"though we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches i"
P14-1013,P03-1021,0,0.0503972,"tions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the"
P14-1013,P12-1079,0,0.66379,"s are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Mar"
P14-1013,P02-1040,0,0.0916175,"News Weblog Total Chinese Docs Words 5.7M 5.4B 2.1M 8B 7.8M 13.4B using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method i"
P14-1013,P09-1036,0,0.0149151,"y LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches inefficient when appli"
P14-1013,P05-1044,0,0.0214738,"ss consists of a linear layer and a non-linear layer with similar network structures, but different parameters. It transforms the L-dimensional vector g(˜ x) to a V -dimensional vector h(g(˜x)). To minimize reconstruction error with respect to ˜ x, we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input: L(h(g(˜ x)), x) = kh(g(˜ x)) − xk2 Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence. Inspired by the contrastive estimation method (Smith and Eisner, 2005), for each parallel sentence pair hf, ei as a positive instance, we select another sentence pair hf 0 , e0 i from the training data and treat hf, e0 i as a negative instance. To make the similarity of the positive instance larger than the negative instance by some margin η, we utilize the following pairwise ranking loss: (3) Multi-layer neural networks are trained with the standard back-propagation algorithm (Rumelhart et al., 1988). The gradient of the loss function is calculated and back-propagated to the previous layer to update its parameters. Training neural networks involves many factors"
P14-1013,P06-2124,0,0.0322586,"a. By associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling t"
P14-1013,D11-1014,0,0.0446933,"Missing"
P14-1013,P12-1048,0,0.225478,"T Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of pa"
P14-1091,D13-1161,0,0.148238,"rd to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Da"
P14-1091,D11-1039,0,0.0142537,"d over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗ This work was finished while the author was visiting Microsoft Research Asia. 967 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Hanks, Film.Actor.Film, Forrest Gumpi62 and hForrest Gump, Film.Film.Director, Robert Zemeckisi60 are three ordered formal triples corresponding to the three translation steps in Figure 1. We define the task of transforming question spans into formal triples as question translation."
P14-1091,P11-1060,0,0.199678,"um error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗ This work was finished while the author was visiting Microsoft Research Asia. 967 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Hanks, Film.Actor.Film, Forrest Gumpi62 and hForrest Gump, Film.Film.Director, Robert Zemeckisi60 are three ordered formal triples corresponding to the three translation steps in Figure 1. We define the task of transforming question spans into formal triples as question translation. A denotes one final"
P14-1091,P13-5002,0,0.0191882,"Missing"
P14-1091,J13-2005,0,0.357304,"and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce th"
P14-1091,D13-1160,0,0.380905,"unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Data Set WEBQUESTIONS Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an join"
P14-1091,W12-3016,0,0.0735886,"t it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Data Set WEBQUESTIONS Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters. # Questions 5,810 # Words 6.7 Table 1: Statistics of evaluation set. #"
P14-1091,P13-1042,0,0.508703,"Missing"
P14-1091,P03-1021,0,0.0585009,"sing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated translation tables to translate source phrases into target translations, a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions. The final answers can be obtained from the root cell. Derivations generated during such a translation procedure are modeled by a linear model, and minimum error rate training (MERT) (Och, 2003) is used to tune feature weights based on a set of question-answer pairs. Figure 1 shows an example: the question director of movie starred by Tom Hanks is translated to one of its answers Robert Zemeckis by three main steps: (i) translate director of to director of ; (ii) translate movie starred by Tom Hanks to one of its answers Forrest Gump; (iii) translate director of Forrest Gump to a final answer Robert Zemeckis. Note that the updated question covered by Cell[0, 6] is obtained by combining the answers to question spans covered by Cell[0, 1] and Cell[2, 6]. The contributions of this work"
P14-1091,J10-3005,0,0.00981963,"contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, wh"
P14-1091,P13-1092,0,0.0834723,"hastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data"
P14-1091,P03-1003,0,0.0535075,"rsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, • hQP count (·), which counts the number of triples in D that are generated by QP-based question translation method. • hRE count (·), which counts the number of triples in D that are generated by RE-based question translation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 201"
P14-1091,N06-1056,0,0.0739044,"osition component. ref ˆ ˆ M Err(Aref i , Ai ; λ1 ) = 1 − δ(Ai , Ai ) ˆ where δ(Aref i , Ai ) is an indicator function which equals 1 when Aˆi is included in the reference set Aref i , and 0 otherwise. • htriple (·), which counts the number of triples in D, whose predicates are not N ull. 3 • htripleweight (·), which P sums the scores of all triples {ti } in D as ti ∈D ti .score. Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, • hQP count (·), which counts the number of triples in D that are generated by QP-based question translation method. • hRE count (·), which counts the number of triples in D that are generated by RE-based question translation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explai"
P14-1091,W12-0103,0,0.0349442,"Missing"
P14-1091,P07-1121,0,0.270694,"Missing"
P14-1091,D11-1142,0,0.016949,"Missing"
P14-1091,P13-1158,0,0.0557123,"of formal triples T . Each triple t ∈ T is in the form of {esbj , p, eobj }, where esbj ’s mention3 occurs in Q, p is a predicate that denotes the meaning expressed by the context of esbj in Q, eobj is an answer to Q retrieved from KB using a triple query q = {esbj , p, ?}. Note that if no predicate p or answer eobj can be generated, {Q, N ull, Q} will be returned as a special triple, which sets eobj to be Q itself, and p to be N ull. This makes sure the un-answerable spans can be passed on to the higher-level operations. Question translation assumes each span Q is a single-relation question (Fader et al., 2013). Such assumption simplifies the efforts of semantic parsing to the minimum question units, while leaving the capability of handling multiple-relation questions (Figure 1 gives one such example) to the outer CYK-parsing based translation procedure. Two question translation methods are presented in the rest of this subsection, which are based on question patterns and relation expressions respectively. 2.3.1 Question Pattern-based Translation A question pattern QP includes a pattern string QP pattern , which is composed of words and a slot 3 For simplicity, a cleaned entity dictionary dumped fro"
P14-1091,D07-1071,0,0.0608809,"ˆ ˆ M Err(Aref i , Ai ; λ1 ) = 1 − δ(Ai , Ai ) ˆ where δ(Aref i , Ai ) is an indicator function which equals 1 when Aˆi is included in the reference set Aref i , and 0 otherwise. • htriple (·), which counts the number of triples in D, whose predicates are not N ull. 3 • htripleweight (·), which P sums the scores of all triples {ti } in D as ti ∈D ti .score. Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, • hQP count (·), which counts the number of triples in D that are generated by QP-based question translation method. • hRE count (·), which counts the number of triples in D that are generated by RE-based question translation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an"
P14-1091,D10-1119,0,0.079168,"lation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs"
P14-1091,D11-1140,0,0.0182397,"c rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependen"
P14-1140,D13-1106,0,0.373491,"mension of the vector represents a latent aspect of the word, and captures its syntactic and semantic DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier"
P14-1140,W04-3250,0,0.143032,"Missing"
P14-1140,D13-1054,0,0.251501,"bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2 NN to model the end-to-end decoding process. R2 NN is a combination of recursive neura"
P14-1140,P06-1096,0,0.0191709,"Missing"
P14-1140,P13-1078,0,0.283964,"atures of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for"
P14-1140,J04-4002,0,0.0450753,"er combination, according to their plausible scores. coming from France and Russia R2NN coming from France and Russia R2NN coming from Rule Match 来自 laizi France and Russia Rule Match 法国 和 俄罗斯 faguo he eluosi NULL Rule Match 的 de Figure 4: R2 NN for SMT decoding x ˆ[l,n] = x[l,m] ./ s[l,m] ./ x[m,n] ./ s[m,n] [l,n] sj y [l,n] = = f( X X [l,n] i x ˆi wji ) (s[l,n] ./ x[l,n] )j vj (1) (2) (3) j where ./ is a concatenation operator in Equation 1 and Equation 3, and f is a non-linear function, here we use HT anh function, which is defined as: We extract phrase pairs using the conventional method (Och and Ney, 2004). The commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x . During decoding, recurrent input vectors x for internal nodes are calculated accordingly. The difference between our model and the conventional log-linear model includes: • R2 NN is not linear, while the conventional model is a linear combination. (4) • Representations of phrase pairs are automatically learnt to optimize the translation performance, while features used in conventional model are hand-crafted. Figure 4 illustrates the architecture for SMT"
P14-1140,P02-1040,0,0.0905836,"Missing"
P14-1140,P13-1045,0,0.0239661,"ti-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). In this paper, we propose a novel recursive recurrent neural network (R2 NN) to model the end-to-end decoding process for statistical machine translation. R2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A"
P14-1140,J97-3002,0,0.035851,"the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R2 NN. 6.2 Translation Results As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. Here we conduct experiments to ver1497 ify it. We first train the source and target wo"
P14-1140,P10-1049,0,0.0100658,"parameters W and V . The loss function is the commonly used ranking loss with a margin, and it is defined as follows: [l,n] [l,n] LSLT (W, V, s[l,n] ) = max(0, 1 − yoracle + yt ) (6) [l,n] where s[l,n] is the source span. yoracle is the plausible score of a oracle translation result. [l,n] yt is the plausible score for the best translation candidate given the model parameters W and V . The loss function aims to learn a model which assigns the good translation candidate (the oracle candidate) higher score than the bad ones, with a margin 1. Translation candidates generated by forced decoding (Wuebker et al., 2010) are used as oracle translations, which are the positive samples. Forced decoding performs sentence pair segmentation using the same translation system as decoding. For each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding. From the forced decoding result, we can get the ideal derivation tree in the decoder’s search space, and extract positive/oracle translation candidates. 1495 4.3 Supervised Global Training The supervised local training us"
P14-1140,P06-1066,0,0.231824,"propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2 NN to model the end-to-end decoding process. R2 NN is a combination of recursive neural network and recurrent neural network. In R2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure ca"
P14-1140,P13-1017,1,0.829763,"kthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score,"
P14-1140,D13-1112,0,0.0238646,"obal training is proposed to tune the model according to the final translation performance of the whole source sentence. Actually, we can update the model from the root of the decoding tree and perform back propagation along the tree structure. Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training. To handle this problem, we use early update strategy for the supervised global training. Early update is testified to be useful for SMT training with large scale features (Yu et al., 2013). Instead of updating the model using the final translation results, early update approach optimizes the model, when the oracle translation candidate is pruned from the n-best list, meaning that, the model is updated once it performs a unrecoverable mistake. Back propagation is performed along the tree structure, and the phrase pair embeddings of the leaf nodess are updated. The loss function for supervised global training is defined as follows: words, but bilingual corpus is much more difficult to acquire, compared with monolingual corpus. Embedding Word Word Pair Phrase Pair #Data 1G 7M 7M #"
P14-1146,C10-2005,0,0.00563043,"aches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focu"
P14-1146,P07-1056,0,0.114896,"plement this system because the codes are not publicly available 3 . NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier. The results of bagof-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words. NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation"
P14-1146,C10-2028,0,0.00814251,"t classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter senti"
P14-1146,P11-2008,0,0.0123158,"Missing"
P14-1146,P13-1088,0,0.0113325,"Missing"
P14-1146,P11-1016,1,0.370167,"their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment cl"
P14-1146,P13-2087,0,0.792293,"utoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and"
P14-1146,P11-1015,0,0.646787,"(2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from sc"
P14-1146,D12-1110,0,0.368522,"Missing"
P14-1146,P13-1045,0,0.149339,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,S13-2053,0,0.231338,"on has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases"
P14-1146,S13-2052,0,0.0193943,"to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the w"
P14-1146,pak-paroubek-2010-twitter,0,0.047259,"low traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based me"
P14-1146,W02-1011,0,0.132502,"dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the lea"
P14-1146,D11-1014,0,0.926164,"Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat"
P14-1146,D13-1170,0,0.34155,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,J11-2001,0,0.172252,"ion, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy"
P14-1146,P02-1053,0,0.0615851,"asks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011;"
P14-1146,P12-2018,0,0.321669,"ve/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available"
P14-1146,H05-1044,0,0.100893,"he sentiment lexicon, P#Lex PN j=1 β(wi , cij ) i=1 Accuracy = (10) #Lex × N where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, β(wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon HL MPQA Joint Positive 1,331 1,932 1,051 Negative 2,647 2,817 2,024 Total 3,978 4,749 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5"
P14-1146,D11-1016,0,0.0249497,"the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors."
P14-1146,D13-1061,0,0.152584,"representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in1555 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics to the continuous representation of words, so that it is able to separate good and bad to opposite ends"
P14-1146,P10-1040,0,\N,Missing
P14-2009,W02-1011,0,0.0546421,"Missing"
P14-2009,D12-1110,0,0.0247666,"Missing"
P14-2009,P11-2008,0,0.0271014,"Missing"
P14-2009,D13-1170,0,0.118265,"Missing"
P14-2009,J11-2001,0,0.042909,"respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better performances than the baseline methods. 2 We use the dependency parsing results to find the words syntactically connected with the interested target. Adaptive Recursive Neural Network is proposed to propagate the sentiments of words to the target node. We model the adaptive sentiment propagations as semantic compositions. The computation process is conducted in a bottom-up manner, and the vector representations"
P14-2009,P11-1016,1,0.747726,"is to classify their sentiments for a given target as positive, negative, and neutral. People may mention several entities (or targets) in one tweet, which affects the availabilities for most of existing methods. For example, the tweet “@ballmer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better perfo"
P14-2009,P03-1054,0,0.0549664,"Missing"
P14-2009,N10-1120,0,0.0373418,"Missing"
P15-1026,P13-1042,0,0.20894,"itive interpretations for MCCNNs by developing a method to detect salient question words in the different column networks. 2 Related Work The state-of-the-art methods for question answering over a knowledge base can be classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity"
P15-1026,D11-1142,0,0.0529251,"task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used"
P15-1026,P14-1091,1,0.92368,"crosoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candi"
P15-1026,P13-1158,0,0.250666,"The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used to generalize for unseen words and question patterns. 4 Methods The overview of our framework is shown in Figure 1. For instance, for the question when did Avatar release in UK, the related nodes of the entity Avatar are queried from F REEBASE. These related nodes are regarded as candidate answers (Cq ). Then, for every candidate answer a, the model predicts a score S (q, a) to determine whether it is a correct"
P15-1026,P14-1133,0,0.653202,"during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is empl"
P15-1026,D13-1160,0,0.755738,"r, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. The"
P15-1026,D14-1070,0,0.0457346,"Missing"
P15-1026,D12-1069,0,0.0673897,"Missing"
P15-1026,D14-1067,0,0.859341,"Dong†∗ Furu Wei‡ Ming Zhou‡ Ke Xu† † SKLSDE Lab, Beihang University, Beijing, China ‡ Microsoft Research, Beijing, China donglixp@gmail.com {fuwei,mingzhou}@microsoft.com kexu@nlsde.buaa.edu.cn Abstract to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. I"
P15-1026,D10-1119,0,0.0063562,"rgescale knowledge bases, such as F REEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities"
P15-1026,D13-1161,0,0.0182072,"n-domain questions. However, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dim"
P15-1026,C02-1150,0,0.255307,"2, we compute salience scores for several questions, and normalize them by the max values in different columns. We clearly see that these words play different roles in a question. The overall conclusion is that the wh- words (such as what, who and where) tend to be important for question understanding. Moreover, nouns dependent of the wh- words and verbs are important clues to obtain question representations. For instance, the figure demonstrates that the nouns type/country/leader and the verbs speak/located are salient in the columns of networks. These observations agree with previous works (Li and Roth, 2002). Some manually defined rules (Yao and Van Durme, 2014) used in the question answering task are also based on them. Salient Words Detection In order to analyze the model, we detect salient words in questions. The salience score of a question word depends on how much the word affects the computation of question representation. In other words, if a word plays more important role in the model, its salience score should be larger. We compute several salience scores for a same word to illustrate its importance in different columns of networks. For the i-th column, the salience score of word wj in t"
P15-1026,P11-1060,0,0.168473,"sk. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. In addition, they need to manually design features for semantic parsers. The second approach uses information extraction techniques for open question answering. These methods retrieve a set of candidate answers from the knowledge base, and the extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependenc"
P15-1026,W12-3016,0,0.00897462,"ll the answers can be found in F REEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make F REE BASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of W EB Q UESTIONS or C LUE W EB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preproS (q, a) = f1 (q)T g1 (a) + f2 (q)T g2 (a) + f3 (q)T g3 (a) | {z } | {z } | {z } answer path answer context answer type (1) where fi (q) and gi (a) have the same dimension. As shown in Figure 1, the score layer computes scores and adds them together. 4.1 Candidate Generation The first step is to retrieve candidate answers from F REEBASE for a question. Questions should contain an identified entity that ca"
P15-1026,Q14-1030,0,0.218962,"260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candidate answers accordin"
P15-1026,P10-1040,0,0.0222071,"q2 , q3 ) (8) P q1 ,q2 ∈P q3 ∈RP where RP contains kp questions which are randomly sampled from other clusters. The same optimization algorithm described in Section 4.4 is used to update parameters. 5 F1 31.4 39.9 37.5 33.0 39.2 29.7 40.8 Experiments In order to evaluate the model, we use the dataset W EB Q UESTIONS (Section 3) to conduct experiments. Settings The development set is used to select hyper-parameters in the experiments. The nonlinearity function f = tanh is employed. The dimension of word vectors is set to 25. They are initialized by the pre-trained word embeddings provided in (Turian et al., 2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. 5.2 Model Analysis We also conduct ablation exper"
P15-1026,P14-1090,0,0.797611,"Missing"
P15-1026,P14-2105,0,0.262001,"ng et al., Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question pa"
P15-1048,J92-4003,0,0.16594,"inear model for both models (6) and (7) to score a parsing tree as: X Score(T ) = φ(action) · ~λ action Where φ(action) is the feature vector extracted from partial hypothesis T for a certain action and ~λ is the weight vector. φ(action) · ~λ calculates the score of a certain transition action. The score of a parsing tree T is the sum of action scores. In addition to the basic features introduced in (Zhang and Nivre, 2011) that are defined over bag of words and POS-tags as well as tree-based context, our models also integrate three classes of new features combined with Brown cluster features (Brown et al., 1992) that relate to the rightto-left transition-based parsing procedure as detailed below. Simple repetition function • δI (a, b): A logic function which indicates whether a and b are identical. great X Syntax-based repetition function Figure 3: An example of UT model, where ‘N’ means the word is a fluent word and ‘X’ means it is disfluent. Words with italic font are Reparandums. 3.3 Training and decoding • δL (a, b): A logic function which indicates whether a is a left child of b. • δR (a, b): A logic function which indicates whether a is a right child of b. A binary classifier transition-based m"
P15-1048,N01-1016,0,0.830252,"Missing"
P15-1048,W02-1001,0,0.136275,", p0 ); unigrams δL (w0 , ws );δL (p0 , ps ); δR (w0 , ws );δR (p0 , ps ); NI (w0 , ws );NI (p0 , ps ); N# (w0..2 , ws );N# (p0..2 , ps ); Function δI (ws , w0 )δI (ps , p0 ); bigrams δL (w0 , ws )δL (p0 , ps ); δR (w0 , ws )δR (p0 , ps ); NI (w0 , ws )NI (p0 , ps ); N# (w0..2 , ws )N# (p0..2 , ps ); δI (ws , w0 )ws c; δI (ws , w0 )w0 c; Function ws w0 δI (ws , w0 ); trigrams ws w0 δI (ps , p0 ); Table 1: Feature templates designed for disfluency detection and dependency parsing. Similar to the work in (Zhang and Clark, 2008; Zhang and Nivre, 2011), we train our models by averaged perceptron (Collins, 2002). In decoding, beam search is performed to get the optimal parsing tree as well as the tag sequence. 4 4.1 Experiments Experimental setup Our training data is the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) corpus, which consists of telephone conversations about assigned topics. As not all the Switchboard data has syntactic bracketing, we only use the subcorpus of PAESED/MRG/SWBD. Following the experiment settings in (Charniak and Johnson, 2001), the training subcorpus contains directories 2 and 3 in PAESED/MRG/SWBD and directory 4 is split into test and development"
P15-1048,de-marneffe-etal-2006-generating,0,0.019972,"Missing"
P15-1048,P08-2027,0,0.0244163,"am-search decoder to combine multiple models such as M3 N and language model, they achieved the highest f-score. However, direct comparison with their work is difficult as they utilized the whole SWBD data while we only use the subcorpus with syntactic annotation which is only half the SWBD corpus and they also used extra corpus for language model training. Additionally, syntax-based approaches have been proposed which concern parsing and disfluency detection together. Lease and Johnson (2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. Miller and Schuler (2008) used a right corner transform of syntax trees to produce a syntactic tree with speech repairs. But their performance was not as good as labeling models. There exist two methods published recently which are similar to ours. Rasooli and Tetreault (2013) designed a joint model for both disfluency detection and dependency parsing. They regarded the two tasks as a two step classifications. Honnibal and Johnson (2014) presented a new joint model by extending the original transition actions with a new “Edit” transition. They achieved the state-of-theart performance on both disfluency detection and p"
P15-1048,N09-2028,0,0.381295,"“you know”) and repairs. To identify and remove disfluencies, straightforward rules can be designed to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguis"
P15-1048,N13-1102,0,0.883224,"repairs. To identify and remove disfluencies, straightforward rules can be designed to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal"
P15-1048,Q14-1011,0,0.390168,"u, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal and Johnson, 2014; Rasooli and Tetreault, 2013). These methods can jointly perform dependency parsing and disfluency detection. But in these methods, great efforts are made to distinguish normal words from disfluent words as decisions cannot be made imminently from left to right, leading to inefficient implementation as well as performance loss. In this paper, we propose detecting disfluencies using a right-to-left transition-based dependency parsing (R2L parsing), where the words are consumed from right to left to build the parsing tree based on which the current word is predicted to be either disfluent or no"
P15-1048,D13-1013,0,0.553101,"Missing"
P15-1048,C14-1138,0,0.0180684,"ure work, we will try to add new classes of features to further improve performance by capturing the property of disfluencies. We would also like to make an end-to-end MT test over transcribed speech texts with disfluencies removed based on the method proposed in this paper. Recently, the max-margin markov networks (M3 N) based model has achieved great improvement in this task. Qian and Liu (2013) presented a multi-step learning method using weighted M3 N model for disfluency detection. They showed that M3 N model outperformed many other labeling models such as CRF model. Following this work, Wang et al. (2014) used a beam-search decoder to combine multiple models such as M3 N and language model, they achieved the highest f-score. However, direct comparison with their work is difficult as they utilized the whole SWBD data while we only use the subcorpus with syntactic annotation which is only half the SWBD corpus and they also used extra corpus for language model training. Additionally, syntax-based approaches have been proposed which concern parsing and disfluency detection together. Lease and Johnson (2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disf"
P15-1048,D08-1059,0,0.215322,"RightArc : Links the front of the queue to the top of the stack and, removes the front of the queue and pushes it to the stack. The choice of each transition action during parsing is scored by a generalized perceptron (Collins, 496 2002) which can be trained over a rich set of nonlocal features. In decoding, beam search is performed to search the optimal sequence of transition actions. As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2 ∗ N , where N is the length of the sentence. Transition-based dependency parsing (Zhang and Clark, 2008) can be performed in either a leftto-right or a right-to-left way, both of which have a performance that is comparable as illustrated in Section 4. However, when they are applied to disfluency detection, their behaviors are very different due to the disfluency structure constraint. We prove that right-to-left transition-based parsing is more efficient than left-to-right transition-based parsing for disfluency detection. 3 As both the dependency tree and the disfluency tags are generated word by word, we decompose formula (3) into: (D∗ , T ∗ ) = argmax(D,T ) i=1 × P (di |W1i , T1i ) (5) n Y (D∗"
P15-1048,H05-1030,0,0.0216575,"es can be designed to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal and Johnson, 2014; Rasooli and Tetreault, 2013). These methods"
P15-1048,P11-2033,0,0.292641,"ord “did” is obviously disfluent. Unlike UT model, the BCT will not link the word “did” to any word. Instead only a virtual link will add it to the virtual root. 3.4 In practice, we use the same linear model for both models (6) and (7) to score a parsing tree as: X Score(T ) = φ(action) · ~λ action Where φ(action) is the feature vector extracted from partial hypothesis T for a certain action and ~λ is the weight vector. φ(action) · ~λ calculates the score of a certain transition action. The score of a parsing tree T is the sum of action scores. In addition to the basic features introduced in (Zhang and Nivre, 2011) that are defined over bag of words and POS-tags as well as tree-based context, our models also integrate three classes of new features combined with Brown cluster features (Brown et al., 1992) that relate to the rightto-left transition-based parsing procedure as detailed below. Simple repetition function • δI (a, b): A logic function which indicates whether a and b are identical. great X Syntax-based repetition function Figure 3: An example of UT model, where ‘N’ means the word is a fluent word and ‘X’ means it is disfluent. Words with italic font are Reparandums. 3.3 Training and decoding •"
P15-1048,N06-2019,0,0.843419,"d that M3 N model outperformed many other labeling models such as CRF model. Following this work, Wang et al. (2014) used a beam-search decoder to combine multiple models such as M3 N and language model, they achieved the highest f-score. However, direct comparison with their work is difficult as they utilized the whole SWBD data while we only use the subcorpus with syntactic annotation which is only half the SWBD corpus and they also used extra corpus for language model training. Additionally, syntax-based approaches have been proposed which concern parsing and disfluency detection together. Lease and Johnson (2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. Miller and Schuler (2008) used a right corner transform of syntax trees to produce a syntactic tree with speech repairs. But their performance was not as good as labeling models. There exist two methods published recently which are similar to ours. Rasooli and Tetreault (2013) designed a joint model for both disfluency detection and dependency parsing. They regarded the two tasks as a two step classifications. Honnibal and Johnson (2014) presented a new joint model by extending the original tr"
P15-1048,P06-1071,0,0.023487,"to tackle the former four classes of disfluencies since they often belong to a closed set. However, the repair type disfluency poses particularly more There are many related works on disfluency detection, that mainly focus on detecting repair type of disfluencies. Straightforwardly, disfluency detection can be treated as a sequence labeling problem and solved by well-known machine learning algorithms such as conditional random fields (CRF) or max-margin markov network (M3 N) (Liu et al., 2006; Georgila, 2009; Qian and Liu, 2013), and prosodic features are also concerned in (Kahn et al., 2005; Zhang et al., 2006). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Recently, syntax-based models such as transitionbased parser have been used for detecting disflu495 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 495–503, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics encies (Honnibal and Johnson, 2014; Rasooli and Tetreault, 2013). These methods can jointly perform d"
P15-1048,P13-1074,1,0.851961,"i is the partial tree after word wi is consumed, di is the disfluency tag of wi . We simplify the joint optimization in a cascaded way with two different forms (5) and (6). n Y ∗ ∗ P (T1i |W1i , T1i−1 ) (D , T ) = argmax(D,T ) Model ∗ P (di , T1i |W1i , T1i−1 ) i=1 Our method 3.1 n Y argmaxD P (D1n |W1n ) Here, P (T1i |.) is the parsing model, and P (di |.) is the disfluency model used to predict the disluency tags on condition of the contexts of partial trees that have been built. In (5), the parsing model is calculated first, followed by the calculation of the disfluency model. Inspired by (Zhang et al., 2013), we associate the disfluency tags to the transition actions so that the calculation of P (di |W1i , T1i ) can be omitted as di can be inferred from the partial tree T1i . We then get ∗ ∗ (D , T ) = argmax(D,T ) n Y P (di , T1i |W1i , T1i−1 ) i=1 (1) The dependency parsing tree is introduced into model (1) to guide detection. The rewritten formula is shown below: X D∗ = argmaxD P (D1n , T |W1n ) (2) (7) Where the parsing and disfluency detection are unified into one model. We refer to this model as the Unified Transition(UT) model. While in (6), the disfluency model is calculated first, follow"
P15-1048,P11-1071,0,0.224498,"ork In practice, disfluency detection has been extensively studied in both speech processing field and natural language processing field. Noisy channel models have been widely used in the past to detect 501 previous work but also achieve significantly higher performance on disfluency detection as well as dependency parsing. disfluencies. Johnson and Charniak (2004) proposed a TAG-based noisy channel model where the TAG model was used to find rough copies. Thereafter, a language model and MaxEnt reranker were added to the noisy channel model by Johnson et al. (2004). Following their framework, Zwarts and Johnson (2011) extended this model using minimal expected f-loss oriented nbest reranking with additional corpus for language model training. 6 Conclusion and Future Work In this paper, we propose a novel approach for disfluency detection. Our models jointly perform parsing and disfluency detection from right to left by integrating a rich set of disfluency features which can yield parsing structure and difluency tags at the same time with linear complexity. The algorithm is easy to implement without complicated backtrack operations. Experiential results show that our approach outperforms the baselines on th"
P15-1048,J93-2004,0,0.0501048,"(p0 , ps ); NI (w0 , ws )NI (p0 , ps ); N# (w0..2 , ws )N# (p0..2 , ps ); δI (ws , w0 )ws c; δI (ws , w0 )w0 c; Function ws w0 δI (ws , w0 ); trigrams ws w0 δI (ps , p0 ); Table 1: Feature templates designed for disfluency detection and dependency parsing. Similar to the work in (Zhang and Clark, 2008; Zhang and Nivre, 2011), we train our models by averaged perceptron (Collins, 2002). In decoding, beam search is performed to get the optimal parsing tree as well as the tag sequence. 4 4.1 Experiments Experimental setup Our training data is the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) corpus, which consists of telephone conversations about assigned topics. As not all the Switchboard data has syntactic bracketing, we only use the subcorpus of PAESED/MRG/SWBD. Following the experiment settings in (Charniak and Johnson, 2001), the training subcorpus contains directories 2 and 3 in PAESED/MRG/SWBD and directory 4 is split into test and development sets. We use the Stanford dependency converter (De Marneffe 4.2 4.2.1 Experimental results Performance of disfluency detection on English Swtichboard corpus The evaluation results of both disfluency detection and parsing accuracy are"
P15-1048,P05-1012,0,0.235242,"Missing"
P15-1048,P04-1005,0,\N,Missing
P15-2047,P03-1054,0,0.0500541,"output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 287 Model relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 SVM MV-RNN CNN FCM DT-RNN DepNN Contributions of different components baseline (Path words) +Depedency relations +Attached subtrees +Lexical features 50-d 73.8 80.3 81.2 82.7 F1 200-d 75.5 81.8 82.8 83.6 We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the"
P15-2047,N07-2032,0,0.0416795,"Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path"
P15-2047,S10-1057,0,0.346496,"ve for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indi"
P15-2047,D12-1110,0,0.0907578,"word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings."
P15-2047,Q14-1017,0,0.023951,"n learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel"
P15-2047,H05-1091,0,0.250704,"n Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China 3 Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2"
P15-2047,I08-2119,0,0.124479,"Missing"
P15-2047,C14-1220,0,0.431582,"re, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinat"
P15-2047,P05-1053,0,\N,Missing
P15-2047,W08-1301,0,\N,Missing
P15-2047,P04-1054,0,\N,Missing
P15-2047,D14-1070,0,\N,Missing
P15-2047,P06-1104,0,\N,Missing
P15-2136,D14-1181,0,0.00276888,"Missing"
P15-2136,W04-1013,0,0.0525643,"ver, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe , shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp , xe ] (3) wrT (4) yˆ = ×φ AVG-CF Description The position of the sentence. The averaged term frequency values of words in the sentence. The averaged cluster frequency values of words in the sentence. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowl"
P15-2136,W02-0401,0,0.431087,"res beyond word level (e.g., phrases) are seldom involved in current research. The CTSUM system developed by Wan and Zhang (2014) is the most relevant to ours. It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). HowIntroduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September"
P15-2136,P14-2105,0,0.013617,"The underlined phrases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summar"
P15-2136,C14-1220,0,0.00457144,"the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental resu"
P15-2136,W06-1643,0,\N,Missing
P15-2136,E14-1075,0,\N,Missing
P16-1049,P05-1074,0,0.0295791,"Missing"
P16-1049,J93-2003,0,0.0563193,"hich will be introduced below. • response ranking, which ranks all response candidates in C and selects the most possible ˆ response candidate as S: Sˆ = arg max Rank(S, Q) S∈C • response triggering, which decides whether ˆ it is confident enough to response Q using S: 4.1 Word-level Feature We define three word-level features in this work: (1) hW M (S, Q) denotes a word matching feature that counts the number (weighted by the IDF value of each word in S) of non-stopwords shared by S and Q. (2) hW 2W (S, Q) denotes a word-toword translation-based feature that calculates the IBM model 1 score (Brown et al., 1993) of S and Q based on word alignments trained on ‘questionrelated question’ pairs using GIZA++ (Och and Ney, 2003). (3) hW 2V (S, Q) denotes a word embedding-based feature that calculates the average cosine distance between word embeddings of all non-stopword pairs hvSj , vQi i. vSj represent the word vector of j th word in S and vQj represent the word vector of ith word in Q. ˆ Q) I = T rigger(S, where I is a binary value. When I equals to true, let the response R = Sˆ and output R; otherwise, output nothing. In the following three sections, we will describe solutions of these three components"
P16-1049,P13-1158,0,0.0120646,"ng set. Each sentence in the document of a given question is labeled as 1 or 0, where 1 denotes the current sentence is a correct answer sentence, and 0 denotes the opposite meaning. Given a question, the task of WikiQA is to select answer sentences from all sentences in a question’s corresponding document. The training data settings of response ranking features are described below. 6 521 http://aka.ms/WikiQA Features Fw Fp Fs Fd Fr Fty Fto Fw denotes 3 word-level features, hW M , hW 2W and hW 2V . For hW 2W , GIZA++ is used to train word alignments on 11.6M ‘question-related question’ pairs (Fader et al., 2013) crawled from WikiAnswers.7 . For hW 2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English. Fp denotes 2 phrase-level features, hP P and hP T . For hP P , bilingual data8 is used to extract a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Section 4.2.1). For hP T , GIZA++ trains word alignments on 4M ‘question-answer’ pairs9 crawled from Yahoo Answers10 , and then a phrase table is extracted from word alignments using the intersect-diag-grow refinement. Fs denotes 2 sentence-level features, hSCR a"
P16-1049,P15-1152,0,0.0316726,"Missing"
P16-1049,N10-1145,0,0.0135467,"hniques. However, collecting enough Q-R pairs to build chatbots is often intractable for many domains. Compared to previous methods, DocChat learns internal relationships between utterances and responses based on statistical models at different levels of granularity, and relax the dependency on Q-R pairs as response sources. These make DocChat as a general response generation solution to chatbots, with high adaptation capability. For answer sentence selection. Prior work in measuring the relevance between question and answer is mainly in word-level and syntactic-level (Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013). Learning representation by neural network architecture (Yu et al., 2014; Wang and Nyberg, 2015; Severyn and Moschitti, 2015) has become a hot research topic to go beyond word-level or phrase-level methods. Compared to previous works we find that, (i) Large scale existing resources with noise have more advantages as training data. (ii) Knowledge-based semantic models can play important roles. at least one suitable response, but response ranking will output the best possible candidate all the time. So, we have to decide which responses are confident enough to be output, and"
P16-1049,C10-1131,0,0.0206887,"Missing"
P16-1049,P15-2116,0,0.0100083,"Missing"
P16-1049,N03-1017,0,0.0225488,"g document. The training data settings of response ranking features are described below. 6 521 http://aka.ms/WikiQA Features Fw Fp Fs Fd Fr Fty Fto Fw denotes 3 word-level features, hW M , hW 2W and hW 2V . For hW 2W , GIZA++ is used to train word alignments on 11.6M ‘question-related question’ pairs (Fader et al., 2013) crawled from WikiAnswers.7 . For hW 2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English. Fp denotes 2 phrase-level features, hP P and hP T . For hP P , bilingual data8 is used to extract a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Section 4.2.1). For hP T , GIZA++ trains word alignments on 4M ‘question-answer’ pairs9 crawled from Yahoo Answers10 , and then a phrase table is extracted from word alignments using the intersect-diag-grow refinement. Fs denotes 2 sentence-level features, hSCR and hSDR . For hSCR , 4M ‘question-answer’ pairs (the same to hP T ) is used to train the CNN model. For hSDR , we randomly select 0.5M ‘sentence-next sentence’ pairs from English Wikipedia. Fd denotes document-level feature hDM . Here, we didn’t train a new model. Instead, we just reuse the CNN m"
P16-1049,D07-1003,0,0.00742056,"existing resources are readily available (such as Q-Q pairs, Q-A pairs, ‘sentence-next sentence’ pairs, and etc.), instead of requiring manually annotated data (such as WikiQA and QASent). Training of the response ranking model does need labeled data, but the size demanded is acceptable. Second, as the training data used in our approach come from open domain resources, we can expect a high adaptation capability and comparable results on other WikiQAlike tasks, as our models are task-independent. To verify the second advantage, we evaluate DocChat on another answer selection data set, QASent (Wang et al., 2007), and list results in Table 3. CN NW ikiQA and CN NQASent refer to the results of Yang et al. (2015)’s method, where the CNN models are trained on WikiQA’s training set and QASent’s training set respectively. All these three methods train feature weights using QASent’s development set. Table 3 tells, DocChat outperforms CN NW ikiQA in terms of MAP and MRR, and achieves comparable results compared to CN NQASent . The comparisons results show a good adaptation capability of DocChat. Table 4 evaluates the contributions of features at different levels of granularity. To highlight the differences,"
P16-1049,W00-0304,0,0.019256,"also, besides, moreover and etc., as the contents of sentences starting with such phrases usually depend on their context sentences, and they are not suitable for responses. 6 7 Experiments 7.1 Evaluation on QA (English) Take into account response ranking task and answer selection task are similar, we first evaluate DocChat in a QA scenario as a simulation. Here, response ranking is treated as the answer selection task, and response triggering is treated as the answer triggering task. Related Work For modeling dialogue. Previous works mainly focused on rule-based or learning-based approaches (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007). These methods require efforts on designing rules or labeling data for training, which suffer the coverage issue. For short text conversation. With the fast development of social media, such as microblog and CQA services, large scale conversation data and data-driven approaches become possible. Ritter et al. (2011) proposed an SMT based method, which treats response generation as a machine translation task. Shang et al. (2015) presented an RNN based method, which is trained based on a large number of single round conversation data. Grammatic"
P16-1049,D15-1237,0,0.0100947,"Missing"
P16-1049,P13-1171,0,0.046252,"ing enough Q-R pairs to build chatbots is often intractable for many domains. Compared to previous methods, DocChat learns internal relationships between utterances and responses based on statistical models at different levels of granularity, and relax the dependency on Q-R pairs as response sources. These make DocChat as a general response generation solution to chatbots, with high adaptation capability. For answer sentence selection. Prior work in measuring the relevance between question and answer is mainly in word-level and syntactic-level (Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013). Learning representation by neural network architecture (Yu et al., 2014; Wang and Nyberg, 2015; Severyn and Moschitti, 2015) has become a hot research topic to go beyond word-level or phrase-level methods. Compared to previous works we find that, (i) Large scale existing resources with noise have more advantages as training data. (ii) Knowledge-based semantic models can play important roles. at least one suitable response, but response ranking will output the best possible candidate all the time. So, we have to decide which responses are confident enough to be output, and which are not. In t"
P16-1049,P14-2105,0,0.0136102,"nguage) with its answer can be first parsed into a fact formatted as hesbj , rel, eobj i, where esbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, eobj denotes an object entity found from the knowledge base based on esbj and rel. Then we can get hQ, reli pairs. This rel can help for modeling semantic relationships between Q and R. For example, the Q-A pair hWhat does Jimmy Neutron do? − inventori can be parsed into hJimmy Neutron, fictional character occupation, inventori where the rel is fictional character occupation. Similar to Yih et al. (2014), We use hQ, reli pairs as training data, and learn a rel-CNN model, which can encode each question Q (or each relation rel) into a relation embedding. For a given question Q, the corresponding relation rel+ is L = max{0, M − cosine(y(SX ), y(SY )) +cosine(y(SX ), y(SY− ))} where M is a constant, SY− is a negative instance. 4.3.1 Document-level Feature Causality Relationship Modeling We train the first attention-based sentence embedding model based on a set of ‘question-answer’ pairs as input sentence pairs, and then design a causality relationship-based feature as: hSCR (S, Q) = cosine(ySCR ("
P16-1049,J03-1002,0,0.0490153,"ossible ˆ response candidate as S: Sˆ = arg max Rank(S, Q) S∈C • response triggering, which decides whether ˆ it is confident enough to response Q using S: 4.1 Word-level Feature We define three word-level features in this work: (1) hW M (S, Q) denotes a word matching feature that counts the number (weighted by the IDF value of each word in S) of non-stopwords shared by S and Q. (2) hW 2W (S, Q) denotes a word-toword translation-based feature that calculates the IBM model 1 score (Brown et al., 1993) of S and Q based on word alignments trained on ‘questionrelated question’ pairs using GIZA++ (Och and Ney, 2003). (3) hW 2V (S, Q) denotes a word embedding-based feature that calculates the average cosine distance between word embeddings of all non-stopword pairs hvSj , vQi i. vSj represent the word vector of j th word in S and vQj represent the word vector of ith word in Q. ˆ Q) I = T rigger(S, where I is a binary value. When I equals to true, let the response R = Sˆ and output R; otherwise, output nothing. In the following three sections, we will describe solutions of these three components one by one. 3 X Response Retrieval Given a user utterance Q, the goal of response retrieval is to efficiently fi"
P16-1049,D11-1054,0,0.0213887,"sponse ranking is treated as the answer selection task, and response triggering is treated as the answer triggering task. Related Work For modeling dialogue. Previous works mainly focused on rule-based or learning-based approaches (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007). These methods require efforts on designing rules or labeling data for training, which suffer the coverage issue. For short text conversation. With the fast development of social media, such as microblog and CQA services, large scale conversation data and data-driven approaches become possible. Ritter et al. (2011) proposed an SMT based method, which treats response generation as a machine translation task. Shang et al. (2015) presented an RNN based method, which is trained based on a large number of single round conversation data. Grammatical and fluency problems are the biggest issue for such generation-based approaches. Retrievalbased methods selects the most suitable response 7.1.1 Experiment Setup We select WikiQA6 as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which contains 2,118 ‘question-document’ pairs in the training set, 29"
P16-1212,P14-1091,1,0.746378,"er to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM"
P16-1212,P14-1133,0,0.0178386,"variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along wit"
P16-1212,D13-1160,0,0.016404,"der framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be"
P16-1212,D14-1179,0,0.00526158,"Missing"
P16-1212,N03-1017,0,0.0311904,"Target Generation can generate a natural language sentence based on the existing semantic tuples; • Combining them, KBSE can be used to translation a source sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there"
P16-1212,P07-2045,0,0.0586397,"ce sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed"
P16-1212,P11-1060,0,0.0109095,"d on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what i"
P16-1212,P13-1078,0,0.0244095,"es for number of objects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a senten"
P16-1212,P14-1140,1,0.808281,"bjects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also c"
P16-1212,P03-1021,0,0.0453806,"periments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed KBSE, the number of hidden units in both parts are 300. Embedding size of both source and target are 200. Adadelta (Zeiler, 2012) 1 http://www.statmt.org/moses/ ht"
P16-1212,P02-1040,0,0.100737,"Missing"
P16-1212,D15-1199,0,0.0201843,"Missing"
P16-1212,P15-1128,0,0.00689599,"ce into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along with the sentence generate"
P16-1212,J07-2003,0,\N,Missing
P17-1018,P16-1086,0,0.0145181,"t that passage parts are of different importance to the particular question for reading comprehension and question answering. are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style questio"
P17-1018,D14-1159,0,0.00946933,"odel for encoding evidence from the passage, we draw the align194 Result Analysis (a) (b) (c) (d) Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated. model and its ablati"
P17-1018,D15-1161,0,0.0171807,"yen et al., 2016) is also a large-scale dataset. The questions in the dataset Related Work Reading Comprehension and Question Answering Dataset Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Exist195 self-matching attention in our model. It dynamically refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage information. Weightedly attending to word context has been proposed in several works. Ling et al. (2015) propose considering window-based contextual words differently depending on the word and its relative position. Cheng et al. (2016) propose a novel LSTM network to encode words in a sentence which considers the relation between the current token being processed and its past tokens in the memory. Parikh et al. (2016) apply this method to encode words in a sentence according to word form and its distance. Since passage information relevant to question is more helpful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated attention-ba"
P17-1018,P16-1223,0,0.37859,"t. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated. model and its ablation models. As we can see, both four models show the same trend. The questions are split into different groups based on a set of question words we have defined, including “what”, “how”, “who”, “when”, “which”, “where”, and “why”. As we can see, our model is better at “when” and “who” questions, but poorly on “why” questions. This is mainly because the answers to why questions can be very diverse, and they are"
P17-1018,D16-1053,0,0.675629,"ive Innovation Center for Language Ability, Xuzhou, 221009, China {wangwenhui,chbb}@pku.edu.cn {nanya,fuwei,mingzhou}@microsoft.com Abstract 2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016). Rapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages. Inspired by Wang and Jiang (2016b), we introduce a gated self-matching network, illustrated in Figure 1, an end-to-end neural network model for reading comprehension and question answering. Our model consists of four parts:"
P17-1018,P14-5010,0,0.00543631,"Missing"
P17-1018,D14-1179,0,0.0126271,"Missing"
P17-1018,D16-1244,0,0.102298,"Missing"
P17-1018,N16-1170,0,0.257789,"d Question Answering Wenhui Wang†§∗ Nan Yang‡§ Furu Wei‡ Baobao Chang† Ming Zhou‡ † Key Laboratory of Computational Linguistics, Peking University, MOE, China ‡ Microsoft Research, Beijing, China  Collaborative Innovation Center for Language Ability, Xuzhou, 221009, China {wangwenhui,chbb}@pku.edu.cn {nanya,fuwei,mingzhou}@microsoft.com Abstract 2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016). Rapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages. Inspired by Wang and Jiang ("
P17-1018,D14-1162,0,0.122829,"Missing"
P17-1018,D16-1264,0,0.76541,"tation is that it has very limited knowledge of context. One answer candidate is often oblivious to important (5) 191 When predicting the start position, hat−1 represents the initial hidden state of the answer recurrent network. We utilize the question vector rQ as the initial state of the answer recurrent network. rQ = att(uQ , VrQ ) is an attention-pooling vector of the question based on the parameter VrQ : cues in the passage outside its surrounding window. Moreover, there exists some sort of lexical or syntactic divergence between the question and passage in the majority of SQuAD dataset (Rajpurkar et al., 2016). Passage context is necessary to infer the answer. To address this problem, we propose directly matching the question-aware passage representation against itself. It dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation hPt : hPt = BiRNN(hPt−1 , [vtP , ct ]) Q Q sj = vT tanh(WuQ uQ j + Wv V r ) ai = exp(si )/Σm j=1 exp(sj ) Q rQ = Σm i=1 ai ui To train the network, we minimize the sum of the negative log probabilities of the ground truth start"
P17-1018,D13-1020,0,0.104396,"show the ability of the model for encoding evidence from the passage, we draw the align194 Result Analysis (a) (b) (c) (d) Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated."
P17-1018,D15-1237,0,0.122175,"Missing"
P17-1018,D16-1013,0,0.0821462,"cular question for reading comprehension and question answering. are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style question answering task by combining an attentive model with a rerankin"
P17-1046,P16-1094,0,0.0461305,"Missing"
P17-1046,D13-1096,0,0.841801,"t neural network (RNN) which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN. An empirical study on two public data sets shows that SMN can significantly outperform stateof-the-art methods for response selection in multi-turn conversation. 1 utterance 3 utterance 4 utterance 5 response 1 response 2 Table 1: An example of multi-turn conversation the current conversation from a repository with response selection algorithms. While most existing work on retrieval-based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context. The key to response selection lies in inputresponse matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also matching between responses a"
P17-1046,W15-4640,0,0.747982,"urnResponseSelection Our contributions in this paper are three-folds: (1) the proposal of a new context based matching model for multi-turn response selection in retrieval-based chatbots; (2) the publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets. the order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)). We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus, responses in these models connect with the context until the final ste"
P17-1046,C16-1063,0,0.0880695,"r matching in its hidden states in the chronological order of the utterances in context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The matching degree of the context and the response is computed by a logit 2 Related Work Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Ji et al., 2014) has drawn significant attention. Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation-based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing et al., 497 Word Embedding GRU1 Segment Pairs Word Pairs GRU2 u1 .... v1 .... .... .... .... un1 .... h '1 .... vn 1 h 'n1 vn h 'n L( ) Score un r M1, M2 Convolution Utterance-Response Matching (First Layer) Pooling Matching Accumulation Matching Prediction (Second Layer) (Third Layer) Figure 1: Architecture of SMN SMN first decomposes context-response matching into several utterance-response pair matching and then all pairs matchin"
P17-1046,D11-1054,0,0.715416,"class” and “drum” in context are very important. Without them, one may find responses relevant to the message (i.e., the fifth utterance of the context) but nonsense in the context (e.g., “what lessons do you want?”). Second, the message highly depends on the second utterance in the context, and Introduction Conversational agents include task-oriented dialog systems and non-task-oriented chatbots. Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011). Existing work on building chatbots includes generation -based methods and retrieval-based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for ∗ Context Human: How are you doing? ChatBot: I am going to hold a drum class in Shanghai. Anyone wants to join? The location is near Lujiazui. Human: Interesting! Do you have coaches who can help me practice drum? ChatBot: Of course. Human: Can I have a free first lesson? Response Candidates Sure. Have you ever played drum before? X What lessons do you want? 7 Correspondin"
P17-1046,P15-1152,0,0.0917506,"es of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer. 2016; Serban et al., 2016). Our work is a retrievalbased method, in which we study context-based response selection. Early studies of retrieval-based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers have begun to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained. 3 3.1 SMN enjoys several ad"
P17-1046,N16-1174,0,0.0161726,"how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out. 3.5 − 4 With [h01 , . . . , h0n ], we define g(s, r) as (6) where W2 and b2 are parameters. We consider three parameterizations for L[h01 , . . . , h0n ]: (1) only the last hidden state is used. Then L[h01 , . . . , h0n ] = h0n . (2) the hidden states are combined. Then, L[h01 , . . . , h0n ] = Pn linearly 0 i=1 wi hi , where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states. Then, L[h01 , . . . , h0n ] is defined as 5 ti = tanh(W1,1 hui ,nu + W1,2 h0i + b1 ), exp(t> i ts ) , > (exp(t i ts )) i n X αi h0i , L[h01 , . . . , h0n ] = αi = P i=1 [yi log(g(si , ri )) + (1 − yi )log(1 − g(si , ri ))] . (8) Response Candidate Retrieval In practice, a retrieval-based chatbot, to apply the matching approach to the response selection, one needs to retrieve a number of response candidates from an index beforehand. While candidate retrieval is not the focus of the paper, it is an important step in a real system."
P17-1046,D16-1036,0,0.784002,"he publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets. the order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)). We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus, responses in these models connect with the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The m"
P17-1065,P05-1033,0,0.559021,"Missing"
P17-1065,C16-2064,0,0.0395317,"Missing"
P17-1065,W16-4616,0,0.0282882,"Ensemble) RNNsearch SD-NMT BLEU 18.72 18.45 20.36 22.86 24.71 26.22 23.50 25.93 RIBES 0.6511 0.6451 0.6782 0.7508 0.7566 0.7459 0.7540 System Description Moses’ Hierarchical Phrase-based SMT Moses’ Phrase-based SMT Moses’ String-to-Tree Syntax-based SMT Single-layer NMT model without ensemble Self-ensemble of 2-layer NMT model Ensemble of 4 single-layer NMT models Single-layer NMT model Single-layer SD-NMT model Table 2: Evaluation results on Japanese-English translation task. model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL. Cromieres et al., 2016) that are the competitive NMT results on WAT 2016. According to Table 2, NMT results still outperform SMT results similar to our Chinese-English evaluation results. The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in (Cromieres, 2016). Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in (Cromieres, 2016; Cromieres et al., 2016). We believe SD"
P17-1065,P15-1033,0,0.0124,"Missing"
P17-1065,N16-1024,0,0.224787,"ve two RNNs in our model, the termination condition is also different from a conventional NMT model. In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced. Figure 3 (a) gives an overview of our SD-NMT model. Due to space limitation, the detailed interconnections between two RNNs are only illustrated at timestamp j. The encoder of our model 3.1 (10) Syntactic Context for Target Word Prediction Syntax has been proven useful for sentence generation task (Dyer et al., 2016). We propose to leverage target syntax to help translation generation. In our model, the syntactic context Kj at timestamp j is defined as a vector which is computed by a feed-forward network based on current 701 stamp for each training instance. Thus it is easy for the model to process multiple trees in one batch. In the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transition action sequence normalized by the sequence length: parsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the"
P17-1065,W16-4610,0,0.0122509,"n has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT. Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to Translation Example In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testsets. SMT and RNNsearch refer to the translation resu"
P17-1065,P16-1078,0,0.228746,", Nan Yang‡ , Mu Li‡ , Ming Zhou‡ † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research {v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com Abstract Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The translation fragment in italic is locally fluent around the word is, but from a"
P17-1065,W04-0308,0,0.0351929,"Machine Translation (SD-NMT) model in our paper. An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic dependency trees. The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by Nivre (2004). P (Y |X) = n Y j=1 P (yj |y<j , H) (1) Typically, for the jth target word, the probability P (yj |y<j , H) is computed as P (yj |y<j , H) = g(sj , yj−1 , cj ) (2) where g is a nonlinear function that outputs the probability of yj , and sj is the RNN hidden state. The context cj is calculated at each timestamp j based on H by the attention network cj = m X ajk hk (3) k=1 ajk = exp(ejk ) Pm i=1 exp(eji ) ejk = vaT tanh(Wa sj−1 + Ua hk ) (4) (5) where va , Wa , Ua are the weight matrices. The attention mechanism is effective to model the correspondences between source and target. 699 2.2 Depend"
P17-1065,P06-1121,0,0.0266136,"Missing"
P17-1065,P02-1040,0,0.128454,"Missing"
P17-1065,P08-1066,0,0.0170142,"slation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby. Fortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees. In Figure 1, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is. This structure has been successfully applied to significantly improve the performance of statistical machine translation (Shen et al., 2008). On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models: Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation"
P17-1065,D10-1092,0,0.0354514,"Missing"
P17-1065,P16-1159,0,0.0688833,"e-to-Dependency Neural Machine Translation Shuangzhi Wu†∗, Dongdong Zhang‡ , Nan Yang‡ , Mu Li‡ , Ming Zhou‡ † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research {v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com Abstract Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The"
P17-1065,P15-1001,0,0.0266306,"Missing"
P17-1065,P16-1008,0,0.0188276,"the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks. 1 Introduction Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). ∗ Contribution during internship at Microsoft Research. 698 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 c Vancouver, Canada, July 30 - August 4"
P17-1065,1983.tc-1.13,0,0.455856,"Missing"
P17-1065,W04-3250,0,0.020876,"Missing"
P17-1065,P06-1077,0,0.341955,"Missing"
P17-1065,D15-1166,0,0.116367,"NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks. 1 Introduction Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). ∗ Contribution during internship at Microsoft Research. 698 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 c Vancouver, Canada,"
P17-1065,D16-1050,0,0.0111325,"corporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT. Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to Translation Example In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testse"
P17-1065,P15-1002,0,0.0265242,"Missing"
P17-1065,D08-1059,0,0.0126196,"uence length: parsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the stack, w0l and w1l are their leftmost modifiers in the partial tree, w0r and w1r their rightmost modifiers respectively. We define two unigram features and four bigram features. The unigram features are w0 and w1 which are represented by the word embedding vectors. The bigram features are w0 w0l , w0 w0r , w1 w1l and w1 w1r . Each of them is computed by bhc = tanh(Wb Ewh + Ub Ewhc ), h ∈ {0, 1}, c ∈ {l, r}. These kinds of feature template have beeb proven effective in dependency parsing task (Zhang and Clark, 2008). Based on these features, the syntactic context vector Kj is computed as l score = j=1 l 1X δ(SH, aj ) log P (ˆ yj |ˆ y<j , X, A≤j ) n where n is word sequence length and l is action sequence length. 4 where Wk , Uk , Wb , Ub are the weight matrices, E stands for the embedding matrix. Figure 2 (b) gives an overview of the construction of Kj . Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in computation. Adding Kj to Equation 2, the probabilities of transition action and word in Equation 7 and"
P17-1065,P11-2033,0,\N,Missing
P17-1065,P16-5005,0,\N,Missing
P17-1101,P00-1041,0,0.847188,"ling the salient information from the highlight to generate a fluent sentence. We model the distilling process with selective encoding. Introduction Sentence summarization aims to shorten a given sentence and produce a brief summary of it. This is different from document level summarization task since it is hard to apply existing techniques in extractive methods, such as extracting sentence level features and ranking sentences. Early works propose using rule-based methods (Zajic et al., 2007), syntactic tree pruning methods (Knight and Marcu, 2002), statistical machine translation techniques (Banko et al., 2000) and so on for this task. We focus on abstractive sentence summarization task in this paper. Recently, neural network models have been applied in this task. Rush et al. (2015) use autoconstructed sentence-headline pairs to train a neu∗ Contribution during internship at Microsoft Research. All the above works fall into the encodingdecoding paradigm, which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information. As an extension of the encoding-decoding framework, attentionbased approach (Bahdanau et al., 2015)"
P17-1101,P16-1046,0,0.0503931,"Gigaword and DUC 2004 test sets show that the above models achieve state-of-theart results. Gu et al. (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment to segment neural transduction model for sequence-tosequence framework. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. Experiments on this task show that the proposed transduction model per1096 forms comparable to the ABS model. Shen et al. (2016) propose to apply Minimum Risk Training (MRT) in neural machine translation to directly optimize the evaluation metrics. Ayana et al. (2016) apply MRT"
P17-1101,D14-1179,0,0.00739871,"Missing"
P17-1101,N16-1012,0,0.72835,"y statistical machine translation techniques by modeling headline generation as a translation task and use 8000 article-headline pairs to train the system. Rush et al. (2015) propose leveraging news data in Annotated English Gigaword (Napoles et al., 2012) corpus to construct large scale parallel data for sentence summarization task. They propose an ABS model, which consists of an attentive Convolutional Neural Network encoder and an neural network language model (Bengio et al., 2003) decoder. On this Gigaword test set and DUC 2004 test set, the ABS model produces the state-of-theart results. Chopra et al. (2016) extend this work, which keeps the CNN encoder but replaces the decoder with recurrent neural networks. Their experiments showes that the CNN encoder with RNN decoder model performs better than Rush et al. (2015). Nallapati et al. (2016) further change the encoder to an RNN encoder, which leads to a full RNN sequence-to-sequence model. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. Experiments on the Gigaword and DUC 2004 test sets show that the"
P17-1101,W03-0501,0,0.313208,"2 recall and 10.63 ROUGE-2 F1 on these test sets respectively, which improves performance compared to the state-of-the-art methods. 2 Related Work Abstractive sentence summarization, also known as sentence compression and similar to headline generation, is used to help compress or fuse the selected sentences in extractive document summarization systems since they may inadvertently include unnecessary information. The sentence summarization task has been long connected to the headline generation task. There are some previous methods to solve this task, such as the linguistic rule-based method (Dorr et al., 2003). As for the statistical machine learning based methods, Banko et al. (2000) apply statistical machine translation techniques by modeling headline generation as a translation task and use 8000 article-headline pairs to train the system. Rush et al. (2015) propose leveraging news data in Annotated English Gigaword (Napoles et al., 2012) corpus to construct large scale parallel data for sentence summarization task. They propose an ABS model, which consists of an attentive Convolutional Neural Network encoder and an neural network language model (Bengio et al., 2003) decoder. On this Gigaword tes"
P17-1101,P16-1154,0,0.212515,"but replaces the decoder with recurrent neural networks. Their experiments showes that the CNN encoder with RNN decoder model performs better than Rush et al. (2015). Nallapati et al. (2016) further change the encoder to an RNN encoder, which leads to a full RNN sequence-to-sequence model. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. Experiments on the Gigaword and DUC 2004 test sets show that the above models achieve state-of-theart results. Gu et al. (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al"
P17-1101,P16-1014,0,0.192029,"der with recurrent neural networks. Their experiments showes that the CNN encoder with RNN decoder model performs better than Rush et al. (2015). Nallapati et al. (2016) further change the encoder to an RNN encoder, which leads to a full RNN sequence-to-sequence model. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. Experiments on the Gigaword and DUC 2004 test sets show that the above models achieve state-of-theart results. Gu et al. (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment"
P17-1101,N16-1082,0,0.00759375,"ovements compared to the s2s+att model. Overall, these improvements on all groups indicate that the selective encoding method benefits the abstractive sentence summarization task. ROUGE-2 F1 Scores Models 20 15 10 5 0 10 20 30 40 Input Sentence Length 50 60 Figure 4: ROUGE-2 F1 score on different groups of input sentences in terms of their length for s2s+att baseline and our SEASS model on English Gigaword test sets. Saliency Heat Map of Selective Gate Since the output of the selective gate network is a high dimensional vector, it is hard to visualize all the gate values. We use the method in Li et al. (2016) to visualize the contribution of the selective gate to the final output, which can be approximated by the first derivative. Given sentence words x with associated output summary y, the trained model associates the pair (x, y) with a score Sy (x). The goal is to decide which gate g associated with a specific word makes the most significant contribution to Sy (x). We approximate the Sy (g) by computing the first-order Taylor expansion since the score Sy (x) is a highly non-linear function in the deep neural network models: Sy (g) ≈ w(g)T g + b (19) where w(g) is first the derivative of Sy with"
P17-1101,W04-1013,0,0.0638597,"proximately 6,000 source text sentences with multiple manually-created summaries (about 26,000 sentence-summary pairs in total). Toutanova et al. (2016) provide a standard split of the data into training, development, and test sets, with 4,936, 448 and 785 input sentences respectively. Since the training set is too small, we only use the test set as one of our test sets. We denote this dataset as MSR-ATC (Microsoft Research Abstractive Text Compression) test set in the following. Table 2 summarizes the statistic information of the three datasets we used. 5.2 Evaluation Metric We employ ROUGE (Lin, 2004) as our evaluation metric. ROUGE measures the quality of summary by computing overlapping lexical units, such as unigram, bigram, trigram, and longest common subsequence (LCS). It becomes the standard evaluation metric for DUC shared tasks and popular for summarization evaluation. Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bi2 Thanks to Rush et al. (2015), we acquired the test set they used. Following Chopra et al. (2016), we remove pairs with empty titles resulting in slightly different accuracy compared to Rush et al. (2015) for their systems. The cleaned test set contains 1"
P17-1101,D15-1166,0,0.0301138,"r and the selective gate network, we use GRU with attention as the decoder to produce the output summary. At each decoding time step t, the GRU reads the previous word embedding wt−1 and previous context vector ct−1 as inputs to compute the new hidden state st . To initialize the GRU hidden state, we use a linear layer with the last backward encoder hidden state h~1 as input: st = GRU(wt−1 , ct−1 , st−1 ) s0 = tanh(Wd h~1 + b) (10) (11) where Wd is the weight matrix and b is the bias vector. The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state st with each encoder hidden state h0i to get an importance score. The importance scores are then normalized to get the current context vector by weighted sum: (8) (9) where Ws and Us are weight matrices, b is the bias vector, σ denotes sigmoid activation function, and is element-wise multiplication. After the selective gate network, we obtain another sequence of vectors (h01 , h02 , . . . , h0n ). This new sequence is then used as the input sentence representation for the decoder to generate the summary. Summary Decoder et,i = va&gt; tanh(Wa st−1 + Ua h0i"
P17-1101,K16-1028,0,0.26566,"Missing"
P17-1101,W12-3018,0,0.0538818,"Missing"
P17-1101,D16-1033,0,0.0242887,"hat except for the empty titles, this test set has some invalid lines like the input sentence containing only one word. Therefore, we further sample 2000 pairs as our internal test set and release it for future works3 . DUC 2004 Test Set We employ DUC 2004 data for tasks 1 & 2 (Over et al., 2007) in our experiments as one of the test sets since it is too small to train a neural network model on. The dataset pairs each document with 4 different human-written reference summaries which are capped at 75 bytes. It has 500 input sentences with each sentence paired with 4 summaries. MSR-ATC Test Set Toutanova et al. (2016) release a new dataset for sentence summarization task by crowdsourcing. This dataset contains approximately 6,000 source text sentences with multiple manually-created summaries (about 26,000 sentence-summary pairs in total). Toutanova et al. (2016) provide a standard split of the data into training, development, and test sets, with 4,936, 448 and 785 input sentences respectively. Since the training set is too small, we only use the test set as one of our test sets. We denote this dataset as MSR-ATC (Microsoft Research Abstractive Text Compression) test set in the following. Table 2 summarizes"
P17-1101,D16-1138,0,0.0411574,". (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment to segment neural transduction model for sequence-tosequence framework. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. Experiments on this task show that the proposed transduction model per1096 forms comparable to the ABS model. Shen et al. (2016) propose to apply Minimum Risk Training (MRT) in neural machine translation to directly optimize the evaluation metrics. Ayana et al. (2016) apply MRT on abstractive sentence summarization task and the results show that optimizing for ROUGE im"
P17-1101,D15-1044,0,0.425453,"to shorten a given sentence and produce a brief summary of it. This is different from document level summarization task since it is hard to apply existing techniques in extractive methods, such as extracting sentence level features and ranking sentences. Early works propose using rule-based methods (Zajic et al., 2007), syntactic tree pruning methods (Knight and Marcu, 2002), statistical machine translation techniques (Banko et al., 2000) and so on for this task. We focus on abstractive sentence summarization task in this paper. Recently, neural network models have been applied in this task. Rush et al. (2015) use autoconstructed sentence-headline pairs to train a neu∗ Contribution during internship at Microsoft Research. All the above works fall into the encodingdecoding paradigm, which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information. As an extension of the encoding-decoding framework, attentionbased approach (Bahdanau et al., 2015) has been broadly used: the encoder produces a list of vectors for all tokens in the input, and the decoder uses an attention mechanism to dynamically extract encoded informati"
P17-1101,P16-1159,0,0.0161797,"ate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment to segment neural transduction model for sequence-tosequence framework. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. Experiments on this task show that the proposed transduction model per1096 forms comparable to the ABS model. Shen et al. (2016) propose to apply Minimum Risk Training (MRT) in neural machine translation to directly optimize the evaluation metrics. Ayana et al. (2016) apply MRT on abstractive sentence summarization task and the results show that optimizing for ROUGE improves the test performance. 3 Input: Output: South Korean President Kim Young-Sam left here Wednesday on a week - long state visit to Russia and Uzbekistan for talks on North Korea ’s nuclear confrontation and ways to strengthen bilateral ties . Kim leaves for Russia for talks on NKorea nuclear standoff Table 1: An abstractive sentence summarization exam"
P17-1174,W14-4012,0,0.135364,"Missing"
P17-1174,D14-1179,0,0.139136,"Missing"
P17-1174,W16-4616,0,0.0627963,"Missing"
P17-1174,W16-4617,0,0.281753,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,P16-1078,0,0.444205,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,W08-0509,0,0.0958144,"Missing"
P17-1174,D10-1092,0,0.0822748,"Missing"
P17-1174,D13-1176,0,0.543806,"roperties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state h"
P17-1174,2010.eamt-1.27,0,0.242941,"re serious in free word-order languages such as Czech, German, Japanese, and Turkish. In the case of the example in Figure 1, the order of the phrase “早く (early)” and the phrase “家へ (to home)” is flexible. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order. In the past, chunks (or phrases) were utilized to handle the above problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010). By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) word orders and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages. In this paper, we refine the original RNN decoder to consider chunk information in NMT"
P17-1174,N03-1017,0,0.138863,"Missing"
P17-1174,P15-1107,0,0.0436534,"Missing"
P17-1174,D15-1106,1,0.817883,"tructures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network. Considering that our Model 1 described in § 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multi-layer RNN s to capture hierarchical structures in data. Hierarchical RNNs are used for various NLP tasks such as machine translation (Luong and Manning, 2016), document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical encoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly ca"
P17-1174,P02-1040,0,0.098266,"Missing"
P17-1174,P16-1100,0,0.223268,"based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi−1 : hi = GRU(hi−1 , xi ). (1) The function GRU(·) is calculated as ri = σ(Wr xi +"
P17-1174,P15-1002,0,0.0264998,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,D15-1166,0,0.0747669,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,C00-1082,0,0.0179728,"Missing"
P17-1174,W16-4610,0,0.0340474,"Missing"
P17-1174,W15-5003,0,0.0366542,"Missing"
P17-1174,P11-2093,0,0.0894041,"Missing"
P17-1174,J03-1002,0,0.017893,"Missing"
P17-1174,W16-2323,0,0.0517383,"Missing"
P17-1174,P15-1150,0,0.0592716,"Missing"
P17-1174,P03-1039,0,0.041371,"Missing"
P17-1174,D09-1160,1,0.802151,"Missing"
P17-1174,C10-1140,1,0.849059,"Missing"
P17-1174,C14-1103,1,0.828074,"Missing"
P17-1174,P17-2092,0,0.0515711,"trast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes decoders for NMT that can capture plausible linguistic structures such as chunk. Finally, we noticed that (Zhou et al., 2017) (which is accepted at the same time as this paper) have also proposed a chunk-based decoder for NMT . Their good experimental result on Chinese to English translation task also indicates the effectiveness of “chunk-by-chunk” decoders. Although their architecture is similar to our Model 2, there are several differences: (1) they adopt chunk-level attention instead of word-level attention; (2) their model predicts chunk tags (such as noun phrase), while ours only predicts chunk boundaries; and (3) they employ a boundary gate to decide the chunk boundaries, while we do that by simply having the"
P17-1174,P07-2045,0,\N,Missing
P17-4017,D14-1162,0,0.11618,"e “No. The keyboard is extra and functions as the cover”. Therefore, the question ranking problem is essential for an FAQ search engine. Formally, given two questions q and q0, the question ranker will learn a mapping function f where f (q, q0) → [0, 1], so f is actually a semantic similarity metric between two questions, indicating whether they convey the same meaning. We train a regression forest model (Meinshausen, 2006) and use the following features: monolingual word aligner (Sultan et al., 2014), DSSM model (Huang et al., 2013), word embedding composition (max, sum, idf-sum) with GloVe (Pennington et al., 2014), n-gram overlap, subsequence matching, PairingWords (Han et al., 2013), word mover’s distance (Kusner et al., 2015). The model is evaluated on the dataset for SemEval-2016 Task 1. The mean accuracy (Pearson Correlation) of our model on five datasets (0.78455) significantly outperforms the 1st place team (0.77807)1 . Specifically, the accuracy on the question-to-question dataset is 0.75773, surpassing the 1st place team (0.68705) by a large margin. This confirms the effectiveness of our model on the FAQ search task. Q: What’s the CPU? A: The processor is 3 GHz Intel Core i5 for Microsoft Surfa"
P17-4017,J11-1002,0,0.0440437,"e with a keyboard Customer reviews provide rich information for different aspects of the product from users’ perA: No. The keyboard is extra and functions as the cover 1 http://alt.qcri.org/semeval2016/ task1/index.php?id=results 99 spective. They are very important resources for answering opinion-oriented questions. To this end, we first split the review text into sentences and run opinion mining modules to extract the aspects and corresponding opinions. We then use the text question answering module to generate responses (i.e. review sentences). For opinion mining, we use a hybrid approach (Qiu et al., 2011) to extract the aspects from review sentences. We also run the sentiment classifier (Tang et al., 2014) to determine the polarity (i.e. positive, negative, and neutral) of the sentence regarding the specific aspect mentioned. The aspects and polarity are indexed together with keywords using the Lucene toolkit2 . For text QA, given an input query, it outputs the answer based on the following three steps: Q: Is the screen size of surface pro 4 appropriate for reading? R: The screen is not too small like the iPad, neither is it bulky like a laptop. 2.5 The chit-chat engine is mainly designed to r"
P17-4017,Q14-1018,0,0.0226224,"sks “does it have a keyboard”, the question “does it come with a keyboard” will be matched and the answer should be “No. The keyboard is extra and functions as the cover”. Therefore, the question ranking problem is essential for an FAQ search engine. Formally, given two questions q and q0, the question ranker will learn a mapping function f where f (q, q0) → [0, 1], so f is actually a semantic similarity metric between two questions, indicating whether they convey the same meaning. We train a regression forest model (Meinshausen, 2006) and use the following features: monolingual word aligner (Sultan et al., 2014), DSSM model (Huang et al., 2013), word embedding composition (max, sum, idf-sum) with GloVe (Pennington et al., 2014), n-gram overlap, subsequence matching, PairingWords (Han et al., 2013), word mover’s distance (Kusner et al., 2015). The model is evaluated on the dataset for SemEval-2016 Task 1. The mean accuracy (Pearson Correlation) of our model on five datasets (0.78455) significantly outperforms the 1st place team (0.77807)1 . Specifically, the accuracy on the question-to-question dataset is 0.75773, surpassing the 1st place team (0.68705) by a large margin. This confirms the effectivene"
P17-4017,P14-1146,1,0.225227,"users’ perA: No. The keyboard is extra and functions as the cover 1 http://alt.qcri.org/semeval2016/ task1/index.php?id=results 99 spective. They are very important resources for answering opinion-oriented questions. To this end, we first split the review text into sentences and run opinion mining modules to extract the aspects and corresponding opinions. We then use the text question answering module to generate responses (i.e. review sentences). For opinion mining, we use a hybrid approach (Qiu et al., 2011) to extract the aspects from review sentences. We also run the sentiment classifier (Tang et al., 2014) to determine the polarity (i.e. positive, negative, and neutral) of the sentence regarding the specific aspect mentioned. The aspects and polarity are indexed together with keywords using the Lucene toolkit2 . For text QA, given an input query, it outputs the answer based on the following three steps: Q: Is the screen size of surface pro 4 appropriate for reading? R: The screen is not too small like the iPad, neither is it bulky like a laptop. 2.5 The chit-chat engine is mainly designed to reply to greeting queries such as “hello“ and “thank you”, as well as queries that cannot be answered by"
P17-4017,S13-1005,0,0.031184,"ion ranking problem is essential for an FAQ search engine. Formally, given two questions q and q0, the question ranker will learn a mapping function f where f (q, q0) → [0, 1], so f is actually a semantic similarity metric between two questions, indicating whether they convey the same meaning. We train a regression forest model (Meinshausen, 2006) and use the following features: monolingual word aligner (Sultan et al., 2014), DSSM model (Huang et al., 2013), word embedding composition (max, sum, idf-sum) with GloVe (Pennington et al., 2014), n-gram overlap, subsequence matching, PairingWords (Han et al., 2013), word mover’s distance (Kusner et al., 2015). The model is evaluated on the dataset for SemEval-2016 Task 1. The mean accuracy (Pearson Correlation) of our model on five datasets (0.78455) significantly outperforms the 1st place team (0.77807)1 . Specifically, the accuracy on the question-to-question dataset is 0.75773, surpassing the 1st place team (0.68705) by a large margin. This confirms the effectiveness of our model on the FAQ search task. Q: What’s the CPU? A: The processor is 3 GHz Intel Core i5 for Microsoft Surface Pro 4 (128 GB, 4 GB RAM, Intel Core i5) Q: What is the pre-installed"
P17-4017,N16-1170,0,0.0184876,"h ranks all candidate sentences with a regression based ranking framework. • Candidate triggering, which decides whether it is confident enough to output the candidate. Specifically, for candidate retrieval, we use the default ranker in Lucene to retrieve the top 20 candidate sentences. For candidate ranking, we build a regression based framework to rank all candidate sentences based on features designed at different levels of granularity. Our feature set consists of WordCnt, translation model, type matching, WordNet, and two neural network based methods BiCNN (Yu et al., 2014) and MatchLSTM (Wang and Jiang, 2016). We conduct experiments on the WikiQA (Yang et al., 2015) dataset. The results show that we achieve state-of-the-art results with 0.7164 in terms of MAP and 0.7332 in terms of MRR. For candidate triggering, as the ranking model outputs a regression score for each candidate sentence, we only output the candidate sentence whose score is higher than the threshold selected on the development set. The engine’s output examples are shown as follows: Q: hello R: hey how are you? Q: thank you R: you’re very welcome sir Q: you are so cute R: u r more 2.6 Meta Engine For each query, SuperAgent will call"
P17-4017,D15-1237,0,0.0138675,"ng framework. • Candidate triggering, which decides whether it is confident enough to output the candidate. Specifically, for candidate retrieval, we use the default ranker in Lucene to retrieve the top 20 candidate sentences. For candidate ranking, we build a regression based framework to rank all candidate sentences based on features designed at different levels of granularity. Our feature set consists of WordCnt, translation model, type matching, WordNet, and two neural network based methods BiCNN (Yu et al., 2014) and MatchLSTM (Wang and Jiang, 2016). We conduct experiments on the WikiQA (Yang et al., 2015) dataset. The results show that we achieve state-of-the-art results with 0.7164 in terms of MAP and 0.7332 in terms of MRR. For candidate triggering, as the ranking model outputs a regression score for each candidate sentence, we only output the candidate sentence whose score is higher than the threshold selected on the development set. The engine’s output examples are shown as follows: Q: hello R: hey how are you? Q: thank you R: you’re very welcome sir Q: you are so cute R: u r more 2.6 Meta Engine For each query, SuperAgent will call the abovementioned sub-engines in parallel. The meta engi"
P18-1006,W15-3014,0,0.0214419,"nt variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbou"
P18-1006,D13-1176,0,0.0940585,"can be rich) to improve the translation performance of lowresource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSR"
P18-1006,D16-1139,0,0.0206136,"Optimize Θz|y and Θx|z 9: Generate z 0 from p(z 0 |y) and build the training batches B3 = (y, z 0 ) ∪ (y ∗ , z ∗ ), B4 = (x, z 0 ) ∪ (x∗ , z ∗ ) 10: E-step: update Θz|y with B3 (Equation 7) 11: M-step: update Θx|z with B4 (Equation 8) 12: end while 13: return Θz|x , Θy|z , Θz|y and Θx|z Figure 2: Triangular Learning Architecture for Low-Resource NMT 2.3 Training Details A major difficulty in our unified bidirectional training is the exponential search space of the translation candidates, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo"
P18-1006,2005.mtsummit-papers.11,0,0.0832691,"xtra monolingual Z described in Table 1 to do back-translation; to train the model p(x|z), we use monolingual X taken from (X, Y ). Procedures for training p(z|y) and p(y|z) are similar. This method use extra monolingual data of Z compared with our TA-NMT method. But we can incorporate it into our method. choice. Note that in this dataset, low-resource pairs (X, Z) and (Y, Z) are severely overlapped in Z. In addition, English-French bilingual data from WMT2014 dataset are also used to enrich the rich-resource pair. We also use additional EnglishRomanian bilingual data from Europarlv7 dataset (Koehn, 2005). The monolingual data of Z (HE and RO) are taken from the web2 . In both datasets, all sentences are filtered within the length of 5 to 50 after tokenization. Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in Table 1. 3.2 Baselines We compare our method with four baseline systems. The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data"
P18-1006,N03-1017,0,0.0227686,"0 after tokenization. Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in Table 1. 3.2 Baselines We compare our method with four baseline systems. The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes. The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses5 for training and test in our experiments. The third baseline is a teacher-student alike method (Chen et al., 2017). For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3. We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve Figure 3: A teacher-student a"
P18-1006,D15-1166,0,0.0211414,"nd achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Our contributions are listed as follows: of the rich-resource pair English-French. • We propose a novel triangular training architecture (TA-NMT) to effectively tackle the data sparsity problem for rare languages in NMT with an EM framewor"
P18-1006,2012.eamt-1.60,0,0.0142104,"h, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks. As is mentioned in section 1, our method is compatible with methods exploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method. MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We randomly choose subsets of bilingual data of (X, Z) and (Y, Z) in the original dataset to simulate low-resource situations,"
P18-1006,W17-4739,0,0.0156186,"zed with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computa"
P18-1006,P17-1176,0,0.170018,"), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes. The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses5 for training and test in our experiments. The third baseline is a teacher-student alike method (Chen et al., 2017). For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3. We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve Figure 3: A teacher-student alike method for low-resource translation. For training p(z|x) and p(x|z), we mix the true pair (y ∗ , z ∗ ) ∈ D with the pseudo pair (x0 , z ∗ ) generated by teacher model p (x0 |y ∗ ) in the same mini-batch. The training procedure of p(z|y) and p(y|z) is similar. 3.3 Overall Results Experimental results on both datasets are shown in Table 3 and 4 r"
P18-1006,P16-1009,0,0.0988588,"Missing"
P18-1006,P16-1185,0,0.357447,"jing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better capitalize on monolin"
P18-1006,W16-2323,0,0.039451,"anslation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15"
P18-1006,eisele-chen-2010-multiun,0,0.0358089,"tes, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks. As is mentioned in section 1, our method is compatible with methods exploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method. MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We r"
P18-1006,1983.tc-1.13,0,0.45006,"Missing"
P18-1006,D16-1160,0,0.0545878,"ty, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better c"
P18-1006,D16-1163,0,0.272716,"uage Translation ∗ Shuo Ren1,2 , Wenhu Chen3 , Shujie Liu4 , Mu Li4 , Ming Zhou4 and Shuai Ma1,2 1 2 SKLSDE Lab, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint traini"
P18-1006,N16-1101,0,\N,Missing
P18-1034,Q13-1005,0,0.0484716,"udies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model paramet"
P18-1034,D13-1160,0,0.522838,"Missing"
P18-1034,D14-1179,0,0.0149909,"Missing"
P18-1034,H94-1010,0,0.425333,"he input, and outputs a SQL query y. We do not consider the join operation over multiple relational tables, which we leave in the future work. We use WikiSQL (Zhong et al., 2017), the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of S"
P18-1034,P17-1174,1,0.889119,"Missing"
P18-1034,N16-1024,0,0.0300555,"imum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect t"
P18-1034,P17-1089,0,0.106833,"mn names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti"
P18-1034,C12-2040,0,0.0387214,", the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL que"
P18-1034,P17-1167,0,0.0492084,"Missing"
P18-1034,P18-1168,0,0.060629,"Missing"
P18-1034,P16-1154,0,0.051477,"etworks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One advantage of this architecture is that it"
P18-1034,P16-1002,0,0.0341733,"al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. questi"
P18-1034,P16-1014,0,0.0177017,"y We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One"
P18-1034,P16-1086,0,0.028017,"s a probability distribution over the tokens from one of the three channels. X p(yt |y&lt;t , x) = pw (yt |zt , y&lt;t , x)pz (zt |y&lt;t , x) Methodology We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and ge"
P18-1034,P17-1097,0,0.143076,"is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu"
P18-1034,D16-1116,0,0.0360868,"Missing"
P18-1034,D17-1160,0,0.26447,"sign of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4.2 STAMP: Syntax- and Table- Aware seMantic Parser Figure 2 illustrates an overview of the prop"
P18-1034,Q13-1016,0,0.0505279,"tional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) th"
P18-1034,D11-1140,0,0.0801846,"Missing"
P18-1034,P15-1142,0,0.163748,"et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Z"
P18-1034,P16-1003,0,0.0875917,"s in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for le"
P18-1034,P17-1003,0,0.0189993,"tional databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus base"
P18-1034,D14-1162,0,0.0809332,"ells, and weighted average two cell distributions, which is calculated as follows. cell pcell pcell w (j) = λˆ w (j) + (1 − λ)αj 4.5 As the WikiSQL data contains rich supervision of question-SQL pairs, we use them to train model parameters. The model has two cross-entropy loss functions, as given below. One is for the switching gate classifier (pz ) and another is for the attentional probability distribution of a channel (pw ). l=− X X logpw (yt |zt , y&lt;t , x) logpz (zt |y&lt;t , x)− t t (6) Our parameter setting strictly follows Zhong et al. (2017). We represent each word using word embedding2 (Pennington et al., 2014) and the mean of the sub-word embeddings of all the n-grams in the word (Hashimoto et al., 2016)3 . The dimension of the concatenated word embedding is 400. We clamp the embedding values to avoid over-fitting. We set the dimension of encoder and decoder hidden state as 200. During training, we randomize model parameters from a uniform distribution with fan-in and fan-out, set batch size as 64, set the learning rate of SGD as 0.5, and update the model with stochastic gradient descent. Greedy search is used in the inference process. We use the model trained from question-SQL pairs as initializat"
P18-1034,P13-1092,0,0.0220016,"ses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic exec"
P18-1034,P11-1060,0,0.149695,"set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coup"
P18-1034,P17-1105,0,0.048093,"ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has n"
P18-1034,P17-2034,0,0.0222559,"to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic p"
P18-1034,D07-1071,0,0.600848,"Missing"
P18-1034,P07-1121,0,0.306045,"Missing"
P18-1034,D17-1125,0,0.635741,"Missing"
P18-1034,P17-1065,1,0.837395,"17; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4"
P18-1034,P16-1127,0,0.111172,"Missing"
P18-1034,P17-1041,0,0.227409,"the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input se"
P18-1034,N18-2093,0,0.393921,"Missing"
P18-1034,P16-1004,0,\N,Missing
P18-1034,P16-1138,0,\N,Missing
P18-1061,P15-2136,1,0.953154,"nh,mingzhou}@microsoft.com Abstract Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958; Hovy and Lin, 1998; Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the"
P18-1061,P16-1046,0,0.136097,"a sentence. Sentence selection is based on the scores of 655 sidering the DUC tasks have byte length limit for summaries. In this work, we adopt the CNN/Daily Mail dataset to train the neural network model, which does not have this length limit. To prevent the tendency of choosing longer sentences, we use ROUGE F1 as the evaluation function r(·), and set the length limit l as a fixed number of sentences. Therefore, the proposed model is trained to learn a scoring function g(·) of the ROUGE F1 gain, specifically: a two-level attention mechanism to measure the contextual relations of sentences. Cheng and Lapata (2016) propose treating document summarization as a sequence labeling task. They first encode the sentences in the document and then classify each sentence into two classes, i.e., extraction or not. Nallapati et al. (2017) propose a system called SummaRuNNer with more features, which also treat extractive document summarization as a sequence labeling task. The two works are both in the separated paradigm, as they first assign a probability of being extracted to each sentence, and then select sentences according to the probability until reaching the length limit. Ren et al. (2016) train two neural ne"
P18-1061,W09-1802,0,0.104755,"these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Machine learning techniques are also widely used for better sentence modeling and importance estimation. Kupiec et al. (1995) use a Naive Bayes classifier to learn feature combinations. Conroy and O’leary (2001) further use a Hidden Markov Model in document summarization. Gillick and Favre (2009) find that using bigram features consistently yields better performance than unigrams or trigrams for ROUGE (Lin, 2004) measures. Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) method as a heuristic in sentence selection. Systems using MMR select the sentence which has the maximal score and is minimally redundant with previous selected sentences. McDonald (2007) treats sentence selection as an optimization problem under some constraints such as summary length. Therefore, he uses Integer Linear Programming (ILP) to solve this optimization problem. Sentence selectio"
P18-1061,P84-1044,0,0.585141,"Missing"
P18-1061,X98-1026,0,0.345759,"Summarization by Jointly Learning to Score and Select Sentences Qingyu Zhou†∗, Nan Yang‡ , Furu Wei‡ , Shaohan Huang‡ , Ming Zhou‡ , Tiejun Zhao† † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research, Beijing, China {qyzhou,tjzhao}@hit.edu.cn {nanya,fuwei,shaohanh,mingzhou}@microsoft.com Abstract Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958; Hovy and Lin, 1998; Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary"
P18-1061,D14-1162,0,0.0899533,"Missing"
P18-1061,W04-1013,0,0.342842,"ertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Machine learning techniques are also widely used for better sentence modeling and importance estimation. Kupiec et al. (1995) use a Naive Bayes classifier to learn feature combinations. Conroy and O’leary (2001) further use a Hidden Markov Model in document summarization. Gillick and Favre (2009) find that using bigram features consistently yields better performance than unigrams or trigrams for ROUGE (Lin, 2004) measures. Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) method as a heuristic in sentence selection. Systems using MMR select the sentence which has the maximal score and is minimally redundant with previous selected sentences. McDonald (2007) treats sentence selection as an optimization problem under some constraints such as summary length. Therefore, he uses Integer Linear Programming (ILP) to solve this optimization problem. Sentence selection can also be seen as finding the optimal subset of sentences in a document. Lin and Bilmes (2011) propose using submod"
P18-1061,C16-1004,1,0.938489,"entence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the optimal subset of sentences in a document. Ren et al. (2016) train two neural networks with handcrafted features. One is used to rank sentences, and the other one is used to model redundancy during sentence selection. In this paper, we present a neural extractive document summarization (N EU S UM) framework which jointly learns to score and select sentences. Different from previous methods that treat sentence scoring and sentence selection as two tasks, our method integrates the two steps into one endto-end trainable model. Specifically, N EU S UM is a neural network model without any handcrafted features that learns to identify the relative importance"
P18-1061,P11-1052,0,0.101239,"years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the optimal subset of sentences in a document. Ren et al. (2016) train two neural networks with handcrafted features. One is used to rank sentences, and the other one is used to model redundancy during sentence selection. In this paper, we present a neural extractive document summarization (N EU S UM) framework which jointly learns to score and select sentences. Different from previous methods that treat sentence scoring and sentence selection as two tasks, our method integrates the two steps into one endto-end trainable mo"
P18-1061,P17-1099,0,0.785313,"bel the sentences in a given document similar to Nallapati et al. (2017). Specifically, we construct training data by maximizing the ROUGE-2 F1 score. Since it is computationally expensive to find the global optimal combination of sentences, we employ a greedy approach. Given a document with n sentences, we n enumerate the candidates  from 1-combination 1 n to n-combination n . We stopsearching if the n highest ROUGE-2  F1 score in k is less than the n best one in k−1 . Table 1 shows the data statistics of the CNN/Daily Mail dataset. We conduct data preprocessing using the same method2 in See et al. (2017), including sentence splitting and word tokenization. Both Nallapati et al. (2016, 2017) use the anonymized version of the data, where the named entities are replaced by identifiers such as entity4. Following See et al. (2017), we use the non-anonymized version so we can directly operate on the original text. Si ∈D 4.3 (20) Experiments 5.1 whereWm and bm are learnable parameters, and s~1 is the last backward state of the document level encoder BiGRU. Since we do not have any sentences extracted yet, we use a zero vector to represent the previous extracted sentence, i.e., s0 = 0. With the score"
P18-1061,W04-3252,0,0.790016,"∗, Nan Yang‡ , Furu Wei‡ , Shaohan Huang‡ , Ming Zhou‡ , Tiejun Zhao† † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research, Beijing, China {qyzhou,tjzhao}@hit.edu.cn {nanya,fuwei,shaohanh,mingzhou}@microsoft.com Abstract Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958; Hovy and Lin, 1998; Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection"
P18-1061,N06-2046,0,0.175732,"for Computational Linguistics sentences to determine which sentence should be extracted, which is usually done heuristically. Many techniques have been proposed to model and score sentences. Unsupervised methods do not require model training or data annotation. In these methods, many surface features are useful, such as term frequency (Luhn, 1958), TF*IDF weights (Erkan and Radev, 2004), sentence length (Cao et al., 2015a) and sentence positions (Ren et al., 2017). These features can be used alone or combined with weights. Graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006) are also applied broadly to ranking sentences. In these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Machine learning techniques are also widely used for better sentence modeling and importance estimation. Kupiec et al. (1995) use a Naive Bayes classifier to learn feature combinations. Conroy and O’leary (2001) further use"
P18-1061,K16-1028,0,0.224849,"Missing"
P18-1097,I17-2058,0,0.0228356,"el to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We propose a novel fluency boost learning and inference mechanism to o"
P18-1097,W07-1604,0,0.121354,"Missing"
P18-1097,W17-5037,0,0.211908,"0 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result. We first compare our best models – dual-boost learning (+native) with fluency boost inference and shallow fusion LM – to top-performing GEC systems evaluated on CoNLL-2014 dataset: We give more details about disfluency candidates, inclu"
P18-1097,D11-1010,0,0.0136033,"G test set). System Source CAMB14 CAMB16SMT CAMB16NMT CAMB17 (CAMB16SMT based) CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Brisco"
P18-1097,D12-1052,0,0.184701,"use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe use CoNLL-2014 shared task dataset with tical to S ∗ . In the case where additional native data original annotations (Ng et al., 2014), which conis available to help model learning, S becomes: tains 1,312 sentences, as our main test set for evalS = S∗ ∪ C uation. We use MaxMatch (M2 ) precision, recall c c where C = {(x , x )} denotes the set of selfand F0.5 (Dahlmeier and Ng, 2012b) as our evaluacopied sentence pairs from native data. tion metrics. As previous studies, we use CoNLL2013 test data as our development set. 10: 11: 12: 4 Fluency boost inference As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference 5.2 Experimental setting We set up experiments in order to answer the following questions: 1059 Model normal seq2seq back-boost self-boost dual-boost back-boost (+native) self-boost (+native) dual-boost (+native) back-boost (+native)? self-boost (+native)? dual-boost (+nat"
P18-1097,N12-1067,0,0.644898,"use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe use CoNLL-2014 shared task dataset with tical to S ∗ . In the case where additional native data original annotations (Ng et al., 2014), which conis available to help model learning, S becomes: tains 1,312 sentences, as our main test set for evalS = S∗ ∪ C uation. We use MaxMatch (M2 ) precision, recall c c where C = {(x , x )} denotes the set of selfand F0.5 (Dahlmeier and Ng, 2012b) as our evaluacopied sentence pairs from native data. tion metrics. As previous studies, we use CoNLL2013 test data as our development set. 10: 11: 12: 4 Fluency boost inference As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference 5.2 Experimental setting We set up experiments in order to answer the following questions: 1059 Model normal seq2seq back-boost self-boost dual-boost back-boost (+native) self-boost (+native) dual-boost (+native) back-boost (+native)? self-boost (+native)? dual-boost (+nat"
P18-1097,W13-1703,0,0.396373,"h of them 5 Experiments are dynamically updated, which improves each 5.1 Dataset and evaluation other: the disfluency candidates produced by error generation model can benefit training the error As previous studies (Ji et al., 2017), we use the correction model, while the disfluency candidates public Lang-8 Corpus (Mizumoto et al., 2011; created by error correction model can be used as Tajiri et al., 2012), Cambridge Learner Cortraining data for the error generation model. We pus (CLC) (Nicholls, 2003) and NUS Corpus summarize this learning approach in Algorithm 3. of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. 3.4 Fluency boost learning with large-scale Table 1 shows the stats of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended to utilize the huge volume of native data we use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe us"
P18-1097,I13-1122,0,0.0200246,"CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 20"
P18-1097,W11-2838,0,0.0978,"Missing"
P18-1097,P06-1032,0,0.245061,"ult (GLEU=61.50 on JFLEG test set). System Source CAMB14 CAMB16SMT CAMB16NMT CAMB17 (CAMB16SMT based) CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq"
P18-1097,C08-1022,0,0.0710524,"Missing"
P18-1097,P17-1074,0,0.164354,"which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We propose a novel fluency boost learning and inf"
P18-1097,E14-3013,0,0.0794372,"kis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier"
P18-1097,W14-1702,0,0.451367,". It is clear that our approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Ch"
P18-1097,P11-2089,0,0.0269671,"al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language o"
P18-1097,N18-2046,0,0.427784,"t our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5 =54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5 =56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency boost learning and inference mechanism to their models. 8 The recently proposed SMT-NMT hybrid system (Grundkiewicz and Junczys-Dowmunt, 2018), which is tuned towards GLEU on JFLEG Dev set, report"
P18-1097,I11-1017,0,0.618333,"f (x ) {xok |xok ∈ Yn (xr ; Θcrt ) ∪ Yn (xc ; Θgen ) ∧ ≥ σ} input to generate the next output x . The prof (xok ) o cess will not terminate unless x t does not im(7) prove xot−1 in terms of fluency. Moreover, the error correction model and the error generation model are dual and both of them 5 Experiments are dynamically updated, which improves each 5.1 Dataset and evaluation other: the disfluency candidates produced by error generation model can benefit training the error As previous studies (Ji et al., 2017), we use the correction model, while the disfluency candidates public Lang-8 Corpus (Mizumoto et al., 2011; created by error correction model can be used as Tajiri et al., 2012), Cambridge Learner Cortraining data for the error generation model. We pus (CLC) (Nicholls, 2003) and NUS Corpus summarize this learning approach in Algorithm 3. of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. 3.4 Fluency boost learning with large-scale Table 1 shows the stats of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended"
P18-1097,N16-1133,0,0.0116492,"in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovsk"
P18-1097,han-etal-2010-using,0,0.0762136,"Missing"
P18-1097,P17-1070,0,0.569229,"st night. (b) She sees Tom caught by a policeman in the park last night. seq2seq inference She saw Tom caught by a policeman in the park last night. (c) Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can. Introduction Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models usually cannot perfectly correct a s"
P18-1097,W14-1703,0,0.0209873,"ke. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we a"
P18-1097,D16-1161,0,0.415141,"Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Fost"
P18-1097,I17-1005,0,0.0311243,"nd Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Ad"
P18-1097,P15-2097,0,0.523053,"introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset. Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning. Table 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et"
P18-1097,D16-1228,0,0.0192413,"luency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We propose a novel fluency b"
P18-1097,E17-2037,0,0.285943,"7 (AMU16 based) and NUS17. It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMTbased GEC system (AMU16’s framework); thus, they are ensemble models. When we build our approach on top of AMU16 (i.e., we take AMU16’s outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F0.5 score. With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset. Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning. Table 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform we"
P18-1097,W14-1701,0,0.737801,"of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended to utilize the huge volume of native data we use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe use CoNLL-2014 shared task dataset with tical to S ∗ . In the case where additional native data original annotations (Ng et al., 2014), which conis available to help model learning, S becomes: tains 1,312 sentences, as our main test set for evalS = S∗ ∪ C uation. We use MaxMatch (M2 ) precision, recall c c where C = {(x , x )} denotes the set of selfand F0.5 (Dahlmeier and Ng, 2012b) as our evaluacopied sentence pairs from native data. tion metrics. As previous studies, we use CoNLL2013 test data as our development set. 10: 11: 12: 4 Fluency boost inference As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference 5.2 Experimental settin"
P18-1097,P02-1040,0,0.103417,"tively, which is a stateof-the-art result7 on CoNLL-2014 dataset. Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning. Table 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset"
P18-1097,W17-5032,0,0.151049,"GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015;"
P18-1097,P16-1112,0,0.0226045,"ockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and"
P18-1097,W14-1704,0,0.178962,"which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the"
P18-1097,D15-1166,0,0.0469225,"g. • Whether is fluency boost learning mechanism helpful for training the error correction model, and which of the strategies (back-boost, selfboost, dual-boost) is the most effective? • Whether does our fluency boost inference improve normal seq2seq inference for GEC? • Whether can our approach improve neural GEC to achieve state-of-the-art results? The training details for our seq2seq error correction model and error generation model are as follows: the encoder of the seq2seq models is a 2-layer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism (Luong et al., 2015). Both the dimensionality of word embeddings and the hidden size of GRU cells are 500. The vocabulary sizes of the encoder and decoder are 100,000 and 50,000 respectively. The models’ parameters are uniformly initialized in [-0.1,0.1]. We train the models with an Adam optimizer with a learning rate of 0.0001 up to 40 epochs with batch size = 128. Dropout is applied to non-recurrent connections at a ratio of 0.15. For fluency boost learning, we generate disfluency candidates from 10-best outputs. During model inference, we set beam size to 5 and decode 1-best result with a 2-layer GRU RNN langu"
P18-1097,N10-1018,0,0.0270497,"o, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tet"
P18-1097,P11-1093,0,0.0615507,"Missing"
P18-1097,P16-1208,0,0.142982,"the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result. We first compare our best models – dual-boost learning (+native) with fluenc"
P18-1097,W12-2032,0,0.0199865,"et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani e"
P18-1097,Q16-1013,0,0.0185555,". Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We pro"
P18-1097,I17-2062,0,0.100917,"Missing"
P18-1097,D17-1298,0,0.549359,"sees Tom caught by a policeman in the park last night. seq2seq inference She saw Tom caught by a policeman in the park last night. (c) Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can. Introduction Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models usually cannot perfectly correct a sentence with many gramm"
P18-1097,N16-1042,0,0.564323,"s Tom caught by a policeman in the park last night. (b) She sees Tom caught by a policeman in the park last night. seq2seq inference She saw Tom caught by a policeman in the park last night. (c) Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can. Introduction Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models u"
P18-1097,W16-0530,0,0.298583,"r approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a chara"
P18-1097,W13-3607,0,0.0939701,"AMB17 (CAMB16SMT based) CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi"
P18-1097,P16-1009,0,0.0318795,"uring training, as Figure 2(a) illustrates, so that these pairs can further help model learning. In this section, we present three fluency boost learning strategies: back-boost, self-boost, and 2 Fluency of a sentence in this paper refers to how likely the sentence is written by a native speaker. In other words, if a sentence is very likely to be written by a native speaker, it should be regarded highly fluent. dual-boost that generate fluency boost sentence pairs in different ways, as illustrated in Figure 3. 3.1 Back-boost learning Back-boost learning borrows the idea from back translation (Sennrich et al., 2016) in NMT, referring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a fluent sentence to a less fluent sentence with errors. Since the less fluent sentences are generated by the error generation seq2seq model trained with error-corrected data, they usually do not change the original sentence’s meaning; thus, they can be paired with their correct sentences, establishing fluency boost sentence pairs that can be used as training instances for error correction models, as Figure 3(a) shows. Specifically, we first train a"
P18-1097,D14-1102,0,0.129719,"40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result. We first compare our best models – dual-boost learning (+native) with fluency boost inference and shallow fusion LM – to top-performing GEC systems evaluated on CoNLL-2014 dataset: W"
P18-1097,P12-2039,0,0.452191,"erate the next output x . The prof (xok ) o cess will not terminate unless x t does not im(7) prove xot−1 in terms of fluency. Moreover, the error correction model and the error generation model are dual and both of them 5 Experiments are dynamically updated, which improves each 5.1 Dataset and evaluation other: the disfluency candidates produced by error generation model can benefit training the error As previous studies (Ji et al., 2017), we use the correction model, while the disfluency candidates public Lang-8 Corpus (Mizumoto et al., 2011; created by error correction model can be used as Tajiri et al., 2012), Cambridge Learner Cortraining data for the error generation model. We pus (CLC) (Nicholls, 2003) and NUS Corpus summarize this learning approach in Algorithm 3. of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. 3.4 Fluency boost learning with large-scale Table 1 shows the stats of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended to utilize the huge volume of native data we use for fluency boost lea"
P18-1097,P10-2065,0,0.224949,"luation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5 =54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5 =56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency b"
P18-1097,W10-1006,0,0.11465,"luation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5 =54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5 =56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency b"
P18-1097,D17-1297,0,0.439115,"es with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses"
P18-2065,D14-1179,0,0.0380087,"Missing"
P18-2065,P16-1014,0,0.0145111,"nitial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 partitions and used data sampling in OpenNMT to train the model. This reduces the length of the epochs for more frequent learning rate updates and validation perplexity computation. Copying Mechanism Since most encoder-decoder methods maintain a fixed vocabulary of frequent words and convert a large number of long-tail words into a special symbol “hunki”, the copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017; Meng et al., 2017) is designed to copy words from the input sequence to the output sequence, thus enlarging the vocabulary and reducing the proportion of generated unknown words. For the neural Open IE task, the copying mechanism is more important because the output vocabulary is directly from the input vocabulary except for the placeholder symbols. We simplify the copying method in (See et al., 2017), the probability of generating the word yt comes from two parts as follows: ( p(yt |y1 , y2 , ..., yt−1 ; X) if yt ∈ V p(yt ) = P t otherwise i:xi =yt ai (5) where V is the ta"
P18-2065,W10-0907,0,0.035619,"n phrases. The first and second generation Open IE systems extract only relations that are mediated by verbs and ignore contexts. To alleviate these limitations, the third generation O LLIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, O PENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford O PENIE (Angeli et al., 2015), PropS (Stan"
P18-2065,P17-4012,0,0.0385064,"the-art baselines including O LLIE, ClausIE, Stanford O PENIE, PropS and O PENIE4. The evaluation metrics are precision and recall. αij hj j=1 exp(eij ) αij = Pn k=1 exp(eik ) (4) eij = a(si−1 , hj ) where a is an alignment model that scores how well the inputs around position j and the output at position i match, which is measured by the encoder hidden state hj and the decoder hidden state si−1 . The encoder and decoder are jointly optimized to maximize the log probability of the output sequence conditioned on the input sequence. 2.3 3.2 We implemented the neural Open IE model using OpenNMT (Klein et al., 2017), which is an open source encoder-decoder framework. We used 4 M60 GPUs for parallel training, which takes 3 days. The encoder is a 3-layer bidirectional LSTM and the decoder is another 3-layer LSTM. Our model has 256-dimensional hidden states and 256-dimensional word embeddings. A vocabulary of 50k words is used for both the source and target sides. We optimized the model with SGD and the initial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 part"
P18-2065,D15-1166,0,0.150582,"Missing"
P18-2065,D11-1142,0,0.950754,"a. For instance, given the sentence “deep learning is a subfield of machine learning”, the triple (deep learning; is a subfield of ; machine learning) can be extracted, where the relation phrase “is a subfield of ” indicates the semantic relationship between two arguments. Open IE plays a key role in natural language understanding and fosters many downstream NLP applications such as knowledge base construction, question answering, text comprehension, and others. The Open IE system was first introduced by T EXT RUNNER (Banko et al., 2007), followed by several popular systems such as R E V ERB (Fader et al., 2011), O LLIE (Mausam 1 2 https://github.com/allenai/openie-standalone https://github.com/dair-iitd/OpenIE-standalone 407 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 407–413 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics encoder decoder ?1 copying ?2 … ?? + 3-layer LSTM ?1 ?2 … ?? embedding attention ? ?1 ?2 … Figure 1: The encoder-decoder model architecture for the neural Open IE system learning h/arg2i”. We obtain the input and output sequence pairs from highly confident tuples bootstrapp"
P18-2065,D12-1048,0,0.755908,"r knowledge, this is the first time that the Open IE task is addressed using an end-to-end neural approach, bypassing the handcrafted patterns and alleviating error propagation. precision-recall curve. Following these efforts, the second generation known as R2A2 (Etzioni et al., 2011) was developed based on R E V ERB and an argument identifier, A RG L EARNER, to better extract the arguments for the relation phrases. The first and second generation Open IE systems extract only relations that are mediated by verbs and ignore contexts. To alleviate these limitations, the third generation O LLIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for"
P18-2065,D17-1278,0,0.446789,"Missing"
P18-2065,P17-1054,0,0.0620639,"ial to solve the problems of cascading errors to alleviate extracting incorrect tuples. To this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called ‘context vector’ which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, given the input sequence “deep learning is a subfield of machine learning”, the output sequence will"
P18-2065,W16-1307,0,0.153303,"LIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, O PENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford O PENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016), and more. 5 Conclusion and Future Work We proposed a neural Open IE approach using an encoder-decoder framework. The neural Open IE model is trained with highly"
P18-2065,P16-1154,0,0.0231749,"ith SGD and the initial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 partitions and used data sampling in OpenNMT to train the model. This reduces the length of the epochs for more frequent learning rate updates and validation perplexity computation. Copying Mechanism Since most encoder-decoder methods maintain a fixed vocabulary of frequent words and convert a large number of long-tail words into a special symbol “hunki”, the copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017; Meng et al., 2017) is designed to copy words from the input sequence to the output sequence, thus enlarging the vocabulary and reducing the proportion of generated unknown words. For the neural Open IE task, the copying mechanism is more important because the output vocabulary is directly from the input vocabulary except for the placeholder symbols. We simplify the copying method in (See et al., 2017), the probability of generating the word yt comes from two parts as follows: ( p(yt |y1 , y2 , ..., yt−1 ; X) if yt ∈ V p(yt ) = P t otherwise i:xi =yt a"
P18-2065,D15-1044,0,0.0472825,"(Banko et al., 2007; Gashteovski et al., 2017; Schneider et al., 2017). Therefore, it is essential to solve the problems of cascading errors to alleviate extracting incorrect tuples. To this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called ‘context vector’ which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, give"
P18-2065,P17-2050,0,0.167661,"more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, O PENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford O PENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016), and more. 5 Conclusion and Future Work We proposed a neural Open IE approach using an encoder-decoder framework. The neural Open IE model is trained with highly confident binary extractions bootstrapped from a state-of-the-art Open IE system, therefore it can generate highquality tuples wit"
P18-2065,W17-5402,0,0.116001,"Missing"
P18-2065,P17-1099,0,0.0534503,"017; Schneider et al., 2017). Therefore, it is essential to solve the problems of cascading errors to alleviate extracting incorrect tuples. To this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called ‘context vector’ which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, given the input sequence “deep learning is a su"
P18-2065,D16-1252,0,0.365457,"g n-ary extractions and 408 Open IE system. Both the arguments and relations are sub-spans that correspond to the input sequence. We leverage the attention method proposed by Bahdanau et al. to calculate the context vector c as follows: ci = n X with binary relations. To further obtain highquality tuples, we only kept the tuples whose confidence score is at least 0.9. Finally, there are a total of 36,247,584 hsentence, tuplei pairs extracted. The training data is released for public use at https://1drv.ms/u/s!ApPZx_ TWwibImHl49ZBwxOU0ktHv. For the test data, we used a large benchmark dataset (Stanovsky and Dagan, 2016) that contains 3,200 sentences with 10,359 extractions4 . We compared with several state-of-the-art baselines including O LLIE, ClausIE, Stanford O PENIE, PropS and O PENIE4. The evaluation metrics are precision and recall. αij hj j=1 exp(eij ) αij = Pn k=1 exp(eik ) (4) eij = a(si−1 , hj ) where a is an alignment model that scores how well the inputs around position j and the output at position i match, which is measured by the encoder hidden state hj and the decoder hidden state si−1 . The encoder and decoder are jointly optimized to maximize the log probability of the output sequence condit"
P18-2067,N16-1014,0,0.0430232,"0.526 0.565 everything the same as our approach but replace D with a set constructed by random sampling, denoted as model+WSrand. Table 3 reports the results. We can conclude that both the weak supervision and the strategy of training data construction are important to the success of the proposed learning approach. Training data construction plays a more crucial role, because it involves more true positives and negatives with different semantic distances to the positives into learning. Does updating the Seq2Seq model help? It is well known that Seq2Seq models suffer from the “safe response” (Li et al., 2016a) problem, which may bias the weak supervision signals to high-frequency responses. Therefore, we attempt to iteratively optimize the Seq2Seq model and the matching model and check if the matching model can be further improved. Specifically, we update the Seq2Seq model every 20 mini-batches with the policy-based reinforcement learning approach proposed in (Li et al., 2016b). The reward is defined as the matching score of a context and a response given by the matching model. Unfortunately, we do not observe significant improvement on the matching model. The result is attributed to two factors:"
P18-2067,D16-1127,0,0.0988774,"0.526 0.565 everything the same as our approach but replace D with a set constructed by random sampling, denoted as model+WSrand. Table 3 reports the results. We can conclude that both the weak supervision and the strategy of training data construction are important to the success of the proposed learning approach. Training data construction plays a more crucial role, because it involves more true positives and negatives with different semantic distances to the positives into learning. Does updating the Seq2Seq model help? It is well known that Seq2Seq models suffer from the “safe response” (Li et al., 2016a) problem, which may bias the weak supervision signals to high-frequency responses. Therefore, we attempt to iteratively optimize the Seq2Seq model and the matching model and check if the matching model can be further improved. Specifically, we update the Seq2Seq model every 20 mini-batches with the policy-based reinforcement learning approach proposed in (Li et al., 2016b). The reward is defined as the matching score of a context and a response given by the matching model. Unfortunately, we do not observe significant improvement on the matching model. The result is attributed to two factors:"
P18-2067,W15-4640,0,0.598699,"significant improvements when they are learned with the proposed method. 1 Introduction Recently, more and more attention from both academia and industry is paying to building nontask-oriented chatbots that can naturally converse with humans on any open domain topics. Existing approaches can be categorized into generationbased methods (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Serban et al., 2017; Xing et al., 2018) which synthesize a response with natural language generation techniques, and retrievalbased methods (Hu et al., 2014; Lowe et al., 2015; Yan et al., 2016; Zhou et al., 2016; Wu et al., 2017) which select a response from a pre-built index. In this work, we study response selection for retrieval-based chatbots, not only because retrieval-based methods can return fluent and informative responses, but also because they have been successfully applied to many real products such as the social-bot XiaoIce from Microsoft (Shum et al., 2018) and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). ∗ 1 The model performs well on randomly sampled data, but badly on human labeled data. Corresponding Author 420 Proce"
P18-2067,D13-1096,0,0.46557,"sed approaches in the teacher-student framework (Dehghani et al., 2017a,b), as there are no labeled data in learning. 3 Implementation Details 3.2 Single-turn Response Selection Experiment settings: in the STC (stands for Short Text Conversation) data set, the task is to select a proper response for a post in Weibo2 . The training set contains 4.8 million post-response (true response) pairs. The test set consists of 422 posts with each one associated with around 30 responses labeled by human annotators in “good” and “bad”. In total, there are 12, 402 labeled pairs in the test data. Following (Wang et al., 2013, 2015), we combine the score from a matching model with TF-IDF based cosine similarity using RankSVM whose parameters are chosen by 5-fold cross validation. Precision at position 1 (P@1) is employed as an evaluation metric. In addition to the models compared on the data in the existing literatures, we also implement dual LSTM (Lowe et al., 2015) as a baseline. As case studies, we learn a dual LSTM and an CNN (Hu et al., 2014) with the proposed approach, and denote them as LSTM+WS (Weak Supervision) and CNN+WS, respectively. When constructing D, we build an index with the training data using L"
P18-2067,P17-1046,1,0.89116,"sion for Response Selection in Retrieval-based Chatbots † Yu Wu† , Wei Wu‡ , Zhoujun Li†∗, Ming Zhou♦ State Key Lab of Software Development Environment, Beihang University, Beijing, China † Authors are supported by AdeptMind Scholarship ♦ Microsoft Research, Beijing, China ‡ Microsoft Corporation, Beijing, China {wuyu,lizj}@buaa.edu.cn {wuwei,mingzhou}@microsoft.com Abstract A key step to response selection is measuring the matching degree between a response candidate and an input which is either a single message (Hu et al., 2014) or a conversational context consisting of multiple utterances (Wu et al., 2017). While existing research focuses on how to define a matching model with neural networks, little attention has been paid to how to learn such a model when few labeled data are available. In practice, because human labeling is expensive and exhausting, one cannot have large scale labeled data for model training. Thus, a common practice is to transform the matching problem to a classification problem with human responses as positive examples and randomly sampled ones as negative examples. This strategy, however, oversimplifies the learning problem, as most of the randomly sampled responses are e"
P18-2067,P15-1152,0,0.381248,"Missing"
P18-2067,N15-1020,0,0.100849,"Missing"
P18-2067,D16-1036,0,0.792114,"e learned with the proposed method. 1 Introduction Recently, more and more attention from both academia and industry is paying to building nontask-oriented chatbots that can naturally converse with humans on any open domain topics. Existing approaches can be categorized into generationbased methods (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Serban et al., 2017; Xing et al., 2018) which synthesize a response with natural language generation techniques, and retrievalbased methods (Hu et al., 2014; Lowe et al., 2015; Yan et al., 2016; Zhou et al., 2016; Wu et al., 2017) which select a response from a pre-built index. In this work, we study response selection for retrieval-based chatbots, not only because retrieval-based methods can return fluent and informative responses, but also because they have been successfully applied to many real products such as the social-bot XiaoIce from Microsoft (Shum et al., 2018) and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). ∗ 1 The model performs well on randomly sampled data, but badly on human labeled data. Corresponding Author 420 Proceedings of the 56th Annual Meeting of"
P19-1328,S07-1091,0,0.202177,"that not only are semantically consistent with the original target word and fits in the ∗ This work was done during the first author’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy"
P19-1328,D16-1215,0,0.0947329,"a. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; a"
P19-1328,D10-1113,0,0.0318685,"uthor’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of wor"
P19-1328,D08-1094,0,0.135637,"Missing"
P19-1328,P16-1012,0,0.161898,"ontext; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitute candidates. Although these approaches work well in some cases, they have two key limitations: (1) they rely heavily on lexical resources. While the resources can offer synonyms for substitution, they are not perfect and they are likely to overlook some good candidates, as Figure 1(a) shows. (2) most previous approaches only measure the substitution candidates’ fitness given the context but they do not consider whether the substitution changes the sentence’s meaning. Take Figure 1(b) as an example, although tough may fit in the context as well as"
P19-1328,E14-1057,0,0.0765349,"Missing"
P19-1328,S07-1009,0,0.439037,"target word’s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word’s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution’s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks. 1 The wine he sent to me as my birthday gift is too strong to drink. WordNet Introduction Lexical substitution (McCarthy and Navigli, 2007) aims to replace a target word in a sentence with a substitute word without changing the meaning of the sentence, which is useful for many Natural Language Processing (NLP) tasks like text simplification and paraphrase generation. One main challenge in this task is proposing substitutes that not only are semantically consistent with the original target word and fits in the ∗ This work was done during the first author’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking"
P19-1328,N15-1050,0,0.240569,"synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitu"
P19-1328,K16-1006,0,0.142733,"oach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitute candidates. Although these approaches work well in some cases, they have two key limitations: (1) they rely heavily on lexical resources. While the resources can offer synonyms for substitution, they are not perfect and they are likely to overlook some good candidates, as Figure 1(a) shows. (2) most previous approaches only measure the substitution candidates’ fitness given the conte"
P19-1328,W15-1501,0,0.395774,"synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitu"
P19-1328,N16-1131,0,0.0480592,"Missing"
P19-1328,N13-1133,0,0.175153,"based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitute candidates. Although these approaches work well in some cases, they have two key limitations: (1) they rely heavily on lexical resources. While the resources can offer synonyms for substitution, they are not perfect and they are likely to overlook some good candidates, as Figure 1(a) shows. (2) most previous approaches only measure the substitution candidates’ fitness given the context but they do not consider whether the substitution changes the sentence’s meaning. Take Figure 1(b) as an example, although tough may f"
P19-1328,D13-1198,0,0.0475952,"Missing"
P19-1328,P10-1097,0,0.0821976,"Missing"
P19-1328,S07-1044,0,0.354603,"g substitutes that not only are semantically consistent with the original target word and fits in the ∗ This work was done during the first author’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–"
P19-1499,Q17-1010,0,0.046088,"n (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pennington et al., 2014; Bojanowski et al., 2017). Peters et al. (2018) and Radford et al. (2018) find even a sentence encoder 5060 Figure 1: The architecture of H IBERT during training. senti is a sentence in the document above, which has four sentences in total. sent3 is masked during encoding and the decoder predicts the original sent3 . (not just word embeddings) can also be pre-trained with language model objectives (i.e., predicting the next or previous word). Language model objective is unidirectional, while many tasks can leverage the context in both directions. Therefore, Devlin et al. (2018) propose the naturally bidirectional mask"
P19-1499,N18-1150,0,0.0511629,"ticipants were presented with a document and a list of summaries produced by different systems. We asked subjects to rank these summaries (ties allowed) by taking informativeness (is the summary capture the important information from the document?) and fluency (is the summary grammatical?) into account. Each document is annotated by three different subjects. 4.4 Results Our main results on the CNNDM dataset are shown in Table 1, with abstractive models in the top block and extractive models in the bottom block. Pointer+Coverage (See et al., 2017), Abstract-ML+RL (Paulus et al., 2017) and DCA (Celikyilmaz et al., 2018) are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite (Hsu et al., 2018) and InconsisLoss (Chen and Bansal, 2018) all try to decompose the word by word summary generation into sentence selection from document and “sentence” level summarization (or compression). Bottom-Up (Gehrmann et al., 2018) generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; Nallapati et al. 2017 and N"
P19-1499,D18-1442,0,0.0154095,"04b). As the very successful applications of neural networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs and the sequence labeling (or classification) model is replaced with an LSTM decoder (Cheng and Lapata, 2016; Nallapati et al., 2017). The architecture is widely adopted in recent neural extractive models and is extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018), latent variable models (Zhang et al., 2018), joint scoring (Zhou et al., 2018) and iterative document representation (Chen et al., 2018). Recently, transformer networks (Vaswani et al., 2017) achieves good performance in machine translation (Vaswani et al., 2017) and a range of NLP tasks (Devlin et al., 2018; Radford et al., 2018). Different from the extractive models above, we adopt a hierarchical Transformer for document encoding and also propose a method to pre-train the document encoder. Abstractive Summarization Abstractive summarization aims to generate the summary of a document with rewriting. Most recent abstractive models (Nallapati et al., 2016) are based on neural sequence to sequence learning (Bahdanau et al., 2015"
P19-1499,P18-1063,0,0.214733,"are based on neural sequence to sequence learning (Bahdanau et al., 2015; Sutskever et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning (Chen and Bansal, 2018), fused attention (Hsu et al., 2018) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training"
P19-1499,P16-1046,0,0.399477,"tences), while abstractive methods may generate new words or phrases which are not in the original document. Extractive summarization is usually modeled as a sentence ranking problem with length constraints (e.g., max number of words or sentences). Top ranked sentences (under constraints) are selected as summaries. Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abs"
P19-1499,D18-1409,0,0.306318,"Missing"
P19-1499,P16-1188,0,0.0847286,"el can be trained by minimizing the negative loglikelihood of all sentence labels given their paired documents. 4 Experiments In this section we assess the performance of our model on the document summarization task. We 5063 first introduce the dataset we used for pre-training and the summarization task and give implementation details of our model. We also compare our model against multiple previous models. 4.1 Datasets We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017)3 . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Durrett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for t"
P19-1499,W04-1017,0,0.455222,"or an overview). The most popular two among them are extractive approaches and abstractive approaches. As the name implies, extractive approaches generate summaries by extracting parts of the original document (usually sentences), while abstractive methods may generate new words or phrases which are not in the original document. Extractive summarization is usually modeled as a sentence ranking problem with length constraints (e.g., max number of words or sentences). Top ranked sentences (under constraints) are selected as summaries. Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hier"
P19-1499,D18-1443,0,0.403009,"et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning (Chen and Bansal, 2018), fused attention (Hsu et al., 2018) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pennington et al., 2014; Bojanowski et al., 2017)."
P19-1499,P16-1154,0,0.274615,"2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with copy mechanism (Gu et al., 2016; See et al., 2017), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2017), there is still no guarantee that the generated summaries are grammatical and convey the same meaning as the original document does. It seems that extractive models are more reliable than their abstractive counterparts. However, extractive models require sentence level labels, which are usually not included in most summarization datasets (most datasets only contain document-summary pairs). Sentence labels are usually obtained by rule-based methods (e.g., maximizing the ROUGE score between a"
P19-1499,P18-1013,0,0.127264,"learning (Bahdanau et al., 2015; Sutskever et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning (Chen and Bansal, 2018), fused attention (Hsu et al., 2018) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pe"
P19-1499,W04-1013,0,0.464357,"mmaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Durrett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive summarization, we used a strategy similar to Nallapati et al. (2017). We label the subset of sentences in a document that maximizes ROUGE (Lin, 2004) (against the human summary) as True and all other sentences as False. To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al.,"
P19-1499,W01-0100,0,0.708557,"ich stands for HIerachical Bidirectional Encoder Representations from Transformers. We design an unsupervised method to pre-train H IBERT for document modeling. We apply the pre-trained H IBERT to the task of document summarization and achieve state-of-the-art performance on both the CNN/Dailymail and New York Times dataset. 2 Related Work In this section, we introduce work on extractive summarization, abstractive summarization and pre-trained natural language processing models. For a more comprehensive review of summarization, we refer the interested readers to Nenkova and McKeown (2011) and Mani (2001). Extractive Summarization Extractive summarization aims to select important sentences (sometimes other textual units such as elementary discourse units (EDUs)) from a document as its summary. It is usually modeled as a sentence ranking problem by using the scores from classifiers (Kupiec et al., 1995), sequential labeling models (Conroy and O’leary, 2001) as well as integer linear programmers (Woodsend and Lapata, 2010). Early work with these models above mostly leverage human engineered features such as sentence position and length (Radev et al., 2004), word frequency (Nenkova et al., 2006)"
P19-1499,P14-5010,0,0.00389708,"UGE (Lin, 2004) (against the human summary) as True and all other sentences as False. To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To reduce the vocabulary size, we applied byte pair encoding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption during training, we limit the length of each sentence to be 50 words (51th word and onwards are removed) and split documents with more than 30 sentences into smaller documents with each containing at most 30 sentences. 3 Scripts publicly available at https://github.com/ abisee/cnn-dailymail 4 https://catalog.ldc.upenn.edu/LDC2012T21 4.2 Implementation Details Our model is trained in three stages, which includes two pre-training stages and one finetu"
P19-1499,K16-1028,0,0.0606688,"18), joint scoring (Zhou et al., 2018) and iterative document representation (Chen et al., 2018). Recently, transformer networks (Vaswani et al., 2017) achieves good performance in machine translation (Vaswani et al., 2017) and a range of NLP tasks (Devlin et al., 2018; Radford et al., 2018). Different from the extractive models above, we adopt a hierarchical Transformer for document encoding and also propose a method to pre-train the document encoder. Abstractive Summarization Abstractive summarization aims to generate the summary of a document with rewriting. Most recent abstractive models (Nallapati et al., 2016) are based on neural sequence to sequence learning (Bahdanau et al., 2015; Sutskever et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning ("
P19-1499,N18-1158,0,0.503838,"elected as summaries. Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with c"
P19-1499,D14-1162,0,0.0908302,"8) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pennington et al., 2014; Bojanowski et al., 2017). Peters et al. (2018) and Radford et al. (2018) find even a sentence encoder 5060 Figure 1: The architecture of H IBERT during training. senti is a sentence in the document above, which has four sentences in total. sent3 is masked during encoding and the decoder predicts the original sent3 . (not just word embeddings) can also be pre-trained with language model objectives (i.e., predicting the next or previous word). Language model objective is unidirectional, while many tasks can leverage the context in both directions. Therefore, Devlin et al. (2018) propose the na"
P19-1499,N18-1202,0,0.560136,"August 2, 2019. 2019 Association for Computational Linguistics cently (Cheng and Lapata, 2016; Nallapati et al., 2017) employ hierarchical document encoders and even have neural decoders, which are complex. Training such complex neural models with inaccurate binary labels is challenging. We observed in our initial experiments on one of our dataset that our extractive model (see Section 3.3 for details) overfits to the training set quickly after the second epoch, which indicates the training set may not be fully utilized. Inspired by the recent pre-training work in natural language processing (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), our solution to this problem is to first pre-train the “complex”’ part (i.e., the hierarchical encoder) of the extractive model on unlabeled data and then we learn to classify sentences with our model initialized from the pre-trained encoder. In this paper, we propose H IBERT, which stands for HIerachical Bidirectional Encoder Representations from Transformers. We design an unsupervised method to pre-train H IBERT for document modeling. We apply the pre-trained H IBERT to the task of document summarization and achieve state-of-the-art performance o"
P19-1499,radev-etal-2004-mead,0,0.322522,"Missing"
P19-1499,P17-1099,0,0.626391,"also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with copy mechanism (Gu et al., 2016; See et al., 2017), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2017), there is still no guarantee that the generated summaries are grammatical and convey the same meaning as the original document does. It seems that extractive models are more reliable than their abstractive counterparts. However, extractive models require sentence level labels, which are usually not included in most summarization datasets (most datasets only contain document-summary pairs). Sentence labels are usually obtained by rule-based methods (e.g., maximizing the ROUGE score between a set of sentences an"
P19-1499,P16-1162,0,0.0555691,"nsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To reduce the vocabulary size, we applied byte pair encoding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption during training, we limit the length of each sentence to be 50 words (51th word and onwards are removed) and split documents with more than 30 sentences into smaller documents with each containing at most 30 sentences. 3 Scripts publicly available at https://github.com/ abisee/cnn-dailymail 4 https://catalog.ldc.upenn.edu/LDC2012T21 4.2 Implementation Details Our model is trained in three stages, which includes two pre-training stages and one finetuning stage. The first stage is the open-domain pretraining and in this stage we train H IB"
P19-1499,D18-1548,0,0.0537113,"Missing"
P19-1499,P10-1058,0,0.0552205,"tractive summarization and pre-trained natural language processing models. For a more comprehensive review of summarization, we refer the interested readers to Nenkova and McKeown (2011) and Mani (2001). Extractive Summarization Extractive summarization aims to select important sentences (sometimes other textual units such as elementary discourse units (EDUs)) from a document as its summary. It is usually modeled as a sentence ranking problem by using the scores from classifiers (Kupiec et al., 1995), sequential labeling models (Conroy and O’leary, 2001) as well as integer linear programmers (Woodsend and Lapata, 2010). Early work with these models above mostly leverage human engineered features such as sentence position and length (Radev et al., 2004), word frequency (Nenkova et al., 2006) and event features (Filatova and Hatzivassiloglou, 2004b). As the very successful applications of neural networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs and the sequence labeling (or classification) model is replaced with an LSTM decoder (Cheng and Lapata, 2016; Nallapati et al., 2017). The architecture is widely adopted in recent n"
P19-1499,D19-1324,0,0.667328,"inimizing the negative loglikelihood of all sentence labels given their paired documents. 4 Experiments In this section we assess the performance of our model on the document summarization task. We 5063 first introduce the dataset we used for pre-training and the summarization task and give implementation details of our model. We also compare our model against multiple previous models. 4.1 Datasets We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017)3 . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Durrett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence"
P19-1499,D18-1088,1,0.949714,"Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with copy mechanism (Gu et"
P19-1499,P18-1061,1,0.915666,"006) and event features (Filatova and Hatzivassiloglou, 2004b). As the very successful applications of neural networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs and the sequence labeling (or classification) model is replaced with an LSTM decoder (Cheng and Lapata, 2016; Nallapati et al., 2017). The architecture is widely adopted in recent neural extractive models and is extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018), latent variable models (Zhang et al., 2018), joint scoring (Zhou et al., 2018) and iterative document representation (Chen et al., 2018). Recently, transformer networks (Vaswani et al., 2017) achieves good performance in machine translation (Vaswani et al., 2017) and a range of NLP tasks (Devlin et al., 2018; Radford et al., 2018). Different from the extractive models above, we adopt a hierarchical Transformer for document encoding and also propose a method to pre-train the document encoder. Abstractive Summarization Abstractive summarization aims to generate the summary of a document with rewriting. Most recent abstractive models (Nallapati et al., 2016) are based on n"
P19-1609,W17-4773,0,0.0183401,"cation, many grammatical improvements do not bring task-oriented improvements. The reason is that the parts GEC edits are not the content that should be kept in the results. Also, it is notable that except for Formal→Informal style transfer whose target sentences should be in an informal style, GEC brings much more improvements than adverse effects on the tasks, demonstrating the potential of GEC for NLG. 4 Related Work and Discussion The most related work to ours is the automatic post editing (APE) (Bojar et al., 2016) which has been extensively studied for MT (e.g., (Pal et al., 2016, 2017; Chatterjee et al., 2017; Hokamp, 2017; Tan et al., 2017)) in the past few years. These APE approaches are usually trained with source language input data, target language MT output and target language post editing (PE) data. Although these APE models and systems have proven to be successful in improving MT results, they are taskspecific and cannot be used for other NLG tasks. In contrast, we propose a general post editing approach by applying the current state-of-the-art GEC system to editing the outputs of NLG systems. To the best of our knowledge, this is the first attempt to explore improving seq2seq based NLG mo"
P19-1609,D15-1042,0,0.0608229,"Missing"
P19-1609,D13-1155,0,0.0342569,"entence compression dataset1 (GoogComp). For sentence simplification, we use the state-of-the-art deep reinforcement model DRESS (Zhang and Lapata, 2017) as our base model and test on Newsela text simplification dataset. Table 6 shows the results for the effects of GEC on sentence compression and simplification. For sentence compression, BLEU decreases from 60.38 to 58.77 after GEC post editing. We manually analyze the results and find there are many grammatical errors in the reference sentences. This is not surprising, since the reference sentences are constructed with an automatic approach (Filippova and Altun, 2013). The grammatical errors in the references affect the BLEU evaluation and make it less reliable. The BLEU decrease is also observed in sentence simplification task but for a different reason. In the Newsela dataset, the reference sentences are written by humans and therefore have much fewer grammatical errors compared to GoogComp. In contrast to sentence compression where reference errors are the main reason for the BLEU decrease, the BLEU decrease in sentence simplification usually happens in the cases where the correction of grammatical errors reduces the sentence’s n-gram overlap with the r"
P19-1609,P18-1097,1,0.909232,"al., 2014; Sutskever et al., 2014) has attracted growing attention in natural language processing (NLP). Despite various advantages of seq2seq models, they tend to have a weakness: there is no guarantee that they can always generate sentences without grammatical errors. Table 1 shows examples generated by seq2seq models in various tasks with grammatical errors. One valid solution to this challenge is conducting grammatical error correction (GEC) for machine generated sentences. Recent GEC systems (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018a,b) can achieve human-level performance in GEC benchmarks. We are curious whether they can help improve seq2seq based natural language generation (NLG) models. We therefore propose an empirical study on GEC post editing for various text generation tasks (i.e., machine translation, style transfer, sentence compres• We present an empirical study on GEC post editing for seq2seq text generation. To the best of our knowledge, it is the first work to study improving seq2seq based NLG models using GEC. • We show some interesting results by thoroughly comparing and analyzing GEC post editing for vari"
P19-1609,N18-2046,0,0.0616355,"quence (seq2seq) text generation (Cho et al., 2014; Sutskever et al., 2014) has attracted growing attention in natural language processing (NLP). Despite various advantages of seq2seq models, they tend to have a weakness: there is no guarantee that they can always generate sentences without grammatical errors. Table 1 shows examples generated by seq2seq models in various tasks with grammatical errors. One valid solution to this challenge is conducting grammatical error correction (GEC) for machine generated sentences. Recent GEC systems (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018a,b) can achieve human-level performance in GEC benchmarks. We are curious whether they can help improve seq2seq based natural language generation (NLG) models. We therefore propose an empirical study on GEC post editing for various text generation tasks (i.e., machine translation, style transfer, sentence compres• We present an empirical study on GEC post editing for seq2seq text generation. To the best of our knowledge, it is the first work to study improving seq2seq based NLG models using GEC. • We show some interesting results by thoroughly comparing and analyzing GEC post"
P19-1609,W17-4775,0,0.0272998,"improvements do not bring task-oriented improvements. The reason is that the parts GEC edits are not the content that should be kept in the results. Also, it is notable that except for Formal→Informal style transfer whose target sentences should be in an informal style, GEC brings much more improvements than adverse effects on the tasks, demonstrating the potential of GEC for NLG. 4 Related Work and Discussion The most related work to ours is the automatic post editing (APE) (Bojar et al., 2016) which has been extensively studied for MT (e.g., (Pal et al., 2016, 2017; Chatterjee et al., 2017; Hokamp, 2017; Tan et al., 2017)) in the past few years. These APE approaches are usually trained with source language input data, target language MT output and target language post editing (PE) data. Although these APE models and systems have proven to be successful in improving MT results, they are taskspecific and cannot be used for other NLG tasks. In contrast, we propose a general post editing approach by applying the current state-of-the-art GEC system to editing the outputs of NLG systems. To the best of our knowledge, this is the first attempt to explore improving seq2seq based NLG models with a st"
P19-1609,N18-1055,0,0.0185797,"d: Introduction Sequence-to-sequence (seq2seq) text generation (Cho et al., 2014; Sutskever et al., 2014) has attracted growing attention in natural language processing (NLP). Despite various advantages of seq2seq models, they tend to have a weakness: there is no guarantee that they can always generate sentences without grammatical errors. Table 1 shows examples generated by seq2seq models in various tasks with grammatical errors. One valid solution to this challenge is conducting grammatical error correction (GEC) for machine generated sentences. Recent GEC systems (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018a,b) can achieve human-level performance in GEC benchmarks. We are curious whether they can help improve seq2seq based natural language generation (NLG) models. We therefore propose an empirical study on GEC post editing for various text generation tasks (i.e., machine translation, style transfer, sentence compres• We present an empirical study on GEC post editing for seq2seq text generation. To the best of our knowledge, it is the first work to study improving seq2seq based NLG models using GEC. • We show some interesting results by tho"
P19-1609,C18-1086,0,0.125372,"Missing"
P19-1609,P16-2046,0,0.0687993,"Missing"
P19-1609,E17-2056,0,0.0440796,"Missing"
P19-1609,N18-1012,0,0.108733,"Missing"
P19-1609,stymne-ahrenberg-2010-using,0,0.0912647,"Missing"
P19-1609,W17-4776,0,0.0250691,"o not bring task-oriented improvements. The reason is that the parts GEC edits are not the content that should be kept in the results. Also, it is notable that except for Formal→Informal style transfer whose target sentences should be in an informal style, GEC brings much more improvements than adverse effects on the tasks, demonstrating the potential of GEC for NLG. 4 Related Work and Discussion The most related work to ours is the automatic post editing (APE) (Bojar et al., 2016) which has been extensively studied for MT (e.g., (Pal et al., 2016, 2017; Chatterjee et al., 2017; Hokamp, 2017; Tan et al., 2017)) in the past few years. These APE approaches are usually trained with source language input data, target language MT output and target language post editing (PE) data. Although these APE models and systems have proven to be successful in improving MT results, they are taskspecific and cannot be used for other NLG tasks. In contrast, we propose a general post editing approach by applying the current state-of-the-art GEC system to editing the outputs of NLG systems. To the best of our knowledge, this is the first attempt to explore improving seq2seq based NLG models with a state-of-the-art neur"
P19-1609,D17-1062,1,0.895997,"Missing"
P19-1641,W05-0909,0,0.051152,"es. 6386 4 Experiment and Case Study 4.1 Evaluation Metrics We separately evaluate the procedure extraction and captioning module. For procedure extraction, we adopt the widely used mJacc (mean of Jaccard) (Bojanowski et al., 2014) and mIoU (mean of IoU) metrics for evaluating the procedure proposition. The Jaccard calculates the intersection of the predicted and ground-truth procedure proposals over the length of the latter. The IoU replaces the denominator part with the union of predicted and ground-truth procedures. For procedure captioning, we adopt BLEU4(Papineni et al., 2002) and METEOR(Banerjee and Lavie, 2005) as the metrics to evaluate the performance on the result of captioning based on both extracted and ground-truth procedures. 4.2 Dataset In this paper, we use the YouCookII3 (Zhou et al., 2018a) dataset to conduct experiments. It contains 2000 videos dumped from YouTube which are all instructional cooking recipe videos. For each video, human annotators were asked to first label the starting and ending time of procedure segments, and then write captions for each procedure. This dataset contains pre-processed frame features (T = 500 frames for each video, each frame feature is a 512-d vector, ex"
P19-1641,N15-1017,0,0.196455,"ed instructional video during procedure extraction and captioning. 2. We employ the pre-trained BERT(Devlin et al., 2018) and self-attention(Vaswani et al., 2017) layer to embed transcript, and then integrate them to visual encoding during procedure extraction. 3. We adopt the sequence-to-sequence model to generate captions by merging tokens of the transcript with the aligned video frames. 2 Related Works Narrated Instructional Video Understanding Previous works aim to ground the description to the video. (Malmaud et al., 2015) adopted an HMM model to align the recipe steps to the narration. (Naim et al., 2015) utilize latent-variable based discriminative models (CRF, Structured Perceptron) for unsupervised alignment. Besides the alignment of transcripts with video, (Alayrac et al., 2016, 2018) propose to learn the main steps from a set of narrated instructional videos for five different tasks and formulate the problem into two clustering problems. Graph-based clustering is also adopted to learn the semantic storyline of instructional videos in (Sener et al., 2015). These works assume that ”one task” has the same procedures. Different from previous works, we focus on learning more complicated proced"
P19-1641,P02-1040,0,0.108101,"le based on the extracted procedures. 6386 4 Experiment and Case Study 4.1 Evaluation Metrics We separately evaluate the procedure extraction and captioning module. For procedure extraction, we adopt the widely used mJacc (mean of Jaccard) (Bojanowski et al., 2014) and mIoU (mean of IoU) metrics for evaluating the procedure proposition. The Jaccard calculates the intersection of the predicted and ground-truth procedure proposals over the length of the latter. The IoU replaces the denominator part with the union of predicted and ground-truth procedures. For procedure captioning, we adopt BLEU4(Papineni et al., 2002) and METEOR(Banerjee and Lavie, 2005) as the metrics to evaluate the performance on the result of captioning based on both extracted and ground-truth procedures. 4.2 Dataset In this paper, we use the YouCookII3 (Zhou et al., 2018a) dataset to conduct experiments. It contains 2000 videos dumped from YouTube which are all instructional cooking recipe videos. For each video, human annotators were asked to first label the starting and ending time of procedure segments, and then write captions for each procedure. This dataset contains pre-processed frame features (T = 500 frames for each video, eac"
P19-1641,N15-1173,0,0.0262622,"to ours, which is designed to detect long complicated event proposals rather than actions. We adopt this framework and inject the textual transcript of narrated instructional videos as our first step. Dense video caption aims to generate descriptive sentences for all events in the video. Different from video captioning and paragraph generation, dense video caption requires segmenting of each video into a sequence of temporal proposals with corresponding captions. (Krishna et al., 2017) resorts to the DAP method (Escorcia et al., 2016) for event detection and apply the contextaware S2VT model (Venugopalan et al., 2015). (Yu et al., 2018) propose to generate long and detailed description for sport videos. (Li et al., 2018) train jointly on unifying the temporal proposal localization and sentence generation for dense video captioning. (Xiong et al., 2018) assembles temporally localized description to produce a descriptive paragraph. (Duan et al., 2018) propose weakly supervised dense event captioning, which does not require temporal segment annotations, and decomposes the problem into a pair of dual tasks. (Wang et al., 2018a) exploit both past and future context for predicting accurate event proposals. (Zhou"
P19-1641,N18-2125,0,0.194606,"Missing"
S14-2033,S13-2053,0,0.595629,"utomatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants o"
S14-2033,P14-2009,1,0.80825,"afted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creat"
S14-2033,S13-2052,0,0.118118,"Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants owning larger training data than us. The performance of only using SSWE as features is comparable to th"
S14-2033,W02-1011,0,0.0200027,"nction 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 http://alt.qcri.org/semeval20"
S14-2033,P11-2008,0,0.291901,"Missing"
S14-2033,P14-1146,1,0.727439,"uru Wei‡ , Bing Qin† , Ting Liu† , Ming Zhou‡ Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment c"
S14-2033,P11-1016,1,0.797284,"res with the state-of-the-art hand-crafted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the orga"
S14-2033,H05-1044,0,0.204982,"Missing"
S14-2033,P14-1062,0,0.00858127,"omputing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To le"
S15-2086,S14-2115,0,0.0131346,"l (Mikolov et al., 2013) to learn 40-dimensional word embeddings from a twitter dataset. Then, we employ K-means algorithm and L2 distance of word vectors to cluster the 255, 657 words to 4960 classes. The clusters are used to represent words. We extract unigrams and bigrams as features, and use them in sub/obj classifier. The word2vec clustering results are publicly available3 for research purposes. As shown in Table 1, similar words are clustered into the same clusters. This feature template is used in sub/obj classification. CNN predicted distribution The convolutional neural networks (dos Santos, 2014) are used to predict the probabilities of three sentiment classes, and the predicted distribution is used as a threedimension feature template. As illustrated in Figure 2, we use the network architecture proposed by Collobert et al. (2011). The dimension of word vectors is 50, and the window size is 5. Then the concatenated word vectors are fed into a convolutional layer. The vector representation of a sentence is obtained by a max pooling layer, and is used to predict the probabilities of three classes by the softmax layer. We employ stochastic gradient descent to minimize the cross-entropy l"
S15-2086,D14-1181,0,0.0112281,"Missing"
S15-2086,S13-2052,0,0.0610536,"performance of our system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment analysis system is a two-stage sentiment classifier which consists of a subjective/objective (sub/obj) classifier and a positive/negative (pos/neg) classifier. By using this artitacture, we can design different feature sets for the two classification steps. Notably, the predicted values of pos/neg classifier is employed to help classify tweets to sub/obj classes. We employ the LIBLINEAR (Fan et al., 2008) with option “-"
S15-2086,N13-1039,0,0.0449785,"ize. Word ngrams We use unigrams and bigrams for words. Character ngrams For each word, character ngrams are extracted. We use four-grams and fivegrams in our system. Word skip-grams For all the trigrams and fourgrams, one of the words is replaced by ∗ to indicate the presence of non-contiguous words. This feature template is used in sub/obj classification. Brown cluster ngrams We use Brown clusters1 to represent words, and extract unigrams and bigrams as features. POS The presence or absence of part-of-speech tags are used as binary features. We use the CMU ARK Twitter Part-of-Speech Tagger (Owoputi et al., 2013) in our implementation. Lexicons The NRC Hashtag Sentiment Lexicon 1 http://www.ark.cs.cmu.edu/TweetNLP/clusters/50mpaths2 516 2.3 Deep Learning Features In order to automatically extract features, we explore using some deep learning techniques in our system. These features and the basic features described in Section 2.2 are used together to learn classifiers. Word2vec cluster ngrams We use the word2vec tool (Mikolov et al., 2013) to learn 40-dimensional word embeddings from a twitter dataset. Then, we employ K-means algorithm and L2 distance of word vectors to cluster the 255, 657 words to 49"
S15-2086,S14-2009,0,0.0279426,"system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment analysis system is a two-stage sentiment classifier which consists of a subjective/objective (sub/obj) classifier and a positive/negative (pos/neg) classifier. By using this artitacture, we can design different feature sets for the two classification steps. Notably, the predicted values of pos/neg classifier is employed to help classify tweets to sub/obj classes. We employ the LIBLINEAR (Fan et al., 2008) with option “-s 1” as our classifier."
S15-2086,S15-2078,0,0.0805037,"he sentiment labels for tweets, which enables us to design different features for subjective/objective classification and positive/negative classification. In addition to n-grams, lexicons, word clusters, and twitter-specific features, we develop several deep learning methods to automatically extract features for the message-level sentiment classification task. Moreover, we propose a polarity boosting trick which improves the performance of our system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment"
S15-2086,D13-1170,0,0.0263646,"Missing"
S15-2086,P14-1146,1,0.837196,"lovve, love/miss, luuuvvv, lubb, lurve Table 1: Examples of word2vec clusters. Similar words are clustered to the same cluster. 3 300 300 300 Softmax layer Hidden layer ... ... Max pooling layer Convolutional layer 5x50 Figure 2: Architecture of convolutional neural network used in our system. The lines represent vectors, and the numbers indicate the vector dimensions. vent overfitting, a L2-norm constraint for the column vectors of weight matrices is used. The backpropagation algorithm (Rumelhart et al., 1986) is employed to compute the gradients for parameters. The word vectors provided by Tang et al. (2014) are used for initialization. Sentiment-specific embedding Tang et al. (2014) improve the word2vec model to learn sentimentspecific word embeddings from tweets annotated by emoticons. We use element-wise max, min, and avg operations for the word vectors to extract features. 2.4 Polarity Boosting Trick Predicted scores indicate the confidence of classifier. If the pos/neg classifier has a high confidence to classify a tweet to positive or negative, it is less likely that this tweet is objective. Consequently, the absolute value of output of pos/neg classifier is used as a feature in sub/obj cla"
S15-2086,H05-1044,0,0.0384794,"ributes to the performances. It provides more explicit sentiment information than word2vec vectors. As shown in Table 2, the polarity boosting trick also contributes to the performance of our system on all the six datasets. Table 2: We compare the macro-averaged F1-scores of our system (Spp) with the best results of other teams in SemEval-2015. Our system achieves the highest F1scores on three out of six datasets. LvJn14 and Sarc14 become better. Moreover, the automatically learned lexicons play a positive role in our system. We also try some manually annotated lexicons (such as MPQA Lexicon (Wilson et al., 2005), and Bing Liu Lexicon (Hu and Liu, 2004)), but the performance drops on the dev data. It illustrates the coverage of lexicons is important for the informal text data. The cluster features are also useful in this task, because the clusters reduce the feature sparsity and have the ability to deal with out-ofvocabulary words. The deep learning significantly improves test results on all the datasets except on the sarcastic tweets. Using the clustering results of word2vec performs better and more stable than directly using the vectors as features. This feature template contributes more than other"
S17-2045,P02-1034,0,0.0962705,"of representations of the two pieces of text is used as a feature. Longest common subsequence: we measure the lexical similarity of each text pair with the term-level longest common subsequence (LCS) (Allison and Dix, 1986). The length of LCS is normalized by dividing the maximum length of the two pieces of text. Word overlap: we calculate the normalized count of common ngrams (n=1,2,3) and nouns. Tree kernels: tree kernels are similarity functions used to measure the syntactic similarity of a text pair. We compute the subtree kernel (ST) (Schlkopf et al., 2003), the subset tree kernel (SST) (Collins and Duffy, 2002), and the partial tree kernel (PTK) (Moschitti, 2006) on the parse trees of a text pair. Translation probability: we learn word-toword translation probabilities using GIZA++ 1 with the unannotated Qatar Living data. In training, we regard questions as source language and their answers as target language. Following (Jeon et al., 2005), we use translation probability p(qusetion A|question B) and p(comment|question) as features for a questionii = σ(W (i) ex,i + U (i) hx,i−1 + b(i) ) fi = σ(W (f ) ex,i + U (f ) hx,i−1 + b(f ) ) oi = σ(W (o) ex,i + U (o) hx,i−1 + b(o) ) ui = tanh(W (u) ex,i + U (u)"
S17-2045,P03-1054,0,0.0058722,"1 , ..., ex,i , ..., ex,I ] and Sy = [ey,1 , ..., ey,i , ..., ey,J ] respectively, where ex,i , ey,i are the embeddings of the i-th words of Sx and Sy respectively. Then Sx and Sy are encoded in hidden sequences by a bi-LSTM which consists of a forward LSTM and a backward LSTM. The forward LSTM reads Sx in its order (i.e., from wx,1 to wx,I ) and transforms it to a forward hidden se→ − → − quence { h x,i }Ii=1 . ∀i ∈ {1, . . . , I}, h x,i is defined by: Preprocessing We exploit NLTK toolkit (Loper and Bird, 2002) to conduct stemming, tokenization, and POS tagging. We use Stanford PCFG parser (Klein and Manning, 2003) to get the parse tree of each sentence. 2.2 Neural Matching Features Traditional NLP Features The following features are designed based on words and syntactic analysis. Tf-idf cosine: each piece of text is converted to a one hot representation weighted by tf-idf values, where tf is the term frequency in the text, and idf is calculated using the unannotated Qatar corpora (Nakov et al., 2017). The cosine of representations of the two pieces of text is used as a feature. Longest common subsequence: we measure the lexical similarity of each text pair with the term-level longest common subsequence"
S17-2045,W02-0109,0,0.132576,", given a text pair (Sx , Sy ), the model looks up an embedding table to convert Sx and Sy to Sx = [ex,1 , ..., ex,i , ..., ex,I ] and Sy = [ey,1 , ..., ey,i , ..., ey,J ] respectively, where ex,i , ey,i are the embeddings of the i-th words of Sx and Sy respectively. Then Sx and Sy are encoded in hidden sequences by a bi-LSTM which consists of a forward LSTM and a backward LSTM. The forward LSTM reads Sx in its order (i.e., from wx,1 to wx,I ) and transforms it to a forward hidden se→ − → − quence { h x,i }Ii=1 . ∀i ∈ {1, . . . , I}, h x,i is defined by: Preprocessing We exploit NLTK toolkit (Loper and Bird, 2002) to conduct stemming, tokenization, and POS tagging. We use Stanford PCFG parser (Klein and Manning, 2003) to get the parse tree of each sentence. 2.2 Neural Matching Features Traditional NLP Features The following features are designed based on words and syntactic analysis. Tf-idf cosine: each piece of text is converted to a one hot representation weighted by tf-idf values, where tf is the term frequency in the text, and idf is calculated using the unannotated Qatar corpora (Nakov et al., 2017). The cosine of representations of the two pieces of text is used as a feature. Longest common subse"
S17-2045,S17-2003,0,0.0948239,"Missing"
W00-1212,J91-4001,0,0.028505,"d performance and high robustness in parsing unrestricted texts and has been applied in a successful machine translation product. Introduction Substantial efforts have been made to parse western languages such as English, and many powerful computational models have been proposed (Gazdar, et al, 1987, Tomita, M, 1986). However, very limited work has been done with Chinese. This is mainly due to the fact that the structure of the Chinese language is quite different from English. Therefore the computational model in processing English may not be directly applied to the Chinese language. Lin-Shan Lee et al (1991) proposed a Chinese natural language processing system with special consideration of some typical phenomena of Chinese. Jinye Zhou et al (1986) presented a deterministic Chinese parsing methodology using formal semantics to combine syntactic and semantic analysis. However, most of the proposed approaches were realized on small-scale lexicon and rule base (usually thousands words and tens or hundreds rules). It is still an open issue whether these models will work on real texts containing various ungrammatical phenomena. A parser capable of handling real text should have not only large lexicon"
W00-1212,C94-2153,1,\N,Missing
W00-1219,O93-1009,0,0.0323252,"Missing"
W03-1610,W97-0703,0,0.0093278,"of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem"
W03-1610,P01-1008,0,0.203179,"Missing"
W03-1610,W02-1029,0,0.445681,"ut not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002) showed that the ensemble extractors outperformed the individual extractors, it still cannot overcome the deficiency of the methods using the monolingual corpus. To overcome the deficiencies of the methods using only one resource, our approach combines both monolingual and bilingual resources to automatically extract synonymous words. By combining the synonyms extracted by the individual extractors using the three resources, our approach can combine the merits of the individua"
W03-1610,P90-1034,0,0.349868,"al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in th"
W03-1610,C02-1084,0,0.015652,"ction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. 1 Introduction This paper addresses the problem of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1"
W03-1610,P98-1116,0,0.0214685,"parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many w"
W03-1610,P98-2127,0,0.347812,"tify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) us"
W03-1610,shimohata-sumita-2002-automatic,0,0.392551,"nym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002)"
W03-1610,C98-1112,0,\N,Missing
W03-1610,C98-2122,0,\N,Missing
W04-3248,P02-1051,0,0.21801,"rded. The scores between the candidate Chinese NEs and the source English NE are calculated via this formula as the value of this feature. 3.1.2 Transliteration Score Although in theory, translation scores can build up relations within correct NE alignments, in practice this is not always the case, due to the characteristics of the corpus. This is more obvious when we have sparse data. For example, most of the person names in Named Entities are sparsely distributed in the corpus and not repeated regularly. Besides that, some English NEs are translated via transliteration (Lee and Chang, 2003; Al-Onaizan and Knight, 2002; Knight and Graehl, 1997) instead of semantic translation. Therefore, it is fairly important to make transliteration models. Given an English Named Entity e, e = {e1 , e 2 ...en } , the procedure of transliterating e into a Chinese Named Entity c, c = {c 1 , c 2 ...c m } , can be described with Formula (3.4) (For simplicity of denotation, we here use e and c to represent English NE and Chinese NE instead of nee and nec ). ) c = arg max P(c |e) (3.4) c According to Bayes’ Rule, it can be transformed to: ) c = arg max P(c) * P(e |c) (3.5) c Since there are more than 6k common-used Chinese chara"
W04-3248,J96-1002,0,0.0190779,"nce we only have NEs identified on the source side, and there is no extra knowledge from the target side. Considering the inherent characteristics of NE translation, we can find several features that can help NE alignment; therefore, we use a maximum entropy model to integrate these features and carry out NE alignment. 3 NE Alignment with a Maximum Entropy Model Without relying on syntactic knowledge from either the English side or the Chinese side, we find there are several valuable features that can be used for Named Entity alignment. Considering the advantages of the maximum entropy model (Berger et al., 1996) to integrate different kinds of features, we use this framework to handle our problem. Suppose the source English NE nee , nee = {e1 , e2 ...en }, consists of n English words and the candidate Chinese NE nec , nec = {c1 , c 2 ...c m }, is composed of m Chinese characters. Suppose also that we have M feature functions hm (nec , nee ), m = 1,..., M . For each feature function, we have a model parameter λm , m = 1,..., M . The alignment probability can be defined as follows (Och and Ney, 2002): P ( ne c |ne e ) = p λ M ( ne c |ne e ) 1 M = exp[ ∑ λ m h m ( ne c , ne e ) ] m =1 M ∑ exp[ ∑ λ ne c&apos;"
W04-3248,P03-1012,0,0.0204953,"n section 2, we discuss related work on NE alignment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the s"
W04-3248,W03-1502,0,0.671577,"Missing"
W04-3248,P00-1050,0,0.0193047,"ure functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word"
W04-3248,J97-2004,0,0.0104284,"strapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs"
W04-3248,N03-1017,0,0.0192372,"Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. Huang et al. (2003) proposed to extract Named Entity translingual equivalences based on the minimization of a linearly combined multi-feature cost. But they require Named Entity Recognition on both the source side and the target side. Moore’s (2003) approach is based on a sequence of c"
W04-3248,P01-1050,0,0.0259095,"l et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. Huang et al. (2003) proposed to extract Named Entity translingual equivalences based on the minimization of a linearly combined multi-feature cost. But they require Named Entity Recognition on both the source side and the target side. Moore’s (2003) approach is based on a sequence of cost models. Ho"
W04-3248,J00-2004,0,0.0120793,"amework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al.,"
W04-3248,E03-1035,0,0.296191,"Missing"
W04-3248,J03-1002,0,0.0136093,"ith the step as 5k every time. 4.2 A translated Chinese NE may appear at a different position from the corresponding English NE in the sentence. IBM Model 4 (Brown et al., 1993) integrates a distortion probability, which is complete enough to account for this tendency. The HMM model (Vogel et al., 1996) conducts word alignment with a strong tendency to preserve localization from one language to another. Therefore we extract NE alignments based on the results of these two models as our baseline systems. For the alignments of IBM Model 4 and HMM, we use the published software package, GIZA++ 3 (Och and Ney, 2003) for processing. Some recent research has proposed to extract phrase translations based on the results from IBM Model (Koehn et al., 2003). We extract EnglishChinese NE alignments based on the results from IBM Model 4 and HMM. The extraction strategy takes each of the continuous aligned segments as one possible candidate, and finally the one with the highest frequency in the whole corpus wins. 1. Set the coefficients λi as uniform distribution; 2. Calculate all the feature scores to get the N-best list of the Chinese NE candidates; 3. Candidates with their values over a given threshold are con"
W04-3248,P02-1038,0,0.0222831,"that can be used for Named Entity alignment. Considering the advantages of the maximum entropy model (Berger et al., 1996) to integrate different kinds of features, we use this framework to handle our problem. Suppose the source English NE nee , nee = {e1 , e2 ...en }, consists of n English words and the candidate Chinese NE nec , nec = {c1 , c 2 ...c m }, is composed of m Chinese characters. Suppose also that we have M feature functions hm (nec , nee ), m = 1,..., M . For each feature function, we have a model parameter λm , m = 1,..., M . The alignment probability can be defined as follows (Och and Ney, 2002): P ( ne c |ne e ) = p λ M ( ne c |ne e ) 1 M = exp[ ∑ λ m h m ( ne c , ne e ) ] m =1 M ∑ exp[ ∑ λ ne c&apos; m (3.1) h m ( ne c&apos; , ne e ) ] m =1 The decision rule to choose the most probable aligned target NE of the English NE is (Och and Ney, 2002): neˆc = arg max{P(nec |nee )} nec M  (3.2) = arg max ∑ λ m hm (nec , nee ) nec  m =1  In our approach, considering the characteristics of NE translation, we adopt 4 features: translation score, transliteration score, the source NE and target NE’s co-occurrence score, and distortion score for distinguishing identical NEs in the same sentence. Next"
W04-3248,P00-1056,0,0.0425389,"ives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language"
W04-3248,P02-1052,0,0.0386143,"s related work on NE alignment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to corres"
W04-3248,J93-2003,0,\N,Missing
W04-3248,C96-2141,0,\N,Missing
W04-3248,C02-1010,1,\N,Missing
W04-3248,W03-0317,0,\N,Missing
W04-3248,P97-1017,0,\N,Missing
W07-0736,2004.tmi-1.8,0,0.240032,"Missing"
W07-0736,W02-1039,0,0.0340365,"s the reference and the other three references as candidate translations. Presumably, since the candidate sentences are near-optimal translations, the BLEU scores obtained in such a way should be close to 1. But our analysis shows a mean BLEU of only 0.1456398, with a standard deviation of 0.1522381, which means that BLEU is not very predictive of sentence level evaluation. The BLEU score is, however, still informative in judging the average MT system’s translation. 4.2.2 Dependency Structure Matching Dependency relation information has been widely used in Machine Translation in recent years. Fox (2002) reported that dependency trees correspond better across translation pairs than constituent trees. The information summarization community has also seen successful implementation of ideas similar to the depedency structure. Zhou et al.(2005) and Hovy et al.(2005) reported using Basic Elements (BE) in text summarization and its evaluation. In the current 3 We added an extremely small number to both matched ngrams and total number of n-grams. paper, we match a candidate translation with a reference translation on the following ﬁve dependency structure (DS) types: • • • • • Agent - Verb Verb - Pa"
W07-0736,2005.eamt-1.15,0,0.108219,"Missing"
W07-0736,P04-1077,1,0.89288,"Missing"
W07-0736,W05-0904,0,0.150542,"Missing"
W07-0736,P06-2070,0,0.0759207,"Missing"
W07-0736,quirk-2004-training,0,0.0453361,"2005,2006) as well as propose ways to evaluate MT evaluation metrics (Lin, et al. 2004). Previous studies, however, have focused on MT evaluation at the document level in order to ﬁght n-gram sparseness problem. While document level correlation provides us with a general impression of the quality of an MT system, researchers desire to get more informative diagnostic evaluation at sentence level to improve the MT system instead of just an overall score that does not provide details. Recent years have seen several studies investigating MT evaluation at the sentence level (Liu et al., 2005,2006; Quirk, 2004). The state-of-the-art sentence level correlations reported in previous work between human assessments and automatic scoring are around 0.20. Kulesza et al.(2004) applied Support Vector Machine classiﬁcation learning to sentence level MT evaluation and reported improved correlation with human judgment over BLEU. However, the classiﬁcation taxonomy in their work is binary, being either machine translation or human translation. Additionally, as discussed above, the inconsistency from the human annotators weakens the legitimacy of the classiﬁcation approach. Gamon et al.(2005) reported a study of"
W07-0736,W06-1610,1,0.880606,"Missing"
W08-0301,W05-0909,0,0.023308,"Missing"
W08-0301,J07-2003,0,0.506228,"s have, in general, lower scores, and 2 Literature Review Research work in SMT seldom treats SWD as a problem separated from other factors in translation. However, it can be found in different SMT paradigms the mechanism of handling SWD. As to the pioneering IBM word-based SMT models (Brown et al., 1990), IBM models 3, 4 and 5 handle spurious source words by considering them as corresponding to a particular EMPTY word token on the English side, and by the fertility model which allows the English EMPTY to generate a certain number of foreign words. As to the hierarchical phrase-based approach (Chiang, 2007), its hierarchical rules are more powerful in SWD than the phrase pairs 2 Model 1 Model 2 Model 3 ˜ >, its probability is < F˜ , E P () P (|f ) PCRF (|F~ (f ) ( ˜ F˜ ) = P (E| Table 1: Summary of the Three SWD Models ˜ = () P () if E ˜ ˜ F˜ ) otherwise P (¯ )|F |PT (E| ˜ F˜ ) is the probability of the phrase where PT (E| pair as registered in the translation table, and |F˜ | is the length of the phrase F˜ . The estimation of P () is done by MLE: therefore the decoder has a bias towards shorter translations. Word penalty (in fact, it should be renamed as word reward) is used to neutraliz"
W08-0301,P03-1054,0,0.00813748,"d for training language model. The development/test corpora are based on the test sets for NIST MT-2005/6. The alignment matrices of the training data are produced by the GIZA++ (Och and Ney, 2000b) word alignment package with its default settings. The subsequent construction of translation table was done in exactly the same way as explained 1 Maximum Entropy was also tried in our experiments but its performance is not as good as CRF. 4 Data FBIS BFT NIST in (Koehn et al., 2003). For SWD model 2, the phrase enumeration step is modified as described in section 3.2. We used the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar for its POS-tagging as well as finding the head/dependent words of all source words. The CRF toolkit used for model 3 is CRF ++ 2 . The training data for the CRF model should be the same as that for translation table construction. However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) f"
W08-0301,N03-1017,0,0.0738606,"Missing"
W08-0301,C00-2163,0,0.0999851,"special phrase pairs, TO EMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs. Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities. It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO - EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1. 3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to . This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1 − P0 . In the same style, SWD model 1 posits a probability P () for the translation of any source word to . The probabilities of normal phrase pairs should be weighed accordingly. For a source phrase containing only one word, its weight is simply P (¯ ) = 1 − P (). As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into , and therefore the weighing factor"
W08-0301,P00-1056,0,0.184679,"special phrase pairs, TO EMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs. Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities. It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO - EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1. 3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to . This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1 − P0 . In the same style, SWD model 1 posits a probability P () for the translation of any source word to . The probabilities of normal phrase pairs should be weighed accordingly. For a source phrase containing only one word, its weight is simply P (¯ ) = 1 − P (). As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into , and therefore the weighing factor"
W08-0301,P02-1040,0,0.0759968,"performance simply by removing OOVs. In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: baseline 28.01 29.82 29.77 model 1 29.71 31.55 31.39 model 2 29.48 31.61 31.33 model 3 29.64 31.75 31.71 Table 2: BLEU scores in Experiment 1: NIST’05 as dev and NIST’06 as test variation is to test each model when medium size of data are available.4 NIST All the sections of the NIST training set are used. The purpose of this variation is to test each model when a large amount of data are available. (Case-insensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. In each test in our experiments, maximum BLEU training were run 10 times, and thus there are 10 BLEU scores for the test set. In the following we will report the mean scores only. 4.2 Experiment Results and Analysis Table 2 shows the results of the first experiment, which uses the NIST MT-2005 test set as development data and the NIST MT-2006 test set as test data. The most obvious observation is that any SWD model achieves much higher BLEU score than the baseline, as there is at least 1.6 BLEU point improvement in each case, and in some case the improvement"
W08-0301,N03-1028,0,0.0112237,"he source language sentence: each source word is tagged either as “spurious” or “non-spurious”. Under such a perspective, SWD 3 model 2 is merely a unigram tagging model, and it uses only one feature template, viz. the lexical form of the source word in hand. Such a model can by no means encode any contextual information, and therefore it cannot handle the “ACCORDING - TO /dd NP EXPRESS /dd” example in section 1. An obvious solution to this limitation is a more powerful tagging model augmented with contextsensitive feature templates. Inspired by research work like (Lafferty et al., 2001) and (Sha and Pereira, 2003), our SWD model 3 uses first-order Conditional Random Field (CRF) to tackle the tagging task.1 The CRF model uses the following feature templates: The training data for the CRF model comprises the alignment matrices of the bilingual training data for the MT system. A source word (token) in the training data is tagged as “non-spurious” if it is aligned to some target word(s), otherwise it is tagged as “spurious”. The sentences in the training data are also POS-tagged and parsed by some dependency parser, so that each word can be assigned values for the POS-based feature templates as well as the"
W08-0301,D07-1056,1,0.849556,"training data for the CRF model should be the same as that for translation table construction. However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) for CRF training. The decoder in the experiments is our reimplementation of HIERO (Chiang, 2007), augmented with a 5-gram language model and a reordering model based on (Zhang et al., 2007). Note that no hierarchical rule is used with the decoder; the phrase pairs used are still those used in conventional phrase-based SMT. Note also that the decoder does not translate OOV at all even in the baseline case, and thus the SWD models do not improve performance simply by removing OOVs. In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: baseline 28.01 29.82 29.77 model 1 29.71 31.55 31.39 model 2 29.48 31.61 31.33 model 3 29.64 31.75 31.71 Table 2: BLEU scores in Experiment 1: NIST’05 as dev and NIST’06 a"
W08-0301,J90-2002,0,\N,Missing
Y09-1006,C08-1048,1,0.541616,"SS so that the selected words form a fluent sentence satisfying the constraints of Chinese couplets. This process is similar to translating a source language sentence into a target language sentence without word insertion, deletion and reordering, but the target sentence should satisfy some linguistic constraints. Based on this observation, we propose a multi-phase statistical machine translation approach to generate the SS. First, a phrase-based statistical machine translation (SMT) model is applied to generate an ∗ This paper summarizes our work on automatic generation of Chinese couplets (Jiang & Zhou, 2008) and automatic generation of poetry (He et al., to appear), both were conducted in Natural Language Group at Microsoft Research Asia. Copyright 2009 by Ming Zhou, Long Jiang, and Jing He 23rd Pacific Asia Conference on Language, Information and Computation, pages 43–52 43 N-best list of SS candidates. Then, a set of filters based on linguistic constraints for Chinese couplets is used to remove low quality candidates. Finally, a Ranking SVM is applied to rerank the candidates. A comprehensive evaluation using both human judgments and BLEU scores has been conducted and the results demonstrate th"
Y09-1006,N03-1017,0,0.00708971,"corpus: p ( fi |si ) = count ( f i , si ) m ∑ count ( f , s ) r i (2) r =1 where m is the number of distinct phrases that can be mapped to the phrase si and count ( fi, si ) is the number of occurrences that fi and si appear at the corresponding positions in a couplet. The inverted phrase translation model p ( si |fi ) has been proven useful in previous SMT research work (Och and Ney, 2002); so we also include it in our phrase-based SMT model. 45 Lexical weight (LW) Previous research work on phrase-based SMT has found that it is important to validate the quality of a phrase translation pair (Koehn et al., 2003). A good way to do this is to check its lexical weight pw( fi |si ) , which indicates how well its words translate to each other: Ni p w ( fi |si ) = ∏ p ( fj |s j ) (3) j =1 where Ni is the number of characters in fi or si , fj and sj are characters in fi and si respectively, and p ( fj |sj ) is the character translation probability of sj into fj . Like in phrase translation probability estimation, p ( fj |sj ) can be computed by relative frequency: p( f j |s j ) = count ( f j , s j ) m ∑ count ( f r ,sj) (4) r =1 where m is the number of distinct characters that can be mapped to the characte"
Y09-1006,P03-1021,0,0.0132718,"training data. Because the relationships between words and phrases in the FS and SS are usually reversible, to alleviate the data sparseness, we reverse the FS and SS in the training couplets and merge them with original training data for estimating translation probabilities. To smooth the language model, we add about 1,600,000 sentences which are not necessarily couplets, derived from ancient Chinese poetry, for language model training. To estimate the weights λi in formula (1), we use the Minimum Error Rate Training (MERT) algorithm, which is widely used for phrasebased SMT model training (Och, 2003). The training data and criteria (BLEU) for MERT will be explained in Subsection 4.1. 3.3 Linguistic Filters To reflect the linguistic constrains, a set of filters is used to remove candidates that violate linguistic constraints. For instance, Repetition filter removes candidates based on various rules related to word or character repetition. One such rule requires that if there are multiple characters that are identical in the FS, then the corresponding characters in the SS should be identical too. Conversely, if there are no identical words in the FS, then the SS should have no 46 identical"
Y09-1006,P02-1038,0,0.471411,"hts and a language model are used to score the output sentences, and a monotone phrase-based decoder is employed to get the N-best results. Then a set of filters based on linguistic constraints of Chinese couplets are used to remove low quality candidates. Finally a Ranking SVM model is used to rerank the candidates using additional features like word associations, etc. 44 3.1 Phrase-based SMT Model Given a FS denoted as F = { f1 , f 2 ,..., f n } , our objective is to seek a SS denoted as S = {s1 , s2 ,..., sn } , where fi and si are Chinese characters, so that p(S|F) is maximized. Following Och and Ney (2002) which departs from the traditional noisy-channel approach and uses a more general log-linear model, the S* that maximizes p(S|F) can be expressed as follows: S * = arg max p(S |F ) S (1) M = arg max S ∑ λ i log h i ( S , F ) i =1 where the hi(S,F) are feature functions and M is the number of feature functions. In our design, characters are used instead of words as translation units to form phrases. This is because Chinese couplets use dense language mostly following the similar style of ancient Chinese and most of words contain only one character. With this character based approach, we can av"
Y09-1006,J04-4002,0,0.0206873,"l repetition. Given a FS, writing a good SS is a difficult task because the SS must conform to constraints on syntax, rhyme and semantics, as described above. It requires the writer to innovatively use extensive knowledge in multiple disciplines. 3 SMT-Based Couplet Generation Model In this paper, a multi-phase statistical machine translation (SMT) approach is designed, where an SMT system generates an N-best list of candidates and then a ranking model is used to determine the new ranking of the N-best results using additional features. This approach is similar to reranking approaches of SMT (Och and Ney, 2004). In our SMT system, a phrasebased log-linear model is applied where two phrase translation models, two lexical weights and a language model are used to score the output sentences, and a monotone phrase-based decoder is employed to get the N-best results. Then a set of filters based on linguistic constraints of Chinese couplets are used to remove low quality candidates. Finally a Ranking SVM model is used to rerank the candidates using additional features like word associations, etc. 44 3.1 Phrase-based SMT Model Given a FS denoted as F = { f1 , f 2 ,..., f n } , our objective is to seek a SS"
Y09-1006,P02-1040,0,0.0772725,"as follows (Jiang & Zhou, 2008). • Mutual information (MI) score: • MI-based structural similarity (MISS) score: 4 4.1 Experimental Results Evaluation Method Automatic evaluation is very important for parameter estimation and system tuning. An automatic evaluation needs a standard answer data set and a metric to show for a given input sentence the closeness of the system output to the standard answers. Since generating the SS given the FS is viewed as a kind of machine translation process, the widely accepted automatic SMT evaluation methods may be applied to evaluate the generated SSs. BLEU (Papineni et al., 2002) is widely used for automatic evaluation of machine translation systems. It measures the similarity between the MT system output and human-made reference translations. The BLEU metric ranges from 0 to 1 and a higher BLEU score indicates better translation quality. N BLEU = BP • exp(∑ wn log pn) (6) n =1 Some adaptation is necessary to use BLEU for evaluation of our couplet generator. First, pn, the n-gram precision, should be position-sensitive in the evaluation of SSs. Second, BP, the brevity penalty, should be removed, because all system outputs have the same length and it has no effect in e"
