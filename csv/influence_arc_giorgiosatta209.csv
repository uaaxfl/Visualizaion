2000.iwpt-1.4,P96-1023,0,0.068983,"Missing"
2000.iwpt-1.4,P98-1035,0,0.0116737,"pendency grammar [ 13], lexicalized tree-adjoining grammar [ 1 7], link gram mar [20], head-driven phrase-structure grammar [ 14], tree insertion grammar [ 18], combinatorial cat egorial grammar [2 1] and bilexical grammar [7]. Probabilistic lexicalized grammars have also been exploited in state-of-the-art, real-world parsers, as reported in [ 1 1], [ 1], [6], [2], [4], a nd [9]. Other parsers or language models for speech recognition that do not directly exploit a generative grammar, still are heavily based on lexicalization, as for instance the systems presented in [ 10], [ 12], [ 15] and [3]. The wide diffusion of lexicalized grammars is mainly due to the capability of these formalisms to control syntactic acceptability, when this is sensitive to individual words in the language, and word selection, accounting for genuinely lexical factors as well as semantic and world k nowledge conditions. More precisely, lexicalized grammars can select the complements and modifiers that a constituent can take, on the basis of special words playing a particularly informative role within the given constituent and the given complements and modifiers. These special words are typically identified w"
2000.iwpt-1.4,P97-1003,0,0.0240981,"duction In recent years, much of the parsing literature has focused on so-called lexicalized grammars, that is grammars in which each individual rule is specialized for one or more lexical items. Formalisms of this sort include dependency grammar [ 13], lexicalized tree-adjoining grammar [ 1 7], link gram mar [20], head-driven phrase-structure grammar [ 14], tree insertion grammar [ 18], combinatorial cat egorial grammar [2 1] and bilexical grammar [7]. Probabilistic lexicalized grammars have also been exploited in state-of-the-art, real-world parsers, as reported in [ 1 1], [ 1], [6], [2], [4], a nd [9]. Other parsers or language models for speech recognition that do not directly exploit a generative grammar, still are heavily based on lexicalization, as for instance the systems presented in [ 10], [ 12], [ 15] and [3]. The wide diffusion of lexicalized grammars is mainly due to the capability of these formalisms to control syntactic acceptability, when this is sensitive to individual words in the language, and word selection, accounting for genuinely lexical factors as well as semantic and world k nowledge conditions. More precisely, lexicalized grammars can select the complements"
2000.iwpt-1.4,1997.iwpt-1.10,0,0.0236844,"GRAMMARS Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova via Gradenigo 6 / A 35131 Padova, Italy satta@dei.u nipd.it 1 Introduction In recent years, much of the parsing literature has focused on so-called lexicalized grammars, that is grammars in which each individual rule is specialized for one or more lexical items. Formalisms of this sort include dependency grammar [ 13], lexicalized tree-adjoining grammar [ 1 7], link gram mar [20], head-driven phrase-structure grammar [ 14], tree insertion grammar [ 18], combinatorial cat egorial grammar [2 1] and bilexical grammar [7]. Probabilistic lexicalized grammars have also been exploited in state-of-the-art, real-world parsers, as reported in [ 1 1], [ 1], [6], [2], [4], a nd [9]. Other parsers or language models for speech recognition that do not directly exploit a generative grammar, still are heavily based on lexicalization, as for instance the systems presented in [ 10], [ 12], [ 15] and [3]. The wide diffusion of lexicalized grammars is mainly due to the capability of these formalisms to control syntactic acceptability, when this is sensitive to individual words in the language, and word selection, accounting f"
2000.iwpt-1.4,P99-1059,1,0.821525,"and word selection, accounting for genuinely lexical factors as well as semantic and world k nowledge conditions. More precisely, lexicalized grammars can select the complements and modifiers that a constituent can take, on the basis of special words playing a particularly informative role within the given constituent and the given complements and modifiers. These special words are typically identified with the lexical head and co-heads of a constituent, where with the term co-head of a constituent A we denote a lexical head of a ny of the subconstituents of A. To give a simple example {from [8]), the word convene requires a n NP object to form a VP, but some NPs are more lexically or semantically appropriate than others, and the appropriateness depends largely on the NP 's head, e.g., the word meeting vs. the word party. In this way, the grammar is able to make stipulations on the acceptability of input sentences like Nora convened the meeting and Nora convened the party. This was not satisfactorily captured by earlier formalisms that did not make use of lexicalization mechanisms. See [5] for further discussion. W ithin the parsing community, and in the speech community as well, a c"
2000.iwpt-1.4,1997.iwpt-1.13,0,0.20709,"recent years, much of the parsing literature has focused on so-called lexicalized grammars, that is grammars in which each individual rule is specialized for one or more lexical items. Formalisms of this sort include dependency grammar [ 13], lexicalized tree-adjoining grammar [ 1 7], link gram mar [20], head-driven phrase-structure grammar [ 14], tree insertion grammar [ 18], combinatorial cat egorial grammar [2 1] and bilexical grammar [7]. Probabilistic lexicalized grammars have also been exploited in state-of-the-art, real-world parsers, as reported in [ 1 1], [ 1], [6], [2], [4], a nd [9]. Other parsers or language models for speech recognition that do not directly exploit a generative grammar, still are heavily based on lexicalization, as for instance the systems presented in [ 10], [ 12], [ 15] and [3]. The wide diffusion of lexicalized grammars is mainly due to the capability of these formalisms to control syntactic acceptability, when this is sensitive to individual words in the language, and word selection, accounting for genuinely lexical factors as well as semantic and world k nowledge conditions. More precisely, lexicalized grammars can select the complements and modif"
2000.iwpt-1.4,H94-1052,0,0.0456734,"Missing"
2000.iwpt-1.4,C88-2121,0,0.154716,"Missing"
A00-2036,P96-1023,0,0.0609448,"Missing"
A00-2036,P98-1035,0,0.0241458,"es can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and was employed at AT&T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN.&quot; BioInformatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sistemi ad Addestramento Automatico per l&apos;"
A00-2036,P97-1003,0,0.036103,"that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (A1shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibility of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as follows. We introduce bilexical context-free grammars in Section 2, and discuss parsing with the correctprefix property in Secti"
A00-2036,P81-1022,0,0.248991,"g. Although intuitive, the notion of left-to-right parsing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algor!thm (Earley, 1970), tabular left-corner and P L R parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al &quot; - a n , with n _&gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i,j] to denote string a i a i + l &quot; .aj; if i &gt; j, we define w[i, j] = c. Let G -- (VN,VT,P,S) be a CFG and let w = al ... an with n _&gt; 0 be some string over VT. A reco g n i z e r for the CFG class is an algorithm R that, on input (G,w), decides whether w E L(G). We say that R satisfies t"
A00-2036,P99-1059,1,0.764004,"chniques that do not require grammar precompilation cannot be directly extended to process the above mentioned grammars (resp. language models) within an acceptable time bound. The second result provides evidence that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (A1shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibility of particular pairs of words in particular roles, something that was not necessarily true of"
A00-2036,1997.iwpt-1.10,0,0.0552158,"Missing"
A00-2036,P96-1032,1,0.897363,"Missing"
A00-2036,P94-1017,1,0.84454,"sing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algor!thm (Earley, 1970), tabular left-corner and P L R parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al &quot; - a n , with n _&gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i,j] to denote string a i a i + l &quot; .aj; if i &gt; j, we define w[i, j] = c. Let G -- (VN,VT,P,S) be a CFG and let w = al ... an with n _&gt; 0 be some string over VT. A reco g n i z e r for the CFG class is an algorithm R that, on input (G,w), decides whether w E L(G). We say that R satisfies the c o r r e c t - p r e f i x p r o p e r t y (CPP) if"
A00-2036,W97-0301,0,0.0247782,"ctional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and was employed at AT&T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN.&quot; BioInformatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sistemi ad"
A00-2036,P80-1024,0,0.205014,"ition. These algorithms may also consult input symbols from left to right, but the processing that takes place to the right of some position i does not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up methods, such as left-corner parsing without topdown filtering (Wiren, 1987). Algorithms that do satisfy the C P P make use of some form of top-down prediction. Top-down prediction can be implemented at parse-time as in the case of Earley&apos;s algorithm by means of the &quot;predictor&quot; step, or can be precompiled, as in the case of left-corner parsing (Rosenkrantz and Lewis, 1970), by means of the left-corner relation, or as in the case of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construction of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilation of the input grammar. Among algorithms that satisfy the CPP, the most popular example of a recognizer that does i A context-free g r a m m a r G is reduced if every nonterminal of G can be p a r t of at least one derivation t h a t rewrites the s t a r t s y m b o l into some string of termina"
A00-2036,J97-3004,0,0.253017,"Missing"
A00-2036,E87-1037,0,\N,Missing
A00-2036,C98-1035,0,\N,Missing
bosco-etal-2008-comparing,W00-1903,0,\N,Missing
bosco-etal-2008-comparing,D07-1099,0,\N,Missing
bosco-etal-2008-comparing,D07-1097,0,\N,Missing
bosco-etal-2008-comparing,W07-1526,1,\N,Missing
bosco-etal-2008-comparing,J04-4004,0,\N,Missing
bosco-etal-2008-comparing,W01-0521,0,\N,Missing
bosco-etal-2008-comparing,W06-2920,0,\N,Missing
bosco-etal-2008-comparing,J03-4003,0,\N,Missing
bosco-etal-2008-comparing,H05-1066,0,\N,Missing
bosco-etal-2008-comparing,P06-3004,0,\N,Missing
bosco-etal-2008-comparing,P03-1056,0,\N,Missing
bosco-etal-2008-comparing,P99-1065,0,\N,Missing
bosco-etal-2008-comparing,P03-1013,0,\N,Missing
bosco-etal-2008-comparing,W04-1501,1,\N,Missing
bosco-etal-2008-comparing,bosco-lombardo-2006-comparing,1,\N,Missing
bosco-etal-2008-comparing,D07-1096,0,\N,Missing
bosco-etal-2008-comparing,H91-1060,0,\N,Missing
C04-1011,J97-2003,0,0.0259133,"ance is the entropy of the PCFG, which does not rely on the PFA. This means that if we are interested in the relative quality of different approximating PFAs with respect to a single input PCFG, the cross-entropy may be used instead of the KL distance. The constraint of determinism is not a problem in practice, as any FA can be determinized, and FAs derived by approximation algorithms are normally determinized (and minimized). As a second possible application, we now look more closely into the matter of determinization of finite-state models. Not all PFAs can be determinized, as discussed by (Mohri, 1997). This is unfortunate, as deterministic (P)FAs process input with time and space costs independent of the size of the automaton, whereas these costs are linear in the size of the automaton in the nondeterministic case, which may be too high for some real-time applications. Instead of distribution-preserving determinization, we may therefore approximate a nondeterministic PFA by a deterministic PFA whose probability distribution is close to, but not necessarily identical to, that of the first PFA. Again, an important question is how close the two models are to each other. It was argued before b"
C04-1011,W03-3016,1,0.719676,"· · · Xm ); h1 (π∩ ) = , if π∩ is (s, a, t) → a; • h2 (π∩ ) = , if π∩ is S∩ → (q0 , S, s); h2 (π∩ ) = τ , if π∩ is (s, a, t) → a and τ is a s 7→ t; h2 (π∩ ) = , if π∩ is (s0 , A, sm ) → (s0 , X1 , s1 ) · · · (sm−1 , Xm , sm ). We define h(d∩ ) = (h1 (d∩ ), h2 (d∩ )). It can be d∩ easily shown that if S∩ ⇒ w and h(d∩ ) = (d, c), d then forc the same w we have S ⇒ w and (q0 , w) ` (s, ), some s ∈ Qf . Conversely, d if for some w, d and c we have S ⇒ w and c (q0 , w) ` (s, ), some s ∈ Qf , then there is precisely one derivation d∩ such that h(d∩ ) = (d, c) d∩ and S∩ ⇒ w. As noted before by (Nederhof and Satta, 2003), this construction can be extended to apply to a PCFG Gp = (G, pG ) and an FA M . The output is a PCFG G∩,p = (G∩ , pG∩ ), where G∩ is defined as above and pG∩ is defined by: • pG∩ (S∩ → (q0 , S, s)) = 1; • pG∩ ((s0 , A, sm ) → (s0 , X1 , s1 ) ··· (sm−1 , Xm , sm )) = pG (A → X1 · · · Xm ); • pG∩ ((s, a, t) → a) = 1. Note that G∩,p is non-proper. More specifically, probabilities of rules with left-hand side S∩ or (s0 , A, sm ) might not sum to one. This is not a problem for the algorithms presented in this paper, as we have never assumed properness for our PCFGs. What is most important here i"
C04-1011,J00-1003,1,0.845886,"sis may be used to select a small subset of those that seem most promising for full syntactic processing in a next phase, thereby avoiding further computational costs for the less promising hypotheses. As FAs cannot describe structure as such, it is impractical to write the automata required for such applications by hand, and even difficult to derive them automatically by training. For this reason, the used FAs are often derived from CFGs, by means of some form of approximation. An overview of different methods of approximating CFGs by FAs, along with an experimental comparison, was given by (Nederhof, 2000). The next step is to assign probabilities to the transitions of the approximating FA, as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership. Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al., 1994). See also (Mohri and Nederhof, 2001) for discussion of relate"
C04-1011,E91-1027,0,0.0389262,"n derived from CFGs, by means of some form of approximation. An overview of different methods of approximating CFGs by FAs, along with an experimental comparison, was given by (Nederhof, 2000). The next step is to assign probabilities to the transitions of the approximating FA, as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership. Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al., 1994). See also (Mohri and Nederhof, 2001) for discussion of related ideas. An obvious question to ask is then how well the resulting PFA approximates the input PCFG, possibly for different methods of determining an FA and different ways of attaching probabilities to the transitions. Until now, any direct way of measuring the distance between a PCFG and a PFA has been lacking. As we will argue in this paper, the natural distance measure between probability distributions, the Kullback-Leible"
C04-1011,P94-1011,0,0.0419769,"y means of some form of approximation. An overview of different methods of approximating CFGs by FAs, along with an experimental comparison, was given by (Nederhof, 2000). The next step is to assign probabilities to the transitions of the approximating FA, as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership. Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al., 1994). See also (Mohri and Nederhof, 2001) for discussion of related ideas. An obvious question to ask is then how well the resulting PFA approximates the input PCFG, possibly for different methods of determining an FA and different ways of attaching probabilities to the transitions. Until now, any direct way of measuring the distance between a PCFG and a PFA has been lacking. As we will argue in this paper, the natural distance measure between probability distributions, the Kullback-Leibler (KL) distance, is diffic"
C90-3022,P89-1032,0,\N,Missing
C90-3022,C88-1026,0,\N,Missing
C98-2152,J91-3004,0,0.347155,"Missing"
C98-2152,C92-2066,0,0.107344,"Missing"
C98-2152,C88-1075,0,0.099961,"Missing"
C98-2152,J95-2002,0,0.426521,"hnology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta~dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* Pr(at...aT~w). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are ,lot. q~'ee Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various"
C98-2152,W89-0211,0,0.0378846,"and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta~dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* Pr(at...aT~w). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are ,lot. q~'ee Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to enco"
C98-2187,P95-1023,0,0.0166447,"e is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case timc O(nS). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound"
C98-2187,P94-1022,0,0.630794,"he proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re1176 the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G --= (N, Z, I, A, S),"
C98-2187,J94-2002,1,0.874825,"lved in worst case timc O(nS). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re1176 the proposed vast majority been cur"
C98-2187,J94-1004,0,0.0252243,"az)..., (1) where X n is any projection of category X , y m a z is the maximal projection of Y, and the order of the constituents is variable. 3 A c o m p l e m e n t auxiliary tree, on the other hand, introduces a lexical head that subcategorizes for the tree's foot node and assigns it a thematic role. The structure of a complement auxiliary tree may be described as: xmax ._+ . . . yO ... xmax ... (2) where X max is the maximal projection Of some category X , and y 0 is the lexical projection 2The s a m e linguistic distinction is used in the conception of 'modifier' and 'predicative' trees (Schabes and Shieber, 1994), b u t Schabes and Shieber give the trees special properties in the calculation of derivation structures, which we do not. aThe CFG-like notation is taken directly from (Kroch, 1989), where it is used to specify labels at the root and frontier nodes of a tree without placing constraints on the internal structure. 1180 of some category Y, whose maximal projection dominates X max . From this we make the following observations: 1. Because it does not assign a theta role to its foot node, an athematic auxiliary tree may adjoin at any projection of a category, which we take to designate any adjunc"
C98-2187,J95-4002,0,0.508785,". A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re1176 the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G --= ("
C98-2187,P93-1017,0,\N,Missing
D11-1112,P99-1070,0,0.221486,"ntains nonterminal A, and another contains nonterminal B, ∗ where A ⇒ uBα for some u, α, then the system for the latter component must be solved first. The solution for a system of equations such as those described above can be irrational and nonexpressible by radicals, even if we assume that all the probabilities of the rules in the input PCFG are rational numbers, as observed by Etessami and Yannakakis (2009). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al. (1999). This corresponds to the so-called fixed-point iteration method, which is well-known in the numerical calculus literature and is frequently applied to systems of non-linear equations because it can be easily implemented. When a number of standard conditions are met, each iteration of (4) adds a fixed number of bits to the precision of the solution; see Kelley (1995, Chapter 4). Since each iteration can easily be implemented to run in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself and in the number of bits"
D11-1112,J98-2005,0,0.0637473,"of all strings: X p(w) = 1. w mk  X i=1 p(Ak → αk,i ) · |N | Y f (Aj ,αk,i ) zj j=1  . (2) Furthermore, for each i ≥ 1 we recursively define (i) functions gAk (z1 , z2 , . . . , z|N |) by (1) gAk (z1 , z2 , . . . , z|N |) = gAk (z1 , z2 , . . . , z|N |), (3) See (Booth and Thompson, 1973) for further discussion. In practice, PCFGs are often required to satisfy the additional condition: X p(π) = 1, and, for i ≥ 2, by for each A ∈ N . This condition is called properness. PCFGs that naturally arise by parameter estimation from corpora are generally consistent; see (S´anchez and Bened´ı, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed. We define the partition function of G as the function Z that assigns to each A ∈ N the value Using induction it is not difficult to show that, for (i) each k and i as above, gAk (0, 0, . . . , 0) is the sum of the probabilities of all complete derivations from Ak having depth not exceeding i. This implies that, for (i) i = 0, 1, 2, . . ., the sequence of the gAk (0, 0, . . . , 0) monotonically converges to Z(Ak ). For each k with 1 ≤ k ≤ |N |we can now write (i) gAk (z1 , z2 , . . . , z|N |) = gAk ( π=(A→α) Z(A) = X"
D11-1112,J99-1004,0,0.0552586,"lete derivations from Ak having depth not exceeding i. This implies that, for (i) i = 0, 1, 2, . . ., the sequence of the gAk (0, 0, . . . , 0) monotonically converges to Z(Ak ). For each k with 1 ≤ k ≤ |N |we can now write (i) gAk (z1 , z2 , . . . , z|N |) = gAk ( π=(A→α) Z(A) = X d,w d p(A ⇒ w). (1) Note that Z(S) = 1 means that G is consistent. More generally, in later sections we will need to compute the partition function for non-consistent PCFGs. We can characterize the partition function of a PCFG as a solution of a specific system of equations. Following the approach in (Harris, 1963; Chi, 1999), we introduce generating functions associated with the nonterminals of the grammar. For A ∈ N and α ∈ (N ∪ Σ)∗ , we write f (A, α) to denote the number of occurrences of symbol A within string α. Let N = {A1 , A2 , . . . , A|N |}. For each Ak ∈ N , let mk be the number of rules in R with left-hand side Ak , and assume some fixed order for these rules. For each i with 1 ≤ i ≤ mk , let Ak → αk,i be the i-th rule with left-hand side Ak . 1215 (4) (i−1) gA1 (z1 , z2 , . . . , z|N |), (i−1) gA2 (z1 , z2 , . . . , z|N |), . . . , (i−1) gA|N |(z1 , z2 , . . . , z|N |) ). Z(Ak ) = (i) = lim gAk (0, ."
D11-1112,J91-3004,0,0.658059,"ns in modeling of natural language syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 Giorgio Satta Dept. of Information Engineering University of Padua Italy satta@dei.unipd.it or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error cor"
D11-1112,W03-3016,1,0.684028,"he partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method. 3 Intersection of PCFG and FA It was shown by Bar-Hillel et al. (1964) that contextfree languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details. To avoid a number of technical complications, we assume the finite automaton has no epsilon transitions, and has only one final state. In the context of our use of this construction in the following sections, these restrictions are without loss of generality. Thus, a finite automaton (FA) M is represented by a 5-tuple (Σ, Q, q0 , qf , ∆), where Σ and Q are two finite sets of terminals and states, respectively, q0 is the initial state, qf is the final state, and ∆ is a a finite set of transitions, each of the form s 7→ t, where s, t ∈ Q and a ∈ Σ. A complete computation of M"
D11-1112,C92-2065,0,0.222355,"t L = {w1 , . . . , wm }. The objective is to compute: X σinfix (L, G) = p(xwy) w∈L,x,y∈Σ ∗ Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, b, c t0 c a t1 a, b, c b a b b, c a t2 t3 c t4 a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or s"
D11-1112,C92-2066,0,0.237736,"by a finite set L = {w1 , . . . , wm }. The objective is to compute: X σinfix (L, G) = p(xwy) w∈L,x,y∈Σ ∗ Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, b, c t0 c a t1 a, b, c b a b b, c a t2 t3 c t4 a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a"
D11-1112,J95-2002,0,0.935832,"ge syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 Giorgio Satta Dept. of Information Engineering University of Padua Italy satta@dei.unipd.it or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correction, when one i"
D11-1112,P87-1015,0,0.700898,"uction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, b, c t0 c a t1 a, b, c b a b b, c a t2 t3 c t4 a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or space demand, and that might be improved. The experiments were run on a desktop with a 3.0 GHz Pentium 4 processor. The implementation language is C++. The set-up of the experiments is similar to that in (Nederhof and Satta, 2008). A probabilistic contextfree grammar was extracted from sections 2-21 of the Penn Treebank version II. Subtrees that generated the empty string were systematically removed. The result was a CFG w"
D11-1114,W06-2922,0,0.201718,"s. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial"
D11-1114,P89-1018,0,0.0432038,"ng algorithm for simulating the computations of the system from §2–3. Given an input string w, our algorithm produces a compact representation of the set Γ (w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ (w), or else the probability of w, defined as the sum of all probabilities of computations in Γ (w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by 1239 c1 σ h1 minimum stack length in c1 , . . . , cm cm i i+1 i σ i+1 buﬀer size  p(la2 |b3 , b2 , b1 ) = θbrd1 · θbrd22,b1 · θbla32,b2 ,b1 , h1  p(ra1 |b2 , b1 ) = θbrd1 · θbra21,b1 , σ buﬀer p(shb |b1 ) = θbsh1 b , ∀b ∈ Σ , c0 stack distributions p(t |σ) as follows: stack size h2 h3 j Figure 2: Schematic representation of the computations γ associated with item [h1 , i, h2 h3 , j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to de"
D11-1114,W06-2920,0,0.0594122,"g able to handle only projective dependencies. This formulation permits parsing a subset of the non-projective trees, where this subset depends on the degree of the transitions. The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. For instance, on training data for Czech containing 28,934 non-projective relations, 27,181 can be handled by degree two transitions, and 1,668 additional dependencies can be handled by degree three transitions. Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. Our results are based on such a restriction. It is not difficult to extend our algorithms (§4) to higher degree transitions, but this comes at the expense of higher complexity. See §6 for more discussion on this issue. Let w = a0 · · · an−1 be an input string over Σ defined as in §2.1, with a0 = $. Our transition system for non-projective dependency parsing is (np) S (np) = (C, T (np) , I (np) , Ct ), 1236 Deg. 2 180 961 27181 876"
D11-1114,P99-1059,1,0.728094,"been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference. In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on contextfree grammars. See for instance Klein and Manning 1241 (2001) or Nederhof (2003). Such a parse forest encodes all valid computations in Γ (w), as desired. The algorithm runs in O(n8 ) time. Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n7 ). However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section. 5 Inference We turn next to specify exact inference with our model, for computing feature expectations. Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing. Here, a feature is a function over computations, providing the count of a pattern related to a parameter. We denote by fbla32,b2 ,b1 (γ), for instance, the number of occurrences of transition la2 within γ"
D11-1114,H05-1036,0,0.0155602,"the items in the tabular algorithm. More specifically, given a string w, we associate each item [h1 , i, h2 h3 , j] defined as in §4 with two quantities: I([h1 , i, h2 h3 , j]) = X p(γ) ; (5) γ=([h1 ],βi ),...,([h2 ,h3 ],βj ) O([h1 , i, h2 h3 , j]) = X p(γ) · p(γ 0 ) . (6) σ,γ=([¢],β0 ),...,(σ|h1 ,βi ) γ 0 =(σ|h2 |h3 ,βj ),...,([¢,0],βn ) I([h1 , i, h2 h3 , j]) and O([h1 , i, h2 h3 , j]) are called the inside and the outside probabilities, respectively, of item [h1 , i, h2 h3 , j]. The tabular algorithm of §4 can be used to compute the inside probabilities. Using the gradient transformation (Eisner et al., 2005), a technique for deriving outside probabilities from a set of inference rules, we can also compute O([h1 , i, h2 h3 , j]). The use of the gradient transformation is valid in our case because the tabular algorithm is unambiguous (see §4). Using the inside and outside probabilities, we can now efficiently compute feature expectations for our Ep(γ|w) [fbla32,b2 ,b1 (γ)] = X p(γ |w) · fbla32,b2 ,b1 (γ) = γ∈Γ (w) X 1 · p(w) = X p(γ0 ) · p(γ1 ) · p(γ2 ) · p(la2 |b3 , b2 , b1 ) · p(γ3 ) γ1 =(σ|h1 ,βi ),...,(σ|h2 |h3 ,βk ), h1 ,h2 ,h3 ,h4 ,h5 , γ2 =(σ|h2 |h3 ,βk ),...,(σ|h2 |h4 |h5 ,βj ), s.t. ah2 =b"
D11-1114,P10-1151,1,0.917041,"Missing"
D11-1114,E09-1034,1,0.886882,"Missing"
D11-1114,J11-3004,1,0.87406,"Missing"
D11-1114,J99-4004,0,0.0436778,"vre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of eleme"
D11-1114,P10-1110,0,0.56228,"mith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlm"
D11-1114,P98-1106,0,0.150218,"ures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition"
D11-1114,W01-1812,0,0.020146,"Missing"
D11-1114,D07-1015,0,0.0189125,"ed for unsupervised learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been suc"
D11-1114,E09-1055,1,0.906123,"ional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition-based model we use, together with a probabilistic generative i"
D11-1114,P11-1068,1,0.834515,"Missing"
D11-1114,D09-1005,0,0.0203863,"e recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent po"
D11-1114,W07-2216,1,0.910657,"d learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for project"
D11-1114,H05-1066,0,0.196007,"Missing"
D11-1114,J03-1006,0,0.0175004,"e not larger than the size of the stack associated with the initial configuration. As a final remark, we observe that we can keep track of all inference rules that have been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference. In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on contextfree grammars. See for instance Klein and Manning 1241 (2001) or Nederhof (2003). Such a parse forest encodes all valid computations in Γ (w), as desired. The algorithm runs in O(n8 ) time. Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n7 ). However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section. 5 Inference We turn next to specify exact inference with our model, for computing feature expectations. Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing. Here, a feature is a functi"
D11-1114,P05-1013,0,0.0605722,"community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) al"
D11-1114,W04-0308,0,0.121591,"are called reduce transitions, i.e., transitions that consume nodes from the stack. Notice that in the transition system at hand all the reduce transitions decrease the size of the stack by one element. Transition la1 creates a new arc with the topmost node on the stack as the head and the secondtopmost node as the dependent, and removes the latter from the stack. Transition ra1 is symmetric with respect to la1 . Transitions la1 and ra1 have degree one, as already explained. When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). Transition la2 and transition ra2 are very similar to la1 and ra1 , respectively, but with the difference that they create a new arc between the topmost node in the stack and a node which is two positions below the topmost node. Hence, these transitions have degree two, and are the key components in parsing of non-projective dependencies. We turn next to describe the equivalence between our system and the system in Attardi (2006). The transition-based parser presented by Attardi pushes back into the buffer elements that are in the top position of the stack. However, a careful analysis shows"
D11-1114,J08-4003,0,0.65713,"spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1"
D11-1114,P09-1040,0,0.435291,"tical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space represe"
D11-1114,C96-2215,0,0.177144,"Missing"
D11-1114,D07-1014,0,0.0205805,"ve dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and S"
D11-1114,W03-3023,0,0.462207,"n in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as c"
D11-1114,C98-1102,0,\N,Missing
D14-1099,P11-1068,1,0.932721,"Missing"
D14-1099,W06-2922,0,0.723948,"d Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). We provide an implementation for a dynamic oracle for this parser running in polynomial time. We experimentally compare the parser trained with the dynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917 Proceedings of the 2014 Conference on Empirical Methods in Natural Lan"
D14-1099,P13-1104,0,0.0729294,"Missing"
D14-1099,J93-2004,0,0.0459156,"(76.77, 68.20); similarly, for Hungarian we measure (75.66, 67.66) against (77.22, 68.42). Unfortunately, we have no explanation for these performance decreases, in terms of the typology of the non-projective patterns found in these two datasets. Note that Goldberg et al. (2014) also observed a performance decrease on the Basque dataset in the projective case, although not on Hungarian. Datasets We evaluate the parser performance over CoNLL 2006 and CoNLL 2007 datasets. If a language is present in both datasets, we use the latest version. We also include results over the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use the provided part-of-speech tags and the standard training/test partition; for the PTB we use automatically assigned tags, we train on sections 2-21 and test on section 23. 7.2 The parsing times measured in our experiments for the static and the dynamic oracles are the same, since the oracle algorithm is only used during the training stage. Thus the reported improvements in parsing accuracy come at no extra cost for parsing time. In the training stage, the extra processing needed to compute the"
D14-1099,W03-3017,0,0.724913,"is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars. Introduction Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As"
D14-1099,D11-1114,1,0.890516,"Missing"
D14-1099,W04-0308,0,0.38568,"et also h1 , h2 , h3 ∈ Vw . We are interested in computations of the parser processing the substring w0 and having the form c0 , c1 , . . . , cm , m ≥ 1, that satisfy both of the following conditions, exemplified in Figure 2. h3 h2 σ ... cm Figure 2: General form of the computations associated with an item [h1 , h2 , h3 ]. tached as a dependent of the second topmost node. The combination of the shift, left-arc and rightarc transitions provides complete coverage of projective dependency trees, but no support for nonprojectivity, and corresponds to the so-called arcstandard parser introduced by Nivre (2004). • For some sequence of nodes σ with |σ |≥ 0, the stack associated with c0 has the form σ|h1 and the stack associated with cm has the form σ|h2 |h3 . Support for non-projective dependencies is achieved by adding the transitions `la2 and `ra2 , which are variants of the left-arc and right-arc transitions, respectively. These new transitions create dependencies involving the first and the third topmost nodes in the stack. The creation of dependencies between non-adjacent stack nodes might produce crossing arcs and is the key to the construction of non-projective trees. • For each intermediate c"
D14-1099,W03-3023,0,0.796516,"ct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars. Introduction Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McC"
D14-1099,de-marneffe-etal-2006-generating,0,0.0913078,"Missing"
D14-1099,P11-2033,0,0.0407215,"grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006)"
D14-1099,C12-1059,0,0.816109,"tion among those that are optimal. The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been"
D14-1099,Q13-1033,0,0.620865,"ptimal. The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above ment"
D14-1099,Q14-1010,1,0.748368,"namic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several p"
D14-1099,P10-1110,0,0.0368007,"eered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was f"
E09-1055,W06-2922,0,0.0574397,"University Uppsala, Sweden marco.kuhlmann@lingfil.uu.se Giorgio Satta University of Padua Padova, Italy satta@dei.unipd.it Abstract The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using"
E09-1055,W06-2920,0,0.150008,"Missing"
E09-1055,C96-1058,0,0.0654075,"Missing"
E09-1055,E09-1034,0,0.81497,"Missing"
E09-1055,P07-1077,0,0.230527,"j  jwjf .rC1/ /, where P is the set of productions of the input grammar G, w is the input string, r is the maximal rank, and f is the maximal fan-out of a production in G (Seki et al., 1991). For a grammar G extracted by our technique, the number f equals the maximal block-degree per node. Hence, without any further modification, we obtain a parsing algorithm that is polynomial in the length of the sentence, but exponential in both the block-degree and the rank. This is clearly unacceptable in practical systems. The relative frequency of analyses with a block-degree  2 is almost negligible (Havelka, 2007); the bigger obstacle in applying the treebank grammar is the rank of the resulting LCFRS. Therefore, in the remainder of the paper, we present an algorithm that can transform the productions of the input grammar G into an equivalent set of productions with rank at most 2, while preserving the fan-out. This transformation, if it succeeds, yields a parsing algorithm that runs in time O.jP j  r  jwj3f /. 481 nmod root node tmp pp np sbj vc nmod 1 A 2 hearing 3 is 4 scheduled 5 on nmod ! g1 sbj ! g2 .nmod; pp/ root ! g3 .sbj; vc/ vc ! g4 .tmp/ pp ! g5 .np/ nmod ! g6 np ! g7 .nmod/ tmp ! g8 6 th"
E09-1055,P07-1021,1,0.853683,"pose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (Kuhlmann and Möhl, 2007). Furthermore, as pointed out by McDonald and Satta (2007), chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and Markovization, and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be both efficient and accurate. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478–486, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 478 Cont"
E09-1055,P06-2066,1,0.945858,"e Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (Kuhlmann and Möhl, 2007). Furthermore, as pointed out by McDonald and Satta (2007), chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and Markovization, and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be bo"
E09-1055,E06-1011,0,0.0581456,"l and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. In this paper, we provide two key tools for this approach. First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks. For efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonte"
E09-1055,W07-2216,1,0.114973,"ectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. In this paper, we provide two key tools for this approach. First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks. For efficient parsing, the ex"
E09-1055,H05-1066,0,0.272775,"Missing"
E09-1055,P05-1013,0,0.133784,"Missing"
E09-1055,W03-3017,0,0.0960098,"Missing"
E09-1055,N07-1050,0,0.0628004,"sala, Sweden marco.kuhlmann@lingfil.uu.se Giorgio Satta University of Padua Padova, Italy satta@dei.unipd.it Abstract The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing"
E09-1055,P87-1015,0,0.576452,"u. The number of blocks of u is called the block-degree of u. The blockdegree of a dependency tree is the maximum among the block-degrees of its nodes. A dependency tree is projective, if its block-degree is 1. Example 2 The tree shown in Figure 2 is not projective: both node 2 (hearing) and node 4 (scheduled) have block-degree 2. Their blocks are f 2 g; f 5; 6; 7 g and f 4 g; f 8 g, respectively. 2.2 LCFRS Linear Context-Free Rewriting Systems (LCFRS) have been introduced as a generalization of several mildly context-sensitive grammar formalisms. Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS. Recall that each nonterminal symbol A of G comes with a positive integer called the fan-out of A, and that a production p of G has the form A ! g.A1 ; : : : ; Ar / I g.xE1 ; : : : ; xEr / D ˛E ; where A; A1 ; : : : ; Ar are nonterminals with fan-out f; f1 ; : : : ; fr , respectively, g is a function symbol, and the equation to the right of the semicolon specifies the semantics of g. For each i 2 Œr, xEi is an fi -tuple of variables, and ˛E D h˛1 ; : : : ; ˛f i is a tupl"
E09-1055,W05-1505,0,\N,Missing
E17-1051,W04-0308,0,0.582249,"a set of configurations and transitions between them. The basic components of a configuration are a stack of partially processed words and a buffer of unseen input words. Starting from an initial configuration, the system applies transitions until a terminal configuration is reached. The sentence is scanned left to right, with linear time complexity for dependency parsing. This is made possible by the use of a greedy classifier that chooses the transition to be applied at each step. In this paper we introduce a parser for AMR that is inspired by the A RC E AGER dependency transition system of Nivre (2004). The main difference between our system and A RC E AGER is that we need to account for the mapping from word tokens to AMR nodes, non-projectivity of AMR structures and reentrant nodes (multiple incoming edges). Our AMR parser brings closer dependency parsing and AMR parsing by showing that dependency parsing algorithms, with some modAbstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We descri"
E17-1051,J08-4003,0,0.18503,"B. Cohen Giorgio Satta School of Informatics Dept. of Information Engineering University of Edinburgh University of Padua scohen@inf.ed.ac.uk Abstract ied, such as CCG (Steedman, 1996; Steedman, 2000) and UCCA (Abend and Rappoport, 2013). Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015a; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Zhou et al., 2016). This line of research is new and current results suggest a large room for improvement. Greedy transitionbased methods (Nivre, 2008) are one of the most popular choices for dependency parsing, because of their good balance between efficiency and accuracy. These methods seem promising also for AMR, due to the similarity between dependency trees and AMR structures, i.e., both representations use graphs with nodes that have lexical content and edges that represent linguistic relations. A transition system is an abstract machine characterized by a set of configurations and transitions between them. The basic components of a configuration are a stack of partially processed words and a buffer of unseen input words. Starting from"
E17-1051,K15-1004,0,0.465629,"Missing"
E17-1051,P13-1023,0,0.195614,"Missing"
E17-1051,D15-1136,0,0.256154,"Missing"
E17-1051,D15-1198,0,0.515494,"Missing"
E17-1051,W13-2322,0,0.217613,"ses sentences leftto-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity. 1 satta@dei.unipd.it Introduction Semantic parsing aims to solve the problem of canonicalizing language and representing its meaning: given an input sentence, it aims to extract a semantic representation of that sentence. Abstract meaning representation (Banarescu et al., 2013), or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and coreference resolution. AMR is partially motivated by the need to provide the NLP community with a single dataset that includes basic disambiguation information, instead of having to rely on different datasets for each disambiguation problem. The annotation process is straightforward, enabling the development of large datasets. Alternative semantic representations h"
E17-1051,P13-2131,0,0.538359,"Missing"
E17-1051,D14-1082,0,0.00349826,"or named entities and additional sparse features, extracted from the current configuration of the transition system; this is reported in more details in Table 3. The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion charOracle Training our system from data requires an oracle—an algorithm that given a gold-standard AMR graph and a sentence returns transition sequences that maximize the overlap between the gold-standard graph and the graph dictated by the sequence of transitions. We adopt a shortest stack, static oracle similar to Chen and Manning (2014). Informally, static means that if the actual configuration of the parser has no mistakes, the oracle provides a transition that does not introduce any mistake. Shortest stack means that the oracle prefers transitions where the number of items in the stack is minimized. Given the current configuration (σ, β, A) and the gold540 action Shift Shift Shift LArc RArc Shift Shift RArc Reduce Reduce stack [◦] [◦] [◦, boy] [◦, boy, and ] [◦, and ] [◦, and ] [◦, and ] [◦, and, girl ] [◦, and, girl ] [◦, and ] [◦] buffer [the,boy,and,the,girl] [boy,and,the,girl] [and,the,girl] [the,girl] [the,girl] [the,"
E17-1051,S14-2080,0,0.0372974,"t al. (2015a), called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently Zhou et al. (2016) presented a non-greedy transition system for AMR parsing, based on A RC S TANDARD (Nivre, 2004). Our transition system is also related to an adaptation of A RC E AGER for directed acyclic graphs (DAGs), introduced by Sagae and Tsujii (2008). This is also the basis for Ribeyre et al. (2015), a transition system used to parse dependency graphs. Similarly, Du et al. (2014) also address dependency graph parsing by means of transition systems. Analogously to dependency trees, dependency graphs have the property that their nodes consist of the word tokens, which is not true for AMR. As such, these transition systems are more closely related to traditional transition systems for dependency parsing. Our contributions in this paper are as follows: Figure 1: Annotation for the sentence “I beg you to excuse me.” Variables are in boldface and concepts and edge labels are in italics. :top beg-01 :ARG0 i you :ARG0 :ARG2 excuse-01 :ARG1 Figure 2: AMR graph representation f"
E17-1051,P15-2140,0,0.0126748,"ument structure. For instance, we may be interested in knowing whether two events or entities are related to each other, while not being concerned with the precise type of relation holding between them. No WSD gives a score that does not take into account word sense disambiguation errors. By ignoring the sense specified by the Propbank frame used (e.g., duck-01 vs duck-02) we have a score that does not take into account this additional complexity in the parsing procedure. To compute this score, we simply strip off the suffixes from all Propbank frames and calculate the Smatch score. Following Sawai et al. (2015), we also evaluate the parsers using the Smatch score on noun phrases only (NP-only), by extracting from the AMR dataset all noun phrases that do not include further NPs. As we previously discussed, reentrancy is a very important characteristic of AMR graphs and it is not trivial to handle. We therefore implement a test for it (Reentrancy), where we compute the Smatch score only on reentrant edges. Concept identification is another critical component of the parsing process and we therefore compute the F-score on the list of predicted concepts (Concepts) too. Identifying the correct concepts is"
E17-1051,P14-1134,0,0.57426,"entity recognizer to make the correct predictions. In order to alleviate the problem of wrong automatic alignments with respect to polarity and better detect negation, we performed a post-processing step on the aligner output where we align the AMR constant - (minus) with words bearing negative polarity such as not, illegitimate and asymmetry. Table 6: Results on test split of LDC2015E86 for JAMR, CAMR and our A MR E AGER. J stands for JAMR and C for CAMR (followed by the year of publication). Best systems are in bold. 6 Experiments We compare our parser11 against two available parsers: JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015b; Wang et al., 2015a), using the LDC2015E86 dataset for evaluation. Both parsers are available online12 and were recently updated for SemEval-2016 Task 8 (Flanigan et al., 2016; Wang et al., 2016). However, CAMR’s SemEval system, which reports a Smatch score of 67, is not publicly available. CAMR has a quadratic worst-case complexity (although linear in practice). In JAMR, the concept identification step is quadratic and the relation identification step is O(|V |2 log |V |), with |V |being the set of nodes in the AMR graph. Table 6 shows the results obtained by the"
E17-1051,S16-1186,0,0.165267,"the aligner output where we align the AMR constant - (minus) with words bearing negative polarity such as not, illegitimate and asymmetry. Table 6: Results on test split of LDC2015E86 for JAMR, CAMR and our A MR E AGER. J stands for JAMR and C for CAMR (followed by the year of publication). Best systems are in bold. 6 Experiments We compare our parser11 against two available parsers: JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015b; Wang et al., 2015a), using the LDC2015E86 dataset for evaluation. Both parsers are available online12 and were recently updated for SemEval-2016 Task 8 (Flanigan et al., 2016; Wang et al., 2016). However, CAMR’s SemEval system, which reports a Smatch score of 67, is not publicly available. CAMR has a quadratic worst-case complexity (although linear in practice). In JAMR, the concept identification step is quadratic and the relation identification step is O(|V |2 log |V |), with |V |being the set of nodes in the AMR graph. Table 6 shows the results obtained by the parsers on all metrics previously introduced. On Smatch, our system does not give state-of-the-art results. However, we do obtain the best results for Unlabeled and Concept and outperform the other parses"
E17-1051,N15-3006,0,0.0865352,"Missing"
E17-1051,P16-1001,0,0.0614125,"Missing"
E17-1051,P15-2141,0,0.706086,", enabling the development of large datasets. Alternative semantic representations have been developed and stud536 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 536–546, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ( b / beg-01 :ARG0 ( i / i :ARG1 ( y / you) :ARG2 ( e / excuse-01 :ARG0 y :ARG1 i ) ) ifications, can be used for AMR. Key properties such as working left-to-right, incrementality1 and linear complexity further strengthen its relevance. The AMR parser of Wang et al. (2015a), called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently Zhou et al. (2016) presented a non-greedy transition system for AMR parsing, based on A RC S TANDARD (Nivre, 2004). Our transition system is also related to an adaptation of A RC E AGER for directed acyclic graphs (DAGs), introduced by Sagae and Tsujii (2008). This is also the basis for Ribeyre et al. (2015), a transition system used to parse dependency graphs. Similarly, Du et al. ("
E17-1051,Q15-1040,0,0.0467316,"u is aligned with xi and v is aligned with xj . The spanning set for e, written S(e), is the set of all nodes w such that π(w) = k and i < k < j if i < j or j < k < i if j < i. We say that e is projective if, for every node w ∈ S(e), all of its parent and child nodes are in S(e) ∪ {u, v}; otherwise, we say that e is non-projective. An AMR is projective if all of its edges are projective, and is non-projective otherwise. This corresponds to the intuitive definition of projectivity for DAGs introduced in Sagae and Tsujii (2008) and is closely related to the definition of non-crossing graphs of Kuhlmann and Jonsson (2015). Table 1 demonstrates that a relatively small percentage of all AMR edges are non-projective. Yet, a large fraction of the sentences contain at least one non-projective edge. Our parser is able to construct non-projective edges, as described in §3. since it is neither injective nor surjective. For each i ∈ [n], we let π −1 (i) = {v |v ∈ V, π(v) = i} be the pre-image of i under π (this set can be empty for some i), which means that we map a token in the sentence to a set of nodes in the AMR. In this way we can align each index i for x to the induced subgraph of G. More formally, we define ← −("
E17-1051,N15-1040,0,0.715322,", enabling the development of large datasets. Alternative semantic representations have been developed and stud536 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 536–546, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ( b / beg-01 :ARG0 ( i / i :ARG1 ( y / you) :ARG2 ( e / excuse-01 :ARG0 y :ARG1 i ) ) ifications, can be used for AMR. Key properties such as working left-to-right, incrementality1 and linear complexity further strengthen its relevance. The AMR parser of Wang et al. (2015a), called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently Zhou et al. (2016) presented a non-greedy transition system for AMR parsing, based on A RC S TANDARD (Nivre, 2004). Our transition system is also related to an adaptation of A RC E AGER for directed acyclic graphs (DAGs), introduced by Sagae and Tsujii (2008). This is also the basis for Ribeyre et al. (2015), a transition system used to parse dependency graphs. Similarly, Du et al. ("
E17-1051,P11-2033,0,0.0466942,"Missing"
E17-1051,D16-1065,0,0.800083,"remental Parser for Abstract Meaning Representation Marco Damonte School of Informatics University of Edinburgh m.damonte@sms.ed.ac.uk Shay B. Cohen Giorgio Satta School of Informatics Dept. of Information Engineering University of Edinburgh University of Padua scohen@inf.ed.ac.uk Abstract ied, such as CCG (Steedman, 1996; Steedman, 2000) and UCCA (Abend and Rappoport, 2013). Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015a; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Zhou et al., 2016). This line of research is new and current results suggest a large room for improvement. Greedy transitionbased methods (Nivre, 2008) are one of the most popular choices for dependency parsing, because of their good balance between efficiency and accuracy. These methods seem promising also for AMR, due to the similarity between dependency trees and AMR structures, i.e., both representations use graphs with nodes that have lexical content and edges that represent linguistic relations. A transition system is an abstract machine characterized by a set of configurations and transitions between the"
E17-1051,P14-5010,0,\N,Missing
E17-1051,C08-1095,0,\N,Missing
E91-1006,W90-0207,0,0.013045,"TAG G and a string w as input, and decides whether w e L ( G ) . This is done by recovering (partial) analyses for substrings of w and by combining them. More precisely, the algorithm factorizes analyses of derived trees by employing a specific structure called state. Each state retains a pointer to a node n in some tree ae l u A , along with two additional pointers (called Idol and rdot) to n itself or to Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) (Harbusch, 1990) to O(n 6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and O(n 9) (Schabes and Joshi, 1988). *Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would also like to thank Anne Abeill6. All errors are of course our own. - 27 - its children in a. Let an be a tree obtained from the maximal subtree of a with root n, by means of some adjoining operations. Informally speaking and with a little bit of simplification, the two following cases are possible. First, ff ldot, rdo~n, state s indicates that t"
E91-1006,W89-0205,1,0.884896,"the input string are selected and in the second step the input string is parsed with respect to this set of trees. Another paper by Schabes and Joshi (1989) shows how parsing strategies can take advantage of lexicalization in order to improve parsers' performance. Two major advantages have been discussed in the cited work: grammar filtering (the parser can use only a subset of the entire grammar) and bottom-up information (further constraints are imposed on the way trees can be combined). Given these premises and starting from an already known method for bidirectional CF language recognition (Satta and Stock, 1989), it seems quite natural to propose an anchor-driven bidirectional parser for Lexicalized TAGs that tries to make more direct use of the information contained within the anchors. The algorithm employs a mixed strategy: it works bottom-up from the lexical anchors and then expands (partial) analyses making top-down predictions. Abstract In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented. The algorithm takes advantage of a peculiar characteristic of Lexicalized TAGs, i.e. that each elementary tree is associated with a lexical item, called its anchor. Th"
E91-1006,C88-2121,0,0.0592882,"Missing"
E91-1006,P88-1032,0,0.0318566,"ides whether w e L ( G ) . This is done by recovering (partial) analyses for substrings of w and by combining them. More precisely, the algorithm factorizes analyses of derived trees by employing a specific structure called state. Each state retains a pointer to a node n in some tree ae l u A , along with two additional pointers (called Idol and rdot) to n itself or to Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) (Harbusch, 1990) to O(n 6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and O(n 9) (Schabes and Joshi, 1988). *Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would also like to thank Anne Abeill6. All errors are of course our own. - 27 - its children in a. Let an be a tree obtained from the maximal subtree of a with root n, by means of some adjoining operations. Informally speaking and with a little bit of simplification, the two following cases are possible. First, ff ldot, rdo~n, state s indicates that the part of a n dominated by the nodes between ldot an"
E91-1006,W89-0235,0,0.0650283,"Missing"
E91-1006,P85-1011,0,0.0877865,"a tabular method that accepts a TAG G and a string w as input, and decides whether w e L ( G ) . This is done by recovering (partial) analyses for substrings of w and by combining them. More precisely, the algorithm factorizes analyses of derived trees by employing a specific structure called state. Each state retains a pointer to a node n in some tree ae l u A , along with two additional pointers (called Idol and rdot) to n itself or to Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) (Harbusch, 1990) to O(n 6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and O(n 9) (Schabes and Joshi, 1988). *Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would also like to thank Anne Abeill6. All errors are of course our own. - 27 - its children in a. Let an be a tree obtained from the maximal subtree of a with root n, by means of some adjoining operations. Informally speaking and with a little bit of simplification, the two following cases are possible. First, ff ldot, rdo~n, state s indi"
E91-1006,P90-1036,0,\N,Missing
H05-1101,C88-1016,0,0.0770429,"Missing"
H05-1101,P05-1033,0,0.0251654,"r bound for the membership problem, in case chart parsing is adopted (Section 3); • we show that translating an input string into the best parse tree in the target language is NPhard, even in case productions are bounded in length (Section 4). Investigation of the computational complexity of translation models has started in (Knight, 1999) for word-to-word models. This paper can be seen as the continuation of that line of research. 2 Synchronous context-free grammars Several definitions for synchronous context-free grammars have been proposed in the literature; see for instance (Chiang, 2004; Chiang, 2005). Our definition is based on syntax-directed translation schemata (SDTS; Aho and Ullman, 1972), with the difference that we do not impose the restriction that two paired context-free productions have the same left-hand side. As it will be discussed in Section 4, this results in an enriched generative capacity when probabilistic extensions are considered. We assume the reader is familiar with the definition of contextfree grammar (CFG) and with the associated notion of derivation. 804 (t ) (t ) (t ) u10 A111 u11 A122 u12 · · · u1r−1 A1rr u1r , (t ) (t ) (t (t ) ) (t ) where r ≥ 0, u1i , u2i ∈ V"
H05-1101,P03-1011,0,0.0195618,"iting, two formal grammars are exploited, one describing the source language and the other describing the target language. Furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously. Formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed. Among the several proposals, we mention here the models presented in (Wu, 1997; Wu and Wong, 1998), (Alshawi et al., 2000), (Yamada and Knight, 2001), (Gildea, 2003) and (Melamed, 2003). In this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof. This is the most common choice 803 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 803–810, Vancouver, October 2005. 2005 Association for Computational Linguistics in statistical translation models that exceed the generative power of finite-state machinery. We focus on two associated computational problems that have been defined in the literature. One is the membership"
H05-1101,J99-4005,0,0.0248836,". Therefore synchronous strings have the general form • we show that the membership problem is NPhard, unless a constant bound is imposed on the length of the productions (Section 3); u20 A21π(1) u21 A22π(2) u22 · · · u2r−1 A2rπ(r) u2r , • we show an exponential time lower bound for the membership problem, in case chart parsing is adopted (Section 3); • we show that translating an input string into the best parse tree in the target language is NPhard, even in case productions are bounded in length (Section 4). Investigation of the computational complexity of translation models has started in (Knight, 1999) for word-to-word models. This paper can be seen as the continuation of that line of research. 2 Synchronous context-free grammars Several definitions for synchronous context-free grammars have been proposed in the literature; see for instance (Chiang, 2004; Chiang, 2005). Our definition is based on syntax-directed translation schemata (SDTS; Aho and Ullman, 1972), with the difference that we do not impose the restriction that two paired context-free productions have the same left-hand side. As it will be discussed in Section 4, this results in an enriched generative capacity when probabilisti"
H05-1101,N03-1019,0,0.0106223,"nformation Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy enoch@dei.unipd.it Abstract The approach started with the so-called IBM models (Brown et al., 1988), implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. These word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (Och et al., 1999; Och and Ney, 2002). Still, the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings. This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic contextfree grammars working in a ‘synchronous’ way. Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the lit"
H05-1101,P04-1084,1,0.349002,"G ([w1 , w2 ]) = σ∈D([w1 ,w2 ]) pG (σ). 3 The membership problem We consider here the membership problem for SCFG, defined as follows: for input instance a SCFG G and a pair [w1 , w2 ], decide whether [w1 , w2 ] is in T (G). This problem has been considered for instance in (Wu, 1997) for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora, as for instance segmentation, bracketing, phrasal and word alignment. We show that the membership problem for SCFGs is NPhard. The result could be derived from the findings in (Melamed et al., 2004) that synchronous rewriting systems as SCFGs are related to the class of so called linear context-free rewriting systems (LCFRSs) and from the result that the membership problem for LCFRSs is NP-hard (Satta, 1992; Kaji and others, 1994). However, we provide here a direct proof, to simplify the presentation. Theorem 1 The membership problem for SCFGs is NP-hard. Proof. We reduce from the three-satisfiability problem (3SAT, Garey and Johnson, 1979). Let hU, Ci be an instance of the 3SAT problem, where U = {u1 , . . . , up } is a set of variables and C = {c1 , . . . , cn } is a set of clauses. Ea"
H05-1101,N03-1021,0,0.100884,"rammars are exploited, one describing the source language and the other describing the target language. Furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously. Formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed. Among the several proposals, we mention here the models presented in (Wu, 1997; Wu and Wong, 1998), (Alshawi et al., 2000), (Yamada and Knight, 2001), (Gildea, 2003) and (Melamed, 2003). In this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof. This is the most common choice 803 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 803–810, Vancouver, October 2005. 2005 Association for Computational Linguistics in statistical translation models that exceed the generative power of finite-state machinery. We focus on two associated computational problems that have been defined in the literature. One is the membership problem, which invol"
H05-1101,P04-1083,0,0.0220389,", defined as: X pG ([w, t]) = pG ([t0 , t]). (2) y(t0 )=w We can now precisely define the variants of the translation problem we are interested in. Given as input a PSCFG Gp = (G, pG ) and two strings w1 , w2 ∈ VT∗ , output the pair of parse trees argmax y(t1 ) = w1 , y(t2 ) = w2 pG ([t1 , t2 ]). (3) If the synchronous productions in the underlying SCFG G have length bounded by some constant, then the above problem can be solved in polynomial time using extensions of the Viterbi search strategy to parse forests. This has been shown for instance in (Wu and Wong, 1998; Yamada and Knight, 2001; Melamed, 2004). 808 t (4) Even in case we impose some constant bound on the length of the synchronous productions in G, the above problem is NP-hard, as we show in what follows. We assume the reader is familiar with the definition of probabilistic context-free grammar (PCFG) and with the associated notion of derivation probability (Wetherell, 1980). We denote a PCFG as a pair (G, pG ), with G = (VN , VT , P, S) the underlying context-free grammar and pG the associated function providing the probability distributions for the productions in P , conditioned on their lefthand side. A probabilistic regular gramm"
H05-1101,P02-1038,0,0.00381584,"Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it Enoch Peserico Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy enoch@dei.unipd.it Abstract The approach started with the so-called IBM models (Brown et al., 1988), implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. These word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (Och et al., 1999; Och and Ney, 2002). Still, the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings. This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic contextfree grammars working in a ‘synchronous’ way. Two hardness results for the class NP are r"
H05-1101,W99-0604,0,0.0498659,"t. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it Enoch Peserico Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy enoch@dei.unipd.it Abstract The approach started with the so-called IBM models (Brown et al., 1988), implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. These word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (Och et al., 1999; Och and Ney, 2002). Still, the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings. This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic contextfree grammars working in a ‘synchronous’ way. Two hardness results fo"
H05-1101,P92-1012,1,0.775818,"). This problem has been considered for instance in (Wu, 1997) for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora, as for instance segmentation, bracketing, phrasal and word alignment. We show that the membership problem for SCFGs is NPhard. The result could be derived from the findings in (Melamed et al., 2004) that synchronous rewriting systems as SCFGs are related to the class of so called linear context-free rewriting systems (LCFRSs) and from the result that the membership problem for LCFRSs is NP-hard (Satta, 1992; Kaji and others, 1994). However, we provide here a direct proof, to simplify the presentation. Theorem 1 The membership problem for SCFGs is NP-hard. Proof. We reduce from the three-satisfiability problem (3SAT, Garey and Johnson, 1979). Let hU, Ci be an instance of the 3SAT problem, where U = {u1 , . . . , up } is a set of variables and C = {c1 , . . . , cn } is a set of clauses. Each clause is a set of three literals from {u1 , u1 , . . . , up , up }. The general idea of the proof is to use a string pair [w1 w2 · · · wp , wc ], where wc is a string representation of C and each wi is a stri"
H05-1101,P98-2230,0,0.0828317,"f compilers and making use of synchronous rewriting. In synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language. Furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously. Formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed. Among the several proposals, we mention here the models presented in (Wu, 1997; Wu and Wong, 1998), (Alshawi et al., 2000), (Yamada and Knight, 2001), (Gildea, 2003) and (Melamed, 2003). In this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof. This is the most common choice 803 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 803–810, Vancouver, October 2005. 2005 Association for Computational Linguistics in statistical translation models that exceed the generative power of finite-state machinery. We focus on two associated computational proble"
H05-1101,J97-3002,0,0.67108,"e theory of compilers and making use of synchronous rewriting. In synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language. Furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously. Formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed. Among the several proposals, we mention here the models presented in (Wu, 1997; Wu and Wong, 1998), (Alshawi et al., 2000), (Yamada and Knight, 2001), (Gildea, 2003) and (Melamed, 2003). In this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof. This is the most common choice 803 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 803–810, Vancouver, October 2005. 2005 Association for Computational Linguistics in statistical translation models that exceed the generative power of finite-state machinery. We focus on two associated"
H05-1101,P01-1067,0,0.509788,"riting. In synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language. Furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously. Formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed. Among the several proposals, we mention here the models presented in (Wu, 1997; Wu and Wong, 1998), (Alshawi et al., 2000), (Yamada and Knight, 2001), (Gildea, 2003) and (Melamed, 2003). In this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof. This is the most common choice 803 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 803–810, Vancouver, October 2005. 2005 Association for Computational Linguistics in statistical translation models that exceed the generative power of finite-state machinery. We focus on two associated computational problems that have been defined in the literature. One is"
H05-1101,C98-2225,0,\N,Missing
J10-3006,J85-4001,0,0.613197,"n problem for TL-MCTAG with rank 1 and unbounded fan-out is NP-complete. Proof We provide a reduction from 3PAR.6 Let s1 , . . . , s3m , t be an input instance of the 3PAR problem, with all of the integers si represented in unary notation. Our target grammar G is deﬁned as follows. We use a set of nonterminal symbols {S, A}, with S being the start symbol. We take the set of terminal symbols to be {a, $}. G contains two elementary tree sets. The ﬁrst set has a single elementary tree γ, corresponding to a context-free production of the form S → (AAA$)m−1 AAA: 6 We follow the proof strategy of Barton (1985) in this proof. 456 Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG Tree γ has a unique link impinging on all of the 3m occurrences of nonterminal A. The second (multi)set of G contains elementary trees γi , 1 ≤ i ≤ 3m. Each γi corresponds to a context-free production of the form A → asi : We also construct a string w = (at $)m−1 at . If there exists a partition for multiset Q = {s1 , . . . , s3m } satisfying the 3PAR requirement, we can directly construct a derivation for w in G, by sorting the elementary trees in the second set accordingly, and by inserting thes"
J10-3006,P81-1022,0,0.758747,"Missing"
J10-3006,W06-1506,0,0.0131711,"ementary structures, such as the syntax and the semantics of a single word or construction or the syntax of a single word or construction and its translation into another language, with a pair of elementary trees. This ﬂexibility permits conceptually simple, highly expressive, and tightly coupled modeling of the relationship between the syntax and semantics of a language or the syntax and semantics of two languages. As a result, it has frequently been put to use in a growing body of research into incorporating semantics into the TreeAdjoining Grammar (TAG) framework (Kallmeyer and Joshi 2003; Han 2006; Nesson and Shieber 2006, 2007). It is also under investigation as a possible base formalism for use in synchronous-grammar based machine translations systems (Nesson 2009). Similar pairing of elementary structures of the TAG formalism is too constrained to capture the inherent divergence in structure between different languages or even between the syntax and semantics of a language. Pairing of more expressive formalisms is too ﬂexible to provide appropriate constraints and has unacceptable consequences for processing efﬁciency. Although TL-MCTAG was ﬁrst introduced by Weir (1988) and shown a"
J10-3006,P92-1012,1,0.603392,"L-MCTAG membership is in NP, we construct a Turing machine that will non-deterministically guess a truncated derivation tree of size no larger than |G |· |w|. It then checks that the guessed derivation successfully derives w. Because the correctness of the derivation can be checked in linear time, this is sufﬁcient to show that TL-MCTAG membership is in NP.  We know from the equivalence of LCFRS and SL-MCTAG (and the rule-to-treetuple conversion method used to prove equivalency) (Weir 1988) and the fact that LCFRS membership is PSPACE-complete that SL-MCTAG membership is also PSPACEcomplete (Kaji et al. 1992, 1994). Until the results shown in Theorems 1 and 2 it was not known whether TL-MCTAG was in NP. Although the difference in generative capacity between TL-MCTAG and SL-MCTAG is well known, this proven difference in complexity (assuming NP = PSPACE) is novel. To understand the reason underlying the difference, we note that the bound on the length of non-splitting chains does not hold for set-local MCTAG. In set-local MCTAG a tree tuple may be non-splitting while also performing a permutation of the order of the lexical output generated by its children. Permutation is possible because set-loca"
J10-3006,J05-2003,0,0.0633171,"Missing"
J10-3006,P08-1069,1,0.869211,"Missing"
J10-3006,W07-0402,1,0.908429,"Missing"
J10-3006,H05-1101,1,0.84571,"e and one corresponding to an assignment of false. The links in the single initial tree permit only one of these two sets to be used. The tree set for a particular truth assignment for a particular variable vi makes it possible to introduce, by means of another adjunction, terminal symbols taken from the set {1, . . . , n} that correspond to each clause in C that would be satisﬁed by the given assignment to vi . In this way, the string w = 1 · · · n can be generated if and only if all clauses are satisﬁed by the truth assignment to some variable they contain. 1 We follow the proof strategy of Satta and Peserico (2005) in this and the proof of Theorem 3. 449 Computational Linguistics Volume 36, Number 3 We deﬁne a tree-local MCTAG G containing the following tree sets. The initial tree set S contains the single tree: In this tree, the “rows” correspond to the variables and the “columns” to the clauses. Each non-terminal node within a row is labeled with the same link to ensure that a tree set representing a single variable’s effect on each clause will adjoin at each link. For every variable vi , 1 ≤ i ≤ p, tree set Ti , used when representing an assignment of the value true to vi , contains n trees, one for"
J10-3006,J95-4002,0,0.0617667,"ess result still holds. The rank, r, of a grammar is the maximum number of derivational children possible for any tree in the grammar, or in other words, the maximum number of links in any tree in the grammar. We show that when rank is bounded, the NP-hardness result also holds. A notable aspect of all of the proofs given here is that they do not make use of the additional expressive power provided by the adjunction operation of TAG. Put simply, the trees in the tree sets used in our constructions meet the constraints of Tree Insertion Grammar (TIG), a known context-free–equivalent formalism (Schabes and Waters 1995). As a result, we can conclude that the increase in complexity stems from the multi-component nature of the formalism rather than from the power added by an unconstrained adjunction operation. 3.1 Universal Recognition of TL-MCTAG is NP-Complete In this section we prove that universal recognition of TL-MCTAG is NP-complete when neither the rank nor the fan-out of the grammar is bounded. Recall the 3SAT decision problem, which is known to be NP-complete. Let V = {v1 , . . . , vp } be a set of variables and C = {c1 , . . . , cn } be a set of clauses. Each clause in C is a disjunction of three li"
J10-3006,J94-1004,1,0.61007,"igatory adjunction constraint indicates that at least one link at a given node must be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate obligatory adjunction constraints by underlining the label of the node to which the constraint applies. Because we use explicit links, the edges in the derivation tree are labeled with the number of the link used rather than the traditional label of the address at which the operation takes place. Multiple adjunction refers to permitting an unbounded number of adjunctions to occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In the standard deﬁnition of TAG, multiple adjunction is disallowed to ensure that each derivation tree unambiguously speciﬁes a single derived tree (Vijay-Shanker 1987). Because each available adjunction is explicitly notated with a numbered link, our notation implicitly disallows multiple adjunction but permits a third possibility: bounded multiple adjunction. Bounded multiple adjunction permits the formalism to obtain some of the potential linguistic advantages of allowing multiple adjunction while preventing unbounded multiple adjunction. The usual constraint of allowing only one adjunct"
J10-3006,P85-1011,0,0.659159,"the derivation and derived trees are distinct. We depart from the traditional deﬁnition in notation only by specifying adjunction sites explicitly with numbered links in order to simplify the presentation of the issues raised by multi-component adjunctions. Each link may be used only once in a derivation. Adjunctions may only occur at nodes marked with a link. A numbered link at a single site in a tree speciﬁes that a single adjunction is available at that site. An obligatory adjunction constraint indicates that at least one link at a given node must be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate obligatory adjunction constraints by underlining the label of the node to which the constraint applies. Because we use explicit links, the edges in the derivation tree are labeled with the number of the link used rather than the traditional label of the address at which the operation takes place. Multiple adjunction refers to permitting an unbounded number of adjunctions to occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In the standard deﬁnition of TAG, multiple adjunction is disallowed to ensure that each derivation tree unambiguously speciﬁes a"
J10-3006,W07-0404,0,0.016427,"dy of work on induction of TAGs from a treebank exempliﬁed by Chen and Shanker (2004). The factorization performed in their work is done on the basis of syntactic constraints rather than with the goal of reducing complexity. Working from a treebank of actual natural language sentences, their work does not have the beneﬁt of explicitly labeled adjunction sites but rather must attempt to reconstruct a derivation from complete derived trees. The factorization problem we address is more closely related to work on factorizing synchronous context-free grammars (CFGs) (Gildea, Satta, and Zhang 2006; Zhang and Gildea 2007) and on factorizing synchronous TAGs (Nesson, Satta, and Shieber 2008). Synchronous grammars are a special case of multicomponent grammars, so the problems are quite similar to the TL-MCTAG factorization problem. However, synchronous grammars are fundamentally set-local rather than tree-local formalisms, which in some cases simpliﬁes their analysis. In the case of CFGs, the problem reduces to one of identifying problematic permutations of non-terminals (Zhang and Gildea 2007) and can be done efﬁciently by using a sorting algorithm to binarize any non-problematic permutations until only the int"
J10-3006,H86-1020,0,\N,Missing
J10-3006,P06-2036,1,\N,Missing
J11-4009,P96-1023,0,0.0314915,"thermore, no more than two symbols can be used in the right-hand side of a rule, and the terminal symbol associated with the lefthand side of a rule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, Un"
J11-4009,P01-1017,0,0.0542691,"lefthand side of a rule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department"
J11-4009,J03-4003,0,0.110964,"ule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department of Information Eng"
J11-4009,C96-1058,0,0.0252044,"e of a rule, and the terminal symbol associated with the lefthand side of a rule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX,"
J11-4009,1997.iwpt-1.10,0,0.355272,"Italy. E-mail: satta@dei.unipd.it. Submission received: 8 September 2010; revised submission received: 21 January 2011; accepted for publication: 17 April 2011. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 4 extent, independent one of the other, parsing based on 2-LCFGs can be asymptotically improved to O(|w|3 ). The reader is referred to Eisner and Satta (1999) for a detailed presentation of these computational results. In the literature, this condition on independence between left and right arguments of each head has been called splittability (Eisner 1997; Eisner and Satta 1999). Testing for splittability on an input 2-LCFG is therefore of central interest to parsing efﬁciency. The computability of this test has never been investigated, however. In this article splittability is deﬁned for 2-LCFGs in terms of equivalence to another grammar in which independence between left and right arguments of each head is ensured by a simple syntactic restriction. This restriction is called split form. Informally, a 2-LCFG is in split form if it can be factorized into individual subgrammars, one for each head, and each subgrammar produces the left and the r"
J11-4009,P99-1059,1,0.841792,"iversity of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department of Information Engineering, University of Padua, via Gradenigo 6/A, I-35131 Padova, Italy. E-mail: satta@dei.unipd.it. Submission received: 8 September 2010; revised submission received: 21 January 2011; accepted for publication: 17 April 2011. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 4 extent, independent one of the other, parsing based on 2-LCFGs can be asymptotically improved to O(|w|3 ). The reader is referred to Eisner and Satta (1999) for a detailed presentation of these computational results. In the literature, this condition on independence between left and right arguments of each head has been called splittability (Eisner 1997; Eisner and Satta 1999). Testing for splittability on an input 2-LCFG is therefore of central interest to parsing efﬁciency. The computability of this test has never been investigated, however. In this article splittability is deﬁned for 2-LCFGs in terms of equivalence to another grammar in which independence between left and right arguments of each head is ensured by a simple syntactic restrictio"
J12-3006,C92-2066,0,0.47281,"sk whether lexicalized TAGs can provide a strong lexicalization of CFGs. Schabes’ second result is that this is indeed the case. This means that, given a CFG G, one can always construct a lexicalized TAG generating the same set of parse trees as G, and consequently the same string language. Following from this result, there arose the possibility of establishing a third result, stating that TAGs are closed under strong lexicalization. Schabes (1990) states that this is the case, and provides an informal argument to justify the claim. The same claim still appears in two subsequent publications (Joshi and Schabes 1992, 1997), but no precise proof of it has appeared until now. We speculate that the claim could be due to the fact that adjunction is more powerful than substitution with respect to weak generative capacity. It turns out, however, that when it comes to strong generative capacity, adjunction also shares some of the restrictions of substitution. This observation leads to the main result of this article: TAGs are not closed under strong lexicalization. In other words, there are TAGs that lack a strongly equivalent lexicalized version. In the same line of investigation, Schabes and Waters (1995) int"
J12-3006,J95-4002,0,0.934162,"tions (Joshi and Schabes 1992, 1997), but no precise proof of it has appeared until now. We speculate that the claim could be due to the fact that adjunction is more powerful than substitution with respect to weak generative capacity. It turns out, however, that when it comes to strong generative capacity, adjunction also shares some of the restrictions of substitution. This observation leads to the main result of this article: TAGs are not closed under strong lexicalization. In other words, there are TAGs that lack a strongly equivalent lexicalized version. In the same line of investigation, Schabes and Waters (1995) introduce a restricted variant of TAG called tree insertion grammars (TIGs). This formalism severely restricts the adjunction operation originally deﬁned for TAGs, in such a way that the class of generated string languages, as well as the class of generated parse trees, are the same as those of CFGs. Schabes and Waters then conjecture that TIGs are closed under strong lexicalization. In this article we also disprove their conjecture. 2. Preliminaries We assume familiarity with the TAG formalism; for a survey, we refer the reader to Joshi and Schabes (1997). We brieﬂy introduce here the basic"
J15-2002,E03-1036,0,0.74025,"1994), one may restrict backward-crossed composition to instances where X and Y are both verbal categories—that is, functions into the category S of sentences (cf. Steedman 2000, Section 4.2.2). With this restriction the unwanted derivation in Figure 1 can be blocked, and a powerful shot by Rivaldo is still accepted as grammatical. Other syntactic phenomena require other grammar-specific restrictions, including the complete ban of certain combinatory rules (cf. Steedman 2000, Section 4.2.1). Over the past 20 years, CCG has evolved to put more emphasis on supporting fully lexicalized grammars (Baldridge and Kruijff 2003; Steedman and Baldridge 2011), in which as much grammatical information as possible is pushed into the lexicon. This follows the tradition of other frameworks such as Lexicalized Tree-Adjoining Grammar (LTAG) and Head-Driven Phrase Structure Grammar (HPSG). Grammar-specific rule restrictions are not connected to individual lexicon entries, and are therefore avoided. Instead, recent versions of CCG have introduced a new, lexicalized control mechanism in the form of modalities or slash types. The basic idea here is that combinatory rules only apply if the slashes in their input categories have"
J15-2002,T75-2001,0,0.4864,"Missing"
J15-2002,P96-1011,0,0.320562,"Missing"
J15-2002,P10-1055,1,0.827264,"Missing"
J15-2002,J93-4002,0,0.599437,"Missing"
J15-2002,C86-1048,0,0.777914,"Missing"
J15-2002,P88-1034,0,0.797741,"Missing"
J16-2002,J07-2003,0,0.060943,"tion algorithms for finding the treewidth of general graphs. 1. Introduction Synchronous context-free grammars (SCFGs) generalize context-free grammars (CFGs) to generate two strings simultaneously. The formalism dates from the early days of automata theory; it was developed under the name syntax-direct translation schemata to model compilers for programming languages (Lewis and Stearns 1968; Aho and Ullman 1969). SCFGs are widely used today to model the patterns of re-ordering between natural languages, and they form the basis of many state-of-the-art statistical machine translation systems (Chiang 2007). Despite the fact that SCFGs are a very natural extension of CFGs, and that the parsing problem for CFGs is rather well understood nowadays, our knowledge of the parsing problem for SCFGs is quite limited, with many questions still left unanswered. In this article we tackle one of these open problems. Unlike CFGs, SCFGs do not admit any canonical form in which rules are bounded in length (Aho and Ullman 1972), as for instance in the well-known Chomsky normal form for CFGs. A consequence of this fact is that the computational complexity of parsing with SCFG depends on the grammar. More precise"
J16-2002,P11-1046,1,0.593005,"Missing"
J16-2002,J16-2002,1,0.0513221,"Missing"
J16-2002,N10-1118,1,0.860575,"ous rules and to the re-ordering of the nonterminals across the two components. Let us call fan-out the number of substrings generated by a parse tree (this notion will be formally defined later). It is well-known among parsing practitioners that the 211 Computational Linguistics Volume 42, Number 2 fan-out affects the number of stored edges for a given input string, and is directly connected to the space and time performance of the algorithm. A binary parsing strategy of fan-out ϕ has space complexity O(n2ϕ) and time complexity at most O(n3ϕ) where n is the sentence length (Seki et al. 1991; Gildea 2010). If we adopt the appropriate parsing strategy, we can reduce the fan-out, resulting in asymptotic improvement in the space and time complexity of our algorithm. To illustrate this claim we discuss a simple example. Example 3 Consider the synchronous rule 1 2 3 4 5 6 7 8 5 7 3 1 8 6 2 4 s : [A → B B B B B B B B A→B B B B B B B B ] (1) The permutation associated with this rule is schematically visualized in Figure 1. A naive parsing strategy for rule s would be to collect the nonterminals B k one at a time and in ascending order of k. For instance, at the first step we combine B 1 and B 2 , con"
J16-2002,J11-1008,1,0.78433,"ide. In the case of an SCFG rule, each nonterminal has four endpoints, two on the English side and two on the Chinese side. If the rule has n linked nonterminals, including the left-hand side linked nonterminal, the dependency graph consists of 2n vertices and n cliques of size four. For instance, the SCFG rule [S → A 1 B 2 C 3 D 4 , S → B 2 D 4 A 1 C 3 ] is shown in the right part of Figure 17. A tree decomposition of the dependency graph corresponds directly to a parsing strategy, and the treewidth of the graph plus one is the exponent in the time complexity of the optimal parsing strategy (Gildea 2011). Each cluster of vertices in the tree decomposition corresponds to a combination step in a parsing strategy. The running intersection property of a tree decomposition ensures that each endpoint in the parsing rule has a consistent value at each step. Treewidth depends on the number of vertices in the largest cluster of the tree decomposition, which in turn determines the largest number of endpoints involved in any combination step of the parsing strategy. Our hardness result in this section is a reduction from treewidth of general graphs. Given an input graph G to the treewidth problem, we co"
J16-2002,N07-1019,1,0.714637,"Missing"
J16-2002,N09-1061,1,0.907213,"Missing"
J16-2002,J09-4009,1,0.923653,"also consider the problem of finding the parsing strategy with the lowest time complexity, and we show that it would require progress on long-standing open problems in graph theory either to find a polynomial algorithm or to show NP-hardness. The parsing complexity of SCFG rules increases with the increase of the number of nonterminals in the rule itself. Practical machine translation systems usually confine themselves to binary rules, that is, rules having no more than two right-hand side nonterminals, because of the complexity issues and because binary rules seem to be adequate empirically (Huang et al. 2009). Longer rules are of theoretical interest because of the naturalness and generality of the SCFG formalism. Longer rules may also be of practical interest as machine translation systems improve. For a fixed SCFG, complexity can be reduced by factoring the parsing of a grammar rule into a sequence of smaller steps, which we refer to as a parsing strategy. Each step of a parsing strategy collects nonterminals from the right-hand side of an SCFG rule into a subset, indicating that a portion of the SCFG rule has been matched to a subsequence of the two input strings, as we explain precisely in Sec"
J16-2002,P87-1015,0,0.837407,"Missing"
J16-2002,N04-1035,0,\N,Missing
J16-2002,N10-1035,1,\N,Missing
J16-2002,J93-2003,0,\N,Missing
J16-2002,H05-1101,1,\N,Missing
J16-2002,W90-0102,0,\N,Missing
J16-2002,C90-3045,0,\N,Missing
J16-2002,P10-1054,1,\N,Missing
J16-2002,N03-1017,0,\N,Missing
J16-2002,J94-1004,0,\N,Missing
J18-1004,W06-2922,0,0.540022,", Italy. E-mail: satta@dei.unipd.it. † Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 a number of extensions of stack-based transition systems to handle non-projective ´ trees (e.g., Attardi 2006; Nivre 2009; Choi and McCallum 2013; Gomez-Rodr´ ıguez and Nivre 2013; Pitler and McDonald 2015). Stack-based transition systems can produce general graphs rather than trees. Perhaps the simplest way to generate graphs is to shift one word at a time onto the stack, and then consider building all possible arcs between each word on the stack and the next word in the buffer. This is essentially the algorithm of Covington (2001), generalized to produce graphs rather than non-projective trees. This algorithm was also cast as a stack-based transition system by Nivre (2008). The algorithm runs in ti"
J18-1004,W13-2322,0,0.438888,"lgorithm for computing the minimal cache size needed to parse a given data set. In general, a graph’s relative treewidth with respect to an input order may be much higher than its absolute treewidth. However, if relative treewidth with respect to the real English word order is low, and not significantly higher than the absolute treewidth, this indicates that the word order provides valuable information about the graph structure to be predicted, and that efficient parsing is possible by making use of this information. We test this hypothesis with experiments on Abstract Meaning Representation (Banarescu et al. 2013), a semantic formalism where the meaning of a sentence is encoded as a directed graph. We find that, for English sentences, these structures have low relative treewidth with respect to the English word order, and can thus be parsed efficiently using a transition-based parser with small cache size. In order to compare across a wider variety of the semantic representations that have been 86 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing proposed (Kuhlmann and Oepen 2016), we also experiment with three sets of semantic dependencies from the Semeval 2015 semantic dependency par"
J18-1004,P13-1104,0,0.10716,"Missing"
J18-1004,E17-1051,1,0.909736,"Missing"
J18-1004,S14-2080,0,0.0199521,"graphs, with the motivation of representing semantically motivated predicate-argument relations and anaphoric references. This is done by dropping the constraint of a single head per word, and by using post-processing transformations that introduce non-projectivity. Titov et al. (2009) and Henderson et al. (2013) present a transition system for synchronous syntactic-semantic parsing, with the motivation of modeling the syntax/semantic interface. On the semantic side, their system mainly captures the predicate-argument structure and semantic role labeling. Their model has then been adapted by Du et al. (2014) for semantic-only parsing. Later, Wang, Xue, and Pradhan (2015) proposed a transition system for AMR parsing. Unlike traditional stack-based transition parsers that process input strings, 114 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing this system takes as input a dependency tree and processes its edges using a stack, applying tree-to-graph transformations that produce a directed acyclic graph. Similarly to Sagae and Tsujii (2008), the system presented by Damonte, Cohen, and Satta (2017) extends standard approaches for transition-based dependency parsing to AMR parsing,"
J18-1004,P14-1134,0,0.111776,"t of methods for unsupervised learning, as for example the inside-outside algorithm (Charniak 1993). Although we have treated the input buffer as an ordering of the vertices of the final graph, this is a simplification of the problem setting of semantic parsing for NLP. Given as input a sequence of English words, the parser must also predict which words correspond to zero, one, or more vertices of the final graph, and possibly insert vertices not corresponding to any English word. This could be accomplished either by preprocessing the input string with a separate concept identification phase (Flanigan et al. 2014), or by extending the actions of the transition system to include moves inserting new vertices into the graph. We have not included moves inserting new vertices, in order to simplify our exposition, but such moves would not fundamentally alter the correspondence between parsing runs and tree decompositions described in this article. The correspondence between runs of our parser and tree decompositions of the output graph allows for a precise characterization of the class of graphs covered, as well as simple and efficient algorithms for providing an oracle sequence of parser moves, and for dete"
J18-1004,Q14-1010,1,0.864703,"Missing"
J18-1004,J13-4002,0,0.242801,"Missing"
J18-1004,hajic-etal-2012-announcing,0,0.0434206,"Missing"
J18-1004,J13-4006,0,0.0213456,"rammars. We conclude this section with a discussion of other transition-based systems explicitly designed for graph parsing, as opposed to tree parsing. Sagae and Tsujii (2008) have possibly been the first authors to extend the stack-based transition framework for dependency tree parsing to directed acyclic graphs, with the motivation of representing semantically motivated predicate-argument relations and anaphoric references. This is done by dropping the constraint of a single head per word, and by using post-processing transformations that introduce non-projectivity. Titov et al. (2009) and Henderson et al. (2013) present a transition system for synchronous syntactic-semantic parsing, with the motivation of modeling the syntax/semantic interface. On the semantic side, their system mainly captures the predicate-argument structure and semantic role labeling. Their model has then been adapted by Du et al. (2014) for semantic-only parsing. Later, Wang, Xue, and Pradhan (2015) proposed a transition system for AMR parsing. Unlike traditional stack-based transition parsers that process input strings, 114 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing this system takes as input a dependency"
J18-1004,P10-1110,0,0.0618603,"Missing"
J18-1004,C12-1083,0,0.0396289,"se order, indicates that the real English word order provides valuable information that our parsing framework can exploit. 7. Comparison with Other Formalisms In this section we compare our cache transition parser with existing formalisms that have been used for graph-based parsing, as well as to similar transition-based systems for dependency tree parsing. 7.1 Connection to Hyperedge Replacement Grammars Hyperedge Replacement Grammars (HRGs) are a general graph rewriting formalism (Drewes, Kreowski, and Habel 1997) that has been applied by a number of authors to semantic graphs such as AMRs (Jones et al. 2012; Jones, Goldwater, and Johnson 2013; Peng, Song, and Gildea 2015). Our parsing formalism can be related to HRG through the concept of tree decomposition. 110 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing 1 → S 2 X 1 1 2 → X 2 1 2 X 1 1 2 2 → X Figure 12 An HRG that generates cycles of any size. Squares indicate grammar nonterminals with numbered ports. Number above vertices in a rule’s right-hand side correspond to the ports of the lefthand side nonterminal. An example derivation is shown in Figure 13. HRGs contain rules that rewrite a nonterminal hyperedge into a graph f"
J18-1004,W13-1810,0,0.339879,"Missing"
J18-1004,P11-1068,1,0.936355,"Missing"
J18-1004,J16-4009,0,0.178254,"Missing"
J18-1004,S16-1166,0,0.102428,"s encoded as a rooted, directed graph. Figure 8 shows an example of an AMR graph in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer, Gildea, and Kingsbury 2005) as well as very fine-grained semantic relations defined specifically for AMR. We use the training set of LDC2015E86 for SemEval 2016 task 8 on meaning representation parsing (May 2016), which contains 16,833 sentences. This data set covers various domains including newswire and Web discussion forums. For each graph, we derive a vertex order corresponding to the English word order by using the automatically generated alignments provided with the data set, which want-01 ARG1 ARG0 ARG1 like-01 ARG0 person name name op1 “John” person name name op1 “Mary” Figure 8 An example AMR graph for the sentence “John wants Mary to like him.” 106 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing 6000 5000 4000 3000 2000 1000 0 0 1 2 3 4 5 6 7 >=8 Figure 9 The distribution"
J18-1004,J08-4003,0,0.156637,"ions: They take as input a sentence and produce as output a graph representation of the semantics of the sentence itself. At the same time, recent years have seen a general trend from chart-based syntactic parsers toward stack-based transition systems, as the accuracy of transition systems has increased, and as speed has become increasingly important for real-world applications. On the syntactic side, stack-based transition systems for projective dependency parsing run in time O(n), where n is the sentence length; for a general overview of these systems, see, for instance, the presentation of Nivre (2008). There have also been ∗ Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu. ∗∗ Dipartimento di Ingegneria dell’Informazione, Universit`a di Padova, Via Gradenigo 6/A, 35131 Padova, Italy. E-mail: satta@dei.unipd.it. † Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative"
J18-1004,P09-1040,0,0.365254,"l: satta@dei.unipd.it. † Computer Science Department, University of Rochester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 a number of extensions of stack-based transition systems to handle non-projective ´ trees (e.g., Attardi 2006; Nivre 2009; Choi and McCallum 2013; Gomez-Rodr´ ıguez and Nivre 2013; Pitler and McDonald 2015). Stack-based transition systems can produce general graphs rather than trees. Perhaps the simplest way to generate graphs is to shift one word at a time onto the stack, and then consider building all possible arcs between each word on the stack and the next word in the buffer. This is essentially the algorithm of Covington (2001), generalized to produce graphs rather than non-projective trees. This algorithm was also cast as a stack-based transition system by Nivre (2008). The algorithm runs in time O(n2 ), a"
J18-1004,S15-2153,0,0.168651,"if we reverse the vertex order (reversed string order), the 108 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing relative treewidth is 3.08. This number is slightly larger than using the string order. The reason might be that English is more likely to have relation arcs going from left to right. If we randomize the vertex order, the relative treewidth becomes 4.84. We also evaluate the coverage of our algorithm on semantic graph-based representations other than AMR. We consider the set of semantic graphs in the Broad-Coverage Semantic Dependency Parsing task of SemEval 2015 (Oepen et al. 2015), which uses three distinct graph representations for English semantic dependencies. r r r DELPH-IN MRS-Derived Bi-Lexical Dependencies (DM): These semantic dependencies are derived from the annotation of Sections 00-21 of the WSJ Corpus with gold-standard HPSG analyses provided by the LinGO English Resource Grammar (Flickinger 2000; Flickinger, Zhang, and Kordoni 2012). Among other layers of linguistic analysis, this representation also includes logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS) (Copestake et al. 2005). Enju Predicate-Argument Structure"
J18-1004,J05-1004,1,0.381751,"Missing"
J18-1004,K15-1004,1,0.937101,"Missing"
J18-1004,N15-1068,0,0.383383,"chester, Rochester NY 14627. E-mail: xpeng@cs.rochester.edu.s Submission received: 19 December 2016; revised version received: 26 June 2017; accepted for publication: 7 July 2017. doi:10.1162/COLI a 00308 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 a number of extensions of stack-based transition systems to handle non-projective ´ trees (e.g., Attardi 2006; Nivre 2009; Choi and McCallum 2013; Gomez-Rodr´ ıguez and Nivre 2013; Pitler and McDonald 2015). Stack-based transition systems can produce general graphs rather than trees. Perhaps the simplest way to generate graphs is to shift one word at a time onto the stack, and then consider building all possible arcs between each word on the stack and the next word in the buffer. This is essentially the algorithm of Covington (2001), generalized to produce graphs rather than non-projective trees. This algorithm was also cast as a stack-based transition system by Nivre (2008). The algorithm runs in time O(n2 ), and requires the system to discriminate the arcs to be built from a large set of possi"
J18-1004,D14-1048,0,0.0840109,"Missing"
J18-1004,C08-1095,0,0.146218,"s the input order of the tokens, making it possible to produce non-projective trees. The cache parser can then be viewed as a generalization of the two-register transition systems. This is because in a cache parser one can move tokens in and out of the cache repeatedly, as already discussed. This is not possible in a register transition system. It would be interesting then to explore the use of our cache parsers for non-projective dependency grammars. We conclude this section with a discussion of other transition-based systems explicitly designed for graph parsing, as opposed to tree parsing. Sagae and Tsujii (2008) have possibly been the first authors to extend the stack-based transition framework for dependency tree parsing to directed acyclic graphs, with the motivation of representing semantically motivated predicate-argument relations and anaphoric references. This is done by dropping the constraint of a single head per word, and by using post-processing transformations that introduce non-projectivity. Titov et al. (2009) and Henderson et al. (2013) present a transition system for synchronous syntactic-semantic parsing, with the motivation of modeling the syntax/semantic interface. On the semantic s"
J18-1004,N15-1040,0,0.146164,"Missing"
J18-1004,D16-1065,0,0.0117091,"parsing. Unlike traditional stack-based transition parsers that process input strings, 114 Gildea, Satta, and Peng Cache Transition Systems for Graph Parsing this system takes as input a dependency tree and processes its edges using a stack, applying tree-to-graph transformations that produce a directed acyclic graph. Similarly to Sagae and Tsujii (2008), the system presented by Damonte, Cohen, and Satta (2017) extends standard approaches for transition-based dependency parsing to AMR parsing, allowing re-entrancies. Similar extensions of transition-based systems to AMR parsing also appear in Zhou et al. (2016) and Ribeyre, de La Clergerie, and Seddah (2015). All of these approaches are based on the idea of extending the transition inventory of standard transition-based dependency parsing systems in order to produce graph representations. On a theoretical perspective, what is missing from these proposals is a mathematical characterizaton of the set of graphs that can be produced and, with few exceptions, a precise description of the oracle algorithms that are used to produce training data from the gold graphs. Furthermore, all of these proposals still retain the stack and buffer architecture of the"
J18-1005,P13-1023,0,0.134502,"nt of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for r"
J18-1005,P99-1070,0,0.343387,"Missing"
J18-1005,W13-2322,0,0.530691,"process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov"
J18-1005,P91-1034,0,0.10154,"Missing"
J18-1005,J99-1004,0,0.0256352,"is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL"
J18-1005,P97-1003,0,0.185191,"parkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016),"
J18-1005,W95-0103,0,0.261819,"ther stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various seman"
J18-1005,P02-1001,0,0.117651,"iently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge,"
J18-1005,N16-1087,0,0.0374856,"niak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) conver"
J18-1005,P14-1134,0,0.0479906,"se representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equival"
J18-1005,P00-1065,1,0.624731,"Y-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 1. Introduction Statistical models of natural language semantics are making rapid progress. At the risk of oversimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible"
J18-1005,J99-4004,0,0.32362,"i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted DAG Automata for Semantic Graphs Unfortunately, we cannot derive a closed-form solution for the zeros of Equation (3). We therefore use gradient ascent. In CRF training for finite automata, the expectation in Equation (3) is computed efficiently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to pro"
J18-1005,N06-2015,0,0.0282756,"rsimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations,"
J18-1005,P99-1069,0,0.188893,"d Let w ∈ R be a vector of feature weights, which are the parameters to be estimated. Then we can parameterize δ in terms of the features and feature weights: δ(t) = exp w · Φ(t) so that δ(ρ ) = exp w · Φ(ρ ) X exp w · Φ(ρ ) [[M]] (D) = run ρ on D To obtain a probability model of runs of M on D, we simply renormalize the run weights: δ (ρ ) [[M]] (D) Assume a set of training examples {(Di , ρi ) |1 ≤ i ≤ N}, where each example consists of a DAG Di and an associated run ρi . We can train the model by analogy with conditional random fields (CRFs), which are log-linear models on finite automata (Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001). The training procedure is essentially gradient ascent on the log-likelihood, which is pM (ρ |D) = LL = N X log pM (ρi |Di ) i=1 = N X log δ(ρi ) − log [[M]] (Di )  i=1 The gradient of LL is: ∂LL = ∂w N  X i=1  ∂ log δ(ρ ) − ∂ log [[M]] (D ) i i ∂w ∂w N  X  1 1 ∂ ∂ = δ(ρi ) − [[M]] (Di ) δ(ρi ) ∂w [[M]] (Di ) ∂w i=1   N X X 1 ∂ δ ( ρ )  1 ∂ δ(ρi ) − = ∂w δ(ρi ) ∂w [[M]] (Di ) ρ on Di i=1   N X X δ ( ρ ) Φ(ρi ) − = Φ ( ρ ) [[M]] (Di ) i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted"
J18-1005,W15-4502,0,0.0159569,"ic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to"
J18-1005,N15-1114,0,0.0541523,"Missing"
J18-1005,J93-2004,0,0.0616988,"Missing"
J18-1005,S16-1166,0,0.0130652,"ch can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsin"
J18-1005,S15-2153,0,0.0866557,"Missing"
J18-1005,S14-2008,0,0.118213,"neration (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for research in semantic processing. What is still missing—in our view—is a formal framework for creating, combining, and using models involving graphs that parallels those for strings and trees. Finite string automata and transducers served as"
J18-1005,N15-1119,0,0.0128014,"nsolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences"
J18-1005,K15-1004,1,0.920031,"Missing"
J18-1005,P06-1055,0,0.0700356,"noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et"
J18-1005,W12-4209,0,0.402951,"lack a similar framework for learning and inferring semantic representations. Two such formalisms have recently been proposed for NLP: one is hyperedge replacement graph grammars, or HRGs (Bauderon and Courcelle 1987; Habel and Kreowski 1987; Habel 1992; Drewes, Kreowski, and Habel 1997), applied to AMR 120 Chiang et al. Weighted DAG Automata for Semantic Graphs ¨ parsing by various authors (Chiang et al. 2013; Peng, Song, and Gildea 2015; Bjorklund, Drewes, and Ericson 2016). The other formalism is directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki (1981) and extended by Quernheim and Knight (2012). In this article, we study DAG automata in depth, with the goal of enabling efficient algorithms for natural language processing applications. After some background on the use of graph-based representations in natural language processing in Section 2, we define our variant of DAG automata in Section 3. We then show the following properties of our formalism: r r r Path languages are regular, as is desirable for a formal model of AMRs (Section 4.1). The class of hyperedge-replacement languages is closed under intersection with languages recognized by DAG automata (Section 4.2). Emptiness is dec"
J18-1005,W95-0107,0,0.44496,"o be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more re"
J18-1005,W96-0213,0,0.301856,"wledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997"
J18-1005,J07-4003,0,0.0362617,"pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL = log pM (ρi , Di ) i ∂LL = ∂w X Φ(ρi ) − ED0 ,ρ [Φ(ρ )] i by using MCMC to estimate the second expectation. Finally, we may wish to learn a distribution over DAGs by learning the states in an unsupervised manner, either because it is not practical to annotate states by hand, or because we wish to automatically find the set of stat"
J18-1005,J01-4004,0,0.292972,"Missing"
J18-1005,P87-1015,0,0.781806,"Missing"
J18-1005,P15-2141,0,0.0234808,"Missing"
J18-1005,N15-1040,0,0.0381581,"Missing"
J18-3004,E03-1036,0,0.0328142,"his article) how specific features of the grammar contribute to the complexity of the parsing task. More precisely, when investigating the universal recognition problem one expresses the computational complexity of parsing in terms of several parameters (other than the input string length), as for instance the number of nonterminals, maximum size of rules, or maximum length of unary derivations. This provides a much more finegrained picture than the one that we obtain when analyzing the membership problem, and discloses the effects that each individual feature of the grammar has on parsing. 1 Baldridge and Kruijff (2003) show that the weak generative power of their formalism for multi-modal CCG is at most as strong as that of TAG, but they do not show that it is at least as strong. 449 Computational Linguistics Volume 44, Number 3 Structure of the Article. The remainder of this article is structured as follows. After presenting the VW-CCG formalism in Section 2, we first study in Section 3 the universal recognition problem for a restricted class of VW-CCG, where each category is “lexicalized” in the sense of the Principle of Adjacency. We show that for this subclass, universal recognition is NP-complete. Unde"
J18-3004,J07-4004,0,0.064094,"kenmaier and Steedman 2007), there has been a surge of interest in CCG within statistical and, more recently, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing a"
J18-3004,N10-1035,1,0.876518,"Missing"
J18-3004,J07-3004,0,0.059077,"and Baldridge 2011) is a wellestablished grammatical framework that has supported a large amount of work both in linguistic analysis and natural language processing. From the perspective of linguistics, the two most prominent features of CCG are its tight coupling of syntactic and semantic information, and its capability to compactly encode this information entirely within the lexicon. Despite the strong lexicalization that characterizes CCG, it is able to handle non-local dependencies in a simple and effective way (Rimell, Clark, and Steedman 2009). After the release of annotated data sets (Hockenmaier and Steedman 2007), there has been a surge of interest in CCG within statistical and, more recently, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhan"
J18-3004,P92-1012,1,0.392571,"Missing"
J18-3004,J15-2002,1,0.890049,"Missing"
J18-3004,Q14-1032,1,0.843929,"orithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in contrast with the situation for TAG, where the runtime is (roughly) quadratic with respect to grammar size (Schabes 1990). The only other polynomial-time parsing algorithms for CCG that we are aware of (Vijay-Shanker and Weir 1993; Kuhlmann and Satta 2014) exhibit the same behavior. Kuhlmann and Satta (2014) ask whether parsing may be inherently more complex for CCG than for TAG when grammar size is taken into account. Our main technical result in this article is that the answer to this question is positive: We show that any parsing algorithm for CCG in the formalism considered by Vijay-Shanker and Weir will necessarily take in the worst case exponential time when the size of the grammar is included in the analysis. Formally, we prove that the universal recognition problem for this formalism is EXPTIME-complete. The following paragraphs provide"
J18-3004,D16-1262,0,0.0351114,"Missing"
J18-3004,D13-1064,0,0.0198982,"Missing"
J18-3004,P87-1016,0,0.777533,"Missing"
J18-3004,D09-1085,0,0.052595,"Missing"
J18-3004,P86-1006,0,0.376979,"mmar assigns to a generated sentence, not in the membership of the sentence per se. Therefore, the universal recognition problem is a more accurate model of parsing than the membership problem, as the latter also admits decision procedures where the grammar is replaced with some other mechanism that may produce no or completely different descriptions than the ones we are interested in. The universal recognition problem is also favored when the ambition is to characterize parsing time in terms of all relevant inputs—both the length of the input string and the size and structure of the grammar (Ristad 1986). Such an analysis often reveals (and does so even in this article) how specific features of the grammar contribute to the complexity of the parsing task. More precisely, when investigating the universal recognition problem one expresses the computational complexity of parsing in terms of several parameters (other than the input string length), as for instance the number of nonterminals, maximum size of rules, or maximum length of unary derivations. This provides a much more finegrained picture than the one that we obtain when analyzing the membership problem, and discloses the effects that ea"
J18-3004,P85-1011,0,0.783263,"Missing"
J18-3004,P90-1001,0,0.728604,"ommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in cont"
J18-3004,J93-4002,0,0.5991,"exity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in contrast with the situation for TAG, where the runtime is (roughly) quadratic with respect to grammar size (Schabes 1990). The only other polynomial-time parsing algorithms for CCG that we are aware of (Vijay-Shanker and Weir 1993; Kuhlmann and Satta 2014) exhibit the same behavior. Kuhlmann and Satta (2014) ask whether parsing may be inherently more complex for CCG than for TAG when grammar size is taken into account. Our main technical result in this article is that the answer to this question is positive: We show that any parsing algorithm for CCG in the formalism considered by Vijay-Shanker and Weir will necessarily take in the worst case exponential time when the size of the grammar is included in the analysis. Formally, we prove that the universal recognition problem for this formalism is EXPTIME-complete. The fo"
J18-3004,P87-1015,0,0.822758,"Missing"
J18-3004,P88-1034,0,0.625578,"man 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result that CCG and TAG are weakly equivalent (Weir and Joshi 1988; Vijay-Shanker and Weir 1994). However, although the runtime of Vijay-Shanker and Weir’s algorithm is polynomial in the length of the input sentence, it is exponential in the size of the grammar. This is in contrast with the situation for TAG, where the runtime is (roughly) quadratic with respect to grammar size (Schabes 1990). The only other polynomial-time parsing algorithms for CCG that we are aware of (Vijay-Shanker and Weir 1993; Kuhlmann and Satta 2014) exhibit the same behavior. Kuhlmann and Satta (2014) ask whether parsing may be inherently more complex for CCG than for TAG when gramm"
J18-3004,J10-2001,0,0.0286866,"Missing"
J18-3004,P11-1069,0,0.0220947,"007), there has been a surge of interest in CCG within statistical and, more recently, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjo"
J18-3004,J15-3005,0,0.0229674,"tly, neural natural Submission received: 21 February 2017; revised version received: 4 May 2018; accepted for publication: 18 May 2018. doi:10.1162/COLI_a_00324 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 language processing. The wide range of applications for which CCG has been used includes data-driven syntactic parsing (Clark and Curran 2007; Zhang and Clark 2011), natural language generation (White, Clark, and Moore 2010; Zhang and Clark 2015), machine translation (Lewis and Steedman 2013), and broad-coverage semantic parsing (Lewis and Steedman 2014; Lee, Lewis, and Zettlemoyer 2016). In this article we study the parsing complexity of CCG. Our point of departure is the work of Vijay-Shanker and Weir (1990), who presented the first polynomial-time parsing algorithm for CCG. The runtime complexity of this algorithm is in O(n6 ), where n is the length of the input sentence. This matches the runtime complexity of standard parsing algorithms for Tree Adjoining Grammar (TAG; Schabes 1990), which fits nicely with the celebrated result th"
J18-3004,D14-1107,0,\N,Missing
J19-2005,W13-2322,0,0.0236108,"ho introduce a number of heuristics to find tree decompositions of semantic graphs of natural language sentences. They extract corresponding HRG rules and analyze the characteristics of the resulting grammars. Unlike the methods we present in this article, the methods of Jones, Goldwater, and Johnson do not refer to the natural language string corresponding to the semantic graph, and therefore are not applicable to string-to-graph parsing. In this article, we experiment with semantic data sets annotated according to the linguistic formalism called Abstract Meaning Representation, described by Banarescu et al. (2013). A general discussion of alternative formalisms and linguistic graph banks can be found in Kuhlmann and Oepen (2016). 2. Tree Decompositions and Parse Trees The graphs that we use in this article have directed arcs, because this is a standard requirement for semantic representation of natural language sentences. We denote a directed graph as G = (V, E), where V is the set of vertices and E is the set of edges—that is, ordered pairs of the form (v, u) with v, u ∈ V. A tree decomposition of G is a special tree where each node is associated with a subset of V. Because a tree is a particular kind"
J19-2005,P13-1091,0,0.027257,"luated on the task of extracting semantic dependency graphs from text. Considering the already mentioned graph parsing problem, where the input consists of a graph and a graph-rewriting grammar, one of the first algorithms for parsing based on HRG has been proposed by Lautemann (1990). For general HRGs, this algorithm runs in exponential time. This complexity result comes as no surprise, since it is known that graph parsing for HRG is an NP-hard problem, even for fixed grammars (Aalbersberg, Rozenberg, and Ehrenfeucht 1986; Lange and Welzl 1987). In the context of natural language processing, Chiang et al. (2013) proposed an optimized version of Lautemann, also providing a fine-grained complexity analysis that is missing in the original article. The running time of the optimized algorithm is an exponential function of both the treewidth of the input graph (to be defined in Section 2) and of the maximum degree of its nodes. Polynomial time parsing algorithms for subclasses of HRG have also been investigated in the graph grammar literature. A predictive top–down parsing algorithm has been presented by Drewes, Hoffmann, and Minas (2015), inspired by the LL(1) parsing method for CFGs, and working for a re"
J19-2005,P96-1025,0,0.0429396,", and h in a partial analysis can range from 0 to n, the length of w, for a total number of partial analyses in O(n3 ). Furthermore, the left/right tree attachment operations can be implemented in constant time for fixed values of i, k, j, h, and h0 . Then the overall time used by the algorithm when combining trees is in O(n5 ). We conclude that our parsing algorithm runs in polynomial time and space. What we have briefly summarized here is a well-known adaptation of the standard Cocke-Kasami-Younger (CKY) algorithm for parsing using CFGs (Aho and Ullman 1972), as applied to lexicalized CFGs (Collins 1996) and to dependency grammars (Eisner 1996). Let us now turn to string-to-graph parsing. When applying dynamic programming, we can exploit ideas similar to those presented for the string-to-tree case, but we need to make some generalizations. The input to the algorithm is now a sequence of h (a) j i [i, j; h] h h h0 ⇒ (b) i k [i, k; h0 ] j k [k, j; h] j i [i, j; h] Figure 1 Graphical representation for (a) the partial analysis [i, j; h] and (b) the combination of partial analyses [i, k; h0 ] and [k, j; h], implementing left attachment and producing partial analysis [i, j; h]. 342 Gildea, Satta,"
J19-2005,E17-1051,1,0.89149,"Missing"
J19-2005,C96-1058,0,0.0189606,"om 0 to n, the length of w, for a total number of partial analyses in O(n3 ). Furthermore, the left/right tree attachment operations can be implemented in constant time for fixed values of i, k, j, h, and h0 . Then the overall time used by the algorithm when combining trees is in O(n5 ). We conclude that our parsing algorithm runs in polynomial time and space. What we have briefly summarized here is a well-known adaptation of the standard Cocke-Kasami-Younger (CKY) algorithm for parsing using CFGs (Aho and Ullman 1972), as applied to lexicalized CFGs (Collins 1996) and to dependency grammars (Eisner 1996). Let us now turn to string-to-graph parsing. When applying dynamic programming, we can exploit ideas similar to those presented for the string-to-tree case, but we need to make some generalizations. The input to the algorithm is now a sequence of h (a) j i [i, j; h] h h h0 ⇒ (b) i k [i, k; h0 ] j k [k, j; h] j i [i, j; h] Figure 1 Graphical representation for (a) the partial analysis [i, j; h] and (b) the combination of partial analyses [i, k; h0 ] and [k, j; h], implementing left attachment and producing partial analysis [i, j; h]. 342 Gildea, Satta, and Peng Ordered Tree Decomposition for H"
J19-2005,P14-1134,0,0.02064,"tices at the left and right boundaries of the span of the subgraph itself. This condition rules out crossing arcs and is only possible because of the projective restriction on the processed graphs. If we ignore the fact that Kuhlmann and Johnsson and Schluter use a grammarless approach, the parsing algorithm we present in this article can be viewed as a generalization of the work by those authors, since we relax the condition d = 2 and we do not impose any restriction on the position of the attachment vertices. Other grammarless algorithms for dependency semantic parsing have been reported by Flanigan et al. (2014), who use maximum weight spanning techniques, and by Damonte, Cohen, and Satta (2017) and Gildea, Satta, and Peng (2018), who use special transition systems combined with greedy methods. 347 Computational Linguistics Volume 45, Number 2 The connection between tree decomposition of a graph and HRG rules, which we use in this article, was first made by Lautemann (1988). In the context of natural language processing, the same idea has been previously exploited by Jones, Goldwater, and Johnson (2013), who introduce a number of heuristics to find tree decompositions of semantic graphs of natural la"
J19-2005,J18-1004,1,0.801701,"Missing"
J19-2005,J76-4004,0,0.590919,"s investigating string-to-graph parsing. This article provides a contribution in this direction. 340 Gildea, Satta, and Peng Ordered Tree Decomposition for HRG Rule Extraction String-to-tree parsing is rather well understood. If we assume that when generating a string our trees do not have crossing arcs,1 then string-to-tree parsing can be efficiently solved using dynamic programming algorithms, also called chart-based methods or tabular methods in the natural language processing community. For an introduction to dynamic programming methods for string-to-tree parsing, we refer the reader to ¨ Graham and Harrison (1976) and Nederhof and Satta (2004) for CFGs, and to Kubler, McDonald, and Nivre (2009, Chapter 5) for projective dependency grammars. Dynamic programming is perhaps the most general way to attack string-to-tree parsing, and other alternative methods such as greedy algorithms or beam search algorithms can be derived as special cases of dynamic programming, as discussed by Huang and Sagae ´ (2010) and Kuhlmann, Gomez-Rodr´ ıguez, and Satta (2011). In most cases of interest, dynamic programming provides polynomial space and time solutions for non-crossing string-to-tree parsing as well as for unsuper"
J19-2005,P10-1110,0,0.0878263,"Missing"
J19-2005,P16-1025,0,0.0276898,"emantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-driven phrase structure grammars in the era when grammars were generally manually developed rather than learned from data. Crucially, semantic representations are graph structures, as opposed to tree structures. There is therefore a need for formal grammars that generate graphs. Graph grammars have been investigated in the formal l"
J19-2005,C12-1083,0,0.222335,"cal natural language processing, semantic analysis had been mainly investigated by means of separated tasks (such as named entity recognition, semantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-driven phrase structure grammars in the era when grammars were generally manually developed rather than learned from data. Crucially, semantic representations are graph structures, as oppose"
J19-2005,W13-1810,0,0.0603293,"Missing"
J19-2005,P04-1061,0,0.0294878,"Missing"
J19-2005,P11-1068,1,0.813397,"Missing"
J19-2005,Q15-1040,0,0.0219803,"g, meaning that in the definition of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term projective is a generalization to graphs of the same concept as used in dependency trees: Informally, a projective graph does not have two arcs that cross each other, with respect to the ordering in w. The algorithm is based on the already mentioned CKY algorithm for text-to-string parsing under a CFG (Aho and Ullman 1972). It explores proj"
J19-2005,J16-4009,0,0.0786897,"tention toward semantic analysis of sentences with the conviction that, along with syntactic information, semantic information will greatly improve end-user applications. Whereas, in statistical natural language processing, semantic analysis had been mainly investigated by means of separated tasks (such as named entity recognition, semantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-drive"
J19-2005,N15-1114,0,0.0672862,"Missing"
J19-2005,S16-1166,0,0.037782,"Missing"
J19-2005,S15-2153,0,0.0288086,"The weight of a graph G is then computed by summing up the weight of each occurrence of a pattern in G, where different occurrences might overlap in case of patterns of order higher than 1. In this respect, we may therefore view semantic dependency parsing as the grammarless version of string-to-graph parsing, meaning that in the definition of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term projective is a generaliz"
J19-2005,S14-2008,0,0.0247619,"d a k-th order pattern. The weight of a graph G is then computed by summing up the weight of each occurrence of a pattern in G, where different occurrences might overlap in case of patterns of order higher than 1. In this respect, we may therefore view semantic dependency parsing as the grammarless version of string-to-graph parsing, meaning that in the definition of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term p"
J19-2005,K15-1004,1,0.892111,"Missing"
J19-2005,P18-1171,1,0.86951,"Missing"
J19-2005,D14-1048,0,0.046467,"Missing"
J19-2005,W17-2315,0,0.0284204,"Missing"
J19-2005,S15-1031,0,0.0233415,"n of the former problem the search space need not be defined by a generative grammar. Dependency semantic parsing has been a prominent shared task at recent editions of the International Workshops on Semantic Evaluation (SemEval). See Oepen et al. (2014) and Oepen et al. (2015) for a quick overview of the different approaches that have been evaluated. Two grammarless algorithms for dependency semantic parsing are worth discussing here, because they are related to the dynamic programming approach that we use in Section 5 to solve string-to-graph parsing for HRG. Kuhlmann and Jonsson (2015) and Schluter (2015) have independently derived essentially the same algorithm for finding a maximum weight projective directed acyclic graph, given an input vertex sequence w and using a first-order model (a model using only first order patterns). The term projective is a generalization to graphs of the same concept as used in dependency trees: Informally, a projective graph does not have two arcs that cross each other, with respect to the ordering in w. The algorithm is based on the already mentioned CKY algorithm for text-to-string parsing under a CFG (Aho and Ullman 1972). It explores projective graphs whose"
J19-2005,D16-1112,0,0.0210111,"ks (such as named entity recognition, semantic role labeling, and co-reference resolution), we now witness the attempt to incorporate all these different aspects into a single process of semantic parsing. This trend is attested to by the wide variety of semantic representations that are being proposed (Kuhlmann and Oepen 2016) and by the number of semantically annotated data sets being produced and distributed. As a follow-up, semantic parsing is now being exploited in machine translation (Jones et al. 2012), text summarization (Liu et al. 2015; Dohare and Karnick 2017), sentence compression (Takase et al. 2016), event extraction (Huang et al. 2016; Rao et al. 2017; Wang et al. 2017), and question answering (Khashabi et al. 2018). It seems therefore that the field is returning to the task of semantic parsing after a long hiatus, since semantic analysis was already part of the agenda in formalisms such as head-driven phrase structure grammars in the era when grammars were generally manually developed rather than learned from data. Crucially, semantic representations are graph structures, as opposed to tree structures. There is therefore a need for formal grammars that generate graphs. Graph grammars h"
J19-2005,P05-1073,0,0.0127583,"Missing"
J92-3011,P89-1018,0,0.156445,"w comparisons only indirectly related to average-case time complexity. The first measure depends on how different methods are implemented. The latter is very weakly related to time, because an edge construction depends on a number of tests that can grow with the grammar and the input string length, and also since a reduction operation associated with a nonterminal shift cannot be considered an elementary operation as well. The reader should therefore be careful in the interpretation of the results, since these kinds of experiments require a uniform framework to be carried out (see for example Billot and Lang 1989). The reader should also be warned that the bottom-up and the bidirectional methods defined in this chapter involve polynomial computations of unbounded degree, but methods are found in the literature that perform much better, still using general context-free grammars. The contribution by Johnson, entitled ""The computational complexity of GLR parsing,"" discusses two issues regarding the worst-case computational complexity of 377 Computational Linguistics Volume 18, Number 3 Tomita's algorithm. Johnson points out that a crude representation of a packed parse forest can lead the algorithm to use"
J92-3011,J91-3004,0,0.023365,"prefix of the input sentence analyzed so far. On the basis of the proposed framework, the authors discuss an application in uncertain input parsing. This chapter assumes considerable confidence with stochastic context-free grammars on the part of the reader. As a technical note, I observe that the authors compute the probability pBB of all possible left-recursive derivations B ~ Bw, B a nonterminal and w a terminal string, as Y~wPr(B ~ Bw). But events B ~ Bw are not mutually disjointed, and this summation is no longer a probability (such a quantity corresponds to quantity QL(B ~ B) studied in Jelinek and Lafferty 1991). Probabilities PBB for every nonterminal B can be correctly computed by solving a system of linear equations. The last three chapters discuss applications of Tomita's algorithm to cases of corrupted input. The contribution by Malone and Felshin, entitled ""GLR parsing for erroneous input,"" describes a system developed for use by language learners. The system is based on Tomita's algorithm, and is able to parse in the presence of ill-formed input of various kinds. Errors are grouped in different categories, and techniques to handle them are discussed along with the use of a scoring method. Amon"
J92-3011,P89-1017,0,0.0358683,"bilinear covers for the input grammar). The second point in this chapter is a demonstration that there exist context-free grammars G whose collection of LR(0) items is exponentially larger than IGI. Johnson exhibits input strings such that all these items are exploited by the algorithm, forcing exponential running time. Although these worst cases may not be relevant for natural language processing applications, it is interesting to note that nondeterministic LR automata are found in the literature that use a set of states always proportional to IGI--for example the LL/LR automaton proposed by Leermakers (1989); see also Schabes (1991) for computational complexity issues. The contribution by Kipps entitled ""GLR parsing in time O(n3)"" is also based on the above observation about the number of different matchings of immediate constituent boundaries. As a consequence, Kipps shows that in the worst case, an exponential amount of time with respect to the grammar length may be required by reduction operations in the original version of Tomita's algorithm. The author redesigns the reduce procedure of the algorithm using a tabular technique, thereby solving the problem in an efficient way. As a minor note,"
J92-3011,E91-1012,0,0.0476565,"Missing"
J92-3011,P91-1014,0,0.144149,"t grammar). The second point in this chapter is a demonstration that there exist context-free grammars G whose collection of LR(0) items is exponentially larger than IGI. Johnson exhibits input strings such that all these items are exploited by the algorithm, forcing exponential running time. Although these worst cases may not be relevant for natural language processing applications, it is interesting to note that nondeterministic LR automata are found in the literature that use a set of states always proportional to IGI--for example the LL/LR automaton proposed by Leermakers (1989); see also Schabes (1991) for computational complexity issues. The contribution by Kipps entitled ""GLR parsing in time O(n3)"" is also based on the above observation about the number of different matchings of immediate constituent boundaries. As a consequence, Kipps shows that in the worst case, an exponential amount of time with respect to the grammar length may be required by reduction operations in the original version of Tomita's algorithm. The author redesigns the reduce procedure of the algorithm using a tabular technique, thereby solving the problem in an efficient way. As a minor note, in the discussion of Earl"
J94-2002,E91-1006,1,0.842959,"at: m CiJ ~- V aik /~ bkj, k~l 1 <_ i,j <_ m. (1) An instance of the Boolean matrix multiplication problem is therefore a pair (A, B) and the solution to such an instance consists of the matrix C such that C = A x B. In what follows BMM will denote the set of all possible instances of the Boolean matrix multiplication problem. 2.2 Tree-Adjoining Grammars The definition of TAG and the associated notion of derivation are briefly introduced in the following; the reader is also referred to the standard literature (see, for instance, Vijay-Shanker and Joshi [1985] or Joshi, Vijay-Shanker, and Weir [1991]). A tree-adjoining grammar is a tree rewriting system denoted by a tuple G = (VN, VT, S, I, A), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols respectively, S E VN is a distinguished symbol, and I and A are finite sets of elementary trees. Trees in ! and A are called initial and auxiliary trees respectively and meet the following specifications. Internal (nonleaf) nodes in an elementary tree 174 Giorgio Satta Tree-Adjoining Grammar Parsing auxiliarytro~ initialtree S A d <. ,4 d terminalnodes (a) adjunction > ,4 (b) Figure 1 Definitions of (a) initial and auxili"
J94-2002,1991.iwpt-1.4,0,0.0377206,"arse forest--one that needs a time-expensive process for parse tree retrieval. 1 More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem is defined to be any pair (G, w), and the unique"
J94-2002,P88-1032,0,0.142167,"considered as a highly compressed representation of the parse forest--one that needs a time-expensive process for parse tree retrieval. 1 More explicit representations offer the advantage of time-efficient retrieval of parse trees, at the cost of an increase in storage resources. In practice, most commonly used algorithms solve the parsing problem for TAGs by computing a superset of a parse relation (defined as above) and by representing it in such a way that its instances can be tested in constant time; such a condition is satisfied by the methods reported in Vijay-Shanker and Joshi (1985), Schabes and Joshi (1988), Palis, Schende, and Wei (1990), Schabes (1991), Lavelli and Satta (1991), Lang (1992), and Vijay-Shanker and Weir (1993). From such a representation, time-efficient computations can be used later to retrieve parse structures of the input string. On the basis of the previous observation, we assume in the following that the solution of the parsing problem involves (at least) the computation of a representation for Rp such that its instances can be tested in constant time: we base our results on such an assumption. More precisely, an input instance of the tree-adjoining grammar parsing problem"
J94-2002,P85-1011,0,0.155845,"tten A x B, is a Boolean matrix C such that: m CiJ ~- V aik /~ bkj, k~l 1 <_ i,j <_ m. (1) An instance of the Boolean matrix multiplication problem is therefore a pair (A, B) and the solution to such an instance consists of the matrix C such that C = A x B. In what follows BMM will denote the set of all possible instances of the Boolean matrix multiplication problem. 2.2 Tree-Adjoining Grammars The definition of TAG and the associated notion of derivation are briefly introduced in the following; the reader is also referred to the standard literature (see, for instance, Vijay-Shanker and Joshi [1985] or Joshi, Vijay-Shanker, and Weir [1991]). A tree-adjoining grammar is a tree rewriting system denoted by a tuple G = (VN, VT, S, I, A), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols respectively, S E VN is a distinguished symbol, and I and A are finite sets of elementary trees. Trees in ! and A are called initial and auxiliary trees respectively and meet the following specifications. Internal (nonleaf) nodes in an elementary tree 174 Giorgio Satta Tree-Adjoining Grammar Parsing auxiliarytro~ initialtree S A d <. ,4 d terminalnodes (a) adjunction > ,4 (b) Figur"
J94-2002,H86-1020,0,\N,Missing
J94-2002,E93-1045,0,\N,Missing
J98-1007,J95-4003,0,0.0371695,"Missing"
J98-2006,J94-1003,0,0.00968644,"e transducer to the input of the next, a process that can be compiled out into a single transducer (Kaplan and Kay 1994). 1 Using this insight, a vast majority of computational implementations of phonological rule systems have been done using finite-state transducers or extensions thereof (Sproat 1992). Recently, there has been a shift in much of the work on phonological theory, from systems of rules to sets of well-formedness constraints (Paradis 1988, Scobbie 1991, Prince and Smolensky 1993, Burzio 1994). This shift has, however, had relatively little impact upon computational work (but see Bird and Ellison 1994). In this paper, we begin an examination of the effects of the move from rule-based to constraint-based theories upon the generative properties of phonological theories. Specifically, we will focus our efforts on the issue of whether the widely adopted constraint-based view known as Optimality Theory (OT) may be instantiated in a finite-state transducer. 2 OT * Department of Cognitive Science, 3400 N. Charles Street, Baltimore, MD 21218. E-mail: rfrank@cogsci.jhu.edu. This author is also affiliated with the Center for Language and Speech Processing, Johns Hopkins University. f Dipartimento di"
J98-2006,C94-2163,0,0.401882,"cer to the input of the next, a process that can be compiled out into a single transducer (Kaplan and Kay 1994). 1 Using this insight, a vast majority of computational implementations of phonological rule systems have been done using finite-state transducers or extensions thereof (Sproat 1992). Recently, there has been a shift in much of the work on phonological theory, from systems of rules to sets of well-formedness constraints (Paradis 1988, Scobbie 1991, Prince and Smolensky 1993, Burzio 1994). This shift has, however, had relatively little impact upon computational work (but see Bird and Ellison 1994). In this paper, we begin an examination of the effects of the move from rule-based to constraint-based theories upon the generative properties of phonological theories. Specifically, we will focus our efforts on the issue of whether the widely adopted constraint-based view known as Optimality Theory (OT) may be instantiated in a finite-state transducer. 2 OT * Department of Cognitive Science, 3400 N. Charles Street, Baltimore, MD 21218. E-mail: rfrank@cogsci.jhu.edu. This author is also affiliated with the Center for Language and Speech Processing, Johns Hopkins University. f Dipartimento di"
J98-2006,P84-1038,0,0.126963,"3400 N. Charles Street, Baltimore, MD 21218. E-mail: rfrank@cogsci.jhu.edu. This author is also affiliated with the Center for Language and Speech Processing, Johns Hopkins University. f Dipartimento di Elettronica ed Informatica, Via Gradenigo 6/a, 1-35131 Padova, Italy. E-mail: satta@dei.unipd.it. Part of the present research was done while this author was visiting the Center for Language and Speech Processing, Johns Hopkins University. 1 An alternative to composition o f transducers involves running multiple rule transducers in parallel, producing so-called two-level phonological systems (Koskenniemi 1984). See Barton, Berwick, and Ristad (1987) for discussion of space and time complexity issues. 2 We are aware of two papers that study related matters. Ellison (1994) addresses the question of Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 raises a particularly interesting theoretical question in this context: it allows the specification of a ranking a m o n g the constraints and allows lower-ranked constraints to be violated in order for higher-ranked constraints to be satisfied. This violability p r o p e r t y means that certain well-known compu"
J98-2006,J94-3001,0,\N,Missing
J98-2006,J94-3010,0,\N,Missing
J98-2006,W83-0114,0,\N,Missing
N06-1043,P01-1017,0,0.0161409,"w that the result also holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that • the cross-entropy between the unrestricted tree distribution given as input and the tree distribution induced by the estimated probabilistic context-free grammar; and • the derivational entropy of the estimated probabilistic context-free grammar. These two quantities are usually unrelated. We show that these two quantities take the same value when the probabilistic context-free grammar is trained using the minimal cross-entropy criterion. We then translate back this prop"
N06-1043,P98-1035,0,0.0370611,"d the grammar itself. We show that the result also holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that • the cross-entropy between the unrestricted tree distribution given as input and the tree distribution induced by the estimated probabilistic context-free grammar; and • the derivational entropy of the estimated probabilistic context-free grammar. These two quantities are usually unrelated. We show that these two quantities take the same value when the probabilistic context-free grammar is trained using the minimal cross-entropy criterion. We then transl"
N06-1043,J98-2005,0,0.301629,"A → α a probability specified as the ratio between the expected number of A → α and the expected number of A, under the distribution pT . We remark here that the minimization of the cross-entropy above is equivalent to the minimization of the Kullback-Leibler distance between pT and pG , viewed as tree distributions. Also, note that the likelihood of an infinite set of derivations would always be zero and therefore cannot be considered here. To be used in the next section, we now show that the PCFG G obtained as above is consistent. The line of our argument below follows a proof provided in (Chi and Geman, 1998) for the maximum likelihood estimator based on finite tree distributions. Without loss of generality, we assume that in G the start symbol S is never used in the right-hand side of a rule. For each A ∈ N , let qA be the probability that a derivation in G rooted in A fails to terminate. We can then write X qA ≤ qB · X pG (A → α)f (B, α).(11) α B∈N The inequality follows from the fact that the events considered in the right-hand side of (11) are not mutually exclusive. Combining (10) and (11) we obtain qA · EpT f (A, t) ≤ ≤ X qB · B∈N X EpT f (A → α, t)f (B, α). α where fc (B, t) indicates the n"
N06-1043,J99-1004,0,0.86385,"es, 1992). Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions. This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar. The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution. In (Chi, 1999) an estimator that maximizes the likelihood of a probability distribution defined over a finite set of trees is introduced, as a generalization of the maximum likelihood estimator. Again, the problems we consider here can be thought of as generalizations of such estimator to the case of distributions over infinite sets of trees or sentences. The remainder of this paper is structured as follows. Section 2 introduces the basic notation and definitions and Section 3 discusses our new estimation method. Section 4 presents our main result, which is transferred in Section 5 to the method of maximum"
N06-1043,J03-4003,0,0.0877966,"elihood estimator on (finite) tree banks. We prove an unexpected theoretical property of grammars that are trained in this way, namely, we show that the derivational entropy of the grammar takes the same value as the crossentropy between the input distribution and the grammar itself. We show that the result also holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that • the cross-entropy between the unrestricted tree distribution given as input and the tree distribution induced by the estimated probabilistic context-free grammar; and • the derivation"
N06-1043,C04-1011,1,0.681698,"rs at the root of the trees in T (G). Then it is easy to see that, for every A 6= S, we have EpT fc (A, t) = EpT f (A, t), while EpT fc (S, t) = 0 and EpT f (S, t) = 1. Using these relations in (12) we obtain qS · EpT f (S, T ) ≤ qS · EpT fc (S, T ), from which we conclude qS = 0, thus implying the consistency of G. 4 Cross-entropy and derivational entropy In this section we present the main result of the paper. We show that, when G = (G, pG ) is estimated by minimizing the cross-entropy in (5), then such cross-entropy takes the same value as the derivational entropy of G, defined in (3). In (Nederhof and Satta, 2004) relations are derived for the exact computation of Hd (pG ). For later use, we report these relations below, under the assumption that G is consistent (see Section 3). We have Hd (pG ) = X out G (A) · HA (pG ). (13) A∈N Quantities HA (pG ), A ∈ N , have been defined in (4). For each A ∈ N , quantity out G (A) is the sum of the probabilities of all trees generated by G, having root labeled by S and having a yield composed of terminal symbols with an unexpanded occurrence of nonterminal A. Again, we assume that symbol S does not appear in any of the right-hand sides of the rules in R. This mean"
N06-1043,J05-2002,0,0.392117,"ihood estimation. Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, 335 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 335–342, c New York, June 2006. 2006 Association for Computational Linguistics as for instance probabilistic tree adjoining grammars (Schabes, 1992). Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions. This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar. The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution. In (Chi, 1999) an estimator that maximizes the likelihood of a probability distribution defined over a finite set of trees is introduced, as a generalization of the maximum likelih"
N06-1043,C92-2066,0,0.0727749,"when the probabilistic context-free grammar is trained using the minimal cross-entropy criterion. We then translate back this property to the method of maximum likelihood estimation. Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, 335 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 335–342, c New York, June 2006. 2006 Association for Computational Linguistics as for instance probabilistic tree adjoining grammars (Schabes, 1992). Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions. This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar. The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution. In (Chi, 1999"
N06-1044,P99-1070,0,0.031294,"btain pG (A → α) = P d∈D pD (d) · f (A → α, d) P = d∈D pD (d) · f (A, d) = = P f (d,D) |D |· f (A → α, d) P f (d,D) d∈D |D |· f (A, d) d∈D P f (d, D) · f (A → α, d) . d∈D f (d, D) · f (A, d) d∈D P (14) 346 This is the supervised MLE estimator in (7). This reminds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG distribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. 4 Renormalization In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = (G, pG ), with G = (N, Σ, S, R). We define the renormalization of G as the PCFG R(G) = (G, pR ) with pR specified by pR (A → α) = P d d,w pG (α ⇒ w) d,w pG (A ⇒ w) pG (A → α) · P d . (15) It is not difficult to see that R(G) is a proper PCFG. We now show an important property of R(G), discussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free"
N06-1044,J98-2005,0,0.245306,"rty for a probabilistic context-free grammar is that it be consistent, that is, the grammar should assign probability of one to the set of all finite strings or parse trees that it generates. In other words, the grammar should not lose probability mass with strings or trees of infinite length. Several methods for the empirical estimation of probabilistic context-free grammars have been proposed in the literature, based on the optimization of some function on the probabilities of the observed data, such as the maximization of the likelihood of In later work by (S´anchez and Bened´ı, 1997) and (Chi and Geman, 1998), the result was independently extended to expectation maximization, which is an unsupervised method exploited to estimate probabilistic context-free grammars by finding local maxima of the likelihood of a sample of unannotated sentences. The proof in (S´anchez and Bened´ı, 1997) makes use of spectral analysis of expectation matrices, while the proof in (Chi and Geman, 1998) is based on a simpler counting argument. Both these proofs assume restrictions on the underlying context-free grammars. More specifically, in (Chi and Geman, 1998) empty rules and unary rules are not allowed, thus excludin"
N06-1044,J99-1004,0,0.103634,"∈D pD (d) · f (A → α, d) P = d∈D pD (d) · f (A, d) = = P f (d,D) |D |· f (A → α, d) P f (d,D) d∈D |D |· f (A, d) d∈D P f (d, D) · f (A → α, d) . d∈D f (d, D) · f (A, d) d∈D P (14) 346 This is the supervised MLE estimator in (7). This reminds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG distribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. 4 Renormalization In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = (G, pG ), with G = (N, Σ, S, R). We define the renormalization of G as the PCFG R(G) = (G, pR ) with pR specified by pR (A → α) = P d d,w pG (α ⇒ w) d,w pG (A ⇒ w) pG (A → α) · P d . (15) It is not difficult to see that R(G) is a proper PCFG. We now show an important property of R(G), discussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free grammars. d"
N06-1044,N06-1043,1,0.910503,"an iterative algorithm called inside/outside (Charniak, 1993), which implements the expectation maximization (EM) method (Dempster et al., 1977). Starting with an initial function pG that probabilistically extends G, a so-called growth transformation is computed, defined as f (w, C)· Following (Baum, 1972), one can show that pG (C) ≥ pG (C). Thus, by iterating the growth transformation above, we are guaranteed to reach a local maximum for (8), or possibly a saddle point. We refer to this as the unsupervised MLE method. We now discuss a third estimation method for PCFGs, which was proposed in (Corazza and Satta, 2006). This method can be viewed as a generalization of the supervised MLE method to probability distributions defined over infinite sets of complete derivations. Let D be an infinite set of complete derivations using nonterminal symbols in N , start symbol S ∈ N and terminal symbols in Σ. We assume that the set of rules that are observed in D is drawn from some finite set R. Let pD be a probability distribution defined over D, that is, a function from set D to interval [0, 1] such that P d∈D pD (d) = 1. Consider the CFG G = (N, Σ, R, S). Note that D ⊆ D(G). We wish to extend G to some PCFG G = (G,"
N06-1044,W03-3016,1,0.838784,"→ α, d) P = d∈D pD (d) · f (A, d) = = P f (d,D) |D |· f (A → α, d) P f (d,D) d∈D |D |· f (A, d) d∈D P f (d, D) · f (A → α, d) . d∈D f (d, D) · f (A, d) d∈D P (14) 346 This is the supervised MLE estimator in (7). This reminds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG distribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. 4 Renormalization In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = (G, pG ), with G = (N, Σ, S, R). We define the renormalization of G as the PCFG R(G) = (G, pR ) with pR specified by pR (A → α) = P d d,w pG (α ⇒ w) d,w pG (A ⇒ w) pG (A → α) · P d . (15) It is not difficult to see that R(G) is a proper PCFG. We now show an important property of R(G), discussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free grammars. d Lemma 1 For each derivation d w"
N06-1044,P98-2190,0,0.0227201,"e used a novel proof technique that exploits an already known construction for the renormalization of probabilistic contextfree grammars. Our proof technique seems more intuitive than arguments previously used in the literature to prove the consistency property, based on counting arguments or on spectral analysis. It is not difficult to see that our proof technique can also be used with probabilistic rewriting formalisms whose underlying derivations can be characterized by means of context-free rewriting. This is for instance the case with probabilistic tree-adjoining grammars (Schabes, 1992; Sarkar, 1998), for which consistency results have not yet been shown in the literature. A f (y(d),C) (d) · pGpG(y(d)) ·f (A → α, d) d∈D(C) |C| P f (y(d),C) pG (d) · pG (y(d)) ·f (A, d) d∈D(C) |C| P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A → α, d) P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A, d) P estimator (10) already discussed in Section 3. In fact, for each w ∈ L(G) and d ∈ D(G), we have (d) pG (d |w) = ppGG(w) . We conclude with the desired result, namely that a general form PCFG obtained at any iteration of the EM method for the unsupervised MLE is always consistent. Cross-entropy minimization In or"
N06-1044,C92-2066,0,0.0526242,"results, we have used a novel proof technique that exploits an already known construction for the renormalization of probabilistic contextfree grammars. Our proof technique seems more intuitive than arguments previously used in the literature to prove the consistency property, based on counting arguments or on spectral analysis. It is not difficult to see that our proof technique can also be used with probabilistic rewriting formalisms whose underlying derivations can be characterized by means of context-free rewriting. This is for instance the case with probabilistic tree-adjoining grammars (Schabes, 1992; Sarkar, 1998), for which consistency results have not yet been shown in the literature. A f (y(d),C) (d) · pGpG(y(d)) ·f (A → α, d) d∈D(C) |C| P f (y(d),C) pG (d) · pG (y(d)) ·f (A, d) d∈D(C) |C| P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A → α, d) P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A, d) P estimator (10) already discussed in Section 3. In fact, for each w ∈ L(G) and d ∈ D(G), we have (d) pG (d |w) = ppGG(w) . We conclude with the desired result, namely that a general form PCFG obtained at any iteration of the EM method for the unsupervised MLE is always consistent. Cross-entropy min"
N06-1044,C98-2185,0,\N,Missing
N09-1061,N07-1019,0,0.20138,"Missing"
N09-1061,E09-1034,1,0.84924,"Missing"
N09-1061,C92-2066,0,0.278857,"valent to LCFRS that has been introduced for syntax-based machine translation. However, the grammar produced by our algorithm has optimal (minimal) fan-out. This is an important improvement over the result in (Melamed et al., 2004), as this quantity enters into the parsing complexity of both multitext grammars and LCFRS as an exponential factor, and therefore must be kept as low as possible to ensure practically viable parsing. Rank reduction is also investigated in Nesson et al. (2008) for synchronous tree-adjoining grammars, a synchronous rewriting formalism based on tree-adjoining grammars Joshi and Schabes (1992). In this case the search space of possible reductions is strongly restricted by the tree structures specified by the formalism, resulting in simplified computation for the reduction algorithms. This feature is not present in the case of LCFRS. There is a close parallel between the technique used in the M INIMAL -B INARIZATION algorithm and deductive parsing techniques as proposed by Shieber et al. (1995), that are usually implemented by means of tabular methods. The idea of exploiting tabular parsing in production factorization was first expressed in Zhang et al. (2006). In fact, the 546 part"
N09-1061,P06-2066,1,0.79325,"th a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally introduced as a generalization of several so-called mildly context-sensitive grammar formalisms. In the context of machine translation, LCFRS is an interesting generalization of SCFG because it does not restrict the fan-out to 2, allowing productions with arbitrary fan-out (and arbitrary rank). Given an LCFRS, our algorithm computes a strongly equivalent grammar with rank 2 and minHuman Langua"
N09-1061,E09-1055,1,0.585799,"re particularly relevant to this paper. 5.1 The tradeoff between rank and fan-out The algorithm introduced in this paper can be used to transform an LCFRS into an equivalent form with rank 2. This will result into a more efficiently parsable LCFRS, since rank exponentially affects parsing complexity. However, we must take into account that parsing complexity is also influenced by fan-out. Our algorithm guarantees a minimal increase in fan-out. In practical cases it seems such an increase is quite small. For example, in the context of dependency parsing, both G´omezRodr´ıguez et al. (2009) and Kuhlmann and Satta (2009) show that all the structures in several wellknown non-projective dependency treebanks are binarizable without any increase in their fan-out. More in general, it has been shown by Seki et al. (1991) that parsing of LCFRS can be carried out in time O(n|pM |), where n is the length of the input string and pM is the production in the grammar with largest size.3 Thus, there may be cases in which one has to find an optimal tradeoff between rank and fanout, in order to minimize the size of pM . This requires some kind of Viterbi search over the space of all possible binarizations, constructed as des"
N09-1061,P04-1084,1,0.120203,"ions of the original grammar can be reconstructed using some simple homomorphism (c.f. Nijholt, 1980). Our contribution is significant because the existing algorithms for decomposing SCFG, based on Uno and Yagiura (2000), cannot be applied to LCFRS, as they rely on the crucial property that components of biphrases are strictly separated in the generated string: Given a pair of synchronized nonterminal symbols, the material derived from the source nonterminal must precede the material derived from the target nonterminal, or vice versa. The problem that we solve has been previously addressed by Melamed et al. (2004), but in contrast to our result, their algorithm does not guarantee an optimal (minimal) increase in the fanout of the resulting grammar. However, this is essential for the practical applicability of the transformed grammar, as the parsing complexity of LCFRS is exponential in both the rank and the fan-out. Structure of the paper The remainder of the paper is structured as follows. Section 2 introduces the terminology and notation that we use for LCFRS. In Section 3, we present the technical background of our algorithm; the algorithm itself is discussed in Section 4. Section 5 concludes the pa"
N09-1061,P08-1069,1,0.851562,"Missing"
N09-1061,P87-1015,1,0.604286,"Missing"
N09-1061,P06-1123,0,0.0348496,"racted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally"
N09-1061,C08-1136,0,0.0189435,"ences therein. One practical problem with this approach, apart from the sheer number of the rules that result from the extraction procedure, is that the parsing complexity of all synchronous formalisms that we are aware of is exponential in the rank of a rule, defined as the number of nonterminals on the righthand side. Therefore, it is important that the rules of the extracted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studi"
N09-1061,N06-1033,0,0.095259,"ear Context-Free Rewriting Systems Carlos G´omez-Rodr´ıguez1 , Marco Kuhlmann2 , Giorgio Satta3 and David Weir4 1 Departamento de Computaci´on, Universidade da Coru˜na, Spain (cgomezr@udc.es) Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se) 2 3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it) 4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk) Abstract terms of parsing complexity, but smaller rules can also improve a translation model’s ability to generalize to new data (Zhang et al., 2006). Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation. The parsing complexity of an LCFRS is exponential in both the rank of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, called fan-out. In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which all productions have rank at most 2, and has minimal fan-out. Our results generalize previous work on Synchronous Context-Free Grammar, and are pa"
N09-1061,J07-2003,0,\N,Missing
N10-1035,C02-1028,0,0.0403826,"e form x1,1 · · · xm,1 ; and (ii) for each i ∈ [m], the sequence obtained from f by reading variables of the form xi,j from left to right has the form xi,1 · · · xi,ki . An LCFRS is called canonical, if each of its composition operations is canonical. We omit the proof that every LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1"
N10-1035,N10-1118,0,0.32636,". . . , (lm,ϕ(Am ) , rm,ϕ(Am ) )] [A0 , (l0,1 , r0,1 ), . . . , (l0,ϕ(A0 ) , r0,ϕ(A0 ) )] (a) The general rule for a parsing schema for LCFRS [B, (l1 , r1 ), . . . , (lm , rm )] [C, (l10 , r10 ), . . . (ln0 , rn0 )] [A, (l1 , r1 ), . . . , (lm , r10 ), . . . (ln0 , rn0 )] rm = l10 (b) Deduction step for concatenation [B, (l1 , r1 ), . . . , (lm , rm )] [C, (l10 , r10 ), . . . (ln0 , rn0 )] [A, (l1 , r1 ), . . . , (li , r10 ), . . . (ln0 , ri+1 ), . . . , (lm , rm )] ri = l10 , rn0 = li+1 (c) Deduction step for wrapping Figure 2: Deduction steps for parsing LCFRS. Thus, the parsing complexity (Gildea, 2010) of a production p = A0 → fP (A1 , . . . , Am ) is determined by ϕ(A0 ) l-indexes and i∈[m] ϕ(Ai ) r-indexes, for a total complexity of P O(|w|ϕ(A0 )+ i∈[m] ϕ(Ai ) ) where |w |is the length of the input string. The parsing complexity of an LCFRS will correspond to the maximum parsing complexity among its productions. Note that this general complexity matches the result given by Seki et al. (1991). In an LCFRS of rank ρ and fan-out ϕ, the maximum possible parsing complexity is O(|w|ϕ(ρ+1) ), obtained by applying the above expression to a production of rank ρ and where each nonterminal has fanou"
N10-1035,N09-1061,1,0.733957,"le parsing complexity is O(|w|ϕ(ρ+1) ), obtained by applying the above expression to a production of rank ρ and where each nonterminal has fanout ϕ. The asymptotic time complexity of LCFRS parsing is therefore exponential both in its rank and its fan-out. This means that it is interesting to transform LCFRS into equivalent forms that reduce their rank while preserving the fan-out. For sets of LCFRS that can be transformed into a binary form (i.e., such that all its rules have rank at most 2), the ρ factor in the complexity is reduced to a constant, and complexity is improved to O(|w|3ϕ ) (see Gómez-Rodríguez et al. (2009) for further discussion). Unfortunately, it is known by previous results (Rambow and Satta, 1999) that it is not always possible to convert an LCFRS into such a binary form without increasing the fan-out. However, we will show that it is always possible to build such a binarization for well-nested LCFRS. Combining this result with the inference rule and complexity analysis given above, we would obtain a parser for well-nested LCFRS running in 280 O(|w|3ϕ ) time. But the construction of our binary normal form additionally restricts binary composition operations in the binarized LCFRS to be of t"
N10-1035,P07-1077,0,0.032643,"arsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrangement of components might be needed. More specifically, analyses of both dependency and constituency treebanks (Kuhlmann and Nivre, 2006; Havelka, 2007; Maier and Lichte, 2009) have shown that rearrangements of argument tuples almost always satisfy the so-called well-nestedness condition, a generalization 276 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276–284, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of the standard condition on balanced brackets. This condition states that any two components x1 , x2 of some tuple will never be composed with any two components y1 , y2 of some other tuple in such a way that a ‘crossing’ configuration is re"
N10-1035,W09-3808,0,0.0143045,"xm,1 ; and (ii) for each i ∈ [m], the sequence obtained from f by reading variables of the form xi,j from left to right has the form xi,1 · · · xi,ki . An LCFRS is called canonical, if each of its composition operations is canonical. We omit the proof that every LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1 $x2,2 x1,2 is wellnested, wh"
N10-1035,P07-1021,1,0.883517,"LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1 $x2,2 x1,2 is wellnested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS is called well-nested, if it contains only well-nested composition operations. The class of languages generated by well-nested LCFRS is properly included in the class of languages generated by general LCFRS; see Kanazaw"
N10-1035,P06-2066,1,0.896161,"ly convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrangement of components might be needed. More specifically, analyses of both dependency and constituency treebanks (Kuhlmann and Nivre, 2006; Havelka, 2007; Maier and Lichte, 2009) have shown that rearrangements of argument tuples almost always satisfy the so-called well-nestedness condition, a generalization 276 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276–284, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of the standard condition on balanced brackets. This condition states that any two components x1 , x2 of some tuple will never be composed with any two components y1 , y2 of some other tuple in such a way that a ‘crossing’ conf"
N10-1035,E09-1055,1,0.547229,"stigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), and these systems lack Chomsky-like normal forms for fixed fan-out (Rambow and Satta, 1999) that are especially convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal fo"
N10-1035,P92-1012,1,0.837614,"r et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), and these systems lack Chomsky-like normal forms for fixed fan-out (Rambow and Satta, 1999) that are especially convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrange"
N10-1035,P87-1015,0,0.833476,"l linguistics has been the modeling of natural language syntax by means of formal grammars. Following results by Huybregts (1984) and Shieber (1985), special attention has been given to formalisms that enlarge the generative power of context-free grammars, but still remain below the full generative power of context-sensitive grammars. On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), a"
N13-1052,H91-1060,0,0.485743,"for z. It is often the case that parsing aims to find the highest scoring tree τ ∗ for z according to the underlying PCFG, also called the “Viterbi parse:” τ ∗ = argmax p(τ ) τ ∈T (z) j∈[m],k∈[m] We also let C(1,2) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,2) (y 1 , y 2 )]k = Ci,j,k yi1 yj2 . i∈[m],j∈[m] Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,3) (y 1 , y 2 )]j = Ci,j,k yi1 yk2 . i∈[m],k∈[m] Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) defined as: µ(a, i, j) = For two vectors x, y ∈ Rm we denote by x y ∈ the Hadamard product of x and y, i.e., [x y]i = xi yi . Finally, for vectors x, y, z ∈ Rm , xy &gt; z &gt; is the Rm 488 X p(τ ). τ ∈T (z) : (a,i,j)∈τ Here we write (a, i, j) ∈ τ if nonterminal a spans words xi · · · xj in the parse tree τ . Inputs: Sentence x1 · · ·"
N13-1052,W03-3005,0,0.0380165,"s of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section int"
N13-1052,N06-1022,0,0.0230182,"ncluding beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for"
N13-1052,P12-1024,1,0.92836,"approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section introduces the special representation for probabilistic context-free grammars that we adopt in this paper, along with the decoding algorithm that we investigate. For an integer i ≥ 1, we let [i] = {1, 2, . . . ,"
N13-1052,W05-1504,0,0.0275876,"ee grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on"
N13-1052,P96-1024,0,0.442285,"Bayes-Risk Decoding Let z = x1 · · · xN be some input sentence; we write T (z) to denote the set of all possible trees for z. It is often the case that parsing aims to find the highest scoring tree τ ∗ for z according to the underlying PCFG, also called the “Viterbi parse:” τ ∗ = argmax p(τ ) τ ∈T (z) j∈[m],k∈[m] We also let C(1,2) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,2) (y 1 , y 2 )]k = Ci,j,k yi1 yj2 . i∈[m],j∈[m] Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,3) (y 1 , y 2 )]j = Ci,j,k yi1 yk2 . i∈[m],k∈[m] Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) defined as: µ(a, i, j) = For two vectors x, y ∈ Rm we denote by x y ∈ the Hadamard product of x and y, i.e., [x y]i = xi yi . Finally, for vectors x, y, z ∈ Rm , xy &gt; z &gt; is the Rm 488 X p(τ ). τ ∈T (z) : (a,"
N13-1052,P03-1054,0,0.0240128,"up of a factor of 4.75 for Arabic (r = 140) and 6.5 for English (r = 260) while retaining similar performance. Perhaps more surprising is that using the tensor approximation actually improves performance in several cases. We hypothesize that the cause of this is that the tensor decomposition requires less parameters to express the rule probabilities in the grammar, and therefore leads to better generalization than a vanilla maximum likelihood estimate. We include results for a more complex model for Arabic, which uses horizontal Markovization of order 1 and vertical Markovization of order 2 (Klein and Manning, 2003). This grammar includes 2,188 binary rules. Parsing exhaustively using this grammar takes 1.30 seconds per sentence (on average) with an F1 measure of 64.43. Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence (on average) with an F1 measure of 64.05. The probability of Pa sentence z under a PCFG is defined as p(z) = τ ∈T (z) p(τ ), and can be approximated using the algorithm in Section 4.3, running in time O(rN 3 + rmN 2 ). Of theoretical interest, we discuss here a time O(rN 3 + r2 N 2 ) algorithm, which is more convenient when r &lt; m. Observe that in Eq. (3) vector"
N13-1052,J93-2004,0,0.0421226,"Missing"
N13-1052,P05-1010,0,0.121189,"s algorithm does not enforce that the output parse trees are included in the tree language of the PCFG, that is, certain combinations of children and parent nonterminals may violate the rules in the grammar. In our experiments we departed from this, and changed Goodman’s algorithm by incorporating the grammar into the dynamic programming algorithm in Figure 1. The reason this is important for our experiments is that we binarize the grammar prior to parsing, and we need to enforce the links between the split nonterminals (in the binarized grammar) that refer to the same syntactic category. See Matsuzaki et al. (2005) for more details about the binarization scheme we used. This step changes the dynamic programming equation of Goodman to be linear in the size of the grammar (figure 1). However, empirically, it is the insideoutside algorithm which takes most of the time to compute with Goodman’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulatio"
N13-1052,J00-1003,0,0.0675288,"mproves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. We test our algorithm on two treebanks, and get significant improvements in parsing speed. 1 Introduction The problem of speeding-up parsing algorithms based on probabilistic context-free grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters"
P02-1015,P81-1022,0,0.306895,"rom   ] ÁT' f  . If the item in   , and such that YS'X4  > A ? + @ I ? B | > D / @ 5 F G jl  n can be derived by the algorithm, then the intersection of the language generated by   and the language accepted by the PDA (generated by  ) is non-empty. 4 Earley’s algorithm The CKY algorithm from Figure 5 can be seen to filter out a selection of the computations that may be derived by the deduction system from Figure 2. One may however be even more selective in determining which computations of the PDA to consider. The basis for the algorithm in this section is Earley’s algorithm (Earley, 1970). This algorithm differs from the CKY algorithm in that it satisfies the correct-prefix property (Harrison, 1978). The new algorithm is presented by Figure 6. There are now two types of item involved. The first item has the form j  0Âm:1Z WÂÃS'UWÂÃ¥Tn , where £ 0ÄmÅ1 has the same role as the dotted rules in Earley’s original algorithm. The second and third components are stacks of the PDA as before, but these stacks now contain a distinguished position, indicated by Ã . The existence of an item j Æ 0 m¥1# WÃÇS'XWÃTn implies that ^W_S' 3  ] ^WT' f  , where 3 is now a strin"
P02-1015,A00-2023,0,0.535887,"Italy satta@dei.unipd.it one by one then leads to an unnecessary duplication of subcomputations, since each occurrence of a repeated substring has to be independently parsed. As this approach may be prohibitively expensive, it is preferable to find a parsing algorithm that shares subcomputations among different strings by working directly on the nonterminals and the rules of the non-recursive CFG. In this way, “parsing” a nonterminal of the grammar amounts to shared parsing of all the substrings encoded by that nonterminal. To give a few examples, in some natural language generation systems (Langkilde, 2000) nonrecursive CFGs are used to encode very large sets of candidate sentences realizing some input conceptual representation (Langkilde calls such grammars forests). Each CFG is later “parsed” using a language model, in order to rank the sentences in the set according to their likelyhood. Similarly, in some approaches to automatic speech understand ing (Corazza and Lavelli, 1994) the -best sentences obtained from the speech recognition module are “compressed” into a non-recursive CFG grammar, which is later provided as input to a parser. Finally, in some machine translation applications relate"
P04-1069,P99-1070,0,0.665635,"he question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than"
P04-1069,P89-1018,0,0.330703,"p(τ ) = 1, for each X, Y ∈ Q such that there is at least one transition Y X 7→ Z, Z ∈ Q. The probability of a computation c = τ1 · · · τm , τi ∈ ∆ for 1 ≤ i ≤ m, is p(c) = Q m Pi=1 p(τi ). The probability of a string w is p(w) = (Xin ,w,ε)`c (Xf in ,ε,v) p(c). A PPDT is consistent if Σw∈Σ ∗ p(w) = 1. A PPDT (A, p) is reduced if A is reduced. 3 Parsing Strategies The term “parsing strategy” is often used informally to refer to a class of parsing algorithms that behave similarly in some way. In this paper, we assign a formal meaning to this term, relying on the observation by (Lang, 1974) and (Billot and Lang, 1989) that many parsing algorithms for CFGs can be described in two steps. The first is a construction of push-down devices from CFGs, and the second is a method for handling nondeterminism (e.g. backtracking or dynamic programming). Parsing algorithms that handle nondeterminism in different ways but apply the same construction of push-down devices from CFGs are seen as realizations of the same parsing strategy. Thus, we define a parsing strategy to be a function S that maps a reduced CFG G = (Σ , N, S, R) to a pair S(G) = (A, f ) consisting of a reduced PDT A = (Σ , Σ , Q, Xin , Xf in , ∆), and"
P04-1069,J93-1002,0,0.190802,", who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than the underlying PCFGs. However, this is not the case in general. Consider a PCFG with rule/probability pairs: B → bC , 23 B → bD, 13 C → xc, 1 D → xd , 1 S → AB , 1 A → aC , 13 A → aD, 23 There are two key transitions in the associated LR automaton, which represent shift actions over c and d (we denote LR states by their sets of kernel items and encode these states into stack symbols): τc : {C {C τd : {C {C → x • c, D → x • c, D → x • c, D → x • c, D c → x • d} 7→ → x • d} {C → xc •} d"
P04-1069,J98-2005,0,0.322983,"consisting of complete derivations d, such that for each rule π in G there is at least one d ∈ D in which π occurs. Let nπ,d be the number of occurrences of rule π in derivation d ∈ D, and let nπ be Σd∈D nπ,d , the total number of occurrences of π in D. Let nA be the sum of nπ for all rules π with A in the left-hand side. A probability function pG can be defined through “maximum-likelihood estimation” such that pG (π) = nnAπ for each rule π = A → α. For all nonterminals A, Σπ=A→α pG (π) = Σπ=A→α nnAπ = nnA = 1, which means that the A PCFG (G, pG ) is proper. Furthermore, it has been shown in (Chi and Geman, 1998; S´anchez and Bened´ı, 1997) that a PCFG (G, pG ) is consistent if pG was obtained by maximum-likelihood estimation using a set of derivations. Finally, since nπ &gt; 0 for each π, also pG (π) &gt; 0 for each π, and pG (d) &gt; 0 for all complete derivations d. We say a computation is a shortest dead computation if it is dead and none of its proper prefixes is dead. Note that each dead computation has a unique prefix that is a shortest dead computation. For a PDT A, let TA be the union of the set of all complete computations and the set of all shortest dead computations. Lemma 2 For each Σc∈TA pA (c)"
P04-1069,J99-1004,0,0.423485,"where c is the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0 ∈TA pA (c0 ) ≥ Σw∈Σ∗ pA (w) + pA (c) &gt; Σw∈Σ∗ pA (w) = Σw∈Σ∗ pG (w). This is in contradiction with the consistency of (G, pG ). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG ), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G ) is proper and, for every com1 0 plete P derivation d, p0G (d) = C · pG (d), where C = S⇒d0 w,w∈Σ ∗ pG (d ). Note that if PCFG (G, pG ) in the above lemma is consistent, then C = 1 and (G, p0G ) and (G, pG ) define the same distribution on derivations. The normalization procedure P underlying Lemma 4 makes use of quantities A⇒d w,w∈Σ ∗ pG (d) for each A ∈ N . These quantities can be computed to any degree of precision, as discussed for instance in"
P04-1069,H90-1053,0,0.364753,"ted PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the a"
P04-1069,P89-1017,0,0.0532444,"abilistic transduction, or in other words, we may simplify PDTs to PDAs. The proof of Theorem 5 also leads to the observation that parsing strategies with the CPP and the SPP as well as their probabilistic extensions can be described as grammar transformations, as follows. A given (P)CFG is mapped to an equivalent (P)PDT by a (probabilistic) parsing strategy. By ignoring the output components of swap transitions we obtain a (P)PDA, which can be mapped to an equivalent (P)CFG as shown above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG G, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SP"
P04-1069,W03-3016,1,0.763272,"the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0 ∈TA pA (c0 ) ≥ Σw∈Σ∗ pA (w) + pA (c) &gt; Σw∈Σ∗ pA (w) = Σw∈Σ∗ pG (w). This is in contradiction with the consistency of (G, pG ). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG ), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G ) is proper and, for every com1 0 plete P derivation d, p0G (d) = C · pG (d), where C = S⇒d0 w,w∈Σ ∗ pG (d ). Note that if PCFG (G, pG ) in the above lemma is consistent, then C = 1 and (G, p0G ) and (G, pG ) define the same distribution on derivations. The normalization procedure P underlying Lemma 4 makes use of quantities A⇒d w,w∈Σ ∗ pG (d) for each A ∈ N . These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973)"
P04-1069,P94-1017,1,0.725967,"e to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG G, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy. Related to LR parsing is ELR parsing (Purdom and Brown, 1981; Nederhof, 1994), which also lacks the SPP. By an argument similar to the one"
P04-1069,P99-1054,0,0.149528,"the sum of the probabilities of all rules rewriting A must be 1. This Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it means that, out of a total of say m rules rewriting A, only m − 1 rules represent “free” parameters. Depending on the choice of the parsing strategy, the constructed PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be"
P04-1069,P80-1024,0,0.802942,"Missing"
P04-1069,C69-0101,0,0.813844,"nd Geman, We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics, as well as to provide simpler proofs of already known results. We introduce our framework in Section 3 and report our main results in Sections 4 and 5. We discuss applications of our results in Section 6. 2 Preliminaries In this paper we assume some familiarity with definitions of (P)CFGs and (P)PDAs. We refer the reader to standard textbooks and publications as for instance (Harrison, 1978; Booth and Thompson, 1973; Santos, 1972). A CFG G is a tuple (Σ, N, S, R), with Σ and N the sets of terminals and nonterminals, respectively, S the start symbol and R the set of rules. In this paper we only consider left-most derivations, represented as strings d ∈ R∗ and simply called derivations. For α, β ∈ (Σ ∪ N )∗ , we write α ⇒d β with the usual meaning. If α = S and β = w ∈ Σ ∗ , we call d a complete derivation of w. We say a CFG is reduced if each rule in R occurs in some complete derivation. A PCFG is a pair (G, p) consisting of a CFG G and a probability function p from R to real numbers P in the interval [0, 1]. A PCFG is"
P04-1069,J95-2002,0,0.192971,"a 4 Given a non-proper PCFG (G, pG ), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G ) is proper and, for every com1 0 plete P derivation d, p0G (d) = C · pG (d), where C = S⇒d0 w,w∈Σ ∗ pG (d ). Note that if PCFG (G, pG ) in the above lemma is consistent, then C = 1 and (G, p0G ) and (G, pG ) define the same distribution on derivations. The normalization procedure P underlying Lemma 4 makes use of quantities A⇒d w,w∈Σ ∗ pG (d) for each A ∈ N . These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). Thus normalization of a PCFG can be effectively computed. For a fixed PDT, we define the binary relation ; on stack symbols by: Y ; Y 0 if and only if (Y, w, ε) `∗ (Y 0 , ε, v) for some w ∈ Σ∗ and v ∈ Σ∗ . In words, some subcomputation of the PDT may start with stack Y and end with stack Y 0 . Note that all stacks that occur in such a subcomputation must have height of 1 or more. We say that a (P)PDA or a (P)PDT has the strong predictiveness property (SPP) if the existence of three transitions X 7→ XY , XY1 7→ Z1 and XY2 7→ Z2 such that Y ; Y1 and Y ; Y2 implies Z1 = Z2 . Informally, this"
P04-1069,1995.iwpt-1.28,0,0.318533,"ts from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991;"
P04-1070,P89-1018,0,0.3381,"anguage processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this context, the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs. The acronym ‘LR’ stands for ‘Left-to-right processing of the input, producing a Right-most derivation (in reverse)’. When we construct a PDT A from Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it a CFG G by the LR parsing strategy and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G"
P04-1070,J93-1002,0,0.652688,"of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (P"
P04-1070,J98-2005,0,0.0327906,"rammars (PCFGs); we say a PCFG is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Bened´ı, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in exist"
P04-1070,J99-1004,0,0.127671,"ay a PCFG is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Bened´ı, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in existing literat"
P04-1070,H90-1053,0,0.0207152,"ank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pG on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e."
P04-1070,W01-1802,0,0.0313342,"parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several transitions are applicable for a given stack, then the sum of probabilities assigned to those transitions by probability function pA should be 1. This assumption may be motivated by pragmatic considerations, as such a prop"
P04-1070,J98-4004,0,0.481322,"ulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pG on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several trans"
P04-1070,1993.iwpt-1.12,0,0.678874,", under the restrictions of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an i"
P04-1070,P96-1032,1,0.810879,". There are a few superficial differences with LR parsing as it is commonly found in the literature. The most obvious difference is that we divide reductions into ‘binary’ steps. The main reason is that this allows tabular interpretation with a time complexity cubic in the length of the input. Otherwise, the time complexity would be O(nm+1 ), where m is the length of the longest right-hand side of a rule in the CFG. This observation was made before by (Kipps, 1991), who proposed a solution similar to ours, albeit formulated differently. See also a related formulation of tabular LR parsing by (Nederhof and Satta, 1996). To be more specific, instead of one step of the PDT taking stack: σp0 p1 · · · pm immediately to stack: σp0 q where (A → X1 · · · Xm •) ∈ pm , σ is a string of stack symbols and goto(p0 , A) = q, we have a number of smaller steps leading to a series of stacks: σp0 p1 · · · pm−1 pm σp0 p1 · · · pm−1 (A, m−1) σp0 p1 · · · (A, m−2) .. . σp0 (A, 0) σp0 q There are two additional differences. First, we want to avoid steps of the form: σp0 (A, 0) σp0 q ε,ε by transitions p0 (A, 0) 7→ p0 q, as such transitions complicate the generic definition of ‘properness’ for PDTs, to be discussed in the follow"
P04-1070,W03-3016,1,0.915007,"s proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Bened´ı, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in existing literature on probabilistic PDTs ("
P04-1070,P04-1069,1,0.80105,"s than prfe does. (By ‘consistent’ we mean that the probabilities of all strings that are accepted sum to 1.) It may even be the case that a (proper and consistent) probability function pG on the rules of the input grammar G exists that assigns a higher likelihood to the corpus than prfe , and therefore it is not guaranteed that LR parsers allow better probability estimates than the CFGs from which they were constructed, if we constrain probability functions pA to be proper. In this respect, LR parsing differs from at least one other well-known parsing strategy, viz. left-corner parsing. See (Nederhof and Satta, 2004) for a discussion of a property that is shared by left-corner parsing but not by LR parsing, and which explains the above difference. As main contribution of this paper we establish that this restriction on expressible probability distributions can be dispensed with, without losing the ability to perform training by relative frequency estimation. What comes in place of properness is reverse-properness, which can be seen as properness of the reversed pushdown automaton that processes input from right to left instead of from left to right, interpreting the transitions of A backwards. As we will"
P04-1070,1991.iwpt-1.18,0,0.519785,"nsition Q P 7→ R by relative frequency estimation. The resulting PPDT is proper. It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies, among which are top-down parsing and left-corner parsing. This does not hold for PDTs constructed by the LR parsing strategy however, and in fact, properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG. This was formally proven by (Nederhof and Satta, 2004), after (Ng and Tomita, 1991) and (Wright and Wrigley, 1991) had already suggested that creating a probabilistic LR parser that is equivalent to an input PCFG is difficult in general. The same difficulty for ELR parsing was suggested by (Tendeau, 1997). For this reason, we investigate a practical alternative, viz. reverse-properness. Now we have to assume that for each stack symbol R, we either have a,b one or more transitions of the form P 7→ R or a,b Q P 7→ R, or one or more transitions of the form a,b P 7→ P R, but no combination thereof. In the first case, reverse-properness demands that the sum of a,b a,b probabiliti"
P04-1070,C00-2098,0,0.120212,"to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this co"
P04-1070,C69-0101,0,0.753126,"rsers do not use lookahead to decide between alternative transitions, they are called LR(0) parsers. More generally, if LR parsers look ahead k symbols, they are called LR(k) parsers; some simplified LR parsing models that use lookahead are called SLR(k) and LALR(k) parsing (Sippu and Soisalon-Soininen, 1990). In order to simplify the discussion, we abstain from using lookahead in this article, and ‘LR parsing’ can further be read as ‘LR(0) parsing’. We would like to point out however that our observations carry over to LR parsing with lookahead. The theory of probabilistic pushdown automata (Santos, 1972) can be easily applied to LR parsing. A probability is then assigned to each transition, by a function that we will call the probability function pA , and the probability of an accepting computation of A is the product of the probabilities of the applied transitions. As each accepting computation produces a right-most derivation as output string, a probabilistic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a co"
P04-1084,E91-1005,0,0.116877,"lation Schema (SDTS) (Aho and Ullman, 1969) lack both of these properties. Synchronous Tree Adjoining Grammar (STAG) (Shieber, 1994) lacks the latter and allows only limited discontinuities in each tree. Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive Grammar (MCSG), while satisfying both of the above criteria. The move to MCSG is motivated by our desire to more perspicuously account for certain syntactic phenomena that cannot be easily captured by context-free grammars, such as clitic climbing, extraposition, and other types of longdistance movement (Becker et al., 1991). On the other hand, MCSG still observes some restrictions that make the set of languages it generates less ex pensive to analyze than the languages generated by (properly) context-sensitive formalisms. More technically, our proposal starts from Multitext Grammar (MTG), a formalism for synchronizing context-free grammars recently proposed by Melamed (2003). In MTG, synchronous rewriting is implemented by means of an indexing relation that is maintained over occurrences of nonterminals in a sentential form, using essentially the same machinery as SDTS. Unlike SDTS, MTG can extend the dimension"
P04-1084,W01-1807,0,0.0998276,", a formalism for describing a class of MCSGs. The generalization is achieved by allowing context-free productions to rewrite tuples of strings, rather than single strings. Thus, we retain the intuitive top-down definition of synchronous derivation original in SDTS and MTG but not found in LCFRS, while extending the generative power to linear context-free rewriting languages. In this respect, GMTG has also been inspired by the class of Local Unordered Scattered Context Grammars (Rambow and Satta, 1999). A syntactically very different synchronous formalism involving LCFRS has been presented by Bertsch and Nederhof (2001). This paper begins with an informal description of GMTG. It continues with an investigation of this formalism’s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the “weak language preservation property.” Lastly, we propose a synchronous generalization of Chomsky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKYstyle algorithm (Younger, 1967; Melamed, 2004). 2 Informal Description and Comparisons GMTG is a generalization of MTG, w"
P04-1084,W00-2035,0,0.0872965,"NP   PP   NP   NP  , V   V  , DT     A     N    ,   VB     R   NP VP   NP VP&  (12)  V NP   V  PP  (13)  DT A&apos; N(   VB)* R+  (14)  Tim   Tim  (15)  got   got  (16)  a    (17)  pink    (18)  slip    (19)    laid  (20)    off  (21) As described by Melamed (2003), MTG requires production components to be contiguous, except after binarization. GMTG removes this restriction. Take, for example, the sentence pair [(The doctor treats his teeth), (El m´edico le examino los dientes)] (Dras and Bleam, 2000). The Spanish clitic le and the NP los dientes should both be paired with the English NP his teeth, giving rise to a discontinuous constituent in the Spanish component. A GMTG fragment for the sentence is shown below:  S   S   VP   VP !  NP   NP !  V   V   NP   NP NP !  NP VP   NP VP   V NP   NP V NP   The doctor   El m´edico   treats   examino   his teeth   le  los dientes  Note the discontinuity between le and los dientes. Such discontinuities are marked by commas on both the LHS and the RHS of the relevant component. GMTG’s"
P04-1084,P04-1084,1,0.106171,"Missing"
P04-1084,N03-1021,1,0.793017,"Grammar (CFG). GMTG allows both synchronous and independent rewriting. Such flexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalisms. This paper investigates the generative capacity of GMTG, proves that each component grammar of a GMTG retains its generative power, and proposes a generalization of Chomsky Normal Form, which is necessary for synchronous CKY-style parsing. 1 Introduction Synchronous grammars have been proposed for the formal description of parallel texts representing translations of the same document. As shown by Melamed (2003), a plausible model of parallel text must be able to express discontinuous constituents. Since linguistic expressions can vanish in translation, a good model must be able to express independent (in addition to synchronous) rewriting. Inversion Transduction Grammar (ITG) (Wu, 1997) and Syntax-Directed Translation Schema (SDTS) (Aho and Ullman, 1969) lack both of these properties. Synchronous Tree Adjoining Grammar (STAG) (Shieber, 1994) lacks the latter and allows only limited discontinuities in each tree. Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive"
P04-1084,P04-1083,1,0.436176,"t synchronous formalism involving LCFRS has been presented by Bertsch and Nederhof (2001). This paper begins with an informal description of GMTG. It continues with an investigation of this formalism’s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the “weak language preservation property.” Lastly, we propose a synchronous generalization of Chomsky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKYstyle algorithm (Younger, 1967; Melamed, 2004). 2 Informal Description and Comparisons GMTG is a generalization of MTG, which is itself a generalization of CFG to the synchronous case. Here we present MTG in a new notation that shows the relation to CFG more clearly. For example, the following MTG productions can generate the multitext [(I fed the cat), (ya kota kormil)]: 1  PN  VP   PN VP   I   ya   V NP   NP V   fed   kormil   D N   N   the     cat   kota                              (S) (S) PN PN VP VP V V NP NP D N N (1) (2) (3) (4) (5) (6) (7)"
P04-1084,P96-1016,1,0.547411,"in LCFRS, while extending the generative power to linear context-free rewriting languages. In this respect, GMTG has also been inspired by the class of Local Unordered Scattered Context Grammars (Rambow and Satta, 1999). A syntactically very different synchronous formalism involving LCFRS has been presented by Bertsch and Nederhof (2001). This paper begins with an informal description of GMTG. It continues with an investigation of this formalism’s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the “weak language preservation property.” Lastly, we propose a synchronous generalization of Chomsky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKYstyle algorithm (Younger, 1967; Melamed, 2004). 2 Informal Description and Comparisons GMTG is a generalization of MTG, which is itself a generalization of CFG to the synchronous case. Here we present MTG in a new notation that shows the relation to CFG more clearly. For example, the following MTG productions can generate the multitext [(I fed the cat), (ya kota kormil)]: 1  PN  VP   PN VP"
P04-1084,J97-3002,0,0.678472,"ammar of a GMTG retains its generative power, and proposes a generalization of Chomsky Normal Form, which is necessary for synchronous CKY-style parsing. 1 Introduction Synchronous grammars have been proposed for the formal description of parallel texts representing translations of the same document. As shown by Melamed (2003), a plausible model of parallel text must be able to express discontinuous constituents. Since linguistic expressions can vanish in translation, a good model must be able to express independent (in addition to synchronous) rewriting. Inversion Transduction Grammar (ITG) (Wu, 1997) and Syntax-Directed Translation Schema (SDTS) (Aho and Ullman, 1969) lack both of these properties. Synchronous Tree Adjoining Grammar (STAG) (Shieber, 1994) lacks the latter and allows only limited discontinuities in each tree. Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive Grammar (MCSG), while satisfying both of the above criteria. The move to MCSG is motivated by our desire to more perspicuously account for certain syntactic phenomena that cannot be easily captured by context-free grammars, such as clitic climbing, extraposition, and other types"
P04-1084,P92-1018,0,\N,Missing
P06-2036,P05-1033,0,0.0371941,"Science Dept. University of Rochester Rochester, NY 14627 Introduction Synchronous Context-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computat"
P06-2036,N04-1035,0,0.0453813,"in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus on each single synchronous rule and factorize it into synchronous rules of lower rank. If we view the bijective relation associat"
P06-2036,N03-1021,0,0.0211477,"t-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus"
P06-2036,H05-1101,1,0.929841,"Rochester Rochester, NY 14627 Giorgio Satta Dept. of Information Eng’g University of Padua I-35131 Padua, Italy string having a maximum length of N , and consider an SCFG G of size |G|, with a bound of n nonterminals in the right-hand side of each rule in a single dimension, which we call below the rank of G. As an upper bound, parsing can be carried out in time O(|G |N n+4 ) by a dynamic programming algorithm maintaining continuous spans in one dimension. As a lower bound, parsing strategies with discontinuous√spans in both dimensions can take time Ω(|G |N c n ) for unfriendly permutations (Satta and Peserico, 2005). A natural question to ask then is: What if we could reduce the rank of G, preserving the generated translation? As in the case of CFGs, one way of doing this would be to factorize each single rule into several rules of rank strictly smaller than n. It is not difficult to see that this would result in a new grammar of size at most 2 · |G|. In the time complexities reported above, we see that such a size increase would be more than compensated by the reduction in the degree of the polynomial in N . We thus conclude that a reduction in the rank of an SCFG would result in more efficient parsing"
P06-2036,J97-3002,0,0.221999,"problem about recognizing permutations that can be factored. 1 Hao Zhang Computer Science Dept. University of Rochester Rochester, NY 14627 Introduction Synchronous Context-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Confere"
P06-2036,P01-1067,0,0.0784028,"multaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus on each single synchronous rule and factorize it into synchronous rules of lower rank. If we view the bi"
P06-2036,N06-1033,1,0.761511,"torizing a permutation of arity n into the composition of several permutations of arity k < n. Such factorization can be represented as a tree of composed permutations, called in what follows a permutation tree. A permutation tree can be converted into a set of k-ary SCFG rules equivalent to the input rule. For example, the input rule: 4,1,3,5,2 5 8 6 1 4 2,4,1,3 6 3 8 2 5 4 Figure 1: Two permutation trees. The permutations associated with the leaves can be produced by composing the permutations at the internal nodes. spans in one dimension. Previous work on this problem has been presented in Zhang et al. (2006), where a method is provided for casting an SCFG to a form with rank k = 2. If generalized to any value of k, that algorithm would run in time O(n2 ). We thus improve existing factorization methods by almost a factor of n. We also solve an open problem mentioned by Albert et al. (2003), who pose the question of whether irreducible, or simple, permutations can be recognized in time less than Θ(n2 ). [ X → A(1) B (2) C (3) D(4) E (5) F (6) G(7) H (8) , X → B (2) A(1) C (3) D(4) G(7) E (5) H (8) F (6) ] yields the permutation tree of Figure 1(left). Introducing a new grammar nonterminal Xi for ea"
P07-1096,P04-1015,0,0.042089,"at our system does not need a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, 765 Data Set Training Develop Test Sections 0-18 19-21 22-24 Sentences 38,219 5,527 5,462 Tokens 912,344 131,768 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 2005), the search is resume"
P07-1096,W02-1001,0,0.365173,"re, the order of inference and the local classification are dynamically incorporated in the learning phase. Guided learning is not as hard as reinforcement learning. At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable. In this approach, we can easily collect the automatically generated negative samples, and use them in learning. These negative samples are exactly those we will face during inference with the current weight vector. In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and M´ezard, 1987) to improve performance. 3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic. Gim´enez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan. In our model, the order of inference is dynamically incorporated into the training of the local classifier. Toutanova et al. (2003) reported a POS tagger based on cyclic dependency network. In their work, the order of inference is fixed"
P07-1096,gimenez-marquez-2004-svmtool,0,0.162643,"Missing"
P07-1096,H05-1102,1,0.792033,"ed a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, 765 Data Set Training Develop Test Sections 0-18 19-21 22-24 Sentences 38,219 5,527 5,462 Tokens 912,344 131,768 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 2005), the search is resumed after some gold stand"
P07-1096,N03-1033,0,0.607831,"Missing"
P07-1096,H05-1059,0,0.403316,"m being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order. This solution does not suffer from the label bias problem. Compared to the undirected methods, the Perceptron like algorithm is faster in training. In this paper, we will improve upon Collins’ algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost. When a bidirectional strategy is used, the main problem is how to select the order of inference. Tsuruoka and Tsujii (2005) proposed the easiest-first approach which greatly reduced the computation complexity of inference while maintaining the accuracy on labeling. However, the easiest-first approach only serves as a heuristic rule. The order of inference is not incorporated into the training of the MaxEnt classifier for individual labeling. Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004; Daum´e III and Marcu"
P07-1096,J93-2004,0,\N,Missing
P08-1069,P06-2036,1,0.639552,"d. (a) S (b) S (c) S likes 1 N P↓ 1 N P↓ 1 VP V N P↓ 2 N Adj N∗ N∗ Adj red John candies 1 N P↓ 2 V NP VP John V rouges NP V P N P Jean V red likes aime likes N VP S 2 NP N 1 NP Det NP N NP John Jean 1 Adj NP aime Det N N red candies les N N Adj bonbons rouges candies les bonbons Figure 2: An example STAG derivation of the English/French sentence pair “John likes red candies”/“Jean aime les bonbons rouges”. The figure is divided as follows: (a) the STAG grammar, (b) the derivation tree for the sentence pair, and (c) the derived tree pair for the sentences. and Gildea, 2007; Zhang et al., 2006; Gildea, Satta, and Zhang, 2006). The methods for k-arization of SCFG cannot be directly applied to STAG because of the additional complexity introduced by the expressivity-increasing adjunction operation of TAG. In SCFG, where substitution is the only available operation and the depth of elementary structures is limited to one, the k-arization problem reduces to analysis of permutations of strings of nonterminal symbols. In STAG, however, the arbitrary depth of the elementary structures and the lack of restriction to contiguous strings of nonterminals introduced by adjunction substantially complicate the task. In this pape"
P08-1069,W06-1506,0,0.0647525,"ity Cambridge, MA 02138 Giorgio Satta Department of Information Engineering University of Padua I-35131 Padova, Italy Stuart M. Shieber School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 nesson@seas.harvard.edu satta@dei.unipd.it shieber@seas.harvard.edu Abstract the application of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-langua"
P08-1069,W06-1505,0,0.0133878,"ity Cambridge, MA 02138 Giorgio Satta Department of Information Engineering University of Padua I-35131 Padova, Italy Stuart M. Shieber School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 nesson@seas.harvard.edu satta@dei.unipd.it shieber@seas.harvard.edu Abstract the application of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-langua"
P08-1069,W07-0402,1,0.850392,"d Sciences Harvard University Cambridge, MA 02138 Giorgio Satta Department of Information Engineering University of Padua I-35131 Padova, Italy Stuart M. Shieber School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 nesson@seas.harvard.edu satta@dei.unipd.it shieber@seas.harvard.edu Abstract the application of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of nat"
P08-1069,2006.amta-papers.15,1,0.905421,"Missing"
P08-1069,P92-1012,1,0.761867,"ication of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-language syntax and semantics. Current research in both of these areas is actively pursuing its incorporation. However, STAG parsing is known to be NP-hard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures. Given a particular grammar, the p"
P08-1069,C90-3045,1,0.737602,"only once in a derivation. Operations may only occur at nodes marked with a link. For simplicity of presentation we provisionally assume that only one link is permitted at a node. We later drop this assumption. In a synchronous TAG (STAG) the elementary structures are ordered pairs of TAG trees, with a linking relation specified over pairs of nonterminal nodes. Each link has two locations, one in the left tree in a pair and the other in the right tree. An example of an STAG derivation including both substitution and adjunction is given in Figure 2. For further background, refer to the work of Shieber and Schabes (1990) and Shieber (1994). 606 k-arization Algorithm For a synchronous tree pair γ = hγL , γR i, a fragment of γL (or γR ) is a complete subtree rooted at some node n of γL , written γL (n), or else a subtree rooted at n with a gap at node n0 , written γL (n, n0 ); see Figure 3 for an example. We write links(n) and links(n, n0 ) to denote the set of links of γL (n) and γL (n, n0 ), respectively. When we do not know the root or gap nodes of some fragment αL , we also write links(αL ). We say that a set of links Λ from γ can be isolated if there exist fragments αL and αR of γL and γR , respectively, b"
P08-1069,W07-0404,0,0.0804827,"Missing"
P08-1069,N06-1033,0,0.129869,"four may be isolated. (a) S (b) S (c) S likes 1 N P↓ 1 N P↓ 1 VP V N P↓ 2 N Adj N∗ N∗ Adj red John candies 1 N P↓ 2 V NP VP John V rouges NP V P N P Jean V red likes aime likes N VP S 2 NP N 1 NP Det NP N NP John Jean 1 Adj NP aime Det N N red candies les N N Adj bonbons rouges candies les bonbons Figure 2: An example STAG derivation of the English/French sentence pair “John likes red candies”/“Jean aime les bonbons rouges”. The figure is divided as follows: (a) the STAG grammar, (b) the derivation tree for the sentence pair, and (c) the derived tree pair for the sentences. and Gildea, 2007; Zhang et al., 2006; Gildea, Satta, and Zhang, 2006). The methods for k-arization of SCFG cannot be directly applied to STAG because of the additional complexity introduced by the expressivity-increasing adjunction operation of TAG. In SCFG, where substitution is the only available operation and the depth of elementary structures is limited to one, the k-arization problem reduces to analysis of permutations of strings of nonterminal symbols. In STAG, however, the arbitrary depth of the elementary structures and the lack of restriction to contiguous strings of nonterminals introduced by adjunction substantially c"
P08-1069,W06-1501,0,\N,Missing
P08-1069,W90-0102,1,\N,Missing
P09-1111,P87-1015,0,0.929142,"n-out at most 2 into a binary form, whenever this is possible. This results in asymptotical run-time improvement for known parsing algorithms for this class. 1 Introduction Since its early years, the computational linguistics field has devoted much effort to the development of formal systems for modeling the syntax of natural language. There has been a considerable interest in rewriting systems that enlarge the generative power of context-free grammars, still remaining far below the power of the class of contextsensitive grammars; see (Joshi et al., 1991) for discussion. Following this line, (Vijay-Shanker et al., 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community. LCFRSs allow the derivation of tuples of strings,1 i.e., discontinuous phrases, that turn out to be very useful in modeling languages with relatively free word order. This feature has recently been used for mapping non-projective dependency grammars into discontinuous phrase structures (Kuhlmann and Satta, 2009). Furthermore, LCFRSs also implement so-called synchronous 1 In its more general definition, an LCFRS provides a framework where abstract"
P09-1111,C08-1136,0,0.0438906,"Missing"
P09-1111,W05-1502,0,0.0387958,"Missing"
P09-1111,P05-1033,0,0.152422,"Missing"
P09-1111,N09-1061,1,0.814772,"Missing"
P09-1111,E09-1055,1,0.725521,"remaining far below the power of the class of contextsensitive grammars; see (Joshi et al., 1991) for discussion. Following this line, (Vijay-Shanker et al., 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community. LCFRSs allow the derivation of tuples of strings,1 i.e., discontinuous phrases, that turn out to be very useful in modeling languages with relatively free word order. This feature has recently been used for mapping non-projective dependency grammars into discontinuous phrase structures (Kuhlmann and Satta, 2009). Furthermore, LCFRSs also implement so-called synchronous 1 In its more general definition, an LCFRS provides a framework where abstract structures can be generated, as for instance trees and graphs. Throughout this paper we focus on so-called string-based LCFRSs, where rewriting is defined over strings only. 985 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 985–993, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP associating each production p of a grammar with a function g that rearranges the string components in the tuples generated by the"
P09-1111,N03-1021,0,0.0566465,"Missing"
P09-1112,E91-1005,0,0.224501,"languages generated by TT-MCTAG is included in PTIME. We provide a positive answer to this question, using a new characterization of TTMCTAG. 1 Introduction For a large range of linguistic phenomena, extensions of Tree Adjoining Grammars (Joshi et al., 1975), or TAG for short, have been proposed based on the idea of separating the contribution of a lexical item into several components. Instead of single trees, these grammars contain (multi-)sets of trees. Examples are tree-local and set-local multicomponent TAG (Joshi, 1985; Weir, 1988), MCTAG for short, non-local MCTAG with dominance links (Becker et al., 1991), Vector-TAG with dominance links (Rambow, 1994) and, more recently, Tree-Tuple MCTAG with Shared Nodes (Lichte, 2007)), or TT-MCTAG for short. For some of the above formalisms the word recognition problem is NP-hard. This has been shown for non-local MCTAG (Rambow and Satta, 1992), even in the lexicalized case (Champollion, 2007). Some others generate only polynomial languages but their generative capacity is too limited to deal with all natural language phenomena. This has been argued for tree-local and even set-local MCTAG on the basis of scrambling data from lan994 Proceedings of the 47th"
P09-1112,W08-2310,0,0.021556,"the nature of these phenomena is not sufficiently well-understood. Note that, in contrast to non-local MCTAG, in TT-MCTAG the trees coming from the same instance of a tuple in the grammar are not required to be added at the same time. TT-MCTAGs share this property of ‘non-simultaneity’ with other vector grammars such as Unordered Vector Grammars (Cremers and Mayer, 1973) and VectorTAG (Rambow, 1994), V-TAG for short, and it 997 is crucial for the polynomial parsing algorithm. The non-simultaneity seems to be an advantage when using synchronous grammars to model the syntax-semantics interface (Nesson and Shieber, 2008). The closest formalism to TT-MCTAG is V-TAG. However, there are fundamental differences between the two. Firstly, they make a different use of dominance links: In V-TAG dominance links relate different nodes in the trees of a tree set from the grammar. They present dominance requirements that constrain the derived tree. In TT-MCTAG, there are no dominance links between nodes in elementary trees. Instead, the node of a head tree in the derivation tree must dominate all its arguments. Furthermore, even though TT-MCTAG arguments can adjoin with a delay to their head, their possible adjunction si"
P09-1112,J05-2003,1,0.878631,"all of the remaining trees in the set function as arguments of the head. Furthermore, in a TT-MCTAG derivation the argument trees must either adjoin directly to their head tree, or they must be linked in the derivation tree to an elementary tree that attaches to the head tree, by means of a chain of adjunctions at root nodes. In other words, in the corresponding TAG derivation tree, the head tree must dominate the argument trees in such a way that all positions on the path between them, except the first one, must be labeled by ε. This captures the notion of adjunction under node sharing from (Kallmeyer, 2005).2 For a given argument tree β in Γ, h(β) denotes the head of β in Γ. For a given γ ∈ I∪A, a(γ) denotes the set of argument trees of γ, if there are any, or the empty set otherwise. Furthermore, for a given TT-MCTAG G, H(G) is the set of head trees and A(G) is the set of argument trees. Finally, a node v in a derivation tree for G with Lab(v) = γ is called a γ-node. Definition 3 Let G = (VN , VT , S, I, A, T ) be some TT-MCTAG. A derivation tree D = hV, E, ri in the underlying TAG GT is licensed in G if and only if the following conditions (MC) and (SN-TTL) are both satisfied. • (MC): For all"
P09-1112,W08-2308,1,0.76032,"rees of a tree set from the grammar. They present dominance requirements that constrain the derived tree. In TT-MCTAG, there are no dominance links between nodes in elementary trees. Instead, the node of a head tree in the derivation tree must dominate all its arguments. Furthermore, even though TT-MCTAG arguments can adjoin with a delay to their head, their possible adjunction site is restricted with respect to their head. As a result, one obtains a slight degree of locality that can be exploited for natural language phenomena that are unbounded only in a limited domain. This is proposed in (Lichte and Kallmeyer, 2008) where the fact that substitution nodes block argument adjunction to higher heads is used to model the limited domain of scrambling in German. V-TAG does not have any such notion of locality. Instead, it uses explicit constraints, so-called integrity constraints, to establish islands. 3.2 An alternative characterization of TT-MCTAG The definition of TT-MCTAG in subsection 3.1 is taken from (Lichte, 2007; Kallmeyer and Parmentier, 2008). The condition (SN-TTL) on the TAG derivation tree is formulated in terms of heads and arguments belonging together, i.e., coming from the same tuple instance."
P09-1112,P85-1011,0,0.700769,"Missing"
P09-1112,P87-1015,0,0.481062,"ted by the desire to separate the multicomponent property that TTMCTAG shares with a range of related formalisms (e.g., tree-local and set-local MCTAG, VectorTAG, etc.) from the notion of tree-locality with shared nodes that is peculiar to TT-MCTAG. Figure 2 shows a TT-MCTAG derivation for (1). Here, the NPnom auxiliary tree adjoins directly to versucht (its head) while the NPacc tree adjoins to the root of a tree that adjoins to the root of a tree that adjoins to reparieren. TT-MCTAG can generate languages that, in a strong sense, cannot be generated by Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987; Weir, 1988), or LCFRS for short. An example is the language of all strings π(n[1] . . . n[m] )v [1] . . . v [m] with m ≥ 1, π a permutation, and n[i] = n is a nominal argument of v [i] = v for 1 ≤ i ≤ m, i.e., these occurrences come from the same tree set in the grammar. Such a language has been proposed as an abstract description of the scrambling phenomenon as found in German and other free word order languages, Definition 2 A TT-MCTAG is a tuple G = (VN , VT , S, I, A, T ) where GT = (VN , VT , S, I, A) is an underlying TAG and T is a finite set of tree tuples of the form Γ = hγ, {β1 , ."
P09-1112,H86-1020,0,\N,Missing
P10-1054,P92-1012,1,0.694915,"Giorgio Satta Department of Information Engineering University of Padua, Italy satta@dei.unipd.it adjoining languages, LCFRSs with f = 2 can also generate languages in which pair of strings derived from different nonterminals appear in socalled crossing configurations. It has recently been observed that, in this way, LCFRSs with f = 2 can model the vast majority of data in discontinuous phrase structure treebanks and non-projective dependency treebanks (Maier and Lichte, 2009; Kuhlmann and Satta, 2009). Under a theoretical perspective, the parsing problem for LCFRSs with f = 2 is NP-complete (Satta, 1992), and in known parsing algorithms the running time is exponentially affected by the rank r of the grammar. Nonetheless, in natural language parsing applications, it is possible to achieve efficient, polynomial parsing if we succeed in reducing the rank r (number of nonterminals in the right-hand side) of individual LCFRSs’ productions (Kuhlmann and Satta, 2009). This process is called production factorization. Production factorization is very similar to the reduction of a context-free grammar production into Chomsky normal form. However, in the LCFRS case some productions might not be reducibl"
P10-1054,P87-1015,0,0.922266,"Missing"
P10-1054,N10-1118,0,0.267133,"Missing"
P10-1054,P09-1111,1,0.748599,"Missing"
P10-1054,N09-1061,1,0.734354,"Missing"
P10-1054,E09-1055,1,\N,Missing
P10-1054,P92-1018,0,\N,Missing
P10-1055,T75-2001,0,0.544577,"ages. Two things are worth noting. First, our result shows that the ability of CCG to generate non-context-free languages does not hinge on the availability of substitution and type-raising rules: The derivations of G1 only use generalized compositions. Neither does it require the use of functional argument categories: The grammar G1 is first-order in the sense of Koller and Kuhlmann (2009). Proof. To see the inclusion, it suffices to note that pure CCG when restricted to application rules is the same as AB-grammar, the classical categorial formalism investigated by Ajdukiewicz and BarHillel (Bar-Hillel et al., 1964). This formalism is weakly equivalent to context-free grammar. Second, it is important to note that if the composition degree n is restricted to 0 or 1, pure CCG actually collapses to context-free expressive power. This is clear for n D 0 because of the equivalence to AB grammar. For n D 1, observe that the arity of the result of a composition is at most as high as 3.1 CFG ¨ PCCG 537 that of each premise. This means that the arity of any derived category is bounded by the maximal arity of lexical categories in the grammar, which together with Lemma 1 implies that there is only a finite set of"
P10-1055,C04-1180,0,0.0341993,"t-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, w"
P10-1055,J07-4004,0,0.0370889,"of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is"
P10-1055,P96-1011,0,0.391099,"ct the application of individual rules. This means that these formalisms cannot be fully lexicalized, in the sense that certain languages can only be described by selecting language-specific rules. Our result generalizes Koller and Kuhlmann’s (2009) result for pure first-order CCG. Our proof is not as different as it looks at first glance, as their construction of mapping a CCG derivation to a valency tree and back to a derivation provides a different transformation on derivation trees. Our transformation is also technically related to the normal form construction for CCG parsing presented by Eisner (1996). Of course, at the end of the day, the issue that is more relevant to computational linguistics than a formalism’s ability to generate artificial languages such as L3 is how useful it is for modeling natural languages. CCG, and multi-modal CCG in particular, has a very good track record for this. In this sense, our formal result can also be understood as a contribution to a discussion about the expressive power that is needed to model natural languages. Acknowledgments We have profited enormously from discussions with Jason Baldridge and Mark Steedman, and would also like to thank the anonymo"
P10-1055,P02-1043,0,0.0156084,"G is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The gener"
P10-1055,W08-2306,0,0.193753,"nguages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-free grammar can. The discussion of the (weak and strong) generative capacity of CCG and TAG has recently been revived (Hockenmaier and Young, 2008; Koller and Kuhlmann, 2009). In particular, Koller and Kuhlmann (2009) have shown that CCGs that are pure (i.e., they can only use generalized composition rules, and there is no way to restrict the instances of these rules that may be used) and first-order (i.e., all argument categories are atomic) can not generate an b n c n . This shows that the generative capacity of at least first-order CCG crucially relies on its ability to restrict rule instantiations, and is at odds with the general conception of CCG as a fully lexicalized formalism, in which all grammars use one and the same set of un"
P10-1055,E09-1053,1,0.930194,"rsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-free grammar can. The discussion of the (weak and strong) generative capacity of CCG and TAG has recently been revived (Hockenmaier and Young, 2008; Koller and Kuhlmann, 2009). In particular, Koller and Kuhlmann (2009) have shown that CCGs that are pure (i.e., they can only use generalized composition rules, and there is no way to restrict the instances of these rules that may be used) and first-order (i.e., all argument categories are atomic) can not generate an b n c n . This shows that the generative capacity of at least first-order CCG crucially relies on its ability to restrict rule instantiations, and is at odds with the general conception of CCG as a fully lexicalized formalism, in which all grammars use one and the same set of universal rules. A question th"
P10-1055,W06-1637,0,0.0149884,"ammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-fr"
P10-1055,P88-1034,0,0.702937,"string w is called the yield of the resulting derivation tree. A derivation tree is complete, if the last category is the final category of G. The language generated by G, denoted by L.G/, is formed by the yields of all complete derivation trees. 2.4 Degree Restrictions Work on CCG generally assumes an upper bound on the degree of composition rules that can be used in derivations. We also employ this restriction, and only consider grammars with compositions of some bounded (but arbitrary) degree n  0.1 CCG with unbounded-degree compositions is more expressive than bounded-degree CCG or TAG (Weir and Joshi, 1988). Bounded-degree grammars have a number of useful properties, one of which we mention here. The following lemma rephrases Lemma 3.1 in Vijay-Shanker and Weir (1994). Lemma 2 For every grammar G, there is a finite number of categories that can occur as secondary premises in derivations of G. Proof. The arity of a secondary premise c can be written as m C n, where m is the arity of the first argument of the corresponding primary premise, and n is the degree of the rule applied. Since each argument is an argument of some lexical category of G (Lemma 1), and since n is assumed to be bounded, both"
P10-1055,E03-1036,0,\N,Missing
P11-1046,N07-1019,1,0.914196,"Missing"
P11-1046,P06-2036,1,0.855315,"iven strategies for an LCFRS production with a head and n modifiers. Choosing among these possible strategies affects both the time and the space complexity of parsing. In this paper we have shown that optimizing the choice according to either metric is NP-hard. To our knowledge, our results are the first NP-hardness results for a grammar factorization problem. SCFGs and STAGs are specific instances of LCFRSs. Grammar factorization for synchronous models is an important component of current machine translation systems (Zhang et al., 2006), and algorithms for factorization have been studied by Gildea et al. (2006) for SCFGs and by Nesson et al. (2008) for STAGs. These algorithms do not result in what we refer as head-driven strategies, although, as machine translation systems improve, lexicalized rules may become important in this setting as well. However, the results we have presented in this paper do not carry over to the above mentioned synchronous models, since the fan-out of these models is bounded by two, while in our reductions in Section 3 we freely use unbounded values for this parameter. Thus the computational complexity of optimizing the choice of the parsing strategy for SCFGs is still an o"
P11-1046,N10-1118,1,0.786484,"w recent papers have investigated this trade-off taking gen450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 450–459, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics eral LCFRS rules as input. G´omez-Rodr´ıguez et al. (2009) present an algorithm for binarization of LCFRSs while keeping fan-out as small as possible. The algorithm is exponential in the resulting fan-out, and G´omez-Rodr´ıguez et al. (2009) mention as an important open question whether polynomialtime algorithms to minimize fan-out are possible. Gildea (2010) presents a related method for binarizing rules while keeping the time complexity of parsing as small as possible. Binarization turns out to be possible with no penalty in time complexity, but, again, the factorization algorithm is exponential in the resulting time complexity. Gildea (2011) shows that a polynomial time algorithm for factorizing LCFRSs in order to minimize time complexity would imply an improved approximation algorithm for the well-studied graph-theoretic property known as treewidth. However, whether the problem of factorizing LCFRSs in order to minimize time complexity is NP-h"
P11-1046,J11-1008,1,0.931244,"ıguez et al. (2009) present an algorithm for binarization of LCFRSs while keeping fan-out as small as possible. The algorithm is exponential in the resulting fan-out, and G´omez-Rodr´ıguez et al. (2009) mention as an important open question whether polynomialtime algorithms to minimize fan-out are possible. Gildea (2010) presents a related method for binarizing rules while keeping the time complexity of parsing as small as possible. Binarization turns out to be possible with no penalty in time complexity, but, again, the factorization algorithm is exponential in the resulting time complexity. Gildea (2011) shows that a polynomial time algorithm for factorizing LCFRSs in order to minimize time complexity would imply an improved approximation algorithm for the well-studied graph-theoretic property known as treewidth. However, whether the problem of factorizing LCFRSs in order to minimize time complexity is NP-hard is still an open question in the above works. Similar questions have arisen in the context of machine translation, as the SCFGs used to model translation are also instances of LCFRSs, as already mentioned. For SCFG, Satta and Peserico (2005) showed that the exponent in the time complexi"
P11-1046,N09-1061,1,0.878802,"Missing"
P11-1046,N10-1035,1,0.838515,"Missing"
P11-1046,J09-4009,1,0.873338,"Ss in order to minimize time complexity is NP-hard is still an open question in the above works. Similar questions have arisen in the context of machine translation, as the SCFGs used to model translation are also instances of LCFRSs, as already mentioned. For SCFG, Satta and Peserico (2005) showed that the exponent in the time complexity of parsing algorithms must grow at least as fast as the square root of the rule rank, and Gildea and ˇ Stefankoviˇ c (2007) tightened this bound to be linear in the rank. However, neither paper provides an algorithm for finding the best parsing strategy, and Huang et al. (2009) mention that whether finding the optimal parsing strategy for an SCFG rule is NPhard is an important problem for future work. In this paper, we investigate the problem of rule binarization for LCFRSs in the context of headdriven parsing strategies. Head-driven strategies begin with one rhs symbol, and add one nonterminal at a time. This rules out any factorization in which two subsets of nonterminals of size greater than one are combined in a single step. Head-driven strategies allow for the techniques of lexicalization and Markovization that are widely used in (projective) statistical parsin"
P11-1046,C10-1061,0,0.219426,"hankar et al., 1987) constitute a very general grammatical formalism which subsumes contextfree grammars (CFGs) and tree adjoining grammars (TAGs), as well as the synchronous context-free grammars (SCFGs) and synchronous tree adjoining grammars (STAGs) used as models in machine translation.1 LCFRSs retain the fundamental property of CFGs that grammar nonterminals rewrite independently, but allow nonterminals to generate discontinuous phrases, that is, to generate more than one span in the string being produced. This important feature has been recently exploited by Maier and Søgaard (2008) and Kallmeyer and Maier (2010) for modeling phrase structure treebanks with discontinuous constituents, and by Kuhlmann and Satta (2009) for modeling non-projective dependency treebanks. The rules of a LCFRS can be analyzed in terms of the properties of rank and fan-out. Rank is the 1 To be more precise, SCFGs and STAGs generate languages composed by pair of strings, while LCFRSs generate string languages. We can abstract away from this difference by assuming concatenation of components in a string pair. number of nonterminals on the right-hand side (rhs) of a rule, while fan-out is the number of spans of the string genera"
P11-1046,E09-1055,1,0.894676,"FGs) and tree adjoining grammars (TAGs), as well as the synchronous context-free grammars (SCFGs) and synchronous tree adjoining grammars (STAGs) used as models in machine translation.1 LCFRSs retain the fundamental property of CFGs that grammar nonterminals rewrite independently, but allow nonterminals to generate discontinuous phrases, that is, to generate more than one span in the string being produced. This important feature has been recently exploited by Maier and Søgaard (2008) and Kallmeyer and Maier (2010) for modeling phrase structure treebanks with discontinuous constituents, and by Kuhlmann and Satta (2009) for modeling non-projective dependency treebanks. The rules of a LCFRS can be analyzed in terms of the properties of rank and fan-out. Rank is the 1 To be more precise, SCFGs and STAGs generate languages composed by pair of strings, while LCFRSs generate string languages. We can abstract away from this difference by assuming concatenation of components in a string pair. number of nonterminals on the right-hand side (rhs) of a rule, while fan-out is the number of spans of the string generated by the nonterminal in the lefthand side (lhs) of the rule. CFGs are equivalent to LCFRSs with fan-out"
P11-1046,P08-1069,1,0.908781,"n with a head and n modifiers. Choosing among these possible strategies affects both the time and the space complexity of parsing. In this paper we have shown that optimizing the choice according to either metric is NP-hard. To our knowledge, our results are the first NP-hardness results for a grammar factorization problem. SCFGs and STAGs are specific instances of LCFRSs. Grammar factorization for synchronous models is an important component of current machine translation systems (Zhang et al., 2006), and algorithms for factorization have been studied by Gildea et al. (2006) for SCFGs and by Nesson et al. (2008) for STAGs. These algorithms do not result in what we refer as head-driven strategies, although, as machine translation systems improve, lexicalized rules may become important in this setting as well. However, the results we have presented in this paper do not carry over to the above mentioned synchronous models, since the fan-out of these models is bounded by two, while in our reductions in Section 3 we freely use unbounded values for this parameter. Thus the computational complexity of optimizing the choice of the parsing strategy for SCFGs is still an open problem. Finally, our results for"
P11-1046,P10-1054,1,0.771437,"e and space complexity that are dependent on the rank and fan-out of the grammar rules. Whenever it is possible, binarization of LCFRS rules, or reduction of rank to two, is therefore important for parsing, as it reduces the time complexity needed for dynamic programming. This has lead to a number of binarization algorithms for LCFRSs, as well as factorization algorithms that factor rules into new rules with smaller rank, without necessarily reducing rank all the way to two. Kuhlmann and Satta (2009) present an algorithm for binarizing certain LCFRS rules without increasing their fan-out, and Sagot and Satta (2010) show how to reduce rank to the lowest value possible for LCFRS rules of fan-out two, again without increasing fan-out. G´omez-Rodr´ıguez et al. (2010) show how to factorize well-nested LCFRS rules of arbitrary fan-out for efficient parsing. In general there may be a trade-off required between rank and fan-out, and a few recent papers have investigated this trade-off taking gen450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 450–459, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics eral LCFRS rules as input."
P11-1046,H05-1101,1,0.86926,"lgorithm is exponential in the resulting time complexity. Gildea (2011) shows that a polynomial time algorithm for factorizing LCFRSs in order to minimize time complexity would imply an improved approximation algorithm for the well-studied graph-theoretic property known as treewidth. However, whether the problem of factorizing LCFRSs in order to minimize time complexity is NP-hard is still an open question in the above works. Similar questions have arisen in the context of machine translation, as the SCFGs used to model translation are also instances of LCFRSs, as already mentioned. For SCFG, Satta and Peserico (2005) showed that the exponent in the time complexity of parsing algorithms must grow at least as fast as the square root of the rule rank, and Gildea and ˇ Stefankoviˇ c (2007) tightened this bound to be linear in the rank. However, neither paper provides an algorithm for finding the best parsing strategy, and Huang et al. (2009) mention that whether finding the optimal parsing strategy for an SCFG rule is NPhard is an important problem for future work. In this paper, we investigate the problem of rule binarization for LCFRSs in the context of headdriven parsing strategies. Head-driven strategies"
P11-1046,P87-1015,0,0.882354,"tta Dip. di Ingegneria dell’Informazione Universit`a di Padova Abstract We study the problem of finding the best headdriven parsing strategy for Linear ContextFree Rewriting System productions. A headdriven strategy must begin with a specified righthand-side nonterminal (the head) and add the remaining nonterminals one at a time in any order. We show that it is NP-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing. 1 Andrea Marino Dip. di Sistemi e Informatica Universit`a di Firenze Introduction Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shankar et al., 1987) constitute a very general grammatical formalism which subsumes contextfree grammars (CFGs) and tree adjoining grammars (TAGs), as well as the synchronous context-free grammars (SCFGs) and synchronous tree adjoining grammars (STAGs) used as models in machine translation.1 LCFRSs retain the fundamental property of CFGs that grammar nonterminals rewrite independently, but allow nonterminals to generate discontinuous phrases, that is, to generate more than one span in the string being produced. This important feature has been recently exploited by Maier and Søgaard (2008) and Kallmeyer and Maier"
P11-1046,N06-1033,1,0.817626,"to parsers with discontinuous spans. However, there are n! possible head-driven strategies for an LCFRS production with a head and n modifiers. Choosing among these possible strategies affects both the time and the space complexity of parsing. In this paper we have shown that optimizing the choice according to either metric is NP-hard. To our knowledge, our results are the first NP-hardness results for a grammar factorization problem. SCFGs and STAGs are specific instances of LCFRSs. Grammar factorization for synchronous models is an important component of current machine translation systems (Zhang et al., 2006), and algorithms for factorization have been studied by Gildea et al. (2006) for SCFGs and by Nesson et al. (2008) for STAGs. These algorithms do not result in what we refer as head-driven strategies, although, as machine translation systems improve, lexicalized rules may become important in this setting as well. However, the results we have presented in this paper do not carry over to the above mentioned synchronous models, since the fan-out of these models is bounded by two, while in our reductions in Section 3 we freely use unbounded values for this parameter. Thus the computational complex"
P11-1046,P97-1003,0,\N,Missing
P11-1047,P99-1070,0,0.699695,"−1 for each j (1 ≤ j ≤ r). Consider again a synchronous rule s of the form in (2). We say s is an epsilon rule if r = 0 and u10 = u20 = . We say s is a unit rule if r = 1 and u10 = u11 = u20 = u21 = . Similarly to context-free grammars, absence of epsilon rules and unit rules guarantees that there are no cyclic dependencies between items and in this case the inside algorithm correctly computes pG ([w1 , w2 ]). Epsilon rules can be eliminated from PSCFGs by a grammar transformation that is very similar to the transformation eliminating epsilon rules from a probabilistic context-free grammar (Abney et al., 1999). This is sketched in what follows. We first compute the set of all nullable linked pairs of nonterminals of the underlying SCFG, that is, the set of all [A1 , A2 ] ∈ N [2] such that [A11 , A21 ] ⇒∗G [ε, ε]. This can be done in linear time O(|G|) using essentially the same algorithm that identifies nullable nonterminals in a context-free grammar, as presented for instance by Sippu and Soisalon-Soininen (1988). Next, we identify all occurrences of nullable pairs [A1 , A2 ] in the right-hand side components of a rule s, such that A1 and A2 have the same index. For every possible choice of a subs"
P11-1047,J99-1004,0,0.0447973,"veral syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it This paper considers a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix s"
P11-1047,J07-2003,0,0.0608215,"owing interest in so-called syntaxbased translation models, that is, models that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy sa"
P11-1047,W02-1020,0,0.0203238,"an be computed as a special case of the joint prefix probability. Concretely, one can extend the input and the grammar by introducing an end-of-sentence marker $. Let G0 be the underlying SCFG grammar after the extension. Then: pr−prefix ([v1 , v2 ]) = pprefix ([v1 $, v2 ]) G G0 Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation of speech, or alternatively as a predictive tool in applications of interactive machine translation, of the kind described by Foster et al. (2002). We provide some technical details here, generalizing to PSCFGs the approach by Jelinek and Lafferty (1991). Let G = (G, pG ) be a PSCFG, with Σ the alphabet of terminal symbols. We are interested in the probability that the next terminal in the target translation is a ∈ Σ, after having processed a prefix v1 of the source sentence and having produced a prefix v2 468 of the target translation. This can be computed as: pr−word (a |[v1 , v2 ]) = G pprefix ([v1 , v2 a]) G pprefix ([v1 , v2 ]) G Two considerations are relevant when applying the above formula in practice. First, the computation of"
P11-1047,N04-1035,0,0.117483,"Missing"
P11-1047,N07-1019,0,0.0172894,"component. By standard complexity analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1 |rmax +1 · |w2 |rmax +1 ), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes pG ([w1 , w2 ]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic idea, let us first assume that each it"
P11-1047,D10-1063,0,0.0186147,"analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1 |rmax +1 · |w2 |rmax +1 ), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes pG ([w1 , w2 ]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic idea, let us first assume that each item can be inferred in finitely m"
P11-1047,J91-3004,0,0.748813,"ders a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the form vw, for any string w over the alphabet of the model. This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech. This has applications in incremental processing of text or speech from left to right; see again (Jelinek and Lafferty, 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search (Corazza et al., 1991). This paper investigates the problem of computing prefix probabilities for PSCFGs. In this context, a pair of strings v1 and v2 is given as input, and we are asked to compute the probability that any st"
P11-1047,H05-1101,1,0.890375,"or the computation of inside probabilities for PSCFG. This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. Our method for computing the prefix probabilities for PSCFGs runs in exponential time, since that is the running time of existing methods for computing the inside probabilities for PSCFGs. It is unlikely this can be improved, because the recognition problem for PSCFG is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. 2 by annotating nonterminals with indices from an infinite set. We define I(N ) = {A t |A ∈ N, t ∈ N} and VI = I(N ) ∪ Σ. For a string γ ∈ VI∗ , we write index(γ) to denote the set of all indices that appear in symbols in γ. Two strings γ1 , γ2 ∈ VI∗ are synchronous if each index from N occurs at most once in γ1 and at most once in γ2 , and index(γ1 ) = index(γ2 ). Therefore γ1 , γ2 have the general form: t t t γ1 = u10 A111 u11 A122 u12 · · · u1r−"
P11-1047,J95-2002,0,0.658593,"understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the form vw, for any string w over the alphabet of the model. This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech. This has applications in incremental processing of text or speech from left to right; see again (Jelinek and Lafferty, 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search (Corazza et al., 1991). This paper investigates the problem of computing prefix probabilities for PSCFGs. In this context, a pair of strings v1 and v2 is given as input, and we are asked to compute the probability that any string in the source lan"
P11-1047,J97-3002,0,0.205746,"area of statistical machine translation, there has been a growing interest in so-called syntaxbased translation models, that is, models that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. Giorgio Satta Dept. of Information Engineeri"
P11-1047,N06-1033,0,0.0224984,"in the introduction, since the recognition problem for PSCFGs is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. One should add that, in real world machine translation applications, it has been observed that recognition (and computation of inside probabilities) for SCFGs can typically be carried out in low-degree polynomial time, and the worst cases mentioned above are not observed with real data. Further discussion on this issue is due to Zhang et al. (2006). 5 Discussion We have shown that the computation of joint prefix probabilities for PSCFGs can be reduced to the computation of inside probabilities for the same model. Our reduction relies on a novel grammar transformation, followed by elimination of epsilon rules and unit rules. Next to the joint prefix probability, we can also consider the right prefix probability, which is defined by: pr−prefix ([v1 , v2 ]) = G X pG ([v1 , v2 w]) w In words, the entire left string is given, along with a prefix of the right string, and the task is to sum the probabilities of all string pairs for different s"
P11-1068,W06-2922,0,0.039634,"e transition. It is called complete whenever c0 D I.w/, and cm 2 C t . We note that a computation can be uniquely specified by its initial configuration c0 and the sequence of its transitions, understood as a string over T . Complete computations, where c0 is fixed, can be specified by their transition sequences alone. 3 Arc-Standard Model To introduce the core concepts of the paper, we first look at a particularly simple model for transitionbased dependency parsing, known as the arc-standard model. This model has been used, in slightly different variants, by a number of parsers (Nivre, 2004; Attardi, 2006; Huang and Sagae, 2010). 3.1 Transition System The arc-standard model uses three types of transitions: Shift (sh) removes the first node in the buffer and pushes it to the stack. Left-Arc (la) creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the second-topmost node from the stack. Right-Arc (ra) is symmetric to Left-Arc in that it creates an arc with the second-topmost node as the head and the topmost node as the dependent, and removes the topmost node. The three transitions can be formally specified as in Figure 1. The"
P11-1068,P89-1018,0,0.0650338,"es where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eage"
P11-1068,P96-1025,0,0.0250951,"ntermediate configuration, and thereby violates property P1. 3.5 Discussion Let us briefly take stock of what we have achieved so far. We have provided a deduction system capable of tabulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.jwj3 / and time in O.jwj5 /. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.jwj4 /. 4 Adding Features 1 are both push computations with strictly fewer transitions than . Suppose that the last transition in is ra. In this case, ˇ.c/ D ˇk for some i < k < j , .c/ D .c0 /jh with h < k, ˇ.cm 1 / D ˇj , and .cm 1 / D  .c0 /jhjh0 for some k  h0 < j . B"
P11-1068,P99-1059,1,0.293239,"bulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.jwj3 / and time in O.jwj5 /. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.jwj4 /. 4 Adding Features 1 are both push computations with strictly fewer transitions than . Suppose that the last transition in is ra. In this case, ˇ.c/ D ˇk for some i < k < j , .c/ D .c0 /jh with h < k, ˇ.cm 1 / D ˇj , and .cm 1 / D  .c0 /jhjh0 for some k  h0 < j . By induction, we may assume that we have generated items Œi; h; k and Œk; h0 ; j . Applying the inference 676 The main goal with the tabulation of transition-based dependency parsers is to obta"
P11-1068,P08-1110,1,0.950177,"s: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities bet"
P11-1068,J99-4004,0,0.0177162,"ming algorithms, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic progra"
P11-1068,P10-1110,0,0.0834865,"ons in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper"
P11-1068,D09-1127,0,0.0208373,"Missing"
P11-1068,D09-1005,0,0.0163563,", also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a"
P11-1068,W03-3017,0,0.0554594,"s that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of featur"
P11-1068,W04-0308,0,0.783918,"Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008"
P11-1068,J08-4003,0,0.774527,"polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time"
P11-1068,W03-3023,0,0.0736894,"stem takes space O.jwj2 / and time O.jwj3 /. In the original interpretation of the deduction system, an item Œi; j  asserts the existence of a pair of (projective) dependency trees: the first tree rooted at token wi , having all nodes in the substring wi    wk 1 as descendants, where i < k  j ; and the second tree rooted at token wj , having all nodes in the substring wk    wj as descendants. (Note that we use fencepost indexes, while Gómez-Rodríguez et al. (2008) indexes positions.) Deduction System Gómez-Rodríguez et al. (2008) present a deductive version of the dependency parser of Yamada and Matsumoto (2003); their deduction system is given in Fig680 .ji jj; ˇ; A/ ` .ji; ˇ; A [ fi ! j g/ .ra/ We call this transition system the hybrid model, as sh and ra are just like in arc-standard, while lah is like the Left-Arc transition in the arc-eager model (lae ), except that it does not have the precondition. Like the arc-standard but unlike the arc-eager model, the hybrid model builds dependencies bottom-up. 7 Conclusion In this paper, we have provided a general technique for the tabulation of transition-based dependency parsers, and applied it to obtain dynamic programming algorithms for two widely-u"
P11-1068,D08-1059,0,0.415993,"the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our alg"
P12-2058,J07-2003,0,0.0970395,"beam size. Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algor"
P12-2058,P10-4002,0,0.114003,"Missing"
P12-2058,2010.iwslt-papers.8,1,0.817047,"best solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and performance of a real-world machine translation system. Let L = hx0 , . . . , xk−1 i be a list over R, that is, an ordered sequence of real numbers, possibly with repetitions. We write |L |= k to denote the length of L. We say that L is descending if xi ≥ xj for every i, j with 0 ≤ i < j < k. Let L1 = hx0 , . . . , xk−1 i and L2 = hy0 , . . . , yk′ −1 i be two descending lists ove"
P12-2058,W11-2123,0,0.110797,"Missing"
P12-2058,W05-1506,0,0.257813,"chine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and per"
P12-2058,P07-1019,0,0.0210545,"irically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in t"
P12-2058,2005.mtsummit-papers.11,0,0.0634399,"Missing"
P12-2058,D07-1104,0,0.0523893,"Missing"
P12-2058,P03-1021,0,0.0446584,"Missing"
P12-2058,N09-2036,0,0.0214687,"in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate it"
P12-2058,P10-1017,0,0.0180403,"accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and performance of a real-world machine translation"
P13-1014,W06-2922,0,0.033519,"he increased availability of dependency treebanks and the perceived usefulness of dependency structures as an interface to downstream applications, but a very important reason is also the high efficiency offered by dependency parsers, enabling web-scale parsing with high throughput. The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. However, while these parsers are capable of processing tens of thousands of tokens per second with the right choice of classifiers, they are also known to perform slightly below the state-ofthe-art because of search errors and subsequent error propagation (McDonald and Nivre, 2007), and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the entire sentence"
P13-1014,J13-1002,1,0.531399,"Missing"
P13-1014,D12-1133,1,0.832693,"cent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the entire sentence instead of a locally trained classifier that tries to maximize the accuracy of single decisions (given no previous errors), as first proposed by Zhang and Clark (2008). With these methods, transition-based parsers have reached state-of-the-art accuracy for a number of languages (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). However, the drawback with this approach is that parsing speed is proportional to the size of the beam, which means that the most accurate transition-based parsers are not nearly as fast as the original greedy transition-based parsers. Another line of research tries to retain the efficiency of greedy classifier-based parsing by instead improving the way in which classifiers are learned from data. While the classical approach limits training data to parser states that result from oracle predictions (derived from a treebank), these novel approaches allow the classifier to explore states that r"
P13-1014,P11-2121,0,0.0182271,"sing speed is proportional to the size of the beam, which means that the most accurate transition-based parsers are not nearly as fast as the original greedy transition-based parsers. Another line of research tries to retain the efficiency of greedy classifier-based parsing by instead improving the way in which classifiers are learned from data. While the classical approach limits training data to parser states that result from oracle predictions (derived from a treebank), these novel approaches allow the classifier to explore states that result from its own (sometimes erroneous) predictions (Choi and Palmer, 2011; Goldberg and Nivre, 2012). In this paper, we explore an orthogonal approach to improving the accuracy of transition-based parsers, without sacrificing their advantage in efficiency, by introducing a new type of transition system. While all previous transition systems assume a static parsing strategy with respect to top-down and bottom-up processing, our new system allows a dynamic strategy for ordering parsing decisions. This has the advantage that the parser can postpone difficult decisions until the relevant information becomes available, in a way that is not possible in existing transitio"
P13-1014,P02-1034,0,0.0237564,"e c with S HIFT p ← length of left spine of σ1 s ← length of right spine of σ2 T ← {lak |k ∈ [1, p]} ∪ {rak |k ∈ [1, s]} ∪ {sh} bestT ← argmaxt∈T score(t, c) bestCorrectT ← argmaxt∈T ∧isCorrect(t) score(t, c) if bestT 6= bestCorrectT then ω ~ ←ω ~ − φ(bestT , c) +φ(bestCorrectT , c) update c with bestCorrectT a feature vector representation for a transition t applying to a configuration c. The function φ will be discussed at length in §4.3. The vector ω ~ is trained using the perceptron algorithm in combination with the averaging method to avoid overfitting; see Freund and Schapire (1999) and Collins and Duffy (2002) for details. The training data set consists of pairs (w, Ag ), where w is a sentence and Ag is the set of arcs of the gold (desired) dependency tree for w. At training time, each pair (w, Ag ) is processed using the learning algorithm described as Algorithm 2. The algorithm is based on the notions of correct and incorrect transitions, discussed at length in §4.2. Algorithm 2 parses w following Algorithm 1 and using the current ω ~ , until the highest score selected transition bestT is incorrect according to Ag . When this happens, ω ~ is updated by decreasing the weights of the features assoc"
P13-1014,de-marneffe-etal-2006-generating,0,0.0118961,"Missing"
P13-1014,N10-1115,0,0.135202,"Missing"
P13-1014,C12-1059,1,0.875507,"nal to the size of the beam, which means that the most accurate transition-based parsers are not nearly as fast as the original greedy transition-based parsers. Another line of research tries to retain the efficiency of greedy classifier-based parsing by instead improving the way in which classifiers are learned from data. While the classical approach limits training data to parser states that result from oracle predictions (derived from a treebank), these novel approaches allow the classifier to explore states that result from its own (sometimes erroneous) predictions (Choi and Palmer, 2011; Goldberg and Nivre, 2012). In this paper, we explore an orthogonal approach to improving the accuracy of transition-based parsers, without sacrificing their advantage in efficiency, by introducing a new type of transition system. While all previous transition systems assume a static parsing strategy with respect to top-down and bottom-up processing, our new system allows a dynamic strategy for ordering parsing decisions. This has the advantage that the parser can postpone difficult decisions until the relevant information becomes available, in a way that is not possible in existing transition systems. A second advanta"
P13-1014,P10-2035,0,0.115835,"ally static) arc-standard strategy, when evaluated on English. The idea of representing the right spine of a tree within the stack elements of a shift-reduce device is quite old in parsing, predating empirical approaches. It has been mainly exploited to solve the PP-attachment problem, motivated by psycholinguistic models. The same representation is also adopted in applications of discourse parsing, where right spines are usually called right frontiers; see for instance Subba and Di Eugenio (2009). In the context of transition-based dependency parsers, right spines have also been exploited by Kitagawa and Tanaka-Ishii (2010) to decide where to attach the next word from the buffer. In this paper we have generalized their approach by introducing the symmetrical notion of left spine, and by allowing attachment of full trees rather than attachment of a single word.2 Since one can regard a spine as a stack in itself, whose elements are tree nodes, our model is reminiscent of the embedded pushdown automata of Schabes and Vijay-Shanker (1990), used to parse tree adjoining grammars (Joshi and Schabes, 1997) and exploiting a stack of stacks. However, by imposing projectivity, we do not use the extra-power of the latter cl"
P13-1014,J93-2004,0,0.043969,"Missing"
P13-1014,D07-1013,1,0.773892,"he most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. However, while these parsers are capable of processing tens of thousands of tokens per second with the right choice of classifiers, they are also known to perform slightly below the state-ofthe-art because of search errors and subsequent error propagation (McDonald and Nivre, 2007), and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the entire sentence instead of a locally trained classifier that tries to maximize the accuracy of single decisions (given no previous errors), as first proposed by Zhang and Clark (2008). With these methods, transition-based parsers have reached state-of-the-art accuracy for a number of languages (Zhang and Nivre"
P13-1014,W03-3017,1,0.815661,"ors, such as the increased availability of dependency treebanks and the perceived usefulness of dependency structures as an interface to downstream applications, but a very important reason is also the high efficiency offered by dependency parsers, enabling web-scale parsing with high throughput. The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. However, while these parsers are capable of processing tens of thousands of tokens per second with the right choice of classifiers, they are also known to perform slightly below the state-ofthe-art because of search errors and subsequent error propagation (McDonald and Nivre, 2007), and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the"
P13-1014,W04-0308,1,0.921029,"only informally, for individual families of grammar formalisms. In the context of dependency parsing, a parsing strategy is called purely bottom-up if every dependency h → d is constructed only after all dependencies of the form d → i have been constructed. Here h → d denotes a dependency with h the head node and d the dependent node. In contrast, a parsing strategy is called purely top-down if h → d is constructed before any dependency of the form d → i. If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). After building a dependency h → d, this model immediately removes from its stack node d, preventing further attachment of dependents to this node. A second popular parser, the arc-eager model of Nivre (2003), instead adopts a mixed strategy. In this model, a dependency h → d is constructed using a purely bottom-up strategy if it represents a left-arc, that is, if the dependent d is placed to the left of the head h in the input string. In contrast, if h → d represents a right-arc (defined symmetrically), then this dependency is constructed before any right-arc d → i (top-down) but after any l"
P13-1014,J08-4003,1,0.94824,"tegies do not have a general mathematical definition; they are instead specified, often only informally, for individual families of grammar formalisms. In the context of dependency parsing, a parsing strategy is called purely bottom-up if every dependency h → d is constructed only after all dependencies of the form d → i have been constructed. Here h → d denotes a dependency with h the head node and d the dependent node. In contrast, a parsing strategy is called purely top-down if h → d is constructed before any dependency of the form d → i. If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). After building a dependency h → d, this model immediately removes from its stack node d, preventing further attachment of dependents to this node. A second popular parser, the arc-eager model of Nivre (2003), instead adopts a mixed strategy. In this model, a dependency h → d is constructed using a purely bottom-up strategy if it represents a left-arc, that is, if the dependent d is placed to the left of the head h in the input string. In contrast, if h → d represents a right-arc (defined symmetrically), t"
P13-1014,P90-1035,0,0.638239,"Missing"
P13-1014,N09-1064,0,0.0141892,"Missing"
P13-1014,W03-3023,0,0.0555796,"is probably due to many factors, such as the increased availability of dependency treebanks and the perceived usefulness of dependency structures as an interface to downstream applications, but a very important reason is also the high efficiency offered by dependency parsers, enabling web-scale parsing with high throughput. The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. However, while these parsers are capable of processing tens of thousands of tokens per second with the right choice of classifiers, they are also known to perform slightly below the state-ofthe-art because of search errors and subsequent error propagation (McDonald and Nivre, 2007), and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the"
P13-1014,D08-1059,0,0.0727333,"they are also known to perform slightly below the state-ofthe-art because of search errors and subsequent error propagation (McDonald and Nivre, 2007), and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the entire sentence instead of a locally trained classifier that tries to maximize the accuracy of single decisions (given no previous errors), as first proposed by Zhang and Clark (2008). With these methods, transition-based parsers have reached state-of-the-art accuracy for a number of languages (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). However, the drawback with this approach is that parsing speed is proportional to the size of the beam, which means that the most accurate transition-based parsers are not nearly as fast as the original greedy transition-based parsers. Another line of research tries to retain the efficiency of greedy classifier-based parsing by instead improving the way in which classifiers are learned from data. While the classical approach limits tra"
P13-1014,P11-2033,1,0.518003,"nd Nivre, 2007), and recent research on transition-based dependency parsing has therefore explored different ways of improving their accuracy. The most common approach is to use beam search instead of greedy decoding, in combination with a globally trained model that tries to minimize the loss over the entire sentence instead of a locally trained classifier that tries to maximize the accuracy of single decisions (given no previous errors), as first proposed by Zhang and Clark (2008). With these methods, transition-based parsers have reached state-of-the-art accuracy for a number of languages (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). However, the drawback with this approach is that parsing speed is proportional to the size of the beam, which means that the most accurate transition-based parsers are not nearly as fast as the original greedy transition-based parsers. Another line of research tries to retain the efficiency of greedy classifier-based parsing by instead improving the way in which classifiers are learned from data. While the classical approach limits training data to parser states that result from oracle predictions (derived from a treebank), these novel approaches allow the classifier"
P18-1171,P17-1183,0,0.0238709,"ossible output. The transition system can also provide better local context information than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data. More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories. We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing. We extend the hard attention model of Aharoni and Goldberg (2017), which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right. When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the input order. On the decoder side, we augment the prediction of output action with embedding features from the current transition state. Our experiments show that encoding information from the transition state significantly improves sequenceto-sequence mod"
P18-1171,D15-1198,0,0.11179,"which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final pe"
P18-1171,D17-1130,0,0.232718,"step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tok"
P18-1171,W13-2322,0,0.138186,"ch decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semanti"
P18-1171,P17-1112,0,0.681502,"currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer. Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack"
P18-1171,P13-2131,0,0.0629132,"subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens. 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categorization, we use Stanford CoreNLP (Mann"
P18-1171,P17-1193,0,0.0167447,", which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that the monotonic hard attention model can"
P18-1171,E17-1051,1,0.937526,"hard attention only attends to one position of the input, it performs slightly better than the soft attention model, while the time complexity is lower. Impact of Different Cache Sizes The cache size of the transition system can be optimized as a trade-off between coverage of AMR graphs and the prediction accuracy. While larger cache size increases the coverage of AMR graphs, it complicates the prediction procedure with more cache decisions to make. From Table 3 we can see that 1848 System Soft Soft+feats Hard+feats P 0.55 0.69 0.70 R 0.51 0.63 0.64 F 0.53 0.66 0.67 System Peng et al. (2018) Damonte et al. (2017) JAMR Ours Table 2: Impact of various components for the sequence-to-sequence model (dev). Cache Size 4 5 6 P 0.69 0.70 0.69 R 0.63 0.64 0.64 F 0.66 0.67 0.66 Table 3: Impact of cache size for the sequenceto-sequence model, hard attention (dev). the hard attention model performs best with cache size 5. The soft attention model also achieves best performance with the same cache size. Comparison with other Parsers Table 4 shows the comparison with other AMR parsers. The first three systems are some competitive neural models. We can see that our parser significantly outperforms the sequence-to-ac"
P18-1171,S15-2154,0,0.0200108,"t if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that"
P18-1171,P15-1033,0,0.0292887,"parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the traini"
P18-1171,S16-1186,0,0.307711,"numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is more than which aligns to a single concept more-than. We first collapse spans and subgraphs into these categories based on the alignment from the JAMR aligner (Flanigan et al., 2014), which greedily aligns a span of words to AMR subgraphs using a set of heuristics. This categorization procedure enables the parser to capture mappings from continuous spans on the sentence side to connected subgraphs on the AMR side. We use the semi-Markov model from Flanigan et al. (2016) as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph. During decoding, our output has categories, and we need to map 4 For example, verbalization of “teacher” as “(person :ARG0-of teach-01)”, or “minister” as “(person :ARG0-of (have-org-role-91 :ARG2 minister))”. 1847 Peng et al. (2018) Soft+feats Hard+feats ShiftOrPop 0.87 0.93 0.94 PushIndex 0.87 0.84 0.85 ArcBinary 0.83 0.91 0.93 ArcLabel 0.81 0.75 0.77 Table 1: Performance breakdown of each transition phase. each category to the corresponding AMR concept or subgraph. We s"
P18-1171,P14-1134,0,0.551749,"rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled te"
P18-1171,J18-1004,1,0.920933,"o transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer. Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack and buffer. Peng et al. (2018) apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues. In this paper, we propose a sequence-to-actionsequence approach for AMR parsing with cache transition systems. We want to take advantage of t"
P18-1171,P16-1025,0,0.0429727,"ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization"
P18-1171,P17-1014,0,0.290237,"arsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), wh"
P18-1171,N15-1114,0,0.0368503,"s and achieves competitive results in comparison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model"
P18-1171,P14-5010,0,0.00734709,"Missing"
P18-1171,S16-1166,0,0.0868203,"0.84 0.85 ArcBinary 0.83 0.91 0.93 ArcLabel 0.81 0.75 0.77 Table 1: Performance breakdown of each transition phase. each category to the corresponding AMR concept or subgraph. We save a table Q which shows the original subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens. 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named"
P18-1171,J08-4003,0,0.0317682,"rent from Peng et al. (2018) in two ways: the PushIndex phase is initiated before making all the arc decisions; the newly introduced concept is placed at the last cache position instead of the leftmost buffer position, which essentially increases the cache size by 1. Given the sentence “John wants to go” and the recognized concept sequence “Per want-01 go-01” (person name category Per for “John”), our cache transition parser can construct the AMR graph shown in Figure 1 using the run shown in Figure 2 with cache size of 3. 2.1 Oracle Extraction Algorithm We use the following oracle algorithm (Nivre, 2008) to derive the sequence of actions that leads to the gold AMR graph for a cache transition parser with cache size m. The correctness of the oracle is shown by Gildea et al. (2018). Let EG be the set of edges of the gold graph G. We maintain the set of vertices that is not yet shifted into the cache as S, which is initialized with all vertices in G. The vertices are ordered according to their aligned position in the word sequence and the unaligned vertices are listed according to their order in the depth-first traversal of the graph. The oracle algorithm can look into 1844 Figure 3: Sequence-to"
P18-1171,S15-2153,0,0.127115,"hat are close and not if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models."
P18-1171,K15-1004,1,0.889524,"ample of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. H"
P18-1171,N15-1040,0,0.330941,". Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out"
P18-1171,J16-3001,0,0.0600652,"tant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that the monotonic hard"
P18-1171,E17-1035,1,0.839076,"and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by"
P18-1171,D14-1162,0,0.0817312,", . . . , a∗t−1 , X; θ), (6) t=1 where X represents the input word and concept sequences, and θ is the model parameters. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best performance on the dev set is selected to evaluate on the test set. Dropout with rate 0.3 is used during training. Beam search with a beam size of 10 is used for decoding. Both training and decoding use a Tesla K20X GPU. Hidden state sizes for both encoder and decoder are set to 100. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. The embeddings for POS tags and features are randomly initialized, with the sizes of 20 and 50, respectively. 4.2 Preprocessing and Postprocessing As the AMR data is very sparse, we collapse some subgraphs or spans into categories based on the alignment. We define some special categories such as named entities (NE), dates (DATE), single rooted subgraphs involving multiple concepts (MULT)4 , numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is mor"
P18-1171,D15-1136,0,0.229429,"esent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still fal"
P18-1171,W09-1119,0,0.0693426,"ntains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categorization, we use Stanford CoreNLP (Manning et al., 2014) to get the POS tags and dependencies of the categorized dataset. We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases. We use a cache size of 5 in our experiments. 5.2 Results Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction histor"
P18-1171,D16-1112,0,0.0456677,"parison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing."
P18-1171,D17-1129,0,0.670836,". Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence s"
P18-1171,P15-2141,0,0.0742167,". Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out"
P92-1012,P87-1015,0,\N,Missing
P92-1012,J93-4002,0,\N,Missing
P94-1029,E93-1010,0,0.171951,"entful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of G r a m m a r (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th"
P94-1029,P89-1017,0,0.366895,"his p a p e r which has already a p p e a r e d in the literature, in different guises [6, 13, 2, 14]. Extended HI parsing T h e P H I a l g o r i t h m can process s i m u l t a n e o u s l y a comm o n infix a in two different rules A --* 131_~-/1 and A --* 132_~72, which reduces n o n d e t e r m i n i s m . We m a y however also specify an a l g o r i t h m which succeeds in s i m u l t a n e o u s l y processing all c o m m o n infixes, irrespective of whether the left-hand sides of the corresponding rules are the same. This a l g o r i t h m is inspired by exlended L R ( E L R ) parsing [12, 7] for extended context-free g r a m m a r s (where right-hand sides consist of regular expressions over V). By analogy, it will be called extended H I (EHI) parsing. This a l g o r i t h m uses yet a n o t h e r kind of item, viz. of the form [ i , k , { A 1 , A ~ , . . . , A p } --* - / , m , j ] , where there exists at least one rule A --* a_713 for each A E {A1,Au,...,Ap}. W i t h such an item, we simulate c o m p u t a t i o n of different items [i, k, A --* a * -/ * 13, m, j] E I He which would be treated individually by an H C parser. Formally, we have Predictive HI parsing We say two rul"
P94-1029,P94-1017,1,0.723882,"ation of left recursion for traditional T D parsing. In the ease of g r a m m a r s with some p a r a m e t e r mechanism, top-down parsing has the advantage over other kinds of parsing that top-down propagation of parameter values is possible in collaboration with context-free parsing (eft the standard evaluation of definite clause grammars), which m a y lead to more efficient processing. This holds for left-to-right parsing as well as for head-driven parsing [10]. A family of head-driven algorithms This section investigates the adaptation of a family of left-to-right parsing algorithms from [8], viz. top-down, left-corner, PLR, ELR, and LR parsing, to head grammars. Top-down parsing The following is a straightforward adaptation of topdown (TD) parsing [1] to head grammars. There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, the other of the form [i, k, A --* a • 7 •/3, m, j], which also indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, but specifically using the rule A --~ ot7/3, where 7 -&apos;-~* ak+x . . . a,n has already been esta"
P94-1029,1993.iwpt-1.16,1,0.793803,"thm 6 on rheaa is twofold. Firstly, the algorithm above is more appropriate for presentational purposes than an alternative sit is interesting to compare LR parsing for a context-free grammar G with LR parsing for the transformed grammar rtwo(G). The transformation has the effect that a reduction with a rule is replaced by a cascade of reductions with smaller rules; apart from this, the transformation does not affect the global run-time behaviour of LR parsing. More serious are the consequences for the size of the parser: the required number of LR states for the transformed grammar is smaller [9]. 215 P•S ~3,0 X3,1 ~3,1 X3,2 of a rule, and then the remaining members of the rhs, in an outward order• Conversely, we have head-inward ( H I ) derivations, where first the remaining m e m b e r s in the rhs are expanded, in an inward order (toward the head), after which the head itself is recursively expanded. Note that HI parsing recognizes a string by computing an HI derivation in reverse (of. Lit parsing). Let w = axa2 • . - a n , n &gt; 1, be a string over T and let a0 = .1_. For - 1 &lt; i &lt; j &lt; n, we write ( i , j ] , , to denote substring ai+ l • • • aj . ~2 T h e o r e m 1 For A one o f A"
P94-1029,H93-1101,0,0.140272,"for some A. Head-driven T D parsing m a y loop exactly for the g r a m m a r s which are head-recursive. Head reeursion is a generalization of left recursion for traditional T D parsing. In the ease of g r a m m a r s with some p a r a m e t e r mechanism, top-down parsing has the advantage over other kinds of parsing that top-down propagation of parameter values is possible in collaboration with context-free parsing (eft the standard evaluation of definite clause grammars), which m a y lead to more efficient processing. This holds for left-to-right parsing as well as for head-driven parsing [10]. A family of head-driven algorithms This section investigates the adaptation of a family of left-to-right parsing algorithms from [8], viz. top-down, left-corner, PLR, ELR, and LR parsing, to head grammars. Top-down parsing The following is a straightforward adaptation of topdown (TD) parsing [1] to head grammars. There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, the other of the form [i, k, A --* a • 7 •/3, m, j], which also indicates that some subderivation from A is needed"
P94-1029,W89-0205,1,0.843449,"entful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of G r a m m a r (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th"
P94-1029,1993.iwpt-1.21,0,0.601823,"entful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of G r a m m a r (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th"
P94-1029,W89-0206,0,\N,Missing
P96-1016,C90-3001,0,0.339326,"Missing"
P96-1016,J94-3001,0,0.031173,"n, and for corresponding algorithms for transduction. Several different transduction systems have been used in the past by the computational and theoretical linguistics communities. These systems have been borrowed from translation theory, a subfield of formal language theory, or have been originally (and sometimes redundantly) developed. Finite state transducers (for an overview, see, e.g., (Aho and Ullman, 1972)) provide translations between regular languages. These devices have been popular in computational morphology and computational phonology since the early eighties (Koskenniemi, 1983; Kaplan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes"
P96-1016,P91-1032,0,0.0285908,"ystems have been used in the past by the computational and theoretical linguistics communities. These systems have been borrowed from translation theory, a subfield of formal language theory, or have been originally (and sometimes redundantly) developed. Finite state transducers (for an overview, see, e.g., (Aho and Ullman, 1972)) provide translations between regular languages. These devices have been popular in computational morphology and computational phonology since the early eighties (Koskenniemi, 1983; Kaplan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes, 1990; Shieber, 1994) are even more powerful than the previous formalisms, an"
P96-1016,P95-1021,1,0.928057,"c). The vector derivation tree can be seen as representing an &quot;outline&quot; for the derivation. Such a view is attractive from a linguistic perspective: if each vector represents a lexeme and its projection (where the synchronous production is the basis of the lexical projection that the vector represents), then the vector derivation tree is in fact the dependency tree of the sentence (representing direct relations between lexemes such as grammatical function). In this respect, the vector derivation tree of UVG-DL is like the derivation tree of tree adjoining grammar and of D-tree grammars (DTG) (Rambow, Vijay-Shanker, and Weir, 1995), which is not surprising, since all three formalisms share the same extended domain of locality. Furthermore, the vector derivation tree of SynchUVG-DL shares with the the derivation tree of D T G the property that it reflects linguistic dependency uniformly; however, while the definition of D T G was motivated precisely from considerations of dependency, the vector derivation tree is merely a by-product of our definition of SynchUVG-DL, which was motivated from the desire to have a computationally tractable model of synchronization more powerful than SynchTAG.2 We briefly discuss a sample d"
P96-1016,C90-3045,0,0.719437,"plan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes, 1990; Shieber, 1994) are even more powerful than the previous formalisms, and have been applied in machine translation (Abeill6, Schabes, and Joshi, 1990; Egedi and Palmer, 1994; Harbusch and Poller, 1994; Prigent, 1994), natural language generation (Shieber and Schabes, 1991), and theoretical syntax (Abeilld, 1994). The common underlying idea in all of these formalisms is to combine two generative devices through a pairing of their productions (or, in the case of the corresponding automata, of their transitions) in such a way that right-hand side nonterminal symbols in the paired productions are"
P96-1016,P95-1033,0,0.0251903,"onal phonology since the early eighties (Koskenniemi, 1983; Kaplan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes, 1990; Shieber, 1994) are even more powerful than the previous formalisms, and have been applied in machine translation (Abeill6, Schabes, and Joshi, 1990; Egedi and Palmer, 1994; Harbusch and Poller, 1994; Prigent, 1994), natural language generation (Shieber and Schabes, 1991), and theoretical syntax (Abeilld, 1994). The common underlying idea in all of these formalisms is to combine two generative devices through a pairing of their productions (or, in the case of the corresponding automata, of their transitions) in such"
P96-1016,W90-0102,0,\N,Missing
P96-1032,P89-1018,0,0.122098,"nces, the graph-structured stacks used to describe Tomita&apos;s algorithm differ very little from parse fables, or in other words, generalized LR parsing is one of the so called tabular parsing algorithms, among which also the CYK algorithm (Harrison, 1978) and Earley&apos;s algorithm (Earley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given by Lang (1974) and Billot and Lang (1989): the construction of pushdown automata from grammars and the simulation of these automata by means of tabulation yield different tabular algorithms for different such constructions. Another example, on which our presentation is based, was first suggested by Leermakers (1989): a grammar is first transformed and then a standard tabular algorithm along with some filtering condition is applied using the transformed grammar. In our case, the transformation and the subsequent application of the tabular algorithm result in a new form of tabular LR parsing. Our method is more efficient than Tomita&apos;s"
P96-1032,P94-1029,1,0.885891,"such that q&apos; = goto&apos;(q, A). (ii) A 6 Uj,j if (A --+ e) 6 P, A E pred(Uj); (iv) (X,q) (o~) ~ Note that in the case of a reduce/reduce conflict with two grammar rules sharing some suffix in the right-hand side, the gathering steps of A2Lrt will treat both rules simultaneously, until the parts of the right-hand sides are reached where the two rules differ. (See Leermakers (1992a) for a similar sharing of computation for common suffixes.) An interesting fact is that the automaton .A2LR is very similar to the automaton .ALR constructed for a grammar transformed by the transformation rtwo given by Nederhof and Satta (1994). 2 5 The (iii) A 6 Ui,j if B 6 Ui,~, C 6 Uk,j, (A ---. BC) 6 P, A 6 pred(Ui); (iv) A 6 Uij if B 6 Uij, (A ~ B) 6 P, A 6 pred(UO. algorithm This section presents a tabular LR parser, which is the main result of this paper. The parser is derived from the 2LR automata introduced in the previous section. Following the general approach presented by Leermakers (1989), we simulate computations of 2For the earliest mention of this transformation, we have encountered pointers to Schauerte (1973). Regrettably, we have as yet not been able to get hold of a copy of this paper. 242 The string has been acc"
P96-1032,P81-1022,0,0.579562,"omputational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing, also known as Tomita&apos;s algorithm, described by Tomita (1986) and further investigated by, for example, Tomita (1991) and Nederhof (1994a). Despite appearances, the graph-structured stacks used to describe Tomita&apos;s algorithm differ very little from parse fables, or in other words, generalized LR parsing is one of the so called tabular parsing algorithms, among which also the CYK algorithm (Harrison, 1978) and Earley&apos;s algorithm (Earley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given by Lang (1974) and Billot and Lang (1989): the construction of pushdown automata from grammars and the simulation of these automata by means of tabulation yield different tabular algorithms for different such constructions. Another example, on which our presentation is based, was first suggested by Leermakers"
P96-1032,P91-1014,0,0.191887,"wo parsing techniques, expressed by the 245 automata A~,R and A2La. The tabular realisation of the former automata is very close to a variant of Tomita&apos;s algorithm by Kipps (1991). The objective of our experiments was to show that the automata ~4~La provide a better basis than .A~a for tabular LR parsing with regard to space and time complexity. Parsing algorithms that are not based on the LR technique have however been left out of consideration, and so were techniques for unification grammars and techniques incorporating finite-state processes. 3 Theoretical considerations (Leermakers, 1989; Schabes, 1991; Nederhof, 1994b) have suggested that for natural language parsing, LR-based techniques may not necessarily be superior to other parsing techniques, although convincing empirical data to this effect has never been shown. This issue is difficult to resolve because so much of the relative efficiency of the different parsing techniques depends on particular grammars and particular input, as well as on particular implementations of the techniques. We hope the conceptual framework presented in this paper may at least partly alleviate this problem. Acknowledgements The first author is supported by"
P96-1032,P89-1017,0,0.160208,"ley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given by Lang (1974) and Billot and Lang (1989): the construction of pushdown automata from grammars and the simulation of these automata by means of tabulation yield different tabular algorithms for different such constructions. Another example, on which our presentation is based, was first suggested by Leermakers (1989): a grammar is first transformed and then a standard tabular algorithm along with some filtering condition is applied using the transformed grammar. In our case, the transformation and the subsequent application of the tabular algorithm result in a new form of tabular LR parsing. Our method is more efficient than Tomita&apos;s algorithm in two respects. F i r s t , reduce operations are implemented in an efficient way, by splitting them into several, more primitive, operations (a similar idea has been proposed by Kipps (1991) for Tomita&apos;s algorithm). Second, several paths in the computation that mu"
P96-1032,E93-1036,1,0.905825,"the number of parses found by the algorithm for any input, is reduced to exactly that of the source grammar. A practical implementation would construct the parse trees on-the-fly, attaching them to the table entries, allowing packing and sharing of subtrees (cf. the literature on parse forests (Tomita, 1986; Elllot and Lang, 1989)). Our algorithm actually only needs one (packed) subtree for several (X, q) E Ui,k with fixed X , i , k but different q. The resulting parse forests would then be optimally compact, contrary to some other LR-based tabular algorithms, as pointed out by Rekers (1992), Nederhof (1993) and Nederhof (1994b). 6 of the algorithm In this section, we investigate how the steps performed by Algorithm 1 (applied to the 2LR cover) relate to those performed by .A2LR, for the same input. We define a subrelation ~ + of t-+ as: (6, uw) ~+ (66&apos;,w) if and only if (6, uw) = (6, zlz2"".&apos;zmw) t(88l,z2..-zmw) ~- ... ~ (68re,w) = (86&apos;,w), for some m &gt; 1, where I~kl &gt; 0 for all k, 1 &lt; k &lt; m. Informally, we have (6, uw) ~+ (6~&apos;, w) if configuration (~8&apos;, w) can be reached from (6, uw) without the bottom-most part 8 of the intermediate stacks being affected by any of the transitions; furthermore,"
P96-1032,P94-1017,1,0.648277,"Secondly, the static and dynamic complexity of parsing, both in space and time, is significantly reduced. 1 Introduction The efficiency of LR(k) parsing techniques (Sippu and Soisalon-Soininen, 1990) is very attractive from the perspective of natural language processing applications. This has stimulated the computational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing, also known as Tomita&apos;s algorithm, described by Tomita (1986) and further investigated by, for example, Tomita (1991) and Nederhof (1994a). Despite appearances, the graph-structured stacks used to describe Tomita&apos;s algorithm differ very little from parse fables, or in other words, generalized LR parsing is one of the so called tabular parsing algorithms, among which also the CYK algorithm (Harrison, 1978) and Earley&apos;s algorithm (Earley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given"
P97-1057,J95-4004,0,0.784249,"fixed, finite alphabet and e the null string. E* and E+ are the set of all strings and all non-null strings over E, respectively. Let w 6 E*. We denote by Iwl the length o f w . Let w = uxv; u i s a p r e f i x and v is a suffix of w; when x is non-null, it is called a f a c t o r of w. The suffix of w of length i is denoted suffi(w), for O &lt; i _&lt; Iwl. Assume that x is non-null, and w = uixsuffi(w ) for ~ > 0 different values of i but not for ~ + 1, or x is not a factor of w and ~ = 0. Then we say that ~ is the statistic of factor z in w. String transformation systems have been introduced in (Brill, 1995) and have several applications in natural language processing. In this work we consider the computational problem of automatically learning from a given corpus the set of transformations presenting the best evidence. We introduce an original data structure and efficient algorithms that learn some families of transformations that are relevant for part-of-speech tagging and phonological rule systems. We also show that the same learning problem becomes NP-hard in cases of an unbounded use of d o n &apos; t care symbols in a transformation. 1 2 The learning paradigm The learning paradigm we adopt is ca"
P97-1057,J94-3001,0,0.113096,"set of aligned pairs is called an a l i g n e d c o r p u s . Let (w, w &apos;) be an aligned pair and let 7- be some transformation of the form u --~ v. The p o s i t i v e e v i d e n c e of v (w.r.t. (w, w&apos;)) is the number of different positions at which factors u and v are aligned within (w, w&apos;). The n e g a t i v e e v i d e n c e of r (w.r.t. w, w ~) is the number of different positions at which factors u and u are aligned within Introduction Ordered sequences of rewriting rules are used in several applications in natural language processing, including phonological and morphological systems (Kaplan and Kay, 1994), morphological disambiguation, part-of-speech tagging and shallow syntactic parsing (Brill, 1995), (Karlsson et ah, 1995). In (Brill, 1995) a learning paradigm, called errordriven learning, has been introduced for automatic induction of a specific kind of rewriting rules called transformations, and it has been shown that the achieved accuracy of the resulting transformation systems is competitive with that of existing systems. In this work we further elaborate on the errordriven learning paradigm. Our main contribution is summarized in what follows. We consider some families of transformation"
P98-2157,J91-3004,0,0.178971,"amme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adj"
P98-2157,C92-2066,0,0.0422418,"s that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities ~we~* P r"
P98-2157,C88-1075,0,0.0391071,"s for stochastic TAGs. flff/~2 n 4.1 C General equations E The prefix probability is given by: Figure 2: Wrapping of auxiliary trees when computing the prefix probability To derive a method for the computation of prefix probabilities, we give some simple recursive equations. Each equation decomposes an item into other items in all possible ways, in the sense that it expresses the probability of that item as a function of the probabilities of items associated with equal or smaller portions of the input. In specifying the equations, we exploit techniques used in the parsing of incomplete input (Lang, 1988). This allows us to compute the prefix probability as a by-product of computing the inside probability. 955 Pr(al...anw) wEE* = ~ P([t,O,n,-,-]), fEZ where P is a function over items recursively defined as follows: P([t,i,j, fl,f2]) = P([Rt, i,j, fl,f2]); P([t~N,i,j,-,-]) = P([a,i,k,-,-]) k(i < k < j) . P([N,k,j,-,-]), if a ¢ e A -~dft(aN); P([t~N, i, j, fl, f2]) = Z P([a,i,k,-,-])-P([N,k,j, k(i < k < fl) if ~ ¢ ¢ A dft(g); (1) (2) (3) fl,f2]), P([aN, i, j, fl, f2]) = (4) P([a, i, k, fl, f2]). P([N, k, j, -, -]), k(f2 <_ k <_j) if # c^ ((i',j') = (i,j))A = (fl, f2)v ((f~ = f~ = iV f{ = f~ = j"
P98-2157,J95-2002,0,0.358178,"hnology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations i"
P98-2157,W89-0211,0,0.0328774,"and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be use"
P98-2192,P95-1023,0,0.0172198,"syntactic constructions in natural language that can be handled by unrestricted TAGs. In particular, we describe an algorithm for parsing a strict subclass of TAG in O(nS), and a t t e m p t to show that this subclass retains enough generative power to make it useful in the general case. 1 Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n6), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than O(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound o"
P98-2192,P94-1022,0,0.546924,"twofold: • We define a strict subclass of TAG where adjunction of so-called wrapping trees at the spine is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case time O(n5). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 1176 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G = ( N , ~ , I ,"
P98-2192,J94-2002,1,0.873309,"or parsing a strict subclass of TAG in O(nS), and a t t e m p t to show that this subclass retains enough generative power to make it useful in the general case. 1 Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n6), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than O(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which restrictions on TAGs are needed in order to"
P98-2192,J94-1004,0,0.0460098,". (1) where X n is any projection of category X, y,nax is the maximal projection of Y, and the order of the constituents is variable. 3 A c o m p l e m e n t auxiliary tree, on the other hand, introduces a lexical head t h a t subcategorizes for the tree's foot node and assigns it a thematic role. The structure of a complement auxiliary tree may be •described as: Xrnax _+ ... yO . . . Xrna~ . . . , (2) where X r n a ~ is the maximal projection Of some category X, and y 0 is the lexical projection 2The same linguistic distinction is used in the conception of 'modifier' and 'predicative' trees (Schabes and Shieber, 1994), but Schabes and Shieber give the trees special properties in the calculation of derivation structures, which we do not. 3The CFG-like notation is taken directly from (Kroch, 1989), where it is used to specify labels at the root and frontier nodes of a tree without placing constraints on the internal structure. 3. The foot node of an athematic auxiliary tree is dominated only by the root, with no intervening nodes, so it falls outside of the maximal projection of the head. 4. The foot node of a complement auxiliary tree is dominated by the maximal projection of the head, which may also domina"
P98-2192,J95-4002,0,0.627165,"ntribution of this paper is twofold: • We define a strict subclass of TAG where adjunction of so-called wrapping trees at the spine is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case time O(n5). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 1176 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G"
P98-2192,P93-1017,0,\N,Missing
P99-1059,P96-1023,0,0.664127,"f particular pairs of words in particular roles. The acceptability of ""Nora convened the "" The authors were supported respectivelyunder ARPA Grant N6600194-C-6043 ""Human LanguageTechnology"" and Ministero dell&apos;Universitk e della Ricerca Scientifica e Tecnologicaproject ""Methodologiesand Tools of High Performance Systems for Multimedia Applications."" 457 party"" then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation, i The chart parsing algorithms used by most of the above authors run in time O(nS), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size O(n 2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, O(n 4) is possible. • The O(n 4) result also holds for head aut"
P99-1059,P98-1035,0,0.0259716,"m was previously known (Eisner, 1997), the grammar constant can be reduced without harming the O(n 3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). lows (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A --+ a, where A E VN, a E (VN U VT)*. If every production in P has the form A -+ B C or A --+ a, for A , B , C E VN,a E VT, then the grammar is said to be in Chomsky Normal Form (CNF). 2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this paper we adopt the following"
P99-1059,W95-0103,0,0.0265967,"s where an O(n 3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the O(n 3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). lows (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A --+ a, where A E VN, a E (VN U VT)*. If every production in P has the form A -+ B C or A --+ a, for A , B , C E VN,a E VT, then the grammar is said to be in Chomsky Normal Form (CNF). 2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this pa"
P99-1059,P97-1003,0,0.177939,"es. The acceptability of ""Nora convened the "" The authors were supported respectivelyunder ARPA Grant N6600194-C-6043 ""Human LanguageTechnology"" and Ministero dell&apos;Universitk e della Ricerca Scientifica e Tecnologicaproject ""Methodologiesand Tools of High Performance Systems for Multimedia Applications."" 457 party"" then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation, i The chart parsing algorithms used by most of the above authors run in time O(nS), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size O(n 2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, O(n 4) is possible. • The O(n 4) result also holds for head automaton grammars. • For a very common special c"
P99-1059,1997.iwpt-1.10,1,0.797835,"mation Science University of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104 USA j eisner@linc, cis. upenn, edu Dip. di E l e t t r o n i c a e I n f o r m a t i c a Universit£ di Padova via Gradenigo 6/A, 35131 Padova, Italy satt a@dei, unipd, it Abstract Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n 4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n5). For a common special case that was known to allow O(n 3) parsing (Eisner, 1997), we present an O(n 3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language--in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;~uk, 1988; Pollard and Sag, 1994). Besides the possible arguments"
P99-1059,P95-1037,0,0.0650139,"of these grammars where an O(n 3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the O(n 3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). lows (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A --+ a, where A E VN, a E (VN U VT)*. If every production in P has the form A -+ B C or A --+ a, for A , B , C E VN,a E VT, then the grammar is said to be in Chomsky Normal Form (CNF). 2 Every language that can be generated by a CFG can also be generated b"
P99-1059,C88-2121,0,0.0120233,"ds of O(n5). For a common special case that was known to allow O(n 3) parsing (Eisner, 1997), we present an O(n 3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language--in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;~uk, 1988; Pollard and Sag, 1994). Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments. ""Convene"" requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP&apos;s head (e.g., ""meeting""). We use the general term bilexical for a grammar that records such facts. A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles. The acceptability of ""Nora convened the "" The auth"
P99-1059,C92-2066,0,0.0140956,"Missing"
P99-1059,C98-1035,0,\N,Missing
Q13-1022,W06-2920,0,0.0790276,"nherit property, this even increases to two orders of magnitude. However, as already stated in 2, this improvement is paid for by a loss in coverage; for instance, trees of the form shown in Figure 3 cannot be parsed any longer. 7.1 Quantitative Evaluation In order to assess the empirical loss in coverage that the restriction to head-split trees incurs, we evaluated the coverage of several classes of dependency trees on standard data sets. Following Pitler et al. (2012), we report in Table 1 figures for the training sets of six languages used in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006). As we can see, the O.n6 / class of head-split trees has only slightly lower coverage on this data than the baseline class of well-nested dependency trees with block-degree at most 2. The losses are up to 0.2 percentage points on five of the six languages, and 0.9 points on the Dutch data. Our even more restricted O.n5 / class of 1-inherit head-split trees has the same coverage as our O.n6 / class, which is expected given the results of Pitler et al. (2012): Their O.n6 / class of 1-inherit trees has exactly the same coverage as the baseline (and thereby more coverage than our O.n6 / class). I"
Q13-1022,D07-1101,0,0.0725403,"l property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and comple"
Q13-1022,W10-4407,0,0.121032,"‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree  2 can be parsed in time O.n7 / using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O.n6 /, without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n5 / time complexity. The O.n3 / time resu"
Q13-1022,P99-1059,1,0.817837,"g can be done in time O.n6 /, without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n5 / time complexity. The O.n3 / time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other lexicalized formalisms, such as bilexical context-free grammars and head automata (Eisner and Satta, 1999). However, to our knowledge no attempt has been made in the literature to extend these techniques to non-projective dependency parsing. In this article we leverage the central idea from Eisner’s algorithm and extend it to the class of wellnested dependency trees with block-degree at most 2. 267 Transactions of the Association for Computational Linguistics, 1 (2013) 267–278. Action Editor: Brian Roark. c Submitted 3/2013; Published 7/2013. 2013 Association for Computational Linguistics. We introduce a structural property, called head-split, that allows us to split these trees at the positions o"
Q13-1022,W00-2011,1,0.902321,"Missing"
Q13-1022,1997.iwpt-1.10,0,0.756975,"under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time"
Q13-1022,J11-3004,0,0.273733,"Missing"
Q13-1022,P10-1001,0,0.0284387,"allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre"
Q13-1022,P06-2066,1,0.833046,"e. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2."
Q13-1022,W07-2216,1,0.907291,"odel can be computed in time O.n3 /, where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O.n2 / (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restr"
Q13-1022,H05-1066,0,0.501381,"Missing"
Q13-1022,D12-1044,0,0.651878,"This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree  2 can be parsed in time O.n7 / using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O.n6 /, without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O.n5 / time complexity. The O.n3 / time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other le"
Q13-1022,P98-2192,1,0.858164,"Missing"
Q13-1022,C98-2187,1,\N,Missing
Q13-1022,Q13-1002,0,\N,Missing
Q14-1010,de-marneffe-etal-2006-generating,0,0.153213,"Missing"
Q14-1010,C12-1059,1,0.845131,"ing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it returns a single canonical transition. Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. Oracles that are both deterministic and partial are called static. Traditionally, only static oracles have been exploited in training of transition-based parsers. Recently, Goldberg and Nivre (2012; 2013) showed that the accuracy of greedy parsers can be substantially improved without affecting their parsing speed. This improvement relies on the introduction of novel oracles that are nondeterministic and complete. An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser. Oracles that are both nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based"
Q14-1010,Q13-1033,1,0.768824,"ed in training of transition-based parsers. Recently, Goldberg and Nivre (2012; 2013) showed that the accuracy of greedy parsers can be substantially improved without affecting their parsing speed. This improvement relies on the introduction of novel oracles that are nondeterministic and complete. An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser. Oracles that are both nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers. The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition. They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system. In this article, we develop one such oracle (§4) and prove its correctness (§5). An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and all"
Q14-1010,P10-1110,0,0.0878832,"cles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages. 1 Introduction Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right. These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007). However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it returns a single canonical transition. Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. Oracles that are both deterministic and partial are called static. T"
Q14-1010,P11-1068,1,0.722931,"Missing"
Q14-1010,J93-2004,0,0.0526145,"of spurious ambiguities. The explore setup increases the configuration space explored by the parser during training, by exposing the training procedure to non-optimal configurations that are likely to occur during parsing, together with the optimal transitions to take in these configurations. It was shown by Goldberg and Nivre (2012; 2013) that the nondet setup outperforms the static setup, and that the explore setup outperforms the nondet setup. 8 Experimental Evaluation Datasets Performance evaluation is carried out on CoNLL 2007 multilingual dataset, as well as on the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use gold part-of-speech tags, while for the PTB we use automatically assigned tags. As usual, the PTB parser is trained on sections 2-21 and tested on section 23. Setup We train labeled versions of the arc-standard (std) and LR-spine (lrs) parsers under the static, nondet and explore setups, as defined in §7. In the nondet setup we use a nondeterministic partial oracle and in the explore setup we use the nondeterministic complete oracles we present in this paper. In the static setup we resolve oracl"
Q14-1010,W03-3017,0,0.245663,"i+1 · · · wj of w. We write i → j to denote a grammatical dependency of some unspecified type between lexical tokens wi and wj , where wi is the head and wj is the dependent. A dependency tree for w is a directed, ordered tree t = (Vw , A), such that Vw = [0, n] is the set of nodes, A ⊆ Vw × Vw is the set of arcs, and node 0 is the root. Arc (i, j) encodes a dependency i → j, and we will often use the latter notation to denote arcs. 2.2 Transition-Based Dependency Parsing We assume the reader is familiar with the formal framework of transition-based dependency parsing originally introduced by Nivre (2003); see Nivre (2008) for an introduction. We only summarize here our notation. Transition-based dependency parsers use a stack data structure, where each stack element is associated with a tree spanning (generating) some substring of the input w. The parser processes the input string incrementally, from left to right, applying at each step a transition that updates the stack and/or 120 consumes one token from the input. Transitions may also construct new dependencies, which are added to the current configuration of the parser. We represent the stack data structure as an ordered sequence σ = [σd"
Q14-1010,W04-0308,0,0.531753,"at are nondeterministic and complete. An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser. Oracles that are both nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers. The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition. They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system. In this article, we develop one such oracle (§4) and prove its correctness (§5). An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and allows mixing of bottom-up and top-down strategies. This parser, called here the LR-spine parser, achieves state-ofthe-art results for greedy parsing. Like the arc-standard system, the LR-spine parser is not arc-decomposable, and a dynamic oracle for"
Q14-1010,J08-4003,0,0.335047,"ty, Israel Francesco Sartorio Department of Information Engineering University of Padua, Italy Giorgio Satta Department of Information Engineering University of Padua, Italy yoav.goldberg@gmail.com sartorio@dei.unipd.it satta@dei.unipd.it Abstract We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages. 1 Introduction Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right. These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007). However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it retu"
Q14-1010,P13-1014,1,0.802769,"th nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers. The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition. They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system. In this article, we develop one such oracle (§4) and prove its correctness (§5). An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and allows mixing of bottom-up and top-down strategies. This parser, called here the LR-spine parser, achieves state-ofthe-art results for greedy parsing. Like the arc-standard system, the LR-spine parser is not arc-decomposable, and a dynamic oracle for this system was not known. We extend our oracle for the arc-standard system to work for the LR-spine system as well (§6). The dynamic oracles developed by Goldberg and Nivre (2013) for arc-decomposable systems are based on local properties of computations. In contrast, our novel dynamic oracle a"
Q14-1010,D08-1059,0,0.057594,"We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages. 1 Introduction Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right. These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007). However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it returns a single canonical transition. Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. Oracles that are both deterministic and part"
Q14-1010,D07-1096,0,\N,Missing
Q14-1032,E03-1036,0,0.113637,"r context. With this information, rule (7) can perform a check against the restrictions specified for the composition rule, and rules (3) and (6) merely need to test whether the target categories of their two antecedents match, and propagate the common target category to the conclusion. This is essentially the same solution as the one adopted in the V&W algorithm. 6.2 Support for Multi-Modal CCG The modern version of CCG has abandoned rule restrictions in favor of a new, lexicalized control mechanism in the form of modalities or slash types (Steedman and Baldridge, 2011). However, as shown by Baldridge and Kruijff (2003), every multi-modal CCG can be translated into an equivalent CCG with rule restrictions. The basic idea is to specialize the target of each category and argument for a slash type, and to reformulate the multi-modal rules as rules with restrictions that reference this information. With this simulation, our parsing algorithm can also be used as a parsing algorithm for multi-modal CCG. 3 Such restrictions can be used, for example, to impose the linguistically relevant distinction between harmonic and crossed forms of composition. 6.3 Comparison with the V&W Algorithm As already mentioned in Secti"
Q14-1032,C04-1180,0,0.0321294,"ivations into small, shareable parts. Our algorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algor"
Q14-1032,J07-4004,0,0.434328,"parsing algorithm for CCG, based on a new decomposition of derivations into small, shareable parts. Our algorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, r"
Q14-1032,P10-1035,0,0.223189,"tion is a binary rule of the (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a recognition version of the algorithm, standard techniq"
Q14-1032,W08-2306,0,0.0215106,"large enough to accomodate all instances of T =X. Substitution is a binary rule of the (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a re"
Q14-1032,E09-1053,1,0.810304,"instances of T =X. Substitution is a binary rule of the (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a recognition version of the alg"
Q14-1032,P10-1055,1,0.787431,"e (forward) form X =Y jZ Y jZ ) X=Z. This rule is easy to implement if both =Y and jZ are stored in the same item. Otherwise, we need to pass jZ to any item storing the =Y . This can be done by changing the second antecedent of rule (6) to allow a single argument jZ instead of the empty excess &quot;. The price of this change is spurious ambiguity in the derivations of the grammatical deduction system. 7 Conclusion Recently, there has been a surge of interest in the mathematical properties of CCG; see for instance Hockenmaier and Young (2008), Koller and Kuhlmann (2009), Fowler and Penn (2010) and Kuhlmann et al. (2010). Following this line, this article has revisited the parsing problem for CCG. Our work, like the polynomial-time parsing algorithm previously discovered by Vijay-Shanker and Weir (1993), is based on the idea of decomposing large CCG derivations into smaller, shareable pieces. Here we have proposed a derivation decomposition different from the one adopted by Vijay-Shanker and Weir (1993). This results in an algorithm which, in our own opinion, is simpler and easier to understand. Although we have specified only a recognition version of the algorithm, standard techniques can be applied to obtai"
Q14-1032,D10-1119,0,0.0197398,"l, shareable parts. Our algorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we"
Q14-1032,Q13-1015,0,0.0178317,"gorithm has the same asymptotic complexity, O.n6 /, as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practi"
Q14-1032,P87-1012,0,0.712675,"and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among di"
Q14-1032,P88-1031,0,0.649303,"xicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among different derivat"
Q14-1032,P90-1001,0,0.719207,"s of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among different derivations. This sharing is the key to the polynomial runtime. In this art"
Q14-1032,J93-4002,0,0.636143,"itive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We speculate that this has two main reasons: First, some authors The V&W algorithm is based on a special decomposition of CCG derivations into smaller parts that can then be shared among different derivations. This sharing is the key to the polynomial runtime. In this article we build on the same idea, bu"
Q14-1032,W12-3127,0,0.0143754,"as a previous algorithm by VijayShanker and Weir (1993), but is easier to understand, implement, and prove correct. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman and Baldridge (2011)) is a lexicalized grammar formalism that belongs to the class of so-called mildly context-sensitive formalisms, as characterized by Joshi (1985). CCG has been successfully used for a wide range of practical tasks including data-driven parsing (Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004; Kwiatkowski et al., 2010; Lewis and Steedman, 2013) and machine translation (Weese et al., 2012). Several parsing algorithms for CCG have been presented in the literature. Earlier proposals show running time exponential in the length of the input string (Pareschi and Steedman, 1987; Tomita, 1988). A breakthrough came with the work of Vijay-Shanker and Weir (1990) and Vijay-Shanker and Weir (1993) who report the first polynomial-time algorithm for CCG parsing. Until this day, this algorithm, which we shall refer to as the V&W algorithm, remains the only published polynomial-time parsing algorithm for CCG. However, we are not aware of any practical parser for CCG that actually uses it. We"
Q14-1032,P88-1034,0,0.465222,"onential-time parser for CCG, from which we derive our polynomial-time parser in Section 4. Section 5 further simplifies the algorithm and proves its correctness. We then provide a discussion of our algorithm and possible extensions in Section 6. Section 7 concludes the article. 405 Transactions of the Association for Computational Linguistics, 2 (2014) 405–418. Action Editor: Mark Steedman. c Submitted 4/2014; Revised 8/2014; Published 10/2014. 2014 Association for Computational Linguistics. 2 Combinatory Categorial Grammar We assume basic familiarity with CCG in general and the formalism of Weir and Joshi (1988) in particular. In this section we set up our terminology and notation. A CCG has two main parts: a lexicon that associates words with categories, and rules that specify how categories can be combined into other categories. Together, these components give rise to derivations such as the one shown in Figure 1. 2.1 Lexicon The CCG lexicon is a finite set of word–category pairs w WD X.1 Categories are built from a finite set of atomic categories and two binary operators: forward slash (=) and backward slash (=). Atomic categories represent the syntactic types of complete constituents; they includ"
W00-2011,P96-1023,0,0.0406615,"Missing"
W00-2011,1997.iwpt-1.6,0,0.0319964,"Missing"
W00-2011,E99-1025,0,0.056552,"Missing"
W00-2011,C96-1058,1,0.741947,"Missing"
W00-2011,1997.iwpt-1.10,1,0.872288,"Missing"
W00-2011,P99-1059,1,0.910111,"Missing"
W00-2011,C92-2066,0,0.0783217,"Missing"
W00-2011,C92-2065,0,0.0765918,"Missing"
W00-2011,J94-1004,0,0.050216,"Missing"
W00-2011,J99-2004,0,0.0850161,"Missing"
W00-2011,H86-1020,0,\N,Missing
W00-2011,P97-1003,0,\N,Missing
W00-2011,J93-4002,0,\N,Missing
W00-2011,C88-2121,0,\N,Missing
W00-2011,1997.iwpt-1.11,0,\N,Missing
W00-2011,P85-1011,0,\N,Missing
W07-2216,C92-2092,0,0.0423826,"ve parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L"
W07-2216,W06-2920,0,0.698112,"Under the assumption that the root of the graph is the left most word of the sentence, a projective graph is one where the edges can be drawn in the plane above the sentence with no two edges crossing. Conversely, a non-projective dependency graph does not satisfy this property. Figure 2 gives an example of a nonprojective graph for a sentence that has also been extracted from the Penn Treebank. Non-projectivity arises due to long distance dependencies or in languages with flexible word order. For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006). Thus, the ability to learn and infer nonprojective dependency graphs is an important problem in multilingual language processing. Syntactic dependency parsing has seen a number of new learning and inference algorithms which have raised state-of-the-art parsing accuracies for many languages. In this work we focus on datadriven models of dependency parsing. These models are not driven by any underlying grammar, but instead learn to predict dependency graphs based on a set of parameters learned solely from a labeled corpus. The advantage of these models is that they negate the need for the deve"
W07-2216,W02-1001,0,0.119719,"re for both the projective and non-projective case. We see that the non-projective case compares favorably for all three problems. 4 Applications To motivate the algorithms from Section 3, we present some important situations where each calculation is required. 4.1 (i,j)k ∈ET This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al., 2005a; Buchholz et al., 2006). Some algebra reveals, T = = Inference Based Learning = Many learning paradigms can be defined as inference-based learning. These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al., 2005a). In these settings, a models parameters are iteratively updated based on the argmax calculation for a single or set of training instances under the current parameter settings. The work of McDonald et al. (2005b) showed that it is possible to learn a highly accurate non-projective dependency parser for multiple languages using the Chu-Liu-Edmonds algorithm for unlabeled parsing. 4.2 studied for both phrase-structure parsing and dependency parsing (Titov and Henderson, 2006). In that work, as is common with many mi"
W07-2216,A00-2030,0,0.0179914,"Missing"
W07-2216,C96-1058,0,0.984165,"Hamiltonian graph problem suggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong"
W07-2216,P97-1043,0,0.23732,"Missing"
W07-2216,W05-1505,0,0.0831088,"Missing"
W07-2216,P06-2047,0,0.0130804,"orithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for ins"
W07-2216,P98-1106,0,0.418447,"ustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-th"
W07-2216,P05-1013,0,0.0557336,"work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have a"
W07-2216,C04-1010,0,0.00795222,"roblem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models"
W07-2216,P03-1054,0,0.0190043,"grammar-driven literature given by Neuhaus and B¨oker (1997). In that work, an arity constraint is included in their minimal grammar. 5.2 Vertical and Horizontal Markovization In general, we would like to say that every dependency decision is dependent on every other edge in a graph. However, modeling dependency parsing in such a manner would be a computational nightmare. Instead, we would like to make a Markov assumption over the edges of the tree, in a similar way that a Markov assumption can be made for sequential classification problems in order to ensure tractable learning and inference. Klein and Manning (2003) distinguish between two kinds of Markovization for unlexicalized CFG parsing. The first is vertical Markovization, which makes the generation of a non-terminal dependent on other non-terminals that have been generated at different levels in the phrase-structure tree. The second is horizontal Markovization, which makes the generation of a non-terminal dependent on other non-terminals that have been generated at the same level in the tree. For dependency parsing there are analogous notions of vertical and horizontal Markovization for a given edge (i, j)k . First, let us define the vertical and"
W07-2216,W06-1616,0,0.189447,"y through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined"
W07-2216,P04-1061,0,0.0197833,"recursive breadthfirst manner. Thus, pkx,y represents the probability of the word x generating its modifier y with label lk . This distribution is usually smoothed and is often conditioned on more information including the orientation of x relative to y (i.e., to the left/right) and distance between the two words. In the supervised setting this model can be trained with maximum likelihood estimation, which amounts to simple counts over the data. Learning in the unsupervised setting requires EM and is discussed in Section 4.4.2. Another generative dependency model of interest is that given by Klein and Manning (2004). In this model the sentence and tree are generated jointly, which allows one to drop the assumption that p(T |n) is uniform. This requires the addition to the model of parameters px,STOP for each xP∈ Σ, with the normalization condition px,STOP + y,k pkx,y = 1. It is possible to extend the model of Klein and Manning (2004) to the non-projective case. However, the resulting distribution will be over multisets of words from the alphabet instead of strings. The discussion in this section is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Mann"
W07-2216,D07-1015,0,0.704066,"clude the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L = {l1 , . . . , l|L |} be a set of permissible"
W07-2216,J93-2004,0,0.0304382,"tractable for any model richer than the edge-factored model. 1 Introduction Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic dependencies through directed graphs. These representations have been thoroughly studied in descriptive linguistics (Tesni`ere, 1959; Hudson, 1984; Sgall et al., 1986; Me´lˇcuk, 1988) and have been applied in numerous language processing tasks. Figure 1 gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus, which has been extracted from the Penn Treebank (Marcus et al., 1993). Each edge in this graph represents a single syntactic dependency directed from a word to its modifier. In this representation all edges are labeled with the specific syntactic function of the dependency, e.g., SBJ for subject and NMOD for modifier of a noun. To simplify computation and some important definitions, an artificial token is inserted into the sentence as the left most word and will always represent the root of the dependency graph. We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint (Nivre, 2005). The dependency graph i"
W07-2216,E06-1011,1,0.572749,"s are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a). The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting. We start by investigating and extending the edge-factored model of McDonald et al. (2005b). In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function an"
W07-2216,P05-1012,1,0.665191,"the model to new languages. One interesting class of data-driven models are 121 Proceedings of the 10th Conference on Parsing Technologies, pages 121–132, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: A projective dependency graph. Figure 2: Non-projective dependency graph. those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees. Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations ov"
W07-2216,H05-1066,1,0.749014,"Missing"
W07-2216,D07-1014,0,0.608651,"and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L = {l1 , . . . , l|L |} be a set of permissible syntactic edge labels and x"
W07-2216,W04-0307,0,0.01824,"sually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2"
W07-2216,P96-1021,0,0.100479,"Missing"
W07-2216,W03-3023,0,0.123858,"uggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case f"
W07-2216,C98-1102,0,\N,Missing
W09-3801,P99-1070,0,0.423505,"dividual parse tree. It is not difficult to see that PTA satisfying these 7 those applications where a statistical parsing module needs to be coupled with other statistical modules, in such a way that the composition of the probability spaces still induces a probability distribution. In this subsection we deal with the more general problem of how to transform a WTA that is convergent into a PTA that is proper and consistent. This process is called normalization. The normalization technique we propose here has been previously explored, in the context of probabilistic context-free grammars, in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). We start by introducing some new notions. Let us assume that M is a convergent WTA. For every q ∈ Q, we define X wtM (q) = wtM (r) . available in the agenda in the next iteration, and new transitions can now be considered. The algorithm ends when the largest probability has been calculated for the unique final state qS . We now analyze the computational complexity of the algorithm in Figure 3. The ‘repeat-until’ loop runs at most |Q |times. Entirely reprocessing set A at each iteration would be too expensive. We instead implement A as a priority heap and"
W09-3801,J98-4004,0,0.16555,"Missing"
W09-3801,J99-1004,0,0.0384646,"It is not difficult to see that PTA satisfying these 7 those applications where a statistical parsing module needs to be coupled with other statistical modules, in such a way that the composition of the probability spaces still induces a probability distribution. In this subsection we deal with the more general problem of how to transform a WTA that is convergent into a PTA that is proper and consistent. This process is called normalization. The normalization technique we propose here has been previously explored, in the context of probabilistic context-free grammars, in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). We start by introducing some new notions. Let us assume that M is a convergent WTA. For every q ∈ Q, we define X wtM (q) = wtM (r) . available in the agenda in the next iteration, and new transitions can now be considered. The algorithm ends when the largest probability has been calculated for the unique final state qS . We now analyze the computational complexity of the algorithm in Figure 3. The ‘repeat-until’ loop runs at most |Q |times. Entirely reprocessing set A at each iteration would be too expensive. We instead implement A as a priority heap and maintain a"
W09-3801,P81-1022,0,0.645807,"d to a given input string w by some grammar model. The set of all such parse trees is called parse forest. The extension of the Bar-Hillel construction that we have = µk (σ)r(ε)2 ,r(1)2 ···r(k)2 · k   Y · wtM (πM (ri )) · wtN (πN (ri )) i=1 5 σ2 ∈ Σk2 , w2 ∈ Qk2 where Q is the state set of Prod(M, N ). The above amounts to a bottomup strategy that is also used in the Cocke-KasamiYounger recognition algorithm for context-free grammars (Younger, 1967). More sophisticated strategies are also possible. For instance, one could adopt the Earley strategy developed for context-free grammar parsing (Earley, 1970). In this case, parsing is carried out in a top-down left-to-right fashion, and the binarization construction of Section 3 is carried out on the flight. This has the additional advantage that it would be possible to use WTA models that are not restricted to the special normal form of Section 3, still maintaining the cubic time complexity in the length of the input string. We do not pursue this idea any further in this paper, since our main goal here is to outline an abstract framework for parsing based on WTA models. presented in Section 4 can be easily adapted to obtain a parsing algorithm fo"
W09-3801,W03-3016,1,0.93529,"partial identity translation, and such a transducer is composed with T (relation composition). This is usually called the ‘cascaded’ approach. Such an approach can be easily applied also to parsing problems. In contrast with the cascaded approach above, which may be rather inefficient, in this paper we investigate a more direct technique for parsing strings based on weighted and probabilistic tree automata. We do this by extending to weighted tree automata the well-known Bar-Hillel construction defined for context-free grammars (Bar-Hillel et al., 1964) and for weighted context-free grammars (Nederhof and Satta, 2003). This provides an abstract framework under which several parsing algorithms can be directly derived, based on weighted tree automata. We discuss several applications of our results, including algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability. We investigate several algorithms related to the parsing problem for weighted automata, under the assumption that the input is a string rather than a tree. This assumption is motivated by several natural language processing applications. We provide algorithms for"
W09-3801,J03-1006,0,0.069378,"l-state normal form. • If M is convergent (respectively, proper, consistent), then M 0 is such, too. • If M is unambiguous, then M 0 is also unambiguous and for every t ∈ TΣ and r ∈ RunM (t) we have wtM 0 (r0 ) = wtM (r) · F (r(ε)) where r0 (ε) = qS and r0 (w) = r(w) for every w ∈ Pos(t)  {ε}. 2 two properties are still more powerful than the probabilistic context-free grammar models that are commonly used in statistical natural language processing. Once more, we borrow from the literature on parsing for context-free grammars, and adapt a search algorithm developed by Knuth (1977); see also (Nederhof, 2003). The basic idea here is to generalize Dijkstra’s algorithm to compute the shortest path in a weighted graph. The search algorithm is presented in Figure 3. The algorithm takes as input a trim PTA M that recognizes at least one parse tree. We do not impose any bound on the rank of the alphabet symbols for M . Furthermore, M needs not be a proper PTA. In order to simplify the presentation, we provide the algorithm in a form that returns the largest probability assigned to some tree by M . The algorithm records into the δ(q) variables the largest probability found so far for a run that brings M"
W09-3801,P79-1022,0,0.764109,"denote the vector (0, . . . , 0) ∈ (R∞ ≥0 ) as 0 n X 0 . Let X, X 0 ∈ (R∞ ≥0 ) . We write X ≤ X 0 if Xqi ≤ Xqi for every 1 ≤ i ≤ n. Since each polynomial fqi (X) has coefficients represented by positive real numbers, it is not difficult n to see that, for each X, X 0 ∈ (R∞ ≥0 ) , we have F (X) ≤ F (X 0 ) whenever X 0 ≤ X ≤ X 0 . This means that F is an order preserving, or monotone, mapping. n We observe that ((R∞ ≥0 ) , ≤) is a complete lattice with least element X 0 and greatest element (∞, . . . , ∞). Since F is monotone on a complete lattice, by the Knaster-Tarski theorem (Knaster, 1928; Tarski, 1955) there exists a least and a greatest fixed-point of F that are solutions of X = F (X). The Kleene theorem states that the least fixedpoint solution of X = F (X) can be obtained by iterating F starting with the least element X 0 . In other words, the sequence X k = F (X k−1 ), k = 1, 2, . . . converges to the least fixed-point solution. Notice that each X k provides an approximation for the partition function of M where only trees of depth not larger than k are considered. This means that limk→∞ X k converges to the partition function of M , and the least fixed-point solution is also the sought"
W09-3801,J08-3004,0,0.0913631,"ammars, the computation of the partition function has several applications, including the elimination of epsilon rules (Abney et al., 1999) and the computation of probabilistic distances between probability distributions realized by these formalisms (Nederhof and Satta, 2008). Besides what we have seen in Subsection 5.3, we will provide one more application of partition functions for the computations of so-called prefix probabilities in Subsection 5.5 We also add that, when computed on the Bar-Hillel automata of Section 4, the partition function provides the so-called inside probabilities of (Graehl et al., 2008) for the given states and substrings. Let |Q |= n and let us assume an arbitrary ordering q1 , . . . , qn for the states in Q. We can then rewrite the definition of wtM (q) as wtM (r) wtM (r(ε)) k Y X which prove the main statement and the consistency of M 0 , respectively.  which proves that M 0 is a proper PTA. Next, we prove an auxiliary statement. Let t = σ(t1 , . . . , tk ) for some σ ∈ Σk , k ≥ 0, and t1 , . . . , tk ∈ TΣ . We claim that = µ0k (σ)r(ε),r(1)···r(k) · wtM 0 (r) t∈TΣ ,r∈RunM (t) r(ε)=qS =1 , wtM 0 (r) = X t∈TΣ ,r∈RunM 0 (t) r(ε)=qS = wtM (q1 ) · . . . · wtM (qk0 ) wtM (q) X"
W09-3810,2000.iwpt-1.8,0,0.874802,". Furthermore, we transfer this property to grammars extracted from treebanks. 1 Introduction Discontinuous phrases are frequent in natural language, particularly in languages with a relatively free word order. Several formalisms have been proposed in the literature for modeling trees containing such phrases. These include nonprojective dependency grammar (Nivre, 2006), discontinuous phrase structure grammar (DPSG) (Bunt et al., 1987), as well as linear contextfree rewriting systems (LCFRS) (Vijay-Shanker et al., 1987) and the equivalent formalism of simple range concatenation grammar (sRCG) (Boullier, 2000). Kuhlmann (2007) uses LCFRS for non-projective dependency trees. DPSG have been used in Plaehn (2004) for data-driven parsing of treebanks with discontinuous constituent annotation. Maier and Søgaard (2008) extract sRCGs from treebanks with discontinuous constituent structures. Both LCFRS and sRCG can model discontinuities and allow for synchronous rewriting as well. We speak of synchronous rewriting when two or 2 Synchronous Rewriting Trees in German treebanks By synchronous rewriting we indicate the synchronous instantiation of two or more context-free derivation processes. As an example, c"
W09-3810,E87-1034,0,0.536484,"Missing"
W09-3810,P87-1015,0,\N,Missing
W09-3810,J07-2003,0,\N,Missing
W10-2503,W03-3016,1,0.8103,"ormed into a tree transducer implementing a partial identity translation. This transducer is then composed with M (relational composition) to obtain a transducer that represents all translations of w. This is usually called the ‘cascaded’ approach. In contrast with the cascaded approach above, which may be rather inefficient, we investigate a more direct technique for both parsing and translation of strings based on WXTTs. We do this by extending to WXTTs the well-known BAR -H ILLEL construction defined for context-free grammars (Bar-Hillel et al., 1964) and for weighted contextfree grammars (Nederhof and Satta, 2003). We then derive computational complexity results for parsing and translation of input strings on the basis of WXTTs. Finally, we develop a novel factorization algorithm for WXTTs that, in practical applications, can reduce the asymptotic complexity for such problems. This paper proposes a uniform framework for the development of parsing and translation algorithms for weighted extended (top-down) tree transducers and input strings. The asymptotic time complexity of these algorithms can be improved in practice by exploiting an algorithm for rule factorization in the above transducers. 1 Introdu"
W10-2503,P08-1069,1,0.859754,"al form such that the rank is bounded by some constant. This is also expected from the fact that the translation problem for subclasses of WXTTs such as synchronous contextfree grammars is NP-hard (Satta and Peserico, 2005). Nonetheless, there are cases in which a rank reduction is possible, which might result in an improvement of the asymptotical run-time of our construction. Following the above line, we present here a linear time algorithm for reducing the rank of a WXTT under certain conditions. Similar algorithms for tree-based transformation devices have been discussed in the literature. Nesson et al. (2008) consider synchronous tree adjoining grammars; their algorithm is conceptually very similar to ours, but computationally more demanding due to the treatment of adjunction. Following that work, we also demand here that the new WXTT ‘preserves’ the recursive structure of the input WXTT, as formalized below. Galley et al. (2004) algorithm also behaves in linear time, but deals with the different problem of tree to string translation. Rank reduction algorithms for stringbased translation devices have also been discussed by Zhang et al. (2006) and Gildea et al. (2006). Recall that M = (Q, Σ, ∆, I,"
W10-2503,P81-1022,0,0.734364,"transducer rules, resulting in an asymptotic reduction in the complexity for these algorithms. In machine translation applications transducers usually have very large sets of rules. One should then specialize the restriction construction in such a way that the number of useless rules for Prod(Nw , M ) is considerably reduced, resulting in a more efficient construction. This can be achieved by grounding the construction of the new rules by means of specialized strategies, as usually done for parsing based on context-free grammars; see for instance the parsing algorithms by Younger (1967) or by Earley (1970). Then the goal of this section is the efficient computation of a structure-preserving factorization M 0 of M such that rk(M 0 ) = deg(M ). Theorem 7. The WXTT dec(M ) is a structurepreserving factorization of M such that rk(dec(M )) = deg(M ). Moreover, dec(M ) can be computed in time O(|M |). Proof. Let us only discuss the run-time complexity shortly. Clearly, D ECOMPOSE(l, r, x1 , x1 ) should be called once for each rule s (q, l) → (q1 · · · qk , r) ∈ R. In lines 1–4 the structure of l and r is inspected and the properties var(li ) = var(l) and var(ri ) = var(r) are tested in constant time."
W10-2503,H05-1101,1,0.829406,"states are reachable and co-reachable) and unambiguous. In this case, for every γ1 · · · γk ∈ Γ∗ and p, p0 ∈ P there is at most one successful run r : [0, k] → P such that Rule factorization As already discussed, the time complexity of the product construction is an exponential function of the rank of the transducer. Unfortunately, it is not possible in the general case to cast a 23 WXTT into a normal form such that the rank is bounded by some constant. This is also expected from the fact that the translation problem for subclasses of WXTTs such as synchronous contextfree grammars is NP-hard (Satta and Peserico, 2005). Nonetheless, there are cases in which a rank reduction is possible, which might result in an improvement of the asymptotical run-time of our construction. Following the above line, we present here a linear time algorithm for reducing the rank of a WXTT under certain conditions. Similar algorithms for tree-based transformation devices have been discussed in the literature. Nesson et al. (2008) consider synchronous tree adjoining grammars; their algorithm is conceptually very similar to ours, but computationally more demanding due to the treatment of adjunction. Following that work, we also de"
W10-2503,N04-1035,0,0.0408941,"nt of the asymptotical run-time of our construction. Following the above line, we present here a linear time algorithm for reducing the rank of a WXTT under certain conditions. Similar algorithms for tree-based transformation devices have been discussed in the literature. Nesson et al. (2008) consider synchronous tree adjoining grammars; their algorithm is conceptually very similar to ours, but computationally more demanding due to the treatment of adjunction. Following that work, we also demand here that the new WXTT ‘preserves’ the recursive structure of the input WXTT, as formalized below. Galley et al. (2004) algorithm also behaves in linear time, but deals with the different problem of tree to string translation. Rank reduction algorithms for stringbased translation devices have also been discussed by Zhang et al. (2006) and Gildea et al. (2006). Recall that M = (Q, Σ, ∆, I, R) is a standard WXTT. Let M 0 = (Q0 , Σ, ∆, I 0 , R0 ) be a WXTT with Q ⊆ Q0 .3 Then M 0 is a structure-preserving factorization of M if • I 0 (q) = I(q) for every q ∈ Q and I 0 (q) = 0 otherwise, and • hpR10···pn (t, u)q = hpR1 ···pn (t, u)q for every q, p1 , . . . , pn ∈ Q, t ∈ TΣ (Xn ), and u ∈ T∆ (Xn ). In particular, we"
W10-2503,N06-1033,0,0.0222539,"rmation devices have been discussed in the literature. Nesson et al. (2008) consider synchronous tree adjoining grammars; their algorithm is conceptually very similar to ours, but computationally more demanding due to the treatment of adjunction. Following that work, we also demand here that the new WXTT ‘preserves’ the recursive structure of the input WXTT, as formalized below. Galley et al. (2004) algorithm also behaves in linear time, but deals with the different problem of tree to string translation. Rank reduction algorithms for stringbased translation devices have also been discussed by Zhang et al. (2006) and Gildea et al. (2006). Recall that M = (Q, Σ, ∆, I, R) is a standard WXTT. Let M 0 = (Q0 , Σ, ∆, I 0 , R0 ) be a WXTT with Q ⊆ Q0 .3 Then M 0 is a structure-preserving factorization of M if • I 0 (q) = I(q) for every q ∈ Q and I 0 (q) = 0 otherwise, and • hpR10···pn (t, u)q = hpR1 ···pn (t, u)q for every q, p1 , . . . , pn ∈ Q, t ∈ TΣ (Xn ), and u ∈ T∆ (Xn ). In particular, we have hR0 (t, u)q = hR (t, u)q for n = 0. Consequently, M 0 and M are equivalent because X M 0 (t, u) = I 0 (q) · hR0 (t, u)q Informally, a structure-preserving factorization of M consists in a set of new rules that c"
W10-2503,P06-2036,1,0.834975,"n discussed in the literature. Nesson et al. (2008) consider synchronous tree adjoining grammars; their algorithm is conceptually very similar to ours, but computationally more demanding due to the treatment of adjunction. Following that work, we also demand here that the new WXTT ‘preserves’ the recursive structure of the input WXTT, as formalized below. Galley et al. (2004) algorithm also behaves in linear time, but deals with the different problem of tree to string translation. Rank reduction algorithms for stringbased translation devices have also been discussed by Zhang et al. (2006) and Gildea et al. (2006). Recall that M = (Q, Σ, ∆, I, R) is a standard WXTT. Let M 0 = (Q0 , Σ, ∆, I 0 , R0 ) be a WXTT with Q ⊆ Q0 .3 Then M 0 is a structure-preserving factorization of M if • I 0 (q) = I(q) for every q ∈ Q and I 0 (q) = 0 otherwise, and • hpR10···pn (t, u)q = hpR1 ···pn (t, u)q for every q, p1 , . . . , pn ∈ Q, t ∈ TΣ (Xn ), and u ∈ T∆ (Xn ). In particular, we have hR0 (t, u)q = hR (t, u)q for n = 0. Consequently, M 0 and M are equivalent because X M 0 (t, u) = I 0 (q) · hR0 (t, u)q Informally, a structure-preserving factorization of M consists in a set of new rules that can be composed to provide"
W10-2503,N04-1014,0,0.0937314,"iform framework for the development of parsing and translation algorithms for weighted extended (top-down) tree transducers and input strings. The asymptotic time complexity of these algorithms can be improved in practice by exploiting an algorithm for rule factorization in the above transducers. 1 Introduction In the field of statistical machine translation, considerable interest has recently been shown for translation models based on weighted tree transducers. In this paper we consider the so-called weighted extended (top-down) tree transducers (WXTTs for short). WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). WXTTs have enough expressivity to represent hierarchical syntactic analyses for natural language sentences and can directly model most of the elementary operations that rule the process of translation between natural languages (Knight, 2007). Furthermore, the use of weights and internal states allows the encoding of statistical parameters that have recently been shown to be extremely useful in discriminating likely translations from less plausible ones. For an W"
W10-2503,J08-3004,0,0.0189669,"should be added to obtain a single standard rule. To keep the presentation simple, we also construct nonstandard WXTTs in the sequel. However, we implicitly assume that those are converted into standard WXTTs. The semantics of a standard WXTT is inspired by the initial-algebra semantics for classical weighted top-down and bottom-up tree transducers (F¨ul¨op and Vogler, 2009) [also called topdown and bottom-up tree series transducers by Engelfriet et al. (2002)]. Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al. (2008), for example. In fact, we will present an equivalent semantics based on runs later. Let M = (Q, Σ, ∆, I, R) be a WXTT. We present a definition that is more general than immediately necessary, but the generalization will be useful later on. For every n ∈ N, p1 , . . . , pn ∈ Q, and L ⊆ R, we define the mapping hpL1 ···pn : TΣ (Xn ) × T∆ (Xn ) → S Q by hpL1 ···pn (xi , xi )pi = 1 for every 1 ≤ i ≤ n and for every σ ∈ Σk , t1 , . . . , tk ∈ TΣ (V ), 1 ≤ i ≤ k, and w ∈ Pos(ti ). Finally, the set of variables var(t) is given by var(t) = {v ∈ V |∃w ∈ Pos(t) : t(w) = v} . If for every v ∈ var(t) the"
W10-2503,D09-1005,0,0.0169322,"e input product transducer Mw = Prod(Nw , M ) provides a compact representation of the set of all computations of M that translate the string w. From Corollary 5 we have that the weights of these computationsPare also preserved. Thus, Mw (TΣ × T∆ ) = (t,u)∈TΣ ×T∆ Mw (t, u) is the weight of the set of string translations of w. As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (TΣ × T∆ ), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. One could also construct in linear time the range tree automaton for Mw , which can be interpreted as a parsing forest with all the weighted trees assigned to translations of w under M . If we further assume that M is unambiguous, then Mw will also have this property, and we can apply standard techniques to extract from Mw the highest score computation. In machine translation applications, the unambiguity assumption is usually met, and avoids the so-called ‘spurious’ ambiguity, that is, having several computations for an individual pair of trees. The parsing problem fo"
W11-2919,P99-1070,0,0.131943,"Missing"
W11-2919,E09-1055,1,0.917854,"ism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of multiple context-free grammars, such as those considered by Kato et al. (2006). Nonterminals in a PLCFRS can generate discontinuous constituents. For this reason, (P)LCFRSs have recently been used to model discontinuous phrase structure treebanks as well as non-projective dependency treebanks; see (Maier and Lichte, 2009; Kuhlmann and Satta, 2009; Kallmeyer and Maier, 2010). The main contribution of this paper is a method for computing prefix probabilities for PLCFRSs. We are not aware of any existing algorithm in the literature for this task. Our method implies existence of algorithms for the computation of prefix probabilities for probabilistic versions of forWe present a novel method for the computation of prefix probabilities for linear context-free rewriting systems. Our approach streamlines previous procedures to compute prefix probabilities for context-free grammars, synchronous context-free grammars and tree adjoining grammars"
W11-2919,P81-1022,0,0.334633,"Missing"
W11-2919,P09-1111,1,0.900299,"Missing"
W11-2919,J09-4009,0,0.0325231,"some constant, then composition can be carried out in polynomial time. The process of reducing the length of rules in a LCFRS is called factorization. It is known that not all LCFRSs can be factorized in such a way that each rule has length bounded by some constant (Rambow and Satta, 1999). However, in the context of natural language parsing, it has been observed that the vast majority of rules in real world applications can be factorized to some small length, and that excluding the worst-case rules which cannot be handled in this way does not significantly affect accuracy; see for instance (Huang et al., 2009) and (Kuhlmann and Satta, 2009) for discussion. Efficient algorithms for factorization of LCFRSs have been presented by Kuhlmann and Satta (2009), G´omez-Rodr´ıguez and Satta (2009) and Sagot and Satta (2010). We have thus reduced the problem of computing the inside probability of w under G to the problem of computing the values of the partition function for G 0 . Because LG (w) can be less than 1, it is clear that G 0 need not be consistent, even if we assume that G is. As we have discussed in Section 2, if G 0 is any PLCFRS that may not be proper or consistent, then the values LG 0 (A) for t"
W11-2919,J91-3004,0,0.43936,"on There are a number of problems related to probabilistic grammatical formalisms that involve summing an infinite number of values. For example, if P is a probability distribution over strings defined by a probabilistic grammar, and w is a string, then the prefix probability of w is defined to be: X P (wv) v In words, all possible suffixes v that may follow prefix w are considered, and the probabilities of the concatenations of v and w are summed. Prefix probabilities can be exploited to predict the next word or part of speech, for incremental processing of text or speech from left to right (Jelinek and Lafferty, 1991). They can also be used in speech processing to score partial hypotheses in beam search (Corazza et al., 1991). At first sight, it is not clear that prefix probabilities can be effectively computed, as the number of possible strings v is infinite. It was shown however by Jelinek and Lafferty (1991) that in the case of probabilistic context-free grammars, the infinite sums can be isolated from any particular w, and these sums can be computed off-line by solving linear systems of equations. For any particular w, the prefix probability can then be computed in cubic time in the length of w, on the"
W11-2919,P04-1084,1,0.921457,"linek and Lafferty (1991) that in the case of probabilistic context-free grammars, the infinite sums can be isolated from any particular w, and these sums can be computed off-line by solving linear systems of equations. For any particular w, the prefix probability can then be computed in cubic time in the length of w, on the 151 Proceedings of the 12th International Conference on Parsing Technologies, pages 151–162, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. malisms that are special cases of LCFRSs, such as the generalized multitext grammars of Melamed et al. (2004), which are used to model translation, and the already mentioned formalism proposed by Kuhlmann and Satta (2009) to model non-projective dependency structures. We follow essentially the same approach as Nederhof and Satta (2011b), and reduce the problem of computing the prefix probabilities for PLCFRSs to the well-known problem of computing inside probabilities for PLCFRSs. The reduction is obtained by the composition of a PLCFRS with a special finite-state transducer. Most importantly, this composition is independent of the specific input string for which we need to solve the prefix probabili"
W11-2919,C10-1061,0,0.0254805,"context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of multiple context-free grammars, such as those considered by Kato et al. (2006). Nonterminals in a PLCFRS can generate discontinuous constituents. For this reason, (P)LCFRSs have recently been used to model discontinuous phrase structure treebanks as well as non-projective dependency treebanks; see (Maier and Lichte, 2009; Kuhlmann and Satta, 2009; Kallmeyer and Maier, 2010). The main contribution of this paper is a method for computing prefix probabilities for PLCFRSs. We are not aware of any existing algorithm in the literature for this task. Our method impli"
W11-2919,P87-1015,0,0.394068,"uniquely identified with one rule π. The rank of LCFRS G, written ρ(G), is the maximum rank among all rules of G. The fanout of LCFRS G, written φ(G), is the maximum fan-out among all nonterminals of G. Let a rule π be: π : A → g(A1 , A2 , . . . , Ar ), where g( hx1,1 , . . . , x1,φ(A1 ) i, ..., hxr,1 , . . . , xr,φ(Ar ) i ) = h y1,1 · · · y1,m1 , ..., yφ(A),1 · · · yφ(A),mφ(A) i Definitions This section summarizes the terminology and notation of linear context-free rewriting systems, and their probabilistic extension. For more detailed definitions on linear context-free writing systems, see Vijay-Shanker et al. (1987). For an integer n ≥ 1, we write [n] to denote the set {1, . . . , n} and [0] = ∅. We write [n]0 to denote [n] ∪ {0}. A linear context-free rewriting system (LCFRS for short) is a tuple G = (N, Σ, P, S), where N and Σ are finite, disjoint sets of nonterminal and terminal symbols, respectively. Each A ∈ N is associated with an integer value φ(A) ≥ 1, called its fan-out. The nonterminal S is the start symbol, with φ(S) = 1. 152 A LCFRS is said to be reduced if the function g from each rule occurs in some derivation from D(S). Because each function uniquely identifies a rule, this means that also"
W11-2919,D11-1112,1,0.666842,"reas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of"
W11-2919,P11-1047,1,0.710507,"reas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of"
W11-2919,C98-2152,1,0.826569,"versity of St Andrews North Haugh, St Andrews, Fife KY16 9SX United Kingdom Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy Abstract basis of the values computed off-line. Whereas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely"
W11-2919,P10-1054,1,0.819341,"such a way that each rule has length bounded by some constant (Rambow and Satta, 1999). However, in the context of natural language parsing, it has been observed that the vast majority of rules in real world applications can be factorized to some small length, and that excluding the worst-case rules which cannot be handled in this way does not significantly affect accuracy; see for instance (Huang et al., 2009) and (Kuhlmann and Satta, 2009) for discussion. Efficient algorithms for factorization of LCFRSs have been presented by Kuhlmann and Satta (2009), G´omez-Rodr´ıguez and Satta (2009) and Sagot and Satta (2010). We have thus reduced the problem of computing the inside probability of w under G to the problem of computing the values of the partition function for G 0 . Because LG (w) can be less than 1, it is clear that G 0 need not be consistent, even if we assume that G is. As we have discussed in Section 2, if G 0 is any PLCFRS that may not be proper or consistent, then the values LG 0 (A) for the different nonterminals of G 0 can be expressed in terms of a system of equations. Solving such equations can be computationally expensive. A more efficient way to compute the values LG 0 (A) is possible ho"
W11-2919,P92-1012,1,0.653713,"π |new rules in G0 that are derived from π, where each new rule has size O(|π|). Thus the target grammar has size exponential in |G|. Our algorithm for composition can be easily implemented to run in time O(|G0 |), that is, in linear time in the size of the output grammar. Because of the above discussion, the algorithm runs in exponential time in the size of the input. Exponential time for the composition construction is not unexpected: the problem at hand is a generalization of the parsing problem for LCFRS, and the latter problem is known to be NP-hard when the grammar is part of the input (Satta, 1992). The critical term in the above analysis is |π|. If we can cast our LCFRS in a form in which each rule has length bounded by some constant, then composition can be carried out in polynomial time. The process of reducing the length of rules in a LCFRS is called factorization. It is known that not all LCFRSs can be factorized in such a way that each rule has length bounded by some constant (Rambow and Satta, 1999). However, in the context of natural language parsing, it has been observed that the vast majority of rules in real world applications can be factorized to some small length, and that"
W11-2919,J95-2002,0,0.527676,"for Linear Context-Free Rewriting Systems Mark-Jan Nederhof School of Computer Science University of St Andrews North Haugh, St Andrews, Fife KY16 9SX United Kingdom Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy Abstract basis of the values computed off-line. Whereas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a s"
W11-2919,P98-2157,1,\N,Missing
W19-3104,P17-1104,0,0.0182951,"roach to the syntax-semantics interface exploits multi-component synchronous tree-adjoining grammars; see Nesson and Shieber (2006) and references therein. However, these formal models yield tree-like semantic representations, as opposed to general graphs. A common approach in semantic parsing is to extend existing syntactic dependency parsers to produce graphs, realizing translation models from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupe"
W19-3104,D17-1130,0,0.0193403,"formalism and ours is made in Remark 1. An alternative approach to the syntax-semantics interface exploits multi-component synchronous tree-adjoining grammars; see Nesson and Shieber (2006) and references therein. However, these formal models yield tree-like semantic representations, as opposed to general graphs. A common approach in semantic parsing is to extend existing syntactic dependency parsers to produce graphs, realizing translation models from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous"
W19-3104,C12-1083,0,0.0596709,"Missing"
W19-3104,W13-2322,0,0.0236221,"ence resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom-up tree transducers (Thatcher, 1973) have gained significant attention in the field of machine translation, where they are used to map syntactic phrase structure trees from source to target 7 Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing, pages 7–17 c Dresden, Germany, Septembe"
W19-3104,Q15-1040,0,0.0192351,"from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupervised learning algorithms for grammar extraction. Finally, Groschwitz et al. (2018) use a neural supertag parser to map a string into a dependency-style tree representation of the com2 Preliminaries In this section we introduce the notation and terminology that is used throughout this paper. General Notation. The set of natural numbers (including zero) is denoted by N, and N+ = N  {0}. For n"
W19-3104,J16-4009,0,0.0181745,"a natural language sentence and has to output a directed graph representing an associated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom"
W19-3104,E17-1051,1,0.839032,"k 1. An alternative approach to the syntax-semantics interface exploits multi-component synchronous tree-adjoining grammars; see Nesson and Shieber (2006) and references therein. However, these formal models yield tree-like semantic representations, as opposed to general graphs. A common approach in semantic parsing is to extend existing syntactic dependency parsers to produce graphs, realizing translation models from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement g"
W19-3104,S15-2153,0,0.0903727,"Missing"
W19-3104,S14-2008,0,0.0288759,"r. 1 Frank Drewes Ume˚a University Ume˚a, Sweden Introduction In dependency semantic parsing, one is given a natural language sentence and has to output a directed graph representing an associated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based system"
W19-3104,P18-1170,0,0.0364482,"Missing"
W19-3104,K15-1004,0,0.0190018,"by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupervised learning algorithms for grammar extraction. Finally, Groschwitz et al. (2018) use a neural supertag parser to map a string into a dependency-style tree representation of the com2 Preliminaries In this section we introduce the notation and terminology that is used throughout this paper. General Notation. The set of natural numbers (including zero) is denoted by N, and N+ = N  {0}. For n ∈ N the set {1, . . . , n} is abbreviated to [n]. In particular, [0] = ∅. The set of all finite sequences of elements of a set S is wr"
W19-3104,S15-1031,0,0.0242301,"ed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupervised learning algorithms for grammar extraction. Finally, Groschwitz et al. (2018) use a neural supertag parser to map a string into a dependency-style tree representation of the com2 Preliminaries In this section we introduce the notation and terminology that is used throughout this paper. General Notation. The set of natural numbers (including zero) is denoted by N, and N+ = N  {0}. For n ∈ N the set {1, . ."
W19-3104,N18-2023,0,0.0336067,"Missing"
W19-3104,P15-2141,0,0.291807,"ociated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom-up tree transducers (Thatcher, 1973) have gained significant attention in"
W19-3104,N15-1040,0,0.40211,"ociated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom-up tree transducers (Thatcher, 1973) have gained significant attention in"
W89-0205,J76-4004,0,0.144045,"Missing"
W89-0205,E87-1037,0,0.0405823,"Missing"
W98-0130,J91-3004,0,0.0372569,"n.edu Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova satta©dei.unipd.it Abstract vVe show how prefix probabilities can be computed for stochastic linear indexed grammars (SLIGs). Our results apply as weil to stochastic tree-adjoining grammars (STAGs), due to their equivalence to SLIGs. 1 Introd uction Thc problcm of computing prefix probabilities for stochastic context-free languages is defined as follows. Given a word sequence ai ···an over some alphabet E, which we call the input prefix, we must compute quantity LweE• Pr(a1 · · ·anw). This problem has been discussed in [1, 4] with the main motivation of applications in speech recognition, where we are given some word sequence a1 • • • an-li and must hypothesize the next word an. The main idea leading to the solution of this problem is that all parts of context-free derivations that are potentially of unbounded size are captured into a set of equations that can be solved &quot;off-line&quot;, i.e., before a specific prefix is considered. This is possible because the involved deriva.tions do not depend on the given prefix. Once these equations have been solved, the results are stored. When computing the prefix probability for"
W98-0130,C98-2152,1,0.731264,"input symbols. The probabilities of the former subderivations can be computed off-line, and the results are combined with subderivations of the latter kind during computation of the prefix probability for a given string. The distinction between the two kinds of subderivations requires a certain notational system that is difficult to define for tree-adjoining grammars. We will therefore concentrate on stochastic linear indexed grammars instead, relying on their equivalence to STAGs [3]. The solution proposed in the present paper is an alternative to a different approach by the same authors in [2]. In that publication, a set of equations is transformed in order to distinguish off-line and on-line computations. 2 Computation of prefix probabilities We refer the reader to [2] for the definition ofLIG. In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol. Without loss of generality we require that rules are of the form A[17 oo] - a B[17&apos; oo] ß with 11111&apos;1 = 1, or of the form A[] - z, where lzl :::; 1. As usual, - is extended to a binary relation b"
W98-0130,C92-2066,0,0.0278857,"ded size and are -* 116 independent on actual input, and parts that are always of bounded length and do depend on input symbols. The probabilities of the former subderivations can be computed off-line, and the results are combined with subderivations of the latter kind during computation of the prefix probability for a given string. The distinction between the two kinds of subderivations requires a certain notational system that is difficult to define for tree-adjoining grammars. We will therefore concentrate on stochastic linear indexed grammars instead, relying on their equivalence to STAGs [3]. The solution proposed in the present paper is an alternative to a different approach by the same authors in [2]. In that publication, a set of equations is transformed in order to distinguish off-line and on-line computations. 2 Computation of prefix probabilities We refer the reader to [2] for the definition ofLIG. In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol. Without loss of generality we require that rules are of the form A[17 oo] - a B[17&apos;"
W98-0130,J95-2002,0,0.0273054,"n.edu Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova satta©dei.unipd.it Abstract vVe show how prefix probabilities can be computed for stochastic linear indexed grammars (SLIGs). Our results apply as weil to stochastic tree-adjoining grammars (STAGs), due to their equivalence to SLIGs. 1 Introd uction Thc problcm of computing prefix probabilities for stochastic context-free languages is defined as follows. Given a word sequence ai ···an over some alphabet E, which we call the input prefix, we must compute quantity LweE• Pr(a1 · · ·anw). This problem has been discussed in [1, 4] with the main motivation of applications in speech recognition, where we are given some word sequence a1 • • • an-li and must hypothesize the next word an. The main idea leading to the solution of this problem is that all parts of context-free derivations that are potentially of unbounded size are captured into a set of equations that can be solved &quot;off-line&quot;, i.e., before a specific prefix is considered. This is possible because the involved deriva.tions do not depend on the given prefix. Once these equations have been solved, the results are stored. When computing the prefix probability for"
