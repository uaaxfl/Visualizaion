2001.jeptalnrecital-poster.7,M95-1012,0,0.0444057,"Missing"
2001.jeptalnrecital-poster.7,P98-1045,0,0.0602964,"Missing"
2002.jeptalnrecital-long.23,C94-2119,0,0.0357062,"Missing"
2005.jeptalnrecital-long.18,W00-0904,0,0.0693479,"Missing"
2005.jeptalnrecital-long.18,J96-2002,0,0.07194,"Missing"
2005.jeptalnrecital-long.18,C88-1036,0,0.278549,"Missing"
2005.jeptalnrecital-long.18,P98-2172,0,0.0250913,"Missing"
2005.jeptalnrecital-long.18,sekine-nobata-2004-definition,0,0.0364705,"Missing"
2009.jeptalnrecital-long.5,W03-1019,0,0.0755796,"Missing"
2009.jeptalnrecital-long.5,A00-2031,0,0.100384,"Missing"
2009.jeptalnrecital-long.5,W05-0622,0,0.0417204,"Missing"
2009.jeptalnrecital-long.5,P08-1109,0,0.0252866,"Missing"
2009.jeptalnrecital-long.5,J93-2004,0,0.0309833,"Missing"
2009.jeptalnrecital-long.5,W03-0430,0,0.0563435,"Missing"
2009.jeptalnrecital-long.5,H05-1078,0,0.0397979,"Missing"
2009.jeptalnrecital-long.5,W09-1214,1,0.841196,"Missing"
2009.jeptalnrecital-long.5,W05-1509,0,0.0233081,"Missing"
2009.jeptalnrecital-long.5,schluter-van-genabith-2008-treebank,0,0.0382095,"Missing"
2009.jeptalnrecital-long.5,N03-1028,0,0.0726178,"Missing"
2020.cl-4.5,E17-1088,0,0.0206243,"-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steerin"
2020.cl-4.5,D18-1214,0,0.0617967,"Missing"
2020.cl-4.5,P18-1073,0,0.159528,"to principal component analysis) from the input distributional word vectors, since they do not contribute toward distinguishing the actual semantic meaning of different words. The method contains a single (tunable) hyperparameter ddA , which denotes the number of the dominating directions to remove from the initial representations. Previous work has verified the usefulness of ABTT in several English lexical semantic tasks such as semantic similarity, word analogies, and concept categorization, as well as in sentence-level text classification tasks (Mu, Bhat, and Viswanath 2018). (3) UNCOVEC (Artetxe et al. 2018) adjusts the similarity order of an arbitrary input word embedding space, and can emphasize either syntactic or semantic information in the transformed vectors. In short, it transforms the input space X into an adjusted space XWα through a linear map Wα controlled by a single hyperparameter α. The nth -order similarity transformation of the input word vector space X (for which n = 1) can be obtained as M n (X) = M 1 (XW (n − 1)/2 ), with Wα = QΓ α , where Q and Γ are the matrices obtained via eigendecomposition of X T X = QΓQT . Γ is a diagonal matrix containing eigenvalues of X T X; Q is an o"
2020.cl-4.5,P98-1013,0,0.103027,"Missing"
2020.cl-4.5,J82-2005,0,0.629873,"Missing"
2020.cl-4.5,L18-1618,0,0.021365,"999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 2008). For instance, the two senses that are distinguished in English as time and weather are co-lexified in Croatian: the word vrijeme is used in both cases. 854 Vuli´c et al. Multi-SimLex resolution of translat"
2020.cl-4.5,N18-1083,0,0.0292232,"ially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents"
2020.cl-4.5,J19-2006,0,0.0292702,"es to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents share the same (func"
2020.cl-4.5,E17-2036,0,0.0144067,"2) Source: SemEval-17: Task 2 (henceforth SEMEVAL-500; Camacho-Collados et al. 2017). We start from the full data set of 500 concept pairs to extract a total of 334 concept pairs for English Multi-SimLex a) which contain only single-word concepts, b) which are not named entities, c) where POS tags of the two concepts are the same, d) where both concepts occur in the top 250K most frequent word types in the English Wikipedia, and e) which do not already occur in SimLex-999. The original concepts were sampled as to span all the 34 domains available as part of BabelDomains (Camacho-Collados and Navigli 2017), which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample. 3) Source: CARD-660 (Pilehvar et al. 2018). Sixty-seven word pairs are taken from this data set focused on rare word similarity, applying the same selection criteria a to e utilized for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus (Baroni et al. 2009). CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia), and slang (2mrw), which might be difficult to tr"
2020.cl-4.5,S17-2002,0,0.0611659,"uli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 849 Computational Linguistics Volume 46, Numbe"
2020.cl-4.5,D14-1082,0,0.0211604,"Missing"
2020.cl-4.5,2020.acl-main.747,0,0.168947,"Missing"
2020.cl-4.5,D18-1269,0,0.383476,"the coverage also to languages that are resourcelean and/or typologically diverse (e.g., Welsh, Kiswahili, as in this work). Multilingual Data Sets for Natural Language Understanding. The Multi-SimLex initiative and corresponding data sets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual BERT (Devlin et al. 2019) or XLM (Conneau and Lample 2019) are typically probed on XNLI test data (Conneau et al. 2018b) for crosslingual natural language inference. XNLI was created by translating examples from the English MultiNLI data set, and projecting its sentence labels (Williams, Nangia, and Bowman 2018). Other recent multilingual data sets target the task of question answering based on reading comprehension: i) MLQA (Lewis et al. 2019) includes 7 languages; ii) XQuAD (Artetxe, Ruder, and Yogatama 2019) 10 languages; and iii) TyDiQA (Clark et al. 2020) 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English data set, TyDiQA was built independen"
2020.cl-4.5,Q19-1041,1,0.810306,"ingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 diffe"
2020.cl-4.5,C18-1323,0,0.354998,"6), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 8"
2020.cl-4.5,D19-1006,0,0.0554256,"n 2019). 875 Computational Linguistics Volume 46, Number 4 Impact of Unsupervised Post-Processing. First, the results in Table 12 suggest that applying dimension-wise mean centering to the initial vector spaces has positive impact on word similarity scores in all test languages and for all models, both static and contextualized (see the + MC rows in Table 12). Mimno and Thompson (2017) show that distributional word vectors have a tendency toward narrow clusters in the vector space (i.e., they occupy a narrow cone in the vector space and are therefore anisotropic [Mu, Bhat, and Viswanath 2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vu"
2020.cl-4.5,N15-1184,0,0.266878,"trinsic evaluations of specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This make"
2020.cl-4.5,N18-2029,1,0.762404,"Missing"
2020.cl-4.5,P19-1070,1,0.912198,"Missing"
2020.cl-4.5,Q16-1002,1,0.880245,"Missing"
2020.cl-4.5,J15-4004,1,0.93809,"Missing"
2020.cl-4.5,D18-1043,0,0.0596882,"Missing"
2020.cl-4.5,2020.acl-main.560,0,0.0211952,"guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zema"
2020.cl-4.5,D18-1330,0,0.0361587,"Missing"
2020.cl-4.5,W14-1503,0,0.0323243,"wski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney (2012), Kiela and Clark (2014), and Melamud et al. (2016) demonstrated that different choices of hyperparameters in WE algorithms (such as context window) emphasize different relations in the resulting representations. Likewise, Agirre et al. (2009) and Levy and Goldberg (2014) discovered that WEs learned from texts annotated with syntactic information mirror similarity better than simple local bag-of-words neighborhoods. The failure of WEs to capture semantic similarity, in turn, affects model performance in several NLP applications where such knowledge is crucial. In particular, Natural Language Understanding tasks such"
2020.cl-4.5,W16-1607,0,0.0601778,"Missing"
2020.cl-4.5,kipper-etal-2004-extending,0,0.0569229,"Missing"
2020.cl-4.5,D19-1279,0,0.0717479,"Missing"
2020.cl-4.5,2020.emnlp-main.363,1,0.891144,"Missing"
2020.cl-4.5,P15-1027,0,0.0827227,"Missing"
2020.cl-4.5,P14-2050,0,0.0586512,"their associated meaning confounds the two distinct relations (Hill, Reichart, and Korhonen 2015; Schwartz, Reichart, and Rappoport 2015; Vuli´c et al. 2017b). As a result, distributional methods obscure a crucial facet of lexical meaning. This limitation also reflects onto word embeddings (WEs), representations of words as low-dimensional vectors that have become indispensable for a wide range of NLP applications (Collobert et al. 2011; Chen and Manning 2014; Melamud et al. 2016, inter alia). In particular, it involves both static WEs learned from co-occurrence patterns (Mikolov et al. 2013; Levy and Goldberg 2014; Bojanowski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney"
2020.cl-4.5,2020.emnlp-main.484,0,0.0477066,"Missing"
2020.cl-4.5,D18-1521,0,0.0264898,"Missing"
2020.cl-4.5,D17-1308,0,0.0696529,"Missing"
2020.cl-4.5,N19-1386,0,0.0334837,"Missing"
2020.cl-4.5,Q17-1022,1,0.934218,"Missing"
2020.cl-4.5,L18-1381,0,0.0518909,"Missing"
2020.cl-4.5,D18-1169,0,0.15982,"me prominent English word pair data sets such as WordSim-353 (Finkelstein et al. 2002), MEN (Bruni, Tran, and Baroni 2014), or Stanford Rare Words (Luong, Socher, and Manning 2013) did not discriminate between similarity and relatedness, the importance of this distinction was established by Hill, Reichart, and Korhonen (2015) (see again the discussion in §2.1) through the creation of SimLex-999. This inspired other similar data sets that focused on different lexical properties. For instance, SimVerb-3500 (Gerz et al. 2016) provided similarity ratings for 3,500 English verbs, whereas CARD-660 (Pilehvar et al. 2018) aimed at measuring the semantic similarity of infrequent concepts. Semantic Similarity Data Sets in Other Languages. Motivated by the impact of data sets such as SimLex-999 and SimVerb-3500 on representation learning in English, a line of related work put focus on creating similar resources in other languages. The dominant approach is translating and reannotating the entire original English SimLex-999 data set, as done previously for German, Italian, and Russian (Leviant and Reichart 2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venek"
2020.cl-4.5,P19-1493,0,0.0740204,"Missing"
2020.cl-4.5,J19-3005,1,0.889644,"Missing"
2020.cl-4.5,D18-1026,1,0.925849,"Missing"
2020.cl-4.5,Q17-1020,0,0.0167721,"that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation"
2020.cl-4.5,D18-1299,0,0.0140965,"i.e., the distributional information).1 Data sets that quantify the strength of semantic similarity between concept pairs such as SimLex-999 (Hill, Reichart, and Korhonen 2015) or SimVerb-3500 (Gerz et al. 2016) have been instrumental in improving models for distributional semantics and representation learning. Discerning between semantic similarity and relatedness/association is not only crucial for theoretical studies on lexical semantics (see §2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking (Mrkˇsi´c et al. 2017; Ren et al. 2018), spoken language understanding (Kim et al. 2016; Kim, de Marneffe, and Fosler-Lussier 2016), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Eve"
2020.cl-4.5,Q19-1044,1,0.813505,"elines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scal"
2020.cl-4.5,L18-1152,0,0.223909,"2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venekoski and Vankka (2017) applied this process only to a subset of 300 concept pairs from the English SimLex-999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 20"
2020.cl-4.5,P19-1072,0,0.0344182,"Missing"
2020.cl-4.5,K15-1026,1,0.904015,"Missing"
2020.cl-4.5,P18-1072,1,0.919137,"Missing"
2020.cl-4.5,N16-1161,0,0.0133804,"For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in XLM -100 but absent in 886 Vuli´c et al. Multi-SimLex M - BERT help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases (Littell et al. 2017) and from neural architectures (Malaviya, Neubig, and Littell 2017). Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work (Tsvetkov et al. 2016; Ammar et al. 2016; Ponti et al. 2019b) might extend the coverage to more resourcelean languages. Another important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual data sets for both word-level semantic similarity and sentence-level natural language understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put alongside existing resources such as XNLI (Conneau et al. 2018b) for natural language inference or"
2020.cl-4.5,P19-1490,1,0.894602,"Missing"
2020.cl-4.5,Q15-1025,0,0.0155434,"f specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This makes the comparison of s"
2020.cl-4.5,2020.acl-main.536,0,0.0322002,"Missing"
2020.cl-4.5,D19-1077,0,0.0931228,". Because the concept pairs in Multi-SimLex are lowercased, 12 We also tested another encoding method where we fed pairs instead of single words/concepts into the pretrained encoder. The rationale is that the other concept in the pair can be used as a disambiguation signal. However, this method consistently led to sub-par performance across all experimental runs. 873 Computational Linguistics Volume 46, Number 4 we use the uncased version of M - BERT.13 M - BERT comprises all Multi-SimLex languages, and its evident ability to perform crosslingual transfer (Pires, Schlinger, and Garrette 2019; Wu and Dredze 2019; Wang et al. 2020) also makes it a convenient baseline model for crosslingual experiments later in §8. The second multilingual model we consider, XLM -100,14 is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a 1,280-dimensional representation. In contrast to M - BERT, XLM -100 drops the next-sentence prediction objective and adds a crosslingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the first H = 4 hidden layers in all experiments.15 Besides M - BERT and XLM, covering multiple lang"
2020.cl-4.5,K18-2001,0,0.0637127,"Missing"
2020.cl-4.5,P19-1307,0,0.0177035,"2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vuli´c et al. 2019), and for modeling lexical semantic change (Schlechtweg et al. 2019). However, to the best of our knowledge, the results summarized in Table 12 are the first evidence that also confirms its importance for semantic similarity in a wide array of languages. In sum, as a general rule of thumb, we suggest always mean-centering representations for semantic tasks. The results further indicate that additional post-processing methods such as ABTT and UNCOVEC on top of mean-centered vector spaces can lead to further gains in most languages. The gains are even visible for languages t"
2020.cl-4.5,K19-1021,1,0.900219,"Missing"
2020.cl-4.5,C98-1013,0,\N,Missing
2020.cl-4.5,J10-4006,0,\N,Missing
2020.cl-4.5,P94-1019,0,\N,Missing
2020.cl-4.5,J06-1003,0,\N,Missing
2020.cl-4.5,N09-1003,0,\N,Missing
2020.cl-4.5,D14-1034,1,\N,Missing
2020.cl-4.5,W13-3512,0,\N,Missing
2020.cl-4.5,D15-1242,0,\N,Missing
2020.cl-4.5,P15-2001,0,\N,Missing
2020.cl-4.5,kamholz-etal-2014-panlex,0,\N,Missing
2020.cl-4.5,N15-1104,0,\N,Missing
2020.cl-4.5,N16-1060,1,\N,Missing
2020.cl-4.5,Q17-1010,0,\N,Missing
2020.cl-4.5,P16-1024,1,\N,Missing
2020.cl-4.5,J17-4004,1,\N,Missing
2020.cl-4.5,E17-1016,1,\N,Missing
2020.cl-4.5,E17-2002,0,\N,Missing
2020.cl-4.5,P17-1042,0,\N,Missing
2020.cl-4.5,P18-1004,1,\N,Missing
2020.cl-4.5,P18-1142,1,\N,Missing
2020.cl-4.5,D18-1027,0,\N,Missing
2020.cl-4.5,D18-1024,0,\N,Missing
2020.cl-4.5,K18-1028,0,\N,Missing
2020.cl-4.5,N19-1391,0,\N,Missing
2020.cl-4.5,N19-1131,0,\N,Missing
2020.cl-4.5,K17-1013,1,\N,Missing
2020.cl-4.5,N19-1423,0,\N,Missing
2020.cl-4.5,N18-1101,0,\N,Missing
2020.cl-4.5,P19-4007,1,\N,Missing
2020.cl-4.5,W19-4310,1,\N,Missing
2020.cl-4.5,D19-1449,1,\N,Missing
2020.cl-4.5,D19-1288,1,\N,Missing
2020.cl-4.5,D19-1226,1,\N,Missing
2020.cl-4.5,D19-1165,0,\N,Missing
2020.cl-4.5,K19-1004,1,\N,Missing
2020.cl-4.5,D19-2007,1,\N,Missing
2020.cl-4.5,W17-0228,0,\N,Missing
2020.latechclfl-1.16,P17-4008,0,0.0260212,"libray). The BnF later gave us access to an even larger corpus of French poems1 , so that the implementation currently integrates a corpus of more than 4,000 French sonnets. All major French authors from the 19th century are included in the database, but also some less known ones. Each sonnet is encoded in a XML format along with related metadata; a TEI version of the database is publicly available (see https://github.com/clement-plancq/oupoco-api) and is regularly expanding. 3 Corpus and Rhyme Analysis The OuPoCo project has nothing to do with the recent neural approach to poetry generation (Ghazvininejad et al., 2017; Van de Cruys, 2020), but it requires to get access to a formal representation of rhymes (as proposed by (Beaudouin, 2002)). In order to do this, the first step is to get a phonetic transcription of the last word of each verse, but this is not enough: for example, “aim´e” and “aim´ee” have the same phonetic transcription, but do not rhyme, according to French rhyming rules (feminine and masculine words, for example words that end with -´e, as opposed to -´ee, do not rhyme); there are also cases where the phonetic transcription is slightly different but words actually rhyme (for example with s"
2020.latechclfl-1.16,2020.acl-main.223,0,0.0869879,"Missing"
C02-1062,J91-4003,0,0.0221662,"ontinental structuralism like decomponential semantics (Cavazza, 1998). The problem is then to have a lexical formalism that allows, for a lexical item, a simple description and some other features which could be dynamically inferred from the text. For example, the dictionary should mention that a “door” is an aperture, but it is more questionable to mention in the dictionary that “one can walk through a door”. However, it can be an important point for the interpretation of a sentence in context. That is the reason why Pustejovsky introduced in the nineties the notion of “generative lexicon” (Pustejovsky, 1991) (Pustejovsky, 1995). His analysis has to deal with the notion of context: he proposes to associate to a word a core semantic description (the fact that a “door” is an “aperture”) and to add some additional features, which can be activated in context (“walk-through” is the telic role of a “door”). However, Pustejovsky does not take into account important notions such as lexical chains and text coherence. He proposes an abstract model distant from real texts. Semantic features can be used to check out text coherence through the notion of “isotopy”. This notion is “the recurrence within a given"
C04-1092,O97-1002,0,0.0189656,"Missing"
C04-1092,C02-1144,0,0.0147716,"they describe a predicative sequence. All these stages are described below, after the description of similarity measures allowing to calculate the semantic proximity between words. Seed pattern selection Semantic net End-user input Paraphrase acquisition Validation Corpus Syntactic expansion Interaction with the end-user Automatic step Semantic expansion Figure 1: Outline of the acquisition process 4 Similarity measures Several studies have recently proposed measures to calculate the semantic proximity between words. Different measures have been proposed, which are not easy to evaluate (see (Lin and Pantel, 2002) for proposals). The methods proposed so far are automatic or manual and generally imply the evaluation of word clusters in different contexts (a word cluster is close to another one if the words it contains are interchangeable in some linguistic contexts). Budanitsky and Hirst (2001) present the evaluation of 5 similarity measures based on the structure of Wordnet. All the algorithms they examine are based on the hypernymhyponym relation which structures the classification of clusters inside Wordnet (the synsets). They sometimes obtain unclear conclusions about the reason of the performances"
C04-1092,A97-1029,0,0.0212178,"ly imply the evaluation of word clusters in different contexts (a word cluster is close to another one if the words it contains are interchangeable in some linguistic contexts). Budanitsky and Hirst (2001) present the evaluation of 5 similarity measures based on the structure of Wordnet. All the algorithms they examine are based on the hypernymhyponym relation which structures the classification of clusters inside Wordnet (the synsets). They sometimes obtain unclear conclusions about the reason of the performances of the different algorithms (for example, comparing Jiang and Conrath’s measure (1997) with Lin’s one (1998): “It remains unclear, however, just why it performed so much better than Lin’s measure, which is but a different arithmetic combination of the same terms”). However, the authors emphases on the fact that the use of the sole hyponym relation is insufficient to capture the complexity of meaning: “Nonetheless, it remains a strong intuition that hyponymy is only one part of semantic relatedness; meronymy, such as w h e e l – c a r, is most definitely an indicator of semantic relatedness, and, a fortiori, semantic relatedness can arise from little more than common or stereoty"
C04-1092,P99-1050,0,0.114295,"o give valuable results for a wide variety of applications, including text filtering and information extraction (Poibeau et al., 2002). 5 The acquisition process The process begins as the end-user provides a predicative linguistic structure to the system along with a representative corpus. The system tries to discover relevant parts of text in the corpus based on the presence of plain words closely related to the ones of the seed pattern. A syntactic analysis of the sentence is then done to verify that these plain words correspond to a paraphrastic structure. The method is close to the one of Morin and Jacquemin (1999), who first try to locate couples of relevant terms and then apply relevant patterns to analyse the nature of their relationship. However, Morin and Jacquemin only focus on term variations whereas we are interested in predicative structures, being either verbal or nominal. The syntactic variations we have to deal with are then different and, for a part, more complex than the ones examined by Morin and Jacquemin. The detail algorithm is described below: 1. The head noun of the example pattern is compared with the head noun of the candidate pattern using the proximity measure from (Dutoit et al."
C04-1092,W99-0613,0,0.0112284,"very first systems using a simple form of learning to build a dictionary of extraction patterns. Ciravegna (2001) demonstrates the interest of independent acquisition of left and right boundaries of extraction patterns during the learning phase. In general, the left part of a pattern is easier to acquire than the right part and some heuristics can be applied to infer the right boundary from the left one. The same method can be applied for argument acquisition: each argument can be acquired independently from the others since the argument structure of a predicate in context is rarely complete. Collins and Singer (1999) demonstrate how two classifiers operating on disjoint features sets recognize named entities with very little supervision. The method is interesting in that the analyst only needs to provide some seed examples to the system in order to learn relevant information. However, these classifiers must be made interactive in order not to diverge from the expected result, since each error is transmitted and amplified by subsequent processing stages. Contrary to this approach, partially reproduced by Duclaye et al. (2003) for paraphrase learning, we prefer a slightly supervised method with clear intera"
C04-1092,C02-1062,1,0.898391,"ibed in (Dutoit et al., 2002) which is based on a knowledge-rich semantic net encoding a large variety of semantic relationships between set of words, including meronymy and stereotypical associations. The semantic distance between two words A and B is based on the notion of nearest common ancestors (NCA) between A and B . NCA is defined as the set of nodes that are daughters of c(A) ∩ c(B) and that are not ancestors in c(A) ∩ c(B). The activation measure d_ is equal to the mean of the weight of each NCA calculated from A and B : n d (A, B) = 1 ∑(d(A,NCAi)+d(B,NCAi)) n i =1 Please, refer to (Dutoit and Poibeau, 2002) for more details and examples. However, this measure is sensitive enough to give valuable results for a wide variety of applications, including text filtering and information extraction (Poibeau et al., 2002). 5 The acquisition process The process begins as the end-user provides a predicative linguistic structure to the system along with a representative corpus. The system tries to discover relevant parts of text in the corpus based on the presence of plain words closely related to the ones of the seed pattern. A syntactic analysis of the sentence is then done to verify that these plain words"
C04-1092,poibeau-etal-2002-evaluating,1,0.856695,"c distance between two words A and B is based on the notion of nearest common ancestors (NCA) between A and B . NCA is defined as the set of nodes that are daughters of c(A) ∩ c(B) and that are not ancestors in c(A) ∩ c(B). The activation measure d_ is equal to the mean of the weight of each NCA calculated from A and B : n d (A, B) = 1 ∑(d(A,NCAi)+d(B,NCAi)) n i =1 Please, refer to (Dutoit and Poibeau, 2002) for more details and examples. However, this measure is sensitive enough to give valuable results for a wide variety of applications, including text filtering and information extraction (Poibeau et al., 2002). 5 The acquisition process The process begins as the end-user provides a predicative linguistic structure to the system along with a representative corpus. The system tries to discover relevant parts of text in the corpus based on the presence of plain words closely related to the ones of the seed pattern. A syntactic analysis of the sentence is then done to verify that these plain words correspond to a paraphrastic structure. The method is close to the one of Morin and Jacquemin (1999), who first try to locate couples of relevant terms and then apply relevant patterns to analyse the nature o"
C10-1119,C08-1002,0,0.0282097,"Missing"
C10-1119,P98-1013,0,0.0945152,"Missing"
C10-1119,W02-1016,0,0.456187,"Missing"
C10-1119,P06-2012,0,0.0166757,"rona (2004). The method is introduced in the following section. The approach involves (i) taking the GRs (SUBJ , OBJ , IOBJ ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes. The empirically determined N 200 was used. The method produced 40 SP clusters. 5 Clustering Methods Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009) and other similar NLP tasks involving high dimensional feature space (Chen et al., 2006). Following Sun and Korhonen (2009) we used the MNCut spectral clustering (Meila and Shi, 2001) which has a wide applicability and a clear probabilistic interpretation (von Luxburg, 2007; Verma and Meila, 2005). However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (Zelnik-Manor and Perona, 2004). Clustering groups a given set of verbs V = {vn }N n=1 into a disjoint partition of K classes. SPEC takes a similarity matrix as input. All our features can be viewed as probabilistic distributions because the combination of different"
C10-1119,P04-2007,0,0.0752821,"a big role in overall accuracy, and should therefore be investigated further (Sun and Korhonen, 2009). The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing. Future research should investigate the source of error at different stages of processing. In addition, it would be interesting to investigate whether language-specific tuning (e.g. using language specific features such as auxiliary classes) can further improve performance on French. Earlier works most closely related to ours are those of Merlo et al. (2002) and Ferrer (2004). Our results contrast with those of Ferrer who showed that a clustering approach does not transfer well from English to Spanish. However, she used basic SCF and named entity features only, and a clustering algorithm less suitable for high dimensional data. Like us, Merlo et al. (2002) created a gold standard by translating Levin classes to another language (Italian). They also applied a method developed for English to Italian, and reported good overall performance using features developed for English. Although the experiment was small (focussing on three classes and a few features only) and i"
C10-1119,C94-1042,0,0.0680517,"Missing"
C10-1119,N06-2015,0,0.0638806,"Missing"
C10-1119,P08-1050,0,0.288681,"ncluding e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Abend et al., 2008). However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis ´ S´eaghdha et al., 2008; Li and Brew, 2008; O and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009). However, most work on Levin type classification has focussed on English. Large-scale research on other languages such as German (Schulte im Walde, 2006) and Japanese (Suzuki and Fukumoto, 2009) has focussed on semantic classification. Although the two classification systems have shared properties, studies comparing the overlap between VerbNet and WordNet (Miller, 1995) have reported that the mapping is only partial and many to many due to fine-grained nature of classes based on synonymy (Shi and Mihalcea, 2005; Abend et al"
C10-1119,P02-1027,0,0.67511,"language more feasible. We take a recent verb clustering approach developed for English (Sun and Korhonen, 2009) and apply it to French – a major language for which no such experiment has been conducted yet. Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic. Our investigation reveals similarities between the English and French classifications, supporting the linguistic hypothesis (Jackendoff, 1990) and the earlier result of Merlo et al. (2002) that Levin classes have a strong cross-linguistic basis. Not only the general methodology but also best performing features are transferable between the languages, making it possible to learn useful classes for French automatically without language-specific tuning. 2 French Gold Standard The development of an automatic verb classification approach requires at least an initial gold standard. Some syntactic (Gross, 1975) and semantic (Vossen, 1998) verb classifications exist for French, along with ones which integrate aspects of both (Saint-Dizier, 1998). Since none of these resources offer cla"
C10-1119,messiant-etal-2008-lexschem,1,0.904217,"passer Table 1: A Levin style gold standard for French • clustered the features using a method which has proved promising in both English and German experiments: spectral clustering, • evaluated the clusters both quantitatively (using the gold standard) and qualitatively, • and compared the performance to that recently obtained for English in order to gain a better understanding of the cross-linguistic and language-specific properties of verb classification This work is described in the subsequent sections. 3.1 Data: the LexSchem Lexicon We extracted the features for clustering from LexSchem (Messiant et al., 2008). This large subcategorization lexicon provides SCF frequency information for 3,297 French verbs. It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI – a recent subcategorization acquisition system for French (Messiant, 2008). Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im Walde, 2006; Li and Brew, 2008; Sun and Korhonen, 2009). Like these other systems, ASSCI takes raw corpus data as input. The data is first tagged and lemmatized using the Tree-Tagger and then parsed using Syntex (Bourig"
C10-1119,C08-1082,0,0.0846082,"Missing"
C10-1119,J05-1004,0,0.135398,"Missing"
C10-1119,D09-1067,1,0.119925,"sambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Abend et al., 2008). However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis ´ S´eaghdha et al., 2008; Li and Brew, 2008; O and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009). However, most work on Levin type classification has focussed on English. Large-scale research on other languages such as German (Schulte im Walde, 2006) and Japanese (Suzuki and Fukumoto, 2009) has focussed on semantic classification. Although the two classification systems have shared properties, studies comparing the overlap between VerbNet and WordNet (Miller, 1995) have reported that the mapping is only partial and many to many due to fine-grained nature of classes based on synonymy (Shi and Mihalcea, 2005; Abend et al., 2008). Only few studies have been conducted on Levin style classifi"
C10-1119,W09-3205,0,0.218445,"Missing"
C10-1119,W09-0210,1,0.901693,"parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Abend et al., 2008). However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis ´ S´eaghdha et al., 2008; Li and Brew, 2008; O and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009). However, most work on Levin type classification has focussed on English. Large-scale research on other languages such as German (Schulte im Walde, 2006) and Japanese (Suzuki and Fukumoto, 2009) has focussed on semantic classification. Although the two classification systems have shared properties, studies comparing the overlap between VerbNet and WordNet (Miller, 1995) have reported that the mapping is only partial and many to many due to fine-grained nature of classes based on synonymy (Shi and Mihalcea, 2005; Abend et al., 2008). Only few studies have been conducte"
C10-1119,W04-3213,0,\N,Missing
C10-1119,J06-2001,0,\N,Missing
C10-1119,P08-3010,1,\N,Missing
C10-1119,C98-1013,0,\N,Missing
C12-1165,P09-1004,0,0.0254928,"roaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, but not full SCFs. Our method makes use of non-negative tensor factorization (NTF) (Shashua and Hazan, 2005). Tensor factorization is the multilinear generalization of matrix factorization. It has been extensively studied in the field of statistics (Kolda and Bader, 2009), and has yielded promising results on SP acquisition (Van de Cruys, 2009). We introduce a novel way of considering SCFs with an arbitrary number of arguments, and SPs as multi-way co-occurrences in the context of these larger SCFs. T"
C12-1165,W10-1612,0,0.307879,"Missing"
C12-1165,P05-1038,0,0.0793575,"COLING 2012: Technical Papers, pages 2703–2720, COLING 2012, Mumbai, December 2012. 2703 1 Introduction Verb subcategorization lexicons and selectional preference models capture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and gen"
C12-1165,D07-1017,0,0.0330541,"uments into account. Lippincott et al. (2012) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a verb, as full scale SCF systems do. Recent SP acquisition approaches use latent semantic information to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-"
C12-1165,W05-0621,0,0.458257,"2012. 2703 1 Introduction Verb subcategorization lexicons and selectional preference models capture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a num"
C12-1165,A97-1052,0,0.365805,"which we investigate the model’s ability to induce preferences for the co-occurrence of a particular verb lemma and all of its arguments at the same time. The model achieves a high accuracy of 77.8 on this new evaluation. We also perform a qualitative evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tu"
C12-1165,briscoe-carroll-2002-robust,0,0.0250342,"ext of these larger SCFs. The resulting model provides an ideal framework for joint acquisition of SCF and SP information. The only form of supervision in the model is parameter estimation and choice of the best feature set via cross-validation. 3 Subcategorization Frame Inventory To facilitate thorough qualitative evaluation (Section 5.6), we defined our SCFs in terms of syntactic slots, and in the form of common GRs. Finer-grained inventories including lexicalized elements and semantic interpretation were left for future work (see Section 7). We use the GR types produced by the RASP parser (Briscoe and Carroll, 2002). Altogether we experimented with combinations of nine GR types out of the 131 which can be headed by verbs, selected on the basis of their frequency in the parsed BNC corpus and relevance for subcategorization. For this initial experiment, we focused on higher-frequency arguments since they will have the greatest impact on downstream applications. Our first eight basic GR types are as follows. In subject position we included non-clausal subjects (SUBJ)2 , ignoring sentences with clausal subjects, which are much less frequent. Since objects are key arguments for subcategorization, we included"
C12-1165,chesley-salmon-alt-2006-automatic,0,0.291885,"evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accura"
C12-1165,D10-1088,0,0.190764,"Missing"
C12-1165,D10-1113,0,0.0175113,"have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to incorporate information beyond the GR co-occurrences and heuristics that identify candidate SCFs on a per-sentence basis. Such cues provide no capacity for learning further from the data, e.g. from the lexical content of verbal arguments or from other GRs which are not part of the SCF. Unsupervised machine learning has been applied to tasks where portability is equally important (Blei et al., 2003; Dinu and Lapata, 2010) but its application to SCF acquisition remains limited. Carroll and Rooth (1996) combined a head-lexicalized context-free grammar with an expectation-maximization (EM) algorithm to acquire an SCF lexicon. D˛ ebowski (2009) used a filtering method based on the point-wise co-occurrence of arguments in parsed data to acquire a Polish SCF lexicon, but this method does not take the semantics of the verb’s arguments into account. Lippincott et al. (2012) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a"
C12-1165,P07-1028,0,0.0469768,"12) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a verb, as full scale SCF systems do. Recent SP acquisition approaches use latent semantic information to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, b"
C12-1165,J02-3001,0,0.0962391,"mantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CC"
C12-1165,han-etal-2000-handling,0,0.0723478,"gument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, dire"
C12-1165,ienco-etal-2008-automatic,0,0.284255,"e joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by thei"
C12-1165,kawahara-kurohashi-2010-acquiring,0,0.239393,"ects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to incorporate information beyond the GR co-occurrences and heuristics that identify candidate SC"
C12-1165,W02-0907,1,0.698114,"el’s ability to induce preferences for the co-occurrence of a particular verb lemma and all of its arguments at the same time. The model achieves a high accuracy of 77.8 on this new evaluation. We also perform a qualitative evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds ar"
C12-1165,korhonen-etal-2006-large,1,0.926297,"the outer products of N (in this case three) vectors. Figure 2: Graphical representation of the NTF as the sum of outer products. Computationally, the NTF model is fitted by applying an alternating least-squares algorithm. In each iteration, two of the modes are fixed and the third one is fitted in a least squares sense. This process is repeated until convergence.3 4.2 Construction of verb-argument tensors In order to discover SCFs and SPs, we construct a tensor that contains the multi-way cooccurrences of a verb and its different arguments. 4.2.1 Corpus data We used a subset of the corpus of Korhonen et al. (2006), which consists of up to 10,000 sentences for each of approximately 6400 verbs, with data taken from five large British and American cross-domain corpora. To ensure sufficient data for each verb, we included verbs with at least 500 occurrences, yielding a total of 1993 verbs. The corpus data was tokenized, POS -tagged, lemmatized, and parsed with the RASP system (Briscoe and Carroll, 2002). RASP uses a tag-sequence grammar, and is unlexicalized, so that the parser’s lexicon does not interfere with SCF acquisition. RASP produces output in the form of GRs. Passive sentences and those with claus"
C12-1165,lenci-etal-2008-unsupervised,0,0.308432,"ich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to incorporate information"
C12-1165,P12-1044,1,0.661234,"Rs which are not part of the SCF. Unsupervised machine learning has been applied to tasks where portability is equally important (Blei et al., 2003; Dinu and Lapata, 2010) but its application to SCF acquisition remains limited. Carroll and Rooth (1996) combined a head-lexicalized context-free grammar with an expectation-maximization (EM) algorithm to acquire an SCF lexicon. D˛ ebowski (2009) used a filtering method based on the point-wise co-occurrence of arguments in parsed data to acquire a Polish SCF lexicon, but this method does not take the semantics of the verb’s arguments into account. Lippincott et al. (2012) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a verb, as full scale SCF systems do. Recent SP acquisition approaches use latent semantic information to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk"
C12-1165,P08-3010,0,0.0181622,"ble of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to in"
C12-1165,W05-1002,0,0.44833,"ion Verb subcategorization lexicons and selectional preference models capture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their sy"
C12-1165,J05-3003,0,0.569078,"Missing"
C12-1165,P10-1045,0,0.135579,"Missing"
C12-1165,P07-1115,1,0.94609,"nduce preferences for the co-occurrence of a particular verb lemma and all of its arguments at the same time. The model achieves a high accuracy of 77.8 on this new evaluation. We also perform a qualitative evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frame"
C12-1165,D11-1130,0,0.285999,"tested. As the two types of lexical information – SCFs and SPs – are closely interlinked and can complement each other, it would make sense to acquire them jointly. However, to the best of our knowledge, no previous work has developed a model for their joint acquisition. Unsupervised machine learning is attractive for lexical acquisition because it works where little labeled data is available, and ports easily between tasks and languages. Increasingly sophisticated techniques have been applied to SP induction (Rooth et al., 1999; Van de Cruys, 2009; Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011) while work 2704 on unsupervised SCF acquisition has been limited (Carroll and Rooth, 1996). In this paper we present a largely unsupervised method for the joint acquisition of SCFs and SPs, adapting a method that has been successfully used for SP induction (Van de Cruys, 2009) so that it learns whether a verb subcategorizes for a particular argument slot together with which lexical items occur in the slot. Our method uses a co-occurrence model augmented with a factorization algorithm to cluster verbs from a large corpus. Specifically, we use non-negative tensor factorization (NTF) (Shashua an"
C12-1165,W97-0209,0,0.142124,"pport NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set of SCFs for a verb can be viewed as a multi-"
C12-1165,P99-1014,0,0.495283,"he full range of verbal arguments, including e.g. clausal complements, has not been tested. As the two types of lexical information – SCFs and SPs – are closely interlinked and can complement each other, it would make sense to acquire them jointly. However, to the best of our knowledge, no previous work has developed a model for their joint acquisition. Unsupervised machine learning is attractive for lexical acquisition because it works where little labeled data is available, and ports easily between tasks and languages. Increasingly sophisticated techniques have been applied to SP induction (Rooth et al., 1999; Van de Cruys, 2009; Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011) while work 2704 on unsupervised SCF acquisition has been limited (Carroll and Rooth, 1996). In this paper we present a largely unsupervised method for the joint acquisition of SCFs and SPs, adapting a method that has been successfully used for SP induction (Van de Cruys, 2009) so that it learns whether a verb subcategorizes for a particular argument slot together with which lexical items occur in the slot. Our method uses a co-occurrence model augmented with a factorization algorithm to cluster verbs"
C12-1165,scheible-2010-evaluation,0,0.017901,"ormation to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, but not full SCFs. Our method makes use of non-negative tensor factorization (NTF) (Shashua and Hazan, 2005). Tensor factorization is the multilinear generalization of matrix factorization. It has been extensively studied in the field of statistics"
C12-1165,J06-2001,0,0.468846,"Missing"
C12-1165,P08-1057,0,0.0542037,"location (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, but not full SCFs. Our method makes use of non-negative tensor factorization (NTF) (Shashua and Hazan, 2005). Tensor factorization is the multilinear generalization of matrix factorization. It has been extensively studied in the field of statistics (Kolda and Bader, 2009), and has yielded promising results on SP acquisition (Van de Cruy"
C12-1165,D11-1097,1,0.853085,"on about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set of SCFs for a verb can be viewed as a multi-way co-occurrence problem of a verb and its differe"
C12-1165,D11-1095,1,0.711543,"apture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show take"
C12-1165,P10-1097,0,0.0405782,"s requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set of SCFs for a verb can be viewed as a multi-way co-occurrence pro"
C12-1165,W09-0211,1,0.880443,"Missing"
C12-1165,C00-2137,0,0.0131193,"or head features, extended PPs, and split clausal types (row 9), without losing out on F-score. This suggests that lexical-semantic features are valuable for SCF acquisition. Another trend is towards more accurate models with fewer additional features; individual features and pairs of features seem to provide the most improvement (rows 1-7) over the base model (row 14), but the model with all additional features (row 16) has markedly worse performance, which may indicate a data sparsity problem. We carried out significance tests for the mentioned model differences using stratified shuffling (Yeh, 2000). These tests indicate that most of the models (rows 1-11) have significantly higher F-score than the baseline, and most show significant pairwise differences in precision and recall. Parameter tuning with cross-validation resulted in a θvoid of 0.4 (though exploration of the models in Table 4 showed that some models performed better with even lower values). This means that the model only needs to assign a relatively low confidence score to the void feature to infer that a slot is not part of an SCF. This is probably because adjuncts and other noise in the data means that these slots are fille"
C12-1165,P09-2019,0,0.06059,"ve for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set"
C12-1165,P11-1156,0,0.0464203,"nd selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [yo"
C12-1165,W98-1505,0,\N,Missing
C12-1165,P10-1044,0,\N,Missing
C16-2062,P10-1024,0,0.0145142,"umenthood of a complement, or build the hierarchical structure of the lexical entries, is partially independent. We assume a list of verbal structures that have been automatically extracted from a large representative corpus. A verbal structure is an occurrence of a verb and its complements (expressed as syntactic dependencies); a complement is an ordered pair of a lexical head and a case marker. Computing the argumenthood of complements Following up on previous studies on the distinction between arguments and adjuncts (Manning, 2003; Merlo and Esteve Ferrer, 2006; Fabre and Bourigault, 2008; Abend and Rappoport, 2010), we propose a new measure of the degree of argumenthood of complements, derived from the famous TF-IDF weighting scheme used in information retrieval: argumenthood(v, c) = (1 + log count(v, c)) log |V | |{v 0 ∈ V : ∃(v 0 , c)}| (1) where c is a complement (i.e. an ordered pair of a lexical head and a case particle); v is a verb; count(v, c) is the number of cooccurrences of the complement c with the verb v; |V |is the total number of unique verbs; |{v 0 ∈ V : ∃(v 0 , c)} |is the number of unique verbs cooccurring with this complement. That is, we are dealing with complements instead of terms,"
C16-2062,D09-1046,0,0.0143991,"s as these continuous models. There are arguments to support this view. For example, it has been demonstrated that semantic categories have fuzzy boundaries and thus the number of word meanings per lexical item is to a large extent arbitrary (Tuggy, 1993). Although this still fuels lots of discussions among linguists and lexicographers, we think that a description can be more or less fine-grained while maintaining accuracy and validity. Moreover, it has been demonstrated that lexical entries in traditional dictionaries overlap and different word meanings can be associated with a sole example (Erk and McCarthy, 2009), showing that meaning cannot be sliced into separate and exclusive word senses. The same problem also arises when it comes to differentiating between arguments and adjuncts. As said by Manning (2003): “There are some very clear arguments (normally, subjects and objects), and some very clear adjuncts (of time and ‘outer’ location), but also a lot of stuff in the middle”. A proper representation thus need to be based on some kind of continuity and should take into consideration not only the subject and the object, but also the prepositional phrases as well as the wider context. Some application"
C16-2062,J06-3002,0,0.0312419,"Missing"
D11-1025,P07-2009,0,0.0712628,"|x, θ) = Z(x) exp( j θj Fj (y, x)), where Fj (y, x) is a real-valued feature function of the states and the observations; θj is the weight of Fj , and Z(x) is a normalization factor. The θ parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF experiments. These features were extracted from the corpus using a number of tools. A tokenizer was used to detect the boundaries of sentences and to separate punctuation from adjacent words e.g. in complex biomedical terms such as 2-amino-3,8-diethylimidazo[4,5f]quinoxaline. The C&C tools (Curran et al., 2007) trained on biomedical literature were employed for POS tagging, lemmatization and parsing. The lemma output was used for creating Word, Bi-gram and Verb features. The GR output was used for creating the GR, Subj, Obj and Voice features. The ”obj” marker in a subject relation indicates passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words 3.2"
D11-1025,W10-1913,1,0.502157,"lations in the corpus. e.g. (ncsubj observed 14 difference 5 obj). The value of this feature equals 1 if it occurs in a particular sentence (and 0 if not). • Subj and Obj. The subjects and objects appearing with any verbs in the corpus (extracted from above GRs). • Voice. The voice of verbs (active or passive) in the corpus. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and without self-training, transductive SVM and semi-supervised CRF. 3.2.1 Supervised methods SVM constructs hyperplanes in a multidimensional space to separate data points of different classes. Good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance between two parall"
D11-1025,I08-1050,0,0.185889,"s obtained, or the conclusions drawn by authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive"
D11-1025,P06-1027,0,0.231964,"u) ) Subject to y (l) (w · x(l) − b) ≥ 1, y (u) (w · x(u) − b) ≥ 1 , y (u) ∈ {−1, 1}, where x(u) is unlabeled data and y (u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data P (l) (l) log additional term PL P p(y |x(u) , θ) with an (u) , θ) log p(y|x , θ) to minimize U Y p(y|x the conditional entropy of the model’s predictions on U nlabeled data (Jiao et al., 2006; Mann and Mccal277 lum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classif ied sentences total no. of sentences in the corpus p= no. of sentences correctly identif ied as Classi total no. of sentences identif ied as Classi r= no. of sentences correctly identif ied as Classi total no. of sentences in Classi f= 2∗p∗r p+r We used 10-fold cross validation for all the metho"
D11-1025,liakata-etal-2010-corpora,0,0.499936,"ave been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML. Relying on a small amount of labeled data and a large"
D11-1025,W06-3309,0,0.0568061,"study, the results obtained, or the conclusions drawn by authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing a"
D11-1025,N07-2028,0,0.0309862,"Missing"
D11-1025,W09-3603,0,0.0989553,"Missing"
D11-1025,W10-0104,0,0.0620553,"Missing"
D11-1025,D09-1067,1,0.212661,"m adjacent words e.g. in complex biomedical terms such as 2-amino-3,8-diethylimidazo[4,5f]quinoxaline. The C&C tools (Curran et al., 2007) trained on biomedical literature were employed for POS tagging, lemmatization and parsing. The lemma output was used for creating Word, Bi-gram and Verb features. The GR output was used for creating the GR, Subj, Obj and Voice features. The ”obj” marker in a subject relation indicates passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words 3.2.2 Weakly-supervised methods and GRs with fewer than 2 occurrences and bi-grams Active SVM (ASVM) starts with a small amount of with fewer than 5 occurrences. labeled data, and iteratively chooses a proportion of 276 unlabeled data for which SVM has less confidence to be labeled (the labels can be restored from the original corpus) and used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller,"
D11-1025,J02-4002,0,0.317771,"guage Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential s"
D11-1025,D09-1155,0,0.518778,"raction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop te"
D11-1025,L10-1000,0,\N,Missing
D11-1094,W06-1669,0,0.0172616,"Missing"
D11-1094,D10-1113,0,0.737717,"nventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in which words, together with their window-based context words and their dependency 1012 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012–1022, c Edinburgh, Scotland, U"
D11-1094,D09-1046,0,0.0573743,"unity’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in whic"
D11-1094,D08-1094,0,0.580303,"Missing"
D11-1094,W09-0208,0,0.459823,"Missing"
D11-1094,P10-2017,0,0.264572,"Missing"
D11-1094,P09-1002,0,0.0201734,"ay for equal work! The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a p"
D11-1094,W10-2803,0,0.0181118,"The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined s"
D11-1094,S07-1029,0,0.0630491,"ch better results than Dinu and Lapata’s approach. The results indicate that our approach, after vector adaptation in context, is still able to provide accurate similarity calculations across the complete word space. While other algorithms are able to rank candidate substitutes at the expense of accurate similarity calculations, our approach is able to do both. This is one of the important advantages of our approach. For reasons of comparison, we also included the scores of the best performing models that participated in the SEMEVAL 2007 lexical substitution task (KU (Yuret, 2007) and IRST 2 (Giuliano et al., 2007), which got the best scores for Rbest and P10 , respectively). These models reach better scores compared to our models. Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e. WordNet, or a machine readable thesaurus). Our system, on the other hand, induces paraphrases in a fully unsupervised way. To our knowledge, this is the first time a fully unsupervised system is tested on the paraphrase induction task. model n v a r vectordep 31.66 23.53 29.91 38.43 NMF context 33.73?? 31.40 33.37? 25.21? 25.97?? 25.99?? 28.58 20.56"
D11-1094,N06-2015,0,0.031274,"carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be"
D11-1094,P98-2127,0,0.0919097,"portant dimensions that come out of the SVD are said to represent latent semantic dimensions, according to which nouns and documents can be represented more efficiently. Our model also applies a factorization technique (albeit a different one) in order to find a reduced semantic space. The nature of a word’s context is a determining factor in the kind of the semantic similarity that is induced. A broad context window (e.g. a paragraph or document) yields broad, topical similarity, whereas a small context window yields tight, synonym-like similarity. This has lead a number of researchers (e.g. Lin (1998)) to use the dependency relations that a particular word takes part in as context features. An overview of dependency-based semantic space models is given in Pad´o and Lapata (2007). A number of researchers have exploited the no1013 tion of context to differentiate between the different senses of a word in an unsupervised way (a task labeled word sense induction or WSI). Sch¨utze (1998) proposed a context-clustering approach, in which context vectors are created for the different instances of a particular word, and those contexts are grouped into a number of clusters, representing the differen"
D11-1094,S07-1009,0,0.372725,"d context words. 2 In this case, the sets of context features contain only one item, so the average probability distribution of the sets is just the latent probability distribution of their respective item. 1016 Evaluation In this section, we present a thorough evaluation of the method described above, and compare it with related methods for meaning computation in context. In order to test the applicability of the method to multiple languages, we present evaluation results for both English and French. 4.1 Datasets For English, we make use of the SEMEVAL 2007 English Lexical Substitution task (McCarthy and Navigli, 2007; McCarthy and Navigli, 2009). The task’s goal is to find suitable substitutes for a target word in a particular context. The complete data set contains 200 target words (about 50 for each part of speech, viz. nouns, verbs, adjectives, and adverbs). Each target word occurs in 10 different sentences, which yields a total of 2000 sentences. Five annotators provided suitable substitutes for each target word in the different contexts. For French, we developed a small-scale lexical substitution task ourselves, closely following the guidelines of the original English task. We manually selected 10 am"
D11-1094,nivre-etal-2006-maltparser,0,0.0298749,"the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3 The task is provided as supplementary material to this paper; it is also available from the first author’s website. 4.2 Implementational details 4.3 The model for English has been trained on part of the UKW a C corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4 , so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (± 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech (nouns, verbs, adjectives and adverbs), which yields four models in tota"
D11-1094,J07-2002,0,0.290981,"Missing"
D11-1094,J98-1004,0,0.428432,"Missing"
D11-1094,W09-2506,0,0.186895,"Missing"
D11-1094,P10-1097,0,0.433017,"Missing"
D11-1094,W00-1308,0,0.317109,"Missing"
D11-1094,N03-1033,0,0.0105463,"for each noun we selected 10 different sentences from the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3 The task is provided as supplementary material to this paper; it is also available from the first author’s website. 4.2 Implementational details 4.3 The model for English has been trained on part of the UKW a C corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4 , so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (± 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech (nouns, verbs, adjectiv"
D11-1094,C08-1117,1,0.916057,"Missing"
D11-1094,C00-2137,0,0.0278975,"and the latter one paraphrase induction. In the next paragraphs, we will describe the actual evaluation measures that have been used for both approaches. Paraphrase ranking Following Dinu and Lapata (2010), we compare the ranking produced by our model with the gold standard ranking using Kendall’s τb (which is adjusted for ties). For reasons of comparison, we also compute general average precision (GAP, Kishida (2005)), which was used by Erk and Pad´o (2010) and Thater et al. (2010) to evaluate their rankings. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared to an arbitrary ranking. The second baseline is a dependency-based vector space model that does not take the context of the particular instance into account (and thus returns the same ranking for each instance of the target word). This is a fairly competitive baseline, as noted by other researchers (Erk and Pad´o, 2008; Thater et al., 2009; Dinu and Lapata, 2010). Paraphrase induction To evaluate the"
D11-1094,S07-1044,0,0.0207272,"dels are able to reach much better results than Dinu and Lapata’s approach. The results indicate that our approach, after vector adaptation in context, is still able to provide accurate similarity calculations across the complete word space. While other algorithms are able to rank candidate substitutes at the expense of accurate similarity calculations, our approach is able to do both. This is one of the important advantages of our approach. For reasons of comparison, we also included the scores of the best performing models that participated in the SEMEVAL 2007 lexical substitution task (KU (Yuret, 2007) and IRST 2 (Giuliano et al., 2007), which got the best scores for Rbest and P10 , respectively). These models reach better scores compared to our models. Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e. WordNet, or a machine readable thesaurus). Our system, on the other hand, induces paraphrases in a fully unsupervised way. To our knowledge, this is the first time a fully unsupervised system is tested on the paraphrase induction task. model n v a r vectordep 31.66 23.53 29.91 38.43 NMF context 33.73?? 31.40 33.37?"
D11-1094,P08-1028,0,\N,Missing
D11-1094,C98-2122,0,\N,Missing
E03-1015,P00-1011,0,0.0123258,"tagged given an appropriate context (especially if a trigger word disambiguates the sequence). • Learning capabilities. We include, in this section, ML algorithms used to tag unknown named entities. Most ML techniques have been If one analyzes a text to tag person names, it is then easy to write a simple program that will automatically extract the sequences previously tagged to generate a dictionary. In this sense, tagging is not that different from elaborating a dictionary! 2 used including maximal entropy, inductive logic programming, decision tree learning, hidden Markov models and others (Bechet et al., 2000) (Bikel et al., 1997) (Collins and Singer, 1999) (Mikheev et al., 1999). We use a kind of theory learning to extend the set of expressions identified by the rule-based system: the lexicon and the grammar is exploited as a domain theory to dynamically find new entities (Mooney, 1993). • Revision capabilities. We implemented revision capabilities in the system so that it can revise tags in a certain context. For example, in an English text, isolated occurrences of Washington can be considered as location names. If one finds a context that potentially suggests another category for the named entit"
E03-1015,A97-1029,0,0.051663,"Missing"
E03-1015,W99-0613,0,0.118818,"ties. Nearly all classical MUC systems were using this approach until the mid&apos; 90s, and most of them are still using this kind of technique (MUC-6, 1995). • Fully automatic learning-based systems. These systems are using Machine Learning (ML) techniques to learn a model in order to accurately tag the texts. The result of the learning task can be a set of rules, a decision tree or a set of numeric data. Note that a human cannot always revise the result if the learning algorithm used does not provide a readable output. These systems are now very popular in the IE community (Bikel et al., 1997) (Collins and Singer, 1999), even if they were initially rather dedicated to audio corpora. 155 • Mixed approach. In this kind of systems, a set of rules is automatically learned and revised by an expert. An alternative can be the dynamic extension of an existing set of core rules previously defined by the expert, so that the system obtains a better coverage of the data. Cucchiarelli and Velardi (2000), among others, have applied this approach to NERC systems. Text Rule-based Systems (1) Lexical analsis (2) Grammar application 1/ Annotated Text 3.2 Overall system architecture In spite of differences in their implementat"
E03-1015,E99-1001,0,0.0160125,"mbiguates the sequence). • Learning capabilities. We include, in this section, ML algorithms used to tag unknown named entities. Most ML techniques have been If one analyzes a text to tag person names, it is then easy to write a simple program that will automatically extract the sequences previously tagged to generate a dictionary. In this sense, tagging is not that different from elaborating a dictionary! 2 used including maximal entropy, inductive logic programming, decision tree learning, hidden Markov models and others (Bechet et al., 2000) (Bikel et al., 1997) (Collins and Singer, 1999) (Mikheev et al., 1999). We use a kind of theory learning to extend the set of expressions identified by the rule-based system: the lexicon and the grammar is exploited as a domain theory to dynamically find new entities (Mooney, 1993). • Revision capabilities. We implemented revision capabilities in the system so that it can revise tags in a certain context. For example, in an English text, isolated occurrences of Washington can be considered as location names. If one finds a context that potentially suggests another category for the named entity (for example, Mrs. Washington) the system will revise the initial tag"
E03-1015,C00-2167,0,0.0156783,"Jamalpour, A. Krul, A. Marcus, F. Picoli and C. Plancq. projecti, so that it also has pedagogic purposes. But, even so, the project seems to be sufficiently attractive to interest industrial partners. We describe the different approaches for named entity recognition. We then present the project and the different analysis techniques used. We will conclude with some considerations on evaluation and future work. 2 State of the art NERC systems In this section, we examine the different approaches to named entity recognition. We then examine previous experiments to compare systems and techniques. Sekine and Eriguchi (2000) present an interesting classification of named entity recognition systems. • Manually created rule-based systems. In this kind of system, developers initially elaborate a set of patterns that will be applied on the text to accurately recognize and tag named entities. Nearly all classical MUC systems were using this approach until the mid&apos; 90s, and most of them are still using this kind of technique (MUC-6, 1995). • Fully automatic learning-based systems. These systems are using Machine Learning (ML) techniques to learn a model in order to accurately tag the texts. The result of the learning t"
E03-1015,P95-1026,0,0.0527999,"Missing"
E03-1082,P00-1011,0,0.0125679,"tagged given an appropriate context (especially if a trigger word disambiguates the sequence). • Learning capabilities. We include, in this section, ML algorithms used to tag unknown named entities. Most ML techniques have been If one analyzes a text to tag person names, it is then easy to write a simple program that will automatically extract the sequences previously tagged to generate a dictionary. In this sense, tagging is not that different from elaborating a dictionary! 2 used including maximal entropy, inductive logic programming, decision tree learning, hidden Markov models and others (Bechet et al., 2000) (Bikel et al., 1997) (Collins and Singer, 1999) (Mikheev et al., 1999). We use a kind of theory learning to extend the set of expressions identified by the rule-based system: the lexicon and the grammar is exploited as a domain theory to dynamically find new entities (Mooney, 1993). • Revision capabilities. We implemented revision capabilities in the system so that it can revise tags in a certain context. For example, in an English text, isolated occurrences of Washington can be considered as location names. If one finds a context that potentially suggests another category for the named entit"
E03-1082,A97-1029,0,0.0327024,"Missing"
E03-1082,W99-0613,0,0.11194,"ties. Nearly all classical MUC systems were using this approach until the mid' 90s, and most of them are still using this kind of technique (MUC-6, 1995). • Fully automatic learning-based systems. These systems are using Machine Learning (ML) techniques to learn a model in order to accurately tag the texts. The result of the learning task can be a set of rules, a decision tree or a set of numeric data. Note that a human cannot always revise the result if the learning algorithm used does not provide a readable output. These systems are now very popular in the IE community (Bikel et al., 1997) (Collins and Singer, 1999), even if they were initially rather dedicated to audio corpora. 155 • Mixed approach. In this kind of systems, a set of rules is automatically learned and revised by an expert. An alternative can be the dynamic extension of an existing set of core rules previously defined by the expert, so that the system obtains a better coverage of the data. Cucchiarelli and Velardi (2000), among others, have applied this approach to NERC systems. Text Rule-based Systems (1) Lexical analsis (2) Grammar application 1/ Annotated Text 3.2 Overall system architecture In spite of differences in their implementat"
E03-1082,E99-1001,0,0.0151072,"mbiguates the sequence). • Learning capabilities. We include, in this section, ML algorithms used to tag unknown named entities. Most ML techniques have been If one analyzes a text to tag person names, it is then easy to write a simple program that will automatically extract the sequences previously tagged to generate a dictionary. In this sense, tagging is not that different from elaborating a dictionary! 2 used including maximal entropy, inductive logic programming, decision tree learning, hidden Markov models and others (Bechet et al., 2000) (Bikel et al., 1997) (Collins and Singer, 1999) (Mikheev et al., 1999). We use a kind of theory learning to extend the set of expressions identified by the rule-based system: the lexicon and the grammar is exploited as a domain theory to dynamically find new entities (Mooney, 1993). • Revision capabilities. We implemented revision capabilities in the system so that it can revise tags in a certain context. For example, in an English text, isolated occurrences of Washington can be considered as location names. If one finds a context that potentially suggests another category for the named entity (for example, Mrs. Washington) the system will revise the initial tag"
E03-1082,C00-2167,0,0.0160055,"Jamalpour, A. Krul, A. Marcus, F. Picoli and C. Plancq. projecti, so that it also has pedagogic purposes. But, even so, the project seems to be sufficiently attractive to interest industrial partners. We describe the different approaches for named entity recognition. We then present the project and the different analysis techniques used. We will conclude with some considerations on evaluation and future work. 2 State of the art NERC systems In this section, we examine the different approaches to named entity recognition. We then examine previous experiments to compare systems and techniques. Sekine and Eriguchi (2000) present an interesting classification of named entity recognition systems. • Manually created rule-based systems. In this kind of system, developers initially elaborate a set of patterns that will be applied on the text to accurately recognize and tag named entities. Nearly all classical MUC systems were using this approach until the mid' 90s, and most of them are still using this kind of technique (MUC-6, 1995). • Fully automatic learning-based systems. These systems are using Machine Learning (ML) techniques to learn a model in order to accurately tag the texts. The result of the learning t"
E03-1082,P95-1026,0,0.0523747,"Missing"
E09-2002,radev-etal-2004-mead,0,0.129603,"ring several times in different documents can be qualified as important. Among recent approaches, the “centroid-based summarization” method developed by (Radev et al., 2004) consists in identifying the centroid of a cluster of documents, in other words the terms which best suit the documents to summarize. Then, the sentences to be extracted are the ones that contain the greatest number of centroids. Radev implemented this method in an online multi-document summarizer, MEAD. Radev further improved MEAD using a different method to extract sentences: “Graph-based centrality” extractor (Erkan and Radev, 2004). It consists in computing similarity between sentences, and then selecting sentences which are considered as “central” in a graph where nodes are sentences and edges are similarities. Sentence selection is then performed by picking the sentences which have been visited most after a random walk on the graph. The last two systems are dealing with redundancy as a post-processing step. (Zhu et al., 2007), assuming that redundancy should be the concept on what is based multi-document summarization, offered a method to deal with redundancy at the In this paper, we present a novel approach for autom"
E09-2002,N07-1013,0,0.0778684,"Missing"
E09-2002,N06-1059,0,\N,Missing
F14-2032,P07-2009,0,0.0427468,"Missing"
F14-2032,D11-1025,1,0.904337,"Missing"
F14-2032,N13-1113,1,0.868489,"Missing"
F14-2032,J02-1004,0,0.013694,"Missing"
F14-2032,W14-0610,1,0.789273,"Missing"
F14-2032,W06-1613,0,0.093832,"Missing"
J19-3005,P13-2037,0,0.0744461,"Missing"
J19-3005,W17-0401,0,0.0483167,"Missing"
J19-3005,P15-2044,0,0.0358947,"Missing"
J19-3005,W14-4203,0,0.0512933,"Missing"
J19-3005,P15-1040,0,0.0140754,"k is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al."
J19-3005,Q16-1031,0,0.0249656,"Missing"
J19-3005,D18-1549,0,0.0165078,"da et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and consequently their rate of infrequent words. Moreover, the ambiguity of mapping between form and meaning of morphemes determines the usefulness of injecting character-level information (Gerz et al. 2018a, 2018b). This variation has to be taken into account in both language transfer and multilingual joint learning. As a final note, we stress that the addition of new features does not concern just future work, but also the existing typology-savvy methods, which c"
J19-3005,D17-1011,0,0.193671,"ases. In this article, we provide an extensive survey of typologically informed NLP methods to date, including the more recent neural approaches not previously surveyed in this area. We consider the impact of typological (including both structural and semantic) information on system performance and discuss the optimal sources for such information. Traditionally, typological information has been obtained from hand-crafted databases and, therefore, it tends to be coarse-grained and incomplete. Recent research has focused on inferring typological information automatically from multilingual data (Asgari and Schütze 2017, inter alia), with the specific purpose of obtaining a more complete and finer-grained set of feature values. We survey these techniques and discuss ways to integrate their predictions into the current NLP algorithms. To the best of our knowledge, this has not yet been covered in the existing literature. In short, the key questions our paper addresses can be summarized as follows: (i) Which NLP tasks and applications can benefit from typology? (ii) What are the advantages and limitations of currently available typological databases? Can data-driven inference of typological features offer an a"
J19-3005,D08-1014,0,0.0572004,"Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be used to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016). Some of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agi´c, Hovy, and Søgaard 2015). Moreover, comparisons of stateof-the-art algorithms revealed that model"
J19-3005,W09-0106,0,0.0830635,"ction The world’s languages may share universal features at a deep, abstract level, but the structures found in real-world, surface-level texts can vary significantly. This crosslingual variation has challenged the development of robust, multilingually applicable Natural Language Processing (NLP) technology, and as a consequence, existing NLP is still largely limited to a handful of resource-rich languages. The architecture design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das"
J19-3005,P07-1036,0,0.126415,"Missing"
J19-3005,Q18-1039,0,0.0162199,"al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representa"
J19-3005,C16-1298,0,0.0281595,"Missing"
J19-3005,P11-1061,0,0.244408,"009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promising source of informati"
J19-3005,P07-1009,0,0.17624,"Missing"
J19-3005,P16-1038,0,0.174612,"16) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whether a language allows two sounds to differ only in voicing, such as /t/ and /d/. Finally, a small number of experiments adopted the entire feature inventory of typological databases, without any sort of pre-selection. In particular, Agi´c (2017) and Ammar et al. (2016) extracted all the features in WALS, whereas Deri and Knight (2016) extracted all the features in URIEL. Schone and Jurafsky (2001) did not resort to basic typological features, but rather to “several hundred [implicational universals] applicable to syntax” drawn from the Universal Archive (Plank and Filiminova 1996). Typological attributes that are extracted from typological databases are typically represented as feature vectors in which each dimension encodes a feature value. This feature representation is often binarized (Georgi, Xia, and Lewis 2010): For each possible value v of each database attribute a, a new feature is created with value 1 if it corres"
J19-3005,P15-2139,0,0.0321535,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D15-1040,0,0.041461,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D16-1136,0,0.0403964,"Missing"
J19-3005,D12-1001,0,0.018952,"Missing"
J19-3005,P17-2093,0,0.047937,"Missing"
J19-3005,N15-1184,0,0.0387115,"et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mrkši´c et al. 2016; Osborne, Narayan, and Cohen 2016) have proposed means through which external knowledge sourced from linguistic resources (such as WordNet, BabelNet, or lists of morphemes) can be encoded in word embeddings. Among the state-of-the-art specialization methods ATTRACT- REPEL (Mrkši´c et al. 2017; Vuli´c et al. 2017) pushes together or pulls apart 589 Computational Linguistics Volume 45, Number 3 vector pairs according to relational constraints, while preserving the relationship between words in the original space and possibly propagating the specializ"
J19-3005,C10-1044,0,0.0863316,"Missing"
J19-3005,Q18-1032,1,0.878475,"Missing"
J19-3005,D18-1029,1,0.881238,"Missing"
J19-3005,N15-1157,0,0.0210384,"Missing"
J19-3005,C16-1002,0,0.0416532,"ching scenarios (Adel, Vu, and Schultz 2013). In fact, multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Univ"
J19-3005,P15-1119,0,0.0603047,"Missing"
J19-3005,P16-1228,0,0.0498272,"Missing"
J19-3005,D16-1173,0,0.0523125,"Missing"
J19-3005,P11-1057,0,0.205918,"gure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer"
J19-3005,C12-1089,0,0.0473073,"Missing"
J19-3005,P13-1117,0,0.0959291,"Missing"
J19-3005,P11-2055,0,0.04082,"Missing"
J19-3005,I08-2093,0,0.0406793,"ions. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Glossed Texts (IGT). Such collections of example sentences are collated by linguists and contain grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train context free grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features. 574 Ponti et al. Modeling Language Variation and Universals Unsupervised Propagation Morphosyntactic Annotation Table 2 An overview of the strategies for prediction of typological features. Author Details Requirements Liu (2010) Lewis and Xia (2008) Treebank count IGT projection Treebank IGT, source chunker 20 97 Bender et al. (2013) IGT projection IGT, source chunker 31 Östling ("
J19-3005,P13-3022,0,0.408676,"k-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word order and case alignment 986 word order 6 word order 325 whole word order and passive whole 14"
J19-3005,W15-1521,0,0.0349482,"Missing"
J19-3005,D17-1268,0,0.142382,"Missing"
J19-3005,P08-1099,0,0.0527456,"hallenge: How can the output of the model be biased to agree with the constraints while the efficiency of the search procedure is kept? In this article we do not answer this question directly but rather survey a number of approaches that succeed in dealing with it. Because linear models have been prominent in NLP research for a much longer time, it is not surprising that frameworks for the integration of soft constraints into these models are much more developed. The approaches proposed for this purpose include posterior regularization (PR) (Ganchev et al. 2010), generalized expectation (GE) (Mann and McCallum 2008), constraint-driven learning (CODL) (Chang, Ratinov, and Roth 2007), dual decomposition (DD) (Globerson and Jaakkola 2007; Komodakis, Paragios, and Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distributio"
J19-3005,P05-1012,0,0.0516887,"Missing"
J19-3005,Q17-1022,1,0.921061,"Missing"
J19-3005,N16-1018,0,0.029262,"Missing"
J19-3005,I17-1046,0,0.464172,"ea, and typology-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme"
J19-3005,P12-1066,0,0.286336,"Missing"
J19-3005,D10-1120,0,0.0346019,"Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distribution is defined on the model parameters. PR has already been used for incorporating universal linguistic knowledge into an unsupervised parsing model (Naseem et al. 2010). In the future, it could be extended to typological knowledge, which is a good fit for soft constraints. As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been"
J19-3005,W11-2124,0,0.0331622,"rameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared across languages. Tied parameters are shown as neurons with identical color. Image adapted from Fang and Cohn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vector"
J19-3005,C16-1123,1,0.918794,"Missing"
J19-3005,Q16-1030,0,0.164108,"Missing"
J19-3005,P15-2034,0,0.172911,"syntactically annotated texts. For example, word order features can be calculated by counting the average direction of dependency relations or constituency hierarchies (Liu 2010). Consider the tree of a sentence in Welsh from Bender et al. (2013) in Figure 6. The relative order of verb– subject, and verb–object can be deduced from the position of the relevant nodes VBD, NNS , and NNO (highlighted). Morphosyntactic annotation is often unavailable for resource-lean languages. In such cases, it can be projected from a source language to a target language through language transfer. For instance, Östling (2015) projects source morphosyntactic annotation directly to several languages through a multilingual word alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Gloss"
J19-3005,E17-2102,0,0.0715416,"g multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig, and Littell 2017, see § 4.3"
J19-3005,H05-1108,0,0.0802046,"cific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promisin"
J19-3005,I17-1102,0,0.0522275,"Missing"
J19-3005,P18-1142,1,0.820943,"Missing"
J19-3005,D18-1026,1,0.847658,"Missing"
J19-3005,S17-1003,1,0.801184,"Missing"
J19-3005,N12-1008,1,0.844552,". As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been applied to minimally supervised learning and domain adaptation with soft (non-typological) constraints (Reichart and Barzilay 2012; Rush et al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, simila"
J19-3005,P15-2040,0,0.0202231,"ribution of each language and example. The selection is typically carried out through general language similarity metrics. For instance, Deri and Knight (2016) base their selection on the URIEL language typology database, considering information about genealogical, geographic, syntactic, and phonetic properties. This facilitates language transfer of grapheme-to-phoneme models, by guiding the choice of source languages and aligning phoneme inventories. Metrics for source selection can also be extracted in a data-driven fashion, without explicit reference to structured taxonomies. For instance, Rosa and Zabokrtsky (2015) estimate the Kullback–Leibler divergence between PoS trigram distributions for delexicalized parser transfer. In order to approximate the divergence in syntactic structures between languages, Ponti et al. (2018a) utilize the Jaccard distance between morphological feature sets and the tree edit distance of delexicalized dependency parses of translationally equivalent sentences. A priori and bottom–up approaches can also be combined. For delexicalized parser transfer, Agi´c (2017) relies on a weighted sum of distances based on (1) the PoS divergence defined by Rosa and Zabokrtsky (2015); (2) th"
J19-3005,P15-1173,0,0.0316864,"Missing"
J19-3005,P18-1084,1,0.891535,"Missing"
J19-3005,C14-1098,0,0.0637796,"Missing"
J19-3005,D12-1131,1,0.808979,"Missing"
J19-3005,S10-1027,0,0.0116844,"s-lingual information about frame semantics can be extracted, for example, from the Valency Patterns Leipzig database (ValPaL). Typological information regarding lexical semantics patterns can further assist various NLP tasks by providing information about translationally equivalent words across languages. Such information is provided in databases such as the World Loanword Database (WOLD), the Intercontinental Dictionary Series (IDS), and the Automated Similarity Judgment Program (ASJP). One example task is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, a"
J19-3005,P08-1084,0,0.26666,"ure design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can"
J19-3005,P11-2120,0,0.0488046,"hand, the accuracy of PoS-based metrics deteriorates easily in scenarios with scarce amounts of data. Source language selection is a special case of source language weighting where weights are one-hot vectors. However, weights can also be gradient and consist of real numbers. Søgaard and Wulff (2012) adapt delexicalized parsers by weighting every 584 Ponti et al. Modeling Language Variation and Universals training instance based on the inverse of the Hamming distance between typological (or genealogical) features in source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing"
J19-3005,C12-2115,0,0.139636,"from the practice of discarding features that are not discriminative, when they are identical for all the languages in the sample. Another group of studies used more comprehensive feature sets. The feature set of Daiber, Stanojevi´c, and Sima’an (2016) included not only WALS word order features but also nominal categories (e.g., “Conjunctions and Universal Quantifiers”) and nominal syntax (e.g., “Possessive Classification”). Berzak, Reichart, and Katz (2015) considered all features from WALS associated with morphosyntax and pruned out the redundant ones, resulting in a total of 119 features. Søgaard and Wulff (2012) utilized all the 571 Computational Linguistics Volume 45, Number 3 Figure 4 Feature sets used in a sample of typologically informed experiments for dependency parsing. The numbers refer to WALS ordering (Dryer and Haspelmath 2013). features in WALS with the exception of phonological features. Tsvetkov et al. (2016) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whet"
J19-3005,N13-1126,0,0.051696,"Missing"
J19-3005,N12-1052,0,0.116447,"Missing"
J19-3005,P15-1150,0,0.0238395,"Missing"
J19-3005,L16-1011,0,0.120366,"Missing"
J19-3005,W15-2137,0,0.099488,"Missing"
J19-3005,P12-1068,0,0.0507951,"d labor. Furthermore, the immense range of possible tasks and languages makes the aim of a complete coverage unrealistic. One solution to this problem explored by the research community abandons the use of annotated resources altogether and instead focuses on unsupervised learning. This class of methods infers probabilistic models of the observations given some latent variables. In other words, it unravels the hidden structures within unlabeled text data. Although these methods have been used extensively for multilingual applications (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011; Titov and Klementiev 2012, inter alia), their performance tends to lag behind the more linguistically informed supervised learning approaches (Täckström, McDonald, and Nivre 2013). Moreover, they have been rarely combined with typological knowledge. For these reasons, we do not review them in this section. Other promising ways to overcome data scarcity include transferring models or data from resource-rich to resource-poor languages (§ 3.1) or learning joint models from annotated examples in multiple languages (§ 3.2) in order to leverage language interdependencies. Early approaches of this kind have relied on univers"
J19-3005,N16-1161,0,0.0936611,"hn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig"
J19-3005,P16-1157,0,0.0178939,"§ 4.3). 3.3 Multilingual Representation Learning The multilingual algorithms reviewed in § 3.1 and § 3.2 are facilitated by dense realvalued vector representations of words, known as multilingual word embeddings. These can be learned from corpora and provide pivotal lexical features to several downstream NLP applications. In multilingual word embeddings, similar words (regardless of the actual language) obtain similar representations. Various methods to generate multilingual word embeddings have been developed. We follow the classification proposed by Ruder (2018), and we refer the reader to Upadhyay et al. (2016) for an empirical comparison. 567 Computational Linguistics Volume 45, Number 3 Monolingual mapping generates independent monolingual representations and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual appro"
J19-3005,P11-2084,1,0.843653,"Missing"
J19-3005,P15-2118,1,0.902813,"Missing"
J19-3005,P17-1006,1,0.822412,"Missing"
J19-3005,Q16-1035,0,0.0476954,"source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing when the source is chosen in a supervised way (performance on target development data) and in an unsupervised way (coverage of target PoS sequences). Rather than generating new synthetic data, Ponti et al. (2018a) leverage typological features to pre-process treebanks in order to reduce their variation in language transfer tasks. In particular, they adapt source trees to the typology of a target language with respect to several constructions in a rule-based fashion. For instance, relative clauses in Arabic (Afro–Asiatic) w"
J19-3005,Q17-1011,0,0.141548,"-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word"
J19-3005,D18-1215,0,0.0199401,"ltilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mr"
J19-3005,Q14-1005,0,0.0236961,"et al. (2005). In its original formulation, as illustrated in Figure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompati"
J19-3005,D14-1187,0,0.0145363,"text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-indepe"
J19-3005,W14-1613,0,0.0141525,"ns and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual approaches merge words with contexts of other languages and generate representations based on this mixed corpus. Substitutions are based on Wiktionary (Xiao and Guo 2014) or machine translation (Gouws and Søgaard 2015; Duong et al. 2016). Moreover, the mixed corpus can be produced by randomly shuffling words between aligned documents in two languages (Vuli´c and Moens 2015). Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014). Joint"
J19-3005,H01-1035,0,0.123357,"Missing"
J19-3005,I08-3008,0,0.0176835,"). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translat"
J19-3005,C16-1044,0,0.0230991,"Missing"
J19-3005,D15-1213,0,0.0647972,"Missing"
J19-3005,N16-1156,0,0.0228005,"Missing"
J19-3005,D12-1125,1,0.809705,"orical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translat"
J19-3005,P16-1133,0,0.0606511,"Missing"
J19-3005,D18-1022,1,0.844622,"om a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and co"
J19-3005,Q17-1024,0,\N,Missing
J19-3005,D18-1269,0,\N,Missing
K17-3006,L16-1262,0,0.0977152,"Missing"
K17-3006,D16-1250,0,0.176916,"rst step from which better parsers can then be derived. For this reason, we re-used most of the training algorithms implemented for the BIST-parser since these have proven to be effective when dealing with sequential information even for long sentences, thanks to bidirectional LSTM feature representations (Kiperwasser and Goldberg, 2016). In addition, our parser can also have recourse to multilingual word embeddings that merge different word vectors in a single vector space in order to get multi-source models. As for multilingual word embeddings, we extend the bilingual word mapping approach (Artetxe et al., 2016) to be able to deal with multilingual data. We have only used this approach based on multilingual word embeddings for two different language groups in this experiment: (i) for resource-poor languages for which less than 30 sentences were provided for training such as surprise languages and Kazakh, and (ii) for another group of 7 resource-rich languages that are all Indo-European languages. This is to show that even the analysis of resource-rich languages can be improved thanks to a multilingual approach. In this paper, we present our multilingual dependency parser developed for the CoNLL 2017"
K17-3006,L16-1680,0,0.0735472,"Missing"
K17-3006,P15-1119,0,0.0478331,"Normale Sup´erieure & PSL Univ. Ecole Univ. Sorbonne nouvelle & USPC Univ. Sorbonne nouvelle & USPC Paris, France Paris, France thierry.poibeau@ens.fr KISTI / DaeJeon, Korea ktlim@ens.fr Abstract to Universal Dependency (Nivre et al., 2016), it is now possible to train a system for several languages from the same set of POS tags. It has also been demonstrated that, with current machine learning approaches, parsing accuracy improves when using multilingual word embeddings (i.e. word embeddings inferred from corpora in different languages) even for resource-rich languages (Ammar et al., 2016a; Guo et al., 2015). In this paper, we describe the development of a system using either a monolingual or multilingual strategy (depending on the kind of resources available for each language considered) for the CoNLL 2017 shared task (Zeman et al., 2017). For the multilingual model, we assume that learning over words and POS sequences is a first step from which better parsers can then be derived. For this reason, we re-used most of the training algorithms implemented for the BIST-parser since these have proven to be effective when dealing with sequential information even for long sentences, thanks to bidirectio"
K17-3006,E89-1018,0,0.223235,"ction Many existing parsers are trainable on monolingual data. Normally such systems take a monolingual corpus in input, along with monolingual word embeddings and possibly monolingual dictionaries as well as other knowledge sources. However for resource-poor languages such as Kurmanji and Buryat2 , there are generally not enough resources to train an efficient parser. One reasonable approach is then to infer knowledge from similar languages (Tiedemann, 2015). Developing tools to process several languages including resource-poor languages has been conducted in many different ways in the past (Heid and Raab, 1989). Thanks 1 2 http://universaldependencies.org/conll17/ http://universaldependencies.org/conll17/surprise.html 63 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 63–70, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. Figure 1: Overall system structure for training language models. (1) Embedding Layer: vectorized features that are feeding into Bidirectional LSTM. (2) Bidirectional-LSTM: train representation of each token as vector values based on bidirectional LSTM neural network. (3) Multi-Layer"
K17-3006,W15-2137,0,0.0494845,"l approach for 11 languages. Our system ranked 5th and achieved 70.93 overall LAS score over the 81 test corpora (macro-averaged LAS F1 score). 1 Introduction Many existing parsers are trainable on monolingual data. Normally such systems take a monolingual corpus in input, along with monolingual word embeddings and possibly monolingual dictionaries as well as other knowledge sources. However for resource-poor languages such as Kurmanji and Buryat2 , there are generally not enough resources to train an efficient parser. One reasonable approach is then to infer knowledge from similar languages (Tiedemann, 2015). Developing tools to process several languages including resource-poor languages has been conducted in many different ways in the past (Heid and Raab, 1989). Thanks 1 2 http://universaldependencies.org/conll17/ http://universaldependencies.org/conll17/surprise.html 63 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 63–70, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. Figure 1: Overall system structure for training language models. (1) Embedding Layer: vectorized features that are feeding int"
K18-2014,D16-1250,0,0.0223471,"e model with bidirectional LSTM using ELMo as an intermediate layer in the bidirectional language model (biLM), and we used ELMo embeddings to improve again the performance of our model. ← → Ri = {xLM , h LM i,j |= 1, ..., L} i = {hLM i,j |= 0, ..., L} ELM oi = E(Ri ; Θ) = γ L X sj hLM i,j Multilingual Feature Representations 3.1 Embedding Projection There are different strategies to produce multilingual word embeddings (Ruder et al., 2018), but a very efficient one consists in simply projecting one word embedding on top of the other to make both representations share the same semantic space (Artetxe et al., 2016). The alternative involves directly generating bilingual word embeddings from bilingual corpora (Gouws et al., 2015; Gouws and Sgaard, 2015), but this requires a large amount of bilingual data aligned at sentence or document level. This kind of resource is not available for most language pairs, especially for underresourced languages. We thus chose to train independently monolingual word embeddings and then map these word embeddings one to another. This approach is powerful since monolingual word embeddings generally share a similar structure (especially if they have been trained on similar co"
K18-2014,N15-1157,0,0.0240366,"s to improve again the performance of our model. ← → Ri = {xLM , h LM i,j |= 1, ..., L} i = {hLM i,j |= 0, ..., L} ELM oi = E(Ri ; Θ) = γ L X sj hLM i,j Multilingual Feature Representations 3.1 Embedding Projection There are different strategies to produce multilingual word embeddings (Ruder et al., 2018), but a very efficient one consists in simply projecting one word embedding on top of the other to make both representations share the same semantic space (Artetxe et al., 2016). The alternative involves directly generating bilingual word embeddings from bilingual corpora (Gouws et al., 2015; Gouws and Sgaard, 2015), but this requires a large amount of bilingual data aligned at sentence or document level. This kind of resource is not available for most language pairs, especially for underresourced languages. We thus chose to train independently monolingual word embeddings and then map these word embeddings one to another. This approach is powerful since monolingual word embeddings generally share a similar structure (especially if they have been trained on similar corpora) and so can be superimposed with little information loss. To project embeddings, we applied the linear transformation method using bil"
K18-2014,P15-1119,0,0.0394243,"hose to use a 12 dimensional vector for corpus representation. This representation tri is concatenated with the token representation xi : 3 The supervised, monolingual approach to parsing, based on syntactically annotated corpora, has long been the most common one. However, thanks to recent developments involving powerful word representation methods (a.k.a. word embeddings), it is now possible to develop accurate multilingual lexical models by mapping several monolingual embeddings into a single vector space. This multilingual approach to parsing has yielded encouraging results for both low- (Guo et al., 2015) and high-resource languages (Ammar et al., 2016). In this work, we extend the recent multilingual dependency parsing approach proposed by Lim and Poibeau (2017) that achieved state-of-the-art performance during the last CoNLL shared task by using multilingual embeddings mapped based on bilingual dictionaries. tri = Treebank(ti ; θtr ) xi = ci ◦ wi ◦ tri We used this approach (corpus representation) for 24 corpora, and its effectiveness will be discussed in Section 5. 2.3 Contextualized Representation ELMo (Embedding from Language Model (Peters et al., 2018)) is a function that provides a repr"
K18-2014,P17-1042,0,0.0137772,"irs, especially for underresourced languages. We thus chose to train independently monolingual word embeddings and then map these word embeddings one to another. This approach is powerful since monolingual word embeddings generally share a similar structure (especially if they have been trained on similar corpora) and so can be superimposed with little information loss. To project embeddings, we applied the linear transformation method using bilingual dictionar(1) (2) j=0 In (1), xLM and hLM i,0 are word embedding veci ← → tors corresponding to the token layer. h LM i,j is 145 ies proposed by Artetxe et al. (2017). We took the bilingual dictionaries from OPUS2 and Wikipedia. The projection method can be described as follows. Let X and Y be the source and target word embedding matrix so that xi refers to ith word embedding of X and yj refers to jth word embedding of Y. And let D be a binary matrix where Dij = 1, if xi and yj are aligned. Our goal is to find a transformation matrix W such that Wx approximates y. This is done by minimizing the sum of squared errors: arg min W m X n X the word representation element wi is thus needed to transform a bidirectional LSTM, as a way to capture the overall contex"
K18-2014,D16-1211,0,0.0228081,"rt of the 2018 Extrinsic Parser Evaluation campaign. 1 • Corpus representation: a vector representation of each training corpus. • Multilingual word representation: a multilingual word representation obtained by the projection of several pre-trained monolingual embeddings into a unique semantic space (following a linear transformation of each embedding). Introduction Feature representation methods are an essential element for neural dependency parsing. Methods such as Feed Forward Neural Network (FFN) (Chen and Manning, 2014) or LSTM-based word representations (Kiperwasser and Goldberg, 2016; Ballesteros et al., 2016) have been proposed to provide fine-grained token representations, and these methods provide state of the art performance. However, learning efficient feature representations is still challenging, especially for underresourced languages. One way to cope with the lack of training data is a multilingual approach, which makes it possible to use different corpora in different languages • ELMo representation: token-based representation integrating abundant contexts gathered from external resources (Peters et al., 2018). In this paper, we extend the multilingual graphbased parser proposed by Lim and"
K18-2014,Q16-1023,0,0.045829,"tional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign. 1 • Corpus representation: a vector representation of each training corpus. • Multilingual word representation: a multilingual word representation obtained by the projection of several pre-trained monolingual embeddings into a unique semantic space (following a linear transformation of each embedding). Introduction Feature representation methods are an essential element for neural dependency parsing. Methods such as Feed Forward Neural Network (FFN) (Chen and Manning, 2014) or LSTM-based word representations (Kiperwasser and Goldberg, 2016; Ballesteros et al., 2016) have been proposed to provide fine-grained token representations, and these methods provide state of the art performance. However, learning efficient feature representations is still challenging, especially for underresourced languages. One way to cope with the lack of training data is a multilingual approach, which makes it possible to use different corpora in different languages • ELMo representation: token-based representation integrating abundant contexts gathered from external resources (Peters et al., 2018). In this paper, we extend the multilingual graphbased"
K18-2014,L18-1352,1,0.756126,"e introduced before. We used 100 dimensional character-level word representations with a 200 dimensional MLP, as presented in Section 2, and for corpus representation, we used a 12 dimensional vector. We set the learning-rate to 0.002 with Adam optimization. Multilingual Embeddings. As described in Section 3, we specifically trained multilingual embedding models for nine low-resource languages. Table 2 gives the list of languages for which we adopted this approach, along with the language used for knowledge transfer. We selected language pairs based on previous studies (Lim and Poibeau, 2017; Lim et al., 2018; Partanen et al., 2018) for bxr, kk, kmr, sme, and hsb, and the others where chosen based on the public availability of bilingual dictionaries (this explains why we chose to map several languages with English, even when there was no real linguistically motivated reason to do so). Since we could not find any pre-trained embeddings for pcm nsc, we applied a delexicalized parsing approach based on an English monolingual model. ELMo. We used ELMo weights to train specific models for five languages: Korean, French, English, Japanese and Chinese. ELMo weights were pre-trained using the CoNLL resour"
K18-2014,D14-1082,0,0.0213087,"representations. Finally, we were also ranked 1st at the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign. 1 • Corpus representation: a vector representation of each training corpus. • Multilingual word representation: a multilingual word representation obtained by the projection of several pre-trained monolingual embeddings into a unique semantic space (following a linear transformation of each embedding). Introduction Feature representation methods are an essential element for neural dependency parsing. Methods such as Feed Forward Neural Network (FFN) (Chen and Manning, 2014) or LSTM-based word representations (Kiperwasser and Goldberg, 2016; Ballesteros et al., 2016) have been proposed to provide fine-grained token representations, and these methods provide state of the art performance. However, learning efficient feature representations is still challenging, especially for underresourced languages. One way to cope with the lack of training data is a multilingual approach, which makes it possible to use different corpora in different languages • ELMo representation: token-based representation integrating abundant contexts gathered from external resources (Peters"
K18-2014,K17-3006,1,0.605287,"}@ens.fr, {parkce, leeck}@kangwon.ac.kr Abstract as training data. In most cases, for instance in the CoNLL 2017 shared task (Zeman et al., 2017), the teams that have adopted this approach used a multilingual delexicalized parser (i.e. a multi-source parser trained without taking into account lexical features). However, it is evident that delexicalized parsing cannot capture contextual features that depend on the meaning of words within the sentence. Following previous proposals promoting a model-transfer approach with lexicalized feature representations (Guo et al., 2016; Ammar et al., 2016; Lim and Poibeau, 2017), we have developed the SEx BiST parser (Semantically EXtended BiLSTM parser), a multi-source trainable parser using three different contextualized lexical representations: We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies). The main characteristic of our work is the encoding of three different modes of contextual information for parsing: (i) Treebank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via unsupe"
K18-2014,K17-3002,0,0.120274,"ion of the hidden layer matrix Hi , for which attention weights ai are calculated as follows: ai = Sofmax(watt Hi T ) ci = ai Hi Deep Contextualized Token Representations Since we apply the Softmax function, making weights sum up to 1 after a linear transformation of Hi with attention parameter watt , the selfattention weight ai intuitively corresponds to the most informational characters of token ti for parsing. Finally, by summing up the hidden state Hi of each word according to its attention weights ai , we obtain our character-level word representation vector for token ti . Most recently, Dozat et al. (2017) suggested an enhanced character-level representation based on the concatenation of hm and ai Hi so as to capture both the summary and context information in one go for parsing. This is an option that could be explored in the future. After some empirical experiments, we chose bidirectional LSTM encoders rather than a single directional one and then introduced the hidden state Hi into the two-layered Multi-Layer Perceptron (MLP) without bias terms for computing the attention weight ai : The architecture of our parser follows the multilingual L ATTICE parser presented in Lim and Poibeau (2017),"
K18-2014,W17-0411,0,0.0321089,"rganizers then ran the different scripts related to the different tasks and computed the corresponding results). We trained one single English model for the three tasks using the three English corpora provided (en lines, en ewt, en gum) without treebank embeddings (tr), since we did not know which corpus embedding would perform better. In addition, 6 we did not apply our ensemble process on TIRA since it would have been too time consuming. Our results are listed in Table 4. They include an intrinsic evaluation (overall performance of the parser on the different corpora considered as a whole) (Nivre and Fang, 2017) and taskspecific evaluations (i.e. results for the three different tasks). In the intrinsic evaluation, we obtained the best LAS among all the participating systems, which confirms the portability of our approach across different domains. As for the taskspecific evaluations, we obtained the best result for event extraction, but our parser did not perform so well on negation resolution and opinion analysis. This means that specific developments would be required to properly address the two tasks under consideration, taking semantics into consideration. 7 Conclusion In this paper, we described"
K18-2014,P99-1059,0,0.0916187,"on wi and then we encode the resulting vector via BiLSTM. This enriches the syntactic representations of the token by back-propagation during training: wi = Word(ti ; θmw ) We applied the multilingual embedding mostly to train the nine low-resource languages of the 2018 CoNLL evaluation, for which only a handful of annotated sentences were provided. 4 vi = BiLSTM(dep) (v0 , (x1 ,x2 ,..xn ))i Following Dozat and Manning (2016), we used a deep bi-affine classifier to score all the possible head and modifier pairs Y = (h,m). We then selected the best dependency graph based on Eisner’s algorithm (Eisner and Satta, 1999). This algorithm tries to find the maximum spanning tree among all the possible graphs: X arg max ScoreM ST (h, m) Multi-Task Learning for Tagging and Parsing In this section, we describe our Part-Of-Speech (POS) tagger and dependency parser using the encoded token representation xi based on Multi-Task Learning (MTL) (Zhang and Yang, 2017). 4.1 Dependency Parser Part-Of-Speech Tagger valid Y As presented in Section 2 and 3, our parser is based on models trained with a combination of features, encoding different contextual information. However, the attention mechanism for the character-level wo"
K18-2014,W18-0201,1,0.823785,"e. We used 100 dimensional character-level word representations with a 200 dimensional MLP, as presented in Section 2, and for corpus representation, we used a 12 dimensional vector. We set the learning-rate to 0.002 with Adam optimization. Multilingual Embeddings. As described in Section 3, we specifically trained multilingual embedding models for nine low-resource languages. Table 2 gives the list of languages for which we adopted this approach, along with the language used for knowledge transfer. We selected language pairs based on previous studies (Lim and Poibeau, 2017; Lim et al., 2018; Partanen et al., 2018) for bxr, kk, kmr, sme, and hsb, and the others where chosen based on the public availability of bilingual dictionaries (this explains why we chose to map several languages with English, even when there was no real linguistically motivated reason to do so). Since we could not find any pre-trained embeddings for pcm nsc, we applied a delexicalized parsing approach based on an English monolingual model. ELMo. We used ELMo weights to train specific models for five languages: Korean, French, English, Japanese and Chinese. ELMo weights were pre-trained using the CoNLL resources provided 4 . We used"
K18-2014,K18-2002,0,0.0483841,"Missing"
K18-2014,N18-1202,0,0.181641,", 2014) or LSTM-based word representations (Kiperwasser and Goldberg, 2016; Ballesteros et al., 2016) have been proposed to provide fine-grained token representations, and these methods provide state of the art performance. However, learning efficient feature representations is still challenging, especially for underresourced languages. One way to cope with the lack of training data is a multilingual approach, which makes it possible to use different corpora in different languages • ELMo representation: token-based representation integrating abundant contexts gathered from external resources (Peters et al., 2018). In this paper, we extend the multilingual graphbased parser proposed by Lim and Poibeau (2017) with the three above representations. Our parser is open source and available at: https://github.com/CoNLL-UD-2018/ LATTICE/. Our parser performed well in the official end-toend evaluation (73.02 LAS – 4th out of 26 teams, and 78.72 UAS – 2nd out of 26). We obtained very 143 Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 143–152 c Brussels, Belgium, October 31 – November 1, 2018. 2018 Association for Computational Linguistics https://d"
K18-2014,K17-3003,0,0.0207509,", and also due a lack of training data for some languages. The structure of the paper is as follows. We first describe the feature extraction and representation methods (Section 2 and 3) and then present our POS tagger and our parser based on multi-task learning (Section 4). We then give some details on our implementation (Section 5) and we finally provide an analysis of our official results (Section 6). 2 For LSTM-based character-level representations, previous studies have shown that the last hidden layer hm represents a summary of all the information based on the input character sequences (Shi et al., 2017). It is then possible to linearly transform this with a parameter wc so as to get the desired dimensionality. Another representation method involves applying an attention-based linear transformation of the hidden layer matrix Hi , for which attention weights ai are calculated as follows: ai = Sofmax(watt Hi T ) ci = ai Hi Deep Contextualized Token Representations Since we apply the Softmax function, making weights sum up to 1 after a linear transformation of Hi with attention parameter watt , the selfattention weight ai intuitively corresponds to the most informational characters of token ti f"
K18-2014,P18-2098,0,0.0615633,"Missing"
K18-2014,K17-3001,0,0.0260425,"Missing"
K18-2014,K18-2001,0,0.0655164,"Missing"
L16-1300,agerri-etal-2014-ixa,0,0.0135282,"itions, formalized as ‹actor, predicate, negotiation point› tuples 6 (see Figure 1). The information is made navigable, together with the original corpus, on a user The terminology adopted is as follows: ‹France, proposed, a new approach to carbon sinks› is a proposition, with actor France, predicate proposed and a new approach to carbon sinks as the negotiation point. 6 interface. In addition, the system extracts keywords and linked entities from the negotiation points, and also displays them on the interface, in response to user queries. 4.1. NLP pipeline We used the IXA Pipes NLP toolkit7 (Agerri et al., 2014), and compatible tools. The toolkit’s default English modules were used for tokenization, part of speech tagging and constituency parsing. Anaphora resolution: Some types of pronominal anaphora (see 4.2) were resolved via custom rules based on coreference chains from CorefGraph 8 , a Python implementation of Stanford’s dcoref (Lee et al. 2013). Dependency parsing and semantic role labelling (SRL) were carried out with ixa-pipe-srl 9 , which provides a wrapper around the mate-tools library (Björkelund, Bohnet et al., 2010). The dependency and SRL format are the CoNLL ones (Surdeanu et al., 2008"
L16-1300,C10-3009,0,0.0613336,"Missing"
L16-1300,J13-4004,0,0.0137231,"the negotiation point. 6 interface. In addition, the system extracts keywords and linked entities from the negotiation points, and also displays them on the interface, in response to user queries. 4.1. NLP pipeline We used the IXA Pipes NLP toolkit7 (Agerri et al., 2014), and compatible tools. The toolkit’s default English modules were used for tokenization, part of speech tagging and constituency parsing. Anaphora resolution: Some types of pronominal anaphora (see 4.2) were resolved via custom rules based on coreference chains from CorefGraph 8 , a Python implementation of Stanford’s dcoref (Lee et al. 2013). Dependency parsing and semantic role labelling (SRL) were carried out with ixa-pipe-srl 9 , which provides a wrapper around the mate-tools library (Björkelund, Bohnet et al., 2010). The dependency and SRL format are the CoNLL ones (Surdeanu et al., 2008). SRL is performed against PropBank (Palmer et al. 2005) and NomBank (Meyers et al. 2004). Keyphrase Extraction: We used YaTeA (Aubin and Hamon, 2006), which extracts multiple-word and single-word terms in an unsupervised manner, using syntactic and statistical criteria. Entity Linking (EL) was performed with the ELCO3 tool from our previous"
L16-1300,D12-1048,0,0.25603,"ches surveyed in Grimmer and Stewart (2013). These methods are useful in order to arrive at an overview of the content of large corpora. However, these techniques do not identify which predicates relate co-occurring elements with each other. If an actor like France is mentioned in the same sentence as a concept, like stricter regulations, which is the verb mediating between both? Is France in favour of, or against stricter regulations? Several technologies can detect related elements in texts, and the predicate that indicates their relation. A recent approach is Open Relation Extraction (e.g. Mausam et al., 2012), where relations are identified without the need to previously specify a vocabulary of predicates or actors. The corpus we’re working on consists of summaries of international climate negotiations (ENB corpus) 1 . A single sentence in this corpus can contain several support and opposition predicates, which can be verbal or nominal (see Figure 1). For this corpus, the results of a workflow based on open relation extraction tools were uneven, particularly with nominal predicates. 2 To address these challenges, we developed an application with a domain model and analysis rules which operate on o"
L16-1300,W04-2705,0,0.15586,"Missing"
L16-1300,J05-1004,0,0.226969,"default English modules were used for tokenization, part of speech tagging and constituency parsing. Anaphora resolution: Some types of pronominal anaphora (see 4.2) were resolved via custom rules based on coreference chains from CorefGraph 8 , a Python implementation of Stanford’s dcoref (Lee et al. 2013). Dependency parsing and semantic role labelling (SRL) were carried out with ixa-pipe-srl 9 , which provides a wrapper around the mate-tools library (Björkelund, Bohnet et al., 2010). The dependency and SRL format are the CoNLL ones (Surdeanu et al., 2008). SRL is performed against PropBank (Palmer et al. 2005) and NomBank (Meyers et al. 2004). Keyphrase Extraction: We used YaTeA (Aubin and Hamon, 2006), which extracts multiple-word and single-word terms in an unsupervised manner, using syntactic and statistical criteria. Entity Linking (EL) was performed with the ELCO3 tool from our previous work (Ruiz and Poibeau, 2015). This combines EL outputs from several public-domain EL systems, and selects the best outputs via a weighted vote. Annotation format: IXA Pipes uses NAF, the NLP Annotation Format (Fokkens et al., 2014). This is an 7 http://ixa2.si.ehu.es/ixa-pipes/ https://bitbucket.org/Josu/coref"
L16-1300,S15-1025,1,0.810673,"ndency parsing and semantic role labelling (SRL) were carried out with ixa-pipe-srl 9 , which provides a wrapper around the mate-tools library (Björkelund, Bohnet et al., 2010). The dependency and SRL format are the CoNLL ones (Surdeanu et al., 2008). SRL is performed against PropBank (Palmer et al. 2005) and NomBank (Meyers et al. 2004). Keyphrase Extraction: We used YaTeA (Aubin and Hamon, 2006), which extracts multiple-word and single-word terms in an unsupervised manner, using syntactic and statistical criteria. Entity Linking (EL) was performed with the ELCO3 tool from our previous work (Ruiz and Poibeau, 2015). This combines EL outputs from several public-domain EL systems, and selects the best outputs via a weighted vote. Annotation format: IXA Pipes uses NAF, the NLP Annotation Format (Fokkens et al., 2014). This is an 7 http://ixa2.si.ehu.es/ixa-pipes/ https://bitbucket.org/Josu/corefgraph Constituency parsing is pre-required by this tool. 9 https://github.com/newsreader/ixa-pipe-srl 8 1903 Figure 2: System architecture: The corpus is indexed in Solr, and enriched with different annotations, that get stored in a MySQL DB: keyphrases, linked entities, and ‹actor, predicate, negotiation point› pro"
L16-1300,W08-2121,0,0.0104887,"(Agerri et al., 2014), and compatible tools. The toolkit’s default English modules were used for tokenization, part of speech tagging and constituency parsing. Anaphora resolution: Some types of pronominal anaphora (see 4.2) were resolved via custom rules based on coreference chains from CorefGraph 8 , a Python implementation of Stanford’s dcoref (Lee et al. 2013). Dependency parsing and semantic role labelling (SRL) were carried out with ixa-pipe-srl 9 , which provides a wrapper around the mate-tools library (Björkelund, Bohnet et al., 2010). The dependency and SRL format are the CoNLL ones (Surdeanu et al., 2008). SRL is performed against PropBank (Palmer et al. 2005) and NomBank (Meyers et al. 2004). Keyphrase Extraction: We used YaTeA (Aubin and Hamon, 2006), which extracts multiple-word and single-word terms in an unsupervised manner, using syntactic and statistical criteria. Entity Linking (EL) was performed with the ELCO3 tool from our previous work (Ruiz and Poibeau, 2015). This combines EL outputs from several public-domain EL systems, and selects the best outputs via a weighted vote. Annotation format: IXA Pipes uses NAF, the NLP Annotation Format (Fokkens et al., 2014). This is an 7 http://ix"
L18-1352,D16-1250,0,0.0394149,"http: //komikyv.org/). For North Saami, we have used the SIKOR North Saami free corpus (http://hdl. handle.net/11509/100), which has been published with a CC-BY 3.0 license. 3.2. Projection of Multiple Word Embeddings onto a Single Space In the previous section, we described how we obtained and trained monolingual embeddings for each language, but each of those embeddings is trained in its own vector space. In order to transform the different embeddings into one single bilingual word embedding (encoded through a single vector space model), we apply the linear transformation method proposed by Artetxe et al. (2016). According to comparisons presented in Artetxe et al. (2017, p. 457), the size of dictionaries we used is well above what is needed to carry out the mapping task using this method. The method is as follows. Let target language X and source language Y be the word embedding matrix trained by two different languages. And let D={(xi ,yi )}m i=1 (where xi ∈ X, yi ∈ Y ) be a bilingual dictionary consisting of wordembedding vector pairs. Our goal is to find a transformation matrix W such that xW approximates y. This is done by 2231 minimizing the sum of squared errors, following Mikolov et al. (2013"
L18-1352,P17-1042,0,0.03571,"R North Saami free corpus (http://hdl. handle.net/11509/100), which has been published with a CC-BY 3.0 license. 3.2. Projection of Multiple Word Embeddings onto a Single Space In the previous section, we described how we obtained and trained monolingual embeddings for each language, but each of those embeddings is trained in its own vector space. In order to transform the different embeddings into one single bilingual word embedding (encoded through a single vector space model), we apply the linear transformation method proposed by Artetxe et al. (2016). According to comparisons presented in Artetxe et al. (2017, p. 457), the size of dictionaries we used is well above what is needed to carry out the mapping task using this method. The method is as follows. Let target language X and source language Y be the word embedding matrix trained by two different languages. And let D={(xi ,yi )}m i=1 (where xi ∈ X, yi ∈ Y ) be a bilingual dictionary consisting of wordembedding vector pairs. Our goal is to find a transformation matrix W such that xW approximates y. This is done by 2231 minimizing the sum of squared errors, following Mikolov et al. (2013): arg min W m X kxi W − yi k 2 (1) i=1 However, the applica"
L18-1352,D16-1211,0,0.0252205,"when compared to previous work, allowing for wider reuse of pre-existing resources when parsing low-resource languages. The study also explores the question of whether contemporary contact languages or genetically related languages would be the most fruitful starting point for multilingual parsing scenarios. Keywords: dependency parsing, word embeddings, Uralic languages 1. Introduction Developing systems for low-resource languages is a crucial issue for Natural Language Processing (NLP). Most NLP systems are built using supervised learning techniques (Weiss et al., 2015; Straka et al., 2016; Ballesteros et al., 2016). These systems require a large amount of annotated data and are thus targeted toward specific languages for which this kind of data exists. Unfortunately, producing enough annotated data is known to be time- and resourceconsuming, which means that annotated data, especially of the type required for parsing, is lacking for most languages. To take a recent example, the 2017 CoNLL Shared Task concerned around 50 languages, roughly all of the languages for which enough syntactically annotated data is available in the Universal Dependency format. This was probably (by far) the most ambitious parsi"
L18-1352,D14-1082,0,0.0895336,"Dependency Parsing Model Traditionally, many parsers have applied linear supervised learning models with hand-crafted feature functions. The feature function takes features for classifying head-modifier and relations among tokens (i.e. “word forms and POS tags from first and second tokens on top of the stack”) (Kiperwasser and Goldberg (2016)). Parsers also require many templates in order to make a decision about the relations between tokens. However, extracting the proper features and templates manually is a difficult and time-consuming job. In order to address the limitation of manual work, Chen and Manning (2014) proposed using non-linear classifiers with a neural network model. This method encodes lexical (words) and non-lexical (POS tags) features as vectors and then concatenates the features of each token to feed the non-linear classifiers. This has two advantages: On the one hand, non-linear classifiers show better performance than linear models in identifying relations between tokens, and on the other hand, the use of a neural network with concatenated features alleviates the need for manual work because the neural model, especially in Recurrent Neural Networks (RNNs), has access to tokens and fe"
L18-1352,K17-3002,0,0.0299457,"use e(wi ) and e(pi ), with additional features such as distance between head node and language-specific lexical features included in the UD corpus. Note that ti feeds into BiLSTM(t1:n i) in order to store the Forward and Backward contexts from the LSTM. 4.3. Parsing Model There are two mainstream approaches to parsing, one being the transition-based model (Nivre, 2004) and the other the graph-based model (McDonald et al., 2005b). For this study, we chose the graph-based approach based on the BIST-parser since graph-based approaches seem to show better performance for parsing UD-type corpora (Dozat et al., 2017). From the features and tokens stored in the BiLSTM layer, the BIST-parser computes a candidate tree for each head word and modifier, after which scores attached to the different candidate trees are computed using the multilayer perceptron (MLP), which is a basic neural network model that can be used as a scoring function. Finally, the system finds the best dependency parsing trees based on the sum of the subtrees. For further information on the graph-based and arc-factored model used in the BISTparser, please see Taskar et al. (2005) and McDonald et al. (2005a). 2232 Case 1 2 3 4 5 Training c"
L18-1352,P15-1119,0,0.277529,"uages in the world: even if one includes only those languages for which written data is available, the 50 languages targeted at CoNLL 2017 cover only a fraction of all the world’s languages. When it comes to parsing, the supervised, monolingual approach based on syntactically annotated corpora has long been the most common one. However, thanks to recent developments involving feature-representation methods (a.k.a. word embeddings) and neural network models, it is now possible to develop accurate multilingual models, too. The multilingual approach has yielded encouraging results for both low- (Guo et al., 2015) and high-resource languages (Ammar et al., 2016a). Generally speaking, the multilingual approach can be implemented in two ways. The first involves projecting annotations available for a high-resource language onto a low-resource language using a parallel corpus, while the second aims at producing a cross-lingual transfer model that can work for several languages. Guo et al. (2016) and Ammar et al. (2016a) have conducted multilingual parsing studies for Indo-European languages using the model transfer approach. They demonstrated that a multilingual model can yield better results than monoling"
L18-1352,Q16-1023,0,0.169952,"ndard source language S and then mapped it with each target language T=(t1 ,t2 ,ti ..). Based on the trained parameter WT = (Wt1 , Wt2 , Wti ..), we can thus build a multilingual word embedding by multiplying the parameter Wti and all the surface forms included in ti . 4. Cross-Lingual Dependency Parsing Model Traditionally, many parsers have applied linear supervised learning models with hand-crafted feature functions. The feature function takes features for classifying head-modifier and relations among tokens (i.e. “word forms and POS tags from first and second tokens on top of the stack”) (Kiperwasser and Goldberg (2016)). Parsers also require many templates in order to make a decision about the relations between tokens. However, extracting the proper features and templates manually is a difficult and time-consuming job. In order to address the limitation of manual work, Chen and Manning (2014) proposed using non-linear classifiers with a neural network model. This method encodes lexical (words) and non-lexical (POS tags) features as vectors and then concatenates the features of each token to feed the non-linear classifiers. This has two advantages: On the one hand, non-linear classifiers show better performa"
L18-1352,K17-3006,1,0.86313,"the other hand, the use of a neural network with concatenated features alleviates the need for manual work because the neural model, especially in Recurrent Neural Networks (RNNs), has access to tokens and features computed previously for a given sentence. Our basic feature representation approach is based on Chen and Manning (2014), with the exception of the method used for pretrained multilingual embeddings. In order to take into account lexical resources during parsing, we have extended the multilingual graph-based parser based on the bidirectional LSTM feature representations proposed by Lim and Poibeau (2017). 4.1. Bidirectional LSTM Feature Representations Recent advances in NLP have been possible largely due to innovative feature representations that provide an accurate overview of word relations inside the sentence (Cho, 2015; Huang et al., 2015). A good example is the BIST-parser proposed by Kiperwasser and Goldberg (2016), which is based on bidirectional LSTM learning. BiLSTM is a powerful learning model for sequential data because it consists of two LSTM layers, a Forward layer that reads the sentence from left to right, and another that reads it from right to left. For example, given a sent"
L18-1352,P05-1012,0,0.153121,"onally, we added a language hot-encoding vector composed of 0 and 1 for each language as proposed earlier (Naseem et al., 2012; Ammar et al., 2016a). Compared with monolingual parsers, most use e(wi ) and e(pi ), with additional features such as distance between head node and language-specific lexical features included in the UD corpus. Note that ti feeds into BiLSTM(t1:n i) in order to store the Forward and Backward contexts from the LSTM. 4.3. Parsing Model There are two mainstream approaches to parsing, one being the transition-based model (Nivre, 2004) and the other the graph-based model (McDonald et al., 2005b). For this study, we chose the graph-based approach based on the BIST-parser since graph-based approaches seem to show better performance for parsing UD-type corpora (Dozat et al., 2017). From the features and tokens stored in the BiLSTM layer, the BIST-parser computes a candidate tree for each head word and modifier, after which scores attached to the different candidate trees are computed using the multilayer perceptron (MLP), which is a basic neural network model that can be used as a scoring function. Finally, the system finds the best dependency parsing trees based on the sum of the sub"
L18-1352,H05-1066,0,0.421461,"Missing"
L18-1352,P12-1066,0,0.0411108,"Missing"
L18-1352,W04-0308,0,0.117001,"ned word embedding introduced in Section 3. Additionally, we added a language hot-encoding vector composed of 0 and 1 for each language as proposed earlier (Naseem et al., 2012; Ammar et al., 2016a). Compared with monolingual parsers, most use e(wi ) and e(pi ), with additional features such as distance between head node and language-specific lexical features included in the UD corpus. Note that ti feeds into BiLSTM(t1:n i) in order to store the Forward and Backward contexts from the LSTM. 4.3. Parsing Model There are two mainstream approaches to parsing, one being the transition-based model (Nivre, 2004) and the other the graph-based model (McDonald et al., 2005b). For this study, we chose the graph-based approach based on the BIST-parser since graph-based approaches seem to show better performance for parsing UD-type corpora (Dozat et al., 2017). From the features and tokens stored in the BiLSTM layer, the BIST-parser computes a candidate tree for each head word and modifier, after which scores attached to the different candidate trees are computed using the multilayer perceptron (MLP), which is a basic neural network model that can be used as a scoring function. Finally, the system finds th"
L18-1352,W18-0201,1,0.85676,"). From this point of view, one could expect Finnish to perform well in parsing both Komi and North Saami, although the similarities between these languages have not been studied in great detail from the perspective of syntax and dependency structures. Other types of experiments have also been conducted using this approach, 5 for example, by using a Komi-Zyrian–Russian multilingual model to parse data that contains both languages in the form of code-switching: in these tests, the parser has been shown to be able to analyse language-specific constructions when they occur within same utterance (Partanen et al., 2018). Bi-dictionary 12,879 12,398 8,746 10,541 12,354 Bi-embedding 2.3GB 2.4GB 7.5GB 2.4GB 5.7GB Table 1: Dictionary sizes and size of bilingual word embeddings generated by each dictionary. We have used the pretrained Finnish and Russian FastText word embeddings published by Facebook in May 2017 (Bojanowski et al., 2016). Since the Komi and Saami Wikipedias are relatively small, we have also trained larger word embeddings using FastText. For Komi, we have used Public Domain books digitalized in the Fenno-Ugrica collection (https://fennougrica. kansalliskirjasto.fi/) and proofread by The Finno-Ugr"
L18-1352,L16-1680,0,0.0445737,"Missing"
L18-1352,P15-1032,0,0.0441466,"Missing"
landragin-etal-2012-analec,N03-4009,0,\N,Missing
landragin-etal-2012-analec,2009.jeptalnrecital-court.23,0,\N,Missing
messiant-etal-2008-lexschem,schulte-im-walde-2002-subcategorisation,0,\N,Missing
messiant-etal-2008-lexschem,poibeau-messiant-2008-still,1,\N,Missing
messiant-etal-2008-lexschem,W98-1114,0,\N,Missing
messiant-etal-2008-lexschem,W00-1325,1,\N,Missing
messiant-etal-2008-lexschem,A97-1052,0,\N,Missing
messiant-etal-2008-lexschem,J93-2002,0,\N,Missing
messiant-etal-2008-lexschem,P05-1038,0,\N,Missing
messiant-etal-2008-lexschem,P93-1032,0,\N,Missing
messiant-etal-2008-lexschem,P02-1029,0,\N,Missing
messiant-etal-2008-lexschem,P07-1115,1,\N,Missing
messiant-etal-2008-lexschem,sagot-etal-2006-lefff,0,\N,Missing
messiant-etal-2008-lexschem,chesley-salmon-alt-2006-automatic,0,\N,Missing
messiant-etal-2008-lexschem,korhonen-etal-2006-large,1,\N,Missing
N13-1134,D10-1115,0,0.796735,"often attributed to Frege, is the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consis"
N13-1134,D12-1050,0,0.0738527,"that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions. Our model has been evaluated"
N13-1134,J90-1003,0,0.141423,"As a baseline, we compute the non-contextualized 1148 similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor X, using 1K verbs × 10K subjects × 10K objects. Note once again that the subject and object instances in the second step are exactly the same as the noun instances in the first step. Tensor X has been weighted using a three-way extension of PMI, following equation 10 (Van de Cruys, 2011). pmi3(x, y, z) = log p(x, y, z) p(x)p(y)p(z) (10) We set K = 300 as our number of latent factors. The"
N13-1134,D10-1113,0,0.0448751,"43 it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009, 2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Dinu and Lapata use non-negative matrix factorization (NMF) to induce latent factors. Similar to their work, our model uses NMF – albeit in a slightly different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic"
N13-1134,D08-1094,0,0.461001,"Missing"
N13-1134,W09-0208,0,0.0859558,"Missing"
N13-1134,N10-3005,0,0.0464449,"different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic Analysis (Landauer and Dumais, 1997), which uses singular value decomposition in order to automatically induce latent factors from term-document matrices. Another well known latent model of meaning, which takes a generative approach, is Latent Dirichlet Allocation (Blei et al., 2003). Tensor factorization has been used before for the modeling of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, subjects, and objects. Our underlying tensor factorization – Tucker decomposition – is the same as Giesbrecht’s; and similar to Van de Cruys (2010), we construct a latent model of verb, subject, and object interactions. The way our model is constructed, however, is significantly different. The former research does not use any syntactic information"
N13-1134,D11-1129,0,0.488078,"Missing"
N13-1134,W11-2507,0,0.0582789,"Missing"
N13-1134,nivre-etal-2006-maltparser,0,0.0282329,"bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor X, using 1K verbs × 10K subjects × 10K objects. Note once again that the subject and object instances in the second step are exactly the same as the noun instances in the first step. Tensor X has been weighted using a three-way extension of PMI, following equation 10 (Van de Cruys, 2011). pmi3(x, y, z) = log p(x, y, z) p(x)p(y)p(z) (10) We set K = 300 as our number of latent factors. The value was chosen as a trade-off between a model that is both rich enough, and does not require an excessiv"
N13-1134,D12-1110,0,0.502234,"states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent"
N13-1134,W09-2506,0,0.0281007,"Missing"
N13-1134,P10-1097,0,0.359266,"Missing"
N13-1134,W11-1303,1,0.86549,"Missing"
N13-1134,C00-2137,0,0.0112051,"he target verb in composition (system meets criterion) to the non-contextualized semantics of the landmark verb (visit). Note that the scores presented in this evaluation (including the baseline score) are significantly higher than the scores presented in Grefenstette and Sadrzadeh (2011b). This is not surprising, since the corpus we use – UKWAC – is an order of magnitude larger than the corpus used in their research – the British National Corpus (BNC). Presumably, the scores are also favoured by our weighting measure. 8 p < 0.01; model differences have been tested using stratified shuffling (Yeh, 2000). 1149 In this paper, we presented a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We used our method to model the composition of subject verb object combinations. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions, represented by a"
N13-1134,P08-1028,0,\N,Missing
N15-3010,villemonte-de-la-clergerie-etal-2008-passage,0,0.0269799,"ers select a set of linked entities to navigate a corpus is specifically relevant to our workflow. Systems that combine entity linkers exist, e.g. NERD (Rizzo et al., 2012). However, there are two important differences in our workflow. First, the set of entity linkers we combine is entirely open source and public. Second, we use a simple voting scheme to optionally offer automatically chosen annotations when linkers provide conflicting outputs. This type of weighted vote had not previously been attempted for EL outputs to our knowledge, and is inspired on the ROVER method (Fiscus, 1997, De la Clergerie et al., 2008). Regarding systems that help users navigate a corpus by choosing a representative set of linked entities, our reference is the ANTA tool (Venturini and Guido, 2012).1 This tool helps users choose entities via an assessment of their corpus frequency and document frequency. Our tool provides such information, besides a measure of each entity’s coherence with the rest of entities in the corpus. 1 https://github.com/medialab/ANTA 46 Proceedings of NAACL-HLT 2015, pages 46–50, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 3 Workflow description The user’"
N15-3010,J97-1003,0,0.217512,"the vote helps to select among conflicting annotation candidates, besides helping identify unreliable annotations. 3.2 Entity types Entity types are assigned by exploiting information provided in the linkers’ responses, e.g. DBpedia ontology types or Wikipedia category 2 http://tagme.di.unipi.it/tagme_help.html https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki 4 http://wikipedia-miner.cms.waikato.ac.nz/ 5 https://github.com/marcocor/bat-framework 6 Our notion of lexical cohesion relies on token overlap across consecutive token sequences, inspired on the block comparison method from Hearst (1997). 3 47 labels. The entity types currently assigned are Organization, Person, Location, Concept. 3.3 Entity coherence measures Once entity selection is completed, a score that quantifies an entity’s coherence with the rest of entities in the corpus is computed. This notion of coherence consists of two components. The first one is an entity’s relatedness to other entities in terms of Milne and Witten’s (2008) Wikipedia Link-based Measure (WLM, details below). The second component is the distance between entities’ categories in a Wikipedia category graph. WLM scores were obtained with Wikipedia M"
N15-3010,S15-1025,1,0.826051,"ut if their confidence score is below the optimal thresholds for those services, reported in Cornolti et al. (2013) and verified using the BATFramework.5 3.1 Annotation voting The purpose of combining several linkers’ results is obtaining combined annotations that are more accurate than each of the linkers’ individual results. To select among the different linkers’ outputs, a vote is performed on the annotations that remain after the initial filtering described above. Our voting scheme is based on De la Clergerie et al.’s (2008) version of the ROVER method. An implementation was evaluated in (Ruiz and Poibeau, 2015). Two factors that our voting scheme considers are annotation confidence, and the number of linkers having produced an annotation. An important factor is also the performance of the annotator having produced each annotation on a corpus similar to the user’s corpus: At the outset of the workflow, the user’s corpus is compared to a set of reference corpora along dimensions that affect EL results, e.g. text-length or lexical cohesion6 in the corpus’ documents. Annotators that perform better on the reference corpus that is most similar along those dimensions to the user’s corpus are given more wei"
N15-3010,E12-2015,0,\N,Missing
N15-3010,W14-2505,0,\N,Missing
poibeau-etal-2002-evaluating,C94-2119,0,\N,Missing
poibeau-etal-2002-evaluating,P99-1050,0,\N,Missing
poibeau-messiant-2008-still,messiant-etal-2008-lexschem,1,\N,Missing
poibeau-messiant-2008-still,J96-3009,0,\N,Missing
poibeau-messiant-2008-still,W00-1325,0,\N,Missing
poibeau-messiant-2008-still,J06-2001,0,\N,Missing
poibeau-messiant-2008-still,A97-1052,0,\N,Missing
poibeau-messiant-2008-still,P07-1051,0,\N,Missing
poibeau-messiant-2008-still,W05-0901,0,\N,Missing
poibeau-messiant-2008-still,sagot-etal-2006-lefff,0,\N,Missing
poibeau-messiant-2008-still,chesley-salmon-alt-2006-automatic,0,\N,Missing
poibeau-messiant-2008-still,poibeau-etal-2002-evaluating,1,\N,Missing
R09-1009,W04-1013,0,0.0101785,"sed of a user query and of two groups of documents. Documents are extracted from the AQUAINT2 corpus (a collection of news stories issued by several press agencies). The first type of summary is the “standard” one, a simple summary of the first document set. The second type of summary is more complex: it has to summarize the information found in the second document set that was not already present in the first document set. Summaries are to be 100 words long at most. For the Update task, two evaluations were given to participants: the first one using PYRAMID, the second one using ROUGE scores [5]. The PYRAMID score depends on the number of basic semantic units the summary contains which are considered as important by human annotators (the importance of a semantic unit depends on the number of times it appears in the summaries generated by human annotators). Summaries have also been scored using five different scores attributed manually for grammaticality, non-redundancy, structure, fluency and overall responsiveness (responsiveness is a subjective score corresponding to the question “How much would you pay for that summary?”). ROUGE metrics are based on n-gram comparison between the a"
R09-1009,radev-etal-2004-mead,0,0.0411227,"names and keywords in the document title, the presence of indicative phrases and the sentence length. More recently, research has mainly focused on multidocument summarization. In this context, a central issue consists in eliminating redundancy since the risks of extracting two sentences conveying the same information is more important than in the single-document paradigm. Moreover, identifying redundancy is a critical task, as information appearing several times in different documents is supposed to be important. The “centroid-based summarization” method developed by Radev and his colleagues [9] is probably the most popular one in the field. It consists in identifying the centroid of a cluster of documents, that is to say the terms which best describe the documents to 45 International Conference RANLP 2009 - Borovets, Bulgaria, pages 45–49 summarize. Then, the sentences to be extracted are the ones that are closest to the centroid. Radev implemented this method in an online multi-document summarizer called MEAD. Radev further improved MEAD using a method inspired by the concept of prestige in social networks. This method called “graph-based centrality” [4] consists in computing simil"
R09-1009,N07-1013,0,0.034142,"where nodes are sentences and edges are similarities. Sentence selection is then performed by picking the sentences which have been visited most after a random walk on the graph. The main limitation of this method is that it only selects central sentences, which means that most of them can be redundant. It is thus necessary to add a module to detect redundancy before producing the final summary. In order to avoid dealing with redundancy as a postprocessing task, various methods have been proposed to integrate redundancy detection during the summarization process itself. For example, Goldberg [10] uses a “Markov absorbing chain random walk” on a graph representing the different sentences of the corpus to summarize. MMR-MD, introduced by Carbonnel in [2], is a measure that needs a “passage” (snippet) clustering: all passages considered as paraphrases are grouped into the same clusters. MMR-MD takes into account the similarity to a query, the coverage of a passage (clusters that it belongs to), the content of the passage, the similarity to passages already selected for the summary, the fact that it belongs to a cluster or to a document that has already contributed a passage to the summar"
R11-1038,W09-1304,0,0.0224731,"deontic, etc.). We do not think it is appropriate to have a so fine grained description as these categories will be inappropriate for most language understanding applications. Note that this more fine grained categorization is not incompatible with our scheme. It just requires that some of the categories are refined. Most recent frameworks do not seem to answer these issues, even for the “event detection” task; they often contain domain specific annotation (Aitken, 2002; Mcdonald et al., 2004; Jayram et al., 2006; Shen et al., 2007; Kim et al., 2008) or focus on a certain type of information (Morante and Daelemans, 2009). So we need to build on the ACE scheme in order to overcome some of its shortcomings. 4 A New Relation Annotation Scheme Semantic relations correspond to a core event with most of time additional information related to the event. These additional pieces of information are most of the time encoded through negations, modalities and higher level clauses (for reported speech for example). Our contribution addresses these elements. 4.1 Enunciative Modalities Basic Event Encoding 5 We consider that a semantic relation is part of the linguistic expression of an event. This relation is most of the ti"
R11-1038,W99-0613,0,0.144711,"Missing"
R11-1038,poesio-artstein-2008-anaphoric,0,0.021243,"re recognition), but the recognition of basic elements and relations between them is nevertheless a shared basis among a large number of systems (Jurafsky and Martin, 2009). This of course explains why there has been an increasing amount of research both on named entity recognition and on relation analysis in the last 20 years (MUC6, 1995; Appelt and Martin, 1999). However, the maturity of these two tasks differs to a large extent. As for named entity recognition, a large number of tools, data and gold standard are 1 One of our reviewers suggested previous studies (like (Carlson et al., 2002; Poesio and Artstein, 2008), among several others). However, none of these propose a general scheme for semantic relation annotation. They generally deal with a specific theory (e.g. Rhetorical Structure Theory (Carlson et al., 2002)) or a specific phenomenon (e.g. anaphora resolution (Poesio and Artstein, 2008)). Recent frameworks like ACE take profits of all these studies but a large number of problems remains unsolved, see (ACE, 2008a). 275 Proceedings of Recent Advances in Natural Language Processing, pages 275–281, Hissar, Bulgaria, 12-14 September 2011. • Under the Note Purchase Agreement: (a) Dolphin Fund II acqu"
R11-1038,sekine-etal-2002-extended,0,0.0965538,"Missing"
R11-1038,C96-1079,0,0.596407,"er in the aggregate principal amount of $988,900, which convertible notes were convertible, as of January 15, 2003 into 3,826,270 shares of Common Stock selves, but we propose to annotate contextual information for a more thorough analysis of relations expressed in texts. Contextual information includes negations, modalities and reported speech, which are surprisingly poorly represented in most schemes. We first show why semantic relation annotation is difficult. We then present previous schemes that have been proposed in different frameworks, esp. the Message Understanding Conferences (MUC) (Grishman and Sundheim, 1996) and the Automatic Content Extraction (ACE) conferences, as well as their limitations. We then propose our own scheme and present two experiments showing that annotators using our scheme were able to quickly annotate a large number of sentences with a very high accuracy. 2 In this example, the text is complex, refers to domain specific concepts and does not even give the key to the annotator: it is not explicitly said if the result of the transaction means a transfer of the control of the company or not. All these refer to knowledge engineering problems: most of the time, a good command of dom"
R11-1038,W01-1605,0,\N,Missing
S07-1093,gravier-etal-2004-ester,0,0.0668081,"Missing"
S07-1093,P03-1008,0,0.481644,"orms only. 3 A (too) Lazy Approach We chose not to use any part-of-speech tagger or syntactic or semantic analyzer; we did not use any external knowledge or any other annotated corpus than the one provided for the training phase. Since no NLP tool was used, we had to duplicate most of the words in order to get the singular and the plural form. Our system is thus very simple compared to 418 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 418–421, c Prague, June 2007. 2007 Association for Computational Linguistics the state-of-art in this domain (e.g. Nissim and Markert, 2003). We used discriminative plain words only. These are gathered as follows: all the words in a given window (here we use a 7 word window, before and after the target entity since it gave the best results on the training data) are extracted and associated with two classes (literal vs. non literal). We thus consider the most discriminative words, i.e. words that appear frequently in some contexts but not in others (literal vs. non-literal readings). Discriminative words are elements that are abnormally frequent or rare in one corpus compared to another one. Characteristic features are selected bas"
S15-1025,D07-1074,0,0.0756087,"tested on four golden sets. First, the two datasets that had also been used as reference sets in order to obtain the weights to vote annotations with (see Section 3.2). These two datasets were AIDA/CONLL B (231 documents with 4485 annotations; 1039 characters avg., news and sports topics) and IITB (103 documents with 11245 annotations; 3879 characters avg., topics from news, science and others). In order to test whether the annotator weights obtained from those two corpora can improve results when applied to annotator combination on other corpora, we tested on two additional datasets: MSNBC (Cucerzan, 2007), with 20 documents and 658 annotations (3316 characters avg., news topics) and AQUAINT (Milne and Witten, 2008b), with 50 documents and 727 annotations (1415 characters avg., news topics). The AQUAINT dataset contains annotations for common noun entities (besides Person, Location, Organization). For this reason, according to the procedure described in 3.2 above, its annotations were weighted according to annotators’ ranking on 10 See Table 1 and Table 2 below for Pmax values in the ranking reference corpora: Pmax is the maximum (excluding row Combined) in columns AIDA/CONLL B and IITB. 213 th"
S15-1025,P14-2076,0,0.0325307,"Missing"
S15-1025,D11-1072,0,0.393767,"is inspired by the ROVER method, which had not been previously attempted for EL to our knowledge. A further difference in our system is that the set of linkers we combine is public and open-source. 211 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 211–215, Denver, Colorado, June 4–5, 2015. 3 Combining Annotators Our workflow performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008a), AIDA4 (Hoffart et al., 2011) and Babelfy5 (Moro et al. 2014). A description of the different systems can be found in (Usbeck et al., 2015). The systems rely on a variety on algorithms and it can be expected that their results will complement each other. 3.1 Obtaining Individual Annotator Outputs First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold,6 which is configured in our workflow. We obtained optimal thresholds for each system (Column t in Tables 1 and 2) with the BAT Framework7 (Cornolti et al., 2013). The"
S15-1025,Q14-1019,0,0.0502309,"ch had not been previously attempted for EL to our knowledge. A further difference in our system is that the set of linkers we combine is public and open-source. 211 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 211–215, Denver, Colorado, June 4–5, 2015. 3 Combining Annotators Our workflow performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008a), AIDA4 (Hoffart et al., 2011) and Babelfy5 (Moro et al. 2014). A description of the different systems can be found in (Usbeck et al., 2015). The systems rely on a variety on algorithms and it can be expected that their results will complement each other. 3.1 Obtaining Individual Annotator Outputs First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold,6 which is configured in our workflow. We obtained optimal thresholds for each system (Column t in Tables 1 and 2) with the BAT Framework7 (Cornolti et al., 2013). The BAT Framework allows calling se"
S15-1025,E12-2015,0,0.143684,"annotators is particularly relevant for the present article. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. ROVER was found to improve parsing results by De la Clergerie et al. (2008). In Named Entity Recognition (NER), Rizzo et al. (2014) improved results combining systems via different machine learning algorithms. In entity linking, the potential benefits of combining annotations have been explored before. Rizzo and Troncy (2012) describe the NERD system, which combines entity linkers. However, we are not aware of a system that, like ours, makes an automatic choice among the systems’ conflicting annotations, based on an estimate of each annotation’s quality. Our approach to choose among conflicting annotations is inspired by the ROVER method, which had not been previously attempted for EL to our knowledge. A further difference in our system is that the set of linkers we combine is public and open-source. 211 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 211–215, D"
S15-1025,rizzo-etal-2014-benchmarking,0,0.178099,"Missing"
S15-1025,villemonte-de-la-clergerie-etal-2008-passage,0,\N,Missing
S15-1025,S15-2049,0,\N,Missing
S15-2060,D11-1072,0,0.0489056,"eference corpora. To perform EL on a new corpus, our heuristic considers the following criteria: First, the types of EL annotations needed by the user. Second, how similar the new corpus is (along dimensions described below) to the reference corpora on which we have pre-ranked the annotators. To apply the workflow to a new corpus, the heuristic chooses the annotator-ranking obtained with the reference corpus that is most similar to that new corpus, while still respecting the annotation-types needed by the user. The reference corpora on which we pre-ranked the annotators are AIDA/CoNLL Test B (Hoffart et al., 2011), and IITB (Kulkarni et al., 2009). These corpora are very different to each other, in terms of character length, topical variety, and regarding whether they annotate common-noun mentions or not. Moreover, some EL systems obtain opposite results when evaluated on AIDA/CoNLL B vs. IITB, as tests by Cornolti et al. (2013) and on the GERBIL platform7 have shown. The heuristic’s first criterion is the types of annotations needed: If the user needs annotations for common-noun mentions, the IITB ranking is used, since IITB is the only one in our reference-datasets that was annotated for such mention"
S15-2060,Q14-1019,0,0.0377474,"nce can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics 3 System Description The system performs English EL to Wikipedia, combining the outputs of the following EL systems: Tagme 21 (Ferragina and Scaiella, 2010), DBpedia Spotlight2 (Mendes et al. 2011), Wikipedia Miner3 (Milne and Witten, 2008) and Babelfy4 (Moro et al. 2014). Babelfy outputs were only considered if they started with a WIKI prefix or their first character was uppercase.5 Details about each of our workflow’s steps follow. 3.1 Individual Systems’ Thresholds First of all, a client requests the annotations for a text from each linker’s web-service, using the services’ default settings except for the confidence threshold, which is configured in our system. Annotations whose confidence is below a threshold are eliminated. All of the linkers used, except Babelfy, output confidence scores for their annotations. Cornolti et al., (2013) reported optimal con"
S15-2060,E12-2015,0,0.17634,"n. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which had not been previously attempted for EL to our knowledge. Systems that combine entity linkers exist (NERD, Rizzo and Troncy, 2012). However, a difference in our system is that the set of linkers we combine is public and open-source. A second difference is the set of methods we employed to combine annotations. EL evaluation work (Cornolti et al., 2013), (Usbeck et al., 2015) has highlighted to what an extent EL systems’ performance can differ depending on characteristics of the corpus. This motivates testing whether different EL systems, properly combined, can complement each other. 355 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359, c Denver, Colorado, June 4-5, 2015. 2"
S15-2060,rizzo-etal-2014-benchmarking,0,0.0190164,"is the following: Section 2 discusses related work, and Section 3 describes the General surveys on EL can be found in (Cornolti et al., 2013) and (Rao et al., 2013). Work on combining NLP annotators and on evaluating EL systems is particularly relevant for our submission. The goal of combining different NLP systems is obtaining combined results that are better than the results of each individual system. Fiscus (1997) created the ROVER method, with weighted voting to improve speech recognition outputs. A ROVER was found to improve parsing results by De la Clergerie et al. (2008). Rizzo et al. (2014) improved Named Entity Recognition results, combining systems via different machine learning algorithms. Our approach is inspired on the ROVER method, which had not been previously attempted for EL to our knowledge. Systems that combine entity linkers exist (NERD, Rizzo and Troncy, 2012). However, a difference in our system is that the set of linkers we combine is public and open-source. A second difference is the set of methods we employed to combine annotations. EL evaluation work (Cornolti et al., 2013), (Usbeck et al., 2015) has highlighted to what an extent EL systems’ performance can dif"
S15-2060,S15-1025,1,0.158922,"threshold to decide on annotations produced by one annotator only. More work is needed to determine the reason 15 Biomedical Math & Computer General N P R F1 TopF1 48 100 83.3 90.9 100 22 100 54.4 70.6 74.3 16 100 81.3 89.7 90.3 Table 2: English EL Run 1 results by domain. Note that the small number of EL items available for each domain limits in our opinion the reliability of interpretations for these results. Since our workflow combines several EL systems, it would be interesting to compare results for each individual system by itself vs. the results for the combined system. In later work (Ruiz and Poibeau, 2015), using an improved version of the system described here, and larger EL golden-sets, we performed such comparisons, finding significant improvements in the combined system vs. the individual ones. 5 Conclusion 88.9 Table 1: English EL results for all domains. 14 for this difference in results, i.e. whether the second approach itself is not useful to combine EL annotations, or whether its worse results were related to our implementation. One of the task’s purposes was to compare systems’ performance across domains. Table 2 shows our best run’s results per domain. Column N reflects the number of"
S15-2060,villemonte-de-la-clergerie-etal-2008-passage,0,\N,Missing
S15-2060,S15-2049,0,\N,Missing
W02-1113,A97-1029,0,0.0836308,"Missing"
W02-1113,P01-1039,0,0.0188209,"the time-consuming task of elaborating IE resources is concerned with the generalization of extraction patterns from examples. (Muslea, 1999) gives an extensive description of the different approaches of that problem. Autoslog (Riloff, 1993) was one of the very first systems using a simple form of learning to build a dictionary of extraction patterns. Successors of AutoSlog like Crystal (Soderland et al., 1995) mainly use decision trees and relational learning techniques to learn set of rules during their extraction step. More recently, the SrV system (Freitag, 1998) and the Pinocchio system (Ciravegna, 2001) use a combination of relational and basic statistical methods inspired from Naïve Bayes for IE tasks. These approaches acquire knowledge from texts but they must be completed with a semantic expansion module. Several authors have presented experiments based on Wordnet (Bagga et al., 1996). Our approach is original given that it consists in an integrated system, using both a semantic network and a corpus to acquire knowledge and overcome the limitations of both knowledge sources. On the one hand, the fact that we use a semantic network allows us to obtain a broader coverage than if we only use"
W02-1113,C02-1062,1,0.863535,"Missing"
W02-1113,P99-1050,0,0.0213903,"versal…Vivendi, that bought Universal). The compilation of the set of meta-graphs produces a graph made of 317 states and 526 relations. These graphs are relatively abstract but the end-user is not intended to directly manipulate them. They generate instantiated graphs, that is to say graphs in which the abstract variables have been replaced linguistic information as modeled in the constraint tables. This method associates a couple of elements with a set of transformation that covers more examples than the one of the training corpus. This generalization process is close to the one imagined by Morin and Jacquemin (1999) for terminology analysis. 5 Evaluation The evaluation concerned the extraction of information from a French financial corpus, about companies buying other companies. The corpus is made of 300 texts (200 texts for the training corpus, 100 texts for the test corpus). A system was first manually developed and evaluated. We then tried to perform the same task with automatically developed resources, so that a comparison is possible. At the beginning, the end-user must provide a set of relevant pattern to the acquisition system. We have developed a filtering tool to help the end user focus on relev"
W04-1207,E99-1043,0,\N,Missing
W07-1015,W98-0314,0,0.0621545,"Missing"
W14-0610,W12-3202,0,0.049503,"ta is generally based on the extraction of key information (authors, keywords) and the discovery of their relationships. The data can be represented as a graph, therefore graph algorithmics can be used to study the topology and the evolution of the graph of collaborations or the graph of linked authors. It is thus possible to observe the evolution of the domain, check some hypotheses or common assumptions about this evolution and provide a strong empirical basis to epistemology studies. The paper “Towards a computational History of the ACL: 1980-2008” is very relevant from this point of view (Anderson et al., 2012). The authors try to determine the evolution of the main sub-domains of research within NLP since 1980 and they obtain very interesting results. For example, they show the influence of the American evaluation campaigns on the domain: when a US agency sponsored a sub-domain of NLP, one can observe a quick concentration effect since a wide number of research groups suddenly concentrated their efforts on the topic; when no evaluation campaign was organized, research was much more widespread across the different sub-domains of NLP. Even if this is partially predictable, it was not obvious to be ab"
W14-0610,J90-2002,0,0.81213,"Missing"
W14-0610,C12-2097,0,0.0232014,"Missing"
W14-0610,J02-4002,0,0.176657,"Missing"
W14-0610,D11-1025,1,0.922557,"ted using this scheme (∼500 sentences; ACL abstracts are generally quite short since most of them are related to conference papers). The selection of the abstracts has been done using stratified sampling over time and journals, so as to obtain a representative corpus (papers must be related to different periods of time and different sub-areas of the domain). The annotation has been done according to the annotation guideline defined by Y. Guo, especially for long sentences when more than one category could be applied (preferences are defined to solve complex cases2 ). The algorithm defined by (Guo et al., 2011) is then adapted to our corpus. The analysis is based on positional, lexical and syntactic features, as explained above. No domain specific information was added, which makes the whole process easy to reproduce. As for parsing, we used the C&C parser (James Curran and Stephen Clark and Johan Bos, 2007). All the implementation details can be found in (Guo et al., 2011), especially concerning annotation and the learning algorithm. As a result, each sentence is associated with a tag corresponding to one of the zones defined in the annotation scheme. rise to different kinds of works, on the one ha"
W14-0610,N13-1113,1,0.704767,"the evaluation of different learning algorithms for the task and more importantly on the reduction of the volume of text to be annotated. Concerning the second point, it is mostly the biological and bio-medical domains that have attracted attention, since scientists in these domains often have to access the literature “vertically” (i.e. experts may need to have access to all the methods and protocols that have been used in a specific domain) (Mizuta et al., 2006; Tbahriti et al., 2006). Guo has since developed a similar trend of research to extend the initial work of Teufel (Guo et al., 2011; Guo et al., 2013): she has tested a large list of features to analyze the zones, evaluated different learning algorithms for the task and proposed new methods to decrease the number of texts to be annotated. The features used for learning are of three categories: i) positional (location of the sentence inside the paper), ii) lexical (words, classes of words, bigrams, etc. are taken into consideration) and iii) syntactic (the different syntactic relations as well as the class of words appearing in subject or object positions are taken into account). The analysis is thus based on more features than in Teufel’s i"
W14-0610,P07-2009,0,0.242342,"Missing"
W14-0610,J02-1004,0,0.131993,", which is positive taking into consideration the small number of sentences annotated for training. The diversity of the features used makes it easy to transfer the technique from one domain to the other without any heavy annotation phase. Results are slightly worse for the METHOD category, probably because this category is more diverse and thus more difficult to recognize. The fact that NLP terms can refer either to objectives or to methods also contributes rendering the recognition of this category more difficult. Figure 1 shows an abstract annotated by the text zoning module (the paper is (Lee et al., 2002): it 74 Table 2: Most specific keywords found in the METHOD sections. Category Method Bayesian methods Vector Space model Genetic algorithms HMM CRF SVM MaxEnt Clustering Language models Parallel Corpora Machine learning Speech & Mach. Trans. Alignment POS tagging Morphology FST Syntax Dependency parsing NLP Methods Parsing Semantics IE and IR Applications Discourse Segmentation Lexical knowledge bases Words and Resource Word similarity Corpora Evaluation Software Evaluation Calculation & complexity Constraints Methods N-grams baesyan space model, vector space, cosine genetic algorithms hidden"
W14-0610,E99-1003,0,\N,Missing
W15-2407,E12-1084,0,0.0539594,"Missing"
W17-0605,P01-1017,0,0.00829544,"to provide satisfactory results but it is well known that the development of such resources is long and costly. Recently machine learning has made it possible to develop resources at a lower cost. It is now possible to automatically analyse large corpora, typically made of several million words, and develop parsers based on the observation of surface regularities at corpus level, along with some annotated data used for training. Powerful unlexicalized parsers have been developed for several languages, with a surprisingly high accuracy given the fact no lexical information is provided in input [2, 3]. The output of these parsers has subsequently been used as a new source of knowledge for the development of large-scale lexical resources. This area of research, known as lexical acquisition, have permitted the development of large-scale dictionaries for example for English [4], French [5], Japanese [1] and lots of other languages as well. It has also been shown that the approach provides interesting (although not perfect) results: thanks to this approach, it for example possible to discover new subcategorization frames for particular verbs [6, 7] and to monitor in real time the evolution of"
W17-0605,P03-1054,0,0.037548,"to provide satisfactory results but it is well known that the development of such resources is long and costly. Recently machine learning has made it possible to develop resources at a lower cost. It is now possible to automatically analyse large corpora, typically made of several million words, and develop parsers based on the observation of surface regularities at corpus level, along with some annotated data used for training. Powerful unlexicalized parsers have been developed for several languages, with a surprisingly high accuracy given the fact no lexical information is provided in input [2, 3]. The output of these parsers has subsequently been used as a new source of knowledge for the development of large-scale lexical resources. This area of research, known as lexical acquisition, have permitted the development of large-scale dictionaries for example for English [4], French [5], Japanese [1] and lots of other languages as well. It has also been shown that the approach provides interesting (although not perfect) results: thanks to this approach, it for example possible to discover new subcategorization frames for particular verbs [6, 7] and to monitor in real time the evolution of"
W17-0605,P07-1115,0,0.227537,"million words, and develop parsers based on the observation of surface regularities at corpus level, along with some annotated data used for training. Powerful unlexicalized parsers have been developed for several languages, with a surprisingly high accuracy given the fact no lexical information is provided in input [2, 3]. The output of these parsers has subsequently been used as a new source of knowledge for the development of large-scale lexical resources. This area of research, known as lexical acquisition, have permitted the development of large-scale dictionaries for example for English [4], French [5], Japanese [1] and lots of other languages as well. It has also been shown that the approach provides interesting (although not perfect) results: thanks to this approach, it for example possible to discover new subcategorization frames for particular verbs [6, 7] and to monitor in real time the evolution of word usage or the creation of new words in a language [1], etc. Results obtained with automatic methods are of course still far from perfect: they need to be manually checked but they usually provide lots of new results and new sources of evidence for further work. One of the mo"
W17-0605,messiant-etal-2008-lexschem,1,0.897626,"s, and develop parsers based on the observation of surface regularities at corpus level, along with some annotated data used for training. Powerful unlexicalized parsers have been developed for several languages, with a surprisingly high accuracy given the fact no lexical information is provided in input [2, 3]. The output of these parsers has subsequently been used as a new source of knowledge for the development of large-scale lexical resources. This area of research, known as lexical acquisition, have permitted the development of large-scale dictionaries for example for English [4], French [5], Japanese [1] and lots of other languages as well. It has also been shown that the approach provides interesting (although not perfect) results: thanks to this approach, it for example possible to discover new subcategorization frames for particular verbs [6, 7] and to monitor in real time the evolution of word usage or the creation of new words in a language [1], etc. Results obtained with automatic methods are of course still far from perfect: they need to be manually checked but they usually provide lots of new results and new sources of evidence for further work. One of the most obvious a"
W17-0605,kipper-etal-2006-extending,0,0.0452749,"t no lexical information is provided in input [2, 3]. The output of these parsers has subsequently been used as a new source of knowledge for the development of large-scale lexical resources. This area of research, known as lexical acquisition, have permitted the development of large-scale dictionaries for example for English [4], French [5], Japanese [1] and lots of other languages as well. It has also been shown that the approach provides interesting (although not perfect) results: thanks to this approach, it for example possible to discover new subcategorization frames for particular verbs [6, 7] and to monitor in real time the evolution of word usage or the creation of new words in a language [1], etc. Results obtained with automatic methods are of course still far from perfect: they need to be manually checked but they usually provide lots of new results and new sources of evidence for further work. One of the most obvious application is probably the fact that automatic methods make it possible to complete existing resources at a lower cost, so as to obtain a better coverage [6]. Automatic methods also provide statistical information, which is a key element for any computational lin"
W17-0605,J93-2002,0,0.115111,"rk on the extraction of families of verb constructions using clustering techniques. We describe how the system is derived from a previous implementation for Japanese [1], with of course an adaptation of all the language-dependent modules to Finnish. 2 Previous Work The first works in automatic lexical acquisition date back to the early 1990s. The need for precise and comprehensive lexical databases was clearly identified as a major need for most NLP tasks (esp. parsing) and automatic acquisition techniques was then seen as a way to solve the resource bottleneck. However, the first experiments [10, 11] were limited (the acquisition process was dealing with a few verbs only and a limited number of predefined subcategorization frames). They were based on local heuristics and did not take into account the wider context. The approach was then refined so as to take into account all the most frequent verbs and subcategorization frames possible [12, 13, 14]. A last innovation consisted 39 in letting the system infer the subcategorization frames directly from the corpus, without having to predefined the list of possible frames. This approach is supposed to be less precise than the previous one, but"
W17-0605,A97-1052,0,0.392086,"ed for precise and comprehensive lexical databases was clearly identified as a major need for most NLP tasks (esp. parsing) and automatic acquisition techniques was then seen as a way to solve the resource bottleneck. However, the first experiments [10, 11] were limited (the acquisition process was dealing with a few verbs only and a limited number of predefined subcategorization frames). They were based on local heuristics and did not take into account the wider context. The approach was then refined so as to take into account all the most frequent verbs and subcategorization frames possible [12, 13, 14]. A last innovation consisted 39 in letting the system infer the subcategorization frames directly from the corpus, without having to predefined the list of possible frames. This approach is supposed to be less precise than the previous one, but most errors can be automatically filtered out since they tend to produce patterns with a very low frequency. Most experiments so far have been made on verbs (since verbs are supposed to have the most complex subcategorization frames), but the approach can also be extended to nouns and adjectives without too many problems [4]. Most developments so far h"
W17-0605,W04-2606,0,0.0386261,"ed for precise and comprehensive lexical databases was clearly identified as a major need for most NLP tasks (esp. parsing) and automatic acquisition techniques was then seen as a way to solve the resource bottleneck. However, the first experiments [10, 11] were limited (the acquisition process was dealing with a few verbs only and a limited number of predefined subcategorization frames). They were based on local heuristics and did not take into account the wider context. The approach was then refined so as to take into account all the most frequent verbs and subcategorization frames possible [12, 13, 14]. A last innovation consisted 39 in letting the system infer the subcategorization frames directly from the corpus, without having to predefined the list of possible frames. This approach is supposed to be less precise than the previous one, but most errors can be automatically filtered out since they tend to produce patterns with a very low frequency. Most experiments so far have been made on verbs (since verbs are supposed to have the most complex subcategorization frames), but the approach can also be extended to nouns and adjectives without too many problems [4]. Most developments so far h"
W17-0605,C04-1104,0,0.019484,"le frames. This approach is supposed to be less precise than the previous one, but most errors can be automatically filtered out since they tend to produce patterns with a very low frequency. Most experiments so far have been made on verbs (since verbs are supposed to have the most complex subcategorization frames), but the approach can also be extended to nouns and adjectives without too many problems [4]. Most developments so far have been done on English, but more and more experiments are now done for other languages as well (see for example, experiments on French [5], German [15], Chinese [16], or Japanese [1] among many others). The quality of the result depends of course on the kind of corpus used for acquisition, and even more on the considered language and on the size of the corpus used. Dictionaries obtained with very large corpora form the Web generally give the best performances. The availability of accurate unlexicalized parser is also a key feature for the quality of the acquisition process. To the best of our knowledge, there has not been any large-scale experiment for Finnish yet. However we are lucky to have access to large corpora of Finnish, as well as to relevant par"
W17-0605,Q13-1034,0,0.0227877,"Missing"
W17-0605,W08-1301,0,0.0374329,"Missing"
W17-0605,J06-3002,0,0.00943902,"ement based on a tf-idf measure (see below). 6.1 Description of our Approach The starting point is a list of verbs along with their complements that have been automatically extracted from a large representative corpus. In our framework, a complement is a phrase directly connected to the verb (or is, in other words, a dependency of the verb), while the verb is the head of the dependents. In what follows we assume that complements are in fact couples made of a head noun and a dependency marker, generally a case marker. 6.1.1 Calculating the Argumenthood of Complements Building on previous works [23, 24, 25, 26], Marchal [21, 1] proposes a new measure combining the prominent features describe in the literature. The measure is derived from the famous tf.idf used in information retrieval, with the major difference that we are dealing with complements instead of terms (or keywords), and with verbs instead of documents. The proposed measure assigns a value between 0 and 1 to all the complements. 0 corresponds to a prototypical adjunct; 1 corresponds to a prototypical argument. 6.1.2 Minimal clustering at the verb entry level Marchal [21, 1] introduces a method for merging verbal structures (i.e. a verb a"
W17-0605,P10-1024,0,0.0238187,"ement based on a tf-idf measure (see below). 6.1 Description of our Approach The starting point is a list of verbs along with their complements that have been automatically extracted from a large representative corpus. In our framework, a complement is a phrase directly connected to the verb (or is, in other words, a dependency of the verb), while the verb is the head of the dependents. In what follows we assume that complements are in fact couples made of a head noun and a dependency marker, generally a case marker. 6.1.1 Calculating the Argumenthood of Complements Building on previous works [23, 24, 25, 26], Marchal [21, 1] proposes a new measure combining the prominent features describe in the literature. The measure is derived from the famous tf.idf used in information retrieval, with the major difference that we are dealing with complements instead of terms (or keywords), and with verbs instead of documents. The proposed measure assigns a value between 0 and 1 to all the complements. 0 corresponds to a prototypical adjunct; 1 corresponds to a prototypical argument. 6.1.2 Minimal clustering at the verb entry level Marchal [21, 1] introduces a method for merging verbal structures (i.e. a verb a"
W17-0605,2002.jeptalnrecital-long.19,0,0.122271,"ement based on a tf-idf measure (see below). 6.1 Description of our Approach The starting point is a list of verbs along with their complements that have been automatically extracted from a large representative corpus. In our framework, a complement is a phrase directly connected to the verb (or is, in other words, a dependency of the verb), while the verb is the head of the dependents. In what follows we assume that complements are in fact couples made of a head noun and a dependency marker, generally a case marker. 6.1.1 Calculating the Argumenthood of Complements Building on previous works [23, 24, 25, 26], Marchal [21, 1] proposes a new measure combining the prominent features describe in the literature. The measure is derived from the famous tf.idf used in information retrieval, with the major difference that we are dealing with complements instead of terms (or keywords), and with verbs instead of documents. The proposed measure assigns a value between 0 and 1 to all the complements. 0 corresponds to a prototypical adjunct; 1 corresponds to a prototypical argument. 6.1.2 Minimal clustering at the verb entry level Marchal [21, 1] introduces a method for merging verbal structures (i.e. a verb a"
W17-2204,agerri-etal-2014-ixa,0,0.0220011,"he system has three components: a preprocessing module to format input poems uniformly, an NLP pipeline, and the enjambment-detection module itself. Besides the enjambment types above, Spang (1983) noted that if a subject or direct object and their related verbs occur in two different lines of poetry, this can also feel unusual for a reader, even 2 https://sites.google.com/site/ spanishenjambment/enjambment-types 3 https://sites.google. com/site/spanishenjambment/ our-large-sonnet-corpus 1 https://sites.google.com/site/ spanishenjambment 28 4 We used the IXA Pipes library as the NLP pipeline (Agerri et al., 2014), obtaining part-ofspeech tags, syntactic constituents and syntactic dependencies with it. Evaluation and Result Discussion We describe the evaluation method (the reference sets, the task and metrics), and present the results along with a brief discussion of error sources. Comments about the relevance of the results for literary studies are provided in section 5. In the absence of data annotated for enjambment, that may allow applying a machine learning approach, we created a rule and dictionary-based system that exploits the information provided by the NLP pipeline. A total of ca. 30 rules id"
W17-2204,L16-1691,0,0.0583299,"Missing"
W17-2204,P08-1037,0,0.0099352,"ped match and typed match. In untyped match, the positions of enjambed lines proposed by the system must match the positions in the reference corpus for a correct result to be counted. In typed match, for a correct result, both the positions and the enjambment type assigned by the system to those positions must match the reference. The untyped match task can be seen as an enjambment recognition task, and typed match corresponds to an enjambment classification task. 4.3 6 https://sites.google.com/site/ spanishenjambment/evaluation 7 PP attachment is a difficulty even in current languages (e.g. Agirre et al. (2008) for English). For historical varieties, Stein’s (2016) results for verbal adjuncts and prepositional complements in Old French also suggest this difficulty. System Results and Discussion Precision, recall and F1 were obtained. Table 2 provides overall results for both corpora. Table 3 provides the per-type results on the diachronic 30 Figure 1: Percentage of enjambments per position in the 15th–17th centuries vs. the 19th. The y-axis represents line-positions; the x-axis is the percentage of enjambed line-pairs for a position over all enjambed line-pairs in the period. Enjambment across quatr"
W17-2204,L16-1112,0,0.060247,"Missing"
W17-6524,baroni-etal-2004-introducing,0,0.00843404,"quisition. Few studies tried to automatically build multilingual SCFs lexica. To the best of our knowledge, there have been few experiments in multilingual verb lexicon with syntactic and semantic information, mostly establishing multilingual links manually (Civit et al., 2005; Hellan et al., 2014). 3 The LexIt Framework LexIt (Lenci et al., 2012) is a computational framework whose aim is to automatically extract distributional information about the argument structure of predicates. It was originally developed to extract information on Italian verbs, nouns and adjectives from “La Repubblica” (Baroni et al., 2004) corpus (ca. 331 millions tokens) and from a “dump” of the Italian section of Wikipedia (ca. 152 millions of tokens). The database resulting from this previous work is freely browsable.2 The whole framework aims at processing linguistic information from a dependency-parsed corpus and then storing the results into a database where each predicate is associated with a distributional profile, i.e. a data structure that combines several statistical information about the combinatorial behaviour of the lemma. This profile is articulated into: 1. a syntactic profile, specifying the syntactic arguments"
W17-6524,J10-4007,0,0.0443432,"Missing"
W17-6524,C04-1104,0,0.038021,"002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The approach consists in automatically infering subcategorization frames directly from the corpus, with or without a predefined list of possible frames. The literature reports a large number of automatically built subcategorization lexica, among which VALEX for English verbs (Korhonen et al., 2006), LexSchem (Messiant et al., 2008) and LexFr (Rambelli et al., 2016) for French verbs, LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). SCFs ac208 quisition has been investigated also for languages such as Chinese (Han et al., 2004) and Japanese (Marchal, 2015). These resources have been of particular interest to classify verbs on the basis of their syntactic and semantic properties, producing several taxonomies comparable to VerbNet (Kipper-Schuler, 2005). Despite the importance of these resources, existing lexica only focus on a single language with a specific syntactic frame representation, strongly dependent on the corpus used for acquisition. Few studies tried to automatically build multilingual SCFs lexica. To the best of our knowledge, there have been few experiments in multilingual verb lexicon with syntactic and"
W17-6524,hellan-etal-2014-multival,0,0.0307835,"verbs on the basis of their syntactic and semantic properties, producing several taxonomies comparable to VerbNet (Kipper-Schuler, 2005). Despite the importance of these resources, existing lexica only focus on a single language with a specific syntactic frame representation, strongly dependent on the corpus used for acquisition. Few studies tried to automatically build multilingual SCFs lexica. To the best of our knowledge, there have been few experiments in multilingual verb lexicon with syntactic and semantic information, mostly establishing multilingual links manually (Civit et al., 2005; Hellan et al., 2014). 3 The LexIt Framework LexIt (Lenci et al., 2012) is a computational framework whose aim is to automatically extract distributional information about the argument structure of predicates. It was originally developed to extract information on Italian verbs, nouns and adjectives from “La Repubblica” (Baroni et al., 2004) corpus (ca. 331 millions tokens) and from a “dump” of the Italian section of Wikipedia (ca. 152 millions of tokens). The database resulting from this previous work is freely browsable.2 The whole framework aims at processing linguistic information from a dependency-parsed corpu"
W17-6524,Y09-1003,0,0.0151959,"res of Italian predicates. Practical issues that arose when building argument structure representations for typologically different languages will also be discussed. 1 Introduction The argument structure of predicates is a key research area in Natural Language Processing (NLP), as verb valency has a decisive impact on sentence structure. Since including information about the syntactic-semantic realization of predicate arguments in a lexicon proved to benefit many NLP applications, e.g. recognition of textual entailment, information retrieval, machine translation and word-sense disambiguation (Korhonen, 2009), research in the (semi-)automatic acquisition of argument structure information from corpora has become widespread. Meanwhile, the last years have also witnessed a growing interest in multilingual studies and evaluation campaigns to test the quality and the robustness of parsing software. By combining these two computational linguistic topics, our work is oriented towards the elabothierry.poibeau@ens.fr ration of a cross-language subcategorization lexicon, i.e. an automatically-built resource that encodes combinatorial properties of verbs at the syntax-semantics interface. This resource will"
W17-6524,lenci-etal-2012-lexit,1,0.895893,", Italy Paris, France g.rambelli1@studenti.unipi.it alessandro.lenci@unipi.it Abstract This paper introduces UDLex, a computational framework for the automatic extraction of argument structures for several languages. By exploiting the versatility of the Universal Dependency annotation scheme, our system acquires subcategorization frames directly from a dependency parsed corpus, regardless of the input language. It thus uses a universal set of language-independent rules to detect verb dependencies in a sentence. In this paper we describe how the system has been developed by adapting the LexIt (Lenci et al., 2012) framework, originally designed to describe argument structures of Italian predicates. Practical issues that arose when building argument structure representations for typologically different languages will also be discussed. 1 Introduction The argument structure of predicates is a key research area in Natural Language Processing (NLP), as verb valency has a decisive impact on sentence structure. Since including information about the syntactic-semantic realization of predicate arguments in a lexicon proved to benefit many NLP applications, e.g. recognition of textual entailment, information re"
W17-6524,W08-1301,0,0.0143417,"Missing"
W17-6524,de-marneffe-etal-2014-universal,0,0.0398988,"Missing"
W17-6524,petrov-etal-2012-universal,0,0.0789985,"Missing"
W17-6524,P07-1115,0,0.0467982,"Missing"
W17-6524,L16-1148,1,0.83588,"Missing"
W17-6524,schulte-im-walde-2002-subcategorisation,0,0.186944,"Missing"
W17-6524,messiant-etal-2008-lexschem,1,0.792363,"tomatic methods have been developed for the identification of verb subcategorization frames (SCFs) (Korhonen, 2002; Messiant et al., 2010; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The approach consists in automatically infering subcategorization frames directly from the corpus, with or without a predefined list of possible frames. The literature reports a large number of automatically built subcategorization lexica, among which VALEX for English verbs (Korhonen et al., 2006), LexSchem (Messiant et al., 2008) and LexFr (Rambelli et al., 2016) for French verbs, LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). SCFs ac208 quisition has been investigated also for languages such as Chinese (Han et al., 2004) and Japanese (Marchal, 2015). These resources have been of particular interest to classify verbs on the basis of their syntactic and semantic properties, producing several taxonomies comparable to VerbNet (Kipper-Schuler, 2005). Despite the importance of these resources, existing lexica only focus on a single language with a specific syntactic frame representation, strongly depen"
W17-6524,I08-3008,0,0.0417221,"ically derive verb subcategorization frames regardless of the specificities of the input language. For our purpose, we decided to exploit Universal Dependencies1 (UD) annotations: UD is developed by the UD community with the final goal of creating a cross-linguistically consistent treebank annotation scheme for many languages (Nivre, 2015). The actual UD design combines the (universal) Stanford dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2014), the Google universal part-of-speech tags (UPOS) (Petrov et al., 2012) and the Interset interlingua for morpho-syntactic tag sets (Zeman and Resnik, 2008). The aim of our project is twofold: on the one hand, we want to test if UD relations are sufficient to describe argument structure for some representative languages, and on the other hand we want to create a multilingual subcategorization lexicon to carry out a contrastive study regarding argument structures, i.e., the analysis of the syntactic realization patterns of verbs arguments across languages. For instance, we would like to know if synonymous predicates across languages occur with similar or different morpho-syntactic frames, or if the same valency frame in two languages is instantiat"
W17-6524,poibeau-messiant-2008-still,1,\N,Missing
W18-0201,D16-1250,0,0.0573419,"y available data adds up to one million tokens. For the contact language Russian we have used pre-trained Wikipedia word embeddings published by Facebook and described in Bojanowski et al. (2016). In a similar manner to the low-resource constraints, Artetxe et al. (2017) suggested a powerful method for projecting two monolingual embeddings in a single vector space with almost no bilingual data. Traditionally, the projection (or mapping) method for word embeddings requires a large parallel corpus or a bilingual dictionary in order to map two different word embeddings in a distributional space (Artetxe et al., 2016; Guo et al., 2015). However, Artetxe et al. (2017) showed a possible method for mapping two different embeddings based on the reinforcement learning approach with just 25 pairs of vocabularies but with almost no degradation of performance. The main idea in this method is to project two embeddings trained by different languages based on the linear transformation with bilingual word pairs. The projection method can be described as follows. Let X and Y be the source and target word embedding matrix so that xi refers to ith word embedding of X and yj refers to jth word embedding of Y. And let D i"
W18-0201,P17-1042,0,0.226725,"using raw text available in the public domain. The Komi texts used have been taken from the National Library of Finland’s Fenno-Ugrica collection, and proofread versions of those Public Domain texts are available in FU-Lab’s portal Komi Nebögain. Niko Partanen has created a list of books included both in Fenno-Ugrica and FU-Lab, and the currently available data adds up to one million tokens. For the contact language Russian we have used pre-trained Wikipedia word embeddings published by Facebook and described in Bojanowski et al. (2016). In a similar manner to the low-resource constraints, Artetxe et al. (2017) suggested a powerful method for projecting two monolingual embeddings in a single vector space with almost no bilingual data. Traditionally, the projection (or mapping) method for word embeddings requires a large parallel corpus or a bilingual dictionary in order to map two different word embeddings in a distributional space (Artetxe et al., 2016; Guo et al., 2015). However, Artetxe et al. (2017) showed a possible method for mapping two different embeddings based on the reinforcement learning approach with just 25 pairs of vocabularies but with almost no degradation of performance. The main i"
W18-0201,W14-3902,0,0.0191852,"del trained according to the source language. On the other hand, the lexicalized approach is able to adapt diverse lexical features while in training. The features adapted for the dependency parsing include cross-lingual word cluster features (Täckström et al., 2012), multilingual word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016b,a) and language identification embeddings (Naseem et al., 2012; Ammar et al., 2016a). From the perspective of code-switching, conversational code-switching problems have been studied mainly with regard to language identification (e.g. Solorio et al., 2014; Barman et al., 2014) and information extraction (e.g. Sharma et al., 2014) problems. This is because in order to process cross-lingual dependency parsing, language identification and morphological analysis for those languages must precede the processing. Ammar et al. (2016b) suggested that his multilingual model-transfer parser could be used to parse input with code-switching but were not able to conduct the experiment due to the lack appropriate test corpora. 4 Cross-Lingual Dependency Parsing In this study, we invested our effort in developing the cross-lingual representation learning method with lexicalized fe"
W18-0201,K17-3002,0,0.0297979,"ation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and target languages. In general, cross-lingual representation learning can be divided into two approaches depending on whether or not the parser uses lexicalized features (e.g. word embedding). Since it is relatively easy to train a parser using supervised learning, many existing cross-lingual representation learning studies have been conducted with the delexicalized approach using POS tag-sets and word sequences (McDonald et al., 2011, 2013; Dozat et al., 2017). Such an approach includes training a dependency model with the source language (e.g. English), then processes the target language (e.g. French) using the model trained according to the source language. On the other hand, the lexicalized approach is able to adapt diverse lexical features while in training. The features adapted for the dependency parsing include cross-lingual word cluster features (Täckström et al., 2012), multilingual word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016b,a) and language identification embeddings (Naseem et al., 2012; Ammar et al., 2016a). From the pers"
W18-0201,W17-0109,1,0.864129,"Missing"
W18-0201,P15-1119,0,0.447429,"be parsed successfully within the same sentence, this indicates that the model is able to learn and deduce language-specific structures even when they co-occur. This would open up new possibilities for automatic analysis of such kind of data. 3 Related Studies Multilingual dependency parsing aims at building a dependency tree for several languages using one and the same model. Three major approaches have been suggested 4 for tackling such a task: 1) the cross-lingual annotation projection approach, 2) the joint modeling approach, and 3) the cross-lingual representation learning approach (cf. Guo et al., 2015). The main idea of the cross-lingual annotation projection approach is to project the syntactic annotations trough word alignments from a source language onto a target language (Mann and Yarowsky, 2001; Tiedemann, 2014). In a similar way, the joint modeling approach is carried out using projected dependency information for grammar inductions (Liu et al., 2013) and rule-based work (Naseem et al., 2010, 2012). The cross-lingual representation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and targe"
W18-0201,K17-3006,1,0.822444,"contacting majority language. Corpus data of this type represents a particular challenge for morphological analysis and especially for dependency parsing. Although the basic morphological properties can usually be analyzed on the basis of individual languages and parsers can be targeted towards those, the syntactic dependencies are inevitably interspersed individual tokens from different languages, and thereby cannot be easily approached with tools that are able to target only monolingual data. The present paper looks at an approach that has been introduced as The Multilingual BIST-Parser by Lim and Poibeau (2017). The tool was developed in order to perform dependency parsing on considerably low-resource languages, and the work was originally carried out within the CONLL-U Shared Task for 2017. Lim and Poibeau (2017) have shown that multilingual word embeddings can be used to train a model that combines data from multiple languages, and these seem to be particularly useful in low-resource scenarios where one of the languages has only a small amount of available training data. The target language in the present paper is Komi-Zyrian (henceforth Komi), which belongs to the Permic branch of the Uralic lang"
W18-0201,P13-1105,0,0.0157487,"nd the same model. Three major approaches have been suggested 4 for tackling such a task: 1) the cross-lingual annotation projection approach, 2) the joint modeling approach, and 3) the cross-lingual representation learning approach (cf. Guo et al., 2015). The main idea of the cross-lingual annotation projection approach is to project the syntactic annotations trough word alignments from a source language onto a target language (Mann and Yarowsky, 2001; Tiedemann, 2014). In a similar way, the joint modeling approach is carried out using projected dependency information for grammar inductions (Liu et al., 2013) and rule-based work (Naseem et al., 2010, 2012). The cross-lingual representation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and target languages. In general, cross-lingual representation learning can be divided into two approaches depending on whether or not the parser uses lexicalized features (e.g. word embedding). Since it is relatively easy to train a parser using supervised learning, many existing cross-lingual representation learning studies have been conducted with the delexicalized"
W18-0201,N01-1020,0,0.0250351,"ies for automatic analysis of such kind of data. 3 Related Studies Multilingual dependency parsing aims at building a dependency tree for several languages using one and the same model. Three major approaches have been suggested 4 for tackling such a task: 1) the cross-lingual annotation projection approach, 2) the joint modeling approach, and 3) the cross-lingual representation learning approach (cf. Guo et al., 2015). The main idea of the cross-lingual annotation projection approach is to project the syntactic annotations trough word alignments from a source language onto a target language (Mann and Yarowsky, 2001; Tiedemann, 2014). In a similar way, the joint modeling approach is carried out using projected dependency information for grammar inductions (Liu et al., 2013) and rule-based work (Naseem et al., 2010, 2012). The cross-lingual representation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and target languages. In general, cross-lingual representation learning can be divided into two approaches depending on whether or not the parser uses lexicalized features (e.g. word embedding). Since it is rel"
W18-0201,D11-1006,0,0.0497249,". The cross-lingual representation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and target languages. In general, cross-lingual representation learning can be divided into two approaches depending on whether or not the parser uses lexicalized features (e.g. word embedding). Since it is relatively easy to train a parser using supervised learning, many existing cross-lingual representation learning studies have been conducted with the delexicalized approach using POS tag-sets and word sequences (McDonald et al., 2011, 2013; Dozat et al., 2017). Such an approach includes training a dependency model with the source language (e.g. English), then processes the target language (e.g. French) using the model trained according to the source language. On the other hand, the lexicalized approach is able to adapt diverse lexical features while in training. The features adapted for the dependency parsing include cross-lingual word cluster features (Täckström et al., 2012), multilingual word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016b,a) and language identification embeddings (Naseem et al., 2012; Ammar et"
W18-0201,P13-2017,0,0.0800418,"Missing"
W18-0201,P12-1066,0,0.0389045,"uences (McDonald et al., 2011, 2013; Dozat et al., 2017). Such an approach includes training a dependency model with the source language (e.g. English), then processes the target language (e.g. French) using the model trained according to the source language. On the other hand, the lexicalized approach is able to adapt diverse lexical features while in training. The features adapted for the dependency parsing include cross-lingual word cluster features (Täckström et al., 2012), multilingual word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016b,a) and language identification embeddings (Naseem et al., 2012; Ammar et al., 2016a). From the perspective of code-switching, conversational code-switching problems have been studied mainly with regard to language identification (e.g. Solorio et al., 2014; Barman et al., 2014) and information extraction (e.g. Sharma et al., 2014) problems. This is because in order to process cross-lingual dependency parsing, language identification and morphological analysis for those languages must precede the processing. Ammar et al. (2016b) suggested that his multilingual model-transfer parser could be used to parse input with code-switching but were not able to condu"
W18-0201,D10-1120,0,0.0206971,"es have been suggested 4 for tackling such a task: 1) the cross-lingual annotation projection approach, 2) the joint modeling approach, and 3) the cross-lingual representation learning approach (cf. Guo et al., 2015). The main idea of the cross-lingual annotation projection approach is to project the syntactic annotations trough word alignments from a source language onto a target language (Mann and Yarowsky, 2001; Tiedemann, 2014). In a similar way, the joint modeling approach is carried out using projected dependency information for grammar inductions (Liu et al., 2013) and rule-based work (Naseem et al., 2010, 2012). The cross-lingual representation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and target languages. In general, cross-lingual representation learning can be divided into two approaches depending on whether or not the parser uses lexicalized features (e.g. word embedding). Since it is relatively easy to train a parser using supervised learning, many existing cross-lingual representation learning studies have been conducted with the delexicalized approach using POS tag-sets and word sequ"
W18-0201,W14-3914,0,0.0277874,"ther hand, the lexicalized approach is able to adapt diverse lexical features while in training. The features adapted for the dependency parsing include cross-lingual word cluster features (Täckström et al., 2012), multilingual word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016b,a) and language identification embeddings (Naseem et al., 2012; Ammar et al., 2016a). From the perspective of code-switching, conversational code-switching problems have been studied mainly with regard to language identification (e.g. Solorio et al., 2014; Barman et al., 2014) and information extraction (e.g. Sharma et al., 2014) problems. This is because in order to process cross-lingual dependency parsing, language identification and morphological analysis for those languages must precede the processing. Ammar et al. (2016b) suggested that his multilingual model-transfer parser could be used to parse input with code-switching but were not able to conduct the experiment due to the lack appropriate test corpora. 4 Cross-Lingual Dependency Parsing In this study, we invested our effort in developing the cross-lingual representation learning method with lexicalized features for the dependency parsing of code-switching sc"
W18-0201,N12-1052,0,0.0271439,"any existing cross-lingual representation learning studies have been conducted with the delexicalized approach using POS tag-sets and word sequences (McDonald et al., 2011, 2013; Dozat et al., 2017). Such an approach includes training a dependency model with the source language (e.g. English), then processes the target language (e.g. French) using the model trained according to the source language. On the other hand, the lexicalized approach is able to adapt diverse lexical features while in training. The features adapted for the dependency parsing include cross-lingual word cluster features (Täckström et al., 2012), multilingual word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016b,a) and language identification embeddings (Naseem et al., 2012; Ammar et al., 2016a). From the perspective of code-switching, conversational code-switching problems have been studied mainly with regard to language identification (e.g. Solorio et al., 2014; Barman et al., 2014) and information extraction (e.g. Sharma et al., 2014) problems. This is because in order to process cross-lingual dependency parsing, language identification and morphological analysis for those languages must precede the processing. Ammar et al."
W18-0201,C14-1175,0,0.0131072,"s of such kind of data. 3 Related Studies Multilingual dependency parsing aims at building a dependency tree for several languages using one and the same model. Three major approaches have been suggested 4 for tackling such a task: 1) the cross-lingual annotation projection approach, 2) the joint modeling approach, and 3) the cross-lingual representation learning approach (cf. Guo et al., 2015). The main idea of the cross-lingual annotation projection approach is to project the syntactic annotations trough word alignments from a source language onto a target language (Mann and Yarowsky, 2001; Tiedemann, 2014). In a similar way, the joint modeling approach is carried out using projected dependency information for grammar inductions (Liu et al., 2013) and rule-based work (Naseem et al., 2010, 2012). The cross-lingual representation learning method is focused on learning crosslingual features by aligning (or mapping) feature representations (e.g. embedding) between the source and target languages. In general, cross-lingual representation learning can be divided into two approaches depending on whether or not the parser uses lexicalized features (e.g. word embedding). Since it is relatively easy to tr"
W18-6015,W17-0109,1,0.830605,"ogy The initial analysis of Komi plain text was created using Giellatekno’s6 open infrastructure (Moshagen et al., 2014), which is currently at a rather mature level for Komi. The syntactic analysis component demands the most further work, which in turn can be guided by the work on treebanks. Similar rule-based architectures have already been used for other treebanks as well. The Northern Saami and Erzya corpora, for example, seem to have been created using a similar approach. Some work has been conducted with integrating these NLP tools into workflows commonly used in language documentation (Gerstenberger et al., 2017a,b, 2016). Since these languages often lack larger annotated resources, the use of infrastructures other than rule-based ones has not been common or possible, but these workflows have been implemented in a modular fashion that would make enable the integration of other tools when they become available or reach needed accuracy. It has been demonstrated that it is possible to convert annotations from Giellatekno’s annotation scheme into the UD scheme (Sheyanova and Tyers, 2017), and this has also worked well in our case, although the exact procedure will continue to be refined while the token c"
W18-6015,W17-0604,1,0.837572,"ogy The initial analysis of Komi plain text was created using Giellatekno’s6 open infrastructure (Moshagen et al., 2014), which is currently at a rather mature level for Komi. The syntactic analysis component demands the most further work, which in turn can be guided by the work on treebanks. Similar rule-based architectures have already been used for other treebanks as well. The Northern Saami and Erzya corpora, for example, seem to have been created using a similar approach. Some work has been conducted with integrating these NLP tools into workflows commonly used in language documentation (Gerstenberger et al., 2017a,b, 2016). Since these languages often lack larger annotated resources, the use of infrastructures other than rule-based ones has not been common or possible, but these workflows have been implemented in a modular fashion that would make enable the integration of other tools when they become available or reach needed accuracy. It has been demonstrated that it is possible to convert annotations from Giellatekno’s annotation scheme into the UD scheme (Sheyanova and Tyers, 2017), and this has also worked well in our case, although the exact procedure will continue to be refined while the token c"
W18-6015,W17-6512,0,0.0300492,"t cross-linguistic comparability and already contains several Uralic languages. Komi-Zyrian is currently the sixth Uralic language to be included in the project. Work with Komi complements well the developments associated with the emergence of new Uralic treebanks in 2017, with new repositories created for North Saami3 and Erzya (Rueter and Tyers, 2018). Another noteworthy trend is that there are several treebanks currently being created for endangered languages in situations similar to that of Komi. As far as we have been able to ascertain, these are, at least: Dargwa spoken in the Caucasus (Kozhukhar, 2017), Pnar4 spoken in South-East Asia and Shipibo-Konibo5 spoken in Peru. The description of the last treebank mentioned does not indicate the use of language documentation materials, but as the language is very small, the context is comparable. To our knowledge, the IKDP treebank discussed here is the first treebank included in the UD release that is directly based on language documentation material. It is too early to say whether there will be more similar treebanks in the future and within what timeframe, but having more materials like these included in UD would fit into the original ideas of t"
W18-6015,W17-0607,0,0.0930869,"Some work has been conducted with integrating these NLP tools into workflows commonly used in language documentation (Gerstenberger et al., 2017a,b, 2016). Since these languages often lack larger annotated resources, the use of infrastructures other than rule-based ones has not been common or possible, but these workflows have been implemented in a modular fashion that would make enable the integration of other tools when they become available or reach needed accuracy. It has been demonstrated that it is possible to convert annotations from Giellatekno’s annotation scheme into the UD scheme (Sheyanova and Tyers, 2017), and this has also worked well in our case, although the exact procedure will continue to be refined while the token count of the corpus grows, which will ultimately also reveal rarer and not-yet-analysed morphosyntactic features. After starting with manually editing CoNLL-U files, the UD Annotatrix tool (Tyers et al., 2018) was adopted in January 2018, which marked the midpoint in the project’s timeline. This greatly improved the annotation speed and consistency. The treebank creation thus consisted of the following steps: 2 Cf., e.g., the Leipzig Glossing Rules https: //www.eva.mpg.de/lingu"
W18-6015,W18-0201,1,0.81494,"ally converting the analyzer’s XPOS-tags into UPOS-tags and converting morphological feature tags into their UD counterparts 5. Manual correction and verification The current workflow involves a rather large amount of manual work. We are interested in testing various approaches to morphological and syntactic analysis so that different (rule-based, statistic-based and hybrid) parsers can eventually replace the manual work. Some tests have already been carried out with the dependency parser used by the Lattice team in the CoNLL-U Shared Task 2017 (Lim and Poibeau, 2017) and a follow-up project (Partanen et al., 2018). The treebank processing pipeline has been tied to several scripts and existing tools. The primary analysisis done within the Giellatekno toolkit (building on FST Morphology and Constraint Grammar), where tokenization, morphological analysis and rule-based disambiguation are tied to the script ‘kpvdep’. The script returns a vislcg3 file that contains all ambiguities left after the analysis. Once the ambiguities are resolved manually, the vislcg3 file can be imported into the UD Annotatrix tool. As a final step, the Giellatekno POS-tags and morphological features are converted to follow the UD"
