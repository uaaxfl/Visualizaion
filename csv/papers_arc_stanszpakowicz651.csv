C18-1033,Authorship Identification for Literary Book Recommendations,2018,0,3,3,0,30754,haifa alharthi,Proceedings of the 27th International Conference on Computational Linguistics,0,"Book recommender systems can help promote the practice of reading for pleasure, which has been declining in recent years. One factor that influences reading preferences is writing style. We propose a system that recommends books after learning their authors{'} style. To our knowledge, this is the first work that applies the information learned by an author-identification model to book recommendations. We evaluated the system according to a top-k recommendation scenario. Our system gives better accuracy when compared with many state-of-the-art methods. We also conducted a qualitative analysis by checking if similar books/authors were annotated similarly by experts."
W17-2201,Metaphor Detection in a Poetry Corpus,2017,-1,-1,3,0,31981,vaibhav kesarwani,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"Metaphor is indispensable in poetry. It showcases the poet{'}s creativity, and contributes to the overall emotional pertinence of the poem while honing its specific rhetorical impact. Previous work on metaphor detection relies on either rule-based or statistical models, none of them applied to poetry. Our method focuses on metaphor detection in a poetry corpus. It combines rule-based and statistical models (word embeddings) to develop a new classification system. Our system has achieved a precision of 0.759 and a recall of 0.804 in identifying one type of metaphor in poetry."
C16-1213,pl{W}ord{N}et 3.0 {--} a Comprehensive Lexical-Semantic Resource,2016,22,12,4,1,6131,marek maziarz,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We have released plWordNet 3.0, a very large wordnet for Polish. In addition to what is expected in wordnets {--} richly interrelated synsets {--} it contains sentiment and emotion annotations, a large set of multi-word expressions, and a mapping onto WordNet 3.1. Part of the release is enWordNet 1.0, a substantially enlarged copy of WordNet 3.1, with material added to allow for a more complete mapping. The paper discusses the design principles of plWordNet, its content, its statistical portrait, a comparison with similar resources, and a partial list of applications."
2016.gwc-1.31,Adverbs in pl{W}ord{N}et: Theory and Implementation,2016,-1,-1,2,1,6131,marek maziarz,Proceedings of the 8th Global WordNet Conference (GWC),0,"Adverbs are seldom well represented in wordnets. Princeton WordNet, for example, derives from adjectives practically all its adverbs and whatever involvement they have. GermaNet stays away from this part of speech. Adverbs in plWordNet will be emphatically present in all their semantic and syntactic distinctness. We briefly discuss the linguistic background of the lexical system of Polish adverbs. We describe an automated generator of accurate candidate adverbs, and introduce the lexicographic procedures which will ensure high consistency of wordnet editors{'} decisions about adverbs."
2016.gwc-1.42,pl{W}ord{N}et 3.0 {--} Almost There,2016,-1,-1,2,0.768729,6159,maciej piasecki,Proceedings of the 8th Global WordNet Conference (GWC),0,"It took us nearly ten years to get from no wordnet for Polish to the largest wordnet ever built. We started small but quickly learned to dream big. Now we are about to release plWordNet 3.0-emo {--} complete with sentiment and emotions annotated {--} and a domestic version of Princeton WordNet, larger than WordNet 3.1 by nearly ten thousand newly added words. The paper retraces the road we travelled and talks a little about the future."
R15-1056,A Procedural Definition of Multi-word Lexical Units,2015,17,2,2,1,6131,marek maziarz,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Multi-word expressions evade a closed definition. Linguists and computational linguists rely on intuition or build lists of MWE types; while practical, that is scientifically and aesthetically unsatisfying. Without presuming to solve a daunting theoretical problem, we propose a decision procedure which steers a lexicographer toward acceptance or rejection of an N-gram as a lexical unit: a decision tree classifies N-grams as MWE or not MWE. It will succeed if it agrees with the native speakersxe2x80x99 judgment. We need a small, linguistically credible set of features, to contend with the multiplicity of adequate trees. Decision tree induction works with a fixed set of annotated classification examples, but the lexical material for MWE recognition is too large to make annotation feasible. We rely on small-scale statistically significant sampling, and on intuition. Of a few decision trees produced by informed trial and error, we select one we consider best in our circumstances. That tree, deployed in a large-scale wordnet construction project, allowed us to gather dependable statistics on its usefulness in lexicographersxe2x80x99 work. Our goal: systematic expansion of a wordnet by tens of thousands of MWEs in a manner as free of personal biases as possible."
R15-1092,A Large {W}ordnet-based Sentiment Lexicon for {P}olish,2015,25,7,3,0,25311,monika zaskozielinska,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"The applications of plWordNet, a very large wordnet for Polish, do not yet include work on sentiment and emotions. We present a pilot project to annotate plWordNet manually with sentiment polarity values and basic emotion values. We work with lexical units, plWordNetxe2x80x99s basic building blocks.1 So far, we have annotated about 30,000 nominal and adjectival LUs. The resulting lexicon is already one of the largest sentiment and emotion resources, in particular among those based on wordnets. We opted for manual annotation to ensure high accuracy, and to provide a reliable starting point for future semi-automated expansion. The paper lists the principal assumptions, outlines the annotation process, and introduces the resulting resource, plWordNetemo. We discuss the selection of the material for the pilot study, show the distribution of annotations across the wordnet, and consider the statistics, including interannotator agreement and the resolution of disagreement."
D15-2005,Learning Semantic Relations from Text,2015,-1,-1,4,0,1636,preslav nakov,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"Every non-trivial text describes interactions and relations between people, institutions, activities, events and so on. What we know about the world consists in large part of such relations, and that knowledge contributes to the understanding of what texts refer to. Newly found relations can in turn become part of this knowledge that is stored for future use.To grasp a text{'}s semantic content, an automatic system must be able to recognize relations in texts and reason about them. This may be done by applying and updating previously acquired knowledge. We focus here in particular on semantic relations which describe the interactions among nouns and compact noun phrases, and we present such relations from both a theoretical and a practical perspective. The theoretical exploration sketches the historical path which has brought us to the contemporary view and interpretation of semantic relations. We discuss a wide range of relation inventories proposed by linguists and by language processing people. Such inventories vary by domain, granularity and suitability for downstream applications.On the practical side, we investigate the recognition and acquisition of relations from texts. In a look at supervised learning methods, we present available datasets, the variety of features which can describe relation instances, and learning algorithms found appropriate for the task. Next, we present weakly supervised and unsupervised learning methods of acquiring relations from large corpora with little or no previously annotated data. We show how enduring the bootstrapping algorithm based on seed examples or patterns has proved to be, and how it has been adapted to tackle Web-scale text collections. We also show a few machine learning techniques which can perform fast and reliable relation extraction by taking advantage of data redundancy and variability."
2015.lilt-12.1,Literature Lifts Up Computational Linguistics,2015,0,0,4,0,37947,david elson,"Linguistic Issues in Language Technology, Volume 12, 2015 - Literature Lifts up Computational Linguistics",0,None
W14-0141,Terminology in {W}ord{N}et and in pl{W}ord{N}et,2014,7,0,2,0,38913,marta dobrowolska,Proceedings of the Seventh Global {W}ordnet Conference,0,"We examine the strategies of organizing terminological information in WordNet, and describe an analogous strategy of adding terminological senses of lexical units to plWordNet, a large Polish wordnet. Wordnet builders must cope with differences in lexical and terminological definitions of a term, and with the boundaries between terminological and lexical information. A somewhat adjusted strategy is required for Polish, though both WordNet and plWordNet rely mainly on semantic relations in organizing the terminological and general-language units. The proposed guidelines for plWordNet, built on several distinct combinations of denotation and connotation, have a solid theoretical underpinning but will require a large-scale verification of their effectiveness in practice."
W14-0142,pl{W}ord{N}et as the Cornerstone of a Toolkit of Lexico-semantic Resources,2014,23,4,4,1,6131,marek maziarz,Proceedings of the Seventh Global {W}ordnet Conference,0,"A wordnet is many things to many people: a graph of inter-related lexicalised concepts, a taxonomy, a thesaurus, and so on. A wordnet makes good sense as the mainstay of any deep automated semantic analysis of text. We have begun the construction of a multi-component, multi-use toolkit of natural language processing tools with plWordNet, a very large Polish wordnet, at its centre. The components will include plWordNet and its mapping onto an ontology (the upper level and elements of the middle level), a lexicon of proper names and a semantic valency lexicon. Some of those elements will be aligned with plWordNet, and there will be a mapping onto Princeton WordNet. Several challenging applications will show the utility of the toolkit in practice. 1 How wordnets evolve Wordnets start small but quickly grow to account for much of the lexical material of the given language. The size of version 3.1 of Princeton WordNet (PWN) (Fellbaum, 1998) is a de facto standard, even if this mature wordnet also keeps growing, albeit slowly.1 One of the resources which approach this size standard is plWordNet (Piasecki et al., 2009), now in version 2.1. Languages change continually, so lexicographers never rest, but one can still ask when the development of a wordnet ought to slow down, and whether there is an appropriate steady state of a wordnet. That clearly is a loaded question, and much depends on the language. For example, suppose that a wordnet for PWN began as a test of a theory of human semantic representation and memory (Collins and Quillian, 1969). It now features a comprehensive vocabulary, a set of universally useful semantic relations, glosses, links to ontologies, and more. a richly inflected language with complex and varied derivation was originally a translation of PWN. Such a wordnet should, sooner or later, acquire semantic relations which account accurately for its unique lexical system.. A wordnet, even as developed as PWN, GermaNet (Hamp and Feldweg, 1997) or plWordNet (Maziarz et al., 2013a), serves many natural language processing (NLP) applications, yet it seems neither feasible nor necessary to remake wordnets into universal NLP resources. Instead, we propose to mark clear boundaries around a wordnet (what it should and what it should not include), and treat it as a pivotal element of an organic toolkit of inter-connected tools and resources for the semantic analysis of texts, along with the auxiliary morphological and syntactic analysis tools. Our case study is such a toolkit, now under development, centred on plWordNet 3.0 (also in development), and intended first and foremost for research in the humanities. In the remainder of the paper, we present the main design assumptions and principles of that project. We explain how comprehensive we want plWordNet 3.0 to become, what size and what coverage we envisage. We attempt to describe how the toolkit will be built around plWordNet, and we outline plans for its large-scale illustrative applications in several domains. We discuss how the components of the toolkit will be expanded or constructed: plWordNet 3.0, its mapping to an ontology, and a semantic lexicon of proper names. We also briefly present resources for morphological and structural description, associated with the plWordNet system, among them a lexicon of lexico-syntactic structures of multiword expressions and a valency lexicon linked to plWordNet but developed independently. This work is meant to take several years of initial effort and years of maintenance. We cannot answer many design questions yet, but many will be answered as the project unfolds. That is to say. we want to interlace theory and practice."
W14-0146,Registers in the System of Semantic Relations in pl{W}ord{N}et,2014,13,2,4,1,6131,marek maziarz,Proceedings of the Seventh Global {W}ordnet Conference,0,"Lexicalised concepts are represented in wordnets by word-sense pairs. The strength of markedness is one of the factors which influence word use. Stylistically unmarked words are largely contextneutral. Technical terms, obsolete words, xe2x80x9cofficialesexe2x80x9d, slangs, obscenities and so on are all marked, often strongly, and that limits their use considerably. We discuss the position of register and markedness in wordnets with respect to semantic relations, and we list typical values of register. We illustrate the discussion with the system of registers in plWordNet, the largest Polish wordnet. We present a decision tree for the assignment of marking labels, and examine the consistency of the editing decisions based on that tree."
C14-1005,Hierarchical Topical Segmentation with Affinity Propagation,2014,30,9,2,1,21616,anna kazantseva,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We present a hierarchical topical segmenter for free text. Hierarchical Affinity Propagation for Segmentation (HAPS) is derived from a clustering algorithm Affinity Propagation. Given a document, HAPS builds a topical tree. The nodes at the top level correspond to the most prominent shifts of topic in the document. Nodes at lower levels correspond to finer topical fluctuations. For each segment in the tree, HAPS identifies a segment centre xe2x80x90 a sentence or a paragraph which best describes its contents. We evaluate the segmenter on a subset of a novel manually segmented by several annotators, and on a dataset of Wikipedia articles. The results suggest that hierarchical segmentations produced by HAPS are better than those obtained by iteratively running several one-level segmenters. An additional advantage of HAPS is that it does not require the xe2x80x9cgold standardxe2x80x9d number of segments in advance."
C14-1046,Measuring Lexical Cohesion: Beyond Word Repetition,2014,25,2,2,1,21616,anna kazantseva,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper considers the problem of finding topical shifts in documents and in particular at what information can be leveraged to identify them. Recent research on topical segmentation usually assumes that topical shifts in discourse are signalled by changes in vocabulary. This information, however, is not always a sufficient indicator of a topical shift, especially for certain genres. This paper explores an additional source of information. Our hypothesis is that the type of a referring expression is an indicator of how accessible its antecedent is. The shorter and less informative the expression (e.g., a personal pronoun versus a lengthy post-modified noun phrase), the more accessible the antecedent is likely to be and the more likely it is that the topic under discussion has remained constant between the two mentions. We explore how this information can be used to augment a lexically-based topical segmenter. We test our hypothesis on two types of data, literary narratives and lecture notes. The results suggest that our similarity metric is useful: depending on the settings it either slightly improves the performance or leaves it unchanged. They also suggest that certain types of referring expressions are more useful than others."
S13-2025,{S}em{E}val-2013 Task 4: Free Paraphrases of Noun Compounds,2013,15,22,5,0.321971,16715,iris hendrickx,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"In this paper, we describe SemEval-2013 Task 4: the definition, the data, the evaluation and the results. The task is to capture some of the meaning of English noun compounds via paraphrasing. Given a two-word noun compound, the participating system is asked to produce an explicitly ranked list of its free-form paraphrases. The list is automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators, recruited and managed through Amazonxe2x80x99s Mechanical Turk. The comparison of raw paraphrases is sensitive to syntactic and morphological variation. The xe2x80x9cgoldxe2x80x9d ranking is based on the relative popularity of paraphrases among annotators. To make the ranking more reliable, highly similar paraphrases are grouped, so as to downplay superficial differences in syntax and morphology. Three systems participated in the task. They all beat a simple baseline on one of the two evaluation measures, but not on both measures. This shows that the task is difficult."
R13-1058,Beyond the Transfer-and-Merge {W}ordnet Construction: pl{W}ord{N}et and a Comparison with {W}ord{N}et,2013,32,8,4,1,6131,marek maziarz,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Wordnets are lexico-semantic resources essential in many NLP tasks. Princeton WordNet is the most widely known, and the most influential, among them. Word- nets for languages other than English tend to adopt unquestioningly WordNet's struc- ture and its net of lexicalised concepts. We discuss a large wordnet constructed inde- pendently of WordNet, upon a model with a small yet significant difference. A map- ping onto WordNet is under way; the large portions already linked open up a unique perspective on the comparison of similar but not fully compatible lexical resources. We also try to characterise numerically a wordnet's aptitude for NLP applications."
W12-3711,Prior versus Contextual Emotion of a Word in a Sentence,2012,30,8,3,0,4776,diman ghazi,Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,0,"A set of words labelled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. No simple function of the labels on the individual words may capture the overall emotion of the sentence; words are interrelated and they mutually influence their affect-related interpretation. We present a method which enables us to take the contextual emotion of a word and the syntactic structure of the sentence into account to classify sentences by emotion classes. We show that this promising method outperforms both a method based on a Bag-of-Words representation and a system based only on the prior emotions of words. The goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important for this task."
N12-1022,Topical Segmentation: a Study of Human Performance and a New Measure of Quality.,2012,21,15,2,1,21616,anna kazantseva,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In a large-scale study of how people find topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel. We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns. The results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks -- places where most prominent topic shifts occur. We recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks. We propose to account for this in a simple modification of the windowDiff metric. We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis."
C12-2101,A Strategy of Mapping {P}olish {W}ord{N}et onto {P}rinceton {W}ord{N}et,2012,16,20,4,0.897436,6142,ewa rudnicka,Proceedings of {COLING} 2012: Posters,0,"We present a strategy and the early results of the mapping of plWordNet xe2x80x90 one of the largest such language resources in existence xe2x80x90 onto Princeton Wo rdNet. The fundamental structural premise of plWordNet differs from those of most other wordnets: lexical units rather than synsets are the basic building blocks. The addition of new material to plWordNet is consistently informed by semantic relations and by various analyses of large corpora. The mapping is difficult because of the subtly distinct structures and because of WordN etxe2x80x99s focus on synsets. We have designed a set of inter-lingual semantic relations and an effectiv e mapping procedure. In the course of mapping, we have discovered a range of systematic difference s between plWordNet and WordNet, and proposed ways of accounting for such differences."
D11-1026,Linear Text Segmentation Using Affinity Propagation,2011,19,24,2,1,21616,anna kazantseva,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres -- data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in high-quality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formulation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines."
W10-0217,Hierarchical versus Flat Classification of Emotions in Text,2010,17,47,3,0,4776,diman ghazi,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,0,"We explore the task of automatic classification of texts by the emotions expressed. Our novel method arranges neutrality, polarity and emotions hierarchically. We test the method on two datasets and show that it outperforms the corresponding flat approach, which does not take into account the hierarchical information. The highly imbalanced structure of most of the datasets in this area, particularly the two datasets with which we worked, has a dramatic effect on the performance of classification. The hierarchical approach helps alleviate the effect."
S10-1006,{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals,2010,4,98,9,0.418724,16715,iris hendrickx,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"SemEval-2 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals. The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research. This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results."
S10-1007,{S}em{E}val-2 Task 9: The Interpretation of Noun Compounds Using Paraphrasing Verbs and Prepositions,2010,28,27,5,1,45588,cristina butnariu,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"Previous research has shown that the meaning of many noun-noun compounds N1 N2 can be approximated reasonably well by paraphrasing clauses of the form 'N2 that ... N1', where '...' stands for a verb with or without a preposition. For example, malaria mosquito is a 'mosquito that carries malaria'. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems."
J10-1003,Summarizing Short Stories,2010,64,27,2,1,21616,anna kazantseva,Computational Linguistics,0,"We present an approach to the automatic creation of extractive summaries of literary short stories. The summaries are produced with a specific objective in mind: to help a reader decide whether she would be interested in reading the complete story. To this end, the summaries give the user relevant information about the setting of the story without revealing its plot. The system relies on assorted surface indicators about clauses in the short story, the most important of which are those related to the aspectual type of a clause and to the main entities in a story. Fifteen judges evaluated the summaries on a number of extrinsic and intrinsic measures. The outcome of this evaluation suggests that the summaries are helpful in achieving the original objective."
J10-1008,Last Words: Failure is an Orphan (Let{'}s Adopt),2010,-1,-1,1,1,30755,stan szpakowicz,Computational Linguistics,0,None
W09-2415,{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals,2009,28,203,9,0.418724,16715,iris hendrickx,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"We present a brief overview of the main challenges in the extraction of semantic relations from English text, and discuss the shortcomings of previous data sets and shared tasks. This leads us to introduce a new task, which will be part of SemEval-2010: multi-way classification of mutually exclusive semantic relations between pairs of common nominals. The task is designed to compare different approaches to the problem and to provide a standard testbed for future research, which can benefit many applications in Natural Language Processing."
W09-2416,{S}em{E}val-2010 Task 9: The Interpretation of Noun Compounds Using Paraphrasing Verbs and Prepositions,2009,22,39,5,1,45588,cristina butnariu,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"We present a brief overview of the main challenges in understanding the semantics of noun compounds and consider some known methods. We introduce a new task to be part of SemEval-2010: the interpretation of noun compounds using paraphrasing verbs and prepositions. The task is meant to provide a standard testbed for future research on noun compound semantics. It should also promote paraphrase-based approaches to the problem, which can benefit many NLP applications."
P08-1048,Evaluating {R}oget{`}s Thesauri,2008,24,22,2,0,47911,alistair kennedy,Proceedings of ACL-08: HLT,1,"Rogetxe2x80x99s Thesaurus has gone through many revisions since it was first published 150 years ago. But how do these revisions affect Rogetxe2x80x99s usefulness for NLP? We examine the differences in content between the 1911 and 1987 versions of Rogetxe2x80x99s, and we test both versions with each other and WordNet on problems such as synonym identification and word relatedness. We also present a novel method for measuring sentence relatedness that can be implemented in either version of Rogetxe2x80x99s or in WordNet. Although the 1987 version of the Thesaurus is better, we show that the 1911 version performs surprisingly well and that often the differences between the versions of Rogetxe2x80x99s and WordNet are not statistically significant. We hope that this work will encourage others to use the 1911 Rogetxe2x80x99s Thesaurus in NLP tasks."
hermet-etal-2008-using,Using the Web as a Linguistic Resource to Automatically Correct Lexico-Syntactic Errors,2008,4,28,3,0,46975,matthieu hermet,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents an algorithm for correcting language errors typical of second-language learners. We focus on preposition errors, which are very common among second-language learners but are not addressed well by current commercial grammar correctors and editing aids. The algorithm takes as input a sentence containing a preposition error (and possibly other errors as well), and outputs the correct preposition for that particular sentence context. We use a two-phase hybrid rule-based and statistical approach. In the first phase, rule-based processing is used to generate a short expression that captures the context of use of the preposition in the input sentence. In the second phase, Web searches are used to evaluate the frequency of this expression, when alternative prepositions are used instead of the original one. We tested this algorithm on a corpus of 133 French sentences written by intermediate second-language learners, and found that it could address 69.9{\%} of those cases. In contrast, we found that the best French grammar and spell checker currently on the market, Antidote, addressed only 3{\%} of those cases. We also showed that performance degrades gracefully when using a corpus of frequent n-grams to evaluate frequencies."
I08-1034,The Telling Tail: Signals of Success in Electronic Negotiation Texts,2008,15,5,3,0,27852,marina sokolova,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We analyze the linguistic behaviour of participants in bilateral electronic negotiations, and discover that particular language characteristics are in contrast with face-to-face negotiations. Language patterns in the later part of electronic negotiation are highly indicative of the successful or unsuccessful outcome of the process, whereas in face-toface negotiations, the first part of the negotiation is more useful for predicting the outcome. We formulate our problem in terms of text classification on negotiation segments of different sizes. The data are represented by a variety of linguistic features that capture the gist of the discussion: negotiationor strategy-related words. We show that, as we consider ever smaller final segments of a negotiation transcript, the negotiationrelated words become more indicative of the negotiation outcome, and give predictions with higher Accuracy than larger segments from the beginning of the process."
I08-1041,Using {R}oget{'}s Thesaurus for Fine-grained Emotion Recognition,2008,23,57,2,0,48672,saima aman,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Recognizing the emotive meaning of text can add another dimension to the understanding of text. We study the task of automatically categorizing sentences in a text into Ekmanxe2x80x99s six basic emotion categories. We experiment with corpus-based features as well as features derived from two emotion lexicons. One lexicon is automatically built using the classification system of Rogetxe2x80x99s Thesaurus, while the other consists of words extracted from WordNet-Affect. Experiments on the data obtained from blogs show that a combination of corpus-based unigram features with emotion-related features provides superior classification performance. We achieve Fmeasure values that outperform the rulebased baseline method for all emotion classes."
S07-1003,{S}em{E}val-2007 Task 04: Classification of Semantic Relations between Nominals,2007,14,148,4,0,28789,roxana girju,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text. We present an evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nominals in a sentence. This is part of SemEval, the 4th edition of the semantic evaluation event previously known as SensEval. We define the task, describe the training/test data and their creation, list the participating systems and discuss their results. There were 14 teams who submitted 15 systems."
W06-3805,A Study of Two Graph Algorithms in Topic-driven Summarization,2006,7,10,2,1,24179,vivi nastase,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"We study how two graph algorithms apply to topic-driven summarization in the scope of Document Understanding Conferences. The DUC 2005 and 2006 tasks were to summarize into 250 words a collection of documents on a topic consisting of a few statements or questions. Our algorithms select sentences for extraction. We measure their performance on the DUC 2005 test data, using the Summary Content Units made available after the challenge. One algorithm matches a graph representing the entire topic against each sentence in the collection. The other algorithm checks, for pairs of open-class words in the topic, whether they can be connected in the syntactic graph of each sentence. Matching performs better than connecting words, but a combination of both methods works best. They also both favour longer sentences, which makes summaries more fluent."
W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,2006,20,12,2,1,24179,vivi nastase,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"We present a graph-matching algorithm for semantic relation assignment. The algorithm is part of an interactive text analysis system. The system automatically extracts pairs of syntactic units from a text and assigns a semantic relation to each pair. This is an incremental learning algorithm, in which previously processed pairs and user feedback guide the process. After each assignment, the system adds to its database a syntactic-semantic graph centered on the main element of each pair of units. A graph consists of the main unit and all syntactic units with which it is syntactically connected. An edge contains information both about syntax and about semantic relations for use in further processing. Syntactic-semantic graph matching is used to produce a list of candidate assignments for 63.75% of the pairs analysed, and in 57% of situations the correct relations is one of the system's suggestions; in 19.6% of situations it suggests only the correct relation."
W06-0702,Challenges in Evaluating Summaries of Short Stories,2006,9,3,2,1,21616,anna kazantseva,Proceedings of the Workshop on Task-Focused Summarization and Question Answering,0,"This paper presents experiments with the evaluation of automatically produced summaries of literary short stories. The summaries are tailored to a particular purpose of helping a reader decide whether she wants to read the story. The evaluation procedure includes extrinsic and intrinsic measures, as well as subjective and factual judgments about the summaries pronounced by human subjects. The experiments confirm the experience of summarizing more conventional genres: sentence overlap between human- and machine-made summaries is not a complete picture of the quality of a summary. In fact, in our case, sentence overlap does not correlate well with human judgment. We explain the evaluation procedures and discuss several challenges of evaluating summaries of works of fiction."
W04-1005,Vocabulary Usage in Newswire Summaries,2004,-1,-1,2,0,51605,terry copeck,Text Summarization Branches Out,0,None
P98-1015,Semi-Automatic Recognition of Noun Modifier Relationships,1998,17,85,2,1,11995,ken barker,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Semantic relationships among words and phrases are often marked by explicit syntactic or lexical clues that help recognize such relationships in texts. Within complex nominals, however, few overt clues are available. Systems that analyze such nominals must compensate for the lack of surface clues with other information. One way is to load the system with lexical semantics for nouns or adjectives. This merely shifts the problem elsewhere: how do we define the lexical semantics and build large semantic lexicons? Another way is to find constructions similar to a given complex nominal, for which the relationships are already known. This is the way we chose, but it too has drawbacks. Similarity is not easily assessed, similar analyzed constructions may not exist, and if they do exist, their analysis may not be appropriate for the current nominal.We present a semi-automatic system that identifies semantic relationships in noun phrases without using precoded noun or adjective semantics. Instead, partial matching on previously analyzed noun phrases leads to a tentative interpretation of a new input. Processing can start without prior analyses, but the early stage requires user interaction. As more noun phrases are analyzed, the system learns to find better interpretations and reduces its reliance on the user. In experiments on English technical texts the system correctly identified 60--70% of relationships automatically."
C98-1015,Semi-Automatic Recognition of Noun Modifier Relationships,1998,17,85,2,1,11995,ken barker,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Semantic relationships among words and phrases are often marked by explicit syntactic or lexical clues that help recognize such relationships in texts. Within complex nominals, however, few overt clues are available. Systems that analyze such nominals must compensate for the lack of surface clues with other information. One way is to load the system with lexical semantics for nouns or adjectives. This merely shifts the problem elsewhere: how do we define the lexical semantics and build large semantic lexicons? Another way is to find constructions similar to a given complex nominal, for which the relationships are already known. This is the way we chose, but it too has drawbacks. Similarity is not easily assessed, similar analyzed constructions may not exist, and if they do exist, their analysis may not be appropriate for the current nominal.We present a semi-automatic system that identifies semantic relationships in noun phrases without using precoded noun or adjective semantics. Instead, partial matching on previously analyzed noun phrases leads to a tentative interpretation of a new input. Processing can start without prior analyses, but the early stage requires user interaction. As more noun phrases are analyzed, the system learns to find better interpretations and reduces its reliance on the user. In experiments on English technical texts the system correctly identified 60--70% of relationships automatically."
J96-1006,Book Reviews: Natural Language Processing for {P}rolog Programmers,1996,-1,-1,2,1,11995,ken barker,Computational Linguistics,0,None
C92-3156,Parsing and Case Analysis in {TANKA},1992,22,7,3,0,51605,terry copeck,{COLING} 1992 Volume 3: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The TANKA project seeks to build a model of a technical domain by semi-automatically processing unedited English text that describes this domain. Each sentence is parsed and conceptual elements are extracted from the parse. Concepts are derived from the Case structure of a sentence, and added to a conceptual network that represents knowledge about the domain. The DIPETT parser has a particularly broad coverage of English syntax; its newest version can also process sentence fragments. The HAIKU subsystem is responsible for user-assisted semantic interpretation. It contains a Case Analyzer module that extracts phrases marking concepts from the parse and uses its past processing experience to derive the most likely Case realizations of each with almost no a priori semantic knowledge. The user must validate these selections. A key issue in our research is minimizing the number of interactions with the user by intelligently generating the alternatives offered."
