2020.aacl-srw.17,P17-2054,0,0.0231503,"shown promising results in many NLP tasks. The primary purpose is to leverage useful information in multiple related tasks to improve all the tasks’ generalization performance (Zhang and Yang, 2017). The objectives are applied together, such as predicting the probability of the sequence and the probability that the sequence contains names (Cheng et al., 2015), the frequency of the next word with part-of-speech (POS) (Plank et al., 2016), surrounding words with other several tasks (Rei, 2017), error detection with additional linguistic information (Rei and Yannakoudakis, 2017). More advanced, Augenstein and Søgaard (2017) explored MTL for classifying keyphrase boundaries incorporating semantic super-sense tagging and identifying multi-word expression. Sanh et al. (2018) introduced a hierarchical model supervising a set of low-level tasks at the bottom layers and more complex tasks at the model’s top layers. There are merely a few existing works that utilize multi-task learning in AES (Cummins et al., 2016; Cummins and Rei, 2018). In this paper, we propose a method to incorporate sentiment analysis and AES. The proposed method utilizes the sentiment aspect for improving an essay scoring system. The sentiment in"
2020.aacl-srw.17,N10-1099,0,0.0812106,"Missing"
2020.aacl-srw.17,D15-1085,0,0.0459817,"Missing"
2020.aacl-srw.17,W14-4012,0,0.117579,"Missing"
2020.aacl-srw.17,P16-1075,0,0.0244395,"part-of-speech (POS) (Plank et al., 2016), surrounding words with other several tasks (Rei, 2017), error detection with additional linguistic information (Rei and Yannakoudakis, 2017). More advanced, Augenstein and Søgaard (2017) explored MTL for classifying keyphrase boundaries incorporating semantic super-sense tagging and identifying multi-word expression. Sanh et al. (2018) introduced a hierarchical model supervising a set of low-level tasks at the bottom layers and more complex tasks at the model’s top layers. There are merely a few existing works that utilize multi-task learning in AES (Cummins et al., 2016; Cummins and Rei, 2018). In this paper, we propose a method to incorporate sentiment analysis and AES. The proposed method utilizes the sentiment aspect for improving an essay scoring system. The sentiment information by both word-based and sentence-based, shown in Figure 1, is applied to enhance textual representations for sentiment perception of the model. To the best of our knowledge, this is the first approach on multi-task learning incorporating sentiment analysis and AES. Our proposed system is based on a hierarchical structure model to learn the features and relations between an essay"
2020.aacl-srw.17,N19-1423,0,0.0454044,"Missing"
2020.aacl-srw.17,D16-1115,0,0.14948,"al network models based on deep learning techniques have been proposed for AES. These approaches involve the use of both recurrent neural networks, e.g., a basic recurrent unit (RNN) (Elman, 1990), gated recurrent unit (GRU) (Cho et al., 2014), or long short-term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997), and convolutional neural networks (Lecun et al., 1998; Kim, 2014). More specifically, Taghipour and Ng (2016) proposed a convolutional recurrent neural network over the word sequence to construct a document representation. Dong et al. employed hierarchical CNN and LSTM structure (Dong and Zhang, 2016; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions in their essays create overall essay structure and quality, especially in persuasive and controversial articles (Pang and Lee, 2008). Sentiment analysis has typically been designed for use with specific domains, such as movie reviews ("
2020.aacl-srw.17,K17-1017,0,0.17889,"d on deep learning techniques have been proposed for AES. These approaches involve the use of both recurrent neural networks, e.g., a basic recurrent unit (RNN) (Elman, 1990), gated recurrent unit (GRU) (Cho et al., 2014), or long short-term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997), and convolutional neural networks (Lecun et al., 1998; Kim, 2014). More specifically, Taghipour and Ng (2016) proposed a convolutional recurrent neural network over the word sequence to construct a document representation. Dong et al. employed hierarchical CNN and LSTM structure (Dong and Zhang, 2016; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions in their essays create overall essay structure and quality, especially in persuasive and controversial articles (Pang and Lee, 2008). Sentiment analysis has typically been designed for use with specific domains, such as movie reviews (Thongtan and Phienth"
2020.aacl-srw.17,P19-1060,0,0.0190413,"ions for sentiment perception of the model. To the best of our knowledge, this is the first approach on multi-task learning incorporating sentiment analysis and AES. Our proposed system is based on a hierarchical structure model to learn the features and relations between an essay score and its sentiments. The model is trained to predict a holistic score at the top-level (document-level) along with sentence and word sentiments at the lower levels, i.e., sentence-level and word-level, respectively. 2 Multi-Task Learning We employ a hierarchical multi-task learning model similar to the model of Farag and Yannakoudakis (2019), shown in Figure 2. The model considers an essay d composed of a sequence of sentences d = {s1 , s2 , ..., sm }, and each sentence si consists of a sequence of words si = {w1 , w2 , ..., wn }. We describe each layer in our framework in detail. 2.1 Sentence Representation Firstly, we consider the left-hand side of the framework in Figure 2. This part aims to learn the context representation of a sentence by taking a word sequence as an input. A word embedding lookup table maps the words in the vocabulary into low dimensional vectors, xi = Ewi , (i = 1, 2, ..., n), (1) where E ∈ R|V |×D be the"
2020.aacl-srw.17,W15-0608,0,0.0446842,"analysis has typically been designed for use with specific domains, such as movie reviews (Thongtan and Phienthrakul, 2019), product reviews (Shrestha and Nasoz, 2019), social media (Song et al., 2019), and news (Godbole et al., 2007). Beigman Klebanov et al. (2012) are the first who attempted to incorporate sentiments with essay data. It involves the use of subjective lexicons to recognize the polarity of a sentence. Another notable work (Klebanov et al., 2013) found a way to measure the compositionality of multi-word expression’s sentiment profile (relative degree of polarities) in essays. Farra et al. (2015) built essay scoring systems that incorporate persuasiveness based on the analysis of opinions 116 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 116–123 c December 4 - 7, 2020. 2020 Association for Computational Linguistics score Sentence representation si d Attention pooling Word sentiment prediction Essay scoring Essay representations Attention pooling Sentence sentiment prediction stm1w stm2w stm2w stm1s stm2s stmms Wor"
2020.aacl-srw.17,D14-1181,0,0.00623381,"oduction Automatic essay scoring (AES) is the task of grading student essays, using natural language processing to assess quality. The system is designed to reduce time and cost from the human graders’ workload. Recently, neural network models based on deep learning techniques have been proposed for AES. These approaches involve the use of both recurrent neural networks, e.g., a basic recurrent unit (RNN) (Elman, 1990), gated recurrent unit (GRU) (Cho et al., 2014), or long short-term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997), and convolutional neural networks (Lecun et al., 1998; Kim, 2014). More specifically, Taghipour and Ng (2016) proposed a convolutional recurrent neural network over the word sequence to construct a document representation. Dong et al. employed hierarchical CNN and LSTM structure (Dong and Zhang, 2016; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions"
2020.aacl-srw.17,D15-1166,0,0.0331496,"o predict the score of an essay. It is predicted by applying a fully connected layer to an essay representation d. Then we bound the score in the range [0, 1] with a sigmoid function, yˆ = σ(W d d), (6) where W d is a learnable weight matrix of a fully connected layer, and yˆ is a predicted score. Since it is a regression task, we use mean square error (MSE) as a loss function, (3) i Lsc = where Wuw and Waw refer to learnable parameters, w uw i and ai are the attention vector and the attention weight of the i-th word in the sentence sj , respectively. The attention mechanism (Xu et al., 2015; Luong et al., 2015) emphasizes the salient words to build better sentence representation sj . 2.2 (7) i=1 where N is the total number of training data, yi is the ground-truth score, and yˆi is a predicted score obtained by the model. Sentiment Prediction Another objective of the model is to predict the sentiments of the words and sentences. To obtain such a probability distribution over word sentiment classes, we use a fully connected layer normalized by a softmax function over the hidden states of word-level Bi-LSTM w w {hw 1 ,h2 ,...,hn }, Essay Representation An essay representation is constructed similarly t"
2020.aacl-srw.17,D18-1464,0,0.0149302,"1997), and convolutional neural networks (Lecun et al., 1998; Kim, 2014). More specifically, Taghipour and Ng (2016) proposed a convolutional recurrent neural network over the word sequence to construct a document representation. Dong et al. employed hierarchical CNN and LSTM structure (Dong and Zhang, 2016; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions in their essays create overall essay structure and quality, especially in persuasive and controversial articles (Pang and Lee, 2008). Sentiment analysis has typically been designed for use with specific domains, such as movie reviews (Thongtan and Phienthrakul, 2019), product reviews (Shrestha and Nasoz, 2019), social media (Song et al., 2019), and news (Godbole et al., 2007). Beigman Klebanov et al. (2012) are the first who attempted to incorporate sentiments with essay data. It involves the use of subjective lexicons to recognize the polarity"
2020.aacl-srw.17,D14-1162,0,0.0835201,"and Ng, 2016) with long shortterm memory units (LSTM). LSTM units make use of three gates to forget or pass the information through time. They showed that using a mean-overtime layer is much more effective than using the last state vector or attention mechanism. Attention-based RCNN (Dong et al., 2017) is similar to hierarchical CNN. Instead, the convolutional layer is replaced by an LSTM layer at the sentence-level to learn global coherence. Above the CNN layer and LSTM layer, an attention pooling 3.5 Implementation Setup In the embedding layer, we used the pre-trained word embedding GloVe3 (Pennington et al., 2014) trained on 6 billion words from Wikipedia 2014 and Gigaword 5. During the training process, word embeddings are fine-tuned. The vocabulary was set to the 4,000 most frequent words by following Taghipour and Ng (2016) and treating other words as unknown words. We set the number of the essay sentences to the maximum for each essay prompts and the maximum sentence length to 128 and trained the models on batch size 16 for 50 epochs. The following hyperparameters were tuned by using optuna4 in 100 trials. 120 • Embedding dimension: {50, 100, 200, 300} • LSTM dimension: {50, 100, 200, 300} • Optimi"
2020.aacl-srw.17,P16-2067,0,0.0199746,"these works are based on feature engineering, which must be carefully handcrafted and selected to fit the appropriate model. Recently, Multi-Task Learning (MTL) approach has been shown promising results in many NLP tasks. The primary purpose is to leverage useful information in multiple related tasks to improve all the tasks’ generalization performance (Zhang and Yang, 2017). The objectives are applied together, such as predicting the probability of the sequence and the probability that the sequence contains names (Cheng et al., 2015), the frequency of the next word with part-of-speech (POS) (Plank et al., 2016), surrounding words with other several tasks (Rei, 2017), error detection with additional linguistic information (Rei and Yannakoudakis, 2017). More advanced, Augenstein and Søgaard (2017) explored MTL for classifying keyphrase boundaries incorporating semantic super-sense tagging and identifying multi-word expression. Sanh et al. (2018) introduced a hierarchical model supervising a set of low-level tasks at the bottom layers and more complex tasks at the model’s top layers. There are merely a few existing works that utilize multi-task learning in AES (Cummins et al., 2016; Cummins and Rei, 20"
2020.aacl-srw.17,P17-1194,0,0.021168,"ully handcrafted and selected to fit the appropriate model. Recently, Multi-Task Learning (MTL) approach has been shown promising results in many NLP tasks. The primary purpose is to leverage useful information in multiple related tasks to improve all the tasks’ generalization performance (Zhang and Yang, 2017). The objectives are applied together, such as predicting the probability of the sequence and the probability that the sequence contains names (Cheng et al., 2015), the frequency of the next word with part-of-speech (POS) (Plank et al., 2016), surrounding words with other several tasks (Rei, 2017), error detection with additional linguistic information (Rei and Yannakoudakis, 2017). More advanced, Augenstein and Søgaard (2017) explored MTL for classifying keyphrase boundaries incorporating semantic super-sense tagging and identifying multi-word expression. Sanh et al. (2018) introduced a hierarchical model supervising a set of low-level tasks at the bottom layers and more complex tasks at the model’s top layers. There are merely a few existing works that utilize multi-task learning in AES (Cummins et al., 2016; Cummins and Rei, 2018). In this paper, we propose a method to incorporate s"
2020.aacl-srw.17,W17-5004,0,0.0187666,", Multi-Task Learning (MTL) approach has been shown promising results in many NLP tasks. The primary purpose is to leverage useful information in multiple related tasks to improve all the tasks’ generalization performance (Zhang and Yang, 2017). The objectives are applied together, such as predicting the probability of the sequence and the probability that the sequence contains names (Cheng et al., 2015), the frequency of the next word with part-of-speech (POS) (Plank et al., 2016), surrounding words with other several tasks (Rei, 2017), error detection with additional linguistic information (Rei and Yannakoudakis, 2017). More advanced, Augenstein and Søgaard (2017) explored MTL for classifying keyphrase boundaries incorporating semantic super-sense tagging and identifying multi-word expression. Sanh et al. (2018) introduced a hierarchical model supervising a set of low-level tasks at the bottom layers and more complex tasks at the model’s top layers. There are merely a few existing works that utilize multi-task learning in AES (Cummins et al., 2016; Cummins and Rei, 2018). In this paper, we propose a method to incorporate sentiment analysis and AES. The proposed method utilizes the sentiment aspect for impro"
2020.aacl-srw.17,D13-1170,0,0.00857062,"] are hyperparameters. 3 Experiments 3.1 Dataset In our experiments, we used the Automated Student Assessment Prize (ASAP)1 public dataset on Kaggle to evaluate our methods. The dataset contains eight different prompts of the essay, as described in Table 1. The prompts elicit responses of different genres and of different lengths. The essays were written by students ranging from grade 7 to grade 10 and graded by at least two human graders. 1 Sentiment annotation We tokenize an essay into sentences and extract its sentiments using the Stanford CoreNLP2 based on Recursive Neural Tensor Network (Socher et al., 2013). It first split an essay into sentences, then annotate each sentence and the words in it with sentiment labels, as shown in Figure 1. The extracted sentiments are represented within five sentiment classes, i.e., very negative, negative, neutral, positive, and very positive. The model was trained on the Standford Sentiment Treebank dataset extracted from movie reviews. 3.3 Evaluation Metric The Quadratic Weighted Kappa (QWK) is used as the evaluation metric, which measures correlation or agreement between two raters (Yannakoudakis and Cummins, 2015), since it is the official evaluation metric"
2020.aacl-srw.17,P17-1011,0,0.0154573,"erm memory unit (LSTM) (Hochreiter and Schmidhuber, 1997), and convolutional neural networks (Lecun et al., 1998; Kim, 2014). More specifically, Taghipour and Ng (2016) proposed a convolutional recurrent neural network over the word sequence to construct a document representation. Dong et al. employed hierarchical CNN and LSTM structure (Dong and Zhang, 2016; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions in their essays create overall essay structure and quality, especially in persuasive and controversial articles (Pang and Lee, 2008). Sentiment analysis has typically been designed for use with specific domains, such as movie reviews (Thongtan and Phienthrakul, 2019), product reviews (Shrestha and Nasoz, 2019), social media (Song et al., 2019), and news (Godbole et al., 2007). Beigman Klebanov et al. (2012) are the first who attempted to incorporate sentiments with essay data. It involves"
2020.aacl-srw.17,D16-1193,0,0.220762,"g (AES) is the task of grading student essays, using natural language processing to assess quality. The system is designed to reduce time and cost from the human graders’ workload. Recently, neural network models based on deep learning techniques have been proposed for AES. These approaches involve the use of both recurrent neural networks, e.g., a basic recurrent unit (RNN) (Elman, 1990), gated recurrent unit (GRU) (Cho et al., 2014), or long short-term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997), and convolutional neural networks (Lecun et al., 1998; Kim, 2014). More specifically, Taghipour and Ng (2016) proposed a convolutional recurrent neural network over the word sequence to construct a document representation. Dong et al. employed hierarchical CNN and LSTM structure (Dong and Zhang, 2016; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions in their essays create overall essay struct"
2020.aacl-srw.17,P19-2057,0,0.0217132,"; Dong et al., 2017) to construct sentences and document representation separately. Similarly, several text properties are utilized for scoring an essay, such as grammatical roles (i.e., subject, object) (Burstein et al., 2010), discourse (Song et al., 2017), or coherence (Tay et al., 2017; Mesgar and Strube, 2018). The divergent and polarizing writers’ opinions in their essays create overall essay structure and quality, especially in persuasive and controversial articles (Pang and Lee, 2008). Sentiment analysis has typically been designed for use with specific domains, such as movie reviews (Thongtan and Phienthrakul, 2019), product reviews (Shrestha and Nasoz, 2019), social media (Song et al., 2019), and news (Godbole et al., 2007). Beigman Klebanov et al. (2012) are the first who attempted to incorporate sentiments with essay data. It involves the use of subjective lexicons to recognize the polarity of a sentence. Another notable work (Klebanov et al., 2013) found a way to measure the compositionality of multi-word expression’s sentiment profile (relative degree of polarities) in essays. Farra et al. (2015) built essay scoring systems that incorporate persuasiveness based on the analysis of opinions 116 Procee"
2020.aacl-srw.17,W15-0625,0,0.0230236,"nford CoreNLP2 based on Recursive Neural Tensor Network (Socher et al., 2013). It first split an essay into sentences, then annotate each sentence and the words in it with sentiment labels, as shown in Figure 1. The extracted sentiments are represented within five sentiment classes, i.e., very negative, negative, neutral, positive, and very positive. The model was trained on the Standford Sentiment Treebank dataset extracted from movie reviews. 3.3 Evaluation Metric The Quadratic Weighted Kappa (QWK) is used as the evaluation metric, which measures correlation or agreement between two raters (Yannakoudakis and Cummins, 2015), since it is the official evaluation metric of the ASAP competition. The QWK score ranges from 0 to 1. It can also become negative if there is less agreement than expected by chance. Therefore, the higher the value of QWK, the better the results. It is calculated using P i,j wi,j Oi,j K =1− P , (13) i,j wi,j Ei,j where matrices w, O, and E are the matrices of weights, observed scores, and expected scores, respectively. A value of Oi,j denotes the number of essays that receive a score i by the first-rater and a score j by the second-rater. The weights are 2 https://www.kaggle.com/c/asap-aes 11"
2020.coling-main.194,J08-1001,0,0.0872861,"ains it together with the text clarity scoring model. The experimental results by using the PeerRead benchmark dataset show the improvement compared with a single model, scoring text clarity model1 . 1 Introduction Text clarity scoring can be defined as a task that assesses well-structured in a text by grading. It is beneficial for not only authors/reviewers of scholarly papers but also students to improve their writing skills. Among several properties such as spelling, grammar, and word choice, local coherence, which captures text relatedness at the level of sentence-to-sentence transitions (Barzilay and Lapata, 2008), is one of the main properties to identify whether a text is well-structured or not. Well-known early attempts for modeling coherence are lexical coherence models (Halliday and Hasan, 1976), psychological models of discourse (Foltz et al., 1998), rhetorical structure theory (Mann and Thompson, 1998), lexical chains (Morris and Hirst, 1991), and entity grid model (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017),"
2020.coling-main.194,D14-1082,0,0.023898,"y, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence within a sentence. Farag and Yannakoudakis (2019) attempted to encode information about the types of grammatical roles, such as clausal modifiers of nouns and coordinating conjunction in a sentence obtained by using the Stanford Dependency Parser (Chen and Manning, 2014). for coherence modeling. The authors utilize a hierarchical neural network model trained two tasks, grammatical roles, and coherence in a multi-task manner. In this paper, we propose a method for text clarity scoring by utilizing a coherence model as an auxiliary manner. Instead of training grammatical roles and coherence tasks with the same data, our method explicitly trains the coherence model by using an existing coherence relation training data and utilizes it for scoring text clarity on the target of scholarly papers. Several ideas were proposed for 1 Our source code is available at http"
2020.coling-main.194,N19-1423,0,0.139058,"2. (d) Its destination is Z¨urich. Figure 1: Coherence relation example from Wolf et al. (2003). Sentence (b) elaborates on sentence (a), sentence (a) is parallel with sentence (c). Furthermore, sentence (b) is in contrast with sentence (d). categorizing coherence relations (Hobbs, 1979; Hovy, 1991; Sanders et al., 1992; Knott and Dale, 1994). We hypothesize that coherence relation knowledge learned from out-domain data is also possible to help to discriminate well-structured of the target text and thus help to score the text clarity. The method based on the language model pre-training BERT (Devlin et al., 2019) firstly trains the local coherence model by using BERT sentential encodings as an auxiliary manner and then re-trains it together with the text clarity scoring model, adapted from the delay multi-task approach (Shimura et al., 2019). In such a way, the coherence relation can also help the model to learn well-structured of a text. The main contributions of our work can be summarized: (1) We propose a method for scoring text clarity based on local coherence relation between two sentences/segments, (2) We introduce a learning framework that firstly trains the local coherence model as an auxiliar"
2020.coling-main.194,P19-1060,0,0.0137564,"del (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence within a sentence. Farag and Yannakoudakis (2019) attempted to encode information about the types of grammatical roles, such as clausal modifiers of nouns and coordinating conjunction in a sentence obtained by using the Stanford Dependency Parser (Chen and Manning, 2014). for coherence modeling. The authors utilize a hierarchical neural network model trained two tasks, grammatical roles, and coherence in a multi-task manner. In this paper, we propose a method for text clarity scoring by utilizing a coherence model as an auxiliary manner. Instead of training grammatical roles and coherence tasks with the same data, our method explicitly train"
2020.coling-main.194,N18-1149,0,0.0446563,"Missing"
2020.coling-main.194,D14-1218,0,0.0283201,"ce transitions (Barzilay and Lapata, 2008), is one of the main properties to identify whether a text is well-structured or not. Well-known early attempts for modeling coherence are lexical coherence models (Halliday and Hasan, 1976), psychological models of discourse (Foltz et al., 1998), rhetorical structure theory (Mann and Thompson, 1998), lexical chains (Morris and Hirst, 1991), and entity grid model (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence within a sentence. Farag and Yannakoudakis (2019) attempted to encode information about the types of grammatical roles, such as clausal modifiers of nouns and coordinating conjunction in a sentence obtained by using the Stanford Dep"
2020.coling-main.194,D17-1019,0,0.0183613,"zilay and Lapata, 2008), is one of the main properties to identify whether a text is well-structured or not. Well-known early attempts for modeling coherence are lexical coherence models (Halliday and Hasan, 1976), psychological models of discourse (Foltz et al., 1998), rhetorical structure theory (Mann and Thompson, 1998), lexical chains (Morris and Hirst, 1991), and entity grid model (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence within a sentence. Farag and Yannakoudakis (2019) attempted to encode information about the types of grammatical roles, such as clausal modifiers of nouns and coordinating conjunction in a sentence obtained by using the Stanford Dependency Parser (Chen and"
2020.coling-main.194,D18-1464,0,0.0184058,"to identify whether a text is well-structured or not. Well-known early attempts for modeling coherence are lexical coherence models (Halliday and Hasan, 1976), psychological models of discourse (Foltz et al., 1998), rhetorical structure theory (Mann and Thompson, 1998), lexical chains (Morris and Hirst, 1991), and entity grid model (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence within a sentence. Farag and Yannakoudakis (2019) attempted to encode information about the types of grammatical roles, such as clausal modifiers of nouns and coordinating conjunction in a sentence obtained by using the Stanford Dependency Parser (Chen and Manning, 2014). for coherence modeling. The authors util"
2020.coling-main.194,J91-1002,0,0.654982,"thors/reviewers of scholarly papers but also students to improve their writing skills. Among several properties such as spelling, grammar, and word choice, local coherence, which captures text relatedness at the level of sentence-to-sentence transitions (Barzilay and Lapata, 2008), is one of the main properties to identify whether a text is well-structured or not. Well-known early attempts for modeling coherence are lexical coherence models (Halliday and Hasan, 1976), psychological models of discourse (Foltz et al., 1998), rhetorical structure theory (Mann and Thompson, 1998), lexical chains (Morris and Hirst, 1991), and entity grid model (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence"
2020.coling-main.194,P19-1105,1,0.72003,"tence (d). categorizing coherence relations (Hobbs, 1979; Hovy, 1991; Sanders et al., 1992; Knott and Dale, 1994). We hypothesize that coherence relation knowledge learned from out-domain data is also possible to help to discriminate well-structured of the target text and thus help to score the text clarity. The method based on the language model pre-training BERT (Devlin et al., 2019) firstly trains the local coherence model by using BERT sentential encodings as an auxiliary manner and then re-trains it together with the text clarity scoring model, adapted from the delay multi-task approach (Shimura et al., 2019). In such a way, the coherence relation can also help the model to learn well-structured of a text. The main contributions of our work can be summarized: (1) We propose a method for scoring text clarity based on local coherence relation between two sentences/segments, (2) We introduce a learning framework that firstly trains the local coherence model as an auxiliary manner and then re-trains it together with the text clarity scoring model, (3) We show that the coherence relations learned from out-domain data are also possible to help for capturing the well-structured target text and thus benef"
2020.coling-main.194,J05-2005,0,0.130361,"etween two sentences/segments, (2) We introduce a learning framework that firstly trains the local coherence model as an auxiliary manner and then re-trains it together with the text clarity scoring model, (3) We show that the coherence relations learned from out-domain data are also possible to help for capturing the well-structured target text and thus beneficial for scoring the text clarity. 2 Neural Clarity Learning 2.1 Learning Local Coherence with Sentential Encoders To learn a coherence model, we utilize an existing coherence relation training data provided by Discourse Graphbank 1.02 (Wolf and Gibson, 2005). We used eleven coherence relations such as elaboration and parallel between adjacent sentences as ”has coherence relation” and none relation between them as ”does not have coherence relation”. An example passage with coherence relations is shown in Figure 1. Figure 2 illustrates our neural clarity learning framework. The left-hand side of the framework shows a clarity score prediction model, and the right-hand side indicates the local coherence model. The contextualized sentence representation that we used is BERT, a Bidirectional Transformer model (Devlin et al., 2019). A transformer encode"
2020.coling-main.194,P19-1067,0,0.0185388,"1976), psychological models of discourse (Foltz et al., 1998), rhetorical structure theory (Mann and Thompson, 1998), lexical chains (Morris and Hirst, 1991), and entity grid model (Barzilay and Lapata, 2008). More recently, the coherence model based on deep learning techniques has been intensively studied. These attempts include recursive and recurrent neural networks (Li and Hovy, 2014; Li and Jurafsky, 2017), a combination of LSTM and CNN (Mesgar and Strube, 2018), a deep coherence model based on CNN (Cui et al., 2017), SKIPFLOW LSTM (Tay et al., 2018), and a pre-trained generative model (Xu et al., 2019). It enables to encode patterns of semantic changes in a text. Despite some successes, techniques explored so far mainly rely on word sequence within a sentence. Farag and Yannakoudakis (2019) attempted to encode information about the types of grammatical roles, such as clausal modifiers of nouns and coordinating conjunction in a sentence obtained by using the Stanford Dependency Parser (Chen and Manning, 2014). for coherence modeling. The authors utilize a hierarchical neural network model trained two tasks, grammatical roles, and coherence in a multi-task manner. In this paper, we propose a"
2020.emnlp-main.545,D18-1514,0,0.0162629,"ross-entropy loss (Kim, 2014). Although the imbalanced loss objectives are better than the vanilla one, their performances remain limited in the cases of extremely imbalanced data because they are not designed for it, i.e., tail (head) categories have extremely small (large) numbers of instances. On the one hand, the recent few-shot learning techniques (e.g., optimization-based methods (Finn et al., 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018), metric-based methods (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017)) have become popular for various NLP tasks (Yu et al., 2018; Han et al., 2018). They have already shown the capability for few-shot classifications. They thus may also perform well for tail category classification (some of the tail categories are few-shot, all have relatively small numbers of instances). On the other hand, for the head categories with many instances, general approaches such as the single CNN model (Kim, 2014; Liu et al., 2017) may be more effective in terms of performance and more efficient in terms of complexity. Therefore, our basic idea for tackling the problem of extremely imbalanced multi-label text classification is a hybrid solution that adapts a"
2020.emnlp-main.545,E17-2068,0,0.0708289,"Missing"
2020.emnlp-main.545,P19-1129,0,0.0360785,"Missing"
2020.emnlp-main.545,D14-1181,0,0.0172221,"the tail or entire categories. 1 Introduction The data imbalance problem is a crucial issue for the multi-label text classification. In many corpora for the classification tasks, the number of instances of a category follows the long tail distribution, where many tail categories has only a small number of instances. To handle this problem, some works sample hard examples for training (Shrivastava et al., 2016); some works address the problem by proposing imbalance loss objectives, e.g., weighted cross-entropy loss and Focal loss (Lin et al., 2017), in place of the vanilla cross-entropy loss (Kim, 2014). Although the imbalanced loss objectives are better than the vanilla one, their performances remain limited in the cases of extremely imbalanced data because they are not designed for it, i.e., tail (head) categories have extremely small (large) numbers of instances. On the one hand, the recent few-shot learning techniques (e.g., optimization-based methods (Finn et al., 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018), metric-based methods (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017)) have become popular for various NLP tasks (Yu et al., 2018; Han et al., 2018). They have"
2020.emnlp-main.545,D18-1093,1,0.885874,"Missing"
2020.emnlp-main.545,2020.acl-main.519,0,0.16734,"d one because it is one of the typical models. We have several alternatives on the loss objectives computed by the predicted categories and true categories. Table 1 lists them. For each instance di , yc1 = 1 if di has the category c, yc0 = 1 − yc1 ; pc1 is the predicted probability that di has category c, pc0 = 1−pc1 . We use both the vanilla Binary Cross Entropy (BCE) and the imbalanced loss objectives including Weighted binary Cross Entropy (WCE) and Focal loss (Lin et al., 2017). We empirically set αc = log ((N − Nc )/Nc ) for WCE loss and γ = 1 for Focal loss following the existing works (Li et al., 2020b). We do not use the Dice loss (Li et al., 2020b) because we empirically observe that it does not perform well for the multi-label text Table 1: Loss Objectives Figure 1: HSCNN Model classification with a large number of categories, although it is also an imbalanced loss objective. Siamese network (Koch et al., 2015) is a typical technique of few-shot learning. It contains two duplicated Single networks, and the inputs are two instances. The output is computed by comparing the representations extracted after the first fully connected layer of the Single network (Linear1 in Figure 1) for the t"
2020.emnlp-main.545,2020.acl-main.45,0,0.317791,"d one because it is one of the typical models. We have several alternatives on the loss objectives computed by the predicted categories and true categories. Table 1 lists them. For each instance di , yc1 = 1 if di has the category c, yc0 = 1 − yc1 ; pc1 is the predicted probability that di has category c, pc0 = 1−pc1 . We use both the vanilla Binary Cross Entropy (BCE) and the imbalanced loss objectives including Weighted binary Cross Entropy (WCE) and Focal loss (Lin et al., 2017). We empirically set αc = log ((N − Nc )/Nc ) for WCE loss and γ = 1 for Focal loss following the existing works (Li et al., 2020b). We do not use the Dice loss (Li et al., 2020b) because we empirically observe that it does not perform well for the multi-label text Table 1: Loss Objectives Figure 1: HSCNN Model classification with a large number of categories, although it is also an imbalanced loss objective. Siamese network (Koch et al., 2015) is a typical technique of few-shot learning. It contains two duplicated Single networks, and the inputs are two instances. The output is computed by comparing the representations extracted after the first fully connected layer of the Single network (Linear1 in Figure 1) for the t"
2020.emnlp-main.545,P19-1105,1,0.894623,"Missing"
2020.emnlp-main.545,P18-1216,0,0.0223027,"”. Denoting the one-hot encoding of category c as qc , a category-specific difference is computed byp h = |xi − xj |◦ hc , where hc = σ((wqc + b)/ |C|). σ is the ReLU activation function and ◦ is the elementwise multiplication. A linear layer with the sigmoid function then computes the similarity. This category-specific similarity can also be explained as a Machine Reading Comprehension (MRC) framework (Li et al., 2020a, 2019), which can improve non-MRC tasks’ performance by learning additional information from the query. It is also related to the joint embedding on the instance and category (Wang et al., 2018), while ours focuses on the category-specific similarity of instances. Sampling method: A common sampling method of the training data for a Siamese network is randomly selecting similar ((di ,c),(dj ,c)) and dissimilar pairs ((di ,ci ),(dj ,cj )) with the ratio of 1:1. In this work, we generate one pair by randomly selecting the categories and selecting the instances in the categories. We set a heuristic rule to ensure that each category can be selected as least T (e.g., ten) times. To follow the asymmetric structure of HSCNN, we propose a specific sampling method for training HSCNN. For each"
2020.emnlp-main.545,N16-1174,0,0.0321467,"category as c. The number of training instances is N . The number of training instances of a category c is Nc . 2.1 Single and Siamese Architectures The Single architecture we use for multi-label text classification is similar to the CNN based models in existing works (Kim, 2014; Liu et al., 2017; Shimura et al., 2018). It includes an embedding layer, a convolutional layer and a pooling layer, and two fully connected layers. The black dashed line in Figure 1 marks a Single architecture. Note that this Single CNN network can also be replaced by other types of Single networks such as RNN, HAN (Yang et al., 2016), and so on. We utilize the CNN based one because it is one of the typical models. We have several alternatives on the loss objectives computed by the predicted categories and true categories. Table 1 lists them. For each instance di , yc1 = 1 if di has the category c, yc0 = 1 − yc1 ; pc1 is the predicted probability that di has category c, pc0 = 1−pc1 . We use both the vanilla Binary Cross Entropy (BCE) and the imbalanced loss objectives including Weighted binary Cross Entropy (WCE) and Focal loss (Lin et al., 2017). We empirically set αc = log ((N − Nc )/Nc ) for WCE loss and γ = 1 for Focal"
2020.emnlp-main.545,N18-1109,0,0.0187558,"of the vanilla cross-entropy loss (Kim, 2014). Although the imbalanced loss objectives are better than the vanilla one, their performances remain limited in the cases of extremely imbalanced data because they are not designed for it, i.e., tail (head) categories have extremely small (large) numbers of instances. On the one hand, the recent few-shot learning techniques (e.g., optimization-based methods (Finn et al., 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018), metric-based methods (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017)) have become popular for various NLP tasks (Yu et al., 2018; Han et al., 2018). They have already shown the capability for few-shot classifications. They thus may also perform well for tail category classification (some of the tail categories are few-shot, all have relatively small numbers of instances). On the other hand, for the head categories with many instances, general approaches such as the single CNN model (Kim, 2014; Liu et al., 2017) may be more effective in terms of performance and more efficient in terms of complexity. Therefore, our basic idea for tackling the problem of extremely imbalanced multi-label text classification is a hybrid sol"
2020.figlang-1.4,N16-1039,0,0.041523,"Missing"
2020.figlang-1.4,W18-0907,0,0.71764,"n paradigm which makes use of the advanced pretraining language model to encode global, local, and question information of the text as well as two types of POS auxiliary features. We also introduced a metaphor preference parameter in the cross-validation phase to improve the model performance. (3) The experimental results on several metaphor datasets show that our model is comparable to the state-of-the-art metaphor detection, especially we verified that fine-grained POS (FGPOS) features contribute to performance improvement in our model. 2 2.1 the best in the NAACL-2018 metaphor shared task (Leong et al., 2018) with an ensemble learning strategy. Gao et al. (2018) proposed a metaphor detection model using global vectors for word representation (GloVe) (Pennington et al., 2014) and deep contextualized word representations (ELMo) (Peters et al., 2018) as text representations. They applied BiLSTM as an encoder. The accuracy of their method surpasses Wu et al.’s method. Mao et al. (2019) presented two metaphor detection models inspired by the theory of metaphor linguistics (Metaphor Identification Procedure (MLP) (Steen et al., 2010) and Selectional Preference Violation (SPV) (Wilks, 1975)), with BiLSTM"
2020.figlang-1.4,E06-1042,0,0.653176,"red task on metaphor detection use VUA as the evaluation data set. There are two tracks, i.e., verbs and all POS metaphor detection. (2) TOEFI4 (Klebanov et al., 2018) is a subset of ETS corpus of non-native written English. It is also used as the evaluation data set in the second shared task on metaphor detection with two tracks, verbs and all POS metaphor detection. (3) MOH-X5 (Mohammad et al., 2016) is a verb metaphor detection database with the data from WordNet (Miller, 1998) example sentences. The average sentence length of MOH-X is the shortest among the four data sets. (4) The TroFi6 (Birke and Sarkar, 2006) is a verb metaphor detection dataset consisting of sentences from the 1987-89 Wall Street Journal Corpus Release 1 (Charniak et al., 2000). The average sentence length of TroFi is the longest among the four data sets. We first sampled the four data sets into four new (S, qi , yj ) triple data sets following the requirements of the reading comprehension paradigm. In this paper, we focus on the VUA and TOEFI as the evaluation data set. MOH-X and TOEFI are used as auxiliary data sets to verify the performance of our designed metaphor detector. We made exploratory data analysis on the data sets o"
2020.figlang-1.4,K17-1034,0,0.0357094,"Missing"
2020.figlang-1.4,2021.ccl-1.108,0,0.135775,"Missing"
2020.figlang-1.4,W15-1405,0,0.0498873,"NLP tasks. Related Work Metaphor Detection As a common language phenomenon, the metaphor was first studied by linguists and psycho-linguists (Wilks, 1975; Glucksberg, 2003; Group, 2007). Metaphor is related to the human cognitive process, and the essential mechanism of metaphor is the conceptual mapping from the source domain to the target domain (Lakoff and Johnson, 2003). Metaphor understanding involves high-level semantic analysis and thus requires special domain knowledge (Tsvetkov et al., 2014). There are three types of metaphor detection methods. One is a lexicon and rule-based methods (Dodge et al., 2015; Mohler et al., 2013), while these methods need manual creation of rules which is extremely costly. The second is a corpus-based statistical algorithm. It has been studied to construct manual features such as unigrams (Klebanov et al., 2014), bag-of-words features (K¨oper and im Walde, 2016), concreteness, abstractness (Turney et al., 2011; Tsvetkov et al., 2014), and sensory features (Shutova et al., 2016). The disadvantage of this method is that it cannot detect rare usages of metaphors as we can hardly deal with all these unexpected linguistic phenomena. The third is a metaphor detection a"
2020.figlang-1.4,P19-1378,0,0.777457,"gy for metaphor understanding. Its task is to give a text sequence and determine whether a token in the given text sequence is a metaphor or literal. The second shared task on metaphor detection1 aims to promote the development of metaphor detection technology. This task provides two data sets, VU Amsterdam Metaphor Corpus (VUA) (Steen, 2010) and TOEFI (a subset of ETS corpus of non-native written English) (Klebanov et al., 2018), each with two tasks. Each dataset has two tasks, i.e., verb metaphor detection and all POS metaphor detection. Previous research (Wu et al., 2018; Gao et al., 2018; Mao et al., 2019) has been limited to treat them as the text classification task or sequence tagging task without deeply investigating and leveraging the linguistic information that may be proper for the specific metaphor understanding task. Motivated by the previous work mentioned in the above, we propose an end-to-end neural based method named DeepMet for detecting metaphor by transforming the token-level metaphor detection task into the reading comprehension task. Our approach encodes the global text, local text and question information as well as incorporating the POS features on two granularity. To improv"
2020.figlang-1.4,D18-1060,0,0.845916,"the basic technology for metaphor understanding. Its task is to give a text sequence and determine whether a token in the given text sequence is a metaphor or literal. The second shared task on metaphor detection1 aims to promote the development of metaphor detection technology. This task provides two data sets, VU Amsterdam Metaphor Corpus (VUA) (Steen, 2010) and TOEFI (a subset of ETS corpus of non-native written English) (Klebanov et al., 2018), each with two tasks. Each dataset has two tasks, i.e., verb metaphor detection and all POS metaphor detection. Previous research (Wu et al., 2018; Gao et al., 2018; Mao et al., 2019) has been limited to treat them as the text classification task or sequence tagging task without deeply investigating and leveraging the linguistic information that may be proper for the specific metaphor understanding task. Motivated by the previous work mentioned in the above, we propose an end-to-end neural based method named DeepMet for detecting metaphor by transforming the token-level metaphor detection task into the reading comprehension task. Our approach encodes the global text, local text and question information as well as incorporating the POS features on two gra"
2020.figlang-1.4,W14-2302,0,0.259596,"and the essential mechanism of metaphor is the conceptual mapping from the source domain to the target domain (Lakoff and Johnson, 2003). Metaphor understanding involves high-level semantic analysis and thus requires special domain knowledge (Tsvetkov et al., 2014). There are three types of metaphor detection methods. One is a lexicon and rule-based methods (Dodge et al., 2015; Mohler et al., 2013), while these methods need manual creation of rules which is extremely costly. The second is a corpus-based statistical algorithm. It has been studied to construct manual features such as unigrams (Klebanov et al., 2014), bag-of-words features (K¨oper and im Walde, 2016), concreteness, abstractness (Turney et al., 2011; Tsvetkov et al., 2014), and sensory features (Shutova et al., 2016). The disadvantage of this method is that it cannot detect rare usages of metaphors as we can hardly deal with all these unexpected linguistic phenomena. The third is a metaphor detection algorithm based on deep learning. With a recent surge of interest in neural networks, metaphor detection based on deep learning techniques has been intensively studied. Wu et al. (2018) proposed a metaphor detection model based on Convolutiona"
2020.figlang-1.4,S16-2003,0,0.250459,"ctions L0 and L1 of the two subtasks (verb task and all POS task of metaphor detection), as shown in Formulas (7)–(9). L0 = L1 = − M X (ˆ y log yτ 0 + (1 − yˆ) log yτ 1 ) NAACL-2018 metaphor shared task and second shared task on metaphor detection use VUA as the evaluation data set. There are two tracks, i.e., verbs and all POS metaphor detection. (2) TOEFI4 (Klebanov et al., 2018) is a subset of ETS corpus of non-native written English. It is also used as the evaluation data set in the second shared task on metaphor detection with two tracks, verbs and all POS metaphor detection. (3) MOH-X5 (Mohammad et al., 2016) is a verb metaphor detection database with the data from WordNet (Miller, 1998) example sentences. The average sentence length of MOH-X is the shortest among the four data sets. (4) The TroFi6 (Birke and Sarkar, 2006) is a verb metaphor detection dataset consisting of sentences from the 1987-89 Wall Street Journal Corpus Release 1 (Charniak et al., 2000). The average sentence length of TroFi is the longest among the four data sets. We first sampled the four data sets into four new (S, qi , yj ) triple data sets following the requirements of the reading comprehension paradigm. In this paper, w"
2020.figlang-1.4,N18-2014,0,0.325888,", University of Yamanashi, Yamanashi 400-8510, Japan {suchuandong,huangxx,wangrongbo,chenzq}@hdu.edu.cn fukumoto@yamanashi.ac.jp garfieldpigljy@gmail.com Abstract tion is the basic technology for metaphor understanding. Its task is to give a text sequence and determine whether a token in the given text sequence is a metaphor or literal. The second shared task on metaphor detection1 aims to promote the development of metaphor detection technology. This task provides two data sets, VU Amsterdam Metaphor Corpus (VUA) (Steen, 2010) and TOEFI (a subset of ETS corpus of non-native written English) (Klebanov et al., 2018), each with two tasks. Each dataset has two tasks, i.e., verb metaphor detection and all POS metaphor detection. Previous research (Wu et al., 2018; Gao et al., 2018; Mao et al., 2019) has been limited to treat them as the text classification task or sequence tagging task without deeply investigating and leveraging the linguistic information that may be proper for the specific metaphor understanding task. Motivated by the previous work mentioned in the above, we propose an end-to-end neural based method named DeepMet for detecting metaphor by transforming the token-level metaphor detection tas"
2020.figlang-1.4,W13-0904,0,0.0787776,"ork Metaphor Detection As a common language phenomenon, the metaphor was first studied by linguists and psycho-linguists (Wilks, 1975; Glucksberg, 2003; Group, 2007). Metaphor is related to the human cognitive process, and the essential mechanism of metaphor is the conceptual mapping from the source domain to the target domain (Lakoff and Johnson, 2003). Metaphor understanding involves high-level semantic analysis and thus requires special domain knowledge (Tsvetkov et al., 2014). There are three types of metaphor detection methods. One is a lexicon and rule-based methods (Dodge et al., 2015; Mohler et al., 2013), while these methods need manual creation of rules which is extremely costly. The second is a corpus-based statistical algorithm. It has been studied to construct manual features such as unigrams (Klebanov et al., 2014), bag-of-words features (K¨oper and im Walde, 2016), concreteness, abstractness (Turney et al., 2011; Tsvetkov et al., 2014), and sensory features (Shutova et al., 2016). The disadvantage of this method is that it cannot detect rare usages of metaphors as we can hardly deal with all these unexpected linguistic phenomena. The third is a metaphor detection algorithm based on deep"
2020.figlang-1.4,D14-1162,0,0.0840659,"liary features. We also introduced a metaphor preference parameter in the cross-validation phase to improve the model performance. (3) The experimental results on several metaphor datasets show that our model is comparable to the state-of-the-art metaphor detection, especially we verified that fine-grained POS (FGPOS) features contribute to performance improvement in our model. 2 2.1 the best in the NAACL-2018 metaphor shared task (Leong et al., 2018) with an ensemble learning strategy. Gao et al. (2018) proposed a metaphor detection model using global vectors for word representation (GloVe) (Pennington et al., 2014) and deep contextualized word representations (ELMo) (Peters et al., 2018) as text representations. They applied BiLSTM as an encoder. The accuracy of their method surpasses Wu et al.’s method. Mao et al. (2019) presented two metaphor detection models inspired by the theory of metaphor linguistics (Metaphor Identification Procedure (MLP) (Steen et al., 2010) and Selectional Preference Violation (SPV) (Wilks, 1975)), with BiLSTM as the encoder and Glove and ELMo as the word embeddings. The method is currently SOTA on metaphor detection tasks. Despite some successes, approaches explored so far u"
2020.figlang-1.4,D11-1063,0,0.679706,"domain (Lakoff and Johnson, 2003). Metaphor understanding involves high-level semantic analysis and thus requires special domain knowledge (Tsvetkov et al., 2014). There are three types of metaphor detection methods. One is a lexicon and rule-based methods (Dodge et al., 2015; Mohler et al., 2013), while these methods need manual creation of rules which is extremely costly. The second is a corpus-based statistical algorithm. It has been studied to construct manual features such as unigrams (Klebanov et al., 2014), bag-of-words features (K¨oper and im Walde, 2016), concreteness, abstractness (Turney et al., 2011; Tsvetkov et al., 2014), and sensory features (Shutova et al., 2016). The disadvantage of this method is that it cannot detect rare usages of metaphors as we can hardly deal with all these unexpected linguistic phenomena. The third is a metaphor detection algorithm based on deep learning. With a recent surge of interest in neural networks, metaphor detection based on deep learning techniques has been intensively studied. Wu et al. (2018) proposed a metaphor detection model based on Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) (Graves and Schmidhuber, 20"
2020.figlang-1.4,N18-1202,0,0.0807828,"s-validation phase to improve the model performance. (3) The experimental results on several metaphor datasets show that our model is comparable to the state-of-the-art metaphor detection, especially we verified that fine-grained POS (FGPOS) features contribute to performance improvement in our model. 2 2.1 the best in the NAACL-2018 metaphor shared task (Leong et al., 2018) with an ensemble learning strategy. Gao et al. (2018) proposed a metaphor detection model using global vectors for word representation (GloVe) (Pennington et al., 2014) and deep contextualized word representations (ELMo) (Peters et al., 2018) as text representations. They applied BiLSTM as an encoder. The accuracy of their method surpasses Wu et al.’s method. Mao et al. (2019) presented two metaphor detection models inspired by the theory of metaphor linguistics (Metaphor Identification Procedure (MLP) (Steen et al., 2010) and Selectional Preference Violation (SPV) (Wilks, 1975)), with BiLSTM as the encoder and Glove and ELMo as the word embeddings. The method is currently SOTA on metaphor detection tasks. Despite some successes, approaches explored so far use classification or sequence labeling and the encoder is based on shallow"
2020.figlang-1.4,W18-0913,0,0.547195,"died to construct manual features such as unigrams (Klebanov et al., 2014), bag-of-words features (K¨oper and im Walde, 2016), concreteness, abstractness (Turney et al., 2011; Tsvetkov et al., 2014), and sensory features (Shutova et al., 2016). The disadvantage of this method is that it cannot detect rare usages of metaphors as we can hardly deal with all these unexpected linguistic phenomena. The third is a metaphor detection algorithm based on deep learning. With a recent surge of interest in neural networks, metaphor detection based on deep learning techniques has been intensively studied. Wu et al. (2018) proposed a metaphor detection model based on Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) (Graves and Schmidhuber, 2005). They utilized Word2Vec (Mikolov et al., 2013) as text representation, and POS and word clusters information for additional features. Their method performed 2.2 Reading Comprehension The reading comprehension in NLP assesses a machine’s understanding of NL by measuring its ability to answer questions based on a given text/document. The answer to this question may be either explicit or implicit in the text and needs to be inferred base"
2020.figlang-1.4,N16-1020,0,0.46818,"high-level semantic analysis and thus requires special domain knowledge (Tsvetkov et al., 2014). There are three types of metaphor detection methods. One is a lexicon and rule-based methods (Dodge et al., 2015; Mohler et al., 2013), while these methods need manual creation of rules which is extremely costly. The second is a corpus-based statistical algorithm. It has been studied to construct manual features such as unigrams (Klebanov et al., 2014), bag-of-words features (K¨oper and im Walde, 2016), concreteness, abstractness (Turney et al., 2011; Tsvetkov et al., 2014), and sensory features (Shutova et al., 2016). The disadvantage of this method is that it cannot detect rare usages of metaphors as we can hardly deal with all these unexpected linguistic phenomena. The third is a metaphor detection algorithm based on deep learning. With a recent surge of interest in neural networks, metaphor detection based on deep learning techniques has been intensively studied. Wu et al. (2018) proposed a metaphor detection model based on Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) (Graves and Schmidhuber, 2005). They utilized Word2Vec (Mikolov et al., 2013) as text representa"
2020.figlang-1.4,P14-1024,0,0.656677,"ntal results by using several metaphor datasets show that our model achieves competitive results in the second shared task on metaphor detection. 1 Introduction Metaphor is one of the figurative languages and often used to express our thoughts in daily conversations. It is deeply related to human cognitive processes (Lakoff and Johnson, 2003). Metaphor is used to implicitly refer one concept to another concept, usually triggered by a verb (Steen et al., 2010). For example, the verb “drink” in “car drinks gasoline” is a metaphorical usage. Other parts of speech can also be used metaphorically (Tsvetkov et al., 2014). For example, the noun “angel” in “she is an angel” and the adjective “bright” in “your idea is very bright” are also metaphorical uses. Metaphor computation technologies are helpful for most NLP tasks such as machine translation, dialogue systems, content analysis, and machine reading comprehension. Of these, token-level metaphor detec1 2 https://competitions.codalab.org/competitions/22188 https://github.com/YU-NLPLab/DeepMet 30 Proceedings of the Second Workshop on Figurative Language Processing, pages 30–39 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.1"
2020.lrec-1.200,C18-1179,0,0.0209437,"bels. Moreover, about 30,000 emotion labels were manually attached to text dialogs in the SemEval-2019 Task 3 (EmoContext) (Chatterjee et al., 2019). Alongside the corpora of SemEval, there are several other English text corpora for emotion classification; however, the number of emotion corpora for other languages is limited. For example, one such corpus was that of Saputri et al. (Saputri et al., 2018), who built an Indonesian Twitter dataset for emotion classification. They collected 7,500 tweets and manually 1 annotated emotion labels to these tweets. On the other hand, Bostan and Klinger (Bostan and Klinger, 2018) investigated and analyzed annotated corpora for emotion classification. They referred to many emotion corpora in their paper, and they also reported that the size of even the largest corpora was fewer than 40,000 labels. With this fact in mind, to build an annotated corpus for emotion classification can be considered an arduous task. However, a defining factor of this study is that it describes a semi-automatically constructed annotated corpus for emotion classification from Japanese tweets; the size of the corpus was about 80,000 labels, which must be considered as one of the largest corpora"
2020.lrec-1.200,S19-2005,0,0.0267048,"on for a text corpus for emotion classification is considerably more difficult. This paper describes an emotion classification of text sentences from Twitter 1 , one of the most popular social networking services. To deal with an emotion classification task, a training dataset has to be prepared and constructed for the emotion classifiers. For example, SemEval-2018 Task 1 (Affect in Tweets)(Mohammad et al., 2018) prepared 8,640 English sentences with manually annotated labels. Moreover, about 30,000 emotion labels were manually attached to text dialogs in the SemEval-2019 Task 3 (EmoContext) (Chatterjee et al., 2019). Alongside the corpora of SemEval, there are several other English text corpora for emotion classification; however, the number of emotion corpora for other languages is limited. For example, one such corpus was that of Saputri et al. (Saputri et al., 2018), who built an Indonesian Twitter dataset for emotion classification. They collected 7,500 tweets and manually 1 annotated emotion labels to these tweets. On the other hand, Bostan and Klinger (Bostan and Klinger, 2018) investigated and analyzed annotated corpora for emotion classification. They referred to many emotion corpora in their pap"
2020.lrec-1.200,N19-1423,0,0.00563523,"able love, comfortable, welcome horrible, terrible, lonely dark, suspect, scaring unexpectedly, unbelievable, great dissappar, no way, impressed sad, disappointment, lonely unfortunate, remorseful, poor rammy, croosh, awkward depressing, gloomy, imperfect offensive, exasperated, sore loud, sleepy, angly hopeful, wary, tension excited, fluttery, atwitter Table 1: Examples of seed keywords for searching emotional tweets the neural network-based emotion classification model used in this study. There are some excellent models such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), which achieved superior results on various natural language processing tasks. However, we did not try to use state-of-the-art neural network architectures because the purpose of this paper is to make a training corpus (sets of a sentence and their emotion labels) for a robust emotion classifier semi-automatically. Therefore, our proposed method for making a training dataset does not depend on any specific neural network architectures. As shown in Figure 1, the neural network-based model is composed of the three following main model architectures: one is a model consisting of two fully connec"
2020.lrec-1.200,W17-3529,0,0.0171423,"et al., 2012), have been solved by deep learning-based machine learning frameworks. A deep learning-based cognitive system needs a large number of datasets for model training to prevent the model from over-training (over-fitting). A dataset for model training usually consists of a pair of input data and its annotated teacher label. However, the construction of a dataset is highly costly because in the past humans have usually had to annotate the teacher labels to the dataset. This has resulted in more cost-effective proposals of data augmentation methods (Cubuk et al., 2019; Nishizaki, 2017; Kafle et al., 2017) being put forward. Although it is comparatively easy to perform data augmentation for image and audio datasets, data augmentation for a text corpus for emotion classification is considerably more difficult. This paper describes an emotion classification of text sentences from Twitter 1 , one of the most popular social networking services. To deal with an emotion classification task, a training dataset has to be prepared and constructed for the emotion classifiers. For example, SemEval-2018 Task 1 (Affect in Tweets)(Mohammad et al., 2018) prepared 8,640 English sentences with manually annotate"
2020.lrec-1.200,W04-3230,0,0.0411582,"from the “Emotion Representation Dictionary” (Nakamura, 1993). We assumed that the dictionary lists representative emotional keywords that have strong ties to each emotion category. Table 1 shows a part of the seed emotional keywords and their corresponding emotion category. Therefore, each seed emotional keyword can be connected to one emotion category. 2. Sentences were collected from Twitter using the 124 seed keywords. Each sentence was labeled depending on the emotion category of the seed keyword. 3. Each sentence was segmented into words using the Japanese morphological analyzer MeCab (Kudo et al., 2004). 4. Ten thousand tweets (sentences) were randomly selected for each of the eight emotion categories. These were then used to construct the initial training data that contained (S1 , L1 ),..., (SN , LN ), where N = 80,000. As described above and in Table 1, a tweet (sentence) was automatically annotated with one emotion label based on the hypothesis that a seed emotional keyword is strongly connected to one emotion category. Therefore, the initial training dataset may contain many wrongly labelled sentences. This is because the emotion category of a seed emotional keyword and the true emotion"
2020.lrec-1.200,S18-1001,0,0.0256737,"augmentation methods (Cubuk et al., 2019; Nishizaki, 2017; Kafle et al., 2017) being put forward. Although it is comparatively easy to perform data augmentation for image and audio datasets, data augmentation for a text corpus for emotion classification is considerably more difficult. This paper describes an emotion classification of text sentences from Twitter 1 , one of the most popular social networking services. To deal with an emotion classification task, a training dataset has to be prepared and constructed for the emotion classifiers. For example, SemEval-2018 Task 1 (Affect in Tweets)(Mohammad et al., 2018) prepared 8,640 English sentences with manually annotated labels. Moreover, about 30,000 emotion labels were manually attached to text dialogs in the SemEval-2019 Task 3 (EmoContext) (Chatterjee et al., 2019). Alongside the corpora of SemEval, there are several other English text corpora for emotion classification; however, the number of emotion corpora for other languages is limited. For example, one such corpus was that of Saputri et al. (Saputri et al., 2018), who built an Indonesian Twitter dataset for emotion classification. They collected 7,500 tweets and manually 1 annotated emotion lab"
2020.sdp-1.14,N18-1021,0,0.0341759,"Missing"
2020.sdp-1.14,D16-1115,0,0.13839,"related international conferences has significantly increased in recent years, it is challenging for the review process. Rejecting some papers with evidently low quality can reduce the workload. On the other hand, suggesting the weak aspects to the authors can also help them improve their papers. There are several existing works related to the paper review which concentrate on the quality of the review (De Silva and Vance, 2017; Langford and Guzdial, 2015). Huang (2018) et al. predicted the acceptance of a paper only based on a paper’s visual appearance (Huang, 2018). Automatic essay scoring (Dong and Zhang, 2016; Dong et al., 2017; Amorim et al., 2018) can be regarded as a related sub-topic that mainly focus on the grammatical and syntactic features in short essays. PeerRead is the first public dataset of scientific peer reviews for research purposes (Kang et al., 2018), which can be used for paper acceptance classification and review aspect score prediction. It provides detailed peerreviews including the final decisions, the aspect scores such as clarity and originality, and the review contents. It raises two NLP tasks, paper acceptance classification and review aspect score prediction. We focus on"
2020.sdp-1.14,K17-1017,0,0.0795069,"conferences has significantly increased in recent years, it is challenging for the review process. Rejecting some papers with evidently low quality can reduce the workload. On the other hand, suggesting the weak aspects to the authors can also help them improve their papers. There are several existing works related to the paper review which concentrate on the quality of the review (De Silva and Vance, 2017; Langford and Guzdial, 2015). Huang (2018) et al. predicted the acceptance of a paper only based on a paper’s visual appearance (Huang, 2018). Automatic essay scoring (Dong and Zhang, 2016; Dong et al., 2017; Amorim et al., 2018) can be regarded as a related sub-topic that mainly focus on the grammatical and syntactic features in short essays. PeerRead is the first public dataset of scientific peer reviews for research purposes (Kang et al., 2018), which can be used for paper acceptance classification and review aspect score prediction. It provides detailed peerreviews including the final decisions, the aspect scores such as clarity and originality, and the review contents. It raises two NLP tasks, paper acceptance classification and review aspect score prediction. We focus on the later one in th"
2020.sdp-1.14,P18-1064,0,0.0265188,"ediction. We focus on the later one in this paper. However, the dataset is relatively small; the set of papers for each review aspect can be different. To improve the performance of aspect score prediction, we propose a solution based on the multi-task learning that can leverage additional rich information from the resources obtained by other aspect scores. We treat the prediction of each aspect as a separate task. The multi-task model for each aspect score has a main-auxiliary manner. Multi-task methods have been widely utilized in many NLP tasks, such as summarization (Isonuma et al., 2017; Guo et al., 2018), classification (Liu et al., 2017b; Shimura et al., 2019), parsing (Hershcovich et al., 2018), sequence labeling (Lin et al., 2018), and Entity and Relation (Luan et al., 2018). When building a multi-task model, there are two critical issues, i.e., which auxiliary resources (tasks) can be used for sharing useful information and how to share the information among the tasks. In these previous studies, researchers always select specific auxiliary resources, and design handcrafted shared structure in the model for a particular NLP topic. However, for different datasets and tasks, there may exist"
2020.sdp-1.14,P18-1035,0,0.015229,"small; the set of papers for each review aspect can be different. To improve the performance of aspect score prediction, we propose a solution based on the multi-task learning that can leverage additional rich information from the resources obtained by other aspect scores. We treat the prediction of each aspect as a separate task. The multi-task model for each aspect score has a main-auxiliary manner. Multi-task methods have been widely utilized in many NLP tasks, such as summarization (Isonuma et al., 2017; Guo et al., 2018), classification (Liu et al., 2017b; Shimura et al., 2019), parsing (Hershcovich et al., 2018), sequence labeling (Lin et al., 2018), and Entity and Relation (Luan et al., 2018). When building a multi-task model, there are two critical issues, i.e., which auxiliary resources (tasks) can be used for sharing useful information and how to share the information among the tasks. In these previous studies, researchers always select specific auxiliary resources, and design handcrafted shared structure in the model for a particular NLP topic. However, for different datasets and tasks, there may exist other better auxiliary resources and shared structures. We thus propose an approach selecting"
2020.sdp-1.14,D17-1223,0,0.0184512,"review aspect score prediction. We focus on the later one in this paper. However, the dataset is relatively small; the set of papers for each review aspect can be different. To improve the performance of aspect score prediction, we propose a solution based on the multi-task learning that can leverage additional rich information from the resources obtained by other aspect scores. We treat the prediction of each aspect as a separate task. The multi-task model for each aspect score has a main-auxiliary manner. Multi-task methods have been widely utilized in many NLP tasks, such as summarization (Isonuma et al., 2017; Guo et al., 2018), classification (Liu et al., 2017b; Shimura et al., 2019), parsing (Hershcovich et al., 2018), sequence labeling (Lin et al., 2018), and Entity and Relation (Luan et al., 2018). When building a multi-task model, there are two critical issues, i.e., which auxiliary resources (tasks) can be used for sharing useful information and how to share the information among the tasks. In these previous studies, researchers always select specific auxiliary resources, and design handcrafted shared structure in the model for a particular NLP topic. However, for different datasets and task"
2020.sdp-1.14,P17-1052,0,0.018539,"for classification to mean squared error for regression. Without loss of generality, we use the basic CNN-based text classification model (Kim, 2014) as the example to facilitate the description of our multi-task approach. Figure 1 shows the architecture of this model for predicting the aspect score. It includes the embedding layer, convolutional and pooling layer, and fully connected layers. The multi-task approach we propose is not limited to be adapted with this model. It can be integrated with similar neural network structures in this example, e.g., XML-CNN (Liu et al., 2017a) and DPCNN (Johnson and Zhang, 2017). We have n single tasks (i.e., aspect scores) and assume that they have the same network structures with k layers. For each task, we regard it as the main task and search the proper shared structures and auxiliary tasks. 2.2 Multi-task Shared Structures To automatically search the proper shared structures and auxiliary tasks, we need to define the exploration space. Because it is difficult to mix diverse parameter sharing manners proposed in various multi-task methods (Ruder, 2017), we utilize the typical manner of hard parameter sharing as the starting point to implement our idea. Other mann"
2020.sdp-1.14,N18-1149,0,0.0612242,"Missing"
2020.sdp-1.14,D14-1181,0,0.0154554,"nt tool for both reviewers and authors. (2). We propose a multi-task shared structure encoding method which automatically selects good shared network structures as well as good auxiliary resources. (3). The experiments based on real paper peer-review datasets show that our approach can build a multi-task model with effective structures and auxiliaries which has better performance than the single-task model and na¨ıve multi-task models. 2 2.1 Our Approach Preliminary Peer-review aspect score prediction is a regression problem with text data. We can utilize existing text classification methods (Kim, 2014; Liu et al., 2017a) based on deep neural network for this problem by changing the loss function from cross-entropy for classification to mean squared error for regression. Without loss of generality, we use the basic CNN-based text classification model (Kim, 2014) as the example to facilitate the description of our multi-task approach. Figure 1 shows the architecture of this model for predicting the aspect score. It includes the embedding layer, convolutional and pooling layer, and fully connected layers. The multi-task approach we propose is not limited to be adapted with this model. It can"
2020.sdp-1.14,P18-1074,0,0.0198704,"t can be different. To improve the performance of aspect score prediction, we propose a solution based on the multi-task learning that can leverage additional rich information from the resources obtained by other aspect scores. We treat the prediction of each aspect as a separate task. The multi-task model for each aspect score has a main-auxiliary manner. Multi-task methods have been widely utilized in many NLP tasks, such as summarization (Isonuma et al., 2017; Guo et al., 2018), classification (Liu et al., 2017b; Shimura et al., 2019), parsing (Hershcovich et al., 2018), sequence labeling (Lin et al., 2018), and Entity and Relation (Luan et al., 2018). When building a multi-task model, there are two critical issues, i.e., which auxiliary resources (tasks) can be used for sharing useful information and how to share the information among the tasks. In these previous studies, researchers always select specific auxiliary resources, and design handcrafted shared structure in the model for a particular NLP topic. However, for different datasets and tasks, there may exist other better auxiliary resources and shared structures. We thus propose an approach selecting the shared structures automatically as"
2020.sdp-1.14,P17-1001,0,0.235303,"in this paper. However, the dataset is relatively small; the set of papers for each review aspect can be different. To improve the performance of aspect score prediction, we propose a solution based on the multi-task learning that can leverage additional rich information from the resources obtained by other aspect scores. We treat the prediction of each aspect as a separate task. The multi-task model for each aspect score has a main-auxiliary manner. Multi-task methods have been widely utilized in many NLP tasks, such as summarization (Isonuma et al., 2017; Guo et al., 2018), classification (Liu et al., 2017b; Shimura et al., 2019), parsing (Hershcovich et al., 2018), sequence labeling (Lin et al., 2018), and Entity and Relation (Luan et al., 2018). When building a multi-task model, there are two critical issues, i.e., which auxiliary resources (tasks) can be used for sharing useful information and how to share the information among the tasks. In these previous studies, researchers always select specific auxiliary resources, and design handcrafted shared structure in the model for a particular NLP topic. However, for different datasets and tasks, there may exist other better auxiliary resources a"
2020.sdp-1.14,D18-1360,0,0.0237346,"ce of aspect score prediction, we propose a solution based on the multi-task learning that can leverage additional rich information from the resources obtained by other aspect scores. We treat the prediction of each aspect as a separate task. The multi-task model for each aspect score has a main-auxiliary manner. Multi-task methods have been widely utilized in many NLP tasks, such as summarization (Isonuma et al., 2017; Guo et al., 2018), classification (Liu et al., 2017b; Shimura et al., 2019), parsing (Hershcovich et al., 2018), sequence labeling (Lin et al., 2018), and Entity and Relation (Luan et al., 2018). When building a multi-task model, there are two critical issues, i.e., which auxiliary resources (tasks) can be used for sharing useful information and how to share the information among the tasks. In these previous studies, researchers always select specific auxiliary resources, and design handcrafted shared structure in the model for a particular NLP topic. However, for different datasets and tasks, there may exist other better auxiliary resources and shared structures. We thus propose an approach selecting the shared structures automatically as well as the auxiliary resources that are mor"
2020.sdp-1.14,D18-1093,1,0.896056,"Missing"
2020.sdp-1.14,P19-1105,1,0.892431,"Missing"
2021.emnlp-main.290,D19-6617,0,0.0142132,"ationale selection task); it also classifies whether able information among each other. Therefore, we the abstract/sentences support or refute the claims propose an approach, named as ARSJ OINT, which (stance prediction task). Wadden et al. (2020) also jointly learns the three modules for the three tasks. provided a dataset called S CI FACT. It has a Machine Reading Comprehension (MRC) Most of the existing works of general claim ver- framework which uses the claim content as the ification are based on pipeline models (Soleimani query to learn additional information. In addition, et al., 2020; Alonso-Reina et al., 2019; Liu et al., we assume that the abstract retrieval module should 3580 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3580–3586 c November 7–11, 2021. 2021 Association for Computational Linguistics have good interpretability and tend to assign high sentence-level attention scores to the evidence sentences that influence the retrieval results; it is consistent with the goal of the rationale selection module. We thus enhance the information exchanges and constraints among tasks by proposing a regularization term based on a symmetric divergence to br"
2021.emnlp-main.290,2020.acl-main.761,0,0.0181916,"g comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset S CI FACT show that our approach outperforms the existing works. Figure 1: An example of scientific claim verification. 2020; Zhou et al., 2019; Nie et al., 2019; Lee et al., 2020b); some works utilize joint optimization strategies (Lu and Li, 2020; Yin and Roth, 2018; Hidey et al., 2020). These models attempted to jointly optimize the rationale selection and stance prediction, but did not directly link the two modules (Li et al., 2020). In the case of the scientific claim verification, Wadden et al. (2020) proposed a baseline model V ERI S CI based on a pipeline of three components for the three tasks. Pradeep et al. (2021) proposed a pipeline model called 1 Introduction V ERT5 ERINI which utilized the pre-trained language model T5 (Raffel et al., 2020) and adapted A system of scientific claim verification can help a pre-trained sequence-to-sequence model. Li et al. the resea"
2021.emnlp-main.290,2020.acl-main.48,0,0.0230059,"the three tasks with a machine reading comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset S CI FACT show that our approach outperforms the existing works. Figure 1: An example of scientific claim verification. 2020; Zhou et al., 2019; Nie et al., 2019; Lee et al., 2020b); some works utilize joint optimization strategies (Lu and Li, 2020; Yin and Roth, 2018; Hidey et al., 2020). These models attempted to jointly optimize the rationale selection and stance prediction, but did not directly link the two modules (Li et al., 2020). In the case of the scientific claim verification, Wadden et al. (2020) proposed a baseline model V ERI S CI based on a pipeline of three components for the three tasks. Pradeep et al. (2021) proposed a pipeline model called 1 Introduction V ERT5 ERINI which utilized the pre-trained language model T5 (Raffel et al., 2020) and adapted A system of scientific claim verification can help a pre-trained sequen"
2021.emnlp-main.290,N18-1049,0,0.0115935,"all of them to the proposed model proper for document classification by considering is time-consuming. Therefore, similar to the ex- the hierarchical document structure (a document isting works on this topic (Wadden et al., 2020; has sentences, a sentence has words). We also comPradeep et al., 2021; Li et al., 2020), we also utilize pute the sentence representation of claim hq ∈ Rd a lightweight method to first roughly select a set of with a word-level attention layer (denoted as g(·)), candidate papers. We used the BioSentVec (Chen hq = g(Hq ). To compute the relevance between et al., 2019; Pagliardini et al., 2018) to obtain the hta and hq , we use a Hadamard product on them embeddings of the claim or a scientific paper based and a Multi-Layer Perception (MLP, denoted as on its title and abstract, and compute the cosine sim- f (·)) with Softmax (denoted as σ(·)); the outputs 3581 Figure 2: Framework of our ARSJ OINT model which jointly learns three modules and has rationale regularization. SUPPORT NOINFO REFUTES ALL are the probabilities that whether the abstract is Train 332 / 370 304 / 220 173 / 194 809 relevant to the claim, [pb0 , pb1 ] = σ(f (hq ◦ ha )). A Dev. 124 / 138 112 / 114 64 / 71 300 cross"
2021.emnlp-main.290,2021.louhi-1.11,0,0.389202,"how that our approach outperforms the existing works. Figure 1: An example of scientific claim verification. 2020; Zhou et al., 2019; Nie et al., 2019; Lee et al., 2020b); some works utilize joint optimization strategies (Lu and Li, 2020; Yin and Roth, 2018; Hidey et al., 2020). These models attempted to jointly optimize the rationale selection and stance prediction, but did not directly link the two modules (Li et al., 2020). In the case of the scientific claim verification, Wadden et al. (2020) proposed a baseline model V ERI S CI based on a pipeline of three components for the three tasks. Pradeep et al. (2021) proposed a pipeline model called 1 Introduction V ERT5 ERINI which utilized the pre-trained language model T5 (Raffel et al., 2020) and adapted A system of scientific claim verification can help a pre-trained sequence-to-sequence model. Li et al. the researchers to easily find the target scientific (2020) jointly trained two tasks of rationale selecpapers with the sentence evidence from a large corpus for the given claim. To address this issue, tion and stance prediction, and had a pipeline on abstract retrieval task and the joint module. Wadden et al. (2020) introduced scientific claim verif"
2021.emnlp-main.290,P19-1282,0,0.0176127,"ty pri1 > pri0 , until eventually all sentences in S r are We compute the sentence representation hsi by a based on the estimated evidences. We set the samword-level attention layer, and use a MLP with pling probability of using the estimated evidences Softmax to estimate the probability pri1 and pri0 that current_epoch−1 π whether si is the evidence of the abstract or not. as psample = sin( 2 × total_epoch−1 ). Rationale Regularization (RR): The attention The cross entropy loss is Lrat . scores have been used for interpretability in NLP Stance Prediction: The module first computes the tasks (Serrano and Smith, 2019; Wiegreffe and Pinsentence representation hsi in a same way with that ter, 2019; Sun and Lu, 2020). We assume that the of rationale selection. After that, it only selects the r r sentences S with the true evidence label yˆi = 1 abstract retrieval module should have good interor the estimated evidence probability pri1 > pri0 ; pretability and tend to assign high sentence-level attention scores to the evidence sentences that inwhether using the true label or the estimated label fluence the retrieval results; it is consistent with is decided by a scheduled sampling which will be the goal of the"
2021.emnlp-main.290,2021.ccl-1.108,0,0.0834009,"Missing"
2021.emnlp-main.290,2020.acl-main.655,0,0.0592091,"Missing"
2021.emnlp-main.290,2020.acl-main.312,0,0.0132262,"a based on the estimated evidences. We set the samword-level attention layer, and use a MLP with pling probability of using the estimated evidences Softmax to estimate the probability pri1 and pri0 that current_epoch−1 π whether si is the evidence of the abstract or not. as psample = sin( 2 × total_epoch−1 ). Rationale Regularization (RR): The attention The cross entropy loss is Lrat . scores have been used for interpretability in NLP Stance Prediction: The module first computes the tasks (Serrano and Smith, 2019; Wiegreffe and Pinsentence representation hsi in a same way with that ter, 2019; Sun and Lu, 2020). We assume that the of rationale selection. After that, it only selects the r r sentences S with the true evidence label yˆi = 1 abstract retrieval module should have good interor the estimated evidence probability pri1 > pri0 ; pretability and tend to assign high sentence-level attention scores to the evidence sentences that inwhether using the true label or the estimated label fluence the retrieval results; it is consistent with is decided by a scheduled sampling which will be the goal of the rationale selection module. We thus introduced later. We then compute the estimated enhance the inf"
2021.emnlp-main.290,N18-1074,0,0.0640675,"Missing"
2021.emnlp-main.290,2020.emnlp-main.609,0,0.0545626,"Missing"
2021.emnlp-main.290,D19-1002,0,0.044526,"Missing"
2021.emnlp-main.290,N16-1174,0,0.0568664,"w1 h∗j + bw1 ) for word-level attention, X exp(Wc2 U?i + bc2 ) , g(H? ) = U?i α?i , α?i = P i j exp(Wc2 U?j + bc2 ) U?j = tanh(Wc1 H?j + bc1 ) for sentence-level attention. (1) Abstract Retrieval: In this task, a title can be regarded as an auxiliary sentence that may contain the information related to the claim for the abstract, we thus use the title with the sentences in the abstract together. We build a document ta = [t, a] and concatenate the word representations of t and a into Hta = [Ht , Ha ] as the input to this module. We use a hierarchical attention network (HAN) 2.2 Pre-processing (Yang et al., 2016) to compute document represenAs there are a huge amount of papers in the cor- tations hta ∈ Rd , hta = HAN(Hta ). HAN is pus, applying all of them to the proposed model proper for document classification by considering is time-consuming. Therefore, similar to the ex- the hierarchical document structure (a document isting works on this topic (Wadden et al., 2020; has sentences, a sentence has words). We also comPradeep et al., 2021; Li et al., 2020), we also utilize pute the sentence representation of claim hq ∈ Rd a lightweight method to first roughly select a set of with a word-level attentio"
2021.emnlp-main.290,N16-1000,0,0.227234,"Missing"
2021.emnlp-main.290,D18-1010,0,0.0202882,"ith a machine reading comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset S CI FACT show that our approach outperforms the existing works. Figure 1: An example of scientific claim verification. 2020; Zhou et al., 2019; Nie et al., 2019; Lee et al., 2020b); some works utilize joint optimization strategies (Lu and Li, 2020; Yin and Roth, 2018; Hidey et al., 2020). These models attempted to jointly optimize the rationale selection and stance prediction, but did not directly link the two modules (Li et al., 2020). In the case of the scientific claim verification, Wadden et al. (2020) proposed a baseline model V ERI S CI based on a pipeline of three components for the three tasks. Pradeep et al. (2021) proposed a pipeline model called 1 Introduction V ERT5 ERINI which utilized the pre-trained language model T5 (Raffel et al., 2020) and adapted A system of scientific claim verification can help a pre-trained sequence-to-sequence model"
2021.emnlp-main.290,P19-1085,0,0.047025,"Missing"
A97-1043,C96-1069,1,\N,Missing
A97-1043,C96-2134,0,\N,Missing
A97-1043,C92-2070,0,\N,Missing
A97-1043,C94-1049,0,\N,Missing
A97-1043,P91-1034,0,\N,Missing
A97-1043,C96-2166,0,\N,Missing
C02-1085,P99-1071,0,0.0333023,"ng shift 6 Related Work Most of the work on summarization task by paragraph or sentence extraction has applied statistical techniques based on word distribution to the target document(Kupiec et al., 1995). More recently, other approaches have investigated the use of machine learning to ﬁnd patterns in documents(Strzalkowski et al., 1998) and the utility of parameterized modules so as to deal with diﬀerent genres or corpora(Goldstein et al., 2000). Some of these approaches to single document summarization have been extended to deal with multi-document summarization(Mani and E.Bloedorn, 1997), (Barzilay et al., 1999), (McKeown et al., 1999). Our work diﬀers from the earlier work in several important respects. First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from diﬀerent events(Radev et al., 2000). Detecting subject shift from the documents in the target event, however, presents special diﬃculties, since these documents are collected from a very restricted domain. We thus present a window adjustment algorithm which automatically adjusts the optimal window in the training documents, so as to include only the data which are suﬃciently relate"
C02-1085,H92-1022,0,0.0168048,"Missing"
C02-1085,W00-0405,0,0.0331787,"he result using the output: the errors by both tracking and detecting shifts were corrected. Figure 4 shows that our method does Figure 6: Precision with and without detecting shift 6 Related Work Most of the work on summarization task by paragraph or sentence extraction has applied statistical techniques based on word distribution to the target document(Kupiec et al., 1995). More recently, other approaches have investigated the use of machine learning to ﬁnd patterns in documents(Strzalkowski et al., 1998) and the utility of parameterized modules so as to deal with diﬀerent genres or corpora(Goldstein et al., 2000). Some of these approaches to single document summarization have been extended to deal with multi-document summarization(Mani and E.Bloedorn, 1997), (Barzilay et al., 1999), (McKeown et al., 1999). Our work diﬀers from the earlier work in several important respects. First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from diﬀerent events(Radev et al., 2000). Detecting subject shift from the documents in the target event, however, presents special diﬃculties, since these documents are collected from a very restricted domain. We thu"
C02-1085,W00-0403,0,0.100558,"Missing"
C04-1125,W99-0606,0,\N,Missing
C04-1125,C02-1101,0,\N,Missing
C04-1125,A00-2020,0,\N,Missing
C08-1030,P06-1011,0,0.0388581,"Missing"
C08-1030,J03-3002,0,0.0282871,"ve consists of two steps: first, crosslingual relevant documents are retrieved from comparable corpora, then bilingual term correspondences within these relevant documents are estimated. Thus, the accuracy depends on the performance of relevant documents retrieval. Much of the previous work in finding relevant documents used MT systems or existing bilingual lexicons to translate one language into another. Document pairs are then retrieved using some measure of document similarity. Another approach to retrieving relevant documents involves the collection of relevant document URLs from the WWW (Resnik and Smith, 2003). Utsuro et al. (2003) proposed a method for acquiring bilingual lexicons that involved retrieval of relevant English and Japanese documents from news sites on the WWW. Our work is also applicable to retrieval of relevant documents on the web because it estimates every bilingual lexicon only appearing in 239 Table 6: Numbers of monolingual and bilingual verb–noun collocations Approach & (Lθ ) # of Candidate collocations Monolingual patterns No hi & Eng (40) Reu Hierarchy (20) Int hi & Eng (20) Jap 25,163 10,576 8,347 Eng 44,762 37,022 21,524 # of collocations D&S 25,163 10,576 8,347 Doc 6,976,"
C08-1030,W04-3208,0,0.0589756,"gual lexicons used comparable corpora. One attempt involved directly retrieving bilingual lexicons from corpora. One approach focused on extracting word translations (Gaussier et al., 2004). The techniques were based on the idea that semantically similar words appear in similar contexts. Unlike parallel corpora, the position of a word in a document is useless for translation into the other language. In these techniques, the frequency of words in the monolingual document is calculated and their contextual similarity is measured across languages. Another approach focused on sentence extraction (Fung and Cheung, 2004). One limitation of all these methods is that they need to control the experimental evaluation to avoid estimation of every bilingual lexicon appearing in comparable corpora. The alternative consists of two steps: first, crosslingual relevant documents are retrieved from comparable corpora, then bilingual term correspondences within these relevant documents are estimated. Thus, the accuracy depends on the performance of relevant documents retrieval. Much of the previous work in finding relevant documents used MT systems or existing bilingual lexicons to translate one language into another. Doc"
C08-1030,P04-1067,0,0.0276235,"Missing"
C08-1030,P03-1004,0,0.0114957,"milar categories. “Int hi & Jap” is the same as “Int hi & Eng” except for the use of dir mt and dm j for comparison. We compared the performance of these tasks, and found that “Int hi & Eng” was better than “Int hi & Jap”. In section 3, we show results with “Int hi & Eng” due to lack of space. 235 (3) 2.3 Acquisition of Bilingual Collocations The final step is to estimate bilingual corresponsim(S vnr , S vnm ) = dences from relevant documents. All Japanese co(S vnr ∩ S mt vnm ) , (6) documents were parsed using the syntactic ana- |S vnr |+ |S mt vnm |−2co(S vnr ∩ S mt vnm ) + 2 lyzer CaboCha (Kudo and Matsumoto, 2003). English documents were parsed with the syntactic an- where |X |is the number of content words in a senalyzer (Lin, 1993). In both English and Japanese, tence X, and co(S vnr ∩ S mt vnm ) refers to the we extracted all the dependency triplets(obj, n, v). number of content words that appear in both S vnr Here, n refers to a noun which is an object(obj) and S mt vnm . S mt vnm is a translation result of of a verb v in a sentence.3 Hereafter, we de- S vmm . We retrieved vnr and vnm as a bilingual scribe the Reuters English dependency triplet as lexicon that satisfies: vnr , and that of Mainichi"
C08-1030,P93-1016,0,0.0120989,"ormance of these tasks, and found that “Int hi & Eng” was better than “Int hi & Jap”. In section 3, we show results with “Int hi & Eng” due to lack of space. 235 (3) 2.3 Acquisition of Bilingual Collocations The final step is to estimate bilingual corresponsim(S vnr , S vnm ) = dences from relevant documents. All Japanese co(S vnr ∩ S mt vnm ) , (6) documents were parsed using the syntactic ana- |S vnr |+ |S mt vnm |−2co(S vnr ∩ S mt vnm ) + 2 lyzer CaboCha (Kudo and Matsumoto, 2003). English documents were parsed with the syntactic an- where |X |is the number of content words in a senalyzer (Lin, 1993). In both English and Japanese, tence X, and co(S vnr ∩ S mt vnm ) refers to the we extracted all the dependency triplets(obj, n, v). number of content words that appear in both S vnr Here, n refers to a noun which is an object(obj) and S mt vnm . S mt vnm is a translation result of of a verb v in a sentence.3 Hereafter, we de- S vmm . We retrieved vnr and vnm as a bilingual scribe the Reuters English dependency triplet as lexicon that satisfies: vnr , and that of Mainichi as vnm . The method to retrieve bilingual correspondences consists of two {vnr , vnm } = argmax S sim(vnr , vnm ) , (7) {"
C08-1030,P98-1041,0,0.0280401,"days. For example, when the date of the RWCP document is 18th Jun., the corresponding Reuters date is from 15th to 21st Jun. We chose Lθ that maximized the average F1 among them. Table 2 shows the test data, i.e., the total number of collected documents and the number of related documents collected manually for the evaluation.5 We implemented the following approaches including related work, and compared these results with those obtained by our methods, Int hi & Eng. 1. No hierarchy: Categories with each hierarchy are not used in the approach. The approach is the same as the method reported by Collier et al. (1998) except for term weights and similarities. We calculate similarities between Reuters and translated Mainichi documents, where the difference in dates is less than ± 3 days. (No hi & Eng). Results Table 1 shows F1 of category correspondences with Lχ2 = .003. “Mai & Reu” shows the results obtained by our method. “Mai” and “Reu” show the results using only one hierarchy. For example, “Mai” shows the results in which both Mainichi and translated Reuters documents are classified into categories with Mainichi hierarchy, and estimated category correspondences. Integrating hierarchies is more effectiv"
C08-1030,J93-1007,0,\N,Missing
C08-1030,E03-1023,0,\N,Missing
C08-1030,C98-1041,0,\N,Missing
C94-2122,J93-1007,0,\N,Missing
C94-2122,A88-1019,0,\N,Missing
C94-2122,P90-1034,0,\N,Missing
C94-2122,P91-1034,0,\N,Missing
C96-1069,C92-2070,0,0.187403,"t i n e n t a l 2 airlines2 . . . _ In Tal)le I, underline signifies polysenmus nolln. &apos;()utlmt.&apos; shows that ea(&apos;h noun is rel)laced l)y a syml)ol word which corresl)onds to each sense of a word. We call &apos; I n l m t &apos; and ~()utput&apos; in Table 1, mt (rriginal a r t M e and a new a r t M e , respectively. Framework Word-Sense 3.1 Disambiguation Every sense of words in artMes which should be (:lustered is automatically disambiguated in advance. Word-sense dismnl)iguation (WSD in short) is a serious problem for NLP, and a wlri(&apos;ty of al)l)roaches have been 1)roposed for solving it (Ih&apos;own, 1991), (Yarowsky, 1992). O u r disalnbiguation m e t h o d is based on Niwa&apos;s m e t h o d which used the similarity 1)etween a sentenee containing a t)olysemous noun and a sen= tence of dictionary-definition. Let x be a t)olysemous noun and a sentence X be X "" • • • ~ 3:-n~ • • • ~ a&apos;-i ~ ~1:~ ~1:1 , • "" "" ~ ilYn~ = ~ V(xi) (Mu(xi,o~),...,Mu(xi,om)) Here, Mu(x, y) is the v&apos;,due of mutual information proposed by (Church, 1991). oj,...,om (We call t h e m basic words) are selected the 1000th most frequent words in the reference Collins English Dictionary (Lil)erman, 1 9 9 0 ) . Let word x have senses sl,s2,...,sp and"
C96-1069,C94-2172,0,\N,Missing
C96-1069,P91-1034,0,\N,Missing
C98-2202,C96-2154,0,0.0206339,"of switch board corpora is rather long and there are many keywords in the discourse. However, for a short discourse, there are few keywords 1272 in a short discourse. Yokoi also proposed a topic identification method using co-occurrence of words for topic identification (Yokoi et al., 1997). He classified each dictated sentence of news into 8 topics. In TV or Radio news, however, it is difficult to segment each sentence automatically. Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model (Sekine, 1996). However, if the statistical model is used for extraction of sentence candidates, we will obtain higher recognition accuracy. Some initial studies of transcription of broadcast news proceed (Bakis et al., 1997). However there are some remaining problems, e.g. speaking styles and domain identification. We conducted domain identification and keyword extraction experiment (Suzuki et al., 1997) for radio news. In the experiment, we classified radio news into 5 domains (i.e. accident, economy, international, politics and sports). The problems which we faced with are; 1. Classification of newspaper"
D15-1093,P07-1033,0,0.151116,"Missing"
D15-1093,W06-1615,0,\N,Missing
D15-1093,W06-3809,0,\N,Missing
D18-1093,N15-1011,0,0.523084,"ently, many authors have attempted to apply deep learning techniques including CNN (Wang et al., 2015; Zhang and Wallace, 2015; Zhang et al., 2017; Wang et al., 2017), the attention based CNN (Yang et al., 2016), bag-of-words based CNN (Johnson and Zhang, 2015a), and the combination of CNN and recurrent neural network (Lee and Dernoncourt, 2016; Zhang et al., 2016) to text categorization. Most of them demonstrated that neural network models are powerful for learning features from texts, while they focused on single-label or a few labels problem. Several efforts have been made to multi-labels (Johnson and Zhang, 2015b; Liu et al., 2017). Liu et al. explored a family of new CNN models which are tailored for extreme multi-label classification (Liu et al., 2017). They used a dynamic max pooling scheme, a binary cross-entropy loss, and a hidden bottleneck layer to improve the overall performance. The results by using six benchmark datasets where the label-set sizes are up to 670K showed that their method attained at the best or second best in comparison with seven state-of-the-art methods including FastText (Joulin et al., 2017) and bag-of-words based CNN (Johnson and Zhang, 2015a). However, all of these atte"
D18-1093,P15-2058,0,0.0388308,"Missing"
D18-1093,E17-2068,0,0.0470175,"label or a few labels problem. Several efforts have been made to multi-labels (Johnson and Zhang, 2015b; Liu et al., 2017). Liu et al. explored a family of new CNN models which are tailored for extreme multi-label classification (Liu et al., 2017). They used a dynamic max pooling scheme, a binary cross-entropy loss, and a hidden bottleneck layer to improve the overall performance. The results by using six benchmark datasets where the label-set sizes are up to 670K showed that their method attained at the best or second best in comparison with seven state-of-the-art methods including FastText (Joulin et al., 2017) and bag-of-words based CNN (Johnson and Zhang, 2015a). However, all of these attempts aimed at utilizing a large volume of data. We address the problem of multi-label short text categorization and explore the use of a HS of categories. The lower level of categories are finegrained compared to the upper level of categories. Moreover, it is often the case that the amount of training data in a lower level is much smaller than that in an upper level which deteriorates the overall performance of categorization. We propose an approach which can effectively utilize the data in the upper levels to co"
D18-1093,N16-1174,0,0.190951,"Missing"
D18-1093,D14-1181,0,0.00707934,"gories to alleviate data sparsity in multi-label short texts. (2) We empirically examined a finetuning with CNN that fits to learn a HS of categories defined by lexicographers, and (3) The results show that our method is competitive to the state-of-the-art CNN based methods by using two benchmark datasets, especially it is effective for categorization of short texts consisting of a few words with a large number of labels. 2 Hierarchical Fine-Tuning based CNN 2.1 CNN architecture Similar to other CNN (Johnson and Zhang, 2015a; Liu et al., 2017), our HFT-CNN model shown in Figure 1 is based on (Kim, 2014). Let xi ∈ Rk be the k-dimensional word vector with the i-th word in a sentence obtained by applying skip-gram model provided in fastText1 . A sentence with length n is represented as x1:n = [x1 , x2 , · · · , xn ] ∈ Rnk . A convolution filter w ∈ Rhk is applied to a window size of h words to produce a new feature, ci = f (w·xi:i+h−1 +b) where b ∈ R indicates a bias term and f refers to a non-linear activation function. We applied this convolution filter to each possible window size in the sentence and obtained a feature map, m ∈ Rn−h+1 . As shown in Figure 1, we then apply a max pooling opera"
D18-1093,N16-1062,0,0.0216152,"re for Multi-label Short Text Categorization Kazuya Shimura1 , Jiyi Li2 and Fumiyo Fukumoto2 Graduate School of Engineering1 Interdisciplinary Graduate School2 University of Yamanashi 4-3-11, Takeda, Kofu, 400-8511 Japan {g17tk008,jyli,fukumoto}@yamanashi.ac.jp Abstract More recently, many authors have attempted to apply deep learning techniques including CNN (Wang et al., 2015; Zhang and Wallace, 2015; Zhang et al., 2017; Wang et al., 2017), the attention based CNN (Yang et al., 2016), bag-of-words based CNN (Johnson and Zhang, 2015a), and the combination of CNN and recurrent neural network (Lee and Dernoncourt, 2016; Zhang et al., 2016) to text categorization. Most of them demonstrated that neural network models are powerful for learning features from texts, while they focused on single-label or a few labels problem. Several efforts have been made to multi-labels (Johnson and Zhang, 2015b; Liu et al., 2017). Liu et al. explored a family of new CNN models which are tailored for extreme multi-label classification (Liu et al., 2017). They used a dynamic max pooling scheme, a binary cross-entropy loss, and a hidden bottleneck layer to improve the overall performance. The results by using six benchmark datase"
D18-1093,N16-1177,0,0.0127479,"xt Categorization Kazuya Shimura1 , Jiyi Li2 and Fumiyo Fukumoto2 Graduate School of Engineering1 Interdisciplinary Graduate School2 University of Yamanashi 4-3-11, Takeda, Kofu, 400-8511 Japan {g17tk008,jyli,fukumoto}@yamanashi.ac.jp Abstract More recently, many authors have attempted to apply deep learning techniques including CNN (Wang et al., 2015; Zhang and Wallace, 2015; Zhang et al., 2017; Wang et al., 2017), the attention based CNN (Yang et al., 2016), bag-of-words based CNN (Johnson and Zhang, 2015a), and the combination of CNN and recurrent neural network (Lee and Dernoncourt, 2016; Zhang et al., 2016) to text categorization. Most of them demonstrated that neural network models are powerful for learning features from texts, while they focused on single-label or a few labels problem. Several efforts have been made to multi-labels (Johnson and Zhang, 2015b; Liu et al., 2017). Liu et al. explored a family of new CNN models which are tailored for extreme multi-label classification (Liu et al., 2017). They used a dynamic max pooling scheme, a binary cross-entropy loss, and a hidden bottleneck layer to improve the overall performance. The results by using six benchmark datasets where the label-se"
D18-1093,P17-2024,0,0.0422789,"Missing"
D19-5904,P17-1028,0,0.0229414,"ity of Yamanashi, Kofu, Japan fukumoto@yamanashi.ac.jp Abstract an instance, because the multiple crowdsourced answers are not golden ones, the aggregation approach for generating a golden one based on these crowdsourced answers is indispensable. In crowdsourcing area, there are many existing work on answer aggregation for labels (Dawid and Skene, 1979; Whitehill et al., 2009; Zheng et al., 2017). Snow et al. (2008) evaluated crowdsourced label annotations for some NLP tasks and used majority voting for label aggregation. However, there is little work on answer aggregation for word sequences. Nguyen et al. (2017) proposed an aggregation method based on HMM for a sequence of categorical labels and needs to be improved for aligning sparse and free word sequences. If treating a word as a category, there are tens of thousands categories and a sequence only contains a small number of them. To address the problem of answer aggregation for word sequences, people need the datasets which contain multiple word sequence answers provided by different crowd workers for one instance. However, we find that most of the existing datasets in NLP area only contain a single golden answer for one instance. In this paper,"
D19-5904,D16-1049,0,0.066246,"Missing"
D19-5904,P02-1040,0,0.104238,"e which is expensive and has an insufficient number, the crowd which is cheaper and easier to access is a good alternative for collecting the gold standard data. Because the ability of crowd workers is diverse, to guarantee the quality of the collected data, one solution is to generate redundant data by assigning multiple workers to one instance and then aggregate the multiple answers into golden ones. How to aggregate multiple word sequences with diverse quality is a research problem. In NLP areas such as machine translation, although a few evaluation metrics (Liu et al., 2016) such as BLEU (Papineni et al., 2002) can use multiple golden answers for 24 Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP, pages 24–28 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 data J1 T1 T2 good word sequence from the candidates. One of them is our original approach which models the reliability of workers, because worker reliability is regarded as an important factor in label aggregation approaches (Zheng et al., 2017). 2 2.1 #que. 250 100 100 #wor. 70 42 43 #ans. 2,490 1,000 1,000 #apq 9.96 10 10 mmr 0.1"
D19-5904,D08-1027,0,0.239032,"Missing"
D19-5904,P15-1162,0,0.0506711,"Missing"
D19-5904,D18-1449,0,0.0257568,"s computed by embedding similarity, we use the mean of sentence-wise GLEU of all answers of a worker (question), in contrast to corpus-wise BLEU measure. In contrast to Figure 1, the quality on J1 is higher. One possible reason is that the low quality workers judged by embedding similarity can provide good words or phrases translations which the GLEU focuses on, but cannot provide good word orders and syntax on the sentence level which the DAN (Iyyer et al., 2015) model of universal sentence encoder considers. 3 Sequence Majority Voting i 3.2 Sequence Maximum Similarity We adapt the method in Kobayashi (2018), which is proposed as a post-ensemble method for multiple summarization generation models. For each question, it extracts the worker answer which has largest sum of similarity with other answers of this question. It can be regarded as creating a kernel density estimator and extract the maximum density answer. The kernel function uses the cosine similarity. This Sequence Maximum Similarity (SMS) method can be formulated as zˆi = P arg maxak1 k1 6=k2 sim(e(aki 1 ), e(aki 2 )). i 3.3 Reliability Aware Sequence Aggregation Both SMV and SMS do not consider the worker reliability. In crowdsourcing,"
D19-5904,1983.tc-1.13,0,0.144952,"Missing"
D19-5904,D16-1230,0,0.014154,"e required. Instead of the oracle which is expensive and has an insufficient number, the crowd which is cheaper and easier to access is a good alternative for collecting the gold standard data. Because the ability of crowd workers is diverse, to guarantee the quality of the collected data, one solution is to generate redundant data by assigning multiple workers to one instance and then aggregate the multiple answers into golden ones. How to aggregate multiple word sequences with diverse quality is a research problem. In NLP areas such as machine translation, although a few evaluation metrics (Liu et al., 2016) such as BLEU (Papineni et al., 2002) can use multiple golden answers for 24 Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP, pages 24–28 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 data J1 T1 T2 good word sequence from the candidates. One of them is our original approach which models the reliability of workers, because worker reliability is regarded as an important factor in label aggregation approaches (Zheng et al., 2017). 2 2.1 #que. 250 100 100 #wor. 70 42 43 #ans. 2,49"
E99-1028,H92-1022,0,0.0139033,"onsists of pairs of collocations of a distinct sense of a polysemous verb and a noun. Let v has senses vl, v2, ""--, v,~. The sense of a polysemous verb v is vi (1 < i < m) if t Mu(vi,ni) is largest among Ej~- Mu(vl,nj), Ej 212 • .. and Et~ Mu(v,~,nj). Here, t is the number of nouns which co-occur with v within the five-word distance. 6 Experiment This section describes an experiment conducted to evaluate the performance of our method. 6.1 Data The data we have used is 1989 Wall Street Journal (WSJ) in A C L / D C I CD-ROM which consists of 2,878,688 occurrences of part-of-speech tagged words (Brill, 1992). The inflected forms of the same nouns and verbs are treated as single units. For example, &apos;book&apos; and &apos;books&apos; are treated as single units. We obtained 5,940,193 word pairs in a window size of 5 words, 2,743,974 different word pairs. From these, we selected collocations of a verb and a noun. As a test data, we used 40 sets of verbs. We selected at most four senses for each verb, the best sense, from among the set of the Collins dictionary and thesaurus (McLeod, 1987), is determined by a human judge. 6.2 Results The results of the experiment are shown in Table 2, Table 3 and Table 4. In Table 2"
E99-1028,P94-1020,0,0.0131135,"earning algorithm for disambiguating verbal word senses using term weight learning. In our method, collocations which characterise every sense are extracted using similarity-based estimation. For the results, term weight learning is performed. Parameters of term weighting are then estimated so as to maximise the collocations which characterise every sense and minimise the other collocations. The resuits of experiment demonstrate the effectiveness of the method. 1 Introduction One of the major approaches to disambiguate word senses is supervised learning (Gale et al., 1992), (Yarowsky, 1992), (Bruce and Janyce, 1994), (Miller et al., 1994), (Niwa and Nitta, 1994), (Luk, 1995), (Ng and Lee, 1996), (Wilks and Stevenson, 1998). However, a major obstacle impedes the acquisition of lexical knowledge from corpora, i.e. the difficulties of manually sensetagging a training corpus, since this limits the applicability of many approaches to domains where this hard to acquire knowledge is already available. This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our approach, an overlapping clustering algorithm based on Mutual information-based (Mu) te"
E99-1028,H94-1046,0,0.0307821,"mbiguating verbal word senses using term weight learning. In our method, collocations which characterise every sense are extracted using similarity-based estimation. For the results, term weight learning is performed. Parameters of term weighting are then estimated so as to maximise the collocations which characterise every sense and minimise the other collocations. The resuits of experiment demonstrate the effectiveness of the method. 1 Introduction One of the major approaches to disambiguate word senses is supervised learning (Gale et al., 1992), (Yarowsky, 1992), (Bruce and Janyce, 1994), (Miller et al., 1994), (Niwa and Nitta, 1994), (Luk, 1995), (Ng and Lee, 1996), (Wilks and Stevenson, 1998). However, a major obstacle impedes the acquisition of lexical knowledge from corpora, i.e. the difficulties of manually sensetagging a training corpus, since this limits the applicability of many approaches to domains where this hard to acquire knowledge is already available. This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our approach, an overlapping clustering algorithm based on Mutual information-based (Mu) term weight learning betw"
E99-1028,C94-1049,0,0.110164,"senses using term weight learning. In our method, collocations which characterise every sense are extracted using similarity-based estimation. For the results, term weight learning is performed. Parameters of term weighting are then estimated so as to maximise the collocations which characterise every sense and minimise the other collocations. The resuits of experiment demonstrate the effectiveness of the method. 1 Introduction One of the major approaches to disambiguate word senses is supervised learning (Gale et al., 1992), (Yarowsky, 1992), (Bruce and Janyce, 1994), (Miller et al., 1994), (Niwa and Nitta, 1994), (Luk, 1995), (Ng and Lee, 1996), (Wilks and Stevenson, 1998). However, a major obstacle impedes the acquisition of lexical knowledge from corpora, i.e. the difficulties of manually sensetagging a training corpus, since this limits the applicability of many approaches to domains where this hard to acquire knowledge is already available. This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our approach, an overlapping clustering algorithm based on Mutual information-based (Mu) term weight learning between a verb and a noun is"
E99-1028,W97-0322,0,0.678304,"Missing"
E99-1028,P98-2228,0,0.0148482,"tions which characterise every sense are extracted using similarity-based estimation. For the results, term weight learning is performed. Parameters of term weighting are then estimated so as to maximise the collocations which characterise every sense and minimise the other collocations. The resuits of experiment demonstrate the effectiveness of the method. 1 Introduction One of the major approaches to disambiguate word senses is supervised learning (Gale et al., 1992), (Yarowsky, 1992), (Bruce and Janyce, 1994), (Miller et al., 1994), (Niwa and Nitta, 1994), (Luk, 1995), (Ng and Lee, 1996), (Wilks and Stevenson, 1998). However, a major obstacle impedes the acquisition of lexical knowledge from corpora, i.e. the difficulties of manually sensetagging a training corpus, since this limits the applicability of many approaches to domains where this hard to acquire knowledge is already available. This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our approach, an overlapping clustering algorithm based on Mutual information-based (Mu) term weight learning between a verb and a noun is applied to a set of verbs. It is preferable that Mu is not lo"
E99-1028,C92-2070,0,0.117809,"bes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our method, collocations which characterise every sense are extracted using similarity-based estimation. For the results, term weight learning is performed. Parameters of term weighting are then estimated so as to maximise the collocations which characterise every sense and minimise the other collocations. The resuits of experiment demonstrate the effectiveness of the method. 1 Introduction One of the major approaches to disambiguate word senses is supervised learning (Gale et al., 1992), (Yarowsky, 1992), (Bruce and Janyce, 1994), (Miller et al., 1994), (Niwa and Nitta, 1994), (Luk, 1995), (Ng and Lee, 1996), (Wilks and Stevenson, 1998). However, a major obstacle impedes the acquisition of lexical knowledge from corpora, i.e. the difficulties of manually sensetagging a training corpus, since this limits the applicability of many approaches to domains where this hard to acquire knowledge is already available. This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our approach, an overlapping clustering algorithm based on Mutual"
E99-1028,P95-1026,0,0.163316,"Missing"
E99-1028,C94-2122,1,\N,Missing
E99-1028,P93-1022,0,\N,Missing
E99-1028,C98-2223,0,\N,Missing
E99-1028,P96-1006,0,\N,Missing
E99-1028,P95-1025,0,\N,Missing
I05-1002,P00-1041,0,0.0368962,"racking. Another linguistic feature is a set of headline words. The basic idea to use headline words for topic tracking is that headline is a compact representation of the original story, which helps people to quickly understand the most important information contained in a story, and therefore, it may include words to understand what the story is about, what is characteristic of this story with respect to other stories, and hopefully include words related to both topic and event in the story. A set of headline words is automatically generated. To do this, we use a technique proposed by Banko [2]. It produces coherent summaries by building statistical models for content selection and surface realization. Another purpose of this work is to create Japanese corpus for topic tracking task. We used Mainichi Shimbun Japanese Newspaper corpus from Oct. to Dec. of 1998 which corresponds to the TDT3 corpus. We annotated these articles against the 60 topics which are defined by the TDT3. The rest of the paper is organized as follows. The next section provides an overview of existing topic tracking techniques. We then describe a brief explanation of a headline generation technique proposed by Ba"
I05-1002,P03-1004,0,0.031297,"i=2 To generate a headline, it is necessary to find a sequence of words that maximizes the probability, under the content selection and surface realization models, that it was generated from the story to be summarized. In formula (3), cross-validation is used to learn weights, α, β and γ for a particular story genre. 4 Extracting Linguistic Features and Tracking We explore two linguistically motivated restrictions on the set of words used for tracking: named entities and headline words. 4.1 Extracting Named Entities and Generating Headline Words For identifying named entities, we use CaboCha [7] for Japanese Mainichi Shimbun corpus, and extracted Person Name, Organization, Place, and Proper Name. Headline generation can be obtained as a weighted combination of the content and structure model log probabilities shown in formula (3). The system was trained on the 3 months Mainichi Shimbun articles((27,133 articles from Jan. to Mar. 1999) for Japanese corpus. We estimate α, β and γ in formula (3) using 5 cross-validation1. Fig. 1 illustrates sample output using Mainichi Shimbun corpus. Numbers to the right are log probabilities of the word sequence. 4.2 Tracking by Hierarchical Classific"
matsuyoshi-etal-2014-annotating,matsuyoshi-etal-2010-annotating,1,\N,Missing
matsuyoshi-etal-2014-annotating,S12-1035,0,\N,Missing
matsuyoshi-etal-2014-annotating,W08-0606,0,\N,Missing
matsuyoshi-etal-2014-annotating,J12-2001,0,\N,Missing
matsuyoshi-etal-2014-annotating,D08-1075,0,\N,Missing
matsuyoshi-etal-2014-annotating,C10-1076,0,\N,Missing
matsuyoshi-etal-2014-annotating,P11-1059,0,\N,Missing
matsuyoshi-etal-2014-annotating,S12-1039,0,\N,Missing
P06-2030,P96-1041,0,0.103617,"Missing"
P06-2030,P98-1041,0,\N,Missing
P06-2030,C98-1041,0,\N,Missing
P11-2097,E09-1006,0,0.0289552,"Missing"
P11-2097,W04-2214,0,0.601271,"Missing"
P11-2097,P07-1007,0,0.0354952,"Missing"
P11-2097,magnini-cavaglia-2000-integrating,0,0.293227,"For implementation, we used a supercomputer, SPARC Enterprise M9000, 64CPU, 1TB memory. 3 http://wordnet/princeton.edu/ 554 test data, i.e., the total number of words and senses, and the number of selected senses (Select S) that the classification accuracy of each domain was equal or higher than the result without word replacement. We used these senses as an input of MRW. There are no existing sense-tagged data for these 20 categories that could be used for evaluation. Therefore, we selected a limited number of words and evaluated these words qualitatively. To do this, we used SFC resources (Magnini and Cavaglia, 2000), which annotate WordNet 2.0 synsets with domain labels. We manually corresponded Reuters and SFC categories. Table 3 shows the results of 12 Reuters categories that could be corresponded to SFC labels. In Table 3, “Reuters” shows categories, and “IDSS” shows the number of senses assigned by our approach. “SFC” refers to the number of senses appearing in the SFC resource. “S & R” denotes the number of senses appearing in both SFC and Reuters corpus. “Prec” is a ratio of correct assignments by “IDSS” divided by the total number of “IDSS” assignments. We manually evaluated senses not appearing i"
P11-2097,P04-1036,0,0.0803278,"Missing"
P11-2097,J07-4005,0,0.0478862,"Missing"
P11-2097,rose-etal-2002-reuters,0,0.101451,"Missing"
P11-2097,D08-1105,0,0.0607237,"Missing"
P11-2097,H93-1061,0,\N,Missing
P13-2084,W99-0606,0,0.0644389,"emi-supervised ML, we apply SVM to the positive and unlabeled data, and add the classification results to the training data. The difference is that before adding the classification results, we applied the MisClassified data Detection and Correction (MCDC) technique to the results of SVM learning in order to improve classification accuracy obtained by the final classifiers. 2 Framework of the System The MCDC method involves category error correction, i.e., correction of misclassified candidates, while there are several strategies for automatically detecting lexical/syntactic errors in corpora (Abney et al., 1999; Eskin, 2000; Dickinson and 474 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474–478, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics PP 1 training N11 N selection Training data D U SVM Extraction of missclassified candidates MCDC N11 RC training training SVM N11 RC SVM classification CP1 test SVM N1 CN Estimation of error reduction CN1 SV label 䍴 NB label D1 Error candidates D 䠸Error candidates D2 Loss function MCDC N12 RC learning NB classification U䠸 N1 P CP D 䠸SV (Support vectors) Correction of Final resu"
P13-2084,P05-1040,0,0.060479,"Missing"
P13-2084,A00-2020,0,0.0433431,"e apply SVM to the positive and unlabeled data, and add the classification results to the training data. The difference is that before adding the classification results, we applied the MisClassified data Detection and Correction (MCDC) technique to the results of SVM learning in order to improve classification accuracy obtained by the final classifiers. 2 Framework of the System The MCDC method involves category error correction, i.e., correction of misclassified candidates, while there are several strategies for automatically detecting lexical/syntactic errors in corpora (Abney et al., 1999; Eskin, 2000; Dickinson and 474 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474–478, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics PP 1 training N11 N selection Training data D U SVM Extraction of missclassified candidates MCDC N11 RC training training SVM N11 RC SVM classification CP1 test SVM N1 CN Estimation of error reduction CN1 SV label 䍴 NB label D1 Error candidates D 䠸Error candidates D2 Loss function MCDC N12 RC learning NB classification U䠸 N1 P CP D 䠸SV (Support vectors) Correction of Final results … misclas"
P14-2040,P03-1004,0,0.0530151,"i → i)= 0 to avoid self transition. The transition probability from si to sj is then defined as follows: p(i → j) =  f (i→j)  , if Σf 6= 0   |S| X f (i→k)   k=1   0 , otherwise. 4.2 NTCIR data The data used in the NTCIR-3 multi-document summarization task is selected from 1998 to 1999 of Mainichi Japanese Newspaper documents. The gold standard data provided to human judges consists of FBFREE DryRun and FormalRun. Each data consists of 30 tasks. There are two types of correct summary according to the character length, “long” and “short”, All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. FBFREE DryRun data is used to tuning parameters, i.e., the number of extracted words according to the tf∗idf value, and the threshold value of KL-distance. The size that optimized the average Rouge-1(R-1) score across 30 tasks was chosen. As a result, we set tf∗idf and KL-distance to 100 and 0.104, respectively. We used FormalRun as a test data, and another set consisted of 218,724 documents from 19"
P14-2040,P10-1084,0,0.216363,"ed to evaluate the performance of the method. All documents were tagged by Tree Tagger (Schmid, 1995) and Stanford Named Entity Tagger 5 (Finkel et al., 2005). We used person name, organization and location for event detection, and noun words including named entities for topic detection. AQUAINT corpus6 which consists of 1,033,461 documents are used as a corpus in LDA and MACD. Table 2 shows Rouge-1 against unigrams. We can see from Table 2 that Rouge-1 obtained by our approach was also the best compared to the baselines. Table 2 also shows the performance of other research sites reported by (Celikylmaz and Hakkani-Tur, 2010). The top site was “HybHSum” by (Celikylmaz and Hakkani-Tur, 2010). However, the method is a semi-supervised technique that needs a tagged training data. Our approach achieves performance approaching the topperforming unsupervised method, “TTM” (Celikylmaz and Hakkani-Tur, 2011), and is competitive to “PYTHY” (Toutanoval et al., 2007) and “hPAM” (Li and McCallum, 2006). Prior work including “TTM” has demonstrated the usefulness of semantic concepts for extracting salient sentences. For future work, we should be able to obtain further advantages in efficacy in our topic detection and summarizat"
P14-2040,P05-3013,0,0.0132863,"tion by LDA (LDA): MRW is applied to the result of topic candidates detection by LDA and (iv) Topic Detection by LDA and MACD (LDA & MACD): MRW is applied to the result of topic detection by LDA and MACD only, i.e., the method does not include event detection. 3.2 Sentence extraction We recall that our hypothesis about key sentences in multiple documents is that they include topic and event words. Each sentence in the documents is represented using a vector of frequency weighted words that can be event or topic words. Like much previous work on extractive summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2008), we used Markov Random Walk (MRW) model to compute the rank scores for the sentences. Given a set of documents to be summarized, G = (S, E) is a graph reflecting the relationships between two sentences. S is a set of vertices, and each vertex si in S is a sentence. E is a set of edges, and each edge eij in E is associated with an affinity weight f (i → j) between sentences si and sj (i 6= j). The affinity weight is computed using cosine measure between the two sentences, si and sj . Two vertices are connected if their affinity weight is larger than 0 and we let f (i → i)="
P14-2040,P11-1050,0,0.0196796,"etection. AQUAINT corpus6 which consists of 1,033,461 documents are used as a corpus in LDA and MACD. Table 2 shows Rouge-1 against unigrams. We can see from Table 2 that Rouge-1 obtained by our approach was also the best compared to the baselines. Table 2 also shows the performance of other research sites reported by (Celikylmaz and Hakkani-Tur, 2010). The top site was “HybHSum” by (Celikylmaz and Hakkani-Tur, 2010). However, the method is a semi-supervised technique that needs a tagged training data. Our approach achieves performance approaching the topperforming unsupervised method, “TTM” (Celikylmaz and Hakkani-Tur, 2011), and is competitive to “PYTHY” (Toutanoval et al., 2007) and “hPAM” (Li and McCallum, 2006). Prior work including “TTM” has demonstrated the usefulness of semantic concepts for extracting salient sentences. For future work, we should be able to obtain further advantages in efficacy in our topic detection and summarization approach by disambiguating topic senses. Figure 3: Entropy against the # of topics and documents Method MRW Event LDA LDA & MACD Event & Topic Short R-1 .369 .625 .525 .630 .678 Long R-1 .454 .724 .712 .742 .744 Table 1: Sentence Extraction (NTCIR-3 test data) LDA and MACD."
P14-2040,W04-3247,0,0.040851,"tion, (iii) Topic Detection by LDA (LDA): MRW is applied to the result of topic candidates detection by LDA and (iv) Topic Detection by LDA and MACD (LDA & MACD): MRW is applied to the result of topic detection by LDA and MACD only, i.e., the method does not include event detection. 3.2 Sentence extraction We recall that our hypothesis about key sentences in multiple documents is that they include topic and event words. Each sentence in the documents is represented using a vector of frequency weighted words that can be event or topic words. Like much previous work on extractive summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2008), we used Markov Random Walk (MRW) model to compute the rank scores for the sentences. Given a set of documents to be summarized, G = (S, E) is a graph reflecting the relationships between two sentences. S is a set of vertices, and each vertex si in S is a sentence. E is a set of edges, and each edge eij in E is associated with an affinity weight f (i → j) between sentences si and sj (i 6= j). The affinity weight is computed using cosine measure between the two sentences, si and sj . Two vertices are connected if their affinity weight is larger th"
P14-2040,P05-1045,0,0.00586093,"㼠㼛㼜㼕㼏㼟㻕 㻠㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 㻢㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 㻤㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 㻜㻚㻜㻞 㻟㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 㻡㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 㻣㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 㻥㻜㻜㻌㻔㼠㼛㼜㼕㼏㼟㻕 R-1 .381 .402 .438 .426 .412 Method Event LDA & MACD R-1 .407 .428 HybHSum TTM .456 .447 Table 2: Comparative results (DUC2007 test data) 㻜㻚㻜㻜 㻞㻜㻜 㻟㻜㻜 㻠㻜㻜 㻡㻜㻜 㻢㻜㻜 㻣㻜㻜 㻤㻜㻜 㻥㻜㻜 Number of documents KL-distance to 80 and 0.9. The minimum entropy value was 0.050 and the number of topics and documents were 500 and 600, respectively. 45 tasks from DUC2007 were used to evaluate the performance of the method. All documents were tagged by Tree Tagger (Schmid, 1995) and Stanford Named Entity Tagger 5 (Finkel et al., 2005). We used person name, organization and location for event detection, and noun words including named entities for topic detection. AQUAINT corpus6 which consists of 1,033,461 documents are used as a corpus in LDA and MACD. Table 2 shows Rouge-1 against unigrams. We can see from Table 2 that Rouge-1 obtained by our approach was also the best compared to the baselines. Table 2 also shows the performance of other research sites reported by (Celikylmaz and Hakkani-Tur, 2010). The top site was “HybHSum” by (Celikylmaz and Hakkani-Tur, 2010). However, the method is a semi-supervised technique that n"
P19-1105,E09-1005,0,0.014256,"its effectiveness on neural machine translation. Since then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to the target word from parsed data. They tested 38 words containing two domains of Sports and Finance from the Reuters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervise"
P19-1105,W18-6219,0,0.0250587,"et al., 2017; Peters et al., 2018). Melamud et al. proposed a method called Context2Vec which learns each sense annotation in the training data by using a bidirectional LSTM trained on an unlabeled corpus (Melamud et al., 2016). More recently, Vaswani et al. introduced the first full-attentional architecture called Transformer. It utilizes only the self-attention mechanism and demonstrated its effectiveness on neural machine translation. Since then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to th"
P19-1105,H92-1073,0,0.289032,"thin which a game is played” described in the WordNet 3.1. This indicates that the meaning becomes a strong clue to assign a domain to the document. However, in the implicit semantic space created by using the neural language model such as the Word2Vec, a word is represented as one vector even if it has several senses. It is often the case that a word which is polysemous is not polysemous in a restricted subject domain. A restriction of the subject domain makes the problem of polysemy less problematic. However, even in texts from a restricted subject domain such as Wall Street Journal corpus (Douglas and Janet, 1992), one encounters quite a large number of polysemous words. Several authors focused on the problem and proposed a new type of deep contextualized word representation such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) that models not only syntax but also semantics including polysemies. Their methods work very well in many NLP tasks such as question answering and sentiment analysis, while their methods are unsupervised manners which they do not explicitly map each sense of a word to its domain. Motivated by solving this problem, we propose a method for text categorization that comp"
P19-1105,D12-1129,0,0.0230932,"ural machine translation. Since then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to the target word from parsed data. They tested 38 words containing two domains of Sports and Finance from the Reuters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervised framework. While conceptu"
P19-1105,N09-2059,0,0.0828961,"Missing"
P19-1105,P17-1001,0,0.181559,"usly categorize documents and predicts a predominant sense for each word. The experimental results using four benchmark datasets including RCV1 show that our method is comparable to the state-of-the-art categorization approach, especially our model works well for categorization of multi-label documents. 1 Introduction Text categorization has been intensively studied since neural network methods have attracted much attention. Most of the previous work on text categorization relies on the use of representation learning where the words are mapped to an implicit semantic space (Wang et al., 2015; Liu et al., 2017a). The Word2Vec is a typical model related to this representation (Mikolov et al., 2013). It learns a vector representation for each word and captures semantic information between words. Pre-training by using the model shows that it improves overall performance in many NLP tasks including text categorization. However, the drawback in the implicit representation is that it often does not work well on polysemous words. The sense of a word depends on the domain in which it is used. The same word can be used differently in different domains. Distributions of the senses of words are often highly s"
P19-1105,N15-1092,0,0.0262479,"ters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervised framework. While conceptually similar, our model differs from these approaches in that it is supervised learning by adopting existing domain-specific sense tags for creating the data. In the context of multi-task learning, many authors have attempted to apply it to NLP tasks (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). Liu et al. proposed adversarial multi-task learning which alleviates the shared and private latent feature spaces from interfering with each other (Liu et al., 2017b). Xiao et al. attempted multi-task CNN which introduces a gate mechanism to reduce the interference (Xiao et al., 2018). They reported that their approach can learn selection rules automatically and gain a great improvement over baselines through the experiments on nine text categorization datasets. Both of them focused on text categorization task only as a multi-task and used the word embeddings which are initialized wit"
P19-1105,magnini-cavaglia-2000-integrating,0,0.139985,"reported that their representation model significantly improves the state-of-the-art across six NLP problems. Similarly, Devlin proposed a model of deep contextualized word representation called BERT that can deal with syntax and semantics including polysemies (Devlin et al., 2018). Their methods attained amazing results in many NLP tasks. However, they do not explicitly map each sense of a word to its domain as their methods are unsupervised manner. Moreover, their model needs a large amount of corpus which leads to computational workload. Our model utilizes existing domain-specific senses (Magnini and Cavaglia, 2000; Magnini et al., 2002) as pseudo rough but explicit word representation data. It enables us to learn feature representations for both predominant senses and text categorization with a small amount of data. Similar to the text categorization task, the recent upsurge of deep learning techniques have also contributed to improving the overall performance on Word Sense Disambiguation (WSD) (Yuan et al., 1116 2016; Raganato et al., 2017; Peters et al., 2018). Melamud et al. proposed a method called Context2Vec which learns each sense annotation in the training data by using a bidirectional LSTM tra"
P19-1105,N15-1011,0,0.203496,"nts by using the result, and (iii) TRF-Delay-Multi, which is a model to start learning predominant sense model at first until the stable, and after that it adapts text categorization simultaneously. This is a mixed method of TRFSequential with fully separated training and TRFMulti with fully simultaneously training. We compared our method with these approaches. For multi-label text categorization by using RCV1 data, we chose XML-CNN as a baseline method because their method is simple but powerful and attained at the best or second best compared to the seven existing methods including Bow-CNN (Johnson and Zhang, 2015) on six 1112 Datasets RCV1 APW 20News AG N 502,383 46,032 10,228 95,700 D 13 5 3 3 L 2.4 1 1 1 W 565 397 404 390 S 992 586 563 562 Sˆ 3,800,197 877,400 46,410 124,885 M 38,645 9,206 3,409 31,900 ˆ M 3,831 1,497 82 222 Table 5: Data Statistics: N is the number of documents, D shows the number of domains, L is the average number of domains per document, W refers to the number of different target words, S is the number of different target senses, and Sˆ denotes the total number of target senses in the documents, M shows the average number of ˆ is the average number of documents per target sense."
P19-1105,E17-2068,0,0.038605,"nction Threshold value for Multi-label learning (λ) Gradient descent Value 100 100 32 ReLu 0.5 Adam Table 7: Model settings: The hyperparameters commonly used in all of the method. benchmark datasets where the label-set sizes are up to 670K (Liu et al., 2017a). Original XMLCNN is implemented by using Theano,6 while we implemented our method by Chainer.7 To avoid the influence of the difference in libraries, we implemented XML-CNN by Chainer and used it as a baseline. We followed the author-provided implementation in our Chainer’s version of XMLCNN. To make a fair comparison, we used fastText (Joulin et al., 2017) as a word-embedding tool with all of the methods. 3.3 Model settings and evaluation metrics The hyperparameters which are commonly used in all of the methods and their own estimated hyperparameters are shown in Tables 7 and 8, respec6 https://drive.google.com/file/d/1Wwy!MNkrJRXZM3WN ZNywa94c2-iEh 6U/view 7 https://chainer.org tively8 . These hyperparameters are optimized by using a hyperparameter optimization framework called Optuna9 . They were independently determined for each dataset. In the experiments, we run five times for each model and obtained the averaged performance. We used stand"
P19-1105,P04-1036,0,0.145943,"trubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to the target word from parsed data. They tested 38 words containing two domains of Sports and Finance from the Reuters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervised framework. While conceptually similar, our model differs from these approaches in that it is supervised learning by adopting existing domain-specific sense ta"
P19-1105,D14-1181,0,0.0164708,"Missing"
P19-1105,P14-1025,0,0.0231175,"arning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to the target word from parsed data. They tested 38 words containing two domains of Sports and Finance from the Reuters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervised framework. While conceptually similar, our model differs from these approaches in that it is supervised learning by adopting existing domain-specific sense tags for creating the data. In the context of multi-task learning, many authors have attempted to apply it to NLP tasks (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). Liu et al. proposed adversarial mult"
P19-1105,J07-4005,0,0.0563311,"t al., 2013). It learns a vector representation for each word and captures semantic information between words. Pre-training by using the model shows that it improves overall performance in many NLP tasks including text categorization. However, the drawback in the implicit representation is that it often does not work well on polysemous words. The sense of a word depends on the domain in which it is used. The same word can be used differently in different domains. Distributions of the senses of words are often highly skewed and a predominant sense of a word depends on the domain of a document (McCarthy et al., 2007; Jin et al., 2009). Suppose the noun word, “court”. The predominant sense of a word “court” would be different in the documents from the “judge/law” and “sports” domains as the sense of the former would be “an assembly (including one or more judges) to conduct judicial business” and the latter is “a specially marked horizontal area within which a game is played” described in the WordNet 3.1. This indicates that the meaning becomes a strong clue to assign a domain to the document. However, in the implicit semantic space created by using the neural language model such as the Word2Vec, a word is"
P19-1105,K16-1006,0,0.029505,"eudo rough but explicit word representation data. It enables us to learn feature representations for both predominant senses and text categorization with a small amount of data. Similar to the text categorization task, the recent upsurge of deep learning techniques have also contributed to improving the overall performance on Word Sense Disambiguation (WSD) (Yuan et al., 1116 2016; Raganato et al., 2017; Peters et al., 2018). Melamud et al. proposed a method called Context2Vec which learns each sense annotation in the training data by using a bidirectional LSTM trained on an unlabeled corpus (Melamud et al., 2016). More recently, Vaswani et al. introduced the first full-attentional architecture called Transformer. It utilizes only the self-attention mechanism and demonstrated its effectiveness on neural machine translation. Since then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sen"
P19-1105,S01-1005,0,0.303103,"Missing"
P19-1105,N18-1202,0,0.251333,"ng the neural language model such as the Word2Vec, a word is represented as one vector even if it has several senses. It is often the case that a word which is polysemous is not polysemous in a restricted subject domain. A restriction of the subject domain makes the problem of polysemy less problematic. However, even in texts from a restricted subject domain such as Wall Street Journal corpus (Douglas and Janet, 1992), one encounters quite a large number of polysemous words. Several authors focused on the problem and proposed a new type of deep contextualized word representation such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) that models not only syntax but also semantics including polysemies. Their methods work very well in many NLP tasks such as question answering and sentiment analysis, while their methods are unsupervised manners which they do not explicitly map each sense of a word to its domain. Motivated by solving this problem, we propose a method for text categorization that complements implicit representation by leveraging the predominant sense of a word. We propose a multi-task learning method based on the encoder structure of the neural network 1109 Proceedings of the 57t"
P19-1105,D17-1120,0,0.0393048,"ervised manner. Moreover, their model needs a large amount of corpus which leads to computational workload. Our model utilizes existing domain-specific senses (Magnini and Cavaglia, 2000; Magnini et al., 2002) as pseudo rough but explicit word representation data. It enables us to learn feature representations for both predominant senses and text categorization with a small amount of data. Similar to the text categorization task, the recent upsurge of deep learning techniques have also contributed to improving the overall performance on Word Sense Disambiguation (WSD) (Yuan et al., 1116 2016; Raganato et al., 2017; Peters et al., 2018). Melamud et al. proposed a method called Context2Vec which learns each sense annotation in the training data by using a bidirectional LSTM trained on an unlabeled corpus (Melamud et al., 2016). More recently, Vaswani et al. introduced the first full-attentional architecture called Transformer. It utilizes only the self-attention mechanism and demonstrated its effectiveness on neural machine translation. Since then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian"
P19-1105,rose-etal-2002-reuters,0,0.051848,"t is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to the target word from parsed data. They tested 38 words containing two domains of Sports and Finance from the Reuters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervised framework. While conceptually similar, our model differs from these approaches in that it is supervised learning by adopting existing domain-specific sense tags for creating the data. In the context of multi-task learning, many authors have attempted to apply it to NLP tasks (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). Liu et"
P19-1105,W04-0811,0,0.201748,"Missing"
P19-1105,D18-1548,0,0.0204701,"guation (WSD) (Yuan et al., 1116 2016; Raganato et al., 2017; Peters et al., 2018). Melamud et al. proposed a method called Context2Vec which learns each sense annotation in the training data by using a bidirectional LSTM trained on an unlabeled corpus (Melamud et al., 2016). More recently, Vaswani et al. introduced the first full-attentional architecture called Transformer. It utilizes only the self-attention mechanism and demonstrated its effectiveness on neural machine translation. Since then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004"
P19-1105,N15-1035,0,0.0235934,"ince then, the transformer has been successfully applied to many NLP tasks including semantic role labeling (Strubell et al., 2018) and sentiment analysis (Ambartsoumian and Popowich, 2018). To the best of our knowledge, this is the first approach for predicting domain-specific senses based on a transformer that is trained with multi-task learning. In the context of predominant sense prediction, several authors have attempted to use domainspecific knowledge to disambiguate senses and show that the knowledge outperforms generic supervised WSD (Agirre and Soroa, 2009; Faralli and Navigli, 2012; Taghipour and Ng, 2015). McCarthy et al. proposed a statistical method for assigning predominant noun senses (McCarthy et al., 2004, 2007). They find words with a similar distribution to the target word from parsed data. They tested 38 words containing two domains of Sports and Finance from the Reuters corpus (Rose et al., 2002). Similarly, Lau et al. (2014) proposed a fully unsupervised topic modeling-based approach to sense frequency estimation. Faralli and Navigli (2012) attempted to performing domain-driven WSD by a pattern-based method with minimally-supervised framework. While conceptually similar, our model d"
P19-1105,N18-2114,0,0.019585,"onceptually similar, our model differs from these approaches in that it is supervised learning by adopting existing domain-specific sense tags for creating the data. In the context of multi-task learning, many authors have attempted to apply it to NLP tasks (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). Liu et al. proposed adversarial multi-task learning which alleviates the shared and private latent feature spaces from interfering with each other (Liu et al., 2017b). Xiao et al. attempted multi-task CNN which introduces a gate mechanism to reduce the interference (Xiao et al., 2018). They reported that their approach can learn selection rules automatically and gain a great improvement over baselines through the experiments on nine text categorization datasets. Both of them focused on text categorization task only as a multi-task and used the word embeddings which are initialized with Word2Vec or GloVe vectors. Aiming at text categorization with relatively small amounts of training data, we demonstrated a predominant sense of a word is effective for text categorization in the framework of multi-task learning with domainspecific sense identification and text categorization"
P19-1105,N16-1174,0,0.143624,"Missing"
P19-1105,C16-1130,0,0.0334471,"Missing"
P19-1105,N16-1177,0,0.0271589,"lti learning model works well, especially in the cases that the number of training data per domain is small. 4 Related Work Deep learning techniques have been great successes for automatically extracting contextsensitive features from a textual corpus. Many authors have attempted to apply deep learning methods including CNN (Kim, 2014; Zhang et al., 2015; Wang et al., 2015; Zhang and Wallace, 2015; Zhang et al., 2017; Wang et al., 2017), the attention based CNN (Yang et al., 2016), bag-of-words based CNN (Johnson and Zhang, 2015), and the combination of CNN and recurrent neural network (RNN) (Zhang et al., 2016) to text categorization. Most of these approaches demonstrated that neural network models are powerful for learning effective features from textual input. However, most of them for learning word vectors only allow a single context-independent representation for each word even if it has several senses. Peters et al. addressed the issue and proposed a model of deep contextualized word representation called ELMo derived from a bidirectional LSTM (Peters et al., 2018). They reported that their representation model significantly improves the state-of-the-art across six NLP problems. Similarly, Devl"
P19-1105,P17-2024,0,0.0475577,"Missing"
P19-1105,P15-2058,0,0.0210112,"model to simultaneously categorize documents and predicts a predominant sense for each word. The experimental results using four benchmark datasets including RCV1 show that our method is comparable to the state-of-the-art categorization approach, especially our model works well for categorization of multi-label documents. 1 Introduction Text categorization has been intensively studied since neural network methods have attracted much attention. Most of the previous work on text categorization relies on the use of representation learning where the words are mapped to an implicit semantic space (Wang et al., 2015; Liu et al., 2017a). The Word2Vec is a typical model related to this representation (Mikolov et al., 2013). It learns a vector representation for each word and captures semantic information between words. Pre-training by using the model shows that it improves overall performance in many NLP tasks including text categorization. However, the drawback in the implicit representation is that it often does not work well on polysemous words. The sense of a word depends on the domain in which it is used. The same word can be used differently in different domains. Distributions of the senses of words"
P98-2207,C96-2154,0,0.0211091,"of switch board corpora is rather long and there are many keywords in the discourse. However, for a short discourse, there are few keywords 1272 in a short discourse. Yokoi also proposed a topic identification method using co-occurrence of words for topic identification (Yokoi et al., 1997). He classified each dictated sentence of news into 8 topics. In TV or Radio news, however, it is difficult to segment each sentence automatically. Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model (Sekine, 1996). However, if the statistical model is used for extraction of sentence candidates, we will obtain higher recognition accuracy. Some initial studies of transcription of broadcast news proceed (Bakis et al., 1997). However there are some remaining problems, e.g. speaking styles and domain identification. We conducted domain identification and keyword extraction experiment (Suzuki et al., 1997) for radio news. In the experiment, we classified radio news into 5 domains (i.e. accident, economy, international, politics and sports). The problems which we faced with are; 1. Classification of newspaper"
U14-1011,W06-3809,0,\N,Missing
W00-0404,W97-0703,0,0.0553415,"dependent (Boguraev and Kennedy. 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identi~,ing certain passages(typically sentences) which, by some metric, are deemed to be the most representative, of the document's content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based oll word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzil~v and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al.. 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 19"
W00-0404,P99-1071,0,0.0191826,"ds and surprising features by supplementing the corpus statistics (Allan and Papka, 1998) (Papka et al., 1999). One of the purpose of this study is to make a distinction between an event aald an event class using surprising features. Here event class features are broad news areas such as politics, death, destruction and ~,'~fare. The idea is considered to be necessary to obtain higti accuracy, while Allan claims that the surprising words do not provide a broad enough coverage to capture all documents on the event. A more recent approach dealing with this problem is Barzilav et. al's approach (Barzilay et al., 1999). They used paraphrasing rules which are maaaually derived from the result of syntactic analysis to identify theme intersection and used language generation to reformulate them as a coherent, summary. While promising to obtain high accuracy: the result of summarization task has not been reported. Like Mani and Barzil~,'s techniques, our approach focuses on the problem that how to identi~"" differences and similarities across documents, rather than the problem that how to form the actual summar:,, (Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). However, while Barzilav's app"
W00-0404,H92-1022,0,0.0191326,"Missing"
W00-0404,A97-1043,1,0.909799,"Missing"
W00-0404,E99-1011,0,0.0397878,"Missing"
W00-0404,W97-0713,0,0.0260191,". based summarization to date is hardly domain or genre-independent (Boguraev and Kennedy. 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identi~,ing certain passages(typically sentences) which, by some metric, are deemed to be the most representative, of the document's content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based oll word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzil~v and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al.. 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and"
W00-0404,J98-3005,0,0.0726732,"Missing"
W00-0404,X98-1028,0,0.0237583,"ibution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzil~v and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al.. 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 1999). Their technique seems sensible. However, as she admits, (i) the order the information should not only depend on topic covered, (ii) background information that helps clari~"" related information should be placed first. More seriously, as Barzilay and Mani claim, summarization of multiple documents requires information about similarities and differences a c r o s s documents. Therefore it is d"
W00-0404,W97-0710,0,0.0425374,"e entities and facts in a document, while work on template-driven, knowledge. based summarization to date is hardly domain or genre-independent (Boguraev and Kennedy. 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identi~,ing certain passages(typically sentences) which, by some metric, are deemed to be the most representative, of the document's content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based oll word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzil~v and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al.. 1999). Their method first summarizes each docume"
W00-0404,C96-2166,0,0.031127,"tification and extraction of certain core entities and facts in a document, while work on template-driven, knowledge. based summarization to date is hardly domain or genre-independent (Boguraev and Kennedy. 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identi~,ing certain passages(typically sentences) which, by some metric, are deemed to be the most representative, of the document's content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based oll word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzil~v and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al."
W08-2005,W06-1664,0,0.197552,"Missing"
W08-2005,H05-1052,0,0.0826882,"Missing"
W08-2005,W06-3811,0,0.0385841,"Missing"
W08-2005,C00-1026,0,0.0814695,"Missing"
W08-2005,J90-1003,0,0.126745,"Missing"
W08-2005,C04-1036,0,0.0315107,"Missing"
W08-2005,J05-4002,0,0.0298456,"Missing"
W08-2005,P90-1034,0,0.489082,"Missing"
W08-2005,C02-1114,0,0.0656345,"Missing"
W08-2005,P99-1004,0,0.131309,"Missing"
W08-2005,P98-2127,0,0.262996,"Missing"
W08-2005,C98-2122,0,\N,Missing
W09-3205,P90-1034,0,0.412835,"th selectional preferences. Korhonen et al. (2003) used verb–frame pairs to cluster verbs into Levin-style semantic classes (Korhonen, 2003). They used the Information Bottleneck, and classified 110 test verbs into Levin-style classes. They had a focus on the interpretation of 3 Selectional Preferences A major approach on word clustering task is to use distribution of a word in a corpus, i.e., words are classified into classes based on their distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words (Hindle, 1990; Lin, 1998; Dagan, 1999). Like previous work on verb classification, we used subcategorization frame distributions with selectional preferences to calculate similarity between verbs (Schulte, 2008). We used the EDR dictionary of selectional preferences consisting of 5,269 basic Japanese verbs and the EDR concept dictionary (EDR, 1986). For selectional preferences, the dictionary has each concept of a verb, the group of possible co-occurrence surface-level case particles, the types of concept relation label that correspond to the surface-level case as well as the range of possible concepts tha"
W09-3205,W02-2014,0,0.0456071,"Missing"
W09-3205,P03-1009,0,0.0626817,"Missing"
W09-3205,P03-1004,0,0.0916782,"Missing"
W09-3205,P99-1004,0,0.0198912,"rns of verbs and each value of the dimension is the frequency of each pattern. 5. α-skew divergence (α div.): The α-skew divergence measure is a variant of KL, and is defined as: αdiv(x, y) 2. The Cosine measure based on probability of relative frequencies (rfCos): The differences between the cosine and the value based on relative frequencies of verb frames with selectional preferences are the values of each dimension, i.e., the former are frequencies of each pattern and the latter are the fraction of the total number of verb frame patterns belonging to the verb. = KL(y, α · x + (1 − α) · y). Lee (1999) reported the best results with α = 0.9. We used the same value. 6. The Jensen-Shannon (JS): The JensenShannon is a measure that relies on the assumption that if x and y are similar, they are close to their average. It is defined as: 2 We report Add-one smoothing results in the evaluation, as it was better than Witten and Bell smoothing. 34 JS(x, y) = carried out through an iterative optimization (minimization) of the objective function Jm with the update of membership degree uij and the cluster centers cj . Jm is defined as: 1 x+y x+y [KL(x, ) + KL(y, )]. 2 2 2 All measures except Cos and rfC"
W09-3205,P98-2127,0,0.158489,"preferences. Korhonen et al. (2003) used verb–frame pairs to cluster verbs into Levin-style semantic classes (Korhonen, 2003). They used the Information Bottleneck, and classified 110 test verbs into Levin-style classes. They had a focus on the interpretation of 3 Selectional Preferences A major approach on word clustering task is to use distribution of a word in a corpus, i.e., words are classified into classes based on their distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words (Hindle, 1990; Lin, 1998; Dagan, 1999). Like previous work on verb classification, we used subcategorization frame distributions with selectional preferences to calculate similarity between verbs (Schulte, 2008). We used the EDR dictionary of selectional preferences consisting of 5,269 basic Japanese verbs and the EDR concept dictionary (EDR, 1986). For selectional preferences, the dictionary has each concept of a verb, the group of possible co-occurrence surface-level case particles, the types of concept relation label that correspond to the surface-level case as well as the range of possible concepts that may fill"
W09-3205,W06-1664,0,0.0355403,"Missing"
W09-3205,H05-1052,0,0.0812655,"Missing"
W09-3205,C02-1114,0,0.0336926,"Missing"
W09-3205,W06-3811,0,0.0197118,"Missing"
W09-3205,P93-1024,0,0.535616,"Missing"
W09-3205,P99-1014,0,0.016823,"class corresponds to a sense of each verb. There are 87 classes for a set from 2007, and 152 classes for a set from 1991 2007. The examples of the test verbs and their senses are shown in Table 2. For evaluation of verb classification, we used the precision, recall, and F-score, which were defined by (Schulte, 2000), especially to capture how many verbs does the algorithm actually detect more than just the predominant sense. For comparison against polysemies, we utilized the EM algorithm which is widely used as a soft clustering technique (Schulte, 2008). We followed the method presented in (Rooth, 1999). We used a probability distribution over verb frames with selectional preferences. The initial probabilities m 2.0 1.5 – λ 0.09 0.07 – C 74 74 87 Prec .815 .700 .308 Rec .483 .477 .903 F .606 .567 .463 Table 4: Results against each measure Measure cos rfcos L1 KL α div. JS EM m 3.0 2.0 2.0 2.0 2.0 1.5 – λ 0.02 0.04 0.04 0.09 0.04 0.03 – C 74 74 74 74 74 74 87 Prec .660 .701 .680 .815 .841 .804 .308 Rec .517 .488 .500 .483 .471 .483 .903 F .580 .576 .576 .606 .604 .603 .463 were often determined randomly. We set the initial probabilities by using the result of the standard k-means. For k-means"
W09-3205,C00-2108,0,0.0968058,"Missing"
W09-3205,P08-1057,0,0.0250423,"Missing"
W09-3205,W97-0803,0,0.0912603,"Missing"
W09-3205,C02-1120,0,0.0569009,"Missing"
W10-2316,W08-1106,0,0.0124923,"he results evaluated by “ROUGE” were worse than those of “cos” at any approaches. One reason is that the difference of summarization technique, i.e., our work is extractive summarization, while the gold standard data provided by NTCIR3 SUMM is the abstracts written by human professionals. As a result, a large number of words in a candidate summary are extracted by our approaches. For future work, it is necessary to extend our method to involve paraphrasing for extracted key sentences to reduce the gap between automatically generated summaries and humanwritten abstracts (Barzilay et al., 1993; Carenini and Cheung, 2008). It is interesting to note how our approach affects for the number of sentences as an input. Figure 1 illustrates the results of summary “long” with evaluated ROUGE score. We can see from Figure 1 that our approach is more robust than k-means and the MRW model, even for a large number of input data. We have seen the same observations from other three results, i.e., the results of short and long with evaluated cos and short with evaluated ROUGE. We recall that the cluster number k is set to the square root of the sentence number. We tested different number of k to see how the cluster number Ta"
W10-2316,W04-3247,0,0.0268407,"n With the exponential growth of information on the Internet, it is becoming increasingly difficult for a user to read and understand all the materials from a series of large-scale document streams that is potentially of interest. Multi-document summarization is an issue to attack the problem. It differs from single document summarization in that it is important to identify differences and similarities across documents. Graph-based ranking methods, such as PageRank (Page et al., 1998) and HITS (Kleinberg, 1999) have recently applied and been successfully used for multi-document summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2005). Given a set of documents, the model constructs graph consisting vertices and edges where vertices are sentences and edges reflect the relationships between sentences. The model then applies a graph-based ranking method to obtain the rank scores for the sentences. Finally, the sentences with large rank scores are chosen into the summary. However, when they are strung together, the resulting summary still contains much This paper focuses extractive summarization, and present a method for detecting key sentences from documents that discuss the same event. Like Wan et"
W10-2316,N03-1020,0,0.160023,"Missing"
W10-2316,P05-3013,0,0.0158006,"growth of information on the Internet, it is becoming increasingly difficult for a user to read and understand all the materials from a series of large-scale document streams that is potentially of interest. Multi-document summarization is an issue to attack the problem. It differs from single document summarization in that it is important to identify differences and similarities across documents. Graph-based ranking methods, such as PageRank (Page et al., 1998) and HITS (Kleinberg, 1999) have recently applied and been successfully used for multi-document summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2005). Given a set of documents, the model constructs graph consisting vertices and edges where vertices are sentences and edges reflect the relationships between sentences. The model then applies a graph-based ranking method to obtain the rank scores for the sentences. Finally, the sentences with large rank scores are chosen into the summary. However, when they are strung together, the resulting summary still contains much This paper focuses extractive summarization, and present a method for detecting key sentences from documents that discuss the same event. Like Wan et al.’s approach, we applied"
W10-2316,N07-1013,0,0.0345965,"Missing"
W10-2316,P99-1071,0,\N,Missing
W12-4702,W97-0713,0,0.183516,"gorithm to create clusters of similar sentences. Next section provides an overview of the existing works regarding discourse relation. Section 3 describes the framework of our system. In Section 4, we report experimental results and conclude our discussion with some direction for further works. 2 Previous Work Since large scale machine readable textual corpus has become available, many techniques have been proposed to harvest vital information from documents using discourse relations analysis. Up until now, discourse relations have benefit various NLP applications such as text summarization ((Marcu, 1997), (Zhang et al., 2002), (Radev et al., 2004), (Uzêda et al., 2009), (Louis et al., 2012)), question answering ((Litkowski, 2002), (Verbe and Oostdijk, 2007)) and natural language generation ((Theune, 2002), (Piwek et al., 2010)). In text summarization, discourse relations are used to produce optimum ordering of sentences in a document, and remove redundancy from generated summaries. One of the well known works is CST based text summarization (Zhang et al., 2002). In this work, sentences with most relations in the documents are considered to be important. They proposed an enhancement of text su"
W12-4702,W10-4327,0,0.0673349,"Missing"
W12-4702,N10-1048,0,0.0353081,"Missing"
W12-4702,W10-2312,0,0.0616946,"Missing"
W12-4702,C94-1079,0,0.0865402,"of words in a sentence, respectively. The feature with higher overlap ratio is set to 1, and 0 for lower value. 22 3. Longest Common Substring Longest Common Substring metric extracts the maximum length of matching word sequence against S1, given two text span, S1 and S2, . lcs ( S1 )  Length ( MaxComSubstring ( S1 , S 2 )) Length ( S1 ) (4) The metric value shows if both sentences are using the same phrase or term, which will benefit the identification of Overlap or Subsumption. 4. Ratio overlap of grammatical relationship for S1 We used a broad-coverage parser of English language, MINIPAR (Lin, 1994) to parse S1 and S2, and extract the grammatical relationship between words in the text span. Here we extracted the number of surface subject and the subject of verb (subject) and object of verbs (object). We then compared the grammatical relationship in S1 which occur in S2, compute as follows: Subj _ ove( S1 )  # comSubj ( S1 , S 2 ) # Subj ( S1 ) (5) Obj _ ove( S1 )  # comObj( S1 , S 2 ) # Obj( S1 ) (6) The ratio value describes whether S2 provides information regarding the same entity of S1 , i.e. Change of Topics. We also compared the subject in S1 with noun of S2 to examine if S1 is di"
W12-4702,P05-1045,0,0.0175473,"Missing"
W12-4702,fillmore-etal-2002-framenet,0,0.060191,"Missing"
W12-4702,radev-etal-2004-cst,0,\N,Missing
W12-4702,J03-4002,0,\N,Missing
W98-1509,C96-1069,1,0.888578,"Missing"
W98-1509,A94-1027,0,0.0668157,"Missing"
W98-1509,H92-1022,0,0.0284072,"Missing"
W98-1509,A97-1043,1,\N,Missing
