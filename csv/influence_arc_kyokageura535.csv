2007.mtsummit-papers.60,P07-2002,1,0.410193,"nputs an English text, and clicks the “search” button, the system outputs the idiom candidates with their meaning (in Japanese). By moving the mouse over an output idiom, the matched parts of the input text in the input area are emphasised. Figure 4 shows that “take the plunge” and “have one’s eye on” were detected for the input: “I decided to take the wild plunge and buy the car I had my eye on.” Integration to the translation editor environment In addition to the independent system interface, we incorporated the idiom look-up system into the integrated translation editor environment QRedit (Abekawa and Kageura, 2007). Figure 5 shows the interface in which automatic idiom lookup functions within the integrated environment. The screen shot shows that the idiom entry “(with) one’s tongue in one’s cheek” matches the sentence “He said that with his big fat tongue in his big fat cheek.” Evaluation We carried out evaluation experiments in order to observe the overall performance of the system, as well as the following three aspects: (1) the effect of standardisation of words; (2) the effect of the POS-based filtering; (3) the overall performance of the system. Experimental setup We observed how many correct idio"
2007.mtsummit-papers.60,2006.eamt-1.7,0,0.0260057,"Missing"
2007.mtsummit-papers.60,P06-2095,0,0.0580952,"Missing"
2007.mtsummit-papers.60,J93-1007,0,0.133809,"tongue in his big fat cheek. Although many available machine translation systems successfully detect the idiomatic expression “with one’s tongue in one’s cheek” in (1), none among those we checked (e.g. Excite, 2005; Fujitsu, 2005; LogoVista, 2005; Sharp, 2004; Toshiba, 2005)1 could properly translate (2). Most existing methods for looking up idioms cannot deal with the rich variations in idioms, that are abundant in ordinary texts2 . In the field of natural language processing (NLP), much research has been carried out into the automatic extraction of collocations and idioms (e.g. Piao, 2006; Smadja, 1993; Widdows and Dorow, 2005), but not much work has been devoted to the automatic matching of idiom entries to their occurrences in running texts. As translators are basically satisfied with the idioms provided in existing high-quality dictionaries but frustrated with the poor look-up functions, 1 Sharp (2004) can detect some gapped idiom occurrences, but it fails to detect complex idiom variations. 2 For instance, our rough survey of online documents revealed that the idiom “hang on” in its basic form, “hang on” with insertion, and “hang on” with passivisation (with or without insertion) occur"
2007.mtsummit-papers.60,W05-1006,0,0.0209282,"big fat cheek. Although many available machine translation systems successfully detect the idiomatic expression “with one’s tongue in one’s cheek” in (1), none among those we checked (e.g. Excite, 2005; Fujitsu, 2005; LogoVista, 2005; Sharp, 2004; Toshiba, 2005)1 could properly translate (2). Most existing methods for looking up idioms cannot deal with the rich variations in idioms, that are abundant in ordinary texts2 . In the field of natural language processing (NLP), much research has been carried out into the automatic extraction of collocations and idioms (e.g. Piao, 2006; Smadja, 1993; Widdows and Dorow, 2005), but not much work has been devoted to the automatic matching of idiom entries to their occurrences in running texts. As translators are basically satisfied with the idioms provided in existing high-quality dictionaries but frustrated with the poor look-up functions, 1 Sharp (2004) can detect some gapped idiom occurrences, but it fails to detect complex idiom variations. 2 For instance, our rough survey of online documents revealed that the idiom “hang on” in its basic form, “hang on” with insertion, and “hang on” with passivisation (with or without insertion) occur in roughly the same freque"
2007.mtsummit-papers.60,W03-1807,0,\N,Missing
2009.mtsummit-posters.14,C02-2020,0,0.750124,"Missing"
2009.mtsummit-posters.14,claveau-2008-automatic,0,0.0239775,"detected transliteration to bilingual resources (see Section 4.1). 3.4.2 Identifying scientific compounds We extracted scientific compounds using a list of 606 medical suffixes and prefixes used in English2 . The process is quite simple: we compile regular expressions for every suffix and prefix and have them matched on the bilingual dictionaries used (see section 4.1). Words extracted are kept with their Japanese translation. Such pairs are then used as anchor points in the alignment process. This list, dedicated to the English language can easily be adapted to French (in accordance with the Claveau (2008) observation). We draw our inspiration from this work to write some simple conversion rules. For example, the -y suffix in English (as in psychology) corresponds to the -ie suffix in French (as in psychologie). After adapting rules to the French language, we performed the same extraction process than with English on the French dictionary, with the converted list of prefixes/suffixes. Some suffixes and prefixes are very productive (especially the a- prefix) and corresponding extracted terms are not necessarily built from this root. All suffixes and prefixes generating more than 1,000 pairs on b"
2009.mtsummit-posters.14,I05-1062,1,0.938445,"Missing"
2009.mtsummit-posters.14,J93-1003,0,0.489932,"is process. Our implementation consists of the following four steps: 1. Building Context-Vector For each lexical unit i, we collect all lexical units in its context and count the number of times these lexical units appear in a window of n words around i. We obtain, for each lexical unit i of the source and the target languages, a context-vector vi which collects the set of co-occurring units j associated with the number of times that j and i occur together. i ¬i j a = occ(i, j) c = occ(¬i, j) ¬j b = occ(i, ¬j) d = occ(¬i, ¬j) Table 1: Contingency table for terms i and j Log Likelihood, eq. 1 (Dunning, 1993), computed from a contingency table (see table 1). λ(i, j) = a log(a) + b log(b) + c log(c) + d log(d) +(a + b + c + d) log(a + b + c + d) −(a + b) log(a + b) − (a + c) log(a + c) −(b + d) log(b + d) − (c + d) log(c + d) (1) 2.2 Results of the direct approach 2. Normalisation of Context-Vector In order to identify specific words in the lexical context and to reduce word frequency artifacts, we normalise context-vectors using an association score. Context-vectors therefore record the association pattern of a word and its neighbours. 3. Translation of the vector Using a bilingual dictionary, we"
2009.mtsummit-posters.14,2005.jeptalnrecital-long.7,0,0.0277188,"e transliterations reflect specialised vocabulary used in document. Finally, Japanese transliteration are easy to identify, since they are written using a set of symbols mostly dedicated to foreign terms, the katakanas. Japanese transliteration are for the most adapted from English, but can be aligned with French term, since French and English share a large common vocabulary. For example, the Japanese term インスリン / i-n-su-ri-n can be aligned with English insulin and with French insuline. We also studied scientific compounds. They are words, in French and in English, built with specifics roots (Namer, 2005). (Claveau, 2007), studying automatic translation of medical vocabulary observe that biomedical terms are built on common Greek and Latine roots, and their derivations are consistent. These compounds are characteristic of a specialised vocabulary, especially in medical documents (Lovis et al., 1997; Namer and Zweigenbaum, 2004). Therefore, they seem to be relevant anchor points in the corpora we are using. Moreover, they can easily be identified from their morphology in French and English. 3.3 Improvement The main idea of this paper is to introduce depth in flat context-vectors, relying on sel"
2009.mtsummit-posters.14,P99-1067,0,0.96696,"arable corpora to show that those elements can efficiently be used to improve the quality of the alignment. 1 Introduction We are currently working on French- and EnglishJapanese term alignment from comparable corpora. Much work has been carried out on bilingual lexicon extraction, used to automatically update linguistic resources. This is especially interesting for specialised vocabulary and is needed by translators since regular bilingual dictionaries can not catch up with the growth of terminology. More specifically since the 90s, studies have focused on extraction from comparable corpora (Rapp, 1999; Fung, 1998). This is partly because there is a lack of parallel corpora, especially for language pairs not involving English. This holds even for language pairs such as French and Japanese, both of which have substantial number of speakers. In contrast, comparable corpora, defined as “sets of texts in different languages that are not translations of each other&quot; (Bowker and Pearson, 2002), are more readily available for wider range of language pairs. It is therefore natural to explore comparable corpora for bilingual term alignment. Kyo Kageura Graduate School of Education University of Tokyo"
2009.mtsummit-posters.22,P07-2002,1,0.866754,"of documents every day, such as blogs, Wikipedia articles, open source software manuals, documents on nongovernmental organization (NGO) activities, and so on. These translations are read for pleasure, for practical purposes, for language learning, and many other reasons. Needless to say, volunteer translators contribute a great deal to the sharing and spreading of information around the world. Consequently, supporting their activities is a very important research issue. Volunteer translators translate a large number of documents everyday. However, they lack proper translation support tools (Abekawa and Kageura, 2007a). Thus, providing a good supporting environment should be of great assistance in improving volunteer translators’ efficiency and increasing the level of enjoyment they experience in translating. This is the motivation for our work in this paper. 2 Hosting volunteer translators Abekawa and Kageura (2007a) have developed a translation aid editor, QRedit, which has been experimentally provided to a limited number of volunteer translators. They report that QRedit is very effective for aiding their work, as described in Section 8. Based on the success of this translation aid editor, we have devel"
2009.mtsummit-posters.22,macklovitch-2006-transtype2,0,0.152808,"cement of translation candidate display [inside, left, right] of the SL area • Synchronized scroll [both directions, source→target, target→source, none] Figure 7 shows an example of a customized QRedit window. (See Figure 5 for a default window.) 8 User response As of 3 July, 2009 – three months after we made MNH and QRedit publicly available – there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. As quantitatively evaluating the benefit of using translation aid systems is technically a difficult task (cf. (Macklovitch, 2006)), and as we are dealing than before. We have not yet been able to obtain responses from group users, because they have only started using QRedit recently and thus have not accumulated sufficient experience to give an informed judgment on the system. We also have not been able to evaluate the usability of MNH because it is still under development and we are now improving its usability based on comments and suggestions from users. Figure 7: Customized QRedit window with volunteer translators who are not working on a “time is money” basis but rather wish to reduce the subjective burden of transl"
2009.mtsummit-posters.22,2001.mtsummit-papers.59,0,0.0378705,"uments; work that is concerned with hosting translated documents, often multilingually; and work that is addressed at aiding translators and translation communities. There are too many joint or collaborative online translation projects to mention here. GlobalVoices Online4 is perhaps one of the most well known, along with TUP5 (Translators United for Peace). Most projects do not provide translation aid facilities or collaborative working environments. They are rather projects defined by interested groups of people, using existing facilities. An example of the second category is Yakushite.net (Shimohata et al., 2001). It provides a collaborative translation environment in which users can use MT for translation, while contributing to collaborative terminology augmentation for the improvement of MT. Except for providing the MT engine, the translation aid functions are weak. Worldwide Lexicon (McConnell, 2007) is another example. Within the project a variety of mechanisms are provided that facilitate the sharing of translated documents world wide, with which one can (i) detect translated texts, if there are any; (ii) translate by oneself; (iii) subscribe to an RSS feed for translation; and (iv) use machine t"
2009.mtsummit-posters.22,P03-1010,1,0.803051,"ions and make derivative works public under certain conditions. Specifically, MNH uses four Creative Attribution Non-Commercial Share Alike This license lets others remix, tweak, and build upon the translator’s work non-commercially, as long as they credit the translator and license their new creations under identical terms. Translators can also use other licenses that are similar to the Creative Commons licenses listed above. In this way, translators can legally share their translations on MNH. These shared translations are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). Currently, MNH has a simple bilingual concordancer as shown in Figure 4. We plan to extend this concordancer in our future work because a bilingual concordancer is a very important tool for translation (Macklovitch et al., 2009). 6 High quality, comprehensive language resources Dictionaries and the web are the two main language resources that online volunteer translators use during translation. MNH, in cooperation with Sanseido, provides the “Grand Concise English Japanese Dictionary” (1) They are native-speakers of the target language (TL). (2) Most of them do not have a native-level comman"
2009.tc-1.4,P07-2002,1,0.458056,"Missing"
2011.tc-1.12,2009.mtsummit-posters.22,1,0.484163,"re eﬀective in detecting new term pairs. On the basis of what has been reported here and what is currently being examined, and upon consultation with translators, we will carry out practical evaluations of the system in order to deﬁne the most useful way of providing potential users with the system and the term pair candidates obtained using the system. QRpotato and the term pair candidates obtained from the Web using QRpotato will ultimately be provided through the translationaid platfrom Minna no Hon’yaku (MNH: translation of/by/for all, available at http://transaid.jp/) that we are running [12]. From a diﬀerent point of view, we are interested in determining how eﬀective QRpotato is in crawling the Web for Korean-English and Chinese-English technical term pairs. As both Korean and Chinese use character sets which are diﬀerent from English, and adopt the same conventions as Japanese in indicating English translations of technical terms in certain types of documents, QRpotato will probably work as eﬀectively as it does when crawling the Web for Japanese-English term pairs. But this needs to be properly evaluated if QRpotato and the Korean-English and Chinese-English term pairs are to"
2012.eamt-1.57,2002.tc-1.7,0,0.0680251,"in the software domain. More specific studies have been undertaken to identify those rules which have the greatest impact on the usability of MT output (e.g., O’Brien and Roturier, 2007). © 2012 European Association for Machine Translation. 237 Overwhelmingly, controlled language studies have focused on English as source language. This is not to say that CL varieties do not exist for languages other than English. Among recent work, Barthe (1998) relates the process of developing GIFAS, the ‘rationalised’ French counterpart of the AECMA documentation standard for the aerospace industry, while Lieske et al. (2002) describe a controlled German. In the case of Japanese, the application of the CL notion dates back to (Nagao and Tanaka, 1984), who describe a framework for assisting authors in producing what they termed ‘machinereadable’ Japanese. Yoshida (1987) outlines a framework for designing a ‘standardised’ Japanese for MT. Kaji (1999) offers a few Japanese examples. More recent computational work has focused on automatic re-writing of what we can term ‘MT-intractable’ Japanese (e.g., Shirai, 1998; Matsuyoshi et al., 2004). Since such re-writing is a machine-internal process, these studies are not nec"
2012.eamt-1.57,1999.mtsummit-1.6,0,0.0603831,"is not to say that CL varieties do not exist for languages other than English. Among recent work, Barthe (1998) relates the process of developing GIFAS, the ‘rationalised’ French counterpart of the AECMA documentation standard for the aerospace industry, while Lieske et al. (2002) describe a controlled German. In the case of Japanese, the application of the CL notion dates back to (Nagao and Tanaka, 1984), who describe a framework for assisting authors in producing what they termed ‘machinereadable’ Japanese. Yoshida (1987) outlines a framework for designing a ‘standardised’ Japanese for MT. Kaji (1999) offers a few Japanese examples. More recent computational work has focused on automatic re-writing of what we can term ‘MT-intractable’ Japanese (e.g., Shirai, 1998; Matsuyoshi et al., 2004). Since such re-writing is a machine-internal process, these studies are not necessarily directly applicable to guiding the authoring of human-readable texts. Morita and Ishida (2011) provide protocols to enable monolingual users to converge on a correct Japanese/English machine translation, but no a priori writing or editing rules are proposed. The proposals in (Sato et al., 2003) are motivated by persona"
2012.eamt-1.9,1999.mtsummit-1.6,0,0.094763,"Missing"
2012.tc-1.1,P07-2002,1,0.672352,"ide-by-side with differences highlighted. Babych, Hartley, Kageura, Thomas, Utiyama 5 MNH-TT: a collaborative platform for translator training Translating and the Computer 34 29-30 November 2012, London, UK Figure 1: Dictionary lookup in QRedit MNH assumes translators work voluntarily and not upon request by customers, so the management of overall workflow in commercial settings is not explicitly provided, although it can be simulated using existing MNH functions. Translation work is carried out using the translation-aid editor QRedit, a two-pane editor which provides the following functions (Abekawa and Kageura, 2007):      lookup of dictionaries and terminologies (idiom variants are matched to their canonical forms; multi-word units and idioms are prioritised) seamless connection to bilingual corpora (TMs) seamless connection to Wikipedia monolingual and bilingual entries seamless connection to Google webpage and dictionary search registration of terms. These functions are triggered by mouse actions starting from the relevant words or phrases in the SL text area. Throughout these actions, the keyboard remains active in the TL text area in order to improve the efficiency of translation (Figure 1). The"
2012.tc-1.1,abekawa-kageura-2008-constructing,1,0.654492,"contribution. The poster may also specify their role and the object or ‘prop’ to which they are referring; these include: translation-brief, set-of-targetdocuments, research-data, glossary, tms, mt-raw-output, as well as text spans such as sentence or word. Figure 3 illustrates the bulletin board displaying a number of interactions between a project manager and volunteers signing up to play various roles in the project. Figure 3: Interaction in MNH-TT structured by dialogue act and referencing role 2.4 MNH-TT: revision categorisation The third extension provides a set of categories based on (Abekawa and Kageura, 2008; Castagnoli et al., 2006; Secara, 2005) to allow revisers to motivate and justify individual revisions (Table 3). The defined categories are grouped thematically.     content- revisions bear on the perceived transfer of ideas between the source and the target document lexis- revisions bear on the choice of words and terms grammar- revisions bear on the well-formedness of the target document text- revisions relate to departures from the conventions holding for the genre of the target document, or to clumsiness, or to a lack of cohesion. Revision categories are represented as a menu. Each t"
2012.tc-1.1,abekawa-etal-2010-community,1,0.806611,"Missing"
2012.tc-1.1,W07-0732,0,0.148105,"Missing"
2012.tc-1.1,2012.tc-1.1,1,0.0530913,"Missing"
2012.tc-1.1,2009.mtsummit-posters.22,1,0.370012,"Missing"
2012.tc-1.1,2009.tc-1.4,1,0.205718,"Missing"
2013.mtsummit-papers.1,C94-2167,0,0.458061,"ped a fully operating system for detecting new bilingual term pairs in order to augment bilingual terminologies. 1 Introduction In this paper we propose a way of detecting new bilingual term pairs for augmenting bilingual terminologies by using a “generate and validate” method. Augmenting bilingual terminologies is sine qua non for terminology managers, translators and document managers (Sager, 1990), and its importance is growing in accordance with the rapid growth of terminologies in many domains. In general, new terms they tend to be created in a systematic way by compounding (Sager, 1990; Ananiadou, 1994; Justeson and Katz, 1995; Cerbah, 2000; Kageura, 2012), resulting in an abundance of multi-word terms (MWTs). This fact results in a tendency for the correspondences between constituent elements to be retained across languages to a substantial extent. This provides us with a chance to take advantage of the information contained in existing terminologies to augment and enrich terminologies with new terms, based on a simple idea: If a terminological lexicon contains “linear programming,” “linear Figure 1: Existing and “potential” term pairs The paper is organised as follows. Section 2 brieﬂy lo"
2013.mtsummit-papers.1,E06-2022,0,0.351172,"lains the system arrangement and the methods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, 2 Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extracting them from comparable corpora, the problem of limited"
2013.mtsummit-papers.1,C02-2020,0,0.237358,"nised as follows. Section 2 brieﬂy looks at related work. Section 3 explains the system arrangement and the methods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, 2 Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extrac"
2013.mtsummit-papers.1,C00-1022,0,0.41396,"new bilingual term pairs in order to augment bilingual terminologies. 1 Introduction In this paper we propose a way of detecting new bilingual term pairs for augmenting bilingual terminologies by using a “generate and validate” method. Augmenting bilingual terminologies is sine qua non for terminology managers, translators and document managers (Sager, 1990), and its importance is growing in accordance with the rapid growth of terminologies in many domains. In general, new terms they tend to be created in a systematic way by compounding (Sager, 1990; Ananiadou, 1994; Justeson and Katz, 1995; Cerbah, 2000; Kageura, 2012), resulting in an abundance of multi-word terms (MWTs). This fact results in a tendency for the correspondences between constituent elements to be retained across languages to a substantial extent. This provides us with a chance to take advantage of the information contained in existing terminologies to augment and enrich terminologies with new terms, based on a simple idea: If a terminological lexicon contains “linear programming,” “linear Figure 1: Existing and “potential” term pairs The paper is organised as follows. Section 2 brieﬂy looks at related work. Section 3 explains"
2013.mtsummit-papers.1,C94-1084,0,0.427323,"Missing"
2013.mtsummit-papers.1,I08-1013,0,0.725437,"ods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, 2 Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extracting them from comparable corpora, the problem of limited availability of parallel corpora has led to a gre"
2013.mtsummit-papers.1,C12-1110,0,0.0230444,"Missing"
2013.mtsummit-papers.1,W97-0119,0,0.192702,"“potential” term pairs The paper is organised as follows. Section 2 brieﬂy looks at related work. Section 3 explains the system arrangement and the methods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, 2 Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora g"
2013.mtsummit-papers.1,P98-1074,0,0.0410561,"he paper is organised as follows. Section 2 brieﬂy looks at related work. Section 3 explains the system arrangement and the methods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, 2 Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora generally attains"
2013.mtsummit-papers.1,C10-1070,0,0.056297,"bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extracting them from comparable corpora, the problem of limited availability of parallel corpora has led to a great deal of research into bilingual term extraction using comparable corpora. In addition to work that has resulted in the steady improvement of algorithms, there are studies that address improvement of corpus comparability (Morin et al., 2010; Li and Gaussier, 2010). In the EU, research into corpus-based term extraction culminated in an"
2013.mtsummit-papers.1,E09-1057,0,0.367979,"Missing"
2013.mtsummit-papers.1,C10-1073,0,0.0915419,"rom parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 3–10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010). Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extracting them from comparable corpora, the problem of limited availability of parallel corpora has led to a great deal of research into bilingual term extraction using comparable corpora. In addition to work that has resulted in the steady improvement of algorithms, there are studies that address improvement of corpus comparability (Morin et al., 2010; Li and Gaussier, 2010). In the EU, research into corpus-based term extraction culminated in an EU project (TTC, 2012)"
2013.mtsummit-papers.1,E06-1029,0,0.502182,"Missing"
2013.mtsummit-papers.1,I05-2020,0,0.3129,"Missing"
2013.mtsummit-papers.1,N03-1033,0,0.011169,"Missing"
2013.mtsummit-papers.1,E93-1015,0,0.118924,"Missing"
2015.mtsummit-papers.8,2012.eamt-1.57,1,0.940682,"ocuments drawing on existing wisdom about technical writing in Japanese. Since the study chiefly referred to writing guidelines intended for human understandability or readability, the overall efficacy of the rules with MT was not significant. Thus, there remains much room for investigating other patterns impactProceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 91 ing on MT performance within the municipal domain. O’Brien (2006) argued persuasively for the need to tune CL rule sets to language pair and MT system. The results of evaluation experiments by (Hartley et al., 2012) also suggested there were differences between rule-based machine translation (RBMT) and statistical machine translation (SMT) systems in terms of the impact of specific CL rules on their performance, although the MT systems as such were not the focus of their investigation. In short, it is still uncertain to what extent CL rules can be effectively generalised across MT systems or how much improvement can be attained if we compile rules specifically tuned to a given system. Practical deployment of CL requires that the readability of the source text (ST) should not be compromised in the interes"
2015.mtsummit-papers.8,J14-1005,0,0.0847801,"der of this paper is structured as follows. We describe related work in Section 2. In Section 3, we discuss how we constructed our CL rules, while Section 4 explains our experimental setup for human evaluation to assess the rules. We present our results accompanied with a discussion in Section 5. Section 6 concludes with implications for future work. 2 Related studies Controlled (natural) language or C(N)L is ‘a constructed language that is based on a certain natural language, being more restrictive concerning lexicon, syntax, and/or semantics while preserving most of its natural properties’ (Kuhn, 2014, p.123). A number of English CL rule sets have been proposed to improve MT performance as well as human comprehension, and they have been actually implemented, mainly in technical documentation (e.g., Kamprath et al., 1998; Nyberg et al., 2003). Evaluation experiments on CL for MT have also been undertaken to assess machine translatability and post-editing productivity (Pym, 1990; Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007; Aikawa et al., 2007), showing evidence of the effectiveness of CL. In the case of Japanese CL, Nagao et al. (1984) devised a controlled grammar to syntactically"
2019.jeptalnrecital-tia.5,C00-1022,0,0.588835,"Missing"
2019.jeptalnrecital-tia.5,2019.jeptalnrecital-tia.5,1,0.0530913,"Missing"
2019.jeptalnrecital-tia.5,W16-4705,1,0.484037,"Missing"
2020.lrec-1.512,P17-1042,0,0.0506233,"Missing"
2020.lrec-1.512,P18-1073,0,0.0335278,"Missing"
2020.lrec-1.512,W16-1614,0,0.0605186,"Missing"
2020.lrec-1.512,Q17-1010,0,0.257319,"4.2. n LW (W |θD ) = − 1X log PθD ( source = 0|W xi ) n i=1 m − 1 X log PθD ( source = 1|yi ) m i=1 (2) 4.2.1. Pre-trained Embeddings from Crosslingual Language Models While the original task of unsupervised bilingual lexicon induction refrains from techniques that are built on parallel corporal resources, in our terminology applications we aim to exploit the power of cross-lingual pre-trained language models. To do so we extracted pre-trained feature embeddings for the terms from the following 3 multilingual models, multilingual BERT (Devlin et al., 2018), XLM-RoBERTa (XLM-R), and FastText (Bojanowski et al., 2017; Joulin et al., 2016). • Multilingual BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) is simultaneously pre-trained on multilingual corpora with a shared vocabulary across languages. The model is pre-trained on a transformer model with objectives on masked language model (MLM) tasks based on a very large-scale general corpus. Method: MUSE (Multilingual Unsupervised or Supervised word Embeddings) Multilingual Unsupervised or Supervised word Embeddings (MUSE) (Conneau et al., 2017) is one of the stateof-the-art methods for bilingual lexicon induction. We sel"
2020.lrec-1.512,A94-1006,0,0.569539,"to Generate Structural Embeddings The key advancement we make is the creation of n-grambased token-individual embedding in each language that, when combined to form a term embedding, is trained to reflect the term’s place in the terminology tree structure (§3.1). This follows the framework: 1. Tokenize the single- and multi-word terms into character n-gram tokens. Structural Embedding Model: Graph Convolutional Networks 2. For each token, assign a randomly initialized embedding. Related Work Bilingual term extraction from parallel and comparable corpora has been studied since the early 1990s (Dagan and Church, 1994; Daille et al., 1994; Morin et al., 2007). Some use compositional translation to match bilingual terms (Tonoike et al., 2005; Delpech et al., 2012). More recently, word embeddings are used for bilingual terminology extraction (Hazem and Morin, 2017). The terminological network is defined as a network where each node is a term (single- and multi-worded) and two nodes share an edge when they have one or more constituent elements in common (Iwai et al., 2016b). Such networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Gra"
2020.lrec-1.512,C94-1084,0,0.606096,"mbeddings The key advancement we make is the creation of n-grambased token-individual embedding in each language that, when combined to form a term embedding, is trained to reflect the term’s place in the terminology tree structure (§3.1). This follows the framework: 1. Tokenize the single- and multi-word terms into character n-gram tokens. Structural Embedding Model: Graph Convolutional Networks 2. For each token, assign a randomly initialized embedding. Related Work Bilingual term extraction from parallel and comparable corpora has been studied since the early 1990s (Dagan and Church, 1994; Daille et al., 1994; Morin et al., 2007). Some use compositional translation to match bilingual terms (Tonoike et al., 2005; Delpech et al., 2012). More recently, word embeddings are used for bilingual terminology extraction (Hazem and Morin, 2017). The terminological network is defined as a network where each node is a term (single- and multi-worded) and two nodes share an edge when they have one or more constituent elements in common (Iwai et al., 2016b). Such networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Graph convolutional netw"
2020.lrec-1.512,C12-1046,0,0.029073,"ombined to form a term embedding, is trained to reflect the term’s place in the terminology tree structure (§3.1). This follows the framework: 1. Tokenize the single- and multi-word terms into character n-gram tokens. Structural Embedding Model: Graph Convolutional Networks 2. For each token, assign a randomly initialized embedding. Related Work Bilingual term extraction from parallel and comparable corpora has been studied since the early 1990s (Dagan and Church, 1994; Daille et al., 1994; Morin et al., 2007). Some use compositional translation to match bilingual terms (Tonoike et al., 2005; Delpech et al., 2012). More recently, word embeddings are used for bilingual terminology extraction (Hazem and Morin, 2017). The terminological network is defined as a network where each node is a term (single- and multi-worded) and two nodes share an edge when they have one or more constituent elements in common (Iwai et al., 2016b). Such networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Graph convolutional networks (GCNs) (Kipf and Welling, 2016) process a graph by generating a feature representation of each node. They enable the numer"
2020.lrec-1.512,E14-1049,0,0.110368,"Missing"
2020.lrec-1.512,I17-1069,0,0.0202889,"cture (§3.1). This follows the framework: 1. Tokenize the single- and multi-word terms into character n-gram tokens. Structural Embedding Model: Graph Convolutional Networks 2. For each token, assign a randomly initialized embedding. Related Work Bilingual term extraction from parallel and comparable corpora has been studied since the early 1990s (Dagan and Church, 1994; Daille et al., 1994; Morin et al., 2007). Some use compositional translation to match bilingual terms (Tonoike et al., 2005; Delpech et al., 2012). More recently, word embeddings are used for bilingual terminology extraction (Hazem and Morin, 2017). The terminological network is defined as a network where each node is a term (single- and multi-worded) and two nodes share an edge when they have one or more constituent elements in common (Iwai et al., 2016b). Such networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Graph convolutional networks (GCNs) (Kipf and Welling, 2016) process a graph by generating a feature representation of each node. They enable the numerical encoding of terminological graph structure into a high-dimensional space. They have been applied"
2020.lrec-1.512,W16-4705,1,0.604135,"ialized embedding. Related Work Bilingual term extraction from parallel and comparable corpora has been studied since the early 1990s (Dagan and Church, 1994; Daille et al., 1994; Morin et al., 2007). Some use compositional translation to match bilingual terms (Tonoike et al., 2005; Delpech et al., 2012). More recently, word embeddings are used for bilingual terminology extraction (Hazem and Morin, 2017). The terminological network is defined as a network where each node is a term (single- and multi-worded) and two nodes share an edge when they have one or more constituent elements in common (Iwai et al., 2016b). Such networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Graph convolutional networks (GCNs) (Kipf and Welling, 2016) process a graph by generating a feature representation of each node. They enable the numerical encoding of terminological graph structure into a high-dimensional space. They have been applied to many real-world data that is naturally represented as graphs, and have found use in text classification (Schlichtkrull et al., 2018), protein interface prediction (Fout et al., 2017), and semantic role label"
2020.lrec-1.512,Q19-1007,0,0.0140523,"ual corpora with a shared vocabulary across languages. The model is pre-trained on a transformer model with objectives on masked language model (MLM) tasks based on a very large-scale general corpus. Method: MUSE (Multilingual Unsupervised or Supervised word Embeddings) Multilingual Unsupervised or Supervised word Embeddings (MUSE) (Conneau et al., 2017) is one of the stateof-the-art methods for bilingual lexicon induction. We selected the method as it has a high degree of support with an open-source library. Despite a few recent works with minor improvements to the model (Patra et al., 2019; Jawanpuria et al., 2019), we decide to apply MUSE due to the fact that it is widely accepted and its performance is ranked at or close to the top in benchmark studies. The MUSE model takes as input two sets of embeddings, one in each language. The model aims to produce a linear mapping W that can transform a set of embeddings in one language to the other language, such that corresponding translation of words have their embeddings close together in the vector space. *** Here you need to articulate the sentences again *** While we refer the reader to the full paper (Conneau et al., 2017) for a detailed description and"
2020.lrec-1.512,P15-1067,0,0.0191992,"ce. They have been applied to many real-world data that is naturally represented as graphs, and have found use in text classification (Schlichtkrull et al., 2018), protein interface prediction (Fout et al., 2017), and semantic role labeling (Marcheggiani and Titov, 2017). For knowledge graphs, alignments between graphs have been an active topic for research. The advent of embeddings methods (Wang et al., 2014) brought new life to the community, with entity embeddings created from attributes and graph structures such as MTransE (Chen and Zaniolo, 2017), TransR (Huang et al., 2017), and TransD (Ji et al., 2015). More recently, graph convolutional networks have Method: Cross-lingual Graph Alignment via Graph Convolutional Networks 3. The pooling of token embeddings produces the embedding of the term, which is trained to reflect its position in the terminology tree structure. 4. For a new term in one language, the sum of the token embedding represents its position in the terminological structure, and can be used to find the corresponding term in the other language’s aligned graph. We applied the same tokenizer from the BERT model to make the results comparable. For each token, we used the defined adja"
2020.lrec-1.512,P19-1493,0,0.0450178,"Missing"
2020.lrec-1.512,D17-1159,0,0.0169333,"uch networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Graph convolutional networks (GCNs) (Kipf and Welling, 2016) process a graph by generating a feature representation of each node. They enable the numerical encoding of terminological graph structure into a high-dimensional space. They have been applied to many real-world data that is naturally represented as graphs, and have found use in text classification (Schlichtkrull et al., 2018), protein interface prediction (Fout et al., 2017), and semantic role labeling (Marcheggiani and Titov, 2017). For knowledge graphs, alignments between graphs have been an active topic for research. The advent of embeddings methods (Wang et al., 2014) brought new life to the community, with entity embeddings created from attributes and graph structures such as MTransE (Chen and Zaniolo, 2017), TransR (Huang et al., 2017), and TransD (Ji et al., 2015). More recently, graph convolutional networks have Method: Cross-lingual Graph Alignment via Graph Convolutional Networks 3. The pooling of token embeddings produces the embedding of the term, which is trained to reflect its position in the terminology tr"
2020.lrec-1.512,P07-1084,1,0.77288,"e hypothesize that domain-specific terminological information can improve the term multilingualization process. Specifically, terminologies exhibit a structural nature not typically expected in general vocabulary (Sager, 1990). We propose a method towards multilingualizing terminologies by taking into account the specialized conceptual structure reflected in existing terminologies. This also amounts to taking into account information on the conceptual system of the domain as opposed to information on domain-specific discourse which has been explored in most term processing (Aker et al., 2007; Morin et al., 2007). Here we outline the major sections of this paper, which also corresponds to the major contributions. • Make the case for the need for terminology multilingualization in the industry (§2) and outlines the construction of terminology language resources by using structural information that is part of the terminology (§2.2). • Measure the limits of semantic-based methods when applied to terminologies (§4). • Propose a compositional approach to the structural encoding of terminologies, and showcase how multilingualization can assist the inclusion of new terms (§5). • Harness the full power of our"
2020.lrec-1.512,P19-1018,0,0.0109936,"trained on multilingual corpora with a shared vocabulary across languages. The model is pre-trained on a transformer model with objectives on masked language model (MLM) tasks based on a very large-scale general corpus. Method: MUSE (Multilingual Unsupervised or Supervised word Embeddings) Multilingual Unsupervised or Supervised word Embeddings (MUSE) (Conneau et al., 2017) is one of the stateof-the-art methods for bilingual lexicon induction. We selected the method as it has a high degree of support with an open-source library. Despite a few recent works with minor improvements to the model (Patra et al., 2019; Jawanpuria et al., 2019), we decide to apply MUSE due to the fact that it is widely accepted and its performance is ranked at or close to the top in benchmark studies. The MUSE model takes as input two sets of embeddings, one in each language. The model aims to produce a linear mapping W that can transform a set of embeddings in one language to the other language, such that corresponding translation of words have their embeddings close together in the vector space. *** Here you need to articulate the sentences again *** While we refer the reader to the full paper (Conneau et al., 2017) for a"
2020.lrec-1.512,I05-2020,0,0.0388044,"language that, when combined to form a term embedding, is trained to reflect the term’s place in the terminology tree structure (§3.1). This follows the framework: 1. Tokenize the single- and multi-word terms into character n-gram tokens. Structural Embedding Model: Graph Convolutional Networks 2. For each token, assign a randomly initialized embedding. Related Work Bilingual term extraction from parallel and comparable corpora has been studied since the early 1990s (Dagan and Church, 1994; Daille et al., 1994; Morin et al., 2007). Some use compositional translation to match bilingual terms (Tonoike et al., 2005; Delpech et al., 2012). More recently, word embeddings are used for bilingual terminology extraction (Hazem and Morin, 2017). The terminological network is defined as a network where each node is a term (single- and multi-worded) and two nodes share an edge when they have one or more constituent elements in common (Iwai et al., 2016b). Such networks have been proven useful in subdomain delineation, and the generation of multi-word terminologies (Iwai et al., 2016a). Graph convolutional networks (GCNs) (Kipf and Welling, 2016) process a graph by generating a feature representation of each node"
2020.lrec-1.512,D14-1167,0,0.0334914,"etworks (GCNs) (Kipf and Welling, 2016) process a graph by generating a feature representation of each node. They enable the numerical encoding of terminological graph structure into a high-dimensional space. They have been applied to many real-world data that is naturally represented as graphs, and have found use in text classification (Schlichtkrull et al., 2018), protein interface prediction (Fout et al., 2017), and semantic role labeling (Marcheggiani and Titov, 2017). For knowledge graphs, alignments between graphs have been an active topic for research. The advent of embeddings methods (Wang et al., 2014) brought new life to the community, with entity embeddings created from attributes and graph structures such as MTransE (Chen and Zaniolo, 2017), TransR (Huang et al., 2017), and TransD (Ji et al., 2015). More recently, graph convolutional networks have Method: Cross-lingual Graph Alignment via Graph Convolutional Networks 3. The pooling of token embeddings produces the embedding of the term, which is trained to reflect its position in the terminology tree structure. 4. For a new term in one language, the sum of the token embedding represents its position in the terminological structure, and c"
2020.lrec-1.512,P19-1304,0,0.0181586,"ken embedding generation. Finally, we see that FastText embeddings, despite the far larger pre-trained vocabulary, do not generally produce better results due to the limited coverage among medical terms (which are often rare words); this is in comparison with the n-gram based language models where all terms can be covered with a pooled embedding. For more discussion, we refer the reader to §5.4, where we compare language pairs across both semantic and structural embedding methods. 5. been applied to knowledge graphs and extended to crosslingual settings (Chen et al., 2016; Shang et al., 2019; Xu et al., 2019). We begin by generating the initial structural embedding of terms by tokenizing and pooling the token embeddings, which are trained from scratch. We then train two graph convolutional networks on top of the term embeddings separately for each language according to the terminology tree, optimizing to minimize the output feature distances and thus aligning the graphs. Finally, we test out the task of multilingualization by predicting the translation of terms unseen during training. 5.3. Tokenization of Terminologies to Generate Structural Embeddings The key advancement we make is the creation o"
abekawa-etal-2010-community,abekawa-kageura-2008-constructing,1,\N,Missing
abekawa-etal-2010-community,P01-1008,0,\N,Missing
abekawa-etal-2010-community,P07-2002,1,\N,Missing
abekawa-etal-2010-community,I08-1032,1,\N,Missing
abekawa-etal-2010-community,2007.mtsummit-papers.60,1,\N,Missing
abekawa-kageura-2008-constructing,I08-1032,1,\N,Missing
abekawa-kageura-2008-constructing,P07-2002,1,\N,Missing
C00-1058,J96-1001,0,\N,Missing
C00-1058,C94-1100,0,\N,Missing
C00-1058,E93-1015,0,\N,Missing
C00-1058,A94-1006,0,\N,Missing
C00-1058,P95-1032,0,\N,Missing
C00-1058,P93-1003,0,\N,Missing
C00-1058,C94-1084,0,\N,Missing
C00-1058,C96-2130,0,\N,Missing
C00-1058,H91-1026,0,\N,Missing
C00-1058,1996.amta-1.13,0,\N,Missing
C02-1086,J90-1003,0,0.0662542,"1996; Eichmann et al, 1998; Yang et al, 1998; Jang et al, 1999; Chun, 2000) based on bilingual dictionaries, multilingual ontology or thesaurus are much more practical. Many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries. In order to achieve a high performance CLIR using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms. One way of resolving query ambiguities is to use the statistics, such as mutual information (Church and Hanks, 1990), to measure associations of query terms, on the basis of existing corpora (Jang et al, 1999). Document clusters, widely adopted in various applications such as browsing and viewing of document results (Hearst and Pedersen, 1996) or topic detection (Allan et al, 1998), also reflect the association of terms and documents. Lee et al (2001) showed that incorporating a document re-ranking method based on document clusters into the vector space retrieval achieved the significant improvement in monolingual IR, as it contributed to resolving ambiguities caused by polysemous query terms. The noise or"
C02-1086,P99-1029,0,0.0662692,"ategories: statistical approaches and translation approaches. Statistical methods establish cross-lingual associations without language translation (Dumais et al, 1997; Rehder et al, 1997; Yang et al, 1998). They require large-scale bilingual corpora. In translation approach, either queries or documents are translated. Though document translation is possible when high quality machine translation systems are available (Kwon et al, 1997; Oard and Hackett, 1997), it is not very practical. Query translation methods (Hull and Grefenstette, 1996; Davis, 1996; Eichmann et al, 1998; Yang et al, 1998; Jang et al, 1999; Chun, 2000) based on bilingual dictionaries, multilingual ontology or thesaurus are much more practical. Many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries. In order to achieve a high performance CLIR using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms. One way of resolving query ambiguities is to use the statistics, such as mutual information (Church and Hanks, 1990), to measure associations of query term"
C02-1086,1998.amta-tutorials.5,0,\N,Missing
C16-2008,W02-2117,1,0.549993,"Missing"
C16-2008,J14-1005,0,0.0222758,"TUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system. 1 Introduction For improved machine translatability, a wide variety of controlled language (CL) rule sets have been proposed (Kittredge, 2003; Kuhn, 2014). Evidence of reduced post-editing costs when a CL is employed is provided (Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007), and several controlled authoring support tools, such as Acrolinx1 and MAXIT2 , have been developed. The fundamental limitation of the CLs proposed hitherto is, however, that they are defined at the level of the sentence rather than at the level of the document (Hartley and Paris, 2001). In fact, the notion of functional document element (see Section 2.1) does figure in some CL rule sets. ASD Simplified Technical English (ASD, 2013), for example, specifies writing p"
C16-2008,2003.eamt-1.10,0,0.0917482,"pic template is the core interface for authoring self-contained topics in a structured manner. The left pane in Figure 3 provides the basic DITA Task topic structure for composing municipal procedural documents. • CL authoring assistant analyses each sentence in the text box and highlights any segment that violates a local CL rule or controlled terminology, together with diagnostic comments and suggestions for rewriting (shown at bottom centre in Figure 3) (Miyata et al., 2016). In addition, we have implemented a preliminary rewriting support function with several of the features advocated by Mitamura et al. (2003). For a particular CL-noncompliant segment, the function offers alternative expressions; clicking one of the suggestions automatically replaces the offending segment in the text box above. • Pre-translation processing automatically modifies source segments in the background following transformation rules defined for each functional element, and then MT produces the translation and back-translation at the same time. 3 We used a Japanese morphological analyser MeCab. http://taku910.github.io/mecab/ 37 MT and back translation DITA task topic CL authoring assistant Figure 3: Task topic template fo"
C16-2008,2015.mtsummit-papers.8,1,0.857596,"Missing"
C98-1101,C80-1091,0,0.264197,"Missing"
C98-1101,J96-4001,0,\N,Missing
I08-1032,P07-2002,1,0.202699,"these modiﬁcations, in order to identify the features that trigger modiﬁcation. Our goal is to construct a system that notiﬁes (English-to-Japanese) volunteer translators of awkward translations. After manually classifying the basic modiﬁcation patterns, we analysed the factors that trigger a change in verb voice from passive to active using SVM. An experimental result shows good prospects for the automatic identiﬁcation of candidates for modiﬁcation. 1 Introduction We are currently developing an English-to-Japanese translation aid system aimed at volunteer translators mainly working online (Abekawa and Kageura, 2007), As part of this project, we are developing a module that notiﬁes (inexperienced) translators of awkwardly translated expressions that may need reﬁnement or editing. In most cases, translators ﬁrst make draft translations, and then examine and edit them later, often repeatedly. Thus there are normally at least two versions of a given translation, i.e. a draft and the ﬁnal translation. In commercial translation environments, it is sometimes the case that texts are ﬁrst translated by inexperienced translators and then edited by experienced translators. However, this does not apply to voluntary"
I08-1032,P01-1008,0,0.0482721,"se modiﬁcation patterns consist of one or more primitive operations. For instance, a “change of voice” may consist of such primitive operations as “changing the case-marker of the subject,” “swapping the position of subject and object,” etc. As preparation, we extracted modiﬁcation patterns from the data2 . In order to do so, we ﬁrst aligned the “Draft” and the “Final” at the sentence level using DP matching, and then at the morpheme level using GIZA++ (Och and Ney, 2003). Figure 1 illustrates an example of word/morpheme level 2 This task is similar to the acquisition of paraphrase knowledge (Barzilay and McKeown, 2001; Shinyama et al., 2002; Quirk et al. 2004; Barzilay and Lee, 2003; Dolan et al., 2004). However, our aim here is to clarify basic modiﬁcation patterns and not automatic identiﬁcation. 242 English: “Draft”: If it was perceived to be true by the majority of Thinkers, ... 人類の 多数によって それが 真実であると JINRUI-NO TASUU-NIYOTTE SORE-GA SINJITU-DE-ARU-TO (thinkersgenitive ) (majorityablative ) (itsubject ) (to be true) “Final”: 人類の 多数が それを 真実と JINRUI-NO TASUU-GA SORE-WO SINJITU-TO (thinkersgenitive ) (majoritysubject ) (itobject ) (to be true) Primitive replace(“NIYOTTE”, replace(“GA”, delete(“DE”) operatio"
I08-1032,N03-1003,0,0.0185843,"or instance, a “change of voice” may consist of such primitive operations as “changing the case-marker of the subject,” “swapping the position of subject and object,” etc. As preparation, we extracted modiﬁcation patterns from the data2 . In order to do so, we ﬁrst aligned the “Draft” and the “Final” at the sentence level using DP matching, and then at the morpheme level using GIZA++ (Och and Ney, 2003). Figure 1 illustrates an example of word/morpheme level 2 This task is similar to the acquisition of paraphrase knowledge (Barzilay and McKeown, 2001; Shinyama et al., 2002; Quirk et al. 2004; Barzilay and Lee, 2003; Dolan et al., 2004). However, our aim here is to clarify basic modiﬁcation patterns and not automatic identiﬁcation. 242 English: “Draft”: If it was perceived to be true by the majority of Thinkers, ... 人類の 多数によって それが 真実であると JINRUI-NO TASUU-NIYOTTE SORE-GA SINJITU-DE-ARU-TO (thinkersgenitive ) (majorityablative ) (itsubject ) (to be true) “Final”: 人類の 多数が それを 真実と JINRUI-NO TASUU-GA SORE-WO SINJITU-TO (thinkersgenitive ) (majoritysubject ) (itobject ) (to be true) Primitive replace(“NIYOTTE”, replace(“GA”, delete(“DE”) operations: “GA”) “WO”) delete(“ARU”) 認識されれば， NINSIKI-SA-RERE-BA (be perce"
I08-1032,C04-1051,0,0.0614831,"Missing"
I08-1032,P96-1018,0,0.0195815,"Missing"
I08-1032,J03-1002,0,0.0042069,"eans of basic linguistic labels such as “change of voice” or “change from nominal modiﬁcation to adverbial modiﬁcation” (cf. Anzai, 1995). These modiﬁcation patterns consist of one or more primitive operations. For instance, a “change of voice” may consist of such primitive operations as “changing the case-marker of the subject,” “swapping the position of subject and object,” etc. As preparation, we extracted modiﬁcation patterns from the data2 . In order to do so, we ﬁrst aligned the “Draft” and the “Final” at the sentence level using DP matching, and then at the morpheme level using GIZA++ (Och and Ney, 2003). Figure 1 illustrates an example of word/morpheme level 2 This task is similar to the acquisition of paraphrase knowledge (Barzilay and McKeown, 2001; Shinyama et al., 2002; Quirk et al. 2004; Barzilay and Lee, 2003; Dolan et al., 2004). However, our aim here is to clarify basic modiﬁcation patterns and not automatic identiﬁcation. 242 English: “Draft”: If it was perceived to be true by the majority of Thinkers, ... 人類の 多数によって それが 真実であると JINRUI-NO TASUU-NIYOTTE SORE-GA SINJITU-DE-ARU-TO (thinkersgenitive ) (majorityablative ) (itsubject ) (to be true) “Final”: 人類の 多数が それを 真実と JINRUI-NO TASUU-"
I08-1032,W04-3219,0,0.022265,"itive operations. For instance, a “change of voice” may consist of such primitive operations as “changing the case-marker of the subject,” “swapping the position of subject and object,” etc. As preparation, we extracted modiﬁcation patterns from the data2 . In order to do so, we ﬁrst aligned the “Draft” and the “Final” at the sentence level using DP matching, and then at the morpheme level using GIZA++ (Och and Ney, 2003). Figure 1 illustrates an example of word/morpheme level 2 This task is similar to the acquisition of paraphrase knowledge (Barzilay and McKeown, 2001; Shinyama et al., 2002; Quirk et al. 2004; Barzilay and Lee, 2003; Dolan et al., 2004). However, our aim here is to clarify basic modiﬁcation patterns and not automatic identiﬁcation. 242 English: “Draft”: If it was perceived to be true by the majority of Thinkers, ... 人類の 多数によって それが 真実であると JINRUI-NO TASUU-NIYOTTE SORE-GA SINJITU-DE-ARU-TO (thinkersgenitive ) (majorityablative ) (itsubject ) (to be true) “Final”: 人類の 多数が それを 真実と JINRUI-NO TASUU-GA SORE-WO SINJITU-TO (thinkersgenitive ) (majoritysubject ) (itobject ) (to be true) Primitive replace(“NIYOTTE”, replace(“GA”, delete(“DE”) operations: “GA”) “WO”) delete(“ARU”) 認識されれば， NINS"
I08-1032,P07-1011,0,0.0410157,"Missing"
kageura-kikui-2006-self,P98-1104,1,\N,Missing
kageura-kikui-2006-self,P98-1105,1,\N,Missing
kageura-kikui-2006-self,C98-1101,1,\N,Missing
kageura-kikui-2006-self,takezawa-etal-2002-toward,0,\N,Missing
O06-5003,C94-1089,0,0.118657,"Missing"
O06-5003,Y05-1005,1,0.430488,"Missing"
O06-5003,2001.mtsummit-papers.66,0,0.369205,"Missing"
P03-2035,A92-1030,0,\N,Missing
P03-2035,C96-1049,0,\N,Missing
P03-2035,W02-1508,0,\N,Missing
P03-2035,C96-1062,0,\N,Missing
P03-2035,P84-1109,0,\N,Missing
P03-2035,J02-3004,0,\N,Missing
P03-2035,W98-0141,0,\N,Missing
P07-1084,C02-1011,0,0.0343127,"Missing"
P07-1084,C02-2020,0,0.786859,"Missing"
P07-1084,C02-1166,0,0.638566,"Missing"
P07-1084,1999.tc-1.8,0,0.0479716,"exical units of the context vectors, which depends on the coverage of the bilingual dictionary vis-à-vis the corpus, is an important step of the direct approach: more elements of the context vector are translated more the context vector will be discrimating for selecting translations in the target language. If the bilingual dictionary provides several translations for a lexical unit, we consider all of them but weight the different translations by their frequency in the target language. If an MWT cannot be directly translated, we generate possible translations by using a compositional method (Grefenstette, 1999). For each element of the MWT found in the bilingual dictionary, we generate all the translated combinations identified by the term extraction program. For example, in the case of the MWT fatigue chronique (chronic fatigue), we have the fol, , lowing four translations for fatigue: , and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: . . In this approach, when"
P07-1084,P99-1067,0,0.865212,"ed and classified into two discourse categories: one contains only scientific documents and the other contains both scientific and popular science documents. We used a state-of-the-art multilingual terminology mining chain composed of two term extraction programs, one in each language, and an alignment program. The term extraction programs are publicly available and both extract multi-word terms that are more precise and specific to a particular scientific domain than single word terms. The alignment program makes use of the direct context-vector approach (Fung, 1998; Peters and Picchi, 1998; Rapp, 1999) slightly modified to handle both singleand multi-word terms. We evaluated the candidate translations of multi-word terms using a reference list compiled from publicly available resources. We found that taking discourse type into account resulted in candidate translations of a better quality even when the corpus size is reduced by half. Thus, even using a state-of-the-art alignment method wellknown as data greedy, we reached the conclusion that the quantity of data is not sufficient to obtain a terminological list of high quality and that a real comparability of corpora is required. 2 Multilin"
P07-1084,E06-1029,0,0.519904,"lowing four translations for fatigue: , and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: . . In this approach, when it is not possible to translate all parts of an MWT, or when the translated combinations are not identified by the term extraction program, the MWT is not taken into account in the translation process. This approach differs from that used by (Robitaille et al., 2006) for French/Japanese translation. They first decompose the French MWT into combinations of shorter multi-word units (MWU) elements. This approach makes the direct translation of a subpart of the MWT possible if it is present in the         7 the French word order is inverted to take into account the different constraints between French and Japanese. 667 fatigue          Table 1: Illustration of the compositional method. The underlined Japanese MWT actually exists.  bilingual dictionary. For an MWT of length , (Robitaille et al., 2006) produce all the combin"
P07-2002,W02-1020,0,0.0203454,"www.omegat.org/ 5 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 5–8, c Prague, June 2007. 2007 Association for Computational Linguistics Some of these characteristics contrast with those of professional translators, for instance, in Canada or in the EU. They have native command in both the source and target languages; they went through university-level training in translation; many of them have a speciality domain; they work on the principle that “time is money” 4 . For this type of translator, facilitating target text input can be important, as is shown in the TransType system (Foster et al., 2002; Macklovitch, 2006). 2.2 Reference units and lookup patterns The major types of reference unit can be summarised as follows (Kageura et al., 2006). Ordinary words: Translators are mostly satisﬁed with the information provided in existing dictionaries. Looking up these references is not a huge burden, though reducing it would be preferable. Idioms and phrases: Translators are mostly satisﬁed with the information provided in dictionaries. However, the lookup process is onerous and many translators worry about failing to recognise idioms in SL texts (as they can often be interpreted literally),"
P07-2002,macklovitch-2006-transtype2,0,0.0684861,"ceedings of the ACL 2007 Demo and Poster Sessions, pages 5–8, c Prague, June 2007. 2007 Association for Computational Linguistics Some of these characteristics contrast with those of professional translators, for instance, in Canada or in the EU. They have native command in both the source and target languages; they went through university-level training in translation; many of them have a speciality domain; they work on the principle that “time is money” 4 . For this type of translator, facilitating target text input can be important, as is shown in the TransType system (Foster et al., 2002; Macklovitch, 2006). 2.2 Reference units and lookup patterns The major types of reference unit can be summarised as follows (Kageura et al., 2006). Ordinary words: Translators are mostly satisﬁed with the information provided in existing dictionaries. Looking up these references is not a huge burden, though reducing it would be preferable. Idioms and phrases: Translators are mostly satisﬁed with the information provided in dictionaries. However, the lookup process is onerous and many translators worry about failing to recognise idioms in SL texts (as they can often be interpreted literally), which may lead to mi"
P98-1104,J96-4001,0,0.0437345,"Missing"
P98-1104,C80-1091,0,0.291,"Missing"
P98-1105,J96-4001,0,\N,Missing
P98-1105,C80-1091,0,\N,Missing
tsuji-etal-2002-extracting,P97-1017,0,\N,Missing
W02-1409,C94-2139,0,\N,Missing
W02-1409,P84-1109,0,\N,Missing
W04-1810,J90-1003,0,0.360988,"Missing"
W10-3303,2008.tc-1.3,0,0.0351409,"Missing"
W10-3303,2007.mtsummit-papers.24,0,0.111395,"Missing"
W10-3508,2009.mtsummit-posters.10,1,0.748465,"o QRedit, it loads the corresponding text into the left panel, as shown in Figure 2. Then, QRedit automatically looks up all words in the SL text. When a user clicks an SL word, its translation candidates are displayed in a pop-up window. 2 http://www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term extraction tool to extract English term candidates. If these English term candidates are found in the English translation candidates, then, we accep"
W10-3508,P09-4005,0,0.0320061,"ble translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. Third, MNH uses Creative Commons Licenses (CCLs) to help translators share their translations. CCLs are essential for sharing and opening translations. 63 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 63–66, Figure 2: Screenshot of QRedit 2 Related work There are many translation support tools, such as Google Translator Toolkit, WikiBABEL (Kumaran et al., 2009), BEYtrans (Bey et al., 2008), Caitra (Koehn, 2009) and Idiom WorldServer system,2 an online multilingual document management system with translation memory functions. The functions that MNH provides are closer to those provided by Idiom WorldServer, but MNH provides a high-quality bilingual dictionaries and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit. It also enables translators to share their translations, which are also used as language resources. 3 Helping Volunteer translators This section describes a set of translation aid tools installed in MNH. 3.1 QRedit QRedit is a translation"
W10-3508,P09-4008,0,0.0291935,"ictionary that was made from the English Wikipedia. This enable translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. Third, MNH uses Creative Commons Licenses (CCLs) to help translators share their translations. CCLs are essential for sharing and opening translations. 63 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 63–66, Figure 2: Screenshot of QRedit 2 Related work There are many translation support tools, such as Google Translator Toolkit, WikiBABEL (Kumaran et al., 2009), BEYtrans (Bey et al., 2008), Caitra (Koehn, 2009) and Idiom WorldServer system,2 an online multilingual document management system with translation memory functions. The functions that MNH provides are closer to those provided by Idiom WorldServer, but MNH provides a high-quality bilingual dictionaries and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit. It also enables translators to share their translations, which are also used as language resources. 3 Helping Volunteer translators This section describes a set of translation aid tools i"
W10-3508,W06-1703,0,0.0239343,"www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term extraction tool to extract English term candidates. If these English term candidates are found in the English translation candidates, then, we accept these term candidates as the translations of those Japanese term candidates. 4 Fostering language resources Being a “one stop” translation aid tool for online translators, MNH incorporates mechanisms which enable users to naturally foster import"
W10-3508,P03-1010,1,0.7968,"em which is designed for volunteer translators working mainly online (Abekawa and Kageura, 2007). When a URL of a source language (SL) text is given to QRedit, it loads the corresponding text into the left panel, as shown in Figure 2. Then, QRedit automatically looks up all words in the SL text. When a user clicks an SL word, its translation candidates are displayed in a pop-up window. 2 http://www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term ex"
W16-4705,E06-2022,0,0.0403158,"of this paper is organised as follows. Section 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefe"
W16-4705,2010.eamt-1.44,0,0.230337,"Missing"
W16-4705,C00-1022,0,0.76447,"artition head-modifier bipartite graph in order to reduce term candidates does not reflect systematic structure of terminologies. Following theoretical research in terminology (Sager, 1990; Kageura, 2002), we understand that new terms are formed within the conceptual-terminological subsystem surrounding the new concepts. So our main task is concerned with consolidating these subsystems consisting of tightly-related or “motivated” terms/concepts within which new terms are formed. 2.2 Structural nature of terminology Terminologies in most languages contain a substantial number of complex terms (Cerbah, 2000; Nomura and Ishii, 1989). Research has shown that complex terms tend to show conceptual relationships systematically, with each constituent element representing an important feature of concepts represented by terms (Felber, 1984; Sager, 1990; Kageura, 2002). 31 Figure 2: Terminology network of a putative terminology Head word set Sub-sphere of conceptual system Modiﬁer word set Head word set Sub-sphere of conceptual system Terminology Modiﬁer word set Sub-sphere of conceptual system Dividing terminology into the subsets Bilingual term candidates Head word set Modiﬁer word set Making headmodiﬁ"
W16-4705,C02-2020,0,0.090854,": Example of generating a term candidate The rest of this paper is organised as follows. Section 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms"
W16-4705,I08-1013,0,0.0220162,"ection 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et a"
W16-4705,W97-0119,0,0.314935,"Osaka, Japan, December 12 2016. Figure 1: Example of generating a term candidate The rest of this paper is organised as follows. Section 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence"
W16-4705,1999.tc-1.8,0,0.0116916,"2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008), i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the “generate and validate” framework. Sato et al. (2013) generated multi-word term pairs as bilingual term candidates by considering all possible pairs of constituent elements of terms in a terminology. The generated pairs are then validated by using web documents. Our method adopts this “generate and validate” framework. More specifically, we take Sato et al. (2013) as a point"
W16-4705,C10-1070,0,0.0257749,"present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008), i.e. they genera"
W16-4705,E09-1057,0,0.031689,"d work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and M"
W16-4705,C10-1073,0,0.0214612,"utomatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008), i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the “generate and validate” framework. Sato et al. (2013) generated multi-word term pairs as bi"
W16-4705,E06-1029,0,0.0290259,"s organised as follows. Section 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. 2 Related work 2.1 Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et"
W16-4705,2013.mtsummit-papers.1,1,0.428025,"also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008), i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the “generate and validate” framework. Sato et al. (2013) generated multi-word term pairs as bilingual term candidates by considering all possible pairs of constituent elements of terms in a terminology. The generated pairs are then validated by using web documents. Our method adopts this “generate and validate” framework. More specifically, we take Sato et al. (2013) as a point of departure as the aim of this work is the same as the present work, i.e. extending existing bilingual terminologies. The method proposed by Sato et al. (2013) takes advantage of a general tendency that if one term is a compound, a part of the term is a term and a part of t"
W16-4705,I05-2020,0,0.677948,"al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008), i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the “generate and validate” framework. Sato et al. (2013) generated multi-word term pairs as bilingual term candidates by considering all possible pairs of constituent elements of terms in a terminology. The generated pairs are then validated by using web documents. Our method adopts this “generate and validate” framework. More specifically, we take Sato et al. (2013) as a point of departure as the ai"
W16-4705,W06-1703,0,0.0333893,"Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010), most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010). The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010). Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008), i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the “generate and validate” framework. Sato et al. (2013) generated multi-word term pairs as bilingual term candidates by considering all possible pairs of constituent elements of terms in a terminology. The generated pairs are then validated by using web documents. Our method adopts this “generate and validate” framework. More specifically, we take Sato et al. (2013) as a point of departure as the aim of this work is the"
W16-4710,P13-1040,0,0.0624037,"Missing"
W16-4710,W03-1802,0,0.15097,"Missing"
W16-4710,2001.mtsummit-papers.16,0,0.134831,"Missing"
W16-4710,2007.mtsummit-papers.36,0,0.0877147,"Missing"
W16-4710,kageura-kikui-2006-self,1,0.727355,"intrinsic status of terminology such as coverage has not been examined much. The methodological difficulty in validating the coverage, i.e. how much of the potential terminology in a given domain is covered by the current terminology, is due to the fact that the population size of the terminology to compare is rarely available.1 Sager (2001, p.763) pointed out, however, that statistical means ‘can be used to decide when the addition of more text does not produce any new terms’. We can tackle this issue by employing a statistical method proposed for inspecting the current status of the corpus (Kageura and Kikui, 2006). It is also difficult to assess the quality of controlled terminology, i.e. how well the term variations are managed and standardised. In this paper, we present the idea of comparing the controlled terminologies of multiple languages to validate the quality of control. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 1 If it is available, we no longer need to evaluate such a gold standard. Licence details: http:// 83 Proceedings of the 5th International Workshop on Computational Terminology, pages 83–93, Osaka, Japan, D"
W16-4710,E14-2014,0,0.0351901,"Missing"
W16-4710,2005.mtsummit-wpt.9,0,0.193485,"Missing"
W16-4710,2013.mtsummit-papers.1,1,0.842359,"Missing"
W17-0807,C08-1113,0,0.0380051,"ot an issue” Table 3: Our decision tree for classifying a given issue: we do not produce questions for distinguishing X1/X2/X3, and X8/X10/X11/X12/X13, considering that their definitions are clear enough. fer) and LA (language) issues, described in §2. The priority of the former over the latter, implicitly assumed in the MeLLANGE typology, is also largely preserved; the sole exceptions are X7 (incorrect translations of terms) and X4b (too literal). Table 2 also shows that our typology includes the following three issue types that are not covered by MQM. of partial/incomplete annotation, e.g., Tsuboi et al. (2008), our procedure is nevertheless different from these in the sense that we leave issues “unannotated” only when identical ones are already annotated. 5 Intrinsic Evaluation of the Scheme It is hard to make a fair and unbiased comparison between different annotation schemes that target the same phenomena, employing the same assessors. We thus evaluated whether our issue classification scheme leads to sufficiently high level of inter-assessor agreement, regarding those poor results described in §2 as baselines, and analyzed the tendencies of disagreements and the distribution of issues. • X6 (ind"
W17-0807,N06-2015,0,0.376172,"nglish and Japanese. Neither have their applicability to translations produced by less advanced learners, such as undergraduate students, been fully examined. Aiming at (i) a consistent human assessment, (ii) of English-to-Japanese translations, (iii) produced by learner translators, we manually constructed a scheme for classifying identified issues. We first collected English-to-Japanese translations from learners in order to assure and validate the applicability of our scheme (§3). We then manually created an issue typology and a decision tree through an application of the OntoNotes method (Hovy et al., 2006), i.e., an iteration of assessing learners’ translations and updating the typology and decision tree (§4). We adopted an existing typology, that of MNH-TT (Babych et al., 2012), as the starting point, because its origin (Castagnoli et al., 2006) was tailored to assessing university student learners’ translations and its applicability across several European languages had been demonstrated. We evaluated our scheme with inter-assessor agreement, employing four assessors and an undergraduate learner translator (§5). 4 http://www.atanet.org/certification/ aboutexams_error.php 5 SAE J2450, the stan"
W17-0807,P02-1040,0,0.114813,"arity of issues depend Introduction Assessing and assuring translation quality is one of the main concerns for translation services, machine translation (MT) industries, and translation teaching institutions.1 The assessment process for a given pair of source document (SD) and its translation, i.e., target document (TD), consists of two tasks. The first task is to identify erroneous text spans in the TD. In professional settings, when assessors consider a text span in a TD as erroneous, 2 While automated metrics for MT quality evaluation are often presented as objective, many, including BLEU (Papineni et al., 2002), rely on comparison with a one or more human reference translations whose quality and subjectivity are merely assumed and not independently validated. 3 http://www.qt21.eu/launchpad/content/ multidimensional-quality-metrics 1 These include both private companies and translationrelated departments in colleges and universities. 57 Proceedings of the 11th Linguistic Annotation Workshop, pages 57–66, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics on the purpose of translations and the aim of human assessments (e.g., formative or summative). However, the typology"
W19-6715,2005.mtsummit-papers.11,0,0.114732,"urce term while the target term may or may not contain the target term. Source/Target English-French English-German English-Finnish English-Romanian Sentence Pairs 21057 14070 17486 2685 Table 2: Size of the human-validated sentence pairs, where the sentence pair is validated such that both source/target sentences contain the source/target term translation. for translators working with the official European Union languages. The European Parliament Proceedings (EuroParl) parallel corpus is extracted from the proceedings of the European Parliament and includes versions in 24 European languages (Koehn, 2005). Size of the parallel corpora differs across language pairs, ranging from 400,000 to around 2.2 million sentence pairs. Since IATE is the official EU-wide terminology as maintained and consulted by the translators under EU’s employment, the combination of the two reflects the typical translation procedure when a commonly-agreed term source is provided for translators. 4.2 Language Pairs and Data Size We choose to investigate four language pairs of the EuroParl parallel corpus, namely English-French (en-fr), English-German (en-de), English-Finnish (en-fi) and English-Romanian (en-ro). The lang"
W19-6715,W06-1004,0,0.0609744,"nderstand the underlying factors of the effectiveness of terminology translation. We test systems with bidirectional translations and validate the terminology equivalence by referring to an established term bank. Dublin, Aug. 19-23, 2019 |p. 101 2 Related Work The Special Case for Terminology Translation Traditional translators often approach terminology translation within the lenses of semasiological assumptions and treat terminology as a type of lexical elements (Achkasov, 2014). Nevertheless, when we take on an onomasiological point of view and understand that terms (AdamskaSalaciak, 2010; Lyding et al., 2006) are essentially definitions of concepts, then the degrees of equivalence are expected to be higher. A key aspect of terminology translation is that the formation of a term in some language/domain is not solely at the discretion of the translator, but has structural, pragmatic, functional, and stylistic aspects that need to be taken into account (Achkasov, 2014). This produces a need for translation of terminologies that takes into account the domain terminology that is in existence (Kageura, 2012; Leitchik and Shelov, 2007) Terminology in Translation Quality Assurance Accuracy of terminologie"
W19-6715,P02-1040,0,0.120359,"learning in recent years has, for better or for worse, changed the landscape of c 2019 The authors. This article is licensed under a Creative ○ Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 2 Kyo Kageura The University of Tokyo Tokyo, Japan kyo@p.u-tokyo.ac.jp translation forever. The typical evaluation of machine translation, due to a requirement of fast, automatic metrics during the training phase, typically involves the comparison with a set of human translation in what is calculated as the BLEU or the NIST scores of the translation (Papineni et al., 2002; Doddington, 2002). These approaches run counter to widely accepted frameworks of translation quality assurance (Görög, 2014; Peter et al., 2016) as the measures do not single out aspects of translation that humans traditionally attach importance. Machine translation in general does not produce the exactness in forms required in term translation. Unlike translation of a text, where target text similar in meanings are equivalent as long as they fulfill the required functions, translated term forms must adhere to term banks (Kageura and Marshman, 2019). Machine translation also has implications"
Y05-1005,1983.tc-1.13,0,0.395224,"er translator communities consist mainly of two types: Mission-oriented translators communities: mission-oriented, strongly-coordinated group of volunteers involved in translating clearly defined sets of documents. These communities cover loosely technical documentation like translation of Linux documentation [15], W3C specifications [17], and open source Mozilla localization software [9]. Subject-oriented translators network communities: individual translators who translate online documents such as news, analysis, and reports and make translations available in personal or group web pages [4] [11]. They form groups of translators without any orientation in advance, and they share similar opinions about events (antiwar humanitarian communities, report translation, news translation, humanitarian help, etc.). For instance, almost all online translators show similar behavior. As for the first communities (henceforth “Linux communities”), volunteer translators in both the Traduc and Mozilla projects are invited to translate a list of documents available on web sites (of each project) in different formats (XML, SGML, HTML, HLP, plain text, etc.). Firstly, they check whether the relevant docu"
Y07-1008,O06-5003,1,0.887934,"ppear in this translation community. In this case, they are related to the organization of translators and to data manipulation, and reflect separated and group functions. It is very difficult to delimit such behaviors either at translator or group level. But some lacks and needed functionalities are common to this and other communities (FRENCHMOZILLA, W3C, etc.). These functionalities, which correspond to the practices (i) and (ii) cited above, can be categorized as follows: a. Functionalities for translator who operates on translation in autonomous way and disposing of private environments (Bey et al., 2006 (a)) (Abekawa et al., 2007). b. Functionalities for collaboration and work between translators (Bey et al., 2006 (b)). 89 In the next section, details of these two categories of practice are described and the specification for functionalities implementation is given. 3. Description of integrated functionalities: the first step toward a solution In the process of translation, a volunteer translator will want individually exploit the potential of various automatic linguistic functionalities (e.g., dictionaries and MT suggestions) and at the same time interact with her/his counterparts by using"
Y07-1008,2001.mtsummit-papers.66,0,0.00924938,"unctionalities (e.g. asking for translation community aid when linguistic aids aren’t sufficient). From this point of view, translator and community functionalities should be implemented as follows. 3.1. Individual translator functionalities The environment should be open to all volunteers without any restriction. Individual translators should be able to take advantage at the following different functionalities: • Text extraction and tokenization: documents to be translated undergo a process of text extraction and tokenization for the identification of Translation Units (TU) (LINGPIPE, 2007) (Walker et al., 2001). • Linguistic aids: linguistic aids are integrated and activated automatically in an online asynchronous manner (see 4.3). • Online translator-oriented editor: translators read the source TUs synchronized with their corresponding target TUs and input the translation in the source TU in order, segment by segment, or jumping to the segment they wish to work on. The target TUs are replaced in fact automatically or manually by the &quot;best&quot; pre-translations of MT or TM – 3.2. Translator community functionalities Extended functionalities for the translation community have been implemented using the W"
Y07-1048,Y05-1004,0,0.0192925,"presented at the 35th annual meeting of the Behaviormetric Society of Japan. We would like to thank the participants for their useful comments. Copyright 2007 by Takafumi Suzuki and Kyo Kageura 459 important material for understanding Japanese politics as they reflect Japanese governmental policies, and prime ministerial attitudes and thoughts (Watanabe, 1974; Kusano, 2005). As the role of the media and the performance of politicians increase in importance in contemporary politics (Kusano, 2006), the style of prime ministers&apos; speeches, as well as their content, have attracted more attention (Ahrens, 2005; Azuma, 2006). Against this background, this study explores the textual characteristics of Japanese prime ministers’ Diet addresses, focusing on (a) the difference between the two types of Diet addresses and (b) the perceived changes made to these addresses by two powerful prime ministers, comparing the characteristics of their addresses with those of all prime ministers from 1945 to 2006. In order to clarify these points, we focus on the quantity and diversity of nouns, textual characteristics strongly related to the content of texts, instead of conventional contentindependent stylistic char"
Y15-2038,D12-1050,0,0.0252945,"thods 1, 2, and 3 and the baseline method using Skip-gram and CBOW with Mitchell and Lapata’s data set. Method 1 using CBOW and size 50 achieved the best result. The reason is the process of learning. The CBOW model predicts a word by adding the vectors of surrounding words. Therefore, Method 1 with the CBOW model predicts a word from the sum of the vectors for a verb and its object. Consequently, representations for verb-object pairs are consistent in the learning and generating processes. Table 5 shows the comparison with other methods. BL, HB, KS, and K denote the results of the methods of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013). Kartsaklis and Sadrzadeh (2013) used the ukWaC corpus (Baroni et al., 2009), and the other methods used the British National Corpus (BNC). Word2vec is the result of Hashimoto et al. (2014). They used the POS-tagged BNC and trained 50-dimensional word vectors with the Skip-gram model. We believe that our methods can be improved by using POS-tagged texts. 334 Method Method 1 with CBOW BL w/ BNC HB w/ BNC KS w/ ukWaC K w/BNC Word2vec Score 0.329 0.35 0.34 0.45 0.41 0.42 7 Conclusion and future work This p"
Y15-2038,D14-1082,0,0.0905814,"Missing"
Y15-2038,N13-1134,0,0.0288472,"Missing"
Y15-2038,W09-0211,0,0.0674344,"Missing"
Y15-2038,de-marneffe-etal-2006-generating,0,0.114345,"Missing"
Y15-2038,D11-1129,0,0.0238944,"tures by using the parse trees generated by an HPSG parser called Enju. Tensor factorization is a method that represents word meaning with not only vectors but also matrices. For example, a concept ‘car’ has many attributes such as information about color, shape, and functions. It seems to be difficult to represent phrases or sentences with a fixed-size vector because many concepts can appear in a sentence and each concept has its own attributes. Baroni and Zamparelli (2010) tried to represent attribute information of each word as a product of a matrix and a vector. Grefenstette and Sadrzadeh (2011) followed this approach and proposed new method that obtains the representations of verb meaning as tensors. Kartsaklis et al. (2012) proposed a method that calculates representations for sentences or phrases containing a subject, a verb and an object, based on Grefenstette and Sadrzadeh (2011)’s method. Recently, three dimensional tensors have been used for representing the relations of a subject, a verb and an object (de Cruys, 2009; de Cruys et al., 2013). 3 Word2vec Word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b) is the method to obtain distributed representations for a word by usi"
Y15-2038,D14-1163,0,0.0553301,"esentations for one sentence using RNN and a given syntactic tree (Socher et al., 2011; Socher et al., 2012). RNNM makes use of syntactic trees of sentences, as shown in Figure 1. It calculates a distributed representation for the parent node from 330 v(He runs the company) v(runs the company) v(the company) v(He) v(runs) v(the) v(company) Figure 1: RNNM structure with syntax tree the distributed representations for the child nodes in the syntactic trees. However, it uses only the skeletal structures of the syntactic trees; category and subject information in the syntactic trees are not used. Hashimoto et al. (2014) proposed a new method that acquires distributed representations for one sentence with information on words and phrase structures by using the parse trees generated by an HPSG parser called Enju. Tensor factorization is a method that represents word meaning with not only vectors but also matrices. For example, a concept ‘car’ has many attributes such as information about color, shape, and functions. It seems to be difficult to represent phrases or sentences with a fixed-size vector because many concepts can appear in a sentence and each concept has its own attributes. Baroni and Zamparelli (20"
Y15-2038,P13-1088,0,0.0169659,"baseline method using Skip-gram and CBOW with Mitchell and Lapata’s data set. Method 1 using CBOW and size 50 achieved the best result. The reason is the process of learning. The CBOW model predicts a word by adding the vectors of surrounding words. Therefore, Method 1 with the CBOW model predicts a word from the sum of the vectors for a verb and its object. Consequently, representations for verb-object pairs are consistent in the learning and generating processes. Table 5 shows the comparison with other methods. BL, HB, KS, and K denote the results of the methods of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013). Kartsaklis and Sadrzadeh (2013) used the ukWaC corpus (Baroni et al., 2009), and the other methods used the British National Corpus (BNC). Word2vec is the result of Hashimoto et al. (2014). They used the POS-tagged BNC and trained 50-dimensional word vectors with the Skip-gram model. We believe that our methods can be improved by using POS-tagged texts. 334 Method Method 1 with CBOW BL w/ BNC HB w/ BNC KS w/ ukWaC K w/BNC Word2vec Score 0.329 0.35 0.34 0.45 0.41 0.42 7 Conclusion and future work This paper proposed methods for ob"
Y15-2038,D13-1166,0,0.0603987,"ible for word2vec to reduce the calculation time dramatically. However, these models basically learn word-to-word relations, not phrase or sentence structures. When we make distributed representations for phrases or sentences, it is necessary to generate conPACLIC 29 stitutive distributed representations for phrases or sentences based on the principle of compositionality. Mitchell and Lapata (2008) and Mitchell and Lapata (2010) proposed the add model, which generates distributed representations for phrase structures, whereas Goller and K¨uchler (1996), Socher et al. (2012) and Tsubaki et al. (2013) proposed Recursive Neural Network (RNN) models for phrase structures. Recently, new models based on tensor factorization have been proposed (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012). The add model is a method to generate distributed representations for phrase structures or multi-word expressions by adding distributed representations for each word that constitutes the phrase structure. However, the word order and syntactic relations are lost as a result of the adding in the model. For example, suppose that we have the distributed representations f"
Y15-2038,C12-2054,0,0.0660267,"s or sentences, it is necessary to generate conPACLIC 29 stitutive distributed representations for phrases or sentences based on the principle of compositionality. Mitchell and Lapata (2008) and Mitchell and Lapata (2010) proposed the add model, which generates distributed representations for phrase structures, whereas Goller and K¨uchler (1996), Socher et al. (2012) and Tsubaki et al. (2013) proposed Recursive Neural Network (RNN) models for phrase structures. Recently, new models based on tensor factorization have been proposed (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012). The add model is a method to generate distributed representations for phrase structures or multi-word expressions by adding distributed representations for each word that constitutes the phrase structure. However, the word order and syntactic relations are lost as a result of the adding in the model. For example, suppose that we have the distributed representations for a verb, a subject and an object. The result of adding the distributed representations is the same if we change the order of the subject and the object. For example, consider the distributed representations for the following tw"
Y15-2038,W13-3513,0,0.0198613,"’s data set. Method 1 using CBOW and size 50 achieved the best result. The reason is the process of learning. The CBOW model predicts a word by adding the vectors of surrounding words. Therefore, Method 1 with the CBOW model predicts a word from the sum of the vectors for a verb and its object. Consequently, representations for verb-object pairs are consistent in the learning and generating processes. Table 5 shows the comparison with other methods. BL, HB, KS, and K denote the results of the methods of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013). Kartsaklis and Sadrzadeh (2013) used the ukWaC corpus (Baroni et al., 2009), and the other methods used the British National Corpus (BNC). Word2vec is the result of Hashimoto et al. (2014). They used the POS-tagged BNC and trained 50-dimensional word vectors with the Skip-gram model. We believe that our methods can be improved by using POS-tagged texts. 334 Method Method 1 with CBOW BL w/ BNC HB w/ BNC KS w/ ukWaC K w/BNC Word2vec Score 0.329 0.35 0.34 0.45 0.41 0.42 7 Conclusion and future work This paper proposed methods for obtaining distributed representations for verb-object pairs by us"
Y15-2038,P08-1028,0,0.695905,"o(dishes)” with the light verb ‘do’. Despite the difference between the representations of these sentences, they have the same meaning “I wash the dishes.” As such, there are various sentences that have the same meaning, but different representations. We examined the performance of each method by measuring the distance between distributed representations for verb-object pairs (‘do’ and ‘dishes’ pair) and those for the corresponding basic verb (‘wash’) or predicated argument structures (“wash(dishes)”). We also experimentally compared the previous methods and ours on the same data set used in (Mitchell and Lapata, 2008). 2 Related work There are many methods for acquiring word representations in vector space models. These methods can be classified into two approaches: the word occurrence approach and the word co-occurrence approach. 329 The word occurrence approach, including Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Probabilistic LSA (PLSA) (Hofman, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003), presupposes that distributions of word frequencies for each document (a word-document matrix) are given as input. In the word frequency approach, word representations are learned"
Y15-2038,P05-1011,0,0.040471,"input 331 4.2 Method 1 The CBOW model of word2vec is learned in a pseudo-task that predicts a word from surrounding words in the text. Thus, we expect that distributed representations for verb-object pairs can be acquired when the object is put near the verb. A large amount of training text is parsed by Enju, and new training text data is generated by inserting the object just after the verb for all verb-object pairs appearing in the corpus as follows. (original) I did many large white and blue round dishes. (modified) I do dish many large white and blue round dish. Enju (Miyao et al., 2005; Miyao and Tsujii, 2005; Ninomiya et al., 2006) is a parser that performs highspeed and high-precision parsing and generates syntactic structures based on HPSG theory (Pollard and Sag, 1994), a sophisticated grammar theory in linguistics. In addition, Enju can generate predicate argument structures. The Stanford Parser (de Marneffe et al., 2006; Chen and D.Manning, 2014) is often used, but it can analyze only syntactic structures. Therefore, we used Enju, which can parse syntactic structures and predicate argument structures. In PACLIC 29 INPUT Method 1, word2vec is trained from the new text data generated by using"
Y15-2038,W06-1619,1,0.738709,"The CBOW model of word2vec is learned in a pseudo-task that predicts a word from surrounding words in the text. Thus, we expect that distributed representations for verb-object pairs can be acquired when the object is put near the verb. A large amount of training text is parsed by Enju, and new training text data is generated by inserting the object just after the verb for all verb-object pairs appearing in the corpus as follows. (original) I did many large white and blue round dishes. (modified) I do dish many large white and blue round dish. Enju (Miyao et al., 2005; Miyao and Tsujii, 2005; Ninomiya et al., 2006) is a parser that performs highspeed and high-precision parsing and generates syntactic structures based on HPSG theory (Pollard and Sag, 1994), a sophisticated grammar theory in linguistics. In addition, Enju can generate predicate argument structures. The Stanford Parser (de Marneffe et al., 2006; Chen and D.Manning, 2014) is often used, but it can analyze only syntactic structures. Therefore, we used Enju, which can parse syntactic structures and predicate argument structures. In PACLIC 29 INPUT Method 1, word2vec is trained from the new text data generated by using Enju’s results to augmen"
Y15-2038,D14-1162,0,0.0862367,"erting the object just after the verb for all verb-object pairs appearing in the corpus as follows. (original) I did many large white and blue round dishes. (modified) I do dish many large white and blue round dish. Enju (Miyao et al., 2005; Miyao and Tsujii, 2005; Ninomiya et al., 2006) is a parser that performs highspeed and high-precision parsing and generates syntactic structures based on HPSG theory (Pollard and Sag, 1994), a sophisticated grammar theory in linguistics. In addition, Enju can generate predicate argument structures. The Stanford Parser (de Marneffe et al., 2006; Chen and D.Manning, 2014) is often used, but it can analyze only syntactic structures. Therefore, we used Enju, which can parse syntactic structures and predicate argument structures. In PACLIC 29 INPUT Method 1, word2vec is trained from the new text data generated by using Enju’s results to augment objects near verbs in the text. Then, distributed representations for verb-object pairs are generated by adding the distributed representations for the verb and the distributed representations for the object. HIDDEN OUTPUT W(t-2) W(t) Verb O(t) Object SUM W(t-1) W(t+1) W(t+2) 4.3 Method 2 We expect that distributed repre"
Y15-2038,D12-1110,0,0.316347,"llobert et al., 2011). It is likely possible for word2vec to reduce the calculation time dramatically. However, these models basically learn word-to-word relations, not phrase or sentence structures. When we make distributed representations for phrases or sentences, it is necessary to generate conPACLIC 29 stitutive distributed representations for phrases or sentences based on the principle of compositionality. Mitchell and Lapata (2008) and Mitchell and Lapata (2010) proposed the add model, which generates distributed representations for phrase structures, whereas Goller and K¨uchler (1996), Socher et al. (2012) and Tsubaki et al. (2013) proposed Recursive Neural Network (RNN) models for phrase structures. Recently, new models based on tensor factorization have been proposed (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012). The add model is a method to generate distributed representations for phrase structures or multi-word expressions by adding distributed representations for each word that constitutes the phrase structure. However, the word order and syntactic relations are lost as a result of the adding in the model. For example, suppose that we have the dis"
Y15-2038,D13-1014,0,0.0171042,"is likely possible for word2vec to reduce the calculation time dramatically. However, these models basically learn word-to-word relations, not phrase or sentence structures. When we make distributed representations for phrases or sentences, it is necessary to generate conPACLIC 29 stitutive distributed representations for phrases or sentences based on the principle of compositionality. Mitchell and Lapata (2008) and Mitchell and Lapata (2010) proposed the add model, which generates distributed representations for phrase structures, whereas Goller and K¨uchler (1996), Socher et al. (2012) and Tsubaki et al. (2013) proposed Recursive Neural Network (RNN) models for phrase structures. Recently, new models based on tensor factorization have been proposed (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012). The add model is a method to generate distributed representations for phrase structures or multi-word expressions by adding distributed representations for each word that constitutes the phrase structure. However, the word order and syntactic relations are lost as a result of the adding in the model. For example, suppose that we have the distributed representations f"
Y15-2038,D10-1115,0,\N,Missing
