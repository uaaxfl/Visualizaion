2020.repl4nlp-1.11,D15-1075,0,0.104178,"Missing"
2020.repl4nlp-1.11,P19-1441,0,0.0696535,"embeddings into the task’s label space. tions with identical size, a test set to evaluate the models and an augmentation set to augment the MultiNLI training set. In our experiments, we treat the augmentation set as a validation set and do not use it for training. 3 Model Description We propose a multi-task BERT model to jointly predict semantic roles and perform natural language inference. BERT is used as the shared encoder module and two separate decoder heads are appended on top of it to perform task-specific operations. The overall picture of the model can be seen from Figure 1. Following Liu et al. (2019), the tasks share the encoder part of the model including the lexicon encoder and all BERT layers. We follow the original sentence pair classification formulation for BERT while training it for NLI task. On the input side, we concatenate the premise and hypothesis tokens, add a [SEP] token at the end of both sentences, and finally add a [CLS] token at the beginning of the whole sequence. Figure 2 shows the token embedding for an example sentence pair. While processing an NLI input, we take the [CLS] token embedding at the BERT’s output and treat it as the summary of the whole sequence. The dim"
2020.repl4nlp-1.11,W19-5045,1,0.841613,"Missing"
2020.repl4nlp-1.11,P19-1595,0,0.0280889,"Missing"
2020.repl4nlp-1.11,P19-1334,0,0.118057,"nt, contradiction and neutral labels. It has been regarded as a central problem in natural language understanding and found its place in benchmarks such as GLUE (Wang et al., 2018). Contemporary neural network based models achieve state-of-the-art (SOTA) results on these benchmarks. However, scoring high on test sets that have a similar distribution to the training sets does not guarantee wider generalization. Models that top the leaderboards on standard test sets may perform poorly on specifically constructed evaluation sets targeting dataset biases. For instance, the HANS challenge dataset (McCoy et al., 2019b) showed that models trained on NLI get fooled easily by heuristics when the 78 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 78–88 c July 9, 2020. 2020 Association for Computational Linguistics Premise VERB ARG0 ARG1 The judge by the actor stopped the banker. stopped The judge by the actor the banker Hypoth. VERB ARG0 ARG1 The banker stopped the actor. stopped The banker the actor humans whereas MultiNLI includes five different genres of written and spoken English such as travel guides and telephone conversations. Both datasets have been used for t"
2020.repl4nlp-1.11,D17-1070,0,0.168943,"used the SNLI dataset as the NLI training source following Dasgupta et al. (2018). We used the validation set released with the Comparisons dataset for hyperparameter optimization during training of both the single-task and multi-task models. Unlike SNLI, this dataset contains only two labels, entailment and contradiction. Therefore, differently than Dasgupta et al. (2018), we converted the predicted neutral labels to contradiction to have a unified negative label. Table 4 compares the overall performance of our BERT based models and the previously examined models on the test set. InferSent (Conneau et al., 2017) is a sentence encoding based NLI model that uses LSTM as the encoder. Although it is more complex than the bag-of-words (BOW-MLP) model, their performances are similar on this set. We see that the performance of BERT models on the same category are much better than the simpler models. The high performance of BERT models on this category can be attributed to the fact that the BERT was pre-trained on a large corpus with missing word prediction and next sentence prediction tasks, making it more aware of the word order. However, in the remaining two categories, both the single-task and multi-task"
2020.repl4nlp-1.11,J05-1004,0,0.0800734,"l. (2018) suggested a collection of various tasks including NLI for a benchmark and proposed a novel approach to solve all those tasks using a single multi-task model. They casted each task to the question answering problem and trained a model to solve all of them jointly. Some recent studies have investigated the benefits of semantic role labeling on the performance of natural language inference models. Noticeably, Zhang et al. (2020, 2018) used SRL as a supplementary task for text comprehension tasks such as textual entailment and question answering. Similar to our work, they used PropBank (Palmer et al., 2005) style role annotations and treated SRL as a sequence tagging problem. Zhang et al. (2020)’s approach is different from ours in that they use a pre-trained, SOTA SRL model to provide semantic embeddings to enrich the contextual embeddings from BERT. They kept the SRL model frozen and trained other parts of the model including the BERT encoder. Similarly, Zhang et al. (2018) employed two different networks where one of them is an SRL model responsible for generating the semantic embeddings to support the other network which is trained to solve the downstream task at hand. Moreover, both network"
2020.repl4nlp-1.11,W18-2501,0,0.0128846,"investigation. Comparisons Dataset Results We trained BERT with single-task and multi-task learning approaches and compared them on the BERT Model Single-task Multi-task Training set: same more/less 85.3 47.9 80.5 47.9 SNLI not Avg. 44.5 59.2 51.3 59.9 Training set: MultiNLI same more/less not Avg. 74.1 88.3 74.3 78.9 63.3 97.3 91.9 84.2 Table 5: Percent accuracy of the BERT models on Comparisons dataset. 84 4.3 Training Details we discuss some models benefiting from syntax or semantic roles and touch on multi-task models. We used the PyTorch (Paszke et al., 2017) framework and the AllenNLP (Gardner et al., 2018) library for implementation. We adapted some code to implement the multi-task training logic from Sanh et al. (2019)’s hierarchical multi-task learning project2 . In all experiments, we used the base version of BERT by initializing it with the weights released by Devlin et al. (2018). We use uniform mini-batches, i.e. a mini-batch contains instances from a single-task. Each dataset is divided into mini-batches with the same size and an iterator for each of them is created that can cycle through a dataset and provide batches indefinitely. In a training step, we decide which task to train with a"
2020.repl4nlp-1.11,D14-1162,0,0.0870312,"approach is different from ours in that they use a pre-trained, SOTA SRL model to provide semantic embeddings to enrich the contextual embeddings from BERT. They kept the SRL model frozen and trained other parts of the model including the BERT encoder. Similarly, Zhang et al. (2018) employed two different networks where one of them is an SRL model responsible for generating the semantic embeddings to support the other network which is trained to solve the downstream task at hand. Moreover, both networks in this model use pretrained word embeddings such as ELMo (Peters et al., 2018) or GloVe (Pennington et al., 2014). The main difference of our approach from these is that we use a single network and train it in a multi-task fashion by sharing encoder representations among different tasks. Moreover, unlike our work, they evaluated their models on the original datasets e.g. MultiNLI, so their focus was not to improve the model performance on the adversarial evaluation sets. 6 Conclusion and Future Work This work presents a multi-task learning approach using SRL task to apply an inductive bias on a BERT based NLI model. Our experiments show that joint training with SRL makes the model more robust to the supe"
2020.repl4nlp-1.11,P17-1044,0,0.0233805,"since the premise contains all words of the hypothesis. Existing approaches commonly tackle such adversaries by training the model with a dataset augmented with similar adversarial examples. As detailed in Section 5, the problem with this approach is that it might lead to overfitting to the adversaries on the augmentation set. Therefore, it can decrease the performance on other possible adversaries and hurt generalization (Nie et al., 2018). Semantic Role Labeling (SRL) asks the “who did what to whom, when and where etc.” questions to find the semantic roles of words or phrases in a sentence (He et al., 2017). We hypothesize that using the SRL task as a joint objective should improve the semantic knowledge of the models, thus making them less prone to dataset biases. Consider the semantic roles in the previous example sentence pair, which are shown in Table 1. We can see that the role of “the banker” differs between the sentences. Since “stop” is not a reciprocal verb, a model that is aware of the semantic roles can find out that the inference relation is non-entailment although the premise contains all words in the hypothesis, albeit with a different order. In contrast, if a model pays too much a"
2020.repl4nlp-1.11,N18-1202,0,0.0123171,"problem. Zhang et al. (2020)’s approach is different from ours in that they use a pre-trained, SOTA SRL model to provide semantic embeddings to enrich the contextual embeddings from BERT. They kept the SRL model frozen and trained other parts of the model including the BERT encoder. Similarly, Zhang et al. (2018) employed two different networks where one of them is an SRL model responsible for generating the semantic embeddings to support the other network which is trained to solve the downstream task at hand. Moreover, both networks in this model use pretrained word embeddings such as ELMo (Peters et al., 2018) or GloVe (Pennington et al., 2014). The main difference of our approach from these is that we use a single network and train it in a multi-task fashion by sharing encoder representations among different tasks. Moreover, unlike our work, they evaluated their models on the original datasets e.g. MultiNLI, so their focus was not to improve the model performance on the adversarial evaluation sets. 6 Conclusion and Future Work This work presents a multi-task learning approach using SRL task to apply an inductive bias on a BERT based NLI model. Our experiments show that joint training with SRL make"
2020.repl4nlp-1.11,P82-1020,0,0.768881,"Missing"
2020.repl4nlp-1.11,W13-3516,0,0.0483141,"Missing"
2020.repl4nlp-1.11,P18-1031,0,0.0196912,"studies generally use a single global optimizer for all tasks (Sanh et al., 2019; Liu et al., 2019). In this work, we tried both this approach and using a different optimizer for each task. The advantage of using multiple optimizers is that the learning rates of the individual tasks can be set to different values, and each task can have its own learning rate scheduler. We used BertAdam optimizer from HuggingFace, and set its maximum learning rate to 2e-5 or 5e-5 according to the validation accuracy on the NLI evaluation task. Moreover, we employed a slanted triangular learning rate scheduler (Howard and Ruder, 2018) with a cut fraction of 0.1 and decay factor of 0.38. In all experiments the maximum sequence length is set to 256, and longer sequences are truncated. In all training experiments, 4 GPUs were used in parallel and the datasets were divided into mini-batches of size 12 based on GPU memory limitations. 5 Some previous studies used sentence embedding based approaches to solve NLI. Noticeably, InferSent (Conneau et al., 2017) uses an LSTM to encode the premise and hypothesis sentences independently. Then, it concatenates the premise, hypothesis embeddings and two feature vectors obtained by their"
2020.repl4nlp-1.11,W18-5446,0,0.0419535,"Missing"
2020.repl4nlp-1.11,N18-1101,0,0.06088,"Missing"
2020.semeval-1.271,2020.lrec-1.758,0,0.0604782,"Missing"
2020.semeval-1.271,N19-1423,0,0.497784,"shared task of Multilingual Offensive Language Identification (OffensEval2020), we focus on detecting offensive language on social media platforms, more specifically, on Twitter. The organizers provided data from five different languages, which we worked on three languages of them, namely, Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). More details about the annotation process have been described in task description paper (Zampieri et al., 2020). The approach used combines the knowledge embedded in pre-trained deep bidirectional transformer BERT (Devlin et al., 2019) with Convolutional Neural Networks (CNN) for text (Kim, 2014), which is one of the most utilized approaches for text classification tasks. This combination of models has been shown to yield better results than using BERT or CNN on their own, as was shown in (Li et al., 2019), and shown in this paper. This model, and with minimum text pre-processing, ranked 4th in Arabic, 4th in Greek, and 3rd in Turkish among more than 40 participants. In the following sections of this paper, previous work is mentioned in Section 2, next, the data is described in Section 3, then the details of the model and t"
2020.semeval-1.271,D14-1181,0,0.0451622,"l2020), we focus on detecting offensive language on social media platforms, more specifically, on Twitter. The organizers provided data from five different languages, which we worked on three languages of them, namely, Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). More details about the annotation process have been described in task description paper (Zampieri et al., 2020). The approach used combines the knowledge embedded in pre-trained deep bidirectional transformer BERT (Devlin et al., 2019) with Convolutional Neural Networks (CNN) for text (Kim, 2014), which is one of the most utilized approaches for text classification tasks. This combination of models has been shown to yield better results than using BERT or CNN on their own, as was shown in (Li et al., 2019), and shown in this paper. This model, and with minimum text pre-processing, ranked 4th in Arabic, 4th in Greek, and 3rd in Turkish among more than 40 participants. In the following sections of this paper, previous work is mentioned in Section 2, next, the data is described in Section 3, then the details of the model and the other experiments are described in Section 4. Finally, the"
2020.semeval-1.271,2020.acl-main.156,0,0.299091,"Missing"
2020.semeval-1.271,2020.lrec-1.629,0,0.0125756,"to this problem vary from manual control to rule-based filtering systems; however, these methods are time-consuming or prone to errors if the full context is not taken into consideration while assessing the sentiment of the text (Saif et al., 2016). In Subtask-A of the shared task of Multilingual Offensive Language Identification (OffensEval2020), we focus on detecting offensive language on social media platforms, more specifically, on Twitter. The organizers provided data from five different languages, which we worked on three languages of them, namely, Arabic (Mubarak et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020). More details about the annotation process have been described in task description paper (Zampieri et al., 2020). The approach used combines the knowledge embedded in pre-trained deep bidirectional transformer BERT (Devlin et al., 2019) with Convolutional Neural Networks (CNN) for text (Kim, 2014), which is one of the most utilized approaches for text classification tasks. This combination of models has been shown to yield better results than using BERT or CNN on their own, as was shown in (Li et al., 2019), and shown in this paper. This model, and with minimu"
2020.semeval-1.271,W17-1101,0,0.0135926,"and 3rd in Turkish among more than 40 participants. In the following sections of this paper, previous work is mentioned in Section 2, next, the data is described in Section 3, then the details of the model and the other experiments are described in Section 4. Finally, the submissions and the other experiments are detailed in Section 5. 2 Background Extensive work has been performed to solve the task of offensive speech identification, which classifies among text classification tasks. Approaches to solve this problem vary from using lexical resources, linguistic features, and meta information (Schmidt and Wiegand, 2017), to machine learning (ML) models This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. Our source code of the main model and the other experiments can be accessed through: https://github.com/ alisafaya/OffensEval2020 2054 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2054–2059 Barcelona, Spain (Online), December 12, 2020. (Davidson et al., 2017), and more recently, deep neural models like CNN and Long-Short Term Memory (LSTM) and their derivatives (Zhang et al., 2018)."
2020.semeval-1.271,N19-1144,0,0.0493629,"Missing"
2021.case-1.1,2021.case-1.4,1,0.808064,"Missing"
2021.case-1.1,2021.case-1.23,1,0.586051,"ned issues in sociopolitical and crisis event studies have been studied by numerous scholars for decades to date, there are still no answers or solutions to them (Wang et al., 2016; Lorenzini et al., 2016; Schrodt, 2020; Raleigh, 2020; Eck, 2021; Boschee, 2021). Therefore, we aim at contributing to the understanding and resolution of event database creation via quantifying performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the system description papers in this proceedings respectively. We provide a summary of the tasks and the findings in the following subsections. Radford (2021) presents a study on geocoding and a new data set. Geocoding is an important subtask of event detection, in which the goal is to find the geographic coordinates associated with event descriptions. The paper presents an “end-to-end pr"
2021.case-1.1,D19-2004,0,0.0230739,"tions for situation awareness, using various branches of artificial intelligence (AI), natural language processing (NLP), machine learning (ML), and advanced statistical methods. ing (ML) and NLP methods to deal better with the vast amount and variety of data in this domain (H¨urriyeto˘glu et al., 2021). Nonetheless, automated approaches suffer from major issues like bias, low generalizability, class imbalance, training data limitations, ethical issues, and lack of recall quantification which affect the quality of the results and their use drastically (Leins et al., 2020; Bhatia et al., 2020; Chang et al., 2019; Y¨or¨uk et al., 2021). Moreover, the results of the automated systems for socio-political event information collection may not be comparable to each other or not of sufficient quality (Wang et al., 2016; Schrodt, 2020). Socio-political events are varied and nuanced. Both the political context and the local language used may affect whether and how they are reported. Therefore, all steps of information collection (event definition, language resources, and manual or algorithmic steps) may need to be constantly updated. This leads us to a series of challenging questions such as: Do events relate"
2021.case-1.1,2021.case-1.16,0,0.401507,"sifier, which identifies whether a pair 7 https://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the evaluation scenarios specified with subtask and language combinations. The best submissions utilized deep learning approaches that combine the training data in various languages, utilize large models, further re-train the models, and create ensemble models (Awasthy et al., 2021; Hettiarachchi et al., 2021; Re et al., 2021; Hu and Stoehr, 2021; Tan et al., 2021). Although training data was limited in Portuguese and Spanish and not available in Hindi, the best performing participants managed to deliver predictions that are between 77.27 and 93.03 F1macro in subtasks 1, 2, and 3 for all languages. The performance of the best system for subtask 4 for all languages was between 66.20 and 78.11 for all languages and 4-5 F1-macro points ahead of all other teams in all languages. 3.2 from the test data and inclusion of some unseen classes the top results obtained can be considered good, however, there"
2021.case-1.1,2021.case-1.20,0,0.462297,"ps://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the evaluation scenarios specified with subtask and language combinations. The best submissions utilized deep learning approaches that combine the training data in various languages, utilize large models, further re-train the models, and create ensemble models (Awasthy et al., 2021; Hettiarachchi et al., 2021; Re et al., 2021; Hu and Stoehr, 2021; Tan et al., 2021). Although training data was limited in Portuguese and Spanish and not available in Hindi, the best performing participants managed to deliver predictions that are between 77.27 and 93.03 F1macro in subtasks 1, 2, and 3 for all languages. The performance of the best system for subtask 4 for all languages was between 66.20 and 78.11 for all languages and 4-5 F1-macro points ahead of all other teams in all languages. 3.2 from the test data and inclusion of some unseen classes the top results obtained can be considered good, however, there is place for improvement. 3.3 Task 3:"
2021.case-1.1,2021.case-1.11,1,0.396514,"Missing"
2021.case-1.1,2021.case-1.3,0,0.0306167,"nt database creation comprises of three steps that are collecting events, classifying them, and measuring utility of the system output, which is an event database, against ground-truth. Each of these steps contains pitfalls and subject to limitations. For instance, the data source utilized maybe biased or a ground-truth may not be available. Although aforementioned issues in sociopolitical and crisis event studies have been studied by numerous scholars for decades to date, there are still no answers or solutions to them (Wang et al., 2016; Lorenzini et al., 2016; Schrodt, 2020; Raleigh, 2020; Eck, 2021; Boschee, 2021). Therefore, we aim at contributing to the understanding and resolution of event database creation via quantifying performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the system desc"
2021.case-1.1,2021.findings-acl.371,0,0.0374574,"Missing"
2021.case-1.1,2021.case-1.5,0,0.0130576,"n Hong Kong sources when the Movement emerged. 3.1 Task 1: Multilingual protest news detection The task is designed to be both multilingual (having both training and test data in English, Portuguese, and Spanish) and cross-lingual (having data in Hindi only for test). There are four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence classification (subtask 3), and event extraction (subtask 4). Event information is at the center of all of the subtasks, i.e. documents and sentences are classified as containing event information in subtasks Kar et al. (2021) describe an algorithm for event argument detection and aggregation. The paper reports on document level aggregation of the following argument types: Time, Place, Casualties, After-Effect, Reason, and Participant. The ArgFuse algorithm is based on a BERT based active learning classifier, which identifies whether a pair 7 https://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the"
2021.case-1.1,2021.case-1.10,0,0.0428229,"Missing"
2021.case-1.1,2020.acl-main.261,0,0.0473763,"Missing"
2021.case-1.1,2021.case-1.22,0,0.170981,"ther a pair 7 https://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the evaluation scenarios specified with subtask and language combinations. The best submissions utilized deep learning approaches that combine the training data in various languages, utilize large models, further re-train the models, and create ensemble models (Awasthy et al., 2021; Hettiarachchi et al., 2021; Re et al., 2021; Hu and Stoehr, 2021; Tan et al., 2021). Although training data was limited in Portuguese and Spanish and not available in Hindi, the best performing participants managed to deliver predictions that are between 77.27 and 93.03 F1macro in subtasks 1, 2, and 3 for all languages. The performance of the best system for subtask 4 for all languages was between 66.20 and 78.11 for all languages and 4-5 F1-macro points ahead of all other teams in all languages. 3.2 from the test data and inclusion of some unseen classes the top results obtained can be considered good, however, there is place for impr"
2021.case-1.1,2021.case-1.7,0,0.0366005,"tasks and the findings in the following subsections. Radford (2021) presents a study on geocoding and a new data set. Geocoding is an important subtask of event detection, in which the goal is to find the geographic coordinates associated with event descriptions. The paper presents an “end-to-end probabilistic model” for geocoding from text data. A novel data set has been created for evaluating the performance of geocoding systems. The output of the new model is compared with a state-of-the-art model, called Mordecai. The comparison clearly shows an improvement provided by the proposed model. Scharf et al. (2021) report on a study on the political bias in Hong Kong published news reporting about protest events. The paper reports on lexical differences between home and Western news sources about protests happening in Hong Kong in the period 1998-2020. Experiments on topic modeling, sentiment analysis, lexical distribution and comparative lexical analysis between Westernand Hong Kong-based sources reveal a bias in the reporting from the Hong Kong press. The evaluation reveals that during the Anti-Extradition Law Amendment Bill Movement reports from Hong Kong made fewer references to police violence comp"
2021.case-1.1,2020.aespen-1.3,0,0.0664894,"the vast amount and variety of data in this domain (H¨urriyeto˘glu et al., 2021). Nonetheless, automated approaches suffer from major issues like bias, low generalizability, class imbalance, training data limitations, ethical issues, and lack of recall quantification which affect the quality of the results and their use drastically (Leins et al., 2020; Bhatia et al., 2020; Chang et al., 2019; Y¨or¨uk et al., 2021). Moreover, the results of the automated systems for socio-political event information collection may not be comparable to each other or not of sufficient quality (Wang et al., 2016; Schrodt, 2020). Socio-political events are varied and nuanced. Both the political context and the local language used may affect whether and how they are reported. Therefore, all steps of information collection (event definition, language resources, and manual or algorithmic steps) may need to be constantly updated. This leads us to a series of challenging questions such as: Do events related to minority groups are represented well? Are new types of events covered? Are the event definitions and their operationalization comparable across systems? We organize the workshop on Challenges and Applications of Aut"
2021.case-1.1,2021.case-1.8,0,0.0985759,"performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the system description papers in this proceedings respectively. We provide a summary of the tasks and the findings in the following subsections. Radford (2021) presents a study on geocoding and a new data set. Geocoding is an important subtask of event detection, in which the goal is to find the geographic coordinates associated with event descriptions. The paper presents an “end-to-end probabilistic model” for geocoding from text data. A novel data set has been created for evaluating the performance of geocoding systems. The output of the new model is compared with a state-of-the-art model, called Mordecai. The comparison clearly shows an improvement provided by the proposed model. Scharf et al. (2021) report on a study on the political bias in Hon"
2021.case-1.1,2020.aespen-1.2,0,0.0325023,"The work on event database creation comprises of three steps that are collecting events, classifying them, and measuring utility of the system output, which is an event database, against ground-truth. Each of these steps contains pitfalls and subject to limitations. For instance, the data source utilized maybe biased or a ground-truth may not be available. Although aforementioned issues in sociopolitical and crisis event studies have been studied by numerous scholars for decades to date, there are still no answers or solutions to them (Wang et al., 2016; Lorenzini et al., 2016; Schrodt, 2020; Raleigh, 2020; Eck, 2021; Boschee, 2021). Therefore, we aim at contributing to the understanding and resolution of event database creation via quantifying performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the"
2021.case-1.1,2021.findings-acl.314,0,0.0197891,"(ERC) Starting Grant 714868 awarded to Dr. Erdem Y¨or¨uk for his project Emerging Welfare. Halterman and Radford (2021) show utility of “upsampling” coarse document labels to finegrained labels or spans for protest size detection; and References Parul Awasthy, Jian Ni, Ken Barker, and Radu Florian. 2021. IBM MNLP IE at CASE 2021 task 1: Multi-granular and multilingual event detection on protest news. In Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), online. Association for Computational Linguistics (ACL). Tsarapatsanis and Aletras (2021) discuss the importance of academic freedom, the diversity of legal and ethical norms, and the threat of moralism in the computational law field. 6 Conclusion This workshop is the fourth event from a series of workshops on automatic extraction of sociopolitical events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission, with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development in the area of event extraction of socio-politica"
2021.case-1.1,2021.case-1.6,0,0.0619007,"Missing"
2021.case-1.1,2021.findings-acl.186,0,0.0387072,"a modified version of BERT. Most papers use BERT embeddings as features in their models and one paper discusses an algorithm, which uses a full syntactic parser. Sentiment analysis is used in one paper, which studies the political bias of the news. The shared task results shed light on critical aspects of the automated socio-political extraction and evaluation methodology. 8 Invited Talks The workshop contains an invited talks session as well. The authors of the papers published in Findings of ACL and related to workshop theme are invited to present their work in this session. The papers are Zhou et al. (2021) propose an event-driven trading strategy that predicts stock movements by detecting corporate events from news articles; Halterman et al. (2021) introduce the IndiaPoliceEvents Corpus—all 21,391 sentences from 1,257 Times of India articles about events in the state of Gujarat during March 2002; Acknowledgments The authors from Koc University were funded by the European Research Council (ERC) Starting Grant 714868 awarded to Dr. Erdem Y¨or¨uk for his project Emerging Welfare. Halterman and Radford (2021) show utility of “upsampling” coarse document labels to finegrained labels or spans for pro"
C10-2159,C04-1080,0,0.0386752,"Missing"
C10-2159,P08-1085,0,0.0388566,"Missing"
C10-2159,P07-1094,0,0.0488169,"Missing"
C10-2159,S07-1100,0,0.0507636,"Missing"
C10-2159,D07-1031,0,0.0380928,"Missing"
C10-2159,J94-2001,0,0.243285,"Missing"
C10-2159,P09-1057,0,0.0843973,"Missing"
C10-2159,P05-1044,0,0.0856545,"Missing"
C10-2159,J10-1004,1,0.853251,"Missing"
C10-2159,S07-1044,1,0.828955,"Missing"
C10-2159,J93-2004,0,\N,Missing
C14-1217,P10-1131,0,0.0574711,"Missing"
C14-1217,P11-1087,0,0.302195,"y different instances of a word in a single category (which we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of a word separately and can handle ambiguous words. Examples of word-based systems include ones that represent each word with the vector of neighboring words (context vectors) and cluster them (Sch¨utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003), restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn, 2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2303 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2303–2313, Dublin, Ireland, August 23-29 2014. Figure 1: The accuracy comparison of word and instance based part-of-speech induction models as a function of target word amb"
C14-1217,J92-4003,0,0.463545,"ve natural language processing systems when used as additional features. Word-based POS induction systems classify different instances of a word in a single category (which we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of a word separately and can handle ambiguous words. Examples of word-based systems include ones that represent each word with the vector of neighboring words (context vectors) and cluster them (Sch¨utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003), restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn, 2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2303 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2303–2313, Dublin, Ireland, August 23-29 2014. Figure"
C14-1217,W06-2920,0,0.055048,"Missing"
C14-1217,D10-1056,0,0.131981,"Missing"
C14-1217,D11-1059,0,0.175487,"he one-tag-per-word assumption). Instance-based systems classify each occurence of a word separately and can handle ambiguous words. Examples of word-based systems include ones that represent each word with the vector of neighboring words (context vectors) and cluster them (Sch¨utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003), restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn, 2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2303 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2303–2313, Dublin, Ireland, August 23-29 2014. Figure 1: The accuracy comparison of word and instance based part-of-speech induction models as a function of target word ambiguity (as measured by gold-standard-tag perplexity described in Section 3.3) on the Penn"
C14-1217,E03-1009,0,0.188735,"processing systems when used as additional features. Word-based POS induction systems classify different instances of a word in a single category (which we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of a word separately and can handle ambiguous words. Examples of word-based systems include ones that represent each word with the vector of neighboring words (context vectors) and cluster them (Sch¨utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003), restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn, 2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2303 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2303–2313, Dublin, Ireland, August 23-29 2014. Figure 1: The accurac"
C14-1217,erjavec-2004-multext,0,0.0271651,"Missing"
C14-1217,D08-1036,0,0.0365692,"Missing"
C14-1217,N06-1041,0,0.0694095,"Missing"
C14-1217,D07-1031,0,0.442302,"Missing"
C14-1217,D10-1078,0,0.0223199,"on 4. Finally, the induced categories or the vector representations generated by the induction algorithms may improve natural language processing systems when used as additional features. Word-based POS induction systems classify different instances of a word in a single category (which we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of a word separately and can handle ambiguous words. Examples of word-based systems include ones that represent each word with the vector of neighboring words (context vectors) and cluster them (Sch¨utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003), restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn, 2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2303 Proceedings of COLING 2014, the 25th International Conf"
C14-1217,P10-2040,0,0.0178195,"on 4. Finally, the induced categories or the vector representations generated by the induction algorithms may improve natural language processing systems when used as additional features. Word-based POS induction systems classify different instances of a word in a single category (which we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of a word separately and can handle ambiguous words. Examples of word-based systems include ones that represent each word with the vector of neighboring words (context vectors) and cluster them (Sch¨utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003), restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn, 2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2303 Proceedings of COLING 2014, the 25th International Conf"
C14-1217,D10-1083,0,0.0172268,"ord ratio on MULTEXT-East Serbian corpus. We use ukWaC corpora to train English language models. We used the default settings in Section 3.2 and incorporated only the orthographic features8 . Extracting unsupervised morphological features for languages with different characteristics would be of great value, but it is beyond the scope of this paper. For each language the number of induced clusters is set to the number of tags in the gold-set. To perform meaningful comparisons with the previous work we train and evaluate our models on the training section of MULTEXT-East9 and CONLL-X languages (Lee et al., 2010). Table 3 presents the performance of our instance based model on 19 corpora in 15 languages together with the corresponding best published results from ⇧ (Yatbaz et al., 2012), ‡ (Blunsom and Cohn, 2011), 6 The difference is not statistically significant at p = 0.05. Latest Wikipedia dump files are freely available at http://dumps.wikimedia.org/ and the text in the dump files can be extracted using WP2TXT (http://wp2txt.rubyforge.org/) 8 All corpora (except German, Spanish and Swedish) label the punctuation marks with the same gold-tag therefore we add an extra punctuation feature for those l"
C14-1217,D07-1043,0,0.0417536,"s incorporate orthographic and morphological features. Berg-Kirkpatrick et al. (2010) and Christodoulopoulos et al. (2010) use instance based models. 3.1 Evaluation We report many-to-one and V-measure scores for our experiments as suggested in (Christodoulopoulos et al., 2010). The many-to-one (MTO) evaluation maps each cluster to its most frequent gold tag and reports the percentage of correctly tagged instances. The MTO score can be increased by simply increasing number of clusters, thus the number of clusters is fixed to match the number of gold tags in each experiment. The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theory motivated metric that calculates the harmonic mean of completeness and homogeneity of the clusters. Completeness of a cluster is maximized when all instances of a gold-tag are grouped into the same cluster and the homogeneity is maximized when the members of a cluster belong to the same gold-tag. 3.2 Experimental Settings and Results To make a direct comparison with previously published results, the Wall Street Journal Section of the Penn Treebank was used as the test corpus (1,173,766 instances, 49,206 unique tokens) for English experiments. PTB uses 45 part-of-speec"
C14-1217,E95-1020,0,0.351812,"Missing"
C14-1217,N03-1033,0,0.130033,"Missing"
C14-1217,D12-1086,1,0.123823,"d achieve the highest instance-based accuracy on PTB. Christodoulopoulos et al. (2010) select prototypes of each cluster from the output of Brown (1992) and feed them to a HMM model that can handle prototypes as features (Haghighi and Klein, 2006). However none of these models achieve results comparable to the best word-based systems. In this work, we show that one can build an instance-based system that can perform significantly better on highly ambiguous words (see Figure 1) and yet is competitive with word-based systems in overall accuracy. We follow the state of the art word-based system (Yatbaz et al., 2012) and use probable substitutes of a word instance as its contextual features. The following examples illustrate how such paradigmatic (substitute based) contextual features can capture the similarity between two contexts where a syntagmatic (neighbor based) representation would fail: 1 NN, VB and VBP are three POS tags from the Penn Treebank corpus and they correspond to singular noun, verb in base form and non-3rd person singular verb in present tense, respectively. The numbers in parentheses are the frequencies. 2304 (1) “Pierre Vinken, 61 years old, will join the board as a nonexecutive dire"
C14-1217,P07-1094,0,\N,Missing
C16-1068,D10-1056,0,0.0237788,"s form new categories depending on the overlaps between the context words. In the related part-of-speech induction literature2 , Sch¨utze (1995) incorporates paradigmatic information by concatenating the left and the right co-occurrence vectors of the right and left neighbors respectively, and groups words that have similar vectors. The limitation of this model is that it uses only bi-gram information and suffers from sparsity as the context size gets larger. Yatbaz et al. (2012) calculate the most probable substitutes of a given context using a 4-gram statistical language model. Their 2 See (Christodoulopoulos et al., 2010) for a comprehensive review of the part-of-speech induction literature. 708 model achieves the state-of-the-art result in the part-of-speech induction literature. Part-of-speech induction aims to induce word-categories from large amounts of unannotated text (mostly news corpora). Our paper evaluates the substitute-based context representation by Yatbaz et al. (2012) as a possible feature for classifying words in relatively small amounts of child-directed speech. 3 Method In this section we explain the experimental methodology we used, including how the input corpora was processed, the language"
C16-1068,E95-1020,0,0.608894,"Missing"
C16-1068,D12-1086,1,0.805961,"tional knowledge when the evidence in the possible context of a word is not enough. Furthermore, they explain how and when language users form new categories depending on the overlaps between the context words. In the related part-of-speech induction literature2 , Sch¨utze (1995) incorporates paradigmatic information by concatenating the left and the right co-occurrence vectors of the right and left neighbors respectively, and groups words that have similar vectors. The limitation of this model is that it uses only bi-gram information and suffers from sparsity as the context size gets larger. Yatbaz et al. (2012) calculate the most probable substitutes of a given context using a 4-gram statistical language model. Their 2 See (Christodoulopoulos et al., 2010) for a comprehensive review of the part-of-speech induction literature. 708 model achieves the state-of-the-art result in the part-of-speech induction literature. Part-of-speech induction aims to induce word-categories from large amounts of unannotated text (mostly news corpora). Our paper evaluates the substitute-based context representation by Yatbaz et al. (2012) as a possible feature for classifying words in relatively small amounts of child-di"
C16-1068,D12-1059,0,\N,Missing
C16-1087,W10-2417,0,0.024538,"ristics. Since our model only uses the labeled training data, and no external resources such as gazetteers, we selected previous works which report the top scores without use of any additional data to make a fair comparison. We also included the best results which do make use of arbitrary external resources for each language. We summarized all the comparisons in Table 6. The results highlight that our model attains good performance and it is robust across variety of languages. We achieve similar to or better than the state-of-the-art in Named Entity Recognition that use no external resources. Abdul-Hamid and Darwish (2010) employ a CRF sequence labeling model which is trained on features that primarily use character n-gram of leading and trailing letters in words and word n-grams. They assert that the proposed features help overcome some of the morphological and orthographic complexities of Arabic. Although they do not utilize any external resource, they apply Arabic specific input preprocessing before training which may be the reason for better performance. Demir and Ozgur (2014) employs a window-based classifier approach with language independent features for Best Best w/o External CharNER Arabic 84.30 [1] 79"
C16-1087,D15-1041,0,0.0122648,"et al. (2015) employs a convolutional neural network (CNN) over characters to form word representations. Ling et al. (2015a) achieve state-of-the-art results in language modeling and part-of-speech tagging by utilizing these word representations. Kim et al. (2015) use word representations constructed by CNN with recurrent neural network for language modeling. They show that taking characters as the primary representation is sufficient to encode both semantic and orthographic information and their model is on par with the existing state-of-the-art despite having significantly fewer parameters. Ballesteros et al. (2015) employs the same strategy with (Ling et al., 2015a) to represent each token for a continuous-state dependency parsing model. They show that the parsing model benefits from incorporating the character-based encodings of words for morphologically rich languages. Zhang and LeCun (2015) demonstrate that convolutional neural networks are successful at mapping characters directly to ontology/sentiment classes and text categories. Ling et al. (2015b) introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. They show that the"
C16-1087,W02-2004,0,0.0432759,"Missing"
C16-1087,P13-1153,0,0.0329147,"Missing"
C16-1087,P16-2044,0,0.0604018,"lein et al., 2003) and employs a much more compact network than (Gillick et al., 2016). 2.3 Other character-based models Character-based models have been used successfully for NLP tasks other than NER as well. Depending on the nature of the task, characters are utilized in two different ways. One line of work uses characters to form a word representation for each token in a sentence (Kim et al., 2015; Ling et al., 2015a; Ballesteros 912 et al., 2015). Alternatively character representations are used without mapping to words first (Klein et al., 2003; Zhang and LeCun, 2015; Ling et al., 2015b; Dhingra et al., 2016). Ling et al. (2015a) construct vector representations of words by composing characters using bidirectional LSTMs and Kim et al. (2015) employs a convolutional neural network (CNN) over characters to form word representations. Ling et al. (2015a) achieve state-of-the-art results in language modeling and part-of-speech tagging by utilizing these word representations. Kim et al. (2015) use word representations constructed by CNN with recurrent neural network for language modeling. They show that taking characters as the primary representation is sufficient to encode both semantic and orthographi"
C16-1087,P15-1033,0,0.026389,") receive the same NE phrase tag. Otherwise, characters get the outside tag (O). Figure 1 shows word-level and character-level tags for an example sentence. John B-PER works O for O Globex B-ORG Corp. I-ORG . O J o h n w o r k s f o r G l o b e x C o r p . . P P P P O O O O O O O O O O O G G G G G G G G G G G G O O Figure 1: An example sentence with word level and character level NER tags. 913 3.2 Long Short-Term Memory Recurrent Neural Networks (RNN) have recently achieved state of the art results in natural language processing tasks such as language modeling (Mikolov et al., 2010), parsing (Dyer et al., 2015), and machine translation (Sutskever et al., 2014). One major problem with simple RNNs is that they are difficult to train for long term dependencies due to the vanishing and the exploding gradient problems (Bengio et al., 1994). Hochreiter and Schmidhuber (1997) developed Long Short-Term Memory (LSTM) to overcome the long term dependency problem. They introduced a special memory cell which is controlled by input, output and forget gates. The input gate controls how much new information should be added to current cell, the forget gate controls what old information should be deleted. The output"
C16-1087,W03-0425,0,0.0662766,"datasets in detail and present our experiments and results. Section 5 summarizes our contributions. 2 Related Work In this section, we first outline the previous work on NER with word-level inputs then move onto character-based NER models. Next, we summarize the applications of character-based models in NLP in general. 2.1 Word based NER Early successful studies on NER use hand-crafted features and language specific name lists with wordlevel classifiers. Both of the first place submissions in CoNLL-2002 (Spanish & Dutch), CoNLL-2003 (English & German) NER shared tasks (Carreras et al., 2002; Florian et al., 2003) use a rich set of handcrafted features along with gazetteers to achieve top performance. Subsequently, semi-supervised approaches (Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Turian et al., 2010) have reported better results by utilizing large unlabeled corpora. Demir and Ozgur (2014) employs a semi-supervised learning approach to achieve best result for Czech. Darwish (2013) exploits cross-lingual features and knowledge bases from English data sources to achieve the top performance on Arabic. The current state-of-the-art system for Turkish (Seker and Eryigit, 2012) is based on Conditiona"
C16-1087,N16-1155,0,0.0291401,"from a given root is common, which makes models even more susceptible to the data sparsity problem. This work is motivated by the desire to eliminate the tedious work of feature engineering, language specific syntactic and morphological analyzers, and language specific lexical or named-entity resources which are considered necessary to accomplish good performance on Named Entity Recognition. We accomplish this by combining the following ideas: First, instead of considering entire words as the basic input features, we take the characters as the primary representation as in (Klein et al., 2003; Gillick et al., 2016). Second, we use a stacked bidirectional Long Short Term Memory (LSTM) model (Hochreiter and Schmidhuber, 1997) which is able to operate on sequential data of arbitrary length and encode observed patterns in its memory at different scales. Finally, we use a Viterbi decoder to convert the character level tag probabilities produced by the LSTM into consistent word level tags. Considering characters as the primary representation proves fruitful in several ways. Characters provide sub-word-level syntactic, morphological, and orthographic information that can be directly exploited by our model, whe"
C16-1087,W03-0428,0,0.0452149,"f hundreds of words from a given root is common, which makes models even more susceptible to the data sparsity problem. This work is motivated by the desire to eliminate the tedious work of feature engineering, language specific syntactic and morphological analyzers, and language specific lexical or named-entity resources which are considered necessary to accomplish good performance on Named Entity Recognition. We accomplish this by combining the following ideas: First, instead of considering entire words as the basic input features, we take the characters as the primary representation as in (Klein et al., 2003; Gillick et al., 2016). Second, we use a stacked bidirectional Long Short Term Memory (LSTM) model (Hochreiter and Schmidhuber, 1997) which is able to operate on sequential data of arbitrary length and encode observed patterns in its memory at different scales. Finally, we use a Viterbi decoder to convert the character level tag probabilities produced by the LSTM into consistent word level tags. Considering characters as the primary representation proves fruitful in several ways. Characters provide sub-word-level syntactic, morphological, and orthographic information that can be directly expl"
C16-1087,N16-1030,0,0.0223315,"Missing"
C16-1087,P16-1101,0,0.0673842,"Missing"
C16-1087,W09-1119,0,0.0945619,"tags. Moreover, one can allow the classifications at adjacent positions to interact by chaining local classifiers together and perform joint inference. To achieve good performance, one has to overcome the data sparsity problem in the labeled training data. This is achieved by handcrafting good word-level features by exploiting affix, capitalization, or punctuation (Zhang and Johnson, 2003), using the output of syntactic analyzers (part-of-speech taggers, chunkers) and external resources such as gazetteers, word embeddings, word cluster ids to improve performance further (Turian et al., 2010; Ratinov and Roth, 2009). These solutions require significant engineering effort or language specific resources that are not readily available in all languages of interest. It is even harder to design a multilingual model with handcrafted features considering each language offers different challenges. A distinguishing feature for one language may not be informative for another. For example, capitalization (a typical feature for English NER) is not a distinguishing orthographic feature for the Arabic script. Models for languages with agglutinative or inflectional morphologies may need to utilize the output of a langua"
C16-1087,C12-1150,0,0.0654719,"Missing"
C16-1087,P08-1076,0,0.0253319,"with word-level inputs then move onto character-based NER models. Next, we summarize the applications of character-based models in NLP in general. 2.1 Word based NER Early successful studies on NER use hand-crafted features and language specific name lists with wordlevel classifiers. Both of the first place submissions in CoNLL-2002 (Spanish & Dutch), CoNLL-2003 (English & German) NER shared tasks (Carreras et al., 2002; Florian et al., 2003) use a rich set of handcrafted features along with gazetteers to achieve top performance. Subsequently, semi-supervised approaches (Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Turian et al., 2010) have reported better results by utilizing large unlabeled corpora. Demir and Ozgur (2014) employs a semi-supervised learning approach to achieve best result for Czech. Darwish (2013) exploits cross-lingual features and knowledge bases from English data sources to achieve the top performance on Arabic. The current state-of-the-art system for Turkish (Seker and Eryigit, 2012) is based on Conditional Random Field (CRF) and utilizes language dependent features along with gazetteers. 2.2 Character based NER Klein et al. (2003) introduce a Hidden Markov Model (HMM) with charac"
C16-1087,P10-1040,0,0.0604948,"ssifier that produces tags. Moreover, one can allow the classifications at adjacent positions to interact by chaining local classifiers together and perform joint inference. To achieve good performance, one has to overcome the data sparsity problem in the labeled training data. This is achieved by handcrafting good word-level features by exploiting affix, capitalization, or punctuation (Zhang and Johnson, 2003), using the output of syntactic analyzers (part-of-speech taggers, chunkers) and external resources such as gazetteers, word embeddings, word cluster ids to improve performance further (Turian et al., 2010; Ratinov and Roth, 2009). These solutions require significant engineering effort or language specific resources that are not readily available in all languages of interest. It is even harder to design a multilingual model with handcrafted features considering each language offers different challenges. A distinguishing feature for one language may not be informative for another. For example, capitalization (a typical feature for English NER) is not a distinguishing orthographic feature for the Arabic script. Models for languages with agglutinative or inflectional morphologies may need to utili"
C16-1087,W03-0434,0,0.0663996,"commonly formulated as a word-level tagging problem where each word in the sentence is mapped to a named entity tag. A typical approach is to slide a window over each word position to extract features for a classifier that produces tags. Moreover, one can allow the classifications at adjacent positions to interact by chaining local classifiers together and perform joint inference. To achieve good performance, one has to overcome the data sparsity problem in the labeled training data. This is achieved by handcrafting good word-level features by exploiting affix, capitalization, or punctuation (Zhang and Johnson, 2003), using the output of syntactic analyzers (part-of-speech taggers, chunkers) and external resources such as gazetteers, word embeddings, word cluster ids to improve performance further (Turian et al., 2010; Ratinov and Roth, 2009). These solutions require significant engineering effort or language specific resources that are not readily available in all languages of interest. It is even harder to design a multilingual model with handcrafted features considering each language offers different challenges. A distinguishing feature for one language may not be informative for another. For example,"
D07-1096,D07-1119,0,0.635846,"Missing"
D07-1096,D07-1120,0,0.0179351,"Missing"
D07-1096,W06-1615,1,0.130888,"d annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follows. Participants were prov"
D07-1096,W06-2920,0,0.770442,"d provide a first analysis of these results. 1 Introduction Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999, 2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005). In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006). In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence. This defines a dependency graph, where In this year’s shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation. The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where th"
D07-1096,D07-1121,0,0.0305771,"Missing"
D07-1096,D07-1101,0,0.829681,"Missing"
D07-1096,W01-0521,0,0.119247,"and Turkish. The treebanks from 2 The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations. Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen). 917 which the data sets were extracted are described in section 3. 2.3 Domain Adaptation Track One well known characteristic of data-driven parsing systems is that they typically perform much worse on data that does not come from the training domain (Gildea, 2001). Due to the large overhead in annotating text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantia"
D07-1096,A00-2018,0,0.0790907,"test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One"
D07-1096,W04-3237,0,0.0190919,"dapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target doma"
D07-1096,D07-1097,1,0.748997,"Missing"
D07-1096,D07-1122,0,0.0269928,"Missing"
D07-1096,P97-1003,0,0.129198,"neralize to unseen test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz an"
D07-1096,D07-1112,0,0.583145,"Missing"
D07-1096,D07-1102,0,0.0310581,"Missing"
D07-1096,W07-2416,0,0.767751,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,D07-1123,0,0.0858787,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,W02-2016,0,0.338207,"expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors inv"
D07-1096,W04-3111,1,0.307142,"task. A new test set of about 9,000 tokens was provided by G¨uls¸en Eryi˘git (Eryi˘git, 2007), who also handled the conversion to the CoNLL format, which means that we could use 919 Domain Adaptation Track As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). This data set is identical to the English training set from the multilingual track (see section 3.1). For the target domains we used three different labeled data sets. The first two were annotated as part of the PennBioIE project (Kulick et al., 2004) and consist of sentences drawn from either biomedical or chemical research abstracts. Like the source WSJ corpus, this data is annotated using the Penn Treebank phrase structure scheme. To convert these sets to dependency structures we used the same procedure as before (Johansson and Nugues, 2007a). Additional care was taken to remove sentences that contained non-WSJ part-of-speech tags or non-terminals (e.g., HYPH part-of-speech tag indicating a hyphen). Furthermore, the annotation scheme for gaps and traces was made consistent with the Penn Treebank wherever possible. As already mentioned,"
D07-1096,D07-1098,0,0.0447748,"Missing"
D07-1096,D07-1124,0,0.0197253,"Missing"
D07-1096,J93-2004,0,0.0597121,"dependency annotation, just as for PADT. It was also used in the shared task 2006, but there are two important changes compared to last year. First, version 2.0 of PDT was used instead of version 1.0, and a conversion script was created by Zdenek Zabokrtsky, using the new XMLbased format of PDT 2.0. Secondly, due to the upper bound on training set size, only sections 1–3 of PDT constitute the training data, which amounts to some 450,000 tokens. The test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turk"
D07-1096,N04-1001,0,0.244784,"se trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated res"
D07-1096,D07-1125,0,0.020143,"Missing"
D07-1096,P06-1043,0,0.463102,"scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follo"
D07-1096,N03-1027,0,0.115089,"text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter set"
D07-1096,D07-1013,1,0.545996,"Missing"
D07-1096,N06-2033,0,0.645238,"Missing"
D07-1096,E06-1011,1,0.349109,"the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research. In order to provide an extended empirical foundation"
D07-1096,D07-1111,0,0.734777,"Missing"
D07-1096,H05-1066,1,0.508532,"Missing"
D07-1096,D07-1126,0,0.0372423,"Missing"
D07-1096,W06-2934,1,0.278986,"Missing"
D07-1096,D07-1128,0,0.0204553,"Missing"
D07-1096,D07-1129,0,0.0286186,"Missing"
D07-1096,W06-2902,0,0.016467,"g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for"
D07-1096,D07-1099,0,0.322183,"Missing"
D07-1096,D07-1130,0,0.0262275,"Missing"
D07-1096,D07-1131,0,0.0212092,"Missing"
D07-1096,W03-3023,0,0.882665,"ults for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an i"
D07-1096,P06-1085,0,\N,Missing
D07-1096,D07-1127,0,\N,Missing
D12-1086,P10-1131,0,0.0503614,"Missing"
D12-1086,N10-1083,0,0.0728367,"Missing"
D12-1086,P06-3002,0,0.0344026,"on the frames they occupy thus employing one-tag-perword assumption from the beginning (with the exception of some methods in (Sch¨utze, 1995)). They may suffer from data sparsity caused by infrequent words and infrequent contexts. The solutions suggested either restrict the set of words and set of contexts to be clustered to the most frequently observed, or use dimensionality reduction. Redington et al. (1998) define context similarity based on the number of common frames bypassing the data sparsity problem but achieve mediocre results. Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words. Sch¨utze (1995) and Lamar et al. (2010b) employ SVD to enhance similarity between less frequently observed words and contexts. Lamar et al. (2010a) represent each context by the currently assigned left and right tag (which eliminates data sparsity) and cluster word types using a soft k-means style iterative algorithm. They report the best clustering result to date of .708 many-to-one accuracy 942 on a 45-tag 1M word corpus. HMMs: The prototypical bitag HMM model maximizes the likelihood ofQ the"
D12-1086,P11-1087,0,0.444691,"Missing"
D12-1086,J92-4003,0,0.778864,"text statistics. Word-feature models incorporate additional morphological and orthographic features. 2.1 Distributional models Distributional models can be further categorized into three subgroups based on the learning algorithm. The first subgroup represents each word type with its context vector and clusters these vectors accordingly (Sch¨utze, 1995). Work in modeling child syntactic category acquisition has generally followed this clustering approach (Redington et al., 1998; Mintz, 2003). The second subgroup consists of probabilistic models based on the Hidden Markov Model (HMM) framework (Brown et al., 1992). A third group of algorithms constructs a low dimensional representation of the data that represents the empirical co-occurrence statistics of word types (Globerson et al., 2007), which is covered in more detail in Section 4. Clustering: Clustering based methods represent context using neighboring words, typically a single word on the left and a single word on the right called a “frame” (e.g., the dog is; the cat is). They cluster word types rather than word tokens based on the frames they occupy thus employing one-tag-perword assumption from the beginning (with the exception of some methods"
D12-1086,D10-1056,0,0.834751,"view of related work. Section 3 describes the dataset and the construction of the substitute vectors. Section 4 describes cooccurrence data embedding, the learning algorithm used in our experiments. Section 5 describes our experiments and compares our results with previous work. Section 6 gives a brief error analysis and Section 7 summarizes our contributions. All the data and the code to replicate the results given in this paper is available from the authors’ website at http://goo.gl/RoqEh. 2 Related Work There are several good reviews of algorithms for unsupervised part-of-speech induction (Christodoulopoulos et al., 2010; Gao and Johnson, 2008) and models of syntactic category acquisition (Ambridge and Lieven, 2011). This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). The problem of induction is important for studying under-resourced languages that lack labeled corpora and high quality dictionaries. It is also esse"
D12-1086,D11-1059,0,0.27965,"Missing"
D12-1086,A88-1019,0,0.12289,"results with previous work. Section 6 gives a brief error analysis and Section 7 summarizes our contributions. All the data and the code to replicate the results given in this paper is available from the authors’ website at http://goo.gl/RoqEh. 2 Related Work There are several good reviews of algorithms for unsupervised part-of-speech induction (Christodoulopoulos et al., 2010; Gao and Johnson, 2008) and models of syntactic category acquisition (Ambridge and Lieven, 2011). This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). The problem of induction is important for studying under-resourced languages that lack labeled corpora and high quality dictionaries. It is also essential in modeling child language acquisition because every child manages to induce syntactic categories without access to labeled sentences, labeled prototypes, or dictionary constraints. Models of unsupervised part-of-speech inducti"
D12-1086,E03-1009,0,0.0608122,"stitutes Another way to use substitute vectors in a discrete setting is simply to sample individual substitute words from them. The random-substitutes algorithm cycles through the test data and pairs each word with a random substitute picked from the precomputed substitute vectors (see Section 3). We ran the random-substitutes algorithm to generate 14 million word (X) – random-substitute (Y ) pairs (12 substitutes for each token) as input to S-CODE. Clustering the resulting φx vectors yields a manyto-one score of .7680 (.0038) and a V-measure of 947 5.4 Morphological and orthographic features Clark (2003) demonstrates that using morphological and orthographic features significantly improves part-of-speech induction with an HMM based model. Section 2 describes a number other approaches that show similar improvements. This section describes one way to integrate additional features to the random-substitute model. The orthographic features we used are similar to the ones in (Berg-Kirkpatrick et al., 2010) with small modifications: • Initial-Capital: this feature is generated for capitalized words with the exception of sentence initial words. • Number: this feature is generated when the token start"
D12-1086,D08-1036,0,0.0330119,"escribes the dataset and the construction of the substitute vectors. Section 4 describes cooccurrence data embedding, the learning algorithm used in our experiments. Section 5 describes our experiments and compares our results with previous work. Section 6 gives a brief error analysis and Section 7 summarizes our contributions. All the data and the code to replicate the results given in this paper is available from the authors’ website at http://goo.gl/RoqEh. 2 Related Work There are several good reviews of algorithms for unsupervised part-of-speech induction (Christodoulopoulos et al., 2010; Gao and Johnson, 2008) and models of syntactic category acquisition (Ambridge and Lieven, 2011). This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). The problem of induction is important for studying under-resourced languages that lack labeled corpora and high quality dictionaries. It is also essential in modeling child"
D12-1086,P07-1094,0,0.0606585,"Missing"
D12-1086,N06-1041,0,0.0147243,"authors’ website at http://goo.gl/RoqEh. 2 Related Work There are several good reviews of algorithms for unsupervised part-of-speech induction (Christodoulopoulos et al., 2010; Gao and Johnson, 2008) and models of syntactic category acquisition (Ambridge and Lieven, 2011). This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). The problem of induction is important for studying under-resourced languages that lack labeled corpora and high quality dictionaries. It is also essential in modeling child language acquisition because every child manages to induce syntactic categories without access to labeled sentences, labeled prototypes, or dictionary constraints. Models of unsupervised part-of-speech induction fall into two broad groups based on the information they utilize. Distributional models only use word types and their context statistics. Word-feature models incorporate additional morphological and orthographic f"
D12-1086,D07-1031,0,0.0499883,"Missing"
D12-1086,D10-1078,0,0.106717,"e, 1995)). They may suffer from data sparsity caused by infrequent words and infrequent contexts. The solutions suggested either restrict the set of words and set of contexts to be clustered to the most frequently observed, or use dimensionality reduction. Redington et al. (1998) define context similarity based on the number of common frames bypassing the data sparsity problem but achieve mediocre results. Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words. Sch¨utze (1995) and Lamar et al. (2010b) employ SVD to enhance similarity between less frequently observed words and contexts. Lamar et al. (2010a) represent each context by the currently assigned left and right tag (which eliminates data sparsity) and cluster word types using a soft k-means style iterative algorithm. They report the best clustering result to date of .708 many-to-one accuracy 942 on a 45-tag 1M word corpus. HMMs: The prototypical bitag HMM model maximizes the likelihood ofQ the corpus w1 . . . wn expressed as P (w1 |c1 ) ni=2 P (wi |ci )P (ci |ci−1 ) where wi are the word tokens and ci are their (hidden) tags. One"
D12-1086,P10-2040,0,0.0452648,"e, 1995)). They may suffer from data sparsity caused by infrequent words and infrequent contexts. The solutions suggested either restrict the set of words and set of contexts to be clustered to the most frequently observed, or use dimensionality reduction. Redington et al. (1998) define context similarity based on the number of common frames bypassing the data sparsity problem but achieve mediocre results. Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words. Sch¨utze (1995) and Lamar et al. (2010b) employ SVD to enhance similarity between less frequently observed words and contexts. Lamar et al. (2010a) represent each context by the currently assigned left and right tag (which eliminates data sparsity) and cluster word types using a soft k-means style iterative algorithm. They report the best clustering result to date of .708 many-to-one accuracy 942 on a 45-tag 1M word corpus. HMMs: The prototypical bitag HMM model maximizes the likelihood ofQ the corpus w1 . . . wn expressed as P (w1 |c1 ) ni=2 P (wi |ci )P (ci |ci−1 ) where wi are the word tokens and ci are their (hidden) tags. One"
D12-1086,D10-1083,0,0.0975218,"Missing"
D12-1086,J94-2001,0,0.211605,"ons. All the data and the code to replicate the results given in this paper is available from the authors’ website at http://goo.gl/RoqEh. 2 Related Work There are several good reviews of algorithms for unsupervised part-of-speech induction (Christodoulopoulos et al., 2010; Gao and Johnson, 2008) and models of syntactic category acquisition (Ambridge and Lieven, 2011). This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). The problem of induction is important for studying under-resourced languages that lack labeled corpora and high quality dictionaries. It is also essential in modeling child language acquisition because every child manages to induce syntactic categories without access to labeled sentences, labeled prototypes, or dictionary constraints. Models of unsupervised part-of-speech induction fall into two broad groups based on the information they utilize. Distributional models only use word type"
D12-1086,D07-1043,0,0.025421,"y concatenating vectors that represent the target word with vectors that represent the context we use S-CODE to model their co-occurrence statistics. 943 2.4 Evaluation We report many-to-one and V-measure scores for our experiments as suggested in (Christodoulopoulos et al., 2010). The many-to-one (MTO) evaluation maps each cluster to its most frequent gold tag and reports the percentage of correctly tagged instances. The MTO score naturally gets higher with increasing number of clusters but it is an intuitive metric when comparing results with the same number of clusters. The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster). In Section 6 we argue that homogeneity is perhaps more important in part-of-speech induction and suggest MTO with a fixed number of clusters as a more intuitive metric. 3 Substitute Vectors In this study, we predict the part of speech of a word in a given context based on its substitute vector. The dimensions of the substitute vector represent words in the vocabulary, a"
D12-1086,E95-1020,0,0.513241,"Missing"
D16-1163,W15-3001,0,0.0163026,"ent models is 0.5 with a decay rate of 0.9 when the development perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm of all parameters is greater than 5. The initial parameter range is [-0.08, +0.08]. We also initialize our forget-gate biases to 1 as specified by J´ozefowicz et al. (2015) and Gers et al. (2000). For decoding we use a beam search of width 12. 4.1 Transfer Results The results for our transfer learning method applied to the four languages above are in Table 2. The parent models were trained on the WMT 2015 (Bojar et al., 2015) French–English corpus for 5 epochs. Our baseline NMT systems (‘NMT’ row) all receive a large B LEU improvement when using the transfer method (the ‘Xfer’ row) with an average B LEU improvement of 5.6. Additionally, when we use unknown word replacement from Luong et al. (2015b) and ensemble together 8 models (the ‘Final’ row) we further improve upon our B LEU scores, bringing the average B LEU improvement to 7.5. Overall our method allows the NMT system to reach competitive scores and outperform the SBMT system in one of the four language pairs. Figure 2: Our NMT model architecture, showing si"
D16-1163,1997.mtsummit-plenaries.5,0,0.630738,"Missing"
D16-1163,N09-1025,1,0.225735,"ores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems when our transfer method is not used. The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). In this system there are two count-based 5-gram language models. One is trained on the English side of the WMT 2015 English–French dataset and the other is trained on the English side of the low-resource bitext. Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). For our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learn1571 ing rate, parameter initialization range, dropout rate, and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K, and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5, as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parent models is 0.5 with a decay rate of 0.9"
D16-1163,P15-1166,0,0.261996,"analysis (Wang and Zheng, 2015). Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, Cires¸an et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters. Ours is the first study to apply transfer learning to neural machine translation. There has also been work on using data from multiple language pairs in NMT to improve performance. Recently, Dong et al. (2015) showed that sharing a source encoder for one language helps performance when using different target decoders Decoder NMT Xfer Final SBMT Hausa 16.8 21.3 24.0 23.7 Turkish 11.4 17.0 18.7 20.4 Uzbek 10.7 14.4 16.8 17.9 Urdu 5.2 13.8 14.5 17.9 Re-scorer Table 2: Our method significantly improves NMT results for None NMT Xfer LM Hausa 23.7 24.5 24.8 23.6 SBMT Decoder Turkish Uzbek 20.4 17.9 21.4 19.5 21.8 19.5 21.1 17.9 Urdu 17.9 18.2 19.1 18.2 the translation of low-resource languages into English. Results Table 3: Our transfer method applied to re-scoring output nshow test-set B LEU scores. The"
D16-1163,N16-1101,0,0.284449,"irst row shows the out transfer, and the ‘Xfer’ row shows results with transfer. The SBMT performance with no re-scoring and the other 3 rows ‘Final’ row shows B LEU after we ensemble 8 models and use show the performance after re-scoring with the selected model. unknown word replacement. Note: the ‘LM’ row shows the results when an RNN LM trained on the large English corpus was used to re-score. for different languages. In that paper the authors showed that using this framework improves performance for low-resource languages by incorporating a mix of low-resource and high-resource languages. Firat et al. (2016) used a similar approach, employing a separate encoder for each source language, a separate decoder for each target language, and a shared attention mechanism across all languages. They then trained these components jointly across multiple different language pairs to show improvements in a lower-resource setting. There are a few key differences between our work and theirs. One is that we are working with truly small amounts of training data. Dong et al. (2015) used a training corpus of about 8m English words for the low-resource experiments, and Firat et al. (2016) used from 2m to 4m words, wh"
D16-1163,N04-1035,1,0.184111,"g translation knowledge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs This work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the chi"
D16-1163,P06-1121,1,0.286724,"ge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs This work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NM"
D16-1163,N06-1014,0,0.0428891,"d the EngPerm–English learns to un-permute scrambled English sentences. The LM is a 2-layer LSTM RNN language model. Dictionary Initialization Using the transfer method, we always initialize input language embeddings for the child model with randomly-assigned embeddings from the parent (which has a different input language). A smarter method might be to initialize child embeddings with similar parent embeddings, where similarity is measured by word-to-word t-table probabilities. To get these probabilities we compose Uzbek–English and English–French t-tables obtained from the Berkeley Aligner (Liang et al., 2006). We see from Figure 4 that this dictionary-based assignment results in faster improvement in the early part of the training. However the final performance is similar to our standard model, indicating that the training is able to untangle the dictionary permutation introduced by randomly-assigned embeddings. 5.6 Transfer Model None French–English Parent English–English Parent EngPerm–English Parent LM Xfer Different Parent Models In the above experiments, we use a parent model trained on a large French–English corpus. One might hypothesize that our gains come from exploit1574 ing the English h"
D16-1163,D15-1166,0,0.858229,"is work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 B LEU on average, and we provide an analysis of why the method works. The final NMT sys"
D16-1163,P15-1002,0,0.348052,"is work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 B LEU on average, and we provide an analysis of why the method works. The final NMT sys"
D16-1163,D13-1140,0,0.0533372,"sfer learning helps French0 (13.3→20.0) more than it helps Uzbek (10.7→15.0). 4.2 5 Re-scoring Results We also use the NMT model with transfer learning as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system. Table 3 shows the results of re-scoring. We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Williams et al., 2015). Additionally, it is trained using dropout with a dropout probability of 0.2 as suggested by Zaremba et al. (2014). Re-scoring with the transfer NMT model yields an improvement of 1.1– 1.6 B LEU points above the strong SBMT system; we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models. In the next section, we describe a number of additional experiments designed to help us understand the contribution of the various components of our transfer model. 1572 Analysis We analyze the effects of using di"
D16-1163,2020.wmt-1.63,0,\N,Missing
D16-1163,2020.coling-tutorials.3,0,\N,Missing
D16-1248,J93-2003,0,0.0983254,"ht length. When we evaluate the system on previously unseen test data, using BLEU (Papineni et al., 2002), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0. Thus, no brevity penalty is incurred. This behavior seems to come for free, without special design. By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length. The original mechanism comes from the IBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences (Brown et al., 1993). But they did not deploy this table when decoding a foreign sentence f into an English sentence e; it did not participate in incremental scoring and pruning of candidate translations. As a result (Brown et al., 1995): “However, for a given f, if the goal is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones. As a result, short English strings are improperly favored over longer English strings. This tendency is counteracted in part by the following modification: Replace P(f|e) with clength(e) · P(f|e) for some empiric"
D16-1248,1997.mtsummit-plenaries.5,0,0.364389,"Missing"
D16-1248,D15-1166,0,0.0106307,"evin Knight1 , and Deniz Yuret2 Information Sciences Institute & Computer Science Department University of Southern California {xingshi,knight}@isi.edu 2 Computer Engineering, Koc¸ University dyuret@ku.edu.tr Abstract We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality. 1 Introduction The neural encoder-decoder framework for machine translation (Neco and Forcada, 1997; Casta˜no and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) provides new tools for addressing the field’s difficult challenges. In this framework (Figure 1), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector into a target sentence. In this paper, we train long shortterm memory (LSTM) neural units (Hochreiter and Schmidhuber, 1997) trained with back-propagation through time (Werbos, 1990). A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length. When we evaluate the syste"
D16-1248,P03-1021,0,0.0481073,"is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones. As a result, short English strings are improperly favored over longer English strings. This tendency is counteracted in part by the following modification: Replace P(f|e) with clength(e) · P(f|e) for some empirically chosen constant c. This modification is treatment of the symptom rather than treatment of the disease itself, but it offers some temporary relief. The cure lies in better modeling.” More temporary relief came from Minimum Error-Rate Training (MERT) (Och, 2003), which automatically sets c to maximize BLEU score. MERT also sets weights for the language model P(e), translation model P(f|e), and other features. The length feature combines so sensitively with other features that MERT frequently returns to it as it revises one weight at a time. NMT’s ability to correctly model length is remarkable for these reasons: • SMT relies on maximum BLEU training to obtain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training. • Standard SMT models explicitly “cross off” 2278 Proceedings of the 2016 Co"
D16-1248,P02-1040,0,0.120643,"difficult challenges. In this framework (Figure 1), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector into a target sentence. In this paper, we train long shortterm memory (LSTM) neural units (Hochreiter and Schmidhuber, 1997) trained with back-propagation through time (Werbos, 1990). A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length. When we evaluate the system on previously unseen test data, using BLEU (Papineni et al., 2002), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0. Thus, no brevity penalty is incurred. This behavior seems to come for free, without special design. By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length. The original mechanism comes from the IBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences (Brown et al., 1993). But they did not deploy this table when decoding a foreign sentence f into an Englis"
J10-1004,P01-1005,0,0.0463123,"Missing"
J10-1004,P00-1037,0,0.0372868,"an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the ﬁrst application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For speciﬁc modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes f"
J10-1004,J90-2002,0,0.619839,"ised systems from SensEval-2 (Cotton et al. 2001), SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, M`arquez, and Wicentowski 2007). Section 5 discusses these results and the idiosyncrasies of the data sets, baselines, and evaluation metrics used. Section 6 presents related work, and Section 7 summarizes our contributions. 112 Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD 2. The Noisy Channel Model for WSD 2.1 Model The noisy channel model has been the foundation of standard models in speech recognition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al. 1990). In this article we explore its application to WSD. The noisy channel model can be used whenever a signal received does not uniquely identify the message being sent. Bayes’ Law is used to interpret the ambiguous signal and identify the most probable intended message. In WSD, we model each context as a distinct channel where the intended message is a word sense (or semantic class) S, and the signal received is an ambiguous word W. In this section we will describe how to model a given context C as a noisy channel, and in particular how to estimate the context-speciﬁc sense distribution without"
J10-1004,W06-1670,0,0.0267911,"Missing"
J10-1004,S01-1016,0,0.0568016,"Missing"
J10-1004,P02-1057,0,0.0739787,"Missing"
J10-1004,C94-2113,0,0.168349,"Missing"
J10-1004,P03-1003,0,0.01682,"xt distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the ﬁrst application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For speciﬁc modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Gro"
J10-1004,S07-1100,0,0.290224,"Missing"
J10-1004,P05-1005,0,0.0352713,". Task WN Nouns FSB 1st 2nd 3rd Unsup senseval2 senseval3 semeval07 1.7 1.7.1 2.1 1,067 892 159 71.9 71.0 64.2 78.0 72.0 68.6 74.5 71.2 66.7 70.0 71.0 66.7 61.8 62.6 63.5 2,118 70.9 74.4 72.5 70.2 62.2 total unsupervised system is given in the last column. The reported unsupervised systems do use the sense ordering and frequency information from WordNet. 4.1 First Experiment: The 25 WordNet Categories In previous work, descendants of 25 special WordNet synsets (known as the unique beginners) have been used as the coarse-grained semantic classes for nouns (Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005). These unique beginners were used to organize the nouns into 25 lexicographer ﬁles based on their semantic category during WordNet development. Figure 2 shows the synsets at the top of the noun hierarchy in WordNet. The 25 unique beginners have been shaded, and the two graphics show how the hierarchy evolved between the two WordNet versions used in this study. Figure 2 The top of the WordNet noun hypernym hierarchy for version 1.7 (left) and version 2.1 (right). The 25 WordNet noun categories are shaded. 118 Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD Table 2 The performance"
J10-1004,P06-1014,0,0.0451777,"Missing"
J10-1004,C92-2070,0,0.405387,"Missing"
J10-1004,W04-0864,1,0.812042,"ong confusable words and show that the learning curves do not converge even after ˙ ∗ Koc¸ University, Department of Computer Engineering, 34450 Sarıyer, Istanbul, Turkey. E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr. Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication: 12 September 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 a billion words of training data. They suggest unsupervised, semi-supervised, or active learning to take advantage of large data sets when labeling is expensive. Yuret (2004) observes that in a supervised naive Bayes WSD system trained on SemCor, approximately half of the test instances do not contain any of the contextual features (e.g., neighboring content words or local collocation patterns) observed in the training data. SemCor is the largest publicly available corpus of sense-tagged text, and has only about a quarter million sense-tagged words. In contrast, our unsupervised system uses the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains counts from a 1012 word corpus derived from publicly-available Web pages. A note on the term “"
J10-1004,S07-1044,1,0.904797,"Missing"
J10-1004,P08-2036,1,0.912538,"for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the ﬁrst application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For speciﬁc modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identiﬁed using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun"
J10-1004,W04-3204,0,\N,Missing
K17-3008,P16-1231,0,0.0133604,"Kiperwasser and Goldberg (2016) train a BiLSTM whose input is word and POS embeddings and whose hidden states are fed to an MLP that decides parsing actions. 2.4 Training Parsing is a structured prediction problem and a number of training objectives and optimization methods have been proposed beyond simple likelihood maximization of correct parser actions. Kiperwasser and Goldberg (2016) use dynamic oracle training proposed in (Goldberg and Nivre, 2012). In dynamic oracle training, the parser takes predicted actions rather than gold actions which lets it explore states otherwise not visited. Andor et al. (2016), use beam training based on (Collins and Roark, 2004). The objective in beam training is to maximize the probability of the whole sequence rather than a single action. Andor et al. (2016) use global normalization with beam search (Collins and Roark, 2004) which normalizes the total score of the action sequence instead of turning the score of each action into a probability. This allows the model to represent a richer set of probability distributions. They report that their MLP Feature extraction A neural network parser uses a feature extractor that represents the state of the parser using cont"
K17-3008,P14-2131,0,0.0275276,"he first choice is between binary (one-hot) vectors vs dense continuous vectors. If dense vectors are to be used, they can be initialized randomly or transferred from a model for a related task such as language modeling. Finally, once initialized, these vectors can be fixed or fine-tuned during the training of the dependency parser. Chen and Manning (2014) initialize with pretrained word vectors from (Collobert et al., 2011) in English and (Mikolov et al., 2013) in Chinese, and dense, randomly initialized vectors for POS tags. Similarly, Dyer et al. (2015) get pre-trained word embeddings from Bansal et al. (2014) and use POS tag vectors that are randomly initialized. Both studies fine-tune the vectors during parser training. Kiperwasser and Goldberg (2016) start with random POS embeddings and fine-tuned word embeddings from (Dyer et al., 2015) and further optimize all embeddings during parser training. They also report that initialization with random word vectors give inferior performance. In (Alberti et al., 2017), a character-level LSTM reads each word character by character and the last hidden state creates a word representation. The word representation is used as input to a wordlevel LSTM whose hi"
K17-3008,S13-2050,1,0.909627,"Missing"
K17-3008,D14-1082,0,0.496421,"ward and backward LSTMs of the language model that are generated while predicting a word. These hidden states summarize the information from the left context and the right context of a word that was useful in predicting it. Our main contribution is to demonstrate that using context embeddings as features leads to a significant improvement in parsing performance. Introduction Recent studies in parsing natural language has seen a shift from shallow models that use high dimensional, sparse, hand engineered features, e.g. (Zhang and Nivre, 2011), to deeper models with dense feature vectors, e.g. (Chen and Manning, 2014). Shallow linear models cannot represent feature conjunctions that may be useful for parsing decisions, therefore designers of such models have to add specific combinations to the feature list by hand: for example Zhang and Nivre (2011) define 72 hand designed conjunctive combinations of 39 primitive features. Deep models can represent and automatically learn feature combinations that are useful for a given task, so the designer only has to come up with a list of primitive features. Two questions about feature representation still remain critical: what parts of the parser state to represent, T"
K17-3008,P04-1015,0,0.0615875,"hose input is word and POS embeddings and whose hidden states are fed to an MLP that decides parsing actions. 2.4 Training Parsing is a structured prediction problem and a number of training objectives and optimization methods have been proposed beyond simple likelihood maximization of correct parser actions. Kiperwasser and Goldberg (2016) use dynamic oracle training proposed in (Goldberg and Nivre, 2012). In dynamic oracle training, the parser takes predicted actions rather than gold actions which lets it explore states otherwise not visited. Andor et al. (2016), use beam training based on (Collins and Roark, 2004). The objective in beam training is to maximize the probability of the whole sequence rather than a single action. Andor et al. (2016) use global normalization with beam search (Collins and Roark, 2004) which normalizes the total score of the action sequence instead of turning the score of each action into a probability. This allows the model to represent a richer set of probability distributions. They report that their MLP Feature extraction A neural network parser uses a feature extractor that represents the state of the parser using continuous embeddings of its various elements. Chen and Ma"
K17-3008,D12-1086,1,0.83004,"word embeddings and context embeddings from the language model. Word embeddings represent the general features of a word type averaged over all its occurrences. Taking advantage of word embeddings derived from language models in other applications is common practice, however, using the same embedding for every occurrence of an ambiguous word ignores polysemy and meaning shifts. To mitigate this problem, we also construct and use context embeddings that represent the immediate context of a word instance. Context embeddings were previously shown to improve tasks such as part-ofspeech induction (Yatbaz et al., 2012) and word sense induction (Bas¸kaya et al., 2013). In this study, we derive context embeddings from the hidden states of the forward and backward LSTMs of the language model that are generated while predicting a word. These hidden states summarize the information from the left context and the right context of a word that was useful in predicting it. Our main contribution is to demonstrate that using context embeddings as features leads to a significant improvement in parsing performance. Introduction Recent studies in parsing natural language has seen a shift from shallow models that use high"
K17-3008,P15-1033,0,0.0912164,"can be initialized and optimized in a number of ways. The first choice is between binary (one-hot) vectors vs dense continuous vectors. If dense vectors are to be used, they can be initialized randomly or transferred from a model for a related task such as language modeling. Finally, once initialized, these vectors can be fixed or fine-tuned during the training of the dependency parser. Chen and Manning (2014) initialize with pretrained word vectors from (Collobert et al., 2011) in English and (Mikolov et al., 2013) in Chinese, and dense, randomly initialized vectors for POS tags. Similarly, Dyer et al. (2015) get pre-trained word embeddings from Bansal et al. (2014) and use POS tag vectors that are randomly initialized. Both studies fine-tune the vectors during parser training. Kiperwasser and Goldberg (2016) start with random POS embeddings and fine-tuned word embeddings from (Dyer et al., 2015) and further optimize all embeddings during parser training. They also report that initialization with random word vectors give inferior performance. In (Alberti et al., 2017), a character-level LSTM reads each word character by character and the last hidden state creates a word representation. The word re"
K17-3008,P11-2033,0,0.0422903,"this study, we derive context embeddings from the hidden states of the forward and backward LSTMs of the language model that are generated while predicting a word. These hidden states summarize the information from the left context and the right context of a word that was useful in predicting it. Our main contribution is to demonstrate that using context embeddings as features leads to a significant improvement in parsing performance. Introduction Recent studies in parsing natural language has seen a shift from shallow models that use high dimensional, sparse, hand engineered features, e.g. (Zhang and Nivre, 2011), to deeper models with dense feature vectors, e.g. (Chen and Manning, 2014). Shallow linear models cannot represent feature conjunctions that may be useful for parsing decisions, therefore designers of such models have to add specific combinations to the feature list by hand: for example Zhang and Nivre (2011) define 72 hand designed conjunctive combinations of 39 primitive features. Deep models can represent and automatically learn feature combinations that are useful for a given task, so the designer only has to come up with a list of primitive features. Two questions about feature represen"
K17-3008,C12-1059,0,0.017688,"have the ability to recover earlier hidden states. They construct the parser state using three stack-LSTMs, representing the buffer, the stack, and the action history. Kiperwasser and Goldberg (2016) train a BiLSTM whose input is word and POS embeddings and whose hidden states are fed to an MLP that decides parsing actions. 2.4 Training Parsing is a structured prediction problem and a number of training objectives and optimization methods have been proposed beyond simple likelihood maximization of correct parser actions. Kiperwasser and Goldberg (2016) use dynamic oracle training proposed in (Goldberg and Nivre, 2012). In dynamic oracle training, the parser takes predicted actions rather than gold actions which lets it explore states otherwise not visited. Andor et al. (2016), use beam training based on (Collins and Roark, 2004). The objective in beam training is to maximize the probability of the whole sequence rather than a single action. Andor et al. (2016) use global normalization with beam search (Collins and Roark, 2004) which normalizes the total score of the action sequence instead of turning the score of each action into a probability. This allows the model to represent a richer set of probability"
K17-3008,Q16-1023,0,0.382013,"ed randomly or transferred from a model for a related task such as language modeling. Finally, once initialized, these vectors can be fixed or fine-tuned during the training of the dependency parser. Chen and Manning (2014) initialize with pretrained word vectors from (Collobert et al., 2011) in English and (Mikolov et al., 2013) in Chinese, and dense, randomly initialized vectors for POS tags. Similarly, Dyer et al. (2015) get pre-trained word embeddings from Bansal et al. (2014) and use POS tag vectors that are randomly initialized. Both studies fine-tune the vectors during parser training. Kiperwasser and Goldberg (2016) start with random POS embeddings and fine-tuned word embeddings from (Dyer et al., 2015) and further optimize all embeddings during parser training. They also report that initialization with random word vectors give inferior performance. In (Alberti et al., 2017), a character-level LSTM reads each word character by character and the last hidden state creates a word representation. The word representation is used as input to a wordlevel LSTM whose hidden states constitute the lookahead representation of each word. Finally, the lookahead representation is used by a tagger LSTM trained to predic"
K17-3008,P11-1068,0,0.0566879,"Missing"
K17-3008,L16-1680,0,0.0726826,"Missing"
K18-2012,L16-1680,0,0.119229,"Missing"
K18-2022,P18-1246,0,0.0148702,"at et al., 2017) at the CoNLL 2017 Shared Task (Zeman et al., 2017) is implemented with four ReLU layers, two layers for finding heads and dependents of each word, and two layers for finding the dependency relations for each head-dependent pair. The outputs are then fed into two biaffine layers, one for determining the head of the word, and another for determining the dependency relation of head-dependent pair. We propose a dependency parsing model based on the graph-based parser by Dozat and Manning (2016). We are adding a meta-biaffine decoder layer, similar to the tagging model proposed by Bohnet et al. (2018), for computing the arc labels based on the full tree constructed from the unlabeled arc scores instead of computing them independently. Our parsing model uses pretrained word embeddings from Kırnap et al. (2017). Our parser uses the same language model with Kırnap et al. (2017), in which graph based-parsing algorithms are applied. However, a transition-based parsing model is given in Kırnap et al. (2017). Therefore, some adaptations are made on the features proposed by Kırnap et al. (2017) in order to use them in a graph based parsing model. We did not use contextual features coming from the"
K18-2022,P81-1022,0,0.728761,"Missing"
K18-2022,K17-3002,0,0.213057,"a contextual vector for each word in a sentence. Use of bi-RNNs is the defacto standard in dependency parsing, as it allows representing each word conditioned on the whole sentence. Our main contribution in the encoder part is to the word embeddings feeding the bi-RNN. We use word vectors coming from a language model pretrained on very large language corpora, similar to Kırnap et al. (2017). We extend word embeddings with learnable embeddings for UPOS tags, XPOS tags and FEATs where applicable. Our decoder can be viewed as a more structured version of the state-of-the-art biaffine decoder of Dozat et al. (2017), where we attempt to condition the label-seeking units to a parsetree instead of simple local predictions. We propose a meta-learning module that allows structured and unstructured predictions to be combined as a weighted sum. This additional computational complexity is paid off by our simple word-level model in the encoder part. We call it that we call structured meta-biaffine decoder or shortly SMeta. We implemented our model using Knet deep We present SParse, our Graph-Based Parsing model submitted for the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies"
K18-2022,C96-1058,0,0.138182,"els). Formally (arc−dep) (2) (rel−dep) Decoder hi = MLP(arc−head) (hi ) hi (arc) si = H (arc−head) W (arc) hi (5) + H (arc−head) bT(arc) where H (arc−head) represents matrix of (arc−head) hi vectors, W (arc) and b(arc) are learnable weights. (1) 218 Parsing algorithms Up to this point, our decoder is identical to the one in (Dozat et al., 2017). The difference is in the computation of predicted arcs. We compute two different predictions: 0 l(arc) yi (arc) = arg max sij (6) p = parse(S (arc) ) (7) j 0 s(arc) yi = pi In our parsing model, Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967) and Eisner (1996)’s algorithm are used interchangeably, during both the training of parser models and parsing phase of test datasets. On the languages whose training dataset consists of more than 250,000 words, Chu-Liu-Edmonds algorithm is used for parsing since it has a complexity of O(n2 ), where n is the number of words. This approach allows us to train our models on relatively larger datasets in less amount of time, compared to the Eisner’s algorithm whose time complexity is O(n3 ). On training datasets having at most 250,000 words, Eisner’s algorithm is used during both training and parsing phase. Eisners"
K18-2022,Q16-1023,0,0.177974,"re FEATs for each word. We follow a simple strategy: we represent each FEAT using a randomly initialized vector and add all FEAT embeddings for each word. We simply used zero for word vectors without any morphological features. For practical reasons, we also needed to represent ROOT word of a sentence. We do so by randomly initializing a word embedding and setting all other embeddings to zero. At test time, we used tags and morphological features produced by MorphNet (Dayanık et al., 2018). For languages where this model is not available, we directly used UDPipe results (Straka et al., 2016). Kiperwasser and Goldberg (2016) use trainable BiLSTMs to represent features of each word, instead of defining the features manually. They formulated the structured prediction using hinge loss based on the gold parse tree and parsed scores. Dozat and Manning (2016) propose deep biaffine attention combined with the parsing model of Kiperwasser and Goldberg (2016), which simplifies the architecture by allowing implementation with a single layer instead of two linear layers. Stanford’s Graph-based Neural Dependency Parser (Dozat et al., 2017) at the CoNLL 2017 Shared Task (Zeman et al., 2017) is implemented with four ReLU layer"
K18-2022,L16-1680,0,0.113404,"Missing"
K18-2022,K17-3008,1,0.890196,"Missing"
K18-2022,K18-2001,0,0.0297714,"Missing"
K18-2022,H05-1066,0,0.338997,"Missing"
K18-2022,L16-1262,0,0.11693,"Missing"
K18-2022,W17-0411,0,0.29126,"Missing"
N06-1042,J95-4004,0,0.0693912,"speech tagging using a variety of rule-based and statistical approaches. In the rule-based approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context (Karlsson et al., 1995; Oflazer and T¨ur, 1997). In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text (Church, 1988; Hakkani-T¨ur et al., 2002). Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning (Brill, 1995), memory based learning (Daelemans et al., 1996), and maximum entropy models (Ratnaparkhi, 1996). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging. Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised learning for of Hebrew (Levinger et al., 1995), maximum entropy modeling for Czech (Hajiˇc and Hladk´a, 1998), combination of statistical and rule-based disambiguation met"
N06-1042,A88-1019,0,0.0492313,"the GPA training algorithm. Section 4 presents the experiments and the results. 2 Related Work There is a large body of work on morphological disambiguation and part of speech tagging using a variety of rule-based and statistical approaches. In the rule-based approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context (Karlsson et al., 1995; Oflazer and T¨ur, 1997). In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text (Church, 1988; Hakkani-T¨ur et al., 2002). Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning (Brill, 1995), memory based learning (Daelemans et al., 1996), and maximum entropy models (Ratnaparkhi, 1996). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging. Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised le"
N06-1042,A92-1018,0,0.100406,"en context (Karlsson et al., 1995; Oflazer and T¨ur, 1997). In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text (Church, 1988; Hakkani-T¨ur et al., 2002). Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning (Brill, 1995), memory based learning (Daelemans et al., 1996), and maximum entropy models (Ratnaparkhi, 1996). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging. Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised learning for of Hebrew (Levinger et al., 1995), maximum entropy modeling for Czech (Hajiˇc and Hladk´a, 1998), combination of statistical and rule-based disambiguation methods for Basque (Ezeiza et al., 1998), transformation based tagging for Hungarian (Megyesi, 1999). Early work on Turkish used a constraint-based approach with hand crafted rules (Oflazer and Kuru¨oz, 1994). A purely statistical morph"
N06-1042,W96-0102,0,0.0241548,"Missing"
N06-1042,P98-1062,0,0.0693802,"Missing"
N06-1042,P98-1080,0,0.0221611,"Missing"
N06-1042,J95-3004,0,0.0413621,"2002). Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning (Brill, 1995), memory based learning (Daelemans et al., 1996), and maximum entropy models (Ratnaparkhi, 1996). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging. Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised learning for of Hebrew (Levinger et al., 1995), maximum entropy modeling for Czech (Hajiˇc and Hladk´a, 1998), combination of statistical and rule-based disambiguation methods for Basque (Ezeiza et al., 1998), transformation based tagging for Hungarian (Megyesi, 1999). Early work on Turkish used a constraint-based approach with hand crafted rules (Oflazer and Kuru¨oz, 1994). A purely statistical morphological disambiguation model was recently introduced (HakkaniT¨ur et al., 2002). To counter the data sparseness problem the morphological parses are split across their derivational boundaries and certain independence assumptions are made in"
N06-1042,W99-0633,0,0.0219443,"parkhi, 1996). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging. Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised learning for of Hebrew (Levinger et al., 1995), maximum entropy modeling for Czech (Hajiˇc and Hladk´a, 1998), combination of statistical and rule-based disambiguation methods for Basque (Ezeiza et al., 1998), transformation based tagging for Hungarian (Megyesi, 1999). Early work on Turkish used a constraint-based approach with hand crafted rules (Oflazer and Kuru¨oz, 1994). A purely statistical morphological disambiguation model was recently introduced (HakkaniT¨ur et al., 2002). To counter the data sparseness problem the morphological parses are split across their derivational boundaries and certain independence assumptions are made in the prediction of each inflectional group. A combination of three ideas makes our approach unique in the field: (1) the use of decision lists and a novel learning algorithm that combine the statistical and rule based techn"
N06-1042,A94-1024,0,0.201713,"Missing"
N06-1042,P97-1029,0,0.379311,"Missing"
N06-1042,W96-0213,0,0.192918,"d approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context (Karlsson et al., 1995; Oflazer and T¨ur, 1997). In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text (Church, 1988; Hakkani-T¨ur et al., 2002). Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning (Brill, 1995), memory based learning (Daelemans et al., 1996), and maximum entropy models (Ratnaparkhi, 1996). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensive overview of syntactic word-class tagging. Previous work on morphological disambiguation of inflectional or agglutinative languages include unsupervised learning for of Hebrew (Levinger et al., 1995), maximum entropy modeling for Czech (Hajiˇc and Hladk´a, 1998), combination of statistical and rule-based disambiguation methods for Basque (Ezeiza et al., 1998), transformation based tagging for Hungarian (Megyesi, 1999"
N06-1042,C98-1077,0,\N,Missing
N06-1042,P98-1063,0,\N,Missing
N06-1042,C98-1060,0,\N,Missing
N16-1089,D15-1138,0,0.281597,"vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natural Language Communication Research Problem Definition Pr"
N16-1089,N16-1181,0,0.0468114,"ation Semantics 3 Encoder Figure 2: Our models all follow the above architecture. 1-Hot word vectors (orange) are fed as input to a Feed-Forward or Recurrent Neural Network for encoding. A semantic representation is extracted (green), which in conjunction with knowledge of the world (blue) is grounded to predict an action. can be trained independently (Sections 5.2 and 5.3) or jointly as a single End-to-End model (Section 5.4). This division of labor also allows for differing amounts of human intervention both during training and in the interpretation of actions and bears some resemblance to (Andreas et al., 2016). Specifically, we will first present results where the model predicts a fixed semantic interpretation of actions which are easily human interpretable (Encoder + Representation). In this setting, the experimenter/human then must convert the semantics to actions in the world. Second, we remove the human interpreter and train a model for Grounding and Predicting from our semantic representation. Finally, we maintain our architecture but remove the human entirely, forcing the model to both converge to and interpret its own internal semantic representation. The model architecture, regardless of ho"
N16-1089,P09-1010,0,0.0488923,"ahon et al., 2006; Yu and Siskind, 2013) but the data, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more"
N16-1089,P11-1028,0,0.022072,"ata, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natura"
N16-1089,S14-2006,0,0.0155527,"umes the ability to plan and execute individual and complex actions: slide the adidas block 2 blocks straight up . then slide it 6 block spaces to the left . The models we introduce in this paper have difficulty dealing with these kind of commands. 5 Models In order to correctly interpret commands in context, we need models that ground entities and understand spatial relations, shapes, and the compositionality of language. This is a large and fertile space for exploration. In contrast with previous work which attempts to produce deep semantic interpretations of commands (Kim and Mooney, 2012; Dukes, 2014), in this paper we explore the degree to which we can solve our communication problem using semantic free models. We are quick to note though that our framework can also assess the performance of semantic-heavy approaches. We outline here three basic neural models that provide a set of reasonable baselines for other researchers interested in solving this problem. Each approach assumes less knowledge injection than the previous. As discussed in Section 3.1, in all three models, the eventual output is a tuple specifying where to find the block to move and where to move it: (x, y, z)S and (x, y,"
N16-1089,D12-1040,0,0.012365,"d in a manner that assumes the ability to plan and execute individual and complex actions: slide the adidas block 2 blocks straight up . then slide it 6 block spaces to the left . The models we introduce in this paper have difficulty dealing with these kind of commands. 5 Models In order to correctly interpret commands in context, we need models that ground entities and understand spatial relations, shapes, and the compositionality of language. This is a large and fertile space for exploration. In contrast with previous work which attempts to produce deep semantic interpretations of commands (Kim and Mooney, 2012; Dukes, 2014), in this paper we explore the degree to which we can solve our communication problem using semantic free models. We are quick to note though that our framework can also assess the performance of semantic-heavy approaches. We outline here three basic neural models that provide a set of reasonable baselines for other researchers interested in solving this problem. Each approach assumes less knowledge injection than the previous. As discussed in Section 3.1, in all three models, the eventual output is a tuple specifying where to find the block to move and where to move it: (x, y, z"
N16-1089,N15-1015,0,0.0193808,"ar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natural Language Communication Research Problem Definition Problem-Solution Sequences. In orde"
N16-1089,P14-5010,0,0.00373747,"of each block in Imagei (a discrete representation of the world corresponding to the image), and a natural language command that a robot needs to understand in order to operate on the world in Imagei . The output consists of an Imagei+1 (what the robot should build) and the (x, y, z) coordinates of each block in Imagei+1 . The training/development/test sections of the data contain ∼177K/31K/48K tokens for the decorated blocks. The overall lexical type and token counts for our data are presented in Table 2. To compute statistics all text was lower-cased and tokenized using Stanford’s CoreNLP (Manning et al., 2014). For the MNIST configurations, digits 0-9 are present in the test data as drawn with both logos and numbered blocks. In contrast, only half the digits appear with MNIST Types Tokens Train Dev Test Total 1,506 583 645 177K 31K 48K 1,359 / 257K Blank Types Tokens 961 444 575 Command Length (l) 58K 8K 17K 1≤l≤5 5 < l ≤ 10 10 < l ≤ 20 20 < l ≤ 40 40 < l ≤ 80 1,172 / 84K Table 2: Type and token counts for the Logo and Number decorated block data sets (left) and the Blank blocks (right). # of Commands MNIST Random 81 3,817 5,752 2,028 192 0 61 995 1,329 107 Table 3: A breakdown of the number of com"
N16-1089,H93-1005,0,0.14877,"e progress in Natural Language Processing can be attributed to defining problems of broad interest (e.g. parsing and machine translation); collecting or creating publicly available corpora that encode meaningful hinput, outputi samples (e.g. Penn TreeBank and LDC Parallel Corpora); and devising simple, objective and computable evaluation metrics to automatically assess the performance of algorithms designed to solve the problems of interest, independent of the approach or technology used (e.g. ParseEval and Bleu). Previous work Most research on Human-Robot Interaction (Klingspor et al., 1997; Thompson et al., 1993; Mavridis, 2015) bridges the gap between natural language commands and the physical world via a set of pre-defined templates characterized by a small vocabulary and grammar. Progress on language in this area has largely focused on grounding visual attributes (Kollar et al., 2013; Matuszek et al., 2014) and on learning spatial relations and actions for small vocabularies with hard-coded abstract concepts (Steels and Vogt, 1997; Roy, 2002; 751 Proceedings of NAACL-HLT 2016, pages 751–761, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Guadarrama et al."
N16-1089,N15-1173,0,0.0156622,"hough robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natural Language Communication Research Problem Definition Problem-Solution Sequences. In order to build models that unde"
N16-1089,H89-1033,0,0.82265,"dyuret@ku.edu.tr Abstract As robots become increasingly ubiquituous, we need to learn to interact with them intelligently, in the same manner we interact with members of our own species. To make rapid progress in this area, we propose to use an intellectual framework that has the same ingredients that have transformed our field: appealing science problem definitions; publicly available datasets; and easily computable, objective evaluation metrics. In this paper, we study the problem of HumanRobot Natural Language Communication in a setting inspired by a traditional AI problem – blocks world (Winograd, 1972). After reviewing previous work (Section 2), we propose a novel Human-Robot Communication Problem that is testable empirically (Section 3.1) and we describe the publicly available datasets (Section 3.2) and evaluation metric that we devised to support our research (Section 6). We then introduce a set of algorithms for solving our problem and we evaluate their performance both objectively and subjectively (Sections 4–8). We propose a framework for devising empirically testable algorithms for bridging the communication gap between humans and robots. We instantiate our framework in the context of"
N16-1089,P13-1006,0,0.00952796,"world via a set of pre-defined templates characterized by a small vocabulary and grammar. Progress on language in this area has largely focused on grounding visual attributes (Kollar et al., 2013; Matuszek et al., 2014) and on learning spatial relations and actions for small vocabularies with hard-coded abstract concepts (Steels and Vogt, 1997; Roy, 2002; 751 Proceedings of NAACL-HLT 2016, pages 751–761, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Guadarrama et al., 2013). Language is sometimes grounded into simple actions (MacMahon et al., 2006; Yu and Siskind, 2013) but the data, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Ro"
N16-1089,Q13-1005,0,\N,Missing
P08-2036,J90-2002,0,0.760511,"Missing"
P08-2036,P96-1041,0,0.0298909,"ding bc less than 40 times. We can estimate their number to be a fraction of this difference. δ is an estimate of the type token ratio of these low count words. Its valid range is between 1/40 and 1, and it can be optimized along with the other parameters. The reader can confirm that P 0 0 0 c N (∗bc) = N (∗b∗) and |c : N (∗bc) > 0 |= N (b∗). The expression for the Kneser-Ney back-off estimate becomes (7) 143 C(abc) C(ab∗) + A A C(ab∗) + A (8) The parameter A can be interpreted as the extra counts added to the given distribution and these extra counts are distributed as the lower order model. Chen and Goodman (1996) suggest that these extra counts should be proportional to the number of words with exactly one count in the given context based on the Good-Turing estimate. The Web 1T dataset does not include one-count n-grams. A reasonable alternative is to take A to be proportional to the missing count due to low-count n-grams: C(ab) − C(ab∗). A(ab) = max(1, K(C(ab) − C(ab∗))) A different K constant is chosen for each n-gram order. Using this formulation as an interpolated 5gram language model gives a cross entropy of 8.05 bits on Brown. 4.5 N 0 (∗b∗) = N (∗b∗) + δ(C(b∗) − C(∗b∗)) max(0, N 0 (∗bc) − D) N 0"
P08-2036,P06-1124,0,0.0304378,"Missing"
P08-2036,S07-1044,1,0.890768,"Missing"
P09-2087,S07-1100,0,0.0140976,"ing n − 1 positions. This allows the ability to model long-distance relationships between tokens without a predefined left-to-right ordering and opens the possibility of using different dependency patterns for different token types. Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Hawker, 2007; Yuret, 2007). Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level. The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n − 1. Morphemes play an important role in the syntactic dependency structure in morphologically rich languages. The dependencies are not only between stems but also between stems and suffixes and if we use complete words as unit tokens, we"
P09-2087,J90-2002,0,0.155028,"Missing"
P09-2087,J92-1002,0,0.271427,"Missing"
P09-2087,S07-1044,1,0.795254,"tions. This allows the ability to model long-distance relationships between tokens without a predefined left-to-right ordering and opens the possibility of using different dependency patterns for different token types. Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Hawker, 2007; Yuret, 2007). Morphologically rich languages pose a challenge to standard modeling techniques because of their relatively large out-of-vocabulary rates and the regularities they possess at the sub-word level. The standard n-gram language model ignores long-distance relationships between words and uses the independence assumption of a Markov chain of order n − 1. Morphemes play an important role in the syntactic dependency structure in morphologically rich languages. The dependencies are not only between stems but also between stems and suffixes and if we use complete words as unit tokens, we will not be a"
P09-2087,N06-1042,1,0.866819,"Missing"
Q19-1036,chrupala-etal-2008-learning,0,0.0804438,"Missing"
Q19-1036,D17-1078,0,0.0354542,"Missing"
Q19-1036,J08-3003,0,0.494634,"Missing"
Q19-1036,E06-1012,0,0.0727964,"Missing"
Q19-1036,W07-1709,0,0.031545,"Missing"
Q19-1036,L16-1262,0,0.0435647,"Missing"
Q19-1036,E17-1048,0,0.0437579,"Missing"
Q19-1036,A94-1024,0,0.650179,"Missing"
Q19-1036,W96-0207,0,0.60558,"Missing"
Q19-1036,P18-1247,0,0.141895,"at a time. This is similar to the way rule-based systems such as finite state transducers output morphological analyses. One advantage of generating features one at a time (e.g., +Acc) rather than as a combined tag (e.g., +Noun+A3sg+Pnon+Acc) is sample efficiency. Table 2 shows the percentage of tags in the test data that have been observed rarely in the training data for several languages. In low resource experiments, we show that our sequence decoder significantly outperforms a variant that is trained to output full tags similar to Heigold et al. (2017), especially with unseen or rare tags. Malaviya et al. (2018) also avoid the data sparsity problem associated with whole tags using a neural factor graph model to predict a set of features, improving the transfer learning performance. In contrast with Malaviya et al. (2018), Morse generates a variable number of features as a sequence rather than a fixed set. This allows it to adequately represent derivations in morphologically complex words. For example, in the last analysis in Table 1, morphological features of the word ‘‘masalı’’ consist of two inflectional groups (IGs), a noun group and an adjective group, separated by a derivational boundary denoted"
Q19-1036,D13-1032,0,0.0705182,"Missing"
Q19-1036,C16-1018,0,0.0789432,"burg, 1983). The first rule-based analyzer for Turkish was developed in Oflazer (1994), we used an updated version of this analyzer (Oflazer, 2018) when creating our new Turkish data set. Morphological disambiguation systems take the possible parses for a given word from an analyzer and predict the correct one in a given context using rule-based (Karlsson et al., 1995; Oflazer and Kuru¨oz, 1994; Oflazer and T¨ur, 1996; Daybelge and C ¸ ic¸ekli, 2007; Daoud, 2009), statistical (Hakkani-T¨ur et al., 2002; Yuret and T¨ure, 2006; Hajiˇc et al., 2007), or neural network based (Yıldız et al., 2016; Shen et al., 2016; Toleu et al., 2017) techniques. Hakkani-Tür et al. (2018) provide a comprehensive summary for Turkish disambiguators. 569 a time. A sample output for a word looks like [si1 , . . . , siRi , fi1 , . . . , fiMi ] where sij ∈ A is an alphanumeric character in the lemma, Ri is the length of the lemma, Mi is the number of features, and fij ∈ T is a morphological feature from a feature set such as T = {Noun,Adj,Nom,A3sg, . . .}. Morse performs morphological analysis and disambiguation with a joint model partly to avoid using a separate morphological analyzer or dictionary. Having a single system c"
Q19-1036,D15-1272,0,0.0395527,"Missing"
Q19-1036,N06-1042,1,0.912155,"Missing"
Q19-1036,P17-2105,0,\N,Missing
S07-1003,W04-3205,0,0.0491922,"ierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We hav"
S07-1003,C92-2082,0,0.0345238,"a variety of methods (since we work with relations between nominals, the part of speech is always noun). We have used WordNet 3.0 on the Web and sense index tags. We chose the following semantic relations: Cause-Effect, Content-Container, InstrumentAgency, Origin-Entity, Part-Whole, ProductProducer and Theme-Tool. We wrote seven detailed definitions, including restrictions and conventions, plus prototypical positive and near-miss negative examples. For each relation separately, we based data collection on wild-card search patterns that Google allows. We built the patterns manually, following Hearst (1992) and Nakov and Hearst (2006). Instances of the relation Content-Container, for example, come up in response to queries such as “* contains *”, “* holds *”, “the * in the *”. Following the model of the Senseval-3 English Lexical Sample Task, we set out to collect 140 training and at least 70 test examples per relation, so we had a number of different patterns to ensure variety. We also aimed to collect a balanced number of positive and negative examples. The use of heuristic patterns to search for both positive and negative examples 14 should naturally result in negative examples that are near"
S07-1003,J02-3004,0,0.0275234,"oun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms. We do not presume to propose a single classification scheme, however allurin"
S07-1003,W04-2609,1,0.951787,"cine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work f"
S07-1003,W01-0511,0,0.263052,"cer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2"
S07-1003,P02-1032,0,0.0943661,"traction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and"
S07-1003,H05-1047,0,0.0359726,"their results. There were 14 teams who submitted 15 systems. 1 Task Description and Related Work The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities – honey bee, for example, shows an instance of the ProductProducer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, w"
S07-1003,H91-1061,0,\N,Missing
S07-1044,P96-1041,0,0.0131414,"d value. The correspondence between the language model and our intuition requires further study. Appendix: Web1T Language Model The Web 1T 5-gram dataset (Brants and Franz, 2006) that was used to build a language model for this work consists of the counts of word sequences up to length 5 in a 1012 word corpus derived from the Web. The data consists of mostly English words that have been tokenized and sentence tagged. Tokens that appear less than 200 times and ngrams that appear less than 40 times have been filtered out. I used a smoothing method loosely based on the one-count method given in (Chen and Goodman, 1996). Because ngrams with low counts are not included in the data I used ngrams with missing counts instead of ngrams with one counts. The missing count is defined as: i−1 m(wi−n+1 ) = i−1 c(wi−n+1 ) − X i c(wi−n+1 ) wi i where wi−n+1 indicates the n-word sequence endi ing with wi , and c(wi−n+1 ) is the count of this sequence. The corresponding smoothing formula is: Claudia Leacock, Martin Chodorow, and George A. Miller. 1998. Using corpus statistics and wordnet relations for sense identification. Computational Linguistics, 24(1):147–166, March. Ken Litkowski and Orin Hargraves. 2007. SemEval2007"
S07-1044,W04-0831,0,0.0553374,"Missing"
S07-1044,N06-2015,0,0.0360981,"Missing"
S07-1044,W04-0807,0,0.0634596,"Missing"
S07-1044,mihalcea-2002-bootstrapping,0,0.0604593,"c Prague, June 2007. 2007 Association for Computational Linguistics “There was a plank meeting” just sounds bad to your linguistic “ear”. In this paper I will describe a system that judges potential substitutions in a given context using a statistical language model as a surrogate for the linguistic “ear”. The likelihoods of the various substitutes are used to select the best sense for a target word. The use of substitutes for WSD is not new. (Leacock et al., 1998) demonstrated the use of related monosemous words (monosemous relatives) to collect examples for a given sense from the Internet. (Mihalcea, 2002) used the monosemous relatives technique for bootstrapping the automatic acquisition of large sense tagged corpora. In both cases, the focus was on collecting more labeled examples to be subsequently used with supervised machine learning techniques. (Martinez et al., 2006) extended the method to make use of polysemous relatives. More importantly, their method places these relatives in the context of the target word to query a search engine and uses the search results to predict the best sense in an unsupervised manner. There are three areas that distinguish my system from the previous work: (i"
S07-1044,U06-1008,0,\N,Missing
S07-1044,J98-1006,0,\N,Missing
S07-1044,S07-1005,0,\N,Missing
S07-1044,S07-1016,0,\N,Missing
S07-1044,S07-1009,0,\N,Missing
S07-1044,W04-0864,1,\N,Missing
S10-1009,W07-2416,0,0.00633628,"ight facts from a given set of 25 English sentences. Each of the following examples consists of a sentence (T), and a short statement (H) derived from this sentence by a computer. Please http://www.cs.brown.edu/˜ec/papers/badPars.txt.gz 53 Stanford Parser (Klein and Manning, 2003). Each parser was trained on sections 02-21 of the WSJ section of Penn Treebank. Outputs of phrase structure parsers were automatically annotated with function tags using Blaheta’s function tagger (Blaheta and Charniak, 2000) and converted to the dependency structure with LTH Constituentto-Dependency Conversion Tool (Johansson and Nugues, 2007). To decide the entailments both the test and hypothesis sentences were parsed. All the content words in the hypothesis sentence were determined by using part-of-speech tags and dependency relations. After applying some heuristics such as active-passive conversion, the extracted dependency path between the content words was searched in the dependency graph of the test sentence. In this search process, same relation types for the direct relations between the content word pairs and isomorphic subgraphs in the test and hypothesis sentences were required for the ”YES” answer. Table 2 lists the bas"
S10-1009,W03-2401,0,0.0260141,"on errors compound the already mentioned problems of treebank based evaluation. In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation. Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing. PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation. These include the grammatical relations (GR) by (Carroll et al., 1999), the PARC representation (King et al., 2003), and Stanford typed dependencies (SD) (De Marneffe et al., 2006) (See (Bos and others, 2008) for other proposals). Each use a set of binary relations between words in a sentence as the primary unit of representation. They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications. Here is an example sentence and its SD representation (De Marneffe and Manning, 2008): Bell, based in Los Angeles, makes and distributes electronic, computer and building products. However there are also significant challenges associa"
S10-1009,P03-1054,0,0.00333375,"ents 1 3. The entailments for which there was unanimous agreement of at least 3 annotators were kept. The instructions for the annotators were brief and targeted people with no linguistic background: Computers try to understand long sentences by dividing them into a set of short facts. You will help judge whether the computer extracted the right facts from a given set of 25 English sentences. Each of the following examples consists of a sentence (T), and a short statement (H) derived from this sentence by a computer. Please http://www.cs.brown.edu/˜ec/papers/badPars.txt.gz 53 Stanford Parser (Klein and Manning, 2003). Each parser was trained on sections 02-21 of the WSJ section of Penn Treebank. Outputs of phrase structure parsers were automatically annotated with function tags using Blaheta’s function tagger (Blaheta and Charniak, 2000) and converted to the dependency structure with LTH Constituentto-Dependency Conversion Tool (Johansson and Nugues, 2007). To decide the entailments both the test and hypothesis sentences were parsed. All the content words in the hypothesis sentence were determined by using part-of-speech tags and dependency relations. After applying some heuristics such as active-passive"
S10-1009,A00-2031,0,0.0241615,"stand long sentences by dividing them into a set of short facts. You will help judge whether the computer extracted the right facts from a given set of 25 English sentences. Each of the following examples consists of a sentence (T), and a short statement (H) derived from this sentence by a computer. Please http://www.cs.brown.edu/˜ec/papers/badPars.txt.gz 53 Stanford Parser (Klein and Manning, 2003). Each parser was trained on sections 02-21 of the WSJ section of Penn Treebank. Outputs of phrase structure parsers were automatically annotated with function tags using Blaheta’s function tagger (Blaheta and Charniak, 2000) and converted to the dependency structure with LTH Constituentto-Dependency Conversion Tool (Johansson and Nugues, 2007). To decide the entailments both the test and hypothesis sentences were parsed. All the content words in the hypothesis sentence were determined by using part-of-speech tags and dependency relations. After applying some heuristics such as active-passive conversion, the extracted dependency path between the content words was searched in the dependency graph of the test sentence. In this search process, same relation types for the direct relations between the content word pair"
S10-1009,P97-1021,0,0.145573,"Missing"
S10-1009,H05-1066,0,0.0220604,"Missing"
S10-1009,P05-1022,0,0.00950069,"Missing"
S10-1009,N07-1051,0,0.007245,"Missing"
S10-1009,J07-4004,0,0.0152833,"Missing"
S10-1009,D09-1085,0,0.029839,"to avoid using a spurious subject. (e.g. To test the verb-object dependency in “John kissed Mary.” we construct the entailment “Mary was kissed.”) Identifying Challenging Dependencies To identify syntactic dependencies that are challenging for current state of the art parsers, we used example sentences from the following sources: • Make a copular sentence or use existential “there” to express noun modification. (e.g. To test the noun-modifier dependency in “The big red boat sank.” we construct the entailment “The boat was big.” or “There was a big boat.”) • The “Unbounded Dependency Corpus” (Rimell et al., 2009). An unbounded dependency construction contains a word or phrase which appears to have been moved, while being interpreted in the position of the resulting “gap”. An unlimited number of clause boundaries may intervene between the moved element and the gap (hence “unbounded”). 2.3 Filtering Entailments To identify the entailments that are clear to human judgement we used the following procedure: • A list of sentences from the Penn Treebank on which the Charniak parser (Charniak and Johnson, 2005) performs poorly1 . 1. Each entailment was tagged by 5 untrained annotators from the Amazon Mechanic"
S10-1009,J03-4003,0,0.0159289,"Missing"
S10-1009,de-marneffe-etal-2006-generating,0,\N,Missing
S10-1009,J93-2004,0,\N,Missing
S10-1009,J07-3004,0,\N,Missing
S10-1009,D07-1096,1,\N,Missing
S13-2050,E09-1013,0,0.0549099,"s of a target word (V´eronis, 2004). Agirre et al. (2006) explored the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Recently, Korkontzelos and Manandhar (2010) proposed a graph-based model which achieved good results on word sense induction and discrimination task in SemEval-2010. 300 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 300–306, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Brody and Lapata (2009) proposed a Bayesian approach modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. Vector-space models, on the other hand, typically create context vector by using first or second order co-occurrences. Once context vector has been constructed, different clustering algorithms may be applied. However, representing the context with first or second order co-occurrences can be difficult since there are plenty of parameters to be considered such as the order of occurrence, context window size,"
S13-2050,S07-1100,0,0.0292587,"ccurrences. Once context vector has been constructed, different clustering algorithms may be applied. However, representing the context with first or second order co-occurrences can be difficult since there are plenty of parameters to be considered such as the order of occurrence, context window size, statistical significance of words in the context window and so on. Instead of dealing with these, we suggest representing the context with the most likely substitutes determined by a statistical language model. Statistical language models based on large corpora has been examined in (Yuret, 2007; Hawker, 2007; Yuret and Yatbaz, 2010) for unsupervised word sense disambiguation and lexical substitution. Moreover, the best results in unsupervised part-of-speech induction achieved by using substitute vectors (Yatbaz et al., 2012). In this paper, we propose a system that represents the context of each target word by using high probability substitutes according to a statistical language model. These substitute words and their probabilities are used to create word pairs (instance id - substitute word) to feed our co-occurrence model. The output of the co-occurrence model is clustered by kmeans algorithm."
S13-2050,S10-1079,0,0.0393468,"n be categorized into graph based models, bayesian, and vectorspace ones. In graph-based approaches, every context word is represented as a vertex and if two context words co-occur in one or more instances of a target word, then two vertices are connected with an edge. When the graph is obtained, one of the graph clustering algorithm is employed. As a result, different partitions indicate the different senses of a target word (V´eronis, 2004). Agirre et al. (2006) explored the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Recently, Korkontzelos and Manandhar (2010) proposed a graph-based model which achieved good results on word sense induction and discrimination task in SemEval-2010. 300 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 300–306, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Brody and Lapata (2009) proposed a Bayesian approach modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. Vector-sp"
S13-2050,S12-1027,0,0.0197205,"tly, participants can use ukWaC1 , a 2billion word web-gathered corpus, for sense induction. Furthermore, unlike in previous WSI tasks, organizers allow participants to use additional contexts not found in the ukWaC under the condition that they submit systems for both using only the ukWaC and with their augmented corpora. The gold-standard of test data was prepared using WordNet 3.1 by 10 annotators. Since WSI systems report their annotations in a different sense inventory than WordNet 3.1, a mapping procedure should be used first. The organizers use the sense mapping procedure explained in (Jurgens, 2012). This procedure has adopted the supervised evaluation setting of past SemEval WSI Tasks, but the main difference is that the former takes into account applicability weights for each sense which is a necessary for graded word sense. Evaluation can be divided into two categories: (1) a traditional WSD task for Unsupervised WSD and WSI systems, (2) a clustering comparison setting that evaluates the similarity of the sense inventories for WSI systems. WSD evaluation is made according to three objectives: • Their ability to detect which senses are applicable (Jaccard Index is used) • Their ability"
S13-2050,S10-1081,0,0.0170323,"unctions as a verb. These additional contexts are labeled with unique numbers so that we can distinguish actual instances in the test data. We follow this procedure for 2 www.cs.york.ac.uk/semeval-2013/task13/ 302 Probability 0.305 0.236 0.096 0.026 0.022 0.014 0.012 ... ... Table 1: The most likely substitutes for meet every target word in the test data. In total, 1 million additional instances were fetched from ukWac. Hereafter we refer to this new dataset with as an expanded dataset. 3.2 Substitute Vectors Unlike other WSI methods which rely on the first or the second order co-occurrences (Pedersen, 2010), we represent the context of each target word instance by finding the most likely substitutes suggested by the 4-gram language model we built from ukWaC corpus. The high probability substitutes reflect both semantic and syntactic properties of the context as seen in Table 1 for the following example: And we need Your help to meet the challenge! For every instance in our expanded dataset, we use three tokens each on the left and the right side of a target word as a context when estimating the probabilities for potential lexical substitutes. This tight window size might seem limited, however, t"
S13-2050,D12-1086,1,0.321223,"Missing"
S13-2050,S07-1044,1,0.81622,"nd order co-occurrences. Once context vector has been constructed, different clustering algorithms may be applied. However, representing the context with first or second order co-occurrences can be difficult since there are plenty of parameters to be considered such as the order of occurrence, context window size, statistical significance of words in the context window and so on. Instead of dealing with these, we suggest representing the context with the most likely substitutes determined by a statistical language model. Statistical language models based on large corpora has been examined in (Yuret, 2007; Hawker, 2007; Yuret and Yatbaz, 2010) for unsupervised word sense disambiguation and lexical substitution. Moreover, the best results in unsupervised part-of-speech induction achieved by using substitute vectors (Yatbaz et al., 2012). In this paper, we propose a system that represents the context of each target word by using high probability substitutes according to a statistical language model. These substitute words and their probabilities are used to create word pairs (instance id - substitute word) to feed our co-occurrence model. The output of the co-occurrence model is clustered by kme"
S13-2050,J10-1004,1,0.810338,"ce context vector has been constructed, different clustering algorithms may be applied. However, representing the context with first or second order co-occurrences can be difficult since there are plenty of parameters to be considered such as the order of occurrence, context window size, statistical significance of words in the context window and so on. Instead of dealing with these, we suggest representing the context with the most likely substitutes determined by a statistical language model. Statistical language models based on large corpora has been examined in (Yuret, 2007; Hawker, 2007; Yuret and Yatbaz, 2010) for unsupervised word sense disambiguation and lexical substitution. Moreover, the best results in unsupervised part-of-speech induction achieved by using substitute vectors (Yatbaz et al., 2012). In this paper, we propose a system that represents the context of each target word by using high probability substitutes according to a statistical language model. These substitute words and their probabilities are used to create word pairs (instance id - substitute word) to feed our co-occurrence model. The output of the co-occurrence model is clustered by kmeans algorithm. Our systems perform well"
S13-2050,W06-1669,0,\N,Missing
S13-2050,P09-1002,0,\N,Missing
W06-2938,dzeroski-etal-2006-towards,0,0.0729245,"Missing"
W06-2938,afonso-etal-2002-floresta,0,0.0255507,"s technique successively and used for parsing. All given attributes are considered as candidates in an attribute selection process before building each model. In addition, attributes indicating suffixes of various lengths and character type information were constructed and used. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with To parse a given sentence, the models are applied sequentially, each one considering candidate word pairs and adding new links without deleting the existing links or creating conflicts (cycles or crossings) with them. Thus, the algorithm can be considered a bottom-up, multi-pass, deterministic parse"
W06-2938,W03-2405,0,0.105658,"efore building each model. In addition, attributes indicating suffixes of various lengths and character type information were constructed and used. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with To parse a given sentence, the models are applied sequentially, each one considering candidate word pairs and adding new links without deleting the existing links or creating conflicts (cycles or crossings) with them. Thus, the algorithm can be considered a bottom-up, multi-pass, deterministic parser. Given a candidate word pair, the models may output “no link”, or give a link with a specified direction and type. Thus labeling is"
W06-2938,N06-1042,1,0.806581,"rm cycles or crossings are never considered, so the parser will only generate projective structures. Section 2 gives the details of the learning algorithm. Section 3 describes the first pass model of links between adjacent words. Section 4 details the approach for identifying long distance links and presents the parsing results. 246 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 246–250, New York City, June 2006. 2006 Association for Computational Linguistics 2 The Learning Algorithm 3 Dependency of Adjacent Words The Greedy Prepend Algorithm (Yuret and Ture, 2006) was used to build decision lists to identify dependency relations. A decision list is an ordered list of rules where each rule consists of a pattern and a classification (Rivest, 1987). The first rule whose pattern matches a given instance is used for its classification. In our application the pattern specifies the attributes of the two words to be linked such as parts of speech and morphological features. The classification indicates the existence and the type of the dependency link between the two words. Table 1 gives a subset of the decision list that identifies links between adjacent word"
W06-2938,W06-2920,0,0.0316915,"ating suffixes of various lengths and character type information were constructed and used. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with To parse a given sentence, the models are applied sequentially, each one considering candidate word pairs and adding new links without deleting the existing links or creating conflicts (cycles or crossings) with them. Thus, the algorithm can be considered a bottom-up, multi-pass, deterministic parser. Given a candidate word pair, the models may output “no link”, or give a link with a specified direction and type. Thus labeling is an integrated step. Word pair candidates that may form cy"
W08-2131,J02-3001,0,0.111687,"Missing"
W08-2131,P05-1012,0,0.134805,"Missing"
W08-2131,W08-2121,0,0.0836372,"Missing"
W10-1741,P09-1087,0,0.0143662,"06 6.3611 .2770 7.3190 .2132 6.4093 .2144 6.4168 .2109 6.4176 .2121 6.4271 en-cz BLEU NIST .1145 4.5008 .1628 5.4501 .1148 4.5187 .1150 4.5172 .1124 4.5143 .1150 4.5264 Table 2: Reranking results using TRegMT, TM, and LM scores. We use approximate randomization test (Riezler and Maxwell, 2005) with 1000 repetitions to determine score difference significance: results in bold are significant with p ≤ 0.01 and italic results with (*) are significant with p ≤ .05. The difference of the remaining from the baseline are not statistically significant. 4.4 Reranking Experiments to be a difficult task (Galley and Manning, 2009). Secondly, increasing the performance with reranking itself is a hard task since possible translations are already constrained by the ones observed in N best lists. Therefore, an increase in the N -best list size may increase the score gaps. Table 2 presents reranking results on all of the language pairs we considered, using TRegMT, TM, and LM scores with the combination weights learned in the development set. We are able to achieve better BLEU and NIST scores on all of the listed systems. We are able to see up to .38 BLEU points increase for the en-es pair. Oracle reranking performances are"
W10-1741,P07-2045,0,0.0129241,"cy than L2 regularized ridge regression on some machine learning benchmark datasets (Chapelle et al., 1999). In an idealized feature mapping matrix where Related Work: Regression techniques can be used to model the relationship between strings (Cortes et al., 2007). Wang et al. (2007) applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset. Later they use L2 regularized least squares regression (Wang and Shawe-Taylor, 2008). Although the translation quality they achieve is not better than Moses (Koehn et al., 2007), which is accepted to be the state-of-the-art, they show the feasibility of the approach. Serrano et al. (2009) use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests. Ueffing (2007) approaches the transductive learning problem for SMT by bootstrapping the training using the translations produced by the SMT system that have a scoring performance above some threshold as estimated by the SMT system itself. 282 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages"
W10-1741,D07-1105,0,0.210038,"Reranking results using TRegMT, TM, and LM scores. bold correspond to the best score in each rectangle of scores. measuring the average BLEU performance of each translation relative to the other translations in the N -best list. Thus, each possible translation in the N -best list is BLEU scored against other translations and the average of these scores is selected as the TM score for the sentence. Sentence level BLEU score calculation avoids singularities in ngram precisions by taking the maximum of the match count and 2|s1 i |for |si |denoting the length of the source sentence si as used in (Macherey and Och, 2007). corpus provided in WMT10. 5.1 Datasets We use the training set provided in WMT10 to index and select transductive instances from. The challenge split the test set for the translation task of 2489 sentences into a tuning set of 455 sentences and a test set with the remaining 2034 sentences. Translation outputs for each system is given in a separate file and the number of system outputs per translation pair varies. We have tokenized and lowercased each of the system outputs and combined these in a single N -best file per language pair. We also segment sentences using some of the punctuation fo"
W10-1741,2001.mtsummit-papers.68,0,0.0757267,"gram match counts to N -gram counts found in the translation and this gives an advantage to shorter translations. Therefore, a brevity penalty (BP) is added to penalize short translations: BLEU Challenge .1309 .1556 .2049 .2106 .1145 Table 1: Initial uncased performances of the translation systems. Feature mappers used are 3-spectrum counting word kernels, which consider all N -grams up to order 3 weighted by the number of tokens in the feature. We segment sentences using some of the punctuation for managing the feature set better and do not consider N -grams that cross segments. We use BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) evaluation metrics for measuring the performance of translations automatically. 4.2 ref-length , 0) trans-length BLEU = e(log(ngramprec )+BP ) BP = min(1 − (10) (11) where ngramprec represent the sum of n-gram precisions. Moses rarely incurs BP as it has a word penalty parameter optimized against BLEU which penalizes translations that are too long or too short. For instance, Moses 1-best translation for en-de system achieves .1309 BLEU versus .1320 BLEU without BP. We handle short translations in two ways. We optimize the λ parameter of QP, which manages the sparsi"
W10-1741,W05-0908,0,0.0170833,"Baseline Oracle L2 FSR LP QP en-de BLEU NIST .1309 5.1417 .1811 6.0252 .1319 5.1680 .1317* 5.1639 .1317 5.1695 .1309 5.1664 de-en BLEU NIST .1556 5.4164 .2101 6.2103 .1555 5.4344 .1559 5.4383 .1561 5.4304 .1550 5.4553 en-fr BLEU NIST .2049 6.3194 .2683 7.2409 .2044 6.3370 .2053 6.3458 .2048 6.3245 .2033 6.3354* en-es BLEU NIST .2106 6.3611 .2770 7.3190 .2132 6.4093 .2144 6.4168 .2109 6.4176 .2121 6.4271 en-cz BLEU NIST .1145 4.5008 .1628 5.4501 .1148 4.5187 .1150 4.5172 .1124 4.5143 .1150 4.5264 Table 2: Reranking results using TRegMT, TM, and LM scores. We use approximate randomization test (Riezler and Maxwell, 2005) with 1000 repetitions to determine score difference significance: results in bold are significant with p ≤ 0.01 and italic results with (*) are significant with p ≤ .05. The difference of the remaining from the baseline are not statistically significant. 4.4 Reranking Experiments to be a difficult task (Galley and Manning, 2009). Secondly, increasing the performance with reranking itself is a hard task since possible translations are already constrained by the ones observed in N best lists. Therefore, an increase in the N -best list size may increase the score gaps. Table 2 presents reranking"
W10-1741,P07-1004,0,0.0510751,"Missing"
W10-1741,W08-0322,0,0.35617,"e translation quality by using transduction. Transductive regression is shown to achieve higher accuracy than L2 regularized ridge regression on some machine learning benchmark datasets (Chapelle et al., 1999). In an idealized feature mapping matrix where Related Work: Regression techniques can be used to model the relationship between strings (Cortes et al., 2007). Wang et al. (2007) applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset. Later they use L2 regularized least squares regression (Wang and Shawe-Taylor, 2008). Although the translation quality they achieve is not better than Moses (Koehn et al., 2007), which is accepted to be the state-of-the-art, they show the feasibility of the approach. Serrano et al. (2009) use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests. Ueffing (2007) approaches the transductive learning problem for SMT by bootstrapping the training using the translations produced by the SMT system that have a scoring performance above some threshold as estimated by the SMT system itself. 282 Pr"
W10-1741,N07-2047,0,0.0270637,"he goal in transductive regression based machine translation (TRegMT) is both reducing the computational burden of the regression approach by reducing the dimensionality of the training set and the feature set and also improving the translation quality by using transduction. Transductive regression is shown to achieve higher accuracy than L2 regularized ridge regression on some machine learning benchmark datasets (Chapelle et al., 1999). In an idealized feature mapping matrix where Related Work: Regression techniques can be used to model the relationship between strings (Cortes et al., 2007). Wang et al. (2007) applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset. Later they use L2 regularized least squares regression (Wang and Shawe-Taylor, 2008). Although the translation quality they achieve is not better than Moses (Koehn et al., 2007), which is accepted to be the state-of-the-art, they show the feasibility of the approach. Serrano et al. (2009) use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests. Ueffi"
W10-1741,P02-1040,0,\N,Missing
W11-2131,ambati-etal-2010-active,0,0.0283862,"Missing"
W11-2131,P01-1005,0,0.0241791,"out-of-domain and includes an instance selection method also designed for improving word alignment results. We list our contributions in the last section. 2 in transductive learning and decreases human effort by identifying the most informative sentences for translation as in active learning. Instance selection in a transductive learning framework selects the best instances for a given test set (Lü et al., 2007). Active learning selects training samples that will benefit the learning algorithm the most over the unlabeled dataset U from a labeled training set L or from U itself after labeling (Banko and Brill, 2001). Active learning in SMT selects which instances to add to the training set to improve the performance of a baseline system (Haffari et al., 2009; Ananthakrishnan et al., 2010). Recent work involves selecting sentence or phrase translation tasks for external human effort (Bloodgood and Callison-Burch, 2010). Below we present examples of both with a label indicating whether they follow an approach close to active learning [AL] or transductive learning [TL] and in our experiments we use the transductive framework. TF-IDF [TL]: Lü et al. (2007) use tf-idf information retrieval technique based cos"
W11-2131,W10-1741,1,0.90981,"Missing"
W11-2131,P10-1088,0,0.0120365,"nstance selection in a transductive learning framework selects the best instances for a given test set (Lü et al., 2007). Active learning selects training samples that will benefit the learning algorithm the most over the unlabeled dataset U from a labeled training set L or from U itself after labeling (Banko and Brill, 2001). Active learning in SMT selects which instances to add to the training set to improve the performance of a baseline system (Haffari et al., 2009; Ananthakrishnan et al., 2010). Recent work involves selecting sentence or phrase translation tasks for external human effort (Bloodgood and Callison-Burch, 2010). Below we present examples of both with a label indicating whether they follow an approach close to active learning [AL] or transductive learning [TL] and in our experiments we use the transductive framework. TF-IDF [TL]: Lü et al. (2007) use tf-idf information retrieval technique based cosine score to select a subset of the parallel corpus close to the test set for SMT training. They outperform the baseline system when the top 500 training instances per test sentence are selected. The terms used in their tf-idf measure correspond to words where this work focuses on bigram feature coverage. W"
W11-2131,2005.mtsummit-papers.30,0,0.0420686,"al. (2007) use tf-idf information retrieval technique based cosine score to select a subset of the parallel corpus close to the test set for SMT training. They outperform the baseline system when the top 500 training instances per test sentence are selected. The terms used in their tf-idf measure correspond to words where this work focuses on bigram feature coverage. When the combination of the top N selected sentences are used as the training set, they show increase in the performance at the beginning and decrease when 2000 sentences are selected for each test sentence. N-gram coverage [AL]: Eck et al. (2005) use n-gram feature coverage to sort and select training instances using the following score: Pn Related Work Transductive learning makes use of test instances, which can sometimes be accessible at training time, to learn specific models tailored towards the test set. Selection of training instances relevant to the test set improves the final translation quality as P unseen x ∈ Xi (S) C(x) , φN GRAM (S) = |S| (1) for sentence S with Xi (S) storing the i-grams found in S and C(x) returning the count of x in the parallel corpus. φN GRAM score sums over unseen n-grams to increase the coverage of"
W11-2131,P09-1021,0,0.0287422,"increases the score of a sentence with increasing frequency of its n-grams found in U and decreases with increasing frequency in the already selected set of sentences, L, in favor of diversity. Let PU (x) denote the probability of feature x in U and CL (x) denote its count in L. Then: (ELPR) score is used: φELP R (S) = = u(S) = φDW DS (S) = 0.6 + |Xoov (S)| X X x∈Xoov (S) h∈H(x) 1 |H(x)| PU (x) PL (x) X log y∈Yh (x) PU (y) , PL (y) (5) where H(x) return the segmentations of x and Yh (x) return the features found in segment h. φELP R performs better than geometric average in their experiments (Haffari and Sarkar, 2009). Perplexity [AL & TL]: Perplexity of the training instance as well as inter-SMT-system disagreement are also used to select training data for translation models (Mandal et al., 2008). The increased difficulty in translating a parallel sentence or its novelty as found by the perplexity adds to its importance for improving the SMT model’s performance. A sentence having high perplexity (a rare sentence) in L and low perplexity (a common sentence) in U is considered as a candidate for addition. They are able to improve the performance of a baseline system trained on some initial corpus together w"
W11-2131,N09-1047,0,0.0726317,"ection. 2 in transductive learning and decreases human effort by identifying the most informative sentences for translation as in active learning. Instance selection in a transductive learning framework selects the best instances for a given test set (Lü et al., 2007). Active learning selects training samples that will benefit the learning algorithm the most over the unlabeled dataset U from a labeled training set L or from U itself after labeling (Banko and Brill, 2001). Active learning in SMT selects which instances to add to the training set to improve the performance of a baseline system (Haffari et al., 2009; Ananthakrishnan et al., 2010). Recent work involves selecting sentence or phrase translation tasks for external human effort (Bloodgood and Callison-Burch, 2010). Below we present examples of both with a label indicating whether they follow an approach close to active learning [AL] or transductive learning [TL] and in our experiments we use the transductive framework. TF-IDF [TL]: Lü et al. (2007) use tf-idf information retrieval technique based cosine score to select a subset of the parallel corpus close to the test set for SMT training. They outperform the baseline system when the top 500"
W11-2131,N07-2015,0,0.0246862,"t L or from U itself after labeling (Banko and Brill, 2001). Active learning in SMT selects which instances to add to the training set to improve the performance of a baseline system (Haffari et al., 2009; Ananthakrishnan et al., 2010). Recent work involves selecting sentence or phrase translation tasks for external human effort (Bloodgood and Callison-Burch, 2010). Below we present examples of both with a label indicating whether they follow an approach close to active learning [AL] or transductive learning [TL] and in our experiments we use the transductive framework. TF-IDF [TL]: Lü et al. (2007) use tf-idf information retrieval technique based cosine score to select a subset of the parallel corpus close to the test set for SMT training. They outperform the baseline system when the top 500 training instances per test sentence are selected. The terms used in their tf-idf measure correspond to words where this work focuses on bigram feature coverage. When the combination of the top N selected sentences are used as the training set, they show increase in the performance at the beginning and decrease when 2000 sentences are selected for each test sentence. N-gram coverage [AL]: Eck et al."
W11-2131,W01-0504,0,0.0928896,"Missing"
W11-2131,P07-2045,0,0.00723602,"i Koç University 34450 Sariyer, Istanbul, Turkey ebicici@ku.edu.tr Abstract Deniz Yuret Koç University 34450 Sariyer, Istanbul, Turkey dyuret@ku.edu.tr 1 Introduction Statistical machine translation (SMT) makes use of a large number of parallel sentences, sentences whose translations are known in the target language, to derive translation tables, estimate parameters, and generate the actual translation. Not all of the parallel corpus nor the translation table that is generated is used during decoding a given set of test sentences and filtering is usually performed for computational advantage (Koehn et al., 2007). Some recent regression-based statistical machine translation systems rely on a small sized training data to learn the mappings between source and target features (Wang and Shawe-Taylor, 2008; Serrano et al., 2009; Bicici and Yuret, 2010). Regression has some computational disadvantages when scaling to large number of training instances. Previous work shows that the more the training data, the better the translations become (Koehn, 2006). However, with the increased size of the parallel corpus there is also the added noise, making relevant instance selection important. Phrasebased SMT systems"
W11-2131,W04-3250,0,0.0731843,"tances selected for each test sentence. L∪F L is selected using all of the features found in the test set. LI L is the set of instances selected for each test sentence. 0.38 0.37 0.36 0.35 BLEU We develop separate Moses systems with each training set and LI corresponds to developing a Moses system for each test sentence. L∪ results are plot in Figure 3 where we increasingly select N ∈ {100, 200, 500, 1000, 2000, 3000, 5000, 10000} instances for each test sentence for training. The improvements over the baseline are statistically significant with paired bootstrap resampling using 1000 samples (Koehn, 2004). As we select more instances, the performance of the SMT system increases as expected and we start to see a decrease in the performance after selecting ∼107 target words. We obtain comparable results for the de-en direction. The performance increase is likely to be due to the reduction in the number of noisy or irrelevant training instances and the increased precision in the probability estimates in the generated BLEU vs. Training Set Size (words) 0.3758 0.3697 0.3653 0.3645 0.3622 0.3577 0.341 0.34 0.33 0.3318 0.32 0.31 0.30 5 10 0.3058 106 107 Training Set Size (words) 108 Figure 3: BLEU vs"
W11-2131,D07-1036,0,0.197616,"to achieve a similar performance with the baseline. The next section reviews related previous work. We discuss the FDA in section 3. Section 4 presents our coverage and translation results both in and out-of-domain and includes an instance selection method also designed for improving word alignment results. We list our contributions in the last section. 2 in transductive learning and decreases human effort by identifying the most informative sentences for translation as in active learning. Instance selection in a transductive learning framework selects the best instances for a given test set (Lü et al., 2007). Active learning selects training samples that will benefit the learning algorithm the most over the unlabeled dataset U from a labeled training set L or from U itself after labeling (Banko and Brill, 2001). Active learning in SMT selects which instances to add to the training set to improve the performance of a baseline system (Haffari et al., 2009; Ananthakrishnan et al., 2010). Recent work involves selecting sentence or phrase translation tasks for external human effort (Bloodgood and Callison-Burch, 2010). Below we present examples of both with a label indicating whether they follow an ap"
W11-2131,J03-1002,0,0.00198039,"e (underlined) using the mean weights and 1 BLEU difference using the union weights. We also experimented with increasing the N -best list size used during MERT optimization (Hasan et al., 2007), with increased computational cost, and observed some increase in the performance. N 1000 2000 3000 5000 DWDS 0.3547 8 279 Instance Selection for Alignment We have shown that high coverage is an integral part of training sets for achieving high BLEU performance. SMT systems also heavily rely on the word alignment of the parallel corpus to derive a phrase table that can be used for translation. GIZA++ (Och and Ney, 2003) is commonly used for word alignment and phrase table generation, which is prone to making more errors as the length of the training sentence increase (Ravi and Knight, 2010). Therefore, we analyze instance selection techniques that optimize coverage and word alignment performance and at the same time do not produce very long sentences. Too few words per sentence may miss the phrasal structure, whereas too many words per sentence may miss the actual word alignment for the features we are interested. We are also trying to retrieve relevant training sentences for a given test sentence to increas"
W11-2131,2001.mtsummit-papers.68,0,0.0290066,"We use the language model corpus provided in WMT’10 (Callison-Burch et al., 2010) to build a 5-gram model. We use target language bigram coverage, tcov, as a quality measure for a given training set, which measures the percentage of the target bigram features of the test sentence found in a given training set. We compare tcov and the translation performance of FDA with related work. We also perform small scale SMT experiments where only a couple of thousand training instances are used for each test sentence. 4.1 The Effect of Coverage on Translation init(f, U) = 1 or log(|U|/cnt(f, U)) BLEU (Papineni et al., 2001) is a precision based init(f, U) measure and uses n-gram match counts up to orinit(f, U) or decay(f, U, L) = 1 + cnt(f, L) 1 + 2cnt(f,L) der n to determine the quality of a given translation. The absence of a given word or translating it as another word interrupts the continuity of the translation and decreases the BLEU score even if the order among the words is determined correctly. init decay en→de de→en Therefore, the target coverage of an out-of-domain 1 none .761 .484 .698 .556 test set whose translation features are not found in log(1/f ) none .855 .516 .801 .604 the training set bounds"
W11-2131,J10-3001,0,0.0169067,"ion (Hasan et al., 2007), with increased computational cost, and observed some increase in the performance. N 1000 2000 3000 5000 DWDS 0.3547 8 279 Instance Selection for Alignment We have shown that high coverage is an integral part of training sets for achieving high BLEU performance. SMT systems also heavily rely on the word alignment of the parallel corpus to derive a phrase table that can be used for translation. GIZA++ (Och and Ney, 2003) is commonly used for word alignment and phrase table generation, which is prone to making more errors as the length of the training sentence increase (Ravi and Knight, 2010). Therefore, we analyze instance selection techniques that optimize coverage and word alignment performance and at the same time do not produce very long sentences. Too few words per sentence may miss the phrasal structure, whereas too many words per sentence may miss the actual word alignment for the features we are interested. We are also trying to retrieve relevant training sentences for a given test sentence to increase the feature alignment performance. Shortest: A baseline strategy that can minimize the training feature set’s size involves selecting the shortest translations containing e"
W11-2131,C10-1124,0,0.0138288,"lso used to select training data for translation models (Mandal et al., 2008). The increased difficulty in translating a parallel sentence or its novelty as found by the perplexity adds to its importance for improving the SMT model’s performance. A sentence having high perplexity (a rare sentence) in L and low perplexity (a common sentence) in U is considered as a candidate for addition. They are able to improve the performance of a baseline system trained on some initial corpus together with additional parallel corpora using the initial corpus and part of the additional data. Alignment [TL]: Uszkoreit et al. (2010) mine parallel text to improve the performance of a baseline translation model on some initial document translation tasks. They retrieve similar documents using inverse document frequency weighted cosine similarity. Then, they filter nonparallel sentences using their word alignment performance, which is estimated using the following score: PU (x)e−λCL (x) (2) |X(S)| P x∈X(S) I(x 6∈ X(L)) (3) |X(S)| 2d(S)u(S) , (4) d(S) + u(S) x∈X(S) where X(S) stores the features of S and λ is a decay parameter. d(S) denotes the density of S proportional to the probability of its features in U and inversely pr"
W11-2131,W08-0322,0,0.122424,"Missing"
W11-2131,D10-1061,0,\N,Missing
W11-2131,P02-1040,0,\N,Missing
W11-2137,W10-1740,1,0.707337,"es with 0.74 BLEU points difference and for ht-en with 4.2 BLEU points difference. For es-en task, there is 0.36 BLEU points difference with the second best model and these models likely to complement each other. The existence of complementing SMT models is important for the reranking approach to achieve a performance better than the best model, as there is a need for the existence of a model performing better than the best model on some test sentences. We can use the competitive SMT model to achieve the performance of the best with a guarantee even when a single model is dominating the rest (Bicici and Kozat, 2010). For competing translation systems in an on-line machine translation setting adaptively learning of model weights can be performed based on the previous transaltion performance (Bicici and Kozat, 2010). 6 Target F1 as a Performance Evaluation Metric We use target sentence F1 measure over the target features as a translation performance evaluation metric. We optimize the parameters of the RegMT model with the F1 measure comparing the target vector with the estimate we get from the RegMT model. F1 measure uses the 0/1-class predictions over the target feature with the estimate vector, ΦY (ˆy)."
W11-2137,W10-1741,1,0.616076,"sion based machine translation (RegMT) is both reducing the computational burden of the regression approach by reducing the dimensionality of the training set and the feature set and also improving the translation quality by using transduction. We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F1 measure over target features as a metric for evaluating translation quality. RegMT work builds on our previous regression-based machine translation results (Bicici and Yuret, 2010) especially with instance selection and additional graph decoding capability. We present our results to this year’s challenges. Outline: Section 2 gives an overview of the RegMT model. In section 3, we present our training instance selection techniques and WMT’11 results. In section 4, we present the graph decoding results on the Haitian Creole-English translation task. Section 5 presents our system combination results using reranking with the RegMT score. Section 6 evaluates the F1 measure that we use for the automatic evaluation metrics challenge. The last section present our contributions."
W11-2137,W11-2131,1,0.835355,"Missing"
W11-2137,P07-2045,0,0.0242738,"or approximation techniques. We use forward stagewise regression (FSR) (Hastie et al., 2006), which approximates lasso for L1 regularized regression. 2.2 Related Work: Regression techniques can be used to model the relationship between strings (Cortes et al., 2007). Wang et al. (2007) applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset. Later they use L2 regularized least squares regression (Wang and Shawe-Taylor, 2008). Although the translation quality they achieve is not better than Moses (Koehn et al., 2007), which is accepted to be the state-of-the-art, they show the feasibility of the approach. Serrano et al. (2009) use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests. Locally weighted regression solves separate weighted least squares problems for each instance (Hastie et al., 2009), weighted by a kernel similarity function. 3 Instance Selection for Machine Translation Proper selection of training instances plays an important role for accurately learning feature mappings with limited computational reso"
W11-2137,D07-1105,0,0.0144929,"model scores each system obtained for each sentence, we estimate translation model performance (CBLEU) by measuring 326 the average BLEU performance of each translation relative to the other translations in the N -best list. Thus, each possible translation in the N -best list is BLEU scored against other translations and the average of these scores is selected as the CBLEU score for the sentence. Sentence level BLEU score calculation avoids singularities in n-gram precisions by taking the maximum of the match count and 2|s1 i |for |si |denoting the length of the source sentence si as used in (Macherey and Och, 2007). Table 2 presents reranking results on all of the language pairs we considered, using RegMT, CBLEU, and LM scores with the same combination weights as above. We also list the performance of the best model (Max) as well as the worst (Min). We are able to achieve close or better BLEU scores in all of the listed systems when compared with the performance of the best translation system except for the ht-en language pair. The lower performance in the ht-en language pair may be due to having a single best translation system that outperforms others significantly. This happens for instance when an un"
W11-2137,P03-1021,0,0.101756,"en-de, de-en, enes, and es-en. We created training sets using all 325 We perform graph-based decoding by first generating a De Bruijn graph from the estimated ˆy (Cortes et al., 2007) and then finding Eulerian paths with maximum path weight. We use four features when scoring paths: (1) estimation weight from regression, (2) language model score, (3) brevity penalty as found by eα(lR −|s|/|path|) for lR representing the length ratio from the parallel corpus and |path |representing the length of the current path, (4) future cost as in Moses (Koehn et al., 2007) and weights are tuned using MERT (Och, 2003) on the de-en dev set. We demonstrate that sparse L1 regularized regression performs better than L2 regularized regression. Graph based decoding can provide an alternative to state of the art phrase-based decoding system Moses in translation domains with small vocabulary and training set size. 4.1 Haitian Creole to English Translation Task with RegMT We have trained a Moses system for the Haitian Creole to English translation task, cleaned corpus, using the options as described in section 3.1. Moses achieves 0.3186 BLEU on this task. We observed that graph decoding performs better where target"
W11-2137,2001.mtsummit-papers.68,0,0.0257978,"f the learning models in identifying the features of the target sentence making minimal error to increase the performance of the decoder and its translation quality. We use gapped word sequence kernels (Taylor and Cristianini, 2004) when using F1 for evaluating translations since a given translation system may not be able to translate a given word but can correctly identify the surrounding phrase. For instance, let the reference translation be the following sentence: a sound compromise has been reached Some possible translations for the reference are given in Table 3 together with their BLEU (Papineni et al., 2001) and F1 scores for comparison. F1 score 327 does not have a brevity penalty but a brief translation is penalized by a low recall value. We use up to 3 tokens as gaps. F1 measure is able to increase the ranking of Trans4 by using a gapped sequence kernel, which can be preferrable to Trans3 . We note that a missing token corresponds to varying decreases in the n-gram precision used in the BLEU score. A sentence containing m tokens has m 1-grams, m − 1 2-grams, and m − n + 1 n-grams. A missing token degrades the performance more in higher order n-gram precision values. A missing to1 ken decreases"
W11-2137,W08-0322,0,0.0122879,"g the derivative but since L1 regularization cost is not differentiable, WL1 is found by optimization or approximation techniques. We use forward stagewise regression (FSR) (Hastie et al., 2006), which approximates lasso for L1 regularized regression. 2.2 Related Work: Regression techniques can be used to model the relationship between strings (Cortes et al., 2007). Wang et al. (2007) applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset. Later they use L2 regularized least squares regression (Wang and Shawe-Taylor, 2008). Although the translation quality they achieve is not better than Moses (Koehn et al., 2007), which is accepted to be the state-of-the-art, they show the feasibility of the approach. Serrano et al. (2009) use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests. Locally weighted regression solves separate weighted least squares problems for each instance (Hastie et al., 2009), weighted by a kernel similarity function. 3 Instance Selection for Machine Translation Proper selection of training instances pla"
W11-2137,N07-2047,0,0.0203859,"penalizing the coefficients better; zeroing the irrele324 Equation 3 presents the lasso (Tibshirani, 1996) solution where the regularization term P is now the L1 matrix norm defined as kWk1 = i,j |Wi,j |. WL2 can be found by taking the derivative but since L1 regularization cost is not differentiable, WL1 is found by optimization or approximation techniques. We use forward stagewise regression (FSR) (Hastie et al., 2006), which approximates lasso for L1 regularized regression. 2.2 Related Work: Regression techniques can be used to model the relationship between strings (Cortes et al., 2007). Wang et al. (2007) applies a string-to-string mapping approach to machine translation by using ordinary least squares regression and n-gram string kernels to a small dataset. Later they use L2 regularized least squares regression (Wang and Shawe-Taylor, 2008). Although the translation quality they achieve is not better than Moses (Koehn et al., 2007), which is accepted to be the state-of-the-art, they show the feasibility of the approach. Serrano et al. (2009) use kernel regression to find translation mappings from source to target feature vectors and experiment with translating hotel front desk requests. Local"
W11-2137,P02-1040,0,\N,Missing
W14-1619,P06-1046,0,0.0538911,"Missing"
W14-1619,N09-1003,0,0.457211,"Missing"
W14-1619,P98-2127,0,0.294726,"d interchangeably, it is common to distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2001; Agirre et al., 2009). Semantic similarity is used to describe ‘likeness’ relations, such as the relations between synonyms, hypernymhyponyms, and co-hyponyms. Semantic relatedness refers to a broader range of relations including also meronymy and various other associative relations as in ‘pencil-paper’ or ‘penguinAntarctica’. In this work we focus on semantic similarity and evaluate all compared methods on several semantic similarity tasks. Following previous works (Lin, 1998; Riedl and Biemann, 2013) we use Wordnet to construct large scale gold standards for semantic similarity evaluations. We perform the evaluations separately for nouns and verbs to test our hypothesis that our model is particularly well-suited for verbs. To further evaluate our results on verbs we use the verb similarity test-set released by (Yang and Powers, 2006), which contains pairs of verbs associated with semantic similarity scores based on human judgements. As intended, this similarity measure promotes word pairs in which both b is likely in the contexts of a and vice versa. Next, we des"
W14-1619,P13-1131,1,0.850707,"ur model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity between phrases. Another direction relates to the well known tendency of many words, and particularly verbs, to assume different meanings (or senses) under different contexts. To address this phenomenon context sensitive similarity and inference models have been proposed (Dinu and Lapata, 2010; Melamud et al., 2013). Similarly to many semantic similarity models, our current model aggregates information from all observed contexts of a target word type regardless of its different senses. However, we believe that our approach is well suited to address context sensitive similarity with proper enhancements, as it considers joint-contexts that can more accurately disambiguate the meaning of target words. As an example, it is possible to consider the likelihood of word b to occur in a subset of the contexts observed for word a, which is biased towards a particular sense of a. Future Directions In this paper we"
W14-1619,J92-4003,0,0.659211,"n particular are correlated with their syntagmatic properties (Levin, 1993; Hanks, 2013). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012), which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme. Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns. 2 3 3.1 Probabilistic Distributional Similarity Motivation In this section we briefly demonstrate the benefits of considering joint-contexts of words. As an illustrative example, we note that the target words like and surround may share many individual word features such as “school” and “campus” in the"
W14-1619,P12-1015,0,0.0428918,"in contrast to the feature vector baselines, whose performance declines for context window orders larger than 2. This suggests that our approach is able to take advantage of larger contexts in comparison to standard feature vector models. The decline in performance for the independent feature vector baseline (IFV) may be related to the fact that independent features farther away from the target word are generally more loosely related to it. This seems consistent with previous works, where narrow windows of the order of two words performed well (Bullinaria and Levy, 2007; Agirre et al., 2009; Bruni et al., 2012) and in particular so when evaluating semantic similarity rather than relatedness. On the other hand, the decline in performance for the composite feature vector baseline (CFV) may be attributed to the data sparseness phenomenon associated with larger windows. The performance of the word embedding baselines (CBOW and SKIP) starts declining very mildly only for window orders larger than 4. This might be attributed to the fact that these models assign lower weights to context words the farther away they are from the center of the window. VerbSim evaluation The publicly available VerbSim test-set"
W14-1619,P08-1028,0,0.0464893,"the nature of its underlying language model. Therefore, we see much potential in exploring the use of other types of language models, such as class-based (Brown et al., 1992), syntax-based (Pauls and Klein, 2012) or hybrid (Tan et al., 2012). Furthermore, a similar approach to ours could be attempted in word embedding models. For instance, our syntagmatic joint-context modeling approach could be investigated by word embedding models to generate better embeddings for verbs. 6 Finally, we note that our model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity between phrases. Another direction relates to the well known tendency of many words, and particularly verbs, to assume different meanings (or senses) under different contexts. To address this phenomenon context sensitive similarity and inference models have been proposed (Dinu and Lapata, 2010; Melamud et al., 2013). Similarly to many semantic similarity models, our current model aggregates information from all obse"
W14-1619,P11-1027,0,0.0166443,"ation (Church and Hanks, 1990), and the combination of Cosine with PPMI was shown to perform particularly well in (Bullinaria and Levy, 2007). We denote Mikolov’s CBOW and Skip-gram baseline models by CBOW W −k and SKIP W −k respectively, where k denotes again the order of the window used to train these models. We used Mikolov’s word2vec utility3 with standard parameters (600 dimensions, negative sampling 15) to learn the word embeddings, and Cosine as the vector similarity measure between them. As the underlying probabilistic language model for our method we use the Berkeley implementation4 (Pauls and Klein, 2011) of the Kneser-Ney n-gram model with the default discount parameters. We denote our model P DS W −k , where PDS stands for “Probabilistic Distributional Similarity”, and k is the order of the context word window. In order to avoid giving our model an unfair advantage of tuning the order of the language model n as an additional parameter, we use a fixed n = k + 1. This means that the conditional probabilities that our n-gram model learns consider a scope of up to half the size of the window, which is the distance in words between the target word and either end of the window. We note that this i"
W14-1619,J90-1003,0,0.467863,"overhead, e.g. by counting the learning corpus occurrences of n-gram templates, in which one of the elements matches any word. (5) 184 denote the composite-feature vector baseline by CF V W −k , where CFV stands for “CompositeFeature Vector”. This baseline constructs traditional-like feature vectors, but considers entire word windows around target word tokens as single features. In both of these baselines we use Cosine as the vector similarity measure, and positive pointwise mutual information (PPMI) for the feature vector weights. PPMI is a well-known variant of pointwise mutual information (Church and Hanks, 1990), and the combination of Cosine with PPMI was shown to perform particularly well in (Bullinaria and Levy, 2007). We denote Mikolov’s CBOW and Skip-gram baseline models by CBOW W −k and SKIP W −k respectively, where k denotes again the order of the window used to train these models. We used Mikolov’s word2vec utility3 with standard parameters (600 dimensions, negative sampling 15) to learn the word embeddings, and Cosine as the vector similarity measure between them. As the underlying probabilistic language model for our method we use the Berkeley implementation4 (Pauls and Klein, 2011) of the"
W14-1619,P12-1101,0,0.0174598,"text windows. We further conjecture that the reason the word embedding baselines did not do as well as our model on verb similarity might be due to their particular choice of joint-context formulation, which is not sensitive to word order. However, these conjectures should be further validated with additional evaluations in future work. First, the performance of our generic scheme is largely inherited from the nature of its underlying language model. Therefore, we see much potential in exploring the use of other types of language models, such as class-based (Brown et al., 1992), syntax-based (Pauls and Klein, 2012) or hybrid (Tan et al., 2012). Furthermore, a similar approach to ours could be attempted in word embedding models. For instance, our syntagmatic joint-context modeling approach could be investigated by word embedding models to generate better embeddings for verbs. 6 Finally, we note that our model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity betwe"
W14-1619,D13-1089,0,0.0532128,"geably, it is common to distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2001; Agirre et al., 2009). Semantic similarity is used to describe ‘likeness’ relations, such as the relations between synonyms, hypernymhyponyms, and co-hyponyms. Semantic relatedness refers to a broader range of relations including also meronymy and various other associative relations as in ‘pencil-paper’ or ‘penguinAntarctica’. In this work we focus on semantic similarity and evaluate all compared methods on several semantic similarity tasks. Following previous works (Lin, 1998; Riedl and Biemann, 2013) we use Wordnet to construct large scale gold standards for semantic similarity evaluations. We perform the evaluations separately for nouns and verbs to test our hypothesis that our model is particularly well-suited for verbs. To further evaluate our results on verbs we use the verb similarity test-set released by (Yang and Powers, 2006), which contains pairs of verbs associated with semantic similarity scores based on human judgements. As intended, this similarity measure promotes word pairs in which both b is likely in the contexts of a and vice versa. Next, we describe a model which implem"
W14-1619,D10-1113,0,0.0195731,"Finally, we note that our model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity between phrases. Another direction relates to the well known tendency of many words, and particularly verbs, to assume different meanings (or senses) under different contexts. To address this phenomenon context sensitive similarity and inference models have been proposed (Dinu and Lapata, 2010; Melamud et al., 2013). Similarly to many semantic similarity models, our current model aggregates information from all observed contexts of a target word type regardless of its different senses. However, we believe that our approach is well suited to address context sensitive similarity with proper enhancements, as it considers joint-contexts that can more accurately disambiguate the meaning of target words. As an example, it is possible to consider the likelihood of word b to occur in a subset of the contexts observed for word a, which is biased towards a particular sense of a. Future Direc"
W14-1619,rose-etal-2002-reuters,0,0.0145447,"n additional parameter, we use a fixed n = k + 1. This means that the conditional probabilities that our n-gram model learns consider a scope of up to half the size of the window, which is the distance in words between the target word and either end of the window. We note that this is the smallest reasonable value for n, as smaller values effectively mean that there will be context words within the window that are more than n words away from the target word, and therefore will not be considered by our model. As learning corpus we used the first CD of the freely available Reuters RCV1 dataset (Rose et al., 2002). This learning corpus contains approximately 100M words, which is comparable in size to the British National Corpus (BNC) (Aston, 1997). We first applied part-of-speech tagging and lemmatization to all words. Then we represented each word w in the corpus as the pair 3 4 [pos(w), lemma(w)], where pos(w) is a coarsegrained part-of-speech category and lemma(w) is the lemmatized form of w. Finally, we converted every pair [pos(w), lemma(w)] that occurs less than 100 times in the learning corpus to the pair [pos(w), ? ], which represents all rare words of the same part-of-speech tag. Ignoring rare"
W14-1619,J12-3007,0,0.161258,"h their syntagmatic properties (Levin, 1993; Hanks, 2013). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012), which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme. Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns. 2 3 3.1 Probabilistic Distributional Similarity Motivation In this section we briefly demonstrate the benefits of considering joint-contexts of words. As an illustrative example, we note that the target words like and surround may share many individual word features such as “school” and “campus” in the sentences “Mary’s son likes"
W14-1619,U07-1017,0,0.0592849,"Missing"
W14-1619,D12-1086,1,0.858489,"an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993; Hanks, 2013). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012), which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme. Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns. 2 3 3.1 Probabilistic Distributional Similarity Motivation In this section we briefly demonstrate th"
W14-1619,C98-2122,0,\N,Missing
W19-1604,N16-1181,0,0.47951,"sions. An instruction refers to a target object using its representative features (e.g. color, shape, size). If a noun phrase does not resolve the ambiguity in the world, the instruction resolves the ambiguity by specifying the target object with its relative position to other surrounding objects. This hierarchy tries to bring the attention to finding the unique object, then shifts the attention to the targeted object. Based on this idea, we model the language grounding process as controlling the attention on the world representation by adapting the neural module networks approach proposed by Andreas et al. (2016b). Our natural language grounder has three components: a preprocessor, an instruction parser and a program executor. Given specific anchor information (Figure 1 – № 1), the preprocessor transforms the anchor information into an intermediate representation in grid form (Figure 1 – № 2). The instruction parser produces a computational program by exploiting the syntactic representation (Figure 1 – № 3) of the instruction with a dependency parser1 . The program executor runs (Figure 1 – № 4) the program on the intermediate representation to produce commands. Preprocessor. The anchoring framework"
W19-1604,E17-1052,0,0.0312841,"re and after state for an action sequence. They modeled the interactive learning as an MDP and solved it with reinforcement learning. Thomason et al. (2015) proposed a system that learns the meaning of natural language commands through human-robot dialog. They represent the meaning of instructions with λ-calculus semantic representation. Their semantic parser starts with an initial knowledge and learns through training examples generated by the human-robot conversations. Their dialog manager is a static policy which generates questions from a discrete set of action, patient, recipient tuples. Padmakumar et al. (2017) improved this work with a learnable dialog manager. They train both the dialog manager and the semantic parser with reinforcement learning. This approach was further extended in (Thomason et al., 2019), where the authors combine the approach in Thomason et al. (2015) and Thomason et al. (2017) to obtain a system that is capable of concept acquisition through clarification dialogues. Instead of asking questions, we implicitly fix the perception with the information hidden in instructions. A further difference to these works is that we learn the language component in a simulated offline step, w"
W19-1604,K15-1023,0,0.0710034,"Missing"
W19-1604,P17-1150,0,0.167946,"vision that an instruction would give to a robot is not present when learning the representation of the world of a robot. These inconsistencies then propagate through to inconsistencies between the instructions a human gives to a robot and the robot’s world model. To ensure that a robot is able to correctly carry out an instruction, such inconsistencies must be resolved and the world model of the robot be matched to the world model of the human. This is not the first paper that tackles the problem of belief revision in robotics. However, prior work (Tellex et al., 2013; Thomason et al., 2015; She and Chai, 2017), with the notable exception of (Mast et al., 2016), relied on explicit information transfer between humans and robots when inconsistencies arose in grounded language and the robot’s world representation. An example would be a robot asking clarification questions until it is clear what the human meant (Tellex et al., 2013). We propose an approach that probabilistically reasons over the grounding of an instruction and a robot’s world representation in order to perform Bayesian learning to update the world representation given the grounding. This is closely related to the work of Mast et al. who"
W19-5045,W19-5039,0,0.0286203,"Missing"
W19-5045,D15-1075,0,0.0843749,"th a task-specific head, and then fine-tuning the entire model on the target task achieved state-of-the-art results on a number of tasks. 2 Directly applying BERT to NLI yields high accuracy if the dataset is large, and from a general domain (Phang et al., 2018). However, the dataset used in this shared task, MedNLI (Romanov and Shivade, 2018), is based on clinical notes (i.e. patient histories) and limited by size, thus it is a particularly challenging NLI task. To address this problem, we started with BERT, trained it on large NLI datasets, such as MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015), to support the inference reasoning, and then trained it further on our target task, MedNLI. This kind of intermediate training was shown to be effective when BERT is trained on a target with limited data (Phang et al., 2018). We call our approach twostage transfer learning. In the first stage, we transfer the knowledge of the task, NLI, into our pretrained model. During the second stage, we speModel Our baseline model is a BERT encoder (Devlin et al., 2018), combined with a classification head. The head outputs probabilities from a three-way softmax, corresponding to the three possible label"
W19-5045,D17-1070,0,0.0562293,"e (NLI) is one of the central problems in artificial intelligence. It requires understanding two input sentences and forming an inference relationship between them. Concretely, given a premise sentence p, and a hypothesis sentence h, NLI is the task of determining the inference relationship from p to h. In MedNLI, this relationship is one of the neutral, entailment and contradiction labels. Therefore, our task can be considered as a three-class sentence pair classification problem. In previous research, sequence encoders connected with a classifier head have been commonly used as NLI systems (Conneau et al., 2017). Traditionally, the encoder layer has been an RNN-based model such as LSTM (Hochreiter and Schmidhuber, 1997). Vaswani et al. (2017) proposed the 427 Proceedings of the BioNLP 2019 workshop, pages 427–436 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Classiﬁcation Head Results Encoder Layer Layer Normalization Layer Normalization Layer Normalization Layer Normalization Encoder Layer Feed Forward Feed Forward Feed Forward Feed Forward Embedding Layer Normalization Layer Normalization Layer Normalization Layer Normalization Multi-Head Attention Multi-Head Att"
W19-5045,P82-1020,0,0.798137,"Missing"
W19-5045,C18-1054,0,0.0364123,"Missing"
W19-5045,P19-1441,0,0.0919982,"Missing"
W19-5045,D18-1187,0,0.0829141,"Missing"
W19-5045,N18-1101,0,0.0499014,"ing a pre-trained BERT encoder with a task-specific head, and then fine-tuning the entire model on the target task achieved state-of-the-art results on a number of tasks. 2 Directly applying BERT to NLI yields high accuracy if the dataset is large, and from a general domain (Phang et al., 2018). However, the dataset used in this shared task, MedNLI (Romanov and Shivade, 2018), is based on clinical notes (i.e. patient histories) and limited by size, thus it is a particularly challenging NLI task. To address this problem, we started with BERT, trained it on large NLI datasets, such as MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015), to support the inference reasoning, and then trained it further on our target task, MedNLI. This kind of intermediate training was shown to be effective when BERT is trained on a target with limited data (Phang et al., 2018). We call our approach twostage transfer learning. In the first stage, we transfer the knowledge of the task, NLI, into our pretrained model. During the second stage, we speModel Our baseline model is a BERT encoder (Devlin et al., 2018), combined with a classification head. The head outputs probabilities from a three-way softmax, correspond"
