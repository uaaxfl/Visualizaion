2021.findings-emnlp.85,Structured abbreviation expansion in context,2021,-1,-1,4,0.35874,1324,kyle gorman,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Ad hoc abbreviations are commonly found in informal communication channels that favor shorter messages. We consider the task of reversing these abbreviations in context to recover normalized, expanded versions of abbreviated messages. The problem is related to, but distinct from, spelling correction, as ad hoc abbreviations are intentional and can involve more substantial differences from the original words. Ad hoc abbreviations are also productively generated on-the-fly, so they cannot be resolved solely by dictionary lookup. We generate a large, open-source data set of ad hoc abbreviations. This data is used to study abbreviation strategies and to develop two strong baselines for abbreviation expansion."
2020.sigtyp-1.3,{NEMO}: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in {SIGTYP} 2020 Shared Task,2020,-1,-1,2,0.689655,11005,alexander gutkin,Proceedings of the Second Workshop on Computational Research in Linguistic Typology,0,"This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguistic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We employ frequentist inference to represent correlations between typological features and use this representation to train simple multi-class estimators that predict individual features. We describe two submitted ridge regression-based configurations which ranked second and third overall in the constrained task. Our best configuration achieved the microaveraged accuracy score of 0.66 on 149 test languages."
2020.coling-main.411,Semi-supervised {URL} Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities,2020,-1,-1,3,0,7671,hao zhang,Proceedings of the 28th International Conference on Computational Linguistics,0,"Breaking domain names such as openresearch into component words open and research is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks (RNNs) using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33{\%} and brings the sequence accuracy to 85{\%}."
J19-2004,Neural Models of Text Normalization for Speech Applications,2019,27,3,2,0,7671,hao zhang,Computational Linguistics,0,"Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis (TTS). In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such {``}unrecoverable{''} errors. Such grammars can largely be learned from data."
C18-1123,Fast and Accurate Reordering with {ITG} Transition {RNN},2018,0,1,3,0,7671,hao zhang,Proceedings of the 27th International Conference on Computational Linguistics,0,"Attention-based sequence-to-sequence neural network models learn to jointly align and translate. The quadratic-time attention mechanism is powerful as it is capable of handling arbitrary long-distance reordering, but computationally expensive. In this paper, towards making neural translation both accurate and efficient, we follow the traditional pre-reordering approach to decouple reordering from translation. We add a reordering RNN that shares the input encoder with the decoder. The RNNs are trained jointly with a multi-task loss function and applied sequentially at inference time. The task of the reordering model is to predict the permutation of the input words following the target language word order. After reordering, the attention in the decoder becomes more peaked and monotonic. For reordering, we adopt the Inversion Transduction Grammars (ITG) and propose a transition system to parse input to trees for reordering. We harness the ITG transition system with RNN. With the modeling power of RNN, we achieve superior reordering accuracy without any feature engineering. In experiments, we apply the model to the task of text normalization. Compared to a strong baseline of attention-based RNN, our ITG RNN re-ordering model can reach the same reordering accuracy with only 1/10 of the training data and is 2.5x faster in decoding."
W16-6323,Keynote Lecture 2: Neural (and other Machine Learning) Approaches to Text Normalization,2016,0,0,1,1,6614,richard sproat,Proceedings of the 13th International Conference on Natural Language Processing,0,None
Q16-1036,Minimally Supervised Number Normalization,2016,6,4,2,0.35874,1324,kyle gorman,Transactions of the Association for Computational Linguistics,0,"We propose two models for verbalizing numbers, a key component in speech recognition and synthesis systems. The first model uses an end-to-end recurrent neural network. The second model, drawing inspiration from the linguistics literature, uses finite-state transducers constructed with a minimal amount of training data. While both models achieve near-perfect performance, the latter model can be trained using several orders of magnitude less data than the former, making it particularly useful for low-resource languages."
L16-1317,{TTS} for Low Resource Languages: A {B}angla Synthesizer,2016,21,7,5,0.689655,11005,alexander gutkin,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a text-to-speech (TTS) system designed for the dialect of Bengali spoken in Bangladesh. This work is part of an ongoing effort to address the needs of under-resourced languages. We propose a process for streamlining the bootstrapping of TTS systems for under-resourced languages. First, we use crowdsourcing to collect the data from multiple ordinary speakers, each speaker recording small amount of sentences. Second, we leverage an existing text normalization system for a related language (Hindi) to bootstrap a linguistic front-end for Bangla. Third, we employ statistical techniques to construct multi-speaker acoustic models using Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) and Hidden Markov Model (HMM) approaches. We then describe our experiments that show that the resulting TTS voices score well in terms of their perceived quality as measured by Mean Opinion Score (MOS) evaluations."
W15-1214,Similarity Measures for Quantifying Restrictive and Repetitive Behavior in Conversations of Autistic Children,2015,26,1,2,1,6163,masoud rouhizadeh,Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,0,"Restrictive and repetitive behavior (RRB) is a core symptom of autism spectrum disorder (ASD) and are manifest in language. Based on this, we expect children with autism to talk about fewer topics, and more repeatedly, during their conversations. We thus hypothesize a higher semantic overlap ratio between dialogue turns in children with ASD compared to those with typical development (TD). Participants of this study include children ages 48, 44 with TD and 25 with ASD without language impairment. We apply several semantic similarity metrics to the childrenxe2x80x99s dialogue turns in semi-structured conversations with examiners. We find that children with ASD have significantly more semantically overlapping turns than children with TD, across different turn intervals. These results support our hypothesis, and could provide a convenient and robust ASD-specific behavioral marker."
P15-2035,Measuring idiosyncratic interests in children with autism,2015,23,1,4,1,6163,masoud rouhizadeh,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,": A defining symptom of autism spectrum disorder (ASD) is the presence of restricted and repetitive activities and interests, which can surface in language as a perseverative focus on idiosyncratic topics. In this paper, we use semantic similarity measures to identify such idiosyncratic topics in narratives produced by children with and without ASD. We find that neurotypical children tend to use the same words and semantic concepts when retelling the same narrative, while children with ASD, even when producing accurate retellings, use different words and concepts relative not only to neurotypical children but also to other children with ASD. Our results indicate that children with ASD not only stray from the target topic but do so in idiosyncratic ways according to their own restricted interests."
W14-3206,Detecting linguistic idiosyncratic interests in autism using distributional semantic models,2014,-1,-1,4,1,6163,masoud rouhizadeh,Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,0,None
P14-2060,Hippocratic Abbreviation Expansion,2014,21,8,2,0,4293,brian roark,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Incorrect normalization of text can be particularly damaging for applications like text-to-speech synthesis (TTS) or typing auto-correction, where the resulting normalization is directly presented to the user, versus feeding downstream applications. In this paper, we focus on abbreviation expansion for TTS, which requires a xe2x80x9cdo no harmxe2x80x9d, high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations unexpanded. In the context of a largescale, real-world TTS scenario, we present methods for training classifiers to establish whether a particular expansion is apt. We achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the TTS system, together with a substantial reduction in incorrect expansions."
sproat-etal-2014-database,A Database for Measuring Linguistic Information Content,2014,3,2,1,1,6614,richard sproat,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Which languages convey the most information in a given amount of space? This is a question often asked of linguists, especially by engineers who often have some information theoretic measure of ÂinformationÂ in mind, but rarely define exactly how they would measure that information. The question is, in fact remarkably hard to answer, and many linguists consider it unanswerable. But it is a question that seems as if it ought to have an answer. If one had a database of close translations between a set of typologically diverse languages, with detailed marking of morphosyntactic and morphosemantic features, one could hope to quantify the differences between how these different languages convey information. Since no appropriate database exists we decided to construct one. The purpose of this paper is to present our work on the database, along with some preliminary results. We plan to release the dataset once complete."
J14-4002,Applications of Lexicographic Semirings to Problems in Speech and Language Processing,2014,39,3,1,1,6614,richard sproat,Computational Linguistics,0,"This paper explores lexicographic semirings and their application to problems in speech and language processing. Specifically, we present two instantiations of binary lexicographic semirings, one involving a pair of tropical weights, and the other a tropical weight paired with a novel string semiring we term the categorial semiring. The first of these is used to yield an exact encoding of backoff models with epsilon transitions. This lexicographic language model semiring allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection. The second of these lexicographic semirings is applied to the problem of extracting, from a lattice of word sequences tagged for part of speech, only the single best-scoring part of speech tagging for each word sequence. We do this by incorporating the tags as a categorial weight in the second component of a {Tropical, Categorial} lexicographic semiring, determinizing the resulting word lattice acceptor in that semiring, and then mapping the tags back as output labels of the word lattice transducer. We compare our approach to a competing method due to Povey et al. (2012)."
D13-1088,{R}ussian Stress Prediction using Maximum Entropy Ranking,2013,9,4,2,0,2189,keith hall,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the wordxe2x80x99s stem and sux. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for ecient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress."
W12-2903,Discourse-Based Modeling for {AAC},2012,28,3,2,0,8841,margaret mitchell,Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies,0,"This paper presents a method for an AAC system to predict a whole response given features of the previous utterance from the interlocutor. It uses a large corpus of scripted dialogs, computes a variety of lexical, syntactic and whole phrase features for the previous utterance, and predicts features that the response should have, using an entropy-based measure. We evaluate the system on a held-out portion of the corpus. We find that for about 3.5% of cases in the held-out corpus, we are able to predict a response, and among those, over half are either exact or at least reasonable substitutes for the actual response. We also present some results on keystroke savings. Finally we compare our approach to a state-of-the-art chatbot, and show (not surprisingly) that a system like ours, tuned for a particular style of conversation, outperforms one that is not.n n Predicting possible responses automatically by mining a corpus of dialogues is a novel contribution to the literature on whole utterance-based methods in AAC. Also useful, we believe, is our estimate that about 3.5--4.0% of utterances in dialogs are in principle predictable given previous context."
W12-2107,Robust kaomoji detection in {T}witter,2012,5,5,4,0,9577,steven bedrick,Proceedings of the Second Workshop on Language in Social Media,0,"In this paper, we look at the problem of robust detection of a very productive class of Asian style emoticons, known as facemarks or kaomoji. We demonstrate the frequency and productivity of these sequences in social media such as Twitter. Previous approaches to detection and analysis of kaomoji have placed limits on the range of phenomena that could be detected with their method, and have looked at largely monolingual evaluation sets (e.g., Japanese blogs). We find that these emoticons occur broadly in many languages, hence our approach is language agnostic. Rather than relying on regular expressions over a predefined set of likely tokens, we build weighted context-free grammars that reward graphical affinity and symmetry within whatever symbols are used to construct the emoticon."
P12-3011,The {O}pen{G}rm open-source finite-state grammar software libraries,2012,12,43,2,0,4293,brian roark,Proceedings of the {ACL} 2012 System Demonstrations,0,"In this paper, we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules into finite-state transducers, and for n-gram language modeling. The OpenGrm libraries use the OpenFst library to provide an efficient encoding of grammars and general algorithms for building, modifying and applying models."
C12-1042,Annotation Tools and Knowledge Representation for a Text-To-Scene System,2012,18,11,4,0,24826,bob coyne,Proceedings of {COLING} 2012,0,"Text-to-scene conversion requires knowledge about how actions and locations are expressed in language and realized in the world. To provide this knowlege, we are creating a lexical resource (VigNet) that extends FrameNet by creating a set of intermediate frames (vignettes) that bridge between the high-level semantics of FrameNet frames and a new set of low-level primitive graphical frames. Vignettes can be thought of as a link between function and form xe2x80x90 between what a scene means and what it looks like. In this paper, we describe the set of primitive graphical frames and the functional properties of 3D objects (affordances) we use in this decomposition. We examine the methods and tools we have developed to populate VigNet with a large number of action and location vignettes."
W11-2303,Towards technology-assisted co-construction with communication partners,2011,24,10,3,0,4293,brian roark,Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies,0,"In this paper, we examine the idea of technology-assisted co-construction, where the communication partner of an AAC user can make guesses about the intended messages, which are included in the user's word completion/prediction interface. We run some human trials to simulate this new interface concept, with subjects predicting words as the user's intended message is being generated in real time with specified typing speeds. Results indicate that people can provide substantial keystroke savings by providing word completion or prediction, but that the savings are not as high as n-gram language models. Interestingly, the language model and human predictions are complementary in certain key ways -- humans doing a better job in some circumstances on contextually salient nouns. We discuss implications of the enhanced co-construction interface for real-time message generation in AAC direct selection devices."
W11-0147,Collecting Semantic Data from {M}echanical {T}urk for a Lexical Knowledge Resource in a Text to Picture Generating System,2011,7,9,3,1,6163,masoud rouhizadeh,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"WordsEye is a system for automatically converting natural language text into 3D scenes representing the meaning of that text. At the core of WordsEye is the Scenario-Based Lexical Knowledge Resource (SBLR), a unified knowledge base and representational system for expressing lexical and real-world knowledge needed to depict scenes from text. To enrich a portion of the SBLR, we need to fill out some contextual information about its objects, including information about their typical parts, typical locations and typical objects located near them. This paper explores our proposed methodology to achieve this goal. First we try to collect some semantic information by using Amazon's Mechanical Turk (AMT). Then, we manually filter and classify the collected data and finally, we compare the manual results with the output of some automatic filtration techniques which use several WordNet similarity and corpus association measures."
P11-2001,Lexicographic Semirings for Exact Automata Encoding of Sequence Models,2011,11,11,2,0,4293,brian roark,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks. We prove that the semiring allows for exact encoding of backoff models with epsilon transitions. This allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection."
qian-etal-2010-python,A Python Toolkit for Universal Transliteration,2010,7,6,5,0,36922,ting qian,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe ScriptTranscriber, an open source toolkit for extracting transliterations in comparable corpora from languages written in different scripts. The system includes various methods for extracting potential terms of interest from raw text, for providing guesses on the pronunciations of terms, and for comparing two strings as possible transliterations using both phonetic and temporal measures. The system works with any script in the Unicode Basic Multilingual Plane and is easily extended to include new modules. Given comparable corpora, such as newswire text, in a pair of languages that use different scripts, ScriptTranscriber provides an easy way to mine transliterations from the comparable texts. This is particularly useful for underresourced languages, where training data for transliteration may be lacking, and where it is thus hard to train good transliterators. ScriptTranscriber provides an open source package that allows for ready incorporation of more sophisticated modules â e.g. a trained transliteration model for a particular language pair. ScriptTranscriber is available as part of the nltk contrib source tree at http://code.google.com/p/nltk/."
J10-4017,Commentary and Discussion: Reply to {R}ao et al. and Lee et al.,2010,11,2,1,1,6614,richard sproat,Computational Linguistics,0,"In the last issue of this journal, I presented a piece that called into question some of the techniques reported in two papers in high-profile journals that purported to provide statistical evidence for the linguistic status of some ancient symbol systems (Sproat 2010a). Not surprisingly, the authors of those two papers took issue with a number of my claims, and have requested the opportunity to respond. The two responses, taken together, are rather lengthy and as a result it is not possible, given limitations of space, for me to address each and every one of their criticisms. I will therefore focus on what I consider to be the most important objections."
J10-3012,"Last Words: Ancient Symbols, Computational Linguistics, and the Reviewing Practices of the General Science Journals",2010,15,15,1,1,6614,richard sproat,Computational Linguistics,0,"Few archaeological finds are as evocative as artifacts inscribed with symbols. Whenever an archaeologist finds a potsherd or a seal impression that seems to have symbols scratched or impressed on the surface, it is natural to want to xe2x80x9creadxe2x80x9d the symbols. And if the symbols come from an undeciphered or previously unknown symbol system it is common to ask what language the symbols supposedly represent and whether the system can be deciphered. Of course the first question that really should be asked is whether the symbols are in fact writing. A writing system, as linguists usually define it, is a symbol system that is used to represent language. Familiar examples are alphabets such as the Latin, Greek, Cyrillic or Hangul alphabets, alphasyllabaries such as Devanagari or Tamil, syllabaries such as Cherokee or Kana, and morphosyllabic systems like Chinese characters. But symbol systems that do not encode language abound: European heraldry, mathematical notation, labanotation (used to represent dance), and boy scout merit badges are all examples of symbol systems that represent things, but do not function as part of a system that represents language. Whether an unknown system is writing or not is a difficult question to answer. It can only be answered definitively in the affirmative if one can develop a verifiable decipherment into some language or languages. Statistical techniques have been used in decipherment for years, but these have always been used under the assumption that the system one is dealing with is writing, and the techniques are used to uncover patterns or regularities that might aid in the decipherment. Patterns of symbol distribution might suggest that a symbol system is not linguistic: for example, odd repetition patterns might make it seem that a symbol system is unlikely to be writing. But until recently nobody had argued that statistical techniques could be used to determine that a system is linguistic.1 It was therefore quite a surprise when, in April 2009, there appeared in Science a short article by Rajesh Rao of the University of Washington and colleagues at two"
W09-3505,Named Entity Transcription with Pair n-Gram Models,2009,7,10,2,0.48736,18149,martin jansche,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"We submitted results for each of the eight shared tasks. Except for Japanese name kanji restoration, which uses a noisy channel model, our Standard Run submissions were produced by generative long-range pair n-gram models, which we mostly augmented with publicly available data (either from LDC datasets or mined from Wikipedia) for the Non-Standard Runs."
P09-1013,Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples,2009,5,10,2,0,4926,suma bhat,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Empirical studies on corpora involve making measurements of several quantities for the purpose of comparing corpora, creating language models or to make generalizations about specific linguistic phenomena in a language. Quantities such as average word length are stable across sample sizes and hence can be reliably estimated from large enough samples. However, quantities such as vocabulary size change with sample size. Thus measurements based on a given sample will need to be extrapolated to obtain their estimates over larger unseen samples. In this work, we propose a novel nonparametric estimator of vocabulary size. Our main result is to show the statistical consistency of the estimator -- the first of its kind in the literature. Finally, we compare our proposal with the state of the art estimators (both parametric and nonparametric) on large standard corpora; apart from showing the favorable performance of our estimator, we also see that the classical Good-Turing estimator consistently underestimates the vocabulary size."
N09-4008,"Writing Systems, Transliteration and Decipherment",2009,0,5,2,0,10368,kevin knight,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts",0,"Nearly all of the core data that computational linguists deal with is in the form of text, which is to say that it consists of language data written (usually) in the standard writing system for the language in question. Yet surprisingly little is generally understood about how writing systems work. This tutorial will be divided into three parts. In the first part we discuss the history of writing and introduce a wide variety of writing systems, explaining their structure and how they encode language. We end this section with a brief review of how some of the properties of writing systems are handled in modern encoding systems, such as Unicode, and some of the continued pitfalls that can occur despite the best intentions of standardization. The second section of the tutorial will focus on the problem of transcription between scripts (often termed transliteration), and how this problem--which is important both for machine translation and named entity recognition--has been addressed. The third section is more theoretical and, at the same time we hope, more fun. We will discuss the problem of decipherment and how computational methods might be brought to bear on the problem of unlocking the mysteries of as yet undeciphered ancient scripts. We start with a brief review of three famous cases of decipherment. We then discuss how techniques that have been used in speech recognition and machine translation might be applied to the problem of decipherment. We end with a survey of the as-yet undeciphered ancient scripts and give some sense of the prospects of deciphering them given currently available data."
J08-4006,Book Review: Mathematical Linguistics by Andr{\\'a}s Kornai,2008,-1,-1,1,1,6614,richard sproat,Computational Linguistics,0,None
W07-1711,Multilingual Word Sense Discrimination: A Comparative Cross-Linguistic Study,2007,9,1,2,1,8356,alla rozovskaya,Proceedings of the Workshop on {B}alto-{S}lavonic Natural Language Processing,0,"We describe a study that evaluates an approach to Word Sense Discrimination on three languages with different linguistic structures, English, Hebrew, and Russian. The goal of the study is to determine whether there are significant performance differences for the languages and to identify language-specific problems. The algorithm is tested on semantically ambiguous words using data from Wikipedia, an online encyclopedia. We evaluate the induced clusters against sense clusters created manually. The results suggest a correlation between the algorithm's performance and morphological complexity of the language. In particular, we obtain FScores of 0.68, 0.66 and 0.61 for English, Hebrew, and Russian, respectively. Moreover, we perform an experiment on Russian, in which the context terms are lemmatized. The lemma-based approach significantly improves the results over the word-based approach, by increasing the FScore by 16%. This result demonstrates the importance of morphological analysis for the task for morphologically rich languages like Russian."
P07-1015,Multilingual Transliteration Using Feature based Phonetic Method,2007,14,32,3,0.705128,24182,suyoun yoon,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In this paper we investigate named entity transliteration based on a phonetic scoring method. The phonetic method is computed using phonetic features and carefully designed pseudo features. The proposed method is tested with four languages xe2x80x93 Arabic, Chinese, Hindi and Korean xe2x80x93 and one source language xe2x80x93 English, using comparable corpora. The proposed method is developed from the phonetic method originally proposed in Tao et al. (2006). In contrast to the phonetic method in Tao et al. (2006) constructed on the basis of pure linguistic knowledge, the method in this study is trained using the Winnow machine learning algorithm. There is salient improvement in Hindi and Arabic compared to the previous study. Moreover, we demonstrate that the method can also achieve comparable results, when it is trained on language data different from the target language. The method can be applied both with minimal data, and without target language data for various languages."
W06-1630,Unsupervised Named Entity Transliteration Using Temporal and Phonetic Correlation,2006,23,40,4,0,49765,tao tao,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we investigate unsupervised name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics --- and therefore share references to named entities --- but are not translations of each other. We present two distinct methods for transliteration, one approach using an unsupervised phonetic transliteration method, and the other using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We believe that the novelty of our approach lies in the phonetic-based scoring method, which is based on a combination of carefully crafted phonetic features, and empirical results from the pronunciation errors of second-language learners of English. Unlike previous approaches to transliteration, this method can in principle work with any pair of languages in the absence of a training dictionary, provided one has an estimate of the pronunciation of words in text."
P06-1010,Named Entity Transliteration with Comparable Corpora,2006,23,94,1,1,6614,richard sproat,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics --- and therefore share references to named entities --- but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step."
2006.bcs-1.1,Challenges in Processing Colloquial {A}rabic,2006,-1,-1,2,1,8356,alla rozovskaya,Proceedings of the International Conference on the Challenge of Arabic for NLP/MT,0,"Processing of Colloquial Arabic is a relatively new area of research, and a number of interesting challenges pertaining to spoken Arabic dialects arise. On the one hand, a whole continuum of Arabic dialects exists, with linguistic differences on phonological, morphological, syntactic, and lexical levels. On the other hand, there are inter-dialectal similarities that need be explored. Furthermore, due to scarcity of dialect-specific linguistic resources and availability of a wide range of resources for Modern Standard Arabic (MSA), it is desirable to explore the possibility of exploiting MSA tools when working on dialects. This paper describes challenges in processing of Colloquial Arabic in the context of language modeling for Automatic Speech Recognition. Using data from Egyptian Colloquial Arabic and MSA, we investigate the question of improving language modeling of Egyptian Arabic with MSA data and resources. As part of the project, we address the problem of linguistic variation between Egyptian Arabic and MSA. To account for differences between MSA and Colloquial Arabic, we experiment with the following techniques of data transformation: morphological simplification (stemming), lexical transductions, and syntactic transformations. While the best performing model remains the one built using only dialectal data, these techniques allow us to obtain an improvement over the baseline MSA model. More specifically, while the effect on perplexity of syntactic transformations is not very significant, stemming of the training and testing data improves the baseline perplexity of the MSA model trained on words by 51{\%}, and lexical transductions yield an 82{\%} perplexity reduction. Although the focus of the present work is on language modeling, we believe the findings of the study will be useful for researchers involved in other areas of processing Arabic dialects, such as parsing and machine translation."
H05-1073,Emotions from Text: Machine Learning for Text-based Emotion Prediction,2005,22,468,3,0,7076,cecilia alm,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a naive baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions."
N04-1017,Lattice-Based Search for Spoken Utterance Retrieval,2004,19,222,2,0,25524,murat saraclar,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"Recent work on spoken document retrieval has suggested that it is adequate to take the singlebest output of ASR, and perform text retrieval on this output. This is reasonable enough for the task of retrieving broadcast news stories, where word error rates are relatively low, and the stories are long enough to contain much redundancy. But it is patently not reasonable if onexe2x80x99s task is to retrieve a short snippet of speech in a domain where WERxe2x80x99s can be as high as 50%; such would be the situation with teleconference speech, where onexe2x80x99s task is to find if and when a participant uttered a certain phrase. In this paper we propose an indexing procedure for spoken utterance retrieval that works on lattices rather than just single-best text. We demonstrate that this procedure can improve F scores by over five points compared to singlebest retrieval on tasks with poor WER and low redundancy. The representation is flexible so that we can represent both word lattices, as well as phone lattices, the latter being important for improving performance when searching for phrases containing OOV words."
W03-1719,The First International {C}hinese Word Segmentation Bakeoff,2003,6,171,1,1,6614,richard sproat,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future."
C02-2026,Creating a Finite-State Parser with Application Semantics,2002,7,10,5,0,1354,owen rambow,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"Parsli is a finite-state (FS) parser which can be tailored to the lexicon, syntax, and semantics of a particular application using a hand-editable declarative lexicon. The lexicon is defined in terms of a lexicalized Tree Adjoining Grammar, which is subsequently mapped to a FS representation. This approach gives the application designer better and easier control over the natural language understanding component than using an off-the-shelf parser. We present results using Parsli on an application that creates 3D-images from typed input."
J01-3005,Book Reviews: Prosody: Theory and Experiment. Studies presented to Gosta Bruce,2001,9,0,2,1,28288,chilin shih,Computational Linguistics,0,None
P96-1029,Compilation of Weighted Finite-State Transducers from Decision Trees,1996,23,96,1,1,6614,richard sproat,34th Annual Meeting of the Association for Computational Linguistics,1,"We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996)."
P96-1031,An Efficient Compiler for Weighted Rewrite Rules,1996,0,0,2,0,39166,mehryar mohri,34th Annual Meeting of the Association for Computational Linguistics,1,"Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this."
O96-2002,Issues in Text-to-Speech Conversion for {M}andarin,1996,-1,-1,2,1,28288,chilin shih,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 1, Number 1, August 1996",0,None
J96-3004,A Stochastic Finite-State Word-Segmentation Algorithm for {C}hinese,1996,30,290,1,1,6614,richard sproat,Computational Linguistics,0,"The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words. For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation. In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to reconstruct the word-boundary information. In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer. The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and---since the primary intended application of this model is to text-to-speech synthesis---provides pronunciations for these words. We evaluate the system's performance by comparing its segmentation judgments with the judgements of a pool of human segmenters, and the system is shown to perform quite well."
J96-2001,Estimating Lexical Priors for Low-Frequency Morphologically Ambiguous Forms,1996,13,28,2,0,11343,harald baayen,Computational Linguistics,0,"Given a form that is previously unseen in a sufficiently large training corpus, and that is morphologically n-ways ambiguous (serves n different lexical functions) what is the best estimator for the lexical prior probabilities for the various functions of the form? We argue that the best estimator is provided by computing the relative frequencies of the various functions among the hapax legomena---the forms that occur exactly once in a corpus; in particular, a hapax-based estimator is better than one based on the proportion of the various functions among words of all frequency ranges. As we shall argue, this is because when one computes an overall measure, one is including high-frequency words, and high-frequency words tend to have idiosyncratic properties that are not at all representative of the much larger mass of (productively formed) low-frequency words. This result has potential importance for various kinds of applications requiring lexical disambiguation, including, in particular, stochastic taggers. This is especially true when some initial hand-tagging of a corpus is required: for predicting lexical priors for very low-frequency morphologically ambiguous types (most of which would not occur in any given corpus), one should concentrate on tagging a good representative sample of the hapax legomena, rather than extensively tagging words of all frequency ranges."
P94-1010,A Stochastic Finite-State Word-Segmentation Algorithm for {C}hinese,1994,27,55,1,1,6614,richard sproat,32nd Annual Meeting of the Association for Computational Linguistics,1,"We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and productively derived words, and providing pronunciations for these words; the method incorporates a class-based model in its treatment of personal names. We also evaluate the system's performance, taking into account the fact that people often do not agree on a single segmentation."
J94-3012,Commentary on {B}ird and {K}lein,1994,1,0,1,1,6614,richard sproat,Computational Linguistics,0,"Bird and Klein show us how various phonological constructs--feature geometry, the prosodic hierarchy, well-formedness constraints on strings of segments, templatic/autosegmental phonology, morphology-phonology interactions, and vowel-zero alternat i o n s m a y be treated in a rigorous fashion using the the formal resources of HPSG. To the incautious critic (or the skeptical phonologist) it may seem that they have given us merely a new notation in which to express conventional phonological analyses, such as Tranel's analysis of French schwa epenthesis or Goldsmith's textbook examples of Sierra Miwok. A central principle of Bird and Klein's approach that takes it beyond notation is Phonological Compositionality and the related concept of monotonicity, mentioned almost in passing in Section 3.3. Among the consequences of monotonicity are: (1) feature values may not be altered; (2) segments or other structural material may not be removed (i.e. delinking and restructuring, such as resyllabification, are prohibited); and (3) constraints are not extrinsically ordered; in fact, extrinsic rule ordering is inexpressible. An accommodation with orthodox generative phonology with regard to (1) may be found by employing underspecification, translating feature-changing rules into feature-filling constraints. Objections to extrinsic ordering have been raised at various times before, and many phonologists would like to be rid of it, so Bird and Klein's proposals regarding (3) are welcome. Many phonologists find (2) very unpalatable, however, despite the fact that Bird and Klein's example of French schwa insertion is also potentially applicable to some putative cases of deletion. A deletion rule operative in Welsh mutation, which removes / g / from the lexical representation of gardd in some environments, but leaves it present in the citation form, could be treated in HPSG phonology as consonant-zero alternation on a par with French schwa. This alternation is only treated as deletion because it is the noncitation form that lacks the initial consonant. Other apparent cases of deletion are more awkward for declarative approaches. In English trisyllabic shortening (e.g. profane --* profanity) and -ic shortening (e.g. t6ne t6nicity), orthodox analysis deletes a vowel slot and association lines incident to it. To treat these as instances of vowel-zero alternation would appear to require the representation of the stem to be sensitive to the presence of a very particular set of suffixes (-ic, -ity, etc.). HPSG phonology might either analyze these using different CV templates, like Sierra Miwok allomorphy, or reject the bisegmental analysis of phonological length, by treating shortening as the addition of a shortness feature (e.g. trisyllabic-shortening(V) --* I-long]). I do not expect either of these proposals to be popular. In any case, reconstructing morphophonological rules such as the above in declarative style will not satisfy some critics, for whom any suggestion of languagespecific rules is anathema."
H94-1050,Weighted Rational Transductions and their Application to Human Language Processing,1994,17,74,3,0,28562,fernando pereira,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing. This generality allows us to represent uniformly such information sources as pronunciation dictionaries, language models and lattices, and to use uniform algorithms for building decoding stages and for optimizing and combining them. In particular, a single automata join algorithm can be used either to combine information sources such as a pronunciation dictionary and a context-dependency model during the construction of a decoder, or dynamically during the operation of the decoder. Applications to speech recognition and to Chinese text segmentation will be discussed."
J91-2005,Book Reviews: {PC}-{KIMMO}: A Two-Level Processor for Morphological Analysis,1991,-1,-1,1,1,6614,richard sproat,Computational Linguistics,0,None
O90-1010,An application of statistical optimization with dynamic programming to phonemic-input-to-character conversion for {C}hinese,1990,0,12,1,1,6614,richard sproat,Proceedings of Rocling {III} Computational Linguistics Conference {III},0,None
P87-1010,Constituent-Based Morphological Parsing: A New Approach to the Problem of Word-Recognition.,1987,9,4,1,1,6614,richard sproat,25th Annual Meeting of the Association for Computational Linguistics,1,"We present a model of morphological processing which directly encodes prosodic constituency, a notion which is clearly crucial in many widespread morphological processes. The model has been implemented for the Australian language Warlpiri and has been successfully interfaced with a syntactic parser for that language (Brunson, 1986). We contrast our approach with approaches to morphological parsing in the KIMMO framework."
P87-1020,Toward Treating {E}nglish Nominals Correctly,1987,12,8,1,1,6614,richard sproat,25th Annual Meeting of the Association for Computational Linguistics,1,We describe a program for assigning correct stress contours to nominals in English. It makes use of idiosyncratic knowledge about the stress behavior of various nominal types and general knowledge about English stress rules. We have also investigated the related issue of parsing complex nominals in English. The importance of this work and related research to the problem of text-to-speech is discussed.
