2003.mtsummit-papers.42,E03-1020,0,0.0260934,"onary of Contemporary English (LDOCE), or from lexical databases such as WordNet. These dictionaries and databases are constructed manually by lexicologists and linguists. However, only little effort has been made to derive the sets of possible senses automatically from corpora. In a recent paper, Pantel & Lin (2002) write: “To the best of our knowledge, there has been no previous work in automatic word sense discovery from text.” Nevertheless, an overview on some literature has been given in a previous paper (Rapp, 2003a). We would like to supplement this overview by mentioning Neill (2002), Dorow & Widdows (2003), and Rapp (2003b). The approach to word sense induction that we suggest here is best described by an example: Let us look at the ambiguous word bank with its money and river senses. We observe that the contexts of bank contain words representative for both meanings, e.g. institute, interest, and account for the money sense and sand, water, and beach for the river sense. We now assume that the meanings of an ambiguous word are best described by those of its significant associates whose features complement each other in an optimal way. In mathematical terms, we would expect that the co-occurren"
2003.mtsummit-papers.42,J93-1003,0,0.24229,"e corpus. By storing it as a sparse matrix, it was feasible to include all of the approximately 375 000 lemmas occurring in the BNC. 3.3 Computation of association strength Although semantic similarities can be successfully computed based on raw word co-occurrence counts, the results can be improved when the observed co-occurrence-frequencies are transformed by some function that reduces the effects of different word frequencies. For example, by applying a significance test that compares the observed cooccurrence counts with the expected co-occurrence counts (e.g. the log-likelihood test; see Dunning, 1993) significant word pairs are strengthened and incidental word pairs are weakened. Other measures applied successfully include TF/IDF and mutual information (Manning & Schütze, 1999). In the remainder of this paper, we refer to co-occurrence matrices that have been transformed by such a function as association matrices. However, in order to further improve similarity estimates, in this study we are applying a singular value decomposition (SVD) to our co-occurrence matrices (see section 3.5). To our surprise, our experiments clearly showed that the log-likelihood test, which was the transformatio"
2003.mtsummit-papers.42,C92-3145,0,0.0384167,"nvironment of native speakers. We therefore chose to use the British National Corpus (BNC), a 100-million-word corpus of written and spoken language that was compiled with the intention of providing a representative sample of British English (Burnard & Aston, 1998). Since function words were not considered important for our analysis of word semantics, to save disk space and processing time we decided to remove them from the text. This was done on the basis of a list of approximately 200 English function words. We also decided to lemmatize the corpus using the lexicon of full forms provided by Karp et al. (1992). This not only improves the sparse data problem but also significantly reduces the size of the co-occurrence matrix to be computed. Since most word forms are unambiguous concerning their possible lemmas, we only conducted a partial lemmatization that does not take the context of a word into account and thus leaves the relatively few words with several possible lemmas unchanged. This way we avoided the need for disambiguation which would have anticipated the purpose of this research. 2.2 Evaluation data In order to quantitatively evaluate our results in sense induction we took the list of 12 a"
2003.mtsummit-papers.42,P95-1026,0,0.490452,"s the sparse data problem but also significantly reduces the size of the co-occurrence matrix to be computed. Since most word forms are unambiguous concerning their possible lemmas, we only conducted a partial lemmatization that does not take the context of a word into account and thus leaves the relatively few words with several possible lemmas unchanged. This way we avoided the need for disambiguation which would have anticipated the purpose of this research. 2.2 Evaluation data In order to quantitatively evaluate our results in sense induction we took the list of 12 ambiguous words used by Yarowsky (1995). Each of these words is considered to have two main senses, and for each sense he provides a word characteristic of that sense. Table 5 (first and second column) shows the list of words together with their sense descriptors. Another kind of test data was used to evaluate our method for computing second-order word similarities. It comprises similarity estimates obtained from human subjects. This data was kindly provided by Thomas K. Landauer, who had taken it from the synonym portion of the Test of English as a Foreign Language (TOEFL). Originally, the data came, along with normative data, fro"
2020.bucc-1.2,2020.bucc-1.11,0,0.0293513,"Missing"
2020.bucc-1.2,W15-3411,1,0.812659,"r of 2 billion, Russian about 3 billion running words (Sharoff et al., 2017). The compressed sizes of the Wikipedia corpora are: English: 3.6 GB, Spanish: 0.9 GB, Chinese: 0.4 GB. They are in a one-line per document format. The first tabseparated field in each line contains metadata, the second field contains the text. Paragraph boundaries are marked with HTML tags. As cleaning up the original Wikipedia dump files is not trivial, occasionally there can be some noise in the form of not fully cleaned HTML and Javascript fragments. Details of the cleanup and preparation procedure can be found in Sharoff et al. (2015). Expressions of interest to participate in the shared task Release of shared task training sets Release of shared task test sets Submission of shared task results 3.2 Table 3: Time schedule. 2 ukWaC deWaC esWiki deWiki frWaC deWaC en deWaC ukWaC Table 4: Language pairs supported and corpora (WaCky or Wikipedia) to be used in the closed track. Table 2: Checklist for participants (abbreviated). Any time Corpora Table 4 lists the corpora to be used for the language pairs supported in the closed track. Due to their free availability for several languages and their size, for the shared task we use"
2020.bucc-1.2,Q17-1010,0,0.151641,"Missing"
2020.bucc-1.2,J17-2001,0,0.0752755,"ir competition between systems. This was accomplished by providing corpora and bilingual datasets for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, and by comparing the results using a common evaluation framework. For the shared task we provided corpora as well as training and test data. However, as we anticipated that these corpora and datasets may not suit all needs, we divided the shared task into two tracks: Quite a few research groups have been working on this problem using a wide variety of approaches. There are comprehensive studies such as Irvine & Callison-Burch (2017) and also overview papers at least in part discussing the topic like Jakubina & Langlais (2016), Rapp et al. (2016), Sharoff et al. (2013). 1 Target (French) bébé poupon bain lit plumard commodité médecin docteur aigle montagne nerveux travail  https://comparable.limsi.fr/bucc2020/bucc2020-task.html 6 In the closed track, participants were required to only use the data provided by the organizers. In this way equal conditions were ensured and, as the outcome of 3. this track, the systems could be compared and ranked according to the quality of their results. In the open track, participants wer"
2020.bucc-1.2,2020.bucc-1.9,0,0.0902575,"Missing"
2020.bucc-1.2,W05-0809,0,0.0808028,"llel data. It has been suggested that it may be possible to extract multilingual lexical knowledge from comparable rather than from parallel corpora (see e.g. Sharoff et al., 2013). From a theoretical perspective, this suggestion may lead to advances in understanding human second language acquisition. From a practical perspective, as comparable corpora are available in much larger quantities than parallel corpora, this approach might help in relieving the data acquisition bottleneck which tends to be especially severe when dealing with language pairs involving low resource languages (see e.g. Martin et al., 2005). Table 1: Sample word translations from English to French. In the shared task a similar tab-separated format was used. However, as up to now there was no standard way to measure the performance of the systems, the published results are not comparable and the pros and cons of the various approaches are not clear. A well-established practical task to approach this topic is bilingual lexicon extraction from comparable corpora, which is in the focus of this shared task. Typically, its aim is to extract word translations such as exemplified in Table 1 from comparable corpora, where a given source"
2020.bucc-1.2,W03-0301,0,0.111131,"r ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition is the results of a number of systems which provide surprisingly good solutions to an ambitious problem. Keywords: bilingual dictionary, lexicon induction, comparable corpora 1. Introduction Source (English) baby baby bath bed bed convenience doctor doctor eagle mountain nervous work In the framework of machine translation, the extraction of bilingual dictionaries from parallel corpora has been conducted very successfully (see e.g. Mihalcea & Pedersen, 2003). But on the other hand, human second language acquisition appears not to be based on parallel data. This means that there must be a way of acquiring and relating lexical knowledge across two or more languages without the use of parallel data. It has been suggested that it may be possible to extract multilingual lexical knowledge from comparable rather than from parallel corpora (see e.g. Sharoff et al., 2013). From a theoretical perspective, this suggestion may lead to advances in understanding human second language acquisition. From a practical perspective, as comparable corpora are availabl"
2020.eamt-1.66,J90-2002,0,0.806621,"implicit in the underlying text corpora or dictionaries. Occasionally it has been argued that it may be difficult to advance MT quality to the next level as long as the systems do not make more explicit use of semantic knowledge. SEBAMAT aims to evaluate three approaches incorporating such knowledge into MT. 1 The current state of the art in MT SEBAMAT aims to show ways on how to improve the translations produced by current MT systems. For several decades the rule-based approach was dominant (Arnold et al., 1994) which focused on grammatical well-formedness. In the statistical approach (SMT, Brown et al. 1990), linguistic rules were replaced by statistical patterns as automatically extracted from large monolingual and parallel text corpora. Recently, the dominance of SMT has been contested by neural MT (NMT), which almost consistently generates better results. Although NMT represents the current state of the art, technical issues and problems have been raised, including NMT’s inferior performance to SMT for limited training data, reduced portability across domains, and sensitivity to semantic divergence in the training data (Koehn & Knowles (2017), Carpuat et al. (2017). Sennrich & Zang (2019) impr"
2020.eamt-1.66,W17-3209,0,0.0176944,"the statistical approach (SMT, Brown et al. 1990), linguistic rules were replaced by statistical patterns as automatically extracted from large monolingual and parallel text corpora. Recently, the dominance of SMT has been contested by neural MT (NMT), which almost consistently generates better results. Although NMT represents the current state of the art, technical issues and problems have been raised, including NMT’s inferior performance to SMT for limited training data, reduced portability across domains, and sensitivity to semantic divergence in the training data (Koehn & Knowles (2017), Carpuat et al. (2017). Sennrich & Zang (2019) improve NMT for small corpora, but marked improvements are gained using larger corpora. However, despite considerable advances, the quality of current MT systems is still limited, and the likely reason is that the algorithms used are of a mechanical nature, employing statistical rules or deep learning architectures, without a human-like understanding of the texts to be translated. Amongst others, Kevin Knight pointed out that MT systems do not sufficiently take into account semantic considerations such as who did what to whom, when where and why. This is also true for"
2020.eamt-1.66,P14-1111,0,0.0259549,"Missing"
2020.eamt-1.66,W17-3204,0,0.0121665,"ical well-formedness. In the statistical approach (SMT, Brown et al. 1990), linguistic rules were replaced by statistical patterns as automatically extracted from large monolingual and parallel text corpora. Recently, the dominance of SMT has been contested by neural MT (NMT), which almost consistently generates better results. Although NMT represents the current state of the art, technical issues and problems have been raised, including NMT’s inferior performance to SMT for limited training data, reduced portability across domains, and sensitivity to semantic divergence in the training data (Koehn & Knowles (2017), Carpuat et al. (2017). Sennrich & Zang (2019) improve NMT for small corpora, but marked improvements are gained using larger corpora. However, despite considerable advances, the quality of current MT systems is still limited, and the likely reason is that the algorithms used are of a mechanical nature, employing statistical rules or deep learning architectures, without a human-like understanding of the texts to be translated. Amongst others, Kevin Knight pointed out that MT systems do not sufficiently take into account semantic considerations such as who did what to whom, when where and why."
2020.eamt-1.66,P99-1067,1,0.6406,"n the concept of multi-stimulus association. The underlying hypothesis is that the meaning of a short sentence or phrase can be characterized by its associations. That is, the content words in this sentence/phrase are used as multiword stimuli, and a meaning vector is computed. The relation between two sentences can be computed by comparing their corresponding meaning vectors using a vector similarity metric. To translate a source language phrase, we first compute its meaning vector. Presupposing the 1 https://framenet.icsi.berkeley.edu/fndrupal/ existence of a basic dictionary, in analogy to Rapp (1999) we can translate this meaning vector into the target language. Assuming that we already know the meaning vectors of a very large number of target language phrases, we select the target language meaning vector which is most similar to the source language meaning vector. The corresponding target language phrase is considered the translation of the source phrase. This is an unsupervised vector space approach requiring meaning vectors for huge numbers of phrases, but no parallel corpora nor training. 5 Project details SEBAMAT is a 2-year Marie Curie individual fellowship funded by the European Co"
2020.eamt-1.66,P19-1021,0,0.0312982,"Missing"
C02-1007,P99-1008,0,0.0398167,"Missing"
C02-1007,J93-1003,0,0.0508731,"eful for determining the type of relationship between words (e.g., synonymy, antonymy, meronymy, hyponymy, etc., see Berland & Charniak, 1999). Although this is not within the scope of this paper, it is very relevant for related tasks, for example, the automatic generation of thesauri. 3 Syntagmatic Associations Syntagmatic associations are words that frequently occur together. Therefore, an obvious approach to extract them from corpora is to look for word pairs whose co-occurrence is significantly larger than chance. To test for significance, the standard chisquare test can be used. However, Dunning (1993) pointed out that for the purpose of corpus statistics, where the sparseness of data is an important issue, it is better to use the log-likelihood ratio. It would then be assumed that the strongest syntagmatic association to a word would be that other word that gets the highest log-likelihood score. Please note that this method is computationally far more efficient than the computation of paradigmatic associations. For the computation of the syntagmatic associations to a stimulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associat"
C02-1007,W93-0113,0,0.143495,"Missing"
C02-1007,P98-2127,0,0.0149129,"c similarities between words can be computed by conducting simple vector comparisons. To determine the words most similar to a given word, its co-occurrence vector is compared to the co-occurrence vectors of all other words using one of the standard similarity measures, for example, the cosine coefficient. Those words that obtain the best values are considered to be most similar. Practical implementations of algorithms based on this principle have led to excellent results as documented in papers by Ruge (1992), Grefenstette (1994), Agarwal (1995), Landauer & Dumais (1997), Schütze (1997), and Lin (1998). 2.1 Human Data In this section we relate the results of our version of such an algorithm to similarity estimates obtained by human subjects. Fortunately, we did not need to conduct our own experiment to obtain the human’s similarity estimates. Instead, such data was kindly provided by Thomas K. Landauer, who had taken it from the synonym portion of the Test of English as a Foreign Language (TOEFL). Originally, the data came, along with normative data, from the Educational Testing Service (Landauer & Dumais 1997). The TOEFL is an obligatory test for foreign students who would like to study at"
C02-1007,P99-1067,1,0.697996,"ritten and spoken language that was compiled with the intention of providing a representative sample of British English. Since this corpus is rather large, to save disk space and processing time we decided to remove all function words from the text. This was done on the basis of a list of approximately 200 English function words. We also decided to lemmatize the corpus as well as the test data. This not only reduces the sparse-data problem but also significantly reduces the size of the co-occurrence matrix to be computed. More details on these two steps of corpus preprocessing can be found in Rapp (1999). 2.3 Co-occurrence Counting For counting word co-occurrences, as in most other studies a fixed window size is chosen and it is determined how often each pair of words occurs within a text window of this size. Choosing a window size usually means a trade-off between two parameters: specificity versus the sparse-data problem. The smaller the window, the stronger the associative relation between the words inside the window, but the more severe the sparse data problem (see figure 1 in section 3.2). In our case, with ±1 word, the window size looks rather small. However, this can be justified since"
C02-1007,J93-1007,0,0.0404159,"paradigmatic associations. For the computation of the syntagmatic associations to a stimulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associations the vector of the stimulus word has to be compared to the vectors of all other words in the vocabulary. The computation of syntagmatic associations is said to be of first-order type, whereas the computation of paradigmatic associations is of second-order type. Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations (Smadja, 1993) and in cognitive psychology for the simulation of associative learning (Wettler & Rapp, 1993). 3.1 Association Norms As we did with the paradigmatic associations, we would like to compare the results of our simulation to human performance. However, it is difficult to say what kind of experiment should be conducted to obtain human data. As with the paradigmatic associations, we decided not to conduct our own experiment but to use the Edinburgh Associative Thesaurus (EAT), a large collection of association norms, as compiled by Kiss et al. (1973). Kiss presented lists of stimulus words to human"
C02-1007,W93-0310,1,0.655148,"imulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associations the vector of the stimulus word has to be compared to the vectors of all other words in the vocabulary. The computation of syntagmatic associations is said to be of first-order type, whereas the computation of paradigmatic associations is of second-order type. Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations (Smadja, 1993) and in cognitive psychology for the simulation of associative learning (Wettler & Rapp, 1993). 3.1 Association Norms As we did with the paradigmatic associations, we would like to compare the results of our simulation to human performance. However, it is difficult to say what kind of experiment should be conducted to obtain human data. As with the paradigmatic associations, we decided not to conduct our own experiment but to use the Edinburgh Associative Thesaurus (EAT), a large collection of association norms, as compiled by Kiss et al. (1973). Kiss presented lists of stimulus words to human subjects and asked them to write after each word the first word that the stimulus word made t"
C02-1007,C98-2122,0,\N,Missing
C14-1200,J90-1003,0,0.57659,"as collected from human subjects and word co-occurrences as observed in a corpus, and our hypothesis is that the strength of this relationship can be used as a measure of corpus representativeness. A corpus leading to simulated associations akin to the ones collected from humans is likely to be a good surrogate for everyday language, although – similarly to what we said about word frequencies – word cooccurrence counts constitute only one of many properties of a corpus. For extracting word associations from corpora, in the literature many algorithms were described (e.g. Wettler & Rapp, 1989; Church & Hanks, 1990; Wettler et al., 2005). In analogy, we used the following procedure: For all words with a BNC corpus frequency of 50 or higher we computed the cooccurrence vectors. That is, each vector contains the number of co-occurrences of the stimulus word with all other co-occurring words. It counts as a co-occurrence if two words appear together within a distance of at most ten words, i.e. a text window of ±10 words around the stimulus word is considered. Hereby the exact distance within the window is not taken into account. In a further step an association measure was applied to the co-occurrence vect"
C14-1200,J93-1003,0,0.496535,"Missing"
C14-1200,1996.amta-1.36,0,0.853784,"Missing"
C14-1200,rapp-2014-using-word,1,0.803295,"words. In terms of human language intuitions, these three statistical properties relate to word familiarities, word associations, and word relatedness. What we suggest is to extract data relating to these three types of statistical properties from a corpus and to compare it to the respective experimental data as obtained from test persons. The higher the average agreement, the more representative the corpus should be for the language environment of the test persons.1 Related work has been conducted by Brisbaert & New (2009), which is mentioned in section 2.1, and in our own previous studies (Rapp, 2014a and Rapp, 2014c), of which the current work is an extension. A nice summary of how to measure corpus representativeness through psycholinguistic measures is provided in a presentation by Francom & Ussishkin (2011). Gries (2010), though in a slightly different context, emphasizes the need of external validation: “For corpus linguists, that means that our measures must be validated against corpus-external evidence because, strictly speaking, as long as we corpus linguists do not show that our dispersions and adjusted frequencies correspond to something outside of our corpora, we have failed to"
C14-1200,rapp-2014-corpus,1,0.826085,"words. In terms of human language intuitions, these three statistical properties relate to word familiarities, word associations, and word relatedness. What we suggest is to extract data relating to these three types of statistical properties from a corpus and to compare it to the respective experimental data as obtained from test persons. The higher the average agreement, the more representative the corpus should be for the language environment of the test persons.1 Related work has been conducted by Brisbaert & New (2009), which is mentioned in section 2.1, and in our own previous studies (Rapp, 2014a and Rapp, 2014c), of which the current work is an extension. A nice summary of how to measure corpus representativeness through psycholinguistic measures is provided in a presentation by Francom & Ussishkin (2011). Gries (2010), though in a slightly different context, emphasizes the need of external validation: “For corpus linguists, that means that our measures must be validated against corpus-external evidence because, strictly speaking, as long as we corpus linguists do not show that our dispersions and adjusted frequencies correspond to something outside of our corpora, we have failed to"
C14-1200,temnikova-etal-2014-sublanguage,0,0.0320502,"balance many decisions concerning the corpus design have to be made. Biber (1993) mentions, among other things, that it has to be decided for what target population a corpus is meant to be representative, that estimates concerning the quantities of various text types are required, and that decisions with regard to the number of individual text samples and their sizes have to be made. However, there is no easy and well established way to verify the success of these measures. Current suggestions include, for example, to consider a corpus as representative if it is not dominated by sublanguage (Temnikova et al., 2014), or to more or less give up on the concept of representativeness and to concentrate on considering the suitability of a corpus for particular tasks. Saldanha (2009) comes to the conclusion that “The problem with making representativeness the defining characteristic of a corpus is that it is very difficult to evaluate.ˮ Our goal here is to make an attempt to measure corpus representativeness in a standardized way, thereby avoiding to observe test persons’ average language input as this would not be very practical. Our starting point is that a representative corpus should reflect as well as pos"
C14-1200,P02-1040,0,\N,Missing
C98-2118,C92-3145,0,0.0770518,"Missing"
C98-2118,J94-2001,0,0.109216,"Missing"
C98-2118,A88-1019,0,\N,Missing
C98-2118,P97-1032,0,\N,Missing
E06-2018,P94-1013,0,0.08049,", in this study we try to give at least a partial answer. Our starting point is the observation that ambiguous words can usually be disambiguated by their context, and that certain context words can be seen as indicators of certain senses. For example, context words such as finger and arm are typical of the hand meaning of palm, whereas coconut and oil are typical of its tree meaning. The essence behind many algorithms for word sense disambiguation is to implicitly or explicitly classify all possible context words into groups relating to one or another sense. This can be done in a supervised (Yarowsky, 1994), a semi-supervised (Yarowsky, 1995) or a fully unsupervised way (Pantel & Lin, 2002). However, the classification can only work if the statistical clues are clear enough and if there are not too many exceptions. In terms of word co-occurrence statistics, we can say that within the local contexts of an ambiguous word, context words typical of the same sense should have high co-occurrence counts, whereas context words associated with different senses should have cooccurrence counts that are considerably lower. Although the relative success of previous disambiguation systems (e.g. Yarowsky, 1995"
E06-2018,P95-1026,0,0.204829,"ast a partial answer. Our starting point is the observation that ambiguous words can usually be disambiguated by their context, and that certain context words can be seen as indicators of certain senses. For example, context words such as finger and arm are typical of the hand meaning of palm, whereas coconut and oil are typical of its tree meaning. The essence behind many algorithms for word sense disambiguation is to implicitly or explicitly classify all possible context words into groups relating to one or another sense. This can be done in a supervised (Yarowsky, 1994), a semi-supervised (Yarowsky, 1995) or a fully unsupervised way (Pantel & Lin, 2002). However, the classification can only work if the statistical clues are clear enough and if there are not too many exceptions. In terms of word co-occurrence statistics, we can say that within the local contexts of an ambiguous word, context words typical of the same sense should have high co-occurrence counts, whereas context words associated with different senses should have cooccurrence counts that are considerably lower. Although the relative success of previous disambiguation systems (e.g. Yarowsky, 1995) suggests that this should be the c"
enguix-etal-2014-graph,J90-1003,0,\N,Missing
enguix-etal-2014-graph,W09-2905,0,\N,Missing
enguix-etal-2014-graph,D09-1066,0,\N,Missing
enguix-etal-2014-graph,W14-0509,1,\N,Missing
enguix-etal-2014-graph,P06-2084,0,\N,Missing
L18-1605,S16-1081,0,0.0234795,"ome past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, plagiarism detection can take advantage of differences in style between the original target text and the translated text, and of intrinsic properties of ‘translationese’. This is not the case in our task, where all sentences are expected to be original. Cross-language text similarity as in SemEval 2016 (Agirre et al., 2016) assesses the level of semantic similarity of pairs of sentences on a given scale. It is also close to our task. Nevertheless, it has been proposed with already paired sentences instead of large monolingual corpora, thus removing the sentence spotting stage. Bilingual document alignment in a large Web collection has been proposed in WMT 2016 (Buck and Koehn, 2016). However, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly availa"
L18-1605,W17-2508,0,0.0208688,"by human review of samples of resulting sequences of two sentences. • A configuration for indexing and search in the Solr search engine, typically based on a tokenizer, stop words, and possibly more language components. These datasets were used in the BUCC 2017 and 2018 Shared Tasks (Zweigenbaum et al., 2017; Zweigenbaum et al., 2018). Participants were taskeed with detecting in a bilingual pair of corpora the inserted parallel sentences. Three of the four language pairs were addressed by the participants in 2017: French, German, and Chinese, with a maximum F-score of 0.84 on German-English (Azpeitia et al., 2017) (see Table 3). All four language pairs were addressed in 2018, with improved F-scores topping at 0.86 for German-English again. • Constraints on the range of sentence lengths. We report here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . I"
L18-1605,W16-2347,0,0.026976,"n the original target text and the translated text, and of intrinsic properties of ‘translationese’. This is not the case in our task, where all sentences are expected to be original. Cross-language text similarity as in SemEval 2016 (Agirre et al., 2016) assesses the level of semantic similarity of pairs of sentences on a given scale. It is also close to our task. Nevertheless, it has been proposed with already paired sentences instead of large monolingual corpora, thus removing the sentence spotting stage. Bilingual document alignment in a large Web collection has been proposed in WMT 2016 (Buck and Koehn, 2016). However, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly available dataset that would make it possible to compare methods that extract parallel sentences from comparable corpora. This paper describes the principles according to which we designed such a corpus, their implementation, the resulting corpus and a first use of that corpus in a shared task. This corpus was built in the context of the BUCC 2017 Shared Task described"
L18-1605,W03-1722,0,0.0180004,"eport here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . Independently of these methods, several guidelines have been proposed for human annotation of Chinese tokens, including the Chinese Penn Treebank and the Peking University standard (Duan et al., 2003). This results in tokens with shorter or larger spans depending on the guideline, for instance 有线 (cable) 电视 (television) according to Peking University vs. 有线电视 (cable television) according to Chinese Penn Treebank. Chinese tokenizers display the same variety in their choices of token span length; some, such as Stanford or jieba,8 leave it to the user to choose which strategy to apply (full=short, default=large, search=multiple solutions). We attempted to avoid these considerations by working directly with characters. This was initially motivated by the technical choice of Solr (v6.4.0), whos"
L18-1605,N04-1034,0,0.166203,"Missing"
L18-1605,W15-3411,1,0.839088,"ora with known parallel sentence pairs. We therefore needed to prevent as much as possible naturally occurring parallel sentence pairs from remaining in our monolingual corpora. The strategy we adopted in this purpose was to desynchronize our comparable corpora. Since we started from Wikipedia articles in two languages, we knew that interlinked articles would be highly likely to contain such parallel sentences: this is indeed a property that is often desired by past work on parallel sentence extraction. This is also how our previous shared task on detection of comparable texts has been setup (Sharoff et al., 2015): the gold standard was based on the iwiki links. In contrast to such work, we built pairs of monolingual corpora which never contained two interlinked Wikipedia articles. This was also in line with our desideratum not to include meta-information on the sentences, such as being found in two interlinked articles. The main drawback in doing so is that the most comparable pairs of documents for a given language pair are removed from the pairs of corpora we built: only one out of two interlinked pages can be kept in one of our corpora. This reduces the comparability of our datasets. However, the t"
L18-1605,N10-1063,0,0.0345766,"they display much more variety and are normally original texts rather than translations. They hold much promise therefore as a complement to parallel texts for machine translation and other applications. One way in which comparable corpora have been used to help machine translation is by spotting parallel sentences that occur naturally in these corpora, and using these sentence pairs to extend parallel corpora (Munteanu et al., 2004). This has motivated research into methods that aim to perform this task, such as (Utiyama and Isahara, 2003; Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). This task is usually called Parallel Sentence Extraction from Comparable Corpora. It is however difficult to compare earlier work and assess progress because of the absence of a shared dataset with gold standard annotations. Some past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, plagiarism detection can take advantage of differences in style between the o"
L18-1605,I05-3027,0,0.0204923,"of 0.84 on German-English (Azpeitia et al., 2017) (see Table 3). All four language pairs were addressed in 2018, with improved F-scores topping at 0.86 for German-English again. • Constraints on the range of sentence lengths. We report here how we included Chinese data in the present corpus. The Chinese writing system does not separate words with spaces6 . This raises issues for tokenization that have consequences on our dataset construction pipeline. Various methods have been proposed to tokenize Chinese, including Conditional Random Fields classifiers in the Stanford Chinese Word Segmenter (Tseng et al., 2005) and in the Chinese Mecab7 . Independently of these methods, several guidelines have been proposed for human annotation of Chinese tokens, including the Chinese Penn Treebank and the Peking University standard (Duan et al., 2003). This results in tokens with shorter or larger spans depending on the guideline, for instance 有线 (cable) 电视 (television) according to Peking University vs. 有线电视 (cable television) according to Chinese Penn Treebank. Chinese tokenizers display the same variety in their choices of token span length; some, such as Stanford or jieba,8 leave it to the user to choose which"
L18-1605,P03-1010,0,0.0953731,"r criteria such as domain, genre, time period. In contrast to parallel corpora, they display much more variety and are normally original texts rather than translations. They hold much promise therefore as a complement to parallel texts for machine translation and other applications. One way in which comparable corpora have been used to help machine translation is by spotting parallel sentences that occur naturally in these corpora, and using these sentence pairs to extend parallel corpora (Munteanu et al., 2004). This has motivated research into methods that aim to perform this task, such as (Utiyama and Isahara, 2003; Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). This task is usually called Parallel Sentence Extraction from Comparable Corpora. It is however difficult to compare earlier work and assess progress because of the absence of a shared dataset with gold standard annotations. Some past shared tasks addressed related objectives. Cross-language plagiarism detection in PAN (Potthast et al., 2012) aims to spot text that has been translated into a target language and reused in (inserted into) text in that target language. It is therefore quite close to our task. However, pla"
L18-1605,W17-2512,1,0.775299,"wever, on the one hand it addressed documents instead of sentences; and on the other hand, it included meta-information in the form of document URLs, a property that we want to avoid. This highlights the need for a publicly available dataset that would make it possible to compare methods that extract parallel sentences from comparable corpora. This paper describes the principles according to which we designed such a corpus, their implementation, the resulting corpus and a first use of that corpus in a shared task. This corpus was built in the context of the BUCC 2017 Shared Task described in (Zweigenbaum et al., 2017). The present paper provides more detail about our motivation and design criteria, about the rationale we followed to implement these design criteria, and about the processing of the Chinese part of the corpus. 2. A Dataset for Parallel Sentence Extraction from Comparable Corpora 2.1. Desiderata for a Dataset for the Task We aimed to build a bilingual corpus to measure progress on the identification of parallel sentences in monolingual corpora. This led us to the following desiderata and design choices. No metadata. We wish to focus on the cross-language comparison of sentence contents. Instea"
P04-3026,C02-1007,1,0.926141,"ing a clustering algorithm to the row vectors (words) of the resulting matrix. This works well since it is a strength of SVD to reduce the effects of sampling errors and to close gaps in the data. arm beach coconut finger hand shoulder tree c1 • • • • c2 • • • c3 • • • c4 c5 • • • • c6 • • vides depends on its association strength to the ambiguous word, in each matrix we removed all words that are not among the top 30 first order associations to the ambiguous word. These top 30 associations were computed fully automatically based on the log-likelihood ratio. We used the procedure described in Rapp (2002), with the only modification being the multiplication of the loglikelihood values with a triangular function that depends on the logarithm of a word’s frequency. This way preference is given to words that are in the middle of the frequency range. Figures 1 to 3 are based on the association lists for the words palm and poach. Given that our term/context matrices are very sparse with each of their individual entries seeming somewhat arbitrary, it is necessary to detect the regularities in the patterns. For this purpose we applied the SVD to each of the matrices, thereby reducing their number of"
P04-3026,2003.mtsummit-papers.42,1,0.848775,"The main difficulty with this approach, namely the problem of data sparseness, could be minimized by looking at only the three main dimensions of the context matrices. 1 Introduction The topic of this paper is word sense induction, that is the automatic discovery of the possible senses of a word. A related problem is word sense disambiguation: Here the senses are assumed to be known and the task is to choose the correct one when given an ambiguous word in context. Whereas until recently the focus of research had been on sense disambiguation, papers like Pantel & Lin (2002), Neill (2002), and Rapp (2003) give evidence that sense induction now also attracts attention. In the approach by Pantel & Lin (2002), all words occurring in a parsed corpus are clustered on the basis of the distances of their co-occurrence vectors. This is called global clustering. Since (by looking at differential vectors) their algorithm allows a word to belong to more than one cluster, each cluster a word is assigned to can be considered as one of its senses. A problem that we see with this approach is that it allows only as many senses as clusters, thereby limiting the granularity of the meaning space. This problem is"
P04-3026,P95-1026,0,0.0793151,". After some testing with various similarity functions and linkage types, we finally opted for the cosine coefficient and single linkage which is the combination that apparently gave the best results. axes: grid/tools crane: bird/machine duty: tax/obligation palm: tree/hand poach: steal/boil space: volume/outer Table 1: Term/context matrix for the word palm. 3 Algorithm As in previous work (Rapp, 2002), our computations are based on a partially lemmatized version of the British National Corpus (BNC) which has the function words removed. Starting from the list of 12 ambiguous words provided by Yarowsky (1995) which is shown in table 2, we created a concordance for each word, with the lines in the concordances each relating to a context window of ±20 words. From the concordances we computed 12 term/context-matrices (analogous to table 1) whose binary entries indicate if a word occurs in a particular context or not. Assuming that the amount of information that a context word probass: fish/music drug: medicine/narcotic motion: legal/physical plant: living/factory sake: benefit/drink tank: vehicle/container Table 2: Ambiguous words and their senses. 4 Results Before we proceed to a quantitative evalua"
P05-3020,J92-4003,0,0.050174,"always considered classifying words as one of their core tasks, and as a consequence accurate lexicons providing such information are readily available for many languages. Nevertheless, deriving word classes automatically is an interesting intellectual challenge with relevance to cognitive science. Also, advantages of the automatic systems are that they should be more objective and can provide precise information on the likelihood distribution for each of a word’s parts of speech, an aspect that is useful for statistical machine translation. The pioneering work on class based n-gram models by Brown et al. (1992) was motivated by such considerations. In contrast, Schütze (1993) by applying a neural network approach put the emphasis on the cognitive side. More recent work includes Clark (2003) who combines distributional and morphological information, and Freitag (2004) who uses a hidden Marcov model in combination with co-clustering. Most studies use abstract statistical measures such as perplexity or the F-measure for evaluation. This is good for quantitative comparisons, but makes it difficult to check if the results agree with human intuitions. In this paper we use a straightforward approach for ev"
P05-3020,E03-1009,0,0.0897995,"ving word classes automatically is an interesting intellectual challenge with relevance to cognitive science. Also, advantages of the automatic systems are that they should be more objective and can provide precise information on the likelihood distribution for each of a word’s parts of speech, an aspect that is useful for statistical machine translation. The pioneering work on class based n-gram models by Brown et al. (1992) was motivated by such considerations. In contrast, Schütze (1993) by applying a neural network approach put the emphasis on the cognitive side. More recent work includes Clark (2003) who combines distributional and morphological information, and Freitag (2004) who uses a hidden Marcov model in combination with co-clustering. Most studies use abstract statistical measures such as perplexity or the F-measure for evaluation. This is good for quantitative comparisons, but makes it difficult to check if the results agree with human intuitions. In this paper we use a straightforward approach for evaluation. It involves checking if the automatically generated word classes agree with the word classes known from grammar books, and whether the class assignments for each word are co"
P05-3020,C04-1052,0,0.362074,"h relevance to cognitive science. Also, advantages of the automatic systems are that they should be more objective and can provide precise information on the likelihood distribution for each of a word’s parts of speech, an aspect that is useful for statistical machine translation. The pioneering work on class based n-gram models by Brown et al. (1992) was motivated by such considerations. In contrast, Schütze (1993) by applying a neural network approach put the emphasis on the cognitive side. More recent work includes Clark (2003) who combines distributional and morphological information, and Freitag (2004) who uses a hidden Marcov model in combination with co-clustering. Most studies use abstract statistical measures such as perplexity or the F-measure for evaluation. This is good for quantitative comparisons, but makes it difficult to check if the results agree with human intuitions. In this paper we use a straightforward approach for evaluation. It involves checking if the automatically generated word classes agree with the word classes known from grammar books, and whether the class assignments for each word are correct. 2 Approach In principle, word classification can be based on a number o"
P05-3020,P04-3026,1,0.796844,"the differential vector to the most appropriate other cluster. This process can be repeated until the length of the differential vector falls below a threshold or, alternatively, the agreement with any of the centroids becomes too low. This way an ambiguous word is assigned to several parts of speech, starting from the most common and proceeding to the least common. Figure 1 illustrates this process. 1 An alternative to relying on this fortunate but somewhat unsatisfactory effect would be not to use global co-occurrence vectors but local ones, as successfully proposed in word sense induction (Rapp, 2004). This means that every occurrence of a word obtains a separate row vector in table 1. The problem with the resulting extremely sparse matrix is that most vectors are either orthogonal to each other or duplicates of some other vector, with the consequence that the dimensionality reduction that is indispensable for such matrices does not lead to sensible results. This problem is not as severe in word sense induction where larger context windows are considered. 78 The procedure that we described so far works in theory but not well in practice. The problem with it is that the matrix is so sparse"
P05-3020,P93-1034,0,0.641814,"a consequence accurate lexicons providing such information are readily available for many languages. Nevertheless, deriving word classes automatically is an interesting intellectual challenge with relevance to cognitive science. Also, advantages of the automatic systems are that they should be more objective and can provide precise information on the likelihood distribution for each of a word’s parts of speech, an aspect that is useful for statistical machine translation. The pioneering work on class based n-gram models by Brown et al. (1992) was motivated by such considerations. In contrast, Schütze (1993) by applying a neural network approach put the emphasis on the cognitive side. More recent work includes Clark (2003) who combines distributional and morphological information, and Freitag (2004) who uses a hidden Marcov model in combination with co-clustering. Most studies use abstract statistical measures such as perplexity or the F-measure for evaluation. This is good for quantitative comparisons, but makes it difficult to check if the results agree with human intuitions. In this paper we use a straightforward approach for evaluation. It involves checking if the automatically generated word"
P07-2014,P05-3020,1,0.719744,"dy is to automatically induce a system of word classes that is in agreement with human intuition, and then to assign all possible parts of speech to a given ambiguous or unambiguous word. Two of the pioneering studies concerning this as yet not satisfactorily solved problem are Finch (1993) and Schütze (1993) who classify words according to their context vectors as derived from a corpus. More recent studies try to solve the problem of POS induction by combining distributional and morphological information (Clark, 2003; Freitag, 2004), or by clustering words and projecting them to POS vectors (Rapp, 2005). Whereas all these studies are based on global co-occurrence vectors who reflect the overall behavior of a word in a corpus, i.e. who in the case of syntactically ambiguous words are based on POSmixtures, in this paper we raise the question if it is really necessary to use an approach based on mixtures or if there is some way to avoid the mixing beforehand. For this purpose, we suggest to look at local contexts instead of global co-occurrence vectors. As can be seen from human performance, in almost all cases the local context of a syntactically ambiguous word is sufficient to disambiguate it"
P07-2014,P93-1034,0,0.0893866,"this position, and by clustering the neighbor pairs on the basis of their middle words as observed in a large corpus. The results obtained in this way are evaluated by comparing them to the part-of-speech distributions as found in the manually tagged Brown corpus. 1 Introduction The purpose of this study is to automatically induce a system of word classes that is in agreement with human intuition, and then to assign all possible parts of speech to a given ambiguous or unambiguous word. Two of the pioneering studies concerning this as yet not satisfactorily solved problem are Finch (1993) and Schütze (1993) who classify words according to their context vectors as derived from a corpus. More recent studies try to solve the problem of POS induction by combining distributional and morphological information (Clark, 2003; Freitag, 2004), or by clustering words and projecting them to POS vectors (Rapp, 2005). Whereas all these studies are based on global co-occurrence vectors who reflect the overall behavior of a word in a corpus, i.e. who in the case of syntactically ambiguous words are based on POSmixtures, in this paper we raise the question if it is really necessary to use an approach based on mix"
P07-2014,E03-1009,0,0.0324792,"ons as found in the manually tagged Brown corpus. 1 Introduction The purpose of this study is to automatically induce a system of word classes that is in agreement with human intuition, and then to assign all possible parts of speech to a given ambiguous or unambiguous word. Two of the pioneering studies concerning this as yet not satisfactorily solved problem are Finch (1993) and Schütze (1993) who classify words according to their context vectors as derived from a corpus. More recent studies try to solve the problem of POS induction by combining distributional and morphological information (Clark, 2003; Freitag, 2004), or by clustering words and projecting them to POS vectors (Rapp, 2005). Whereas all these studies are based on global co-occurrence vectors who reflect the overall behavior of a word in a corpus, i.e. who in the case of syntactically ambiguous words are based on POSmixtures, in this paper we raise the question if it is really necessary to use an approach based on mixtures or if there is some way to avoid the mixing beforehand. For this purpose, we suggest to look at local contexts instead of global co-occurrence vectors. As can be seen from human performance, in almost all ca"
P07-2014,C04-1052,0,0.0211477,"in the manually tagged Brown corpus. 1 Introduction The purpose of this study is to automatically induce a system of word classes that is in agreement with human intuition, and then to assign all possible parts of speech to a given ambiguous or unambiguous word. Two of the pioneering studies concerning this as yet not satisfactorily solved problem are Finch (1993) and Schütze (1993) who classify words according to their context vectors as derived from a corpus. More recent studies try to solve the problem of POS induction by combining distributional and morphological information (Clark, 2003; Freitag, 2004), or by clustering words and projecting them to POS vectors (Rapp, 2005). Whereas all these studies are based on global co-occurrence vectors who reflect the overall behavior of a word in a corpus, i.e. who in the case of syntactically ambiguous words are based on POSmixtures, in this paper we raise the question if it is really necessary to use an approach based on mixtures or if there is some way to avoid the mixing beforehand. For this purpose, we suggest to look at local contexts instead of global co-occurrence vectors. As can be seen from human performance, in almost all cases the local co"
P09-2034,W08-0312,0,0.109999,"sequences between both translations. It could be shown that such methods, of which BLEU (Papineni et al., 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al., 2002; Coughlin, 2003; Koehn & Monz, 2006). Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al., 2006). However, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al., 2008). Papineni et al. (2002) describe this as follows: “BLEU’s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.” Although in many scenarios the above mentioned drawbacks may not be a major problem, it is nevertheless desirable to overcome them. This is what we attempt in this paper by introducing the back-translation score. It is based on the assumption that the MT system considered c"
P09-2034,W08-0309,0,0.0887654,"translations. It could be shown that such methods, of which BLEU (Papineni et al., 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al., 2002; Coughlin, 2003; Koehn & Monz, 2006). Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al., 2006). However, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al., 2008). Papineni et al. (2002) describe this as follows: “BLEU’s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.” Although in many scenarios the above mentioned drawbacks may not be a major problem, it is nevertheless desirable to overcome them. This is what we attempt in this paper by introducing the back-translation score. It is based on the assumption that the MT system considered can translate a language pair i"
P09-2034,E06-1032,0,0.0392882,"nslation with a reference translation produced by humans. The comparison is done by determining the number of matching word sequences between both translations. It could be shown that such methods, of which BLEU (Papineni et al., 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al., 2002; Coughlin, 2003; Koehn & Monz, 2006). Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al., 2006). However, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al., 2008). Papineni et al. (2002) describe this as follows: “BLEU’s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.” Although in many scenarios the above mentioned drawbacks may not be a major problem, it is nevertheless desirable to overcome them. This is what we attempt i"
P09-2034,2003.mtsummit-papers.9,0,0.018085,"Introduction The manual evaluation of the results of machine translation systems requires considerable time and effort. For this reason fast and inexpensive automatic methods were developed. They are based on the comparison of a machine translation with a reference translation produced by humans. The comparison is done by determining the number of matching word sequences between both translations. It could be shown that such methods, of which BLEU (Papineni et al., 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al., 2002; Coughlin, 2003; Koehn & Monz, 2006). Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al., 2006). However, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al., 2008). Papineni et al. (2002) describe this as follows: “BLEU’s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather t"
P09-2034,I05-2014,0,0.110703,"Missing"
P09-2034,2005.mtsummit-papers.11,0,0.0243421,"LEU could be used, our experiments show that a modified version which we call OrthoBLEU is better suited for this purpose as it can deal with compounds and inflexional variants in a more appropriate way. Its operation is based on finding matches of character- rather than word-sequences. It resembles algorithms used in translation memory search for locating orthographically similar sentences. The results that we obtain in this work refute to some extend the common belief that backtranslation (sometimes also called round-trip translation) is not a suitable means for MT evaluation (Somers, 2005; Koehn, 2005). This belief seems to be largely based on the obvious observation that the back-translation score is highest for a trivial translation system that does nothing and simply leaves all source words in place. On the other hand, according to Somers (2005) “until now no one as far as we know has published results demonstrating this” (i.e. that back-translation is not useful for MT evaluation). We would like to add that so far the inappropriateness of back-translation has only been shown by comparisons with other automatic metrics (Somers 2005; Koehn, 2005), which are also 133 Proceedings of the ACL"
P09-2034,W06-3114,0,0.13432,"e manual evaluation of the results of machine translation systems requires considerable time and effort. For this reason fast and inexpensive automatic methods were developed. They are based on the comparison of a machine translation with a reference translation produced by humans. The comparison is done by determining the number of matching word sequences between both translations. It could be shown that such methods, of which BLEU (Papineni et al., 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al., 2002; Coughlin, 2003; Koehn & Monz, 2006). Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al., 2006). However, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al., 2008). Papineni et al. (2002) describe this as follows: “BLEU’s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to div"
P09-2034,P02-1040,0,0.0816644,"k-translation scores with human judgments, it could be shown that the backtranslation score gives an improved performance at the sentence level. 1 Introduction The manual evaluation of the results of machine translation systems requires considerable time and effort. For this reason fast and inexpensive automatic methods were developed. They are based on the comparison of a machine translation with a reference translation produced by humans. The comparison is done by determining the number of matching word sequences between both translations. It could be shown that such methods, of which BLEU (Papineni et al., 2002) is the most common, can deliver evaluation results that show a high agreement with human judgments (Papineni et al., 2002; Coughlin, 2003; Koehn & Monz, 2006). Disadvantages of BLEU and related methods are that a human reference translation is required, and that the results are reliable only at corpus level, i.e. when computed over many sentence pairs (see e.g. Callison-Burch et al., 2006). However, at the sentence level, due to data sparseness the results tend to be unsatisfactory (Agarwal & Lavie, 2008; Callison-Burch et al., 2008). Papineni et al. (2002) describe this as follows: “BLEU’s s"
P09-2034,U05-1019,0,0.645185,"s comparison BLEU could be used, our experiments show that a modified version which we call OrthoBLEU is better suited for this purpose as it can deal with compounds and inflexional variants in a more appropriate way. Its operation is based on finding matches of character- rather than word-sequences. It resembles algorithms used in translation memory search for locating orthographically similar sentences. The results that we obtain in this work refute to some extend the common belief that backtranslation (sometimes also called round-trip translation) is not a suitable means for MT evaluation (Somers, 2005; Koehn, 2005). This belief seems to be largely based on the obvious observation that the back-translation score is highest for a trivial translation system that does nothing and simply leaves all source words in place. On the other hand, according to Somers (2005) “until now no one as far as we know has published results demonstrating this” (i.e. that back-translation is not useful for MT evaluation). We would like to add that so far the inappropriateness of back-translation has only been shown by comparisons with other automatic metrics (Somers 2005; Koehn, 2005), which are also 133 Proceedi"
P95-1050,J90-2002,0,0.0828085,"ment allow the automatic identification of word translations from paxalhl texts. This study suggests that the identification of word translations should also be possible with non-paxMlel and even unrelated texts. The m e t h o d proposed is based on the assumption t h a t there is a correlation between the patterns of word cooccurrences in texts of different languages. 1 Introduction In a number of recent studies it has been shown that word translations can be automatically derived from the statistical distribution of words in bilingual paxallel texts (e. g. Catizone, Russell & Warwick, 1989; Brown et al., 1990; Dagan, Church & Gale, 1993; Kay & Rbscheisen, 1993). Most of the proposed algorithms first conduct an alignment of sentences, i. e. those palxs of sentences axe located that are translations of each other. In a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences. The results achieved with these algorithms have been found useful for the compilation of dictionaries, for checking the consistency of terminological usage in translations, and for assisting the terminological work of translators and interpreters. However, despite serious eff"
P95-1050,J93-1001,0,0.021114,"Rbscheisen, 1993). Most of the proposed algorithms first conduct an alignment of sentences, i. e. those palxs of sentences axe located that are translations of each other. In a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences. The results achieved with these algorithms have been found useful for the compilation of dictionaries, for checking the consistency of terminological usage in translations, and for assisting the terminological work of translators and interpreters. However, despite serious efforts in the compilation of corpora (Church & Mercer, 1993; Armstrong & Thompson, 1995) the availability of a large enough paxallel corpus in a specific field and for a given pair of languages will always be the exception, not the rule. Since the acquisition of non-paxallel texts is usually much easier, it would be desirable to have a program that can determine the translations of words from comparable or even unrelated texts. 2 Approach It is assumed that there is a correlation between the co-occurrences of words which are translations 320 of each other. If - for example - in a text of one language two words A and B co-occur more often than expected"
P95-1050,W93-0301,0,0.141694,"Missing"
P95-1050,J93-1006,0,\N,Missing
P98-2123,C92-3145,0,0.075729,"Missing"
P98-2123,J94-2001,0,0.111206,"Missing"
P98-2123,P97-1032,0,0.0369349,"Missing"
P98-2123,A88-1019,0,\N,Missing
P99-1067,J93-1006,0,0.00968159,"Missing"
P99-1067,P98-1117,0,0.0359745,"Missing"
P99-1067,P98-2127,0,0.0976608,"ivilian walk love drink think wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rappl) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schtitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 525 1993), so we did not try to include these methods in our system. Neve"
P99-1067,P95-1050,1,0.794264,"cross languages. Nevertheless, this clue is applicable in the case of comparable texts, although with a lower reliability than for parallel texts. However, in the case of unrelated texts, its usefulness may be near zero. The third clue is generally limited to the identification of word pairs with similar spelling. For all other pairs, it is usually used in combination with the first clue. Since the first clue does not work with non-parallel texts, the third clue is useless for the identification of the majority of pairs. For unrelated languages, it is not applicable anyway. In this situation, Rapp (1995) proposed using a clue different from the three mentioned above: His co-occurrence c l u e is based on the assumption that there is a correlation between cooccurrence patterns in different languages. For example, if the words teacher and school cooccur more often than expected by chance in a corpus of English, then the German translations of teacher and school, Lehrer and Schule, should also co-occur more often than expected in a corpus of German. In a feasibility study he showed that this assumption actually holds for the language pair English/German even in the case of unrelated texts. When"
P99-1067,P93-1034,0,0.0303412,"Missing"
P99-1067,W93-0310,1,0.520584,"Missing"
P99-1067,J90-2002,0,0.457373,"Missing"
P99-1067,P89-1010,0,0.165004,"method is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages. However, as Rapp (1995) proposed, this correlation may be strengthened by not using the co-occurrence counts directly, but association strengths between words instead. The idea is to eliminate word-frequency effects and to emphasize significant word pairs by comparing their 522 observed co-occurrence counts with their expected co-occurrence counts. In the past, for this purpose a number of measures have been proposed. They were based on mutual information (Church & Hanks, 1989), conditional probabilities (Rapp, 1996), or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio (Dunning, 1993). For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square. In preliminary experiments it also led to slightly better results than the conditional probability measure. Results based on mutual information or co-occurrence counts were significantly worse. For efficient computation of the log-likelihood ratio we used the following formula: 2"
P99-1067,J93-1003,0,0.262808,"995) proposed, this correlation may be strengthened by not using the co-occurrence counts directly, but association strengths between words instead. The idea is to eliminate word-frequency effects and to emphasize significant word pairs by comparing their 522 observed co-occurrence counts with their expected co-occurrence counts. In the past, for this purpose a number of measures have been proposed. They were based on mutual information (Church & Hanks, 1989), conditional probabilities (Rapp, 1996), or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio (Dunning, 1993). For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square. In preliminary experiments it also led to slightly better results than the conditional probability measure. Results based on mutual information or co-occurrence counts were significantly worse. For efficient computation of the log-likelihood ratio we used the following formula: 2 - 2 log ~ = kiiN ~ ki~ log c~Rj i,j~{l,2} kilN -- kl2N = kll log c-~-+kl2 log c, R2 • k21N + k21 log ~ -- k22 N + g22 log c2R2 where C 1 =kll +k12 C 2"
P99-1067,W95-0114,0,0.857596,"rpora, second-best with comparable corpora, and somewhat worse with unrelated corpora. In all three cases, the problem of robustness - as observed when 520 applying the word-order clue to parallel corpor a - is not severe. Transpositions of text segments have virtually no negative effect, and omissions or insertions are not critical. However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required. After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung & Yee, 1998; Fung & McKeown, 1997). By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995). The method described here - although developed independently of Fung's w o r k - goes in the same direction. Conceptually, it is a trivial case of Rapp's matrix permutation method. By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons. Th"
P99-1067,W97-0119,0,0.942113,"atrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton & McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones & Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: Simulation Procedure The results reported in the next section were obtained using the following procedure: 1. Based on the word co-occurrences in the German corpus, for each of the 100 German test words its association vector was computed. In these vectors, all entries belonging to words not found in the English part of the base lexi"
P99-1067,P98-1069,0,0.857356,"problem of robustness - as observed when 520 applying the word-order clue to parallel corpor a - is not severe. Transpositions of text segments have virtually no negative effect, and omissions or insertions are not critical. However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required. After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung & Yee, 1998; Fung & McKeown, 1997). By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995). The method described here - although developed independently of Fung's w o r k - goes in the same direction. Conceptually, it is a trivial case of Rapp's matrix permutation method. By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons. The main contribution of this paper is to describe a practical implementation based on the co-occurrence clue that yie"
P99-1067,J93-1004,0,0.598347,"Missing"
P99-1067,W93-0113,0,0.0537296,"Missing"
P99-1067,J90-1003,0,\N,Missing
P99-1067,C98-1066,0,\N,Missing
P99-1067,C98-2122,0,\N,Missing
P99-1067,C98-1113,0,\N,Missing
rapp-2002-part,C90-3044,0,\N,Missing
rapp-2002-part,A92-1018,0,\N,Missing
rapp-2002-part,J94-2001,0,\N,Missing
rapp-2002-part,C94-1027,0,\N,Missing
rapp-2002-part,P97-1032,0,\N,Missing
rapp-2002-part,P98-2127,0,\N,Missing
rapp-2002-part,C98-2122,0,\N,Missing
rapp-2004-freely,C92-3145,0,\N,Missing
rapp-2004-freely,C02-1007,1,\N,Missing
rapp-2004-freely,N03-1032,0,\N,Missing
rapp-2004-freely,P98-2127,0,\N,Missing
rapp-2004-freely,C98-2122,0,\N,Missing
rapp-2004-utilizing,E03-1020,0,\N,Missing
rapp-2004-utilizing,C02-1007,1,\N,Missing
rapp-2014-corpus,J90-1003,0,\N,Missing
rapp-2014-corpus,W08-1914,1,\N,Missing
rapp-2014-corpus,P99-1067,1,\N,Missing
rapp-2014-using-word,J90-1003,0,\N,Missing
rapp-2014-using-word,P02-1040,0,\N,Missing
rapp-etal-2012-identifying,C10-2070,0,\N,Missing
rapp-etal-2012-identifying,sharoff-etal-2008-designing,1,\N,Missing
rapp-etal-2012-identifying,W00-0901,0,\N,Missing
rapp-etal-2012-identifying,P99-1067,1,\N,Missing
rapp-etal-2012-identifying,J05-4003,0,\N,Missing
rapp-etal-2012-identifying,A00-1031,0,\N,Missing
rapp-etal-2012-identifying,P09-1017,0,\N,Missing
rapp-vide-2006-example,rapp-2004-freely,1,\N,Missing
rapp-vide-2006-example,J93-1004,0,\N,Missing
rapp-vide-2006-example,C90-3044,0,\N,Missing
rapp-vide-2006-example,P98-1069,0,\N,Missing
rapp-vide-2006-example,C98-1066,0,\N,Missing
rapp-vide-2006-example,J90-2002,0,\N,Missing
rapp-vide-2006-example,P02-1040,0,\N,Missing
rapp-vide-2006-example,P99-1067,1,\N,Missing
rapp-vide-2006-example,P04-3026,1,\N,Missing
rapp-vide-2006-example,J03-1002,0,\N,Missing
W08-1914,J90-1003,0,0.609391,"of association by contiguity implies the following two phases: 1) 2) Learning phase: When perceiving language, strong associative connections are developed between words that frequently occur in close temporal succession. Retrieval phase: These associations determine the words that come to mind during generation. Only words that are strongly interconnected or have strong associations to external stimuli can be uttered or written down. Pre-supposing the validity of the law of association, it should be possible to derive free word associations from the distribution of words in texts. Following Church & Hanks (1990), Rapp (2004), and Wettler et al. (2005) this actually seems to be successful. The recent simulation algorithms generate results which largely agree with the free word associations as found in the association norms. An example is shown in Table 1, where the observed and the simulated responses to the stimulus word cold are compared. 102 Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 102–109 Manchester, August 2008 OBSERVED RESPONSE hot ice warm water freeze wet feet freezing nose room sneeze sore winter NUMBER OF SUBJECTS 34 10 7 5 3 3 2 2 2"
W08-1914,J93-1003,0,0.0365089,"econd word is a function word, a window size of plus and minus six words after removal of the function words roughly corresponds to a window size of plus and minus 12 words without such pre-processing. This is a window size that corresponds with what had been found appropriate for the computation of associations in other studies (e.g. Rapp, 2004). As the co-occurrence counts largely depend on overall word frequency, some association measure needs to be applied to eliminate this undesired influence. Many previous studies have shown that the log-likelihood ratio is well suited for this purpose (Dunning, 1993). It successfully eliminates word-frequency effects and emphasizes significant word pairs by comparing their observed co-occurrence counts with their expected co-occurrence counts. It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998). To compute the associations to pairs of stimulus words, it would in principle be possible to consider text positions where both stimulus words occur together, and to count the cooccu"
W08-1914,P98-2123,1,0.817765,"Missing"
W08-1914,P99-1067,1,0.929804,"3 891.6 RANK GUARDIAN 70 44 17 87 34 115 16 494 64 119 2783 1419 1437 2335 19 279 1163 572 803 8 20 413 9 2753 --2393 1932 2773 5216 2 2 922.3 RANK WIKIPEDIA 338 355 4 4 5 384 324 185 38 412 1070 925 86 2078 757 711 51 2 492 1 143 3 13 393 --1387 90 2680 2347 1 2 520.2 Table 2: Crossword puzzle definitions and the computed ranks of their solutions based on three corpora. (‘---’ means that a solution does not occur in a corpus (not taken into account when computing average ranks). 107 tions from monolingual English and German corpora, i.e. from corpora that are not translations of each other (Rapp, 1999). This is a rather difficult task. As our textual basis, for German we use the FAZ corpus as described above, with exactly the same pre-processing. For English we use a similarly sized corpus of the newspaper “The Guardian”, with analogous pre-processing. We apply a two-stage procedure to compute the translation of a source language word: First, by considering the log-likelihood ratios, its strongest source language associations are determined and translated to the target language using a small pocket dictionary. Hereby, associations that are missing in the dictionary are discarded, and of the"
W08-1914,C98-2118,1,\N,Missing
W10-3908,P07-1018,0,0.0170797,"ranslated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have limited scope otherwise. For this reason we will not further discuss it here. 2 51 valents should also co-occur more frequently than expected in a corpus of language B. A great number of variants of this approach has been proposed, e.g. emphasizing aspects of corpus selection or expanding it to collocations or short phrases (Babych et al., 2007). What is common to these studies is that they consider the source and the target language as two distinct semantic spaces, without any links at the beginning. Therefore, in order to connect the two, a base dictionary is required, and the purpose of the system is to expand this base dictionary. Building a dictionary from scratch is not possible this way or at least computationally unfeasible (see Rapp, 1995). Whether the assumption of two completely distinct semantic spaces is realistic remains an open issue. Are separate lexical networks really a reasonable model for the processing of differe"
W10-3908,J93-1003,0,0.36481,"items. The stop words had been manually selected from a corpusderived list of high frequency words. In the resulting corpus associations between words need to be identified, something that is usually done on the basis of co-occurrences. In Note that the results of both directions may be combined. This is something we leave for future work. 3 order to count the co-occurrences between pairs of words, a text window comprising the ten words preceding and following a given foreign word is considered. On the resulting co-occurrence counts a standard association metric like the log-likelihood ratio (Dunning, 1993) is applied. Note that the above mentioned window size of ±10 words from the given word relates to the preprocessed corpus from which function words have already been removed. Since in English roughly every second word tends to be a function word, the effective window size is about ±20 words. This window size is somewhat larger than what we typically find in other studies. However, the reason for this is quite obvious: As citations of foreign words are rare, we have a severe problem of data sparseness, and by looking at a relatively large window we try to somewhat compensate for this.4 Despite"
W10-3908,W97-0119,0,0.381381,"he sentences translated by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have limited scope otherwise. For this reason we will not further discuss it here. 2 51 valents should also co-occur more frequently than expected in a corpus of language B. A grea"
W10-3908,P98-1069,0,0.592771,"Missing"
W10-3908,2005.mtsummit-papers.11,0,0.041683,"can happens to also belong to English, meaning something completely different. Moreover, can is a high frequency word, occurring millions of times in a large corpus. Of course, if we had a perfect word sense disambiguator, we could separate the Catalan and the English occurrences of can, thereby solving the problem. 7 Unfortunately, existing tools are not powerful enough to do the job. What is worse, such collisions are not Which, for example, by using open source tools such as Moses and Giza++ (see www.statmt.org) can be easily generated from parallel corpora, e.g. from the Europarl corpus (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006). 7 If we assume that foreign words typically occur in clusters, we could also use language identification software. 6 uncommon between languages using the same script. So what can we do? Our suggestion is exactly the same as above for the problem of data sparseness, i.e. to look at several source languages in parallel. But it is clear that collapsing all source words into a single item does not work. If only one of them happens to be also a common word in the target language, it is very likely that its co-occurrences will override the co-occ"
W10-3908,J05-4003,0,0.108929,"large corpus of the target language for sentences similar to the translations. The advantage of this procedure is that the sentences retrieved this way are correct sentences as they were produced by humans, whereas the sentences translated by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closel"
W10-3908,P95-1050,1,0.552456,"d by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related languages but appears to have limited scope otherwise. For this reason we will not further discuss it here. 2 51 valents should also co-occur more frequently than expected in a corpus of language B. A great number of"
W10-3908,P99-1067,1,0.813579,"usible model, assuming a person lived for some years in one country, and then for some more years in another country, assuming further that this person never looked at a dictionary or another multilingual document and never communicated with a person mixing both languages. It is known that this can work. The reason is probably the following: Many words of the basic dictionary assumed above correspond to items of the physical world. These items generally have names in natural languages which can serve as mediators. That the extrapolation to more abstract notions is possible has been claimed by Rapp (1999). Still, although persons proceeding this way can easily understand and, after some years, even think in each of the two languages, experience shows that they tend to have some difficulties when making translations, especially literal translations. So, although the above scenario is possible, we do not think that it is a typical one for our modern times. There are certainly good reasons why there are so many language courses, and why there is such an abundance of dictionaries. It is a matter of commonsense that the person trying to acquire a new language will look at a multilingual dictionary."
W10-3908,steinberger-etal-2006-jrc,0,0.0667499,"meaning something completely different. Moreover, can is a high frequency word, occurring millions of times in a large corpus. Of course, if we had a perfect word sense disambiguator, we could separate the Catalan and the English occurrences of can, thereby solving the problem. 7 Unfortunately, existing tools are not powerful enough to do the job. What is worse, such collisions are not Which, for example, by using open source tools such as Moses and Giza++ (see www.statmt.org) can be easily generated from parallel corpora, e.g. from the Europarl corpus (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006). 7 If we assume that foreign words typically occur in clusters, we could also use language identification software. 6 uncommon between languages using the same script. So what can we do? Our suggestion is exactly the same as above for the problem of data sparseness, i.e. to look at several source languages in parallel. But it is clear that collapsing all source words into a single item does not work. If only one of them happens to be also a common word in the target language, it is very likely that its co-occurrences will override the co-occurrences of the foreign words we are interested in."
W10-3908,I05-1023,0,0.280422,"get language for sentences similar to the translations. The advantage of this procedure is that the sentences retrieved this way are correct sentences as they were produced by humans, whereas the sentences translated by a machine tend to be garbled and of lower quality. However, the big problem with this approach is to ensure that the retrieved sentence pairs are indeed translations of each other. While there is no perfect solution to this problem, several studies have shown that such data can be useful for building or supplementing translation models in SMT (see e. g. Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach for exploiting comparable corpora in dictionary generation is based on the observation that word co-occurrence patterns between languages tend to be similar (Fung & McKeown, 1997; Rapp, 1995; Chiao et al., 2004). If, for example, two words X and Y cooccur more often than expected by chance in a corpus of language A, then their translated equiThere is also the approach of identifying orthographically similar words (Koehn & Knight, 2002) which does not even require a corpus as simple word lists will suffice. However, this approach is promising only for closely related language"
W10-3908,C98-1066,0,\N,Missing
W10-3908,W02-0902,0,\N,Missing
W10-4005,J90-2002,0,0.735891,"ing at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as"
W10-4005,J93-1003,0,0.0269198,"assume that the strongest association to a foreign word is likely to be its translation. This can be justified by typical usage patterns of foreign words often involving, for example, an explanation right after their first occurrence in a text. Associations between words can be computed in a straightforward manner by counting word co-occurrences followed by the application of an association measure on the cooccurrence counts. Co-occurrence counts are based on a text window comprising the 20 words on either side of a given foreign word. On the resulting counts we apply the loglikelihood ratio (Dunning, 1993). As explained by Dunning, this measure has the advantage to be applicable also on low counts, which is an important characteristic in our setting where the problem of data sparseness is particularly severe. This is also the reason why we chose a window size somewhat larger than the ones used in most other studies. Despite its simplicity this procedure of computing associations to foreign words is well suited for identifying word translations. As mentioned above, we assume that the strongest association to a foreign word is its best translation. We did this for words from five languages (Engli"
W10-4005,W97-0119,0,0.0277961,"y computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och &"
W10-4005,P98-1069,0,0.0392083,"est associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). Howev"
W10-4005,2005.mtsummit-papers.11,0,0.0245523,"pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why m"
W10-4005,P07-2045,0,0.0132767,"parable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-oc"
W10-4005,J05-4003,0,0.0254999,"ranslated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-occur more often than expected by chance, then in a text of another language those words which are the translations of A and B should also co-occur more frequently than expected. However, to exploit this observation some bridge needs to be built between the two languages. This can be done via a"
W10-4005,J03-1002,0,0.00232831,"1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are a scarce resource for many language pairs (Rapp & Martín Vide, 2007), which is why methods based on comparable corpora have come into focus. One approach is to extract parallel sentences from comparable corpora (Munteanu & Marcu, 2005; Wu & Fung, 2005). Another approach relates co-occurrence patterns between languages. Hereby the underlying assumption is that across languages there is a correlation between the cooccurrences of words which are translations of each other. If, for example, in a text of one language two words A and B co-occur more often than expected"
W10-4005,P99-1067,1,0.926968,"n this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora. Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts. 1 Introduction Established methods for the identification of word translations are based on parallel (Brown et al., 1990) or comparable corpora (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1995; Rapp 1999; Chiao et al., 2004). The work using parallel corpora such as Europarl (Koehn, 2005; Armstrong et al., 1998) or JRC Acquis (Steinberger et al., 2006) typically performs a length-based sentence alignment of the translated texts, and then tries to conduct a word alignment within sentence pairs by determining word correspondences that get support from as many sentence pairs as possible. This approach works very well and can easily be put into practice using a number of freely available open source tools such as Moses (Koehn et al., 2007) and Giza++ (Och & Ney, 2003). However, parallel texts are"
W10-4005,W10-3908,1,0.253732,"losely related to newspaper text. It is usually carefully edited, and the vocabulary is geared towards easy understanding for the intended readership. This implies that foreign word citations are kept to a minimum. In contrast, the WaCky Corpora have been downloaded from the web and represent a great variety of text types and styles. Hence, not all texts can be expected to have been carefully edited, and mixes between languages are probably more frequent than with newsticker text. As in this work English is the main source language, and as we have dealt with it as a target language already in Rapp & Zock (2010), we do not use the respective English versions of these corpora here. We also do not use the Wikipedia XML Corpora (Denoyer et al., 2006) as these greatly vary in size for different languages which makes comparisons across languages somewhat problematic. In contrast, the sizes of the above corpora are within the same order of magnitude (1 billion words each), which is why we do not control for corpus size here. Concerning the number of foreign words within these corpora, we might expect that, given the status of English as the world’s premiere language, English foreign words should be the mos"
W10-4005,steinberger-etal-2006-jrc,0,0.0638993,"Missing"
W10-4005,I05-1023,0,\N,Missing
W10-4005,C98-1066,0,\N,Missing
W10-4005,W02-0902,0,\N,Missing
W12-0114,P07-1018,1,0.869022,"translation quality. (III) Extension to other languages: Structural similarity and translation by pivot languages is used to obtain extension to further languages: High-quality translation between closely related languages (e.g., Russian and Ukrainian or Portuguese and Spanish) can be achieved with relatively simple resources (using linguistic similarity, but also homomorphism assumptions with respect to parallel text, if available), while greater efforts are put into ensuring better-quality translation between more distant languages (e.g. German and Russian). According to our prior research (Babych et al., 2007b) the pipeline between languages of different similarity results in improved translation quality for a larger number of language pairs (e.g., MT from Portuguese or Ukrainian into German is easier if there are highquality analysis and transfer modules for Spanish and Russian into German (respectively). Of course, (III) draws heavily on the detailed analysis and MT systems that the industrial partner in HyghTra provides for a number of languages. In the following sections we give more details of the work currently done with regard to (I) and with regard to parts of (II): the creation of a new M"
W12-0114,2007.mtsummit-papers.5,1,0.957669,"Missing"
W12-0114,E06-1032,0,0.0769042,"Missing"
W12-0114,2001.mtsummit-papers.18,1,0.831384,"able and are unlikely to become available in the future. Also, SMT tends to disregard important classificatory knowledge (such as morphosyntactic, categorical and lexical class features), which can be provided and used relatively easily within non-statistical representations. On the other hand, advantages of RBMT are that its (grammar and lexical) rules and information are understandable by humans and can be exploited for a lot of applications outside of translation (dictionaries, text understanding, dialogue systems, etc.). The slot grammar approach used in Lingenio systems (cf. McCord 1989, Eberle 2001) is a prime example of such linguistically rich representations that can be used for a number of different applications. Fig.1 shows this by a visualization of (an excerpt of) the entry for the ambiguous German verb einstellen in the database that underlies (a) the Lingenio translation products, where it links up with corresponding set of the transfer rules, and (b) Lingenio’s dictionary product TranslateDict, which is primarily intended for human translators. 101 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101–112, c Avign"
W12-0114,eberle-etal-2012-tool,1,0.868581,"Missing"
W12-0114,A94-1016,0,0.083521,"ation and statistical extension and training): (a) We start out with declarative analysis and generation components of the considered languages, and with basic bilingual dictionaries connecting to one another the entries of relatively small vocabularies comprising the most frequent words of each language in a given translation pair (cf. Fig 1 a). (b) Having completed this phase, we extend the dictionaries and train the analysis-, transfer- and generation-components of the rule-based core systems using monolingual and bilingual corpora. 1 A prominent early example is Frederking and colleagues (Frederking & Nirenburg, 1994). For an overview of hybrid MT till the late nineties see Streiter et al. (1999). More recent approaches include Groves & Way (2006a, 2006b). Commercial implementations include AppTek (http://www.apptek.com) and Language Weaver (http://www.languageweaver.com). An ongoing MT important project investigating hybrid methods is EuroMatrixPlus (http://www.euromatrixplus.net/) 102 (II) Error detection and improvement cycle: (a) We automatically discover the most frequent problematic grammatical constructions and multiword expressions for commercial RBMT and SMT systems using automatic construction-ba"
W12-0114,W97-0119,0,0.0461742,"Missing"
W12-0114,J93-1004,0,0.311866,"Missing"
W12-0114,2004.eamt-1.9,0,0.0664357,"Missing"
W12-0114,2006.eamt-1.15,0,0.0619246,"Missing"
W12-0114,habash-dorr-2002-handling,0,0.0417473,"Missing"
W12-0114,J06-4003,0,0.0132216,"Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/sentrick/index.html. 104 Europarl-texts considered (as stored in our database). In order to optimize the results we use the dictionaries of step 1 as set of cognates (cf. Simard at al 1992, Gough & Way 2004), as well as other words easily obtainable from the internet that can be used for this purpose (like company names and other named entities with cross-language identity and terminology translations). Using the morphology component of the new language and the"
W12-0114,2005.mtsummit-papers.11,0,0.0671921,"Missing"
W12-0114,J99-1003,0,0.113762,"Missing"
W12-0114,J05-4003,0,0.0183273,"quire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010, 2012). Section Results"
W12-0114,P02-1038,0,0.0217034,"Missing"
W12-0114,J03-1002,0,0.00439785,"enerationoriented representations from grammar models and statistical combinatorial properties of annotated features. Step 3: Generating dictionary extensions from parallel corpora Based on parallel corpora, dictionaries can be derived using established techniques of automatic sentence alignment and word alignment. For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or – alternatively – Dan Melamed’s GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/se"
W12-0114,P02-1040,0,0.0873178,"Missing"
W12-0114,P95-1050,1,0.218313,"Missing"
W12-0114,rapp-2004-freely,1,0.85454,"Missing"
W12-0114,P99-1068,0,0.0503705,"ps. Step 1: Acquire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010,"
W12-0114,C90-3044,0,0.0668576,"Missing"
W12-0114,P06-2095,1,0.899134,"f a manually compiled kernel does not show 105 an ambiguity problem of similar significance), and as experience shows that most low frequency words in a full-size lexicon tend to be unambiguous, the ambiguity problem is reduced further for the words investigated and extracted by this comparison method. Step 5: Expanding dictionaries comparable corpora (multiword units) using In order to account for technical terms, idioms, collocations, and typical short phrases, an important feature of an MT lexicon is a high coverage of multiword units. Very recent work conducted at the University of Leeds (Sharoff et al., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja,"
W12-0114,sharoff-2006-uniform,1,0.886741,"in a machinetranslated corpus In a later work package of the project, we will run a large parallel corpus through available (competitive) MT engines, which will be enhanced by automatic dictionaries developed during the previous stages. On the source-language side of the corpus we will automatically generate lists of frequent multiword expressions (MWEs) and grammatical constructions using the methodology proposed in (Sharoff et al., 2006). For each of the identified MWEs and constructions we will generate a parallel concordance using open-source CSAR architecture developed by the Leeds team (Sharoff, 2006). The concordance will be generated by running queries to the sentencealigned parallel corpora and will return lists of corresponding sentences from gold-standard human translations and corresponding sentences generated by MT. Each of these concordances will be automatically evaluated using standard MT evaluation metrics, such as BLEU. Under these settings parallel concordances will be used as standard MT evaluation corpora in an automated MT evaluation scenario. Normally BLEU gives reliable results for MT corpora over 7000 words. However, in (Babych and Hartley, 2009; Babych and Hartley, 2008"
W12-0114,J93-1007,0,0.0374134,"., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja, 1993) • To translate a collocation, look up all its words using any dictionary • Generate all possible permutations (sequences) of the word translations • Count the occurrence frequencies of these sequences in a corpus of the target language and test for significance • Consider the most significant sequence to be the translation of the source language collocation Of course, in later steps of the project, we will experiment on filtering these sequences by exploiting structural knowledge similarly to what was described in the two previous steps. This can be obtained on the basis of the declarative an"
W12-0114,2007.mtsummit-aptme.6,0,0.11913,"Missing"
W12-0114,I05-1023,0,\N,Missing
W12-0114,P99-1067,1,\N,Missing
W12-0114,W02-0902,0,\N,Missing
W12-0114,baroni-bernardini-2004-bootcat,0,\N,Missing
W13-2801,W13-2816,0,0.0501134,"Missing"
W13-2801,W13-2813,0,0.0217301,"Missing"
W13-2801,2008.eamt-1.6,0,0.0725213,"Missing"
W13-2801,W13-2804,0,0.0365849,"Missing"
W13-2801,W13-2805,0,0.0466293,"Missing"
W13-2801,W13-2814,0,0.0251731,"d in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving transl"
W13-2801,W13-2806,0,0.0246185,"Missing"
W13-2801,W13-2807,0,0.0435616,"Missing"
W13-2801,W13-2817,0,0.0294611,"responding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the fol"
W13-2801,W13-2815,0,0.0580296,"Missing"
W13-2801,W13-2811,0,0.014586,"nd corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali train"
W13-2801,W13-2810,0,0.0239078,"Missing"
W13-2801,W13-2818,0,0.0263833,"extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the following research work: • Tambouratzis et al. (2013) describe a hybrid MT architecture that uses very few bilingual corpus and a large monolingual one. The linguistic information is extracted using pattern recognition techniques. Table 1 summarizes the papers that have been presented in the Second HyTra Workshop. The papers are arranged into the table according to the linguistic level they address. • Rudnick et al. (2013) present a combination of Maximum Entropy Markov Models and HMM to perform lexical selection in the sense of cross-lingual word sense disambiguation (i.e. by choice from the set of translation alternatives). The system is meant"
W13-2801,W13-2808,0,0.058822,"Missing"
W13-2801,W10-1737,0,0.0651783,"Missing"
W13-2801,W13-2803,0,0.0495517,"Missing"
W13-2801,W13-2809,0,0.0597288,"Missing"
W13-2801,W13-2812,0,0.0196328,"often considered and represented simultaneously (not only in unification-based approaches) and the same is true for MT systems. Syntax had been addressed originally in SMT in the form of so called phrase-based SMT without any reference to linguistic structures; during 3 • Bouillon et al. (2013) presents two methodologies to correct homophone confusions. The first one is based on hand-coded rules and the second one is based on weighted graphs derived from a pronunciation resource. • Laki et al. (2013) combine pre-reordering rules with morphological and factored models for English-to-Turkish. • Li et al. (2013) propose pre-reordering rules to be used for alignment-based reordering, and corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 201"
W14-0509,P06-2084,0,0.123461,"o-occurrence network is able to mimic human associative behavior. Such a network consists of nodes, which in our case correspond to words (or lemmas), and of weights connecting the nodes. The strengths of these weights are computed on the basis of word co-occurrence data, and by optionally applying an association measure. But there are many association measures. Given their number and diversity some researchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quantitative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. ""associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)"". Washtell & Markert (2009) tried to determine whether word associations should be computed via window-based co-occurrence counts or rather via a windowless approach measuring the distances between words. Our work is relate"
W14-0509,J90-1003,0,0.705155,"y (Schwartz & Reisberg, 1991) explains how these strengths (or weights) are acquired. The strength between two perceived events increases by a constant fraction of a maximally possible increment at each co-occurrence, and decreases in the opposite case. Wettler et al. (2005) have shown that this mechanism can be replicated by looking at word co-occurrence frequencies in large text collections. But there had been earlier corpus-linguistic work: For example, Wettler & Rapp (1989) compared several association measures in order to find search terms to be used for queries in information retrieval. Church & Hanks (1990) suggested to use mutual information, an information theoretic measure, for computing association strength. Prior to this, a lot of work had been done without reliance of corpora. For example, Collins & Loftus (1975) used associative semantic networks to show the distance between words. Others (Rosenzweig, 1961:358; Ekpo-Ufot, 1978) tried to show the universal status of a large subset of associations. While all these findings are important, we will not consider them further Free word associations are the words people spontaneously come up with in response to a stimulus word. Such information h"
W14-0509,P01-1025,0,0.0299926,"(CogACLL) @ EACL 2014, pages 43–48, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics here. Rather we will focus on the claim that a corpus-derived co-occurrence network is able to mimic human associative behavior. Such a network consists of nodes, which in our case correspond to words (or lemmas), and of weights connecting the nodes. The strengths of these weights are computed on the basis of word co-occurrence data, and by optionally applying an association measure. But there are many association measures. Given their number and diversity some researchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quantitative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. ""associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)"". Washtell & Markert (2009) trie"
W14-0509,D09-1066,0,0.247164,"earchers (Evert & Krenn, 2001) felt that there was a need to define some criteria and methods in order to allow for quantitative comparisons via task-based evaluations. Pursuing a similar goal, Pecina & Schlesinger (2006) compared 82 different association measures for collocation extraction, while Hoang et al. (2009) classified them. Michelbacher et al. (2011) investigated the potential of asymmetric association measures, i.e. ""associations whose associational strength is significantly greater in one direction (e.g., from Pyrrhic to victory) than in the other (e.g., from victory to Pyrrhic)"". Washtell & Markert (2009) tried to determine whether word associations should be computed via window-based co-occurrence counts or rather via a windowless approach measuring the distances between words. Our work is related to previous studies comparing human word associations with those derived from corpus statistics (e.g. Wettler et al., 2005; Tamir, 2005, Seidensticker, 2006). The main differences are that we categorize our stimulus words and present results for each class, and that we have a stronger focus on the graph aspect of our network. 2 noise and data sparsity while improving speed and accuracy during evalua"
W14-1016,P07-1018,1,0.868464,"ectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille & Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is"
W14-1016,C12-1110,0,0.045264,"Missing"
W14-1016,C12-1046,0,0.0216267,"sy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille & Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is determined by Abstract Most previous attempts"
W14-1016,W95-0114,0,0.668044,"ith this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results. 1 Introduction The task of identifying word translations from comparable text has received considerable attention. Some early papers include Fung (1995) and Rapp (1995). Fung (1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris' (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now clas"
W14-1016,P98-1069,0,0.427217,"(1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris' (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, such as Fung & Yee (1998) 87 Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87–95, c Gothenburg, Sweden, April 27, 2014. 2014 Association for Computational Linguistics the contextual behavior of the full unit, or by the contextual behavior of its components (or by a mix of both). But multiword expressions are of complex nature, as expressed e.g. by Moon (1998): ""there is no unified phenomenon to describe but rather a complex of features that interact in various, often untidy, ways and represent a broad continuum between non-compositional (or idiomatic) and compositional"
W14-1016,P08-1088,0,0.0860555,"matique Fondamentale F-13288 Marseille, France reinhardrapp@gmx.de Serge Sharoff University of Leeds Centre for Translation Studies Leeds, LS2 9JT, UK S.Sharoff@leeds.ac.uk and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. H"
W14-1016,D09-1124,0,0.173008,"on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and wh"
W14-1016,P11-1133,0,0.0845959,"ard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words"
W14-1016,W00-0901,0,0.173899,"Missing"
W14-1016,E06-1029,0,0.290641,"rallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille & Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning"
W14-1016,P95-1050,1,0.772987,"eille Université, Laboratoire d'Informatique Fondamentale F-13288 Marseille, France reinhardrapp@gmx.de Serge Sharoff University of Leeds Centre for Translation Studies Leeds, LS2 9JT, UK S.Sharoff@leeds.ac.uk and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the appro"
W14-1016,W02-2026,0,0.0699667,".uk and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to mu"
W14-1016,P99-1067,1,0.833329,"Missing"
W14-1016,rapp-etal-2012-identifying,1,0.695333,"a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab & Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some p"
W14-1016,C98-1066,0,\N,Missing
W14-4701,more-climent-2014-machine,0,0.0296862,"one stimulus increases significantly the response to another. Meyer and Schvaneveldt (1971) showed in their seminal experiments that people were faster in deciding that a string of letters is a word when it was followed by an associatively or semantically related word. For example, nurse is recognized more quickly following doctor than following bread. These findings supported also the idea of activation spreading as a method of access or search (Collins & Loftus, 1975). Associative networks can be considered as a special type of semantic network which were introduced by Richens (1956) and by Ceccato (1956) for quite a different purpose. They were meant to serve as an interlingua for machine translation. These knowledge representation structures were then further developed in the sixties by Simmons (1963) and Quillian (1963, 1966, 1967, 1968, 1969). They finally became famous due to the work done by Quillian and two psychologistst (Collins & Quillian, 1969 & 1970 and Collins & Loftus, 1975). Note that semantic networks can represent language at various levels of granularity: word, sentence (Sowa, 1984) or discourse (Mann & Thomson, 1988). Also, and very relevant for us here is the fact that at t"
W14-4701,E99-1013,0,0.0538156,"blem have systematically shown (Aitchison, 2003; Brown, 1991; Brown & McNeill, 1996) that users being in this state always know ‘something’ concerning the target word: fragments of the meaning, origin, number of syllables, etc. This being so, any of this could be used to guide the search. Suppose we focused only on the semantic aspects. In such a case it is reasonable to assume that the target form can be found on the basis of its defining elements (bag of the words contained in the definition). While not being perfect, this works quite well (Dutoit & Nugues, 2002; El-Kahlout & Oflazer, 2004; Mandala et al., 1999; Michiels, 1982). Actually, even Google - although not designed for this is able to recover in many cases the elusive word. Just try the following example, spring, typically found in Iceland or in the Yellowstone National Park, discharging hot water and steam, and chances are that you will find the target word geyser. Although not perfect, this is nevertheless quite useful. However, this represents only one kind of cognitive state (knowledge of the definition), and this is certainly neither the only one nor the most frequent one. Indeed, there are many situations where it is hard to come up w"
W14-4701,P99-1067,1,0.237424,"toll, officer, or country can be expected to come up in both cases. That is, their meaning vectors should be similar, and this similarity can be quantified e.g. by computing the cosine similarity between them. We thus have a method which allows us to measure the similarity between sentences in a way that to some extend takes their meanings into account. Finally, we can try to cross language barriers and make the step to association-based machine translation (ABMT). To translate a source language phrase, we compute its meaning vector. Presupposing that we have a basic dictionary, in analogy to Rapp (1999) we can translate this meaning vector into the target language.10 Further assuming that we already know the meaning vectors of a very large number of target language phrases, we next select the target language meaning vector which is most similar to the source language meaning vector. The respective target language phrase can be considered to be the translation of the source language phrase. Optionally, to improve translation quality, the target language phrase can be modified by adding, removing, substituting, or reordering words with the aim of improving the similarity between the meaning ve"
W14-4701,rapp-2014-corpus,1,0.87648,"e stimulus (prime), we have reversed this situation. Given a set of associations, the system was supposed to predict its trigger. More concretely speaking, participants were given 2000 sets of words, each set containing five words. The task was to determine automatically the sixth element, i.e. the prime (or stimulus), evoking the five words. One could object that this task does not really address the word access problem or its solution, but this is not quite so as we will try to show. In particular, it seems quite reasonable to claim that an association network with bi-directional links (see Rapp, 2014) is a suitable resource to support word ‘finding’. Since words are connected via bidirectional links either of the connected items can be the source or the target during the search (or during navigation). Although systems designed for the shared task can have many applications (see Section 6), a prototypical one is the tip-of-the-tongue problem, which is a special case (yet a quite frequent one) of word access. So let us briefly describe this problem and the steps needed to overcome it. One of the most vexing problems in speaking or writing is that one knows a given word, yet fails to access i"
W14-4701,1983.tc-1.13,0,0.644953,"place if exposure to one stimulus increases significantly the response to another. Meyer and Schvaneveldt (1971) showed in their seminal experiments that people were faster in deciding that a string of letters is a word when it was followed by an associatively or semantically related word. For example, nurse is recognized more quickly following doctor than following bread. These findings supported also the idea of activation spreading as a method of access or search (Collins & Loftus, 1975). Associative networks can be considered as a special type of semantic network which were introduced by Richens (1956) and by Ceccato (1956) for quite a different purpose. They were meant to serve as an interlingua for machine translation. These knowledge representation structures were then further developed in the sixties by Simmons (1963) and Quillian (1963, 1966, 1967, 1968, 1969). They finally became famous due to the work done by Quillian and two psychologistst (Collins & Quillian, 1969 & 1970 and Collins & Loftus, 1975). Note that semantic networks can represent language at various levels of granularity: word, sentence (Sowa, 1984) or discourse (Mann & Thomson, 1988). Also, and very relevant for us here"
W14-4701,P98-2123,1,\N,Missing
W14-4701,C98-2118,1,\N,Missing
W15-3411,W15-3413,0,0.0497045,"ld possibly provide more interesting measures, but this would require a baseline system which works with all the languages in question. 3.1 4 Results Overall, we have received eleven runs: one entry for Chinese (Table 2), three entries for French (Table 2), and seven for German (Table 3). 4.1 Metrics Methods used The method used by the system CCNUNLP is described in (Li and Gaussier, 2013). In essence, it uses a bilingual dictionary for converting the word feature vectors between the languages and estimating their overlap. The other systems are discussed in details in the current proceedings (Morin et al., 2015; Zafarian et al., 2015). The LINA system (Morin et al., 2015) is based on matching hapax legomena, i.e., words occurring only once. In addition to using hapax legomena, the quality of linking in one language pair, e.g., French-English, is also assessed by using information available in pages in another language pair, e.g., GermanEnglish. The AUT system (Zafarian et al., 2015) uses the most complicated setup by combining several steps. First, documents in different languages are mapped into the same space using a For each source page there exists exactly one correct linked page in the gold sta"
W15-3411,W15-3412,0,0.0703684,"more interesting measures, but this would require a baseline system which works with all the languages in question. 3.1 4 Results Overall, we have received eleven runs: one entry for Chinese (Table 2), three entries for French (Table 2), and seven for German (Table 3). 4.1 Metrics Methods used The method used by the system CCNUNLP is described in (Li and Gaussier, 2013). In essence, it uses a bilingual dictionary for converting the word feature vectors between the languages and estimating their overlap. The other systems are discussed in details in the current proceedings (Morin et al., 2015; Zafarian et al., 2015). The LINA system (Morin et al., 2015) is based on matching hapax legomena, i.e., words occurring only once. In addition to using hapax legomena, the quality of linking in one language pair, e.g., French-English, is also assessed by using information available in pages in another language pair, e.g., GermanEnglish. The AUT system (Zafarian et al., 2015) uses the most complicated setup by combining several steps. First, documents in different languages are mapped into the same space using a For each source page there exists exactly one correct linked page in the gold standard. Systems were requ"
W15-4108,W95-0114,0,0.325626,"l dictionary from a large parallel corpus of English-French Canadian parliamentary proceedings, and then built a machine translation system around this. The development of such systems has not been without setbacks, but finally, after 15 years of research, it led to a revolution in machine translation technology and provided the basis for machine translation systems such as Moses, Google Translate and Microsoft’s Bing Translator which are used by millions of people worldwide every day. The second strait of research is based on comparable rather than parallel corpora. It was first suggested by Fung (1995) and Rapp (1995). The motivation was that parallel corpora are a scarce resource for most language pairs and subject areas, and that human performance in second language acquisition and in translation shows that there must be a way of crossing the language barrier that does not require the reception of large amounts of translated texts. We suggest here to replace parallel by comparable corpora. Comparable (written or spoken) corpora are far more abundant than parallel corpora, thus offering the chance to overcome the data acquisition bottleneck. This is particularly true as, given n languages"
W15-4108,P04-1067,0,0.033287,"rpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in such a context was conducted by ourselves (Rapp & Zock, 2010a), and uses only a single pivot language. In contrast, here we suggest to take advantage of multiple pivot languages.4 4) Considering word senses rather than words in order to solve the ambiguity problem: Gaussier et al. (2004) use a geometric view to decompose the word vectors according to their senses. In contrast, we will use explicit word sense disambiguation based on the WordNet sense inventory. Annotations consistent with human intuitions are easier to verify and thus the system can be better optimized. 5) Investigating in how far foreign citations in monolingual corpora can be used for dictionary generation: To our knowledge, apart from our own (see Rapp & Zock, 2010b) there is no other previous work on this. 4 Conclusions A core problem in NLP is the problem of ambiguity in a multilingual setting. Entities i"
W15-4108,P08-1088,0,0.0179182,"ses ad hoc test sets were used, and to our knowledge no readily available test set exists for multiword units. Discussion In this section we discuss the relationship of the suggested work to the state of the art of research in the field. Hereby we concentrate on how the previous literature relates to the eight subtopics listed above. A more comprehensive survey of the field of bilingual dictionary extraction from comparable corpora can be found in Sharoff et al. (2013). 1) Eliminating the need for initial dictionaries: This problem has been approached e.g. by Rapp (1995), Diab & Finch (2000), Haghighi et al. (2008), and Vulic & Moens (2012). None of the suggested solutions seems to work well enough for most practical purposes. Through its multilevel approach, the above methodology aims to achieve this. 2) Looking at aligned comparable documents rather than at comparable corpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in suc"
W15-4108,D09-1124,0,0.0304643,"ey of the field of bilingual dictionary extraction from comparable corpora can be found in Sharoff et al. (2013). 1) Eliminating the need for initial dictionaries: This problem has been approached e.g. by Rapp (1995), Diab & Finch (2000), Haghighi et al. (2008), and Vulic & Moens (2012). None of the suggested solutions seems to work well enough for most practical purposes. Through its multilevel approach, the above methodology aims to achieve this. 2) Looking at aligned comparable documents rather than at comparable corpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in such a context was conducted by ourselves (Rapp & Zock, 2010a), and uses only a single pivot language. In contrast, here we suggest to take advantage of multiple pivot languages.4 4) Considering word senses rather than words in order to solve the ambiguity problem: Gaussier et al. (2004) use a geometric view to decompose the word vectors according to their senses."
W15-4108,C10-2070,0,0.145005,"the chances to reach this minimum number are lower for rare words. As multiword units are less frequent than their rarest constituents, on average their frequencies are lower than the frequencies of single words. Therefore it can be expected that they must be less ambiguous on average. The other explanation is that in multiword units the constituents tend to disambiguate each other, so fewer readings remain. 2.7 2.8 Developing a standard test set for evaluation As previous evaluations of the dictionary extraction task were usually conducted with ad hoc test sets and thus were not comparable, Laws et al. (2010) noted an urgent need for standard test sets. In response to this, we intend to work out and publish a gold standard which covers all of our eight language pairs and will ensure that words of a wide range of frequencies are appropriately represented. All results on single words are to be evaluated using this test set. Little work has been done so far on multiword dictionary extraction using comparable corpora (an exception is Rapp & Sharoff, 2010), and no widely accepted gold standard exists. A problem is that there are many ways how to define multiword units. To explore these and to provide f"
W15-4108,J90-2002,0,0.770029,"ailable for other companies, institutions, academia, and individuals. This is an obstacle for the advancement of the field. Given this situation, it would be desirable to be able to generate dictionaries ad hoc as we need them from corpora of the text types we are interested in. So a lot of thought has been spent on how to produce bilingual dictionaries more efficiently than manually in the traditional lexicographic way. From these efforts, two major straits of research arose: The first is based on the exploitation of parallel corpora, i.e. collections of translated documents, as suggested by Brown et al. (1990 and 1993) in their seminal papers. They automatically extracted a bilingual dictionary from a large parallel corpus of English-French Canadian parliamentary proceedings, and then built a machine translation system around this. The development of such systems has not been without setbacks, but finally, after 15 years of research, it led to a revolution in machine translation technology and provided the basis for machine translation systems such as Moses, Google Translate and Microsoft’s Bing Translator which are used by millions of people worldwide every day. The second strait of research is b"
W15-4108,S07-1006,0,0.046984,"Missing"
W15-4108,J93-2003,0,0.106226,"Missing"
W15-4108,E06-1018,0,0.0528165,"Missing"
W15-4108,S10-1034,0,0.256484,"portant clue, or scientific papers whose topics tend to be so narrow that a few specific internationalisms or proper names can be sufficient to identify the correspondences. Once the alignment at the document level has been conducted, the next step is to identify the most salient keywords in each of the documents. There are a number of well established ways of doing so, among them Paul Rayson’s method of comparing 49 the observed term frequencies in a document to the average frequencies in a reference corpus using the log-likelihood ratio, or - alternatively - the Likelysystem as developed by Paukkeri & Honkela (2010). By applying these keyword extraction methods the aligned comparable documents are converted to aligned lists of keywords. Some important properties of these lists of aligned keywords are similar to those of aligned parallel sentences, which means that there is a chance to successfully apply the established statistical machinery developed for parallel sentences. We conducted a pilot study using a self-developed robust alternative to GIZA++, with promising results (Rapp, Sharoff & Babych, 2012). In principle, the method is applicable not only to the problem of identifying the translations of s"
W15-4108,W10-4005,1,0.94064,"ed word/tag combinations. Subsequently, in a second iteration a new distributional thesaurus is computed, leading to further mergers of word/tag combinations. This iterative process is to be repeated until there are no more strong similarities between any entries of a newly created thesaurus. At this point the result is a fully sense tagged corpus where the granularity of the senses can be controlled as it depends on the similarity threshold used for merging thesaurus entries. 2.5 of foreign language citations in large newspaper or web corpora follows similar principles (for a pilot study see Rapp & Zock, 2010b). The following two citations from the Brown Corpus (Francis & Kuςera, 1989) are meant to provide some evidence for this (underscores by us): 1. The tables include those for the classification angles , refractive indices , and melting points of the various types of crystals . Part 2 of Volume /1 , and Parts 2 and 3 of Volume /2 , contain the crystal descriptions . These are grouped into sections according to the crystal system , and within each section compounds are arranged in the same order as in Groth 's CHEMISCHE KRYSTALLOGRAPHIE . An alphabetical list of chemical and mineralogical names"
W15-4108,P11-1133,0,0.0310939,"al dictionary extraction from comparable corpora can be found in Sharoff et al. (2013). 1) Eliminating the need for initial dictionaries: This problem has been approached e.g. by Rapp (1995), Diab & Finch (2000), Haghighi et al. (2008), and Vulic & Moens (2012). None of the suggested solutions seems to work well enough for most practical purposes. Through its multilevel approach, the above methodology aims to achieve this. 2) Looking at aligned comparable documents rather than at comparable corpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in such a context was conducted by ourselves (Rapp & Zock, 2010a), and uses only a single pivot language. In contrast, here we suggest to take advantage of multiple pivot languages.4 4) Considering word senses rather than words in order to solve the ambiguity problem: Gaussier et al. (2004) use a geometric view to decompose the word vectors according to their senses. In contrast, we will use"
W15-4108,P95-1050,1,0.696788,"m a large parallel corpus of English-French Canadian parliamentary proceedings, and then built a machine translation system around this. The development of such systems has not been without setbacks, but finally, after 15 years of research, it led to a revolution in machine translation technology and provided the basis for machine translation systems such as Moses, Google Translate and Microsoft’s Bing Translator which are used by millions of people worldwide every day. The second strait of research is based on comparable rather than parallel corpora. It was first suggested by Fung (1995) and Rapp (1995). The motivation was that parallel corpora are a scarce resource for most language pairs and subject areas, and that human performance in second language acquisition and in translation shows that there must be a way of crossing the language barrier that does not require the reception of large amounts of translated texts. We suggest here to replace parallel by comparable corpora. Comparable (written or spoken) corpora are far more abundant than parallel corpora, thus offering the chance to overcome the data acquisition bottleneck. This is particularly true as, given n languages to be considered"
W15-4108,W14-4701,1,0.818201,"will often be a problem for language pairs involving lesser used languages or when existing dictionaries are copyright protected or not available in machine readable form. Secondly, depending on the coverage of this dictionary, quite a few of the requested translations may not be known. For these reasons a method not requiring an initial dictionary would be desirable. Let us therefore outline our proposal for a novel bootstrapping approach which requires only a few seed translations. The underlying idea is based on multi-stimulus associations (Rapp, 1996; Rapp, 2008; Lafourcade & Zampa, 2009; Rapp & Zock, 2014). There is also related work in cognitive science. It often goes under the label of the remote association test, but essentially pursues the same ideas (Smith et al., 2013). As experience tells, associations to several stimuli are non-random. For example, if we present the word pair circus – laugh to test persons and ask for their spontaneous associations, a typical answer will be clown. Likewise, if we present King – daughter, many will respond with princess. Like the associative responses to single words, the associative answers to pairs of stimuli can also be predicted with high precision b"
W15-4108,P99-1067,1,0.652964,"ication using cross checks based on pivot languages). They cannot solve the problem when used in isolation, but when amended and combined they may well have the potential to lead to substantial improvements. In this paper we try to come up with a roadmap for this. 2 frequently co-occurring words into the target language using an initial dictionary. 3) In the target language, find the word which most frequently cooccurs with these translations. There are two major problems with this approach: Firstly, an already relatively comprehensive initial dictionary of typically more than 10,000 entries (Rapp, 1999) is required which will often be a problem for language pairs involving lesser used languages or when existing dictionaries are copyright protected or not available in machine readable form. Secondly, depending on the coverage of this dictionary, quite a few of the requested translations may not be known. For these reasons a method not requiring an initial dictionary would be desirable. Let us therefore outline our proposal for a novel bootstrapping approach which requires only a few seed translations. The underlying idea is based on multi-stimulus associations (Rapp, 1996; Rapp, 2008; Lafourc"
W15-4108,E06-1029,0,0.0353051,"ting to select translations. e) Multiword named entities taken from JRCNames (as provided by the European Commission's Joint Research Centre). Applying the approach to different text types By their nature, the dictionaries generated using the above algorithms will always reflect the contents of the underlying corpora, i.e. their genre and topic. This means that if the corpora consist of newspaper articles on politics, the generated 52 The results on the multiword dictionary extraction task are to be evaluated using each of these gold standards. 3 6) Generating dictionaries of multiword units: Robitaille et al. (2006) and the TTC project (http://www.ttc-project.eu/) dealt with this in a comparable corpora setting but did not make their results available. In contrast, the intention here is to publish the full dictionaries. 7) Applying the approach to different text types: Although different researchers used a multitude of comparable corpora, to our knowledge there exists no systematic comparative study concerning different text types in the field of bilingual dictionary extraction. 8) Developing a standard test set for evaluation: Laws et al. (2010) pointed out the need for a common test set and provided on"
W15-4108,2003.mtsummit-papers.42,1,0.638938,"he number of languages we also increase the possibilities of mutual cross-validation. In this way a highly effective multi-dimensional cross-check can be realized. Utilizing transitivity is a well established technique in manual dictionary lookup when people interested in uncommon language pairs (where no dictionary is available) use two dictionaries involving a common pivot language. Likewise, lexicographers often use this concept when manually cre2 Similar sense inventories across languages can be expected under the assumption that the senses reflect observations in the real world. 50 2006; Rapp, 2003; SemEval 2007 and 2010 task “Word sense induction”). In an attempt to come up with an improved algorithm, we propose a novel bootstrapping approach which conducts word sense induction and word sense disambiguation in an integrated fashion. It starts by tagging each content word in a corpus with the strongest association that occurs nearby. For example, in the sentence ""He gets money from the bank"", the word bank would be tagged with money as this is the strongest association occurring in this neighborhood. Let us use the notation [bank < money] to indicate this. From the tagged corpus a stand"
W15-4108,W02-2026,0,0.0656002,". A more comprehensive survey of the field of bilingual dictionary extraction from comparable corpora can be found in Sharoff et al. (2013). 1) Eliminating the need for initial dictionaries: This problem has been approached e.g. by Rapp (1995), Diab & Finch (2000), Haghighi et al. (2008), and Vulic & Moens (2012). None of the suggested solutions seems to work well enough for most practical purposes. Through its multilevel approach, the above methodology aims to achieve this. 2) Looking at aligned comparable documents rather than at comparable corpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in such a context was conducted by ourselves (Rapp & Zock, 2010a), and uses only a single pivot language. In contrast, here we suggest to take advantage of multiple pivot languages.4 4) Considering word senses rather than words in order to solve the ambiguity problem: Gaussier et al. (2004) use a geometric view to decompose the word vectors"
W15-4108,W08-1914,1,0.900046,"ntries (Rapp, 1999) is required which will often be a problem for language pairs involving lesser used languages or when existing dictionaries are copyright protected or not available in machine readable form. Secondly, depending on the coverage of this dictionary, quite a few of the requested translations may not be known. For these reasons a method not requiring an initial dictionary would be desirable. Let us therefore outline our proposal for a novel bootstrapping approach which requires only a few seed translations. The underlying idea is based on multi-stimulus associations (Rapp, 1996; Rapp, 2008; Lafourcade & Zampa, 2009; Rapp & Zock, 2014). There is also related work in cognitive science. It often goes under the label of the remote association test, but essentially pursues the same ideas (Smith et al., 2013). As experience tells, associations to several stimuli are non-random. For example, if we present the word pair circus – laugh to test persons and ask for their spontaneous associations, a typical answer will be clown. Likewise, if we present King – daughter, many will respond with princess. Like the associative responses to single words, the associative answers to pairs of stimu"
W15-4108,W14-1016,1,0.814933,"Missing"
W15-4108,J93-1007,0,0.286573,"highly productive so that their number can be orders of magnitude higher, making it infeasible to achieve good coverage using manual methods. In contrast, most automatic methods for dictionary extraction, including the ones described above, can be applied to multiword units in a straightforward way. The only prerequisite is that the multiword units need to be known beforehand, that is, in a pre-processing step they must be identified and tagged as such in the corpora. There exist numerous methods for this, most of them relying on measures of mutual information between neighbouring words (e.g. Smadja, 1993; Paukkeri & Honkela, 2010). Our intention is to adopt the language independent “Likely” system for this purpose (Paukkeri & Honkela, 2010). Using the methods described in Sections 2.1 to 2.5, we will generate dictionaries of multiword units for all language pairs considered, i.e. involving English, French, German, Spanish, and Dutch, and then evaluate the dictionaries as outlined in section 2.8. Our expectation is that the problem of word ambiguity will be less severe with multiword units than it is with single words. There are two reasons for this, which are probably two sides of the same me"
W15-4108,rapp-etal-2012-identifying,1,0.80342,"comparable corpora can be found in Sharoff et al. (2013). 1) Eliminating the need for initial dictionaries: This problem has been approached e.g. by Rapp (1995), Diab & Finch (2000), Haghighi et al. (2008), and Vulic & Moens (2012). None of the suggested solutions seems to work well enough for most practical purposes. Through its multilevel approach, the above methodology aims to achieve this. 2) Looking at aligned comparable documents rather than at comparable corpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in such a context was conducted by ourselves (Rapp & Zock, 2010a), and uses only a single pivot language. In contrast, here we suggest to take advantage of multiple pivot languages.4 4) Considering word senses rather than words in order to solve the ambiguity problem: Gaussier et al. (2004) use a geometric view to decompose the word vectors according to their senses. In contrast, we will use explicit word sense dis"
W15-4108,E12-1046,0,0.0182692,"ed, and to our knowledge no readily available test set exists for multiword units. Discussion In this section we discuss the relationship of the suggested work to the state of the art of research in the field. Hereby we concentrate on how the previous literature relates to the eight subtopics listed above. A more comprehensive survey of the field of bilingual dictionary extraction from comparable corpora can be found in Sharoff et al. (2013). 1) Eliminating the need for initial dictionaries: This problem has been approached e.g. by Rapp (1995), Diab & Finch (2000), Haghighi et al. (2008), and Vulic & Moens (2012). None of the suggested solutions seems to work well enough for most practical purposes. Through its multilevel approach, the above methodology aims to achieve this. 2) Looking at aligned comparable documents rather than at comparable corpora: Previous publications concerning this are Schafer & Yarowsky (2002), Hassan & Mihalcea (2009), Prochasson & Fung (2011) and Rapp et al. (2012). In our view, the full potential has not yet been unveiled. 3) Utilizing multiple pivot languages in order to improve dictionary quality: The (to our knowledge) only previous study in such a context was conducted"
W17-2512,W17-2511,0,0.141723,"ts for a given language pair were generated with the same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the Jaccard coefficient (VIC), possibly with weighting (a function of frequency: VIC; tf"
W17-2512,W09-3109,0,0.0709534,"Missing"
W17-2512,N04-1034,0,0.216543,"detection of comparable documents across languages. The Second BUCC Shared Task,1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora. Let us recall the overall goals, design and principles of this task, which were introduced in (Zweigenbaum et al., 2016). A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains. Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora (Utiyama and Isahara, 2003; Munteanu et al., 2004; AbdulRauf and Schwenk, 2009). These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems. However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly. It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and comparability issues. We"
W17-2512,S16-1081,0,0.426211,"llel sentence spotting task. 1 Introduction Shared tasks and the associated datasets have proved their worth as a driving force in a number of subfields of Natural Language Processing. However, very few shared tasks were organized on the topic of comparable corpora. Therefore, we endeavored to design and organize shared tasks as companions of the BUCC workshop se1 https://comparable.limsi.fr/bucc2017/ bucc2017-task.html 60 Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pages 60–67, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics (Agirre et al., 2016), and WMT’s bilingual document alignment (Buck and Koehn, 2016). The present paper reports the actual organization of the task as a companion to the BUCC 2017 workshop. We describe the final method we used to prepare bilingual corpora in four language pairs: Chinese-English, French-English, GermanEnglish, and Russian-English (Section 2), the evaluation method (Section 3), the participants’ systems (Section 4), the results they obtained (Section 5), and conclude (Section 6). 2 formed the actual insertion after all parallel sentence pairs were thus processed. Additionally, a different distributi"
W17-2512,W17-2508,0,0.183214,"nsertion were then performed on each split separately. Since the training and test sets for a given language pair were generated with the same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the J"
W17-2512,W15-3411,1,0.785307,"Missing"
W17-2512,P16-1189,0,0.0982588,"Missing"
W17-2512,P03-1010,0,0.129546,"f et al., 2015) tackled the detection of comparable documents across languages. The Second BUCC Shared Task,1 presented here, addresses the detection of parallel sentences across languages in nonaligned, monolingual corpora. Let us recall the overall goals, design and principles of this task, which were introduced in (Zweigenbaum et al., 2016). A bottleneck in statistical machine translation is the scarceness of parallel resources for many language pairs and domains. Previous research has shown that this bottleneck can be reduced by utilizing parallel portions found within comparable corpora (Utiyama and Isahara, 2003; Munteanu et al., 2004; AbdulRauf and Schwenk, 2009). These are useful for many purposes, including automatic terminology extraction and the training of statistical MT systems. However, past work relied on metainformation, such as the publication date of news articles or inter-language links in Wikipedia documents, to help select promising sentence pairs before examining them more thoroughly. It is therefore difficult to separate the heuristic part of the methods that deals with this meta-information in clever ways from the cross-language part of the methods that deals with translation and co"
W17-2512,W17-2510,1,0.72964,"he same process and parameters, they received very similar numbers of parallel sentence pairs. This process was applied to five languages (Chinese (zh), English (en), French (fr), German (de), Russian (ru)) to produce four bilingual datasets, each split into sample, training, and test data. Table 1 shows the statistics of the resulting datasets. 3 4 Participants and systems About 17 teams downloaded datasets, among which four teams submitted runs: VIC (Spain) (Azpeitia et al., 2017), RALI (Canada) (Gr´egoire and Langlais, 2017), JUNLP (India) (Mahata et al., 2017), and LIMSI (France: ‘zNLP’) (Zhang and Zweigenbaum, 2017). Table 2 gives more detail about teams and runs. All systems had to include a way to cope with the bilingual dimension of the task. This was addressed with pre-existing dictionaries (LIMSI), machine translation systems (JUNLP, LIMSI), word alignments obtained from parallel corpora (VIC), or bilingual word embeddings trained from parallel corpora (RALI). Cross-language sentence similarity was then handled by Cosine similarity (JUNLP, LIMSI, RALI) or the Jaccard coefficient (VIC), possibly with weighting (a function of frequency: VIC; tf.idf: LIMSI) and with a trained classifier (RALI, LIMSI)."
W17-2512,W17-2509,0,\N,Missing
W17-2512,W16-2347,0,\N,Missing
W93-0310,W89-0240,0,0.0317472,"ociation experiment is conducted with many subjects, tables are obtained which list the frequencies of particular responses to the stimulus words. These tables are called association norms. Many studies in psychology give evidence that there is a relation between the perception, learning and forgetting of verbal material and the associations between words. If we assume that word-associations determine language production, then it should be possible to estimate the strength of an associative relation between two words on the basis of the relative frequencies that these words co-occur in texts. Church et al. (1989), Wettler & Rapp (1989) and Church & Hanks (1990) describe algorithms which do this. However, the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects. This paper describes such a comparison and shows that corpus-based computations of word associations are similar to association norms collected from human subjects. According to the law of association by contiguity, the association strength between two words should be a function of the relative frequency of the two words being perceived together, i.e. the relative frequency of the two wo"
W93-0310,J90-1003,0,0.173979,"cts, tables are obtained which list the frequencies of particular responses to the stimulus words. These tables are called association norms. Many studies in psychology give evidence that there is a relation between the perception, learning and forgetting of verbal material and the associations between words. If we assume that word-associations determine language production, then it should be possible to estimate the strength of an associative relation between two words on the basis of the relative frequencies that these words co-occur in texts. Church et al. (1989), Wettler & Rapp (1989) and Church & Hanks (1990) describe algorithms which do this. However, the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects. This paper describes such a comparison and shows that corpus-based computations of word associations are similar to association norms collected from human subjects. According to the law of association by contiguity, the association strength between two words should be a function of the relative frequency of the two words being perceived together, i.e. the relative frequency of the two words occuring together. Further more, the associat"
