2020.acl-main.135,W07-0734,0,0.04282,"th the answer as inputs to generate the question. However, state-of-the-art semantic parsing models have difficulty in producing accurate semantic graphs for very long documents. We therefore pre-process the original dataset to select relevant sentences, i.e., the evidence statements and the sentences that overlap with the ground-truth question, as the input document. We follow the original data split of HotpotQA to pre-process the data, resulting in 90,440 / 6,072 examples for training and evaluation, respectively. Following previous works, we employ BLEU 1–4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004) as automated evaluation metrics. BLEU measures the average n-gram overlap on a set of reference sentences. Both METEOR and ROUGE-L specialize BLEU’s n-gram overlap idea for machine translation and text summarization evaluation, respectively. Critically, we also conduct human evaluation, where annotators evaluate the generation quality from three important aspects of deep questions: fluency, relevance, and complexity. 4.2 Baselines We compare our proposed model against several strong baselines on question generation. • Seq2Seq + Attn (Bahdanau et al., 2014): the basic"
2020.acl-main.135,D18-1362,0,0.0304405,"east two potential future directions. First, graph structure that can accurately represent the semantic meaning of the document is crucial for our model. Although DP-based and SRL-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van Noord et al., 2018; Liu et al., 2019b) and knowledge graph-enhanced text representations (Cao et al., 2017; Yang et al., 2019). Second, our method can be improved by explicitly modeling the reasoning chains in generation of deep questions, inspired by related methods (Lin et al., 2018; Jiang and Bansal, 2019) in multi-hop question answering. Acknowledgments This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. 1471 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Nicola De Cao, Wilker Aziz"
2020.acl-main.135,P19-1629,0,0.325959,"ling better language flexibility, compared against rule-based methods. A comprehensive survey of QG can be found in Pan et al. (2019). Many improvements have been proposed since the first Seq2Seq model of Du et al. (2017): applying various techniques to encode the answer information, thus allowing for better quality answerfocused questions (Zhou et al., 2017; Sun et al., 2018; Kim et al., 2019); improving the training via combining supervised and reinforcement learning to maximize question-specific rewards (Yuan et al., 2017); and incorporating various linguistic features into the QG process (Liu et al., 2019a). However, these approaches only consider sentence-level QG. In contrast, our work focus on the challenge of generating deep questions with multi-hop reasoning over document-level contexts. Recently, work has started to leverage paragraphlevel contexts to produce better questions. Du and Cardie (2018) incorporated coreference knowledge to better encode entity connections across documents. Zhao et al. (2018) applied a gated selfattention mechanism to encode contextual information. However, in practice, semantic structure is difficult to distil solely via self-attention over the entire documen"
2020.acl-main.135,J08-2001,0,0.053577,"Missing"
2020.acl-main.135,Q18-1043,0,0.0602566,"Missing"
2020.acl-main.135,P02-1040,0,0.107103,"he supporting documents along with the answer as inputs to generate the question. However, state-of-the-art semantic parsing models have difficulty in producing accurate semantic graphs for very long documents. We therefore pre-process the original dataset to select relevant sentences, i.e., the evidence statements and the sentences that overlap with the ground-truth question, as the input document. We follow the original data split of HotpotQA to pre-process the data, resulting in 90,440 / 6,072 examples for training and evaluation, respectively. Following previous works, we employ BLEU 1–4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004) as automated evaluation metrics. BLEU measures the average n-gram overlap on a set of reference sentences. Both METEOR and ROUGE-L specialize BLEU’s n-gram overlap idea for machine translation and text summarization evaluation, respectively. Critically, we also conduct human evaluation, where annotators evaluate the generation quality from three important aspects of deep questions: fluency, relevance, and complexity. 4.2 Baselines We compare our proposed model against several strong baselines on question generation. • Seq2Seq + Attn ("
2020.acl-main.135,D14-1162,0,0.0837784,"Missing"
2020.acl-main.135,P19-1487,0,0.0222493,", What-if, which requires an in-depth understanding of the input source and the ability to reason over disjoint relevant contexts; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. Learning to ask such deep questions has intrinsic research value concerning how human intelligence embodies the skills of curiosity and integration, and will have broad application in future intelligent systems. Despite a clear push towards answering deep questions (exemplified by multi-hop reading comprehension (Cao et al., 2019) and commonsense QA (Rajani et al., 2019)), generating deep questions remains un-investigated. There is thus a clear need to push QG research towards generating deep questions that demand higher cognitive skills. In this paper, we propose the problem of Deep Question Generation (DQG), which aims to generate questions that require reasoning over multiple pieces of information in the passage. Figure 1 b) shows an example of deep question which requires a comparative reasoning over two disjoint pieces of evidences. DQG introduces three additional challenges that are not captured by traditional QG systems. First, unlike generating questi"
2020.acl-main.135,W17-2603,0,0.0409227,"Missing"
2020.acl-main.135,D18-1424,0,0.249997,"xample of Deep Question Generation Figure 1: Examples of shallow/deep QG. The evidence needed to generate the question are highlighted. Introduction Question Generation (QG) systems play a vital role in question answering (QA), dialogue system, and automated tutoring applications – by enriching the training QA corpora, helping chatbots start conversations with intriguing questions, and automatically generating assessment questions, respectively. Existing QG research has typically focused on generating factoid questions relevant to one fact obtainable from a single sentence (Duan et al., 2017; Zhao et al., 2018; Kim et al., 2019), as exemplified in Figure 1 a). However, less explored has been the comprehension and reasoning aspects of questioning, resulting in questions that are shallow and not reflective of the true creative human process. People have the ability to ask deep questions about events, evaluation, opinions, synthesis, or reasons, usually in the form of Why, Why-not, How, What-if, which requires an in-depth understanding of the input source and the ability to reason over disjoint relevant contexts; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy no"
2020.acl-main.135,P17-1099,0,0.0242224,"2 as an example); therefore its constituting words are aligned with the same node representation; (3) keyword information: a word (e.g., a preposition) not appearing in the semantic graph is aligned with the special node vector mentioned before, indicating the word does not carry important information. 3.4 Joint Task Question Generation Based on the semantic-rich input representations, we generate questions via jointly training on two tasks: Question Decoding and Content Selection. Question Decoding. We adopt an attention-based GRU model (Bahdanau et al., 2014) with copying (Gu et al., 2016; See et al., 2017) and coverage mechanisms (Tu et al., 2016) as the question decoder. The decoder takes the semantic-enriched representations ED = {ei , ∀wi ∈ D} from the encoders as the attention memory to generate the output sequence one word at a time. To make the decoder aware of the answer, we use the average word embeddings in the answer to initialize the decoder hidden states. At each decoding step t, the model learns to attend over the input representations ED and compute a context vector ct based on ED and the current decoding state st . Next, the copying probability Pcpy ∈ [0, 1] is calculated from th"
2020.acl-main.135,D18-1427,0,0.0727727,"Missing"
2020.acl-main.135,P16-1008,0,0.0231203,"ng words are aligned with the same node representation; (3) keyword information: a word (e.g., a preposition) not appearing in the semantic graph is aligned with the special node vector mentioned before, indicating the word does not carry important information. 3.4 Joint Task Question Generation Based on the semantic-rich input representations, we generate questions via jointly training on two tasks: Question Decoding and Content Selection. Question Decoding. We adopt an attention-based GRU model (Bahdanau et al., 2014) with copying (Gu et al., 2016; See et al., 2017) and coverage mechanisms (Tu et al., 2016) as the question decoder. The decoder takes the semantic-enriched representations ED = {ei , ∀wi ∈ D} from the encoders as the attention memory to generate the output sequence one word at a time. To make the decoder aware of the answer, we use the average word embeddings in the answer to initialize the decoder hidden states. At each decoding step t, the model learns to attend over the input representations ED and compute a context vector ct based on ED and the current decoding state st . Next, the copying probability Pcpy ∈ [0, 1] is calculated from the context vector ct , the decoder state st"
2020.acl-main.135,D18-1259,0,0.200913,"ncorporates an attention mechanism into the Gated Graph Neural Network (GGNN) (Li et al., 2016), to dynamically model the interactions between different semantic relations; (2) enhancing the word-level passage embeddings and the node-level semantic graph representations to obtain an unified semantic-aware passage representations for question decoding; and (3) introducing an auxiliary content selection task that jointly trains with question decoding, which assists the model in selecting relevant contexts in the semantic graph to form a proper reasoning chain. We evaluate our model on HotpotQA (Yang et al., 2018), a challenging dataset in which the questions are generated by reasoning over text from separate Wikipedia pages. Experimental results show that our model — incorporating both the use of the semantic graph and the content selection task — improves performance by a large margin, in terms of both automated metrics (Section 4.3) and human evaluation (Section 4.5). Error analysis (Section 4.6) validates that our use of the semantic graph greatly reduces the amount of semantic errors in generated questions. In summary, our contributions are: (1) the very first work, to the best of our knowledge, t"
2020.acl-main.135,C12-1030,0,\N,Missing
2020.acl-main.135,P16-1154,0,\N,Missing
2020.acl-main.135,D16-1264,0,\N,Missing
2020.acl-main.135,P17-1149,0,\N,Missing
2020.acl-main.135,D17-1090,0,\N,Missing
2020.acl-main.135,P18-1177,0,\N,Missing
2020.acl-main.135,N19-1240,0,\N,Missing
2020.acl-main.578,D17-1209,0,0.0551015,"Missing"
2020.acl-main.578,P19-1140,0,0.216946,"common neighbors between different KGs. By aligning entities from different knowledge graphs (KGs) to the same real-world identity, entity alignment is a powerful technique for knowledge integration. Unfortunately, entity alignment is nontrivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures. In recent years, embedding-based methods have become the dominated approach for entity alignment (Zhu et al., 2017; Pei et al., 2019a; Cao et al., 2019; Xu et al., 2019; Li et al., 2019a; Sun et al., Corresponding author. 纽约 子行政区 Introduction ∗ subdivision 最大城市 子行政区 布鲁克林区 KG2: Brooklyn # neighbors: 21 # common neighbors: 2 2020). Such approaches have the advantage of not relying on manually constructed features or rules (Mahdisoltani et al., 2015). Using a set of seed alignments, an embedding-based method models the KG structures to automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities. The"
2020.acl-main.578,D19-1274,0,0.313861,"KGs. By aligning entities from different knowledge graphs (KGs) to the same real-world identity, entity alignment is a powerful technique for knowledge integration. Unfortunately, entity alignment is nontrivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures. In recent years, embedding-based methods have become the dominated approach for entity alignment (Zhu et al., 2017; Pei et al., 2019a; Cao et al., 2019; Xu et al., 2019; Li et al., 2019a; Sun et al., Corresponding author. 纽约 子行政区 Introduction ∗ subdivision 最大城市 子行政区 布鲁克林区 KG2: Brooklyn # neighbors: 21 # common neighbors: 2 2020). Such approaches have the advantage of not relying on manually constructed features or rules (Mahdisoltani et al., 2015). Using a set of seed alignments, an embedding-based method models the KG structures to automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities. The vast majority of prior works in t"
2020.acl-main.578,D17-1159,0,0.0254039,"ead for training data labeling. AliNet considers all one-hop neighbors of an entity to be equally important when aggregating information. However, not all one-hop neighbors contribute positively to characterizing the target entity. Thus, considering all of them without careful selection can introduce noise and degrade the performance. NMN avoids these pitfalls. With only a small set of pre-aligned entities as training data, NMN chooses the most informative neighbors for entity alignment. Graph neural networks. GNNs have recently been employed for various NLP tasks like semantic role labeling (Marcheggiani and Titov, 2017) and machine translation (Bastings et al., 2017). GNNs learn node representations by recursively aggregating the representations of neighboring nodes. There are a range of GNN variants, including the Graph Convolutional Network (GCN) (Kipf and Welling, 2017), the Relational Graph Convolutional Network (Schlichtkrull et al., 2018), the Graph Attention Network (Veliˇckovi´c et al., 2018). Giving the powerful capability for modeling graph structures, we also leverage GNNs to encode the structural information of KGs (Sec. 3.2). and Lau, 2013). In our entity alignment framework, we propose a vertex"
2020.acl-main.578,P18-1187,0,0.029954,"regation d(e1, e2) G2 e2 e2 e2 e2 Neighborhood Aggregation Figure 2: Overall architecture and processing pipeline of Neighborhood Matching Network (NMN). (l) (l) (l) (l) (l) where {h1 , h2 , ..., hn |hi ∈ Rd } is the output node (entity) features of l-th GCN layer, i is the normalization constant, Ni is the set of neigh(l) (l−1) bor indices of entity i, and W(l) ∈ Rd ×d is a layer-specific trainable weight matrix. To control the accumulated noise, we also introduce highway networks (Srivastava et al., 2015) to GCN layers, which can effectively control the noise propagation across GCN layers (Rahimi et al., 2018; Wu et al., 2019b). 3.3 Neighborhood Sampling The one-hop neighbors of an entity are key to determine whether the entity should be aligned with other entities. However, as we have discussed in Sec. 1, not all one-hop neighbors contribute positively for entity alignment. To choose the right neighbors, we apply a down-sampling process to select the most informative entities towards the central target entity from its one-hop neighbors. Recall that we use pre-trained word embeddings of entity names to initialize the input node features of GCNs. As a result, the entity embeddings learned by GCNs c"
2020.acl-main.578,D18-1032,0,0.0842916,"ng mechanism to jointly compare discriminative subgraphs of two entities for robust entity alignment (Sec. 3.4). 2 Related Work Embedding-based entity alignment. In recent years, embedding-based methods have emerged as viable means for entity alignment. Early works in the area utilize TransE (Bordes et al., 2013) to embed KG structures, including MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), NAEA (Zhu et al., 2019) and OTEA (Pei et al., 2019b). Some more recent studies use GNNs to model the structures of KGs, including GCN-Align (Wang et al., 2018), GMNN (Xu et al., 2019), RDGCN (Wu et al., 2019a), AVR-GCN (Ye et al., 2019), and HGCN-JE (Wu et al., 2019b). Besides the structural information, some recent methods like KDCoE (Chen et al., 2018), AttrE (Trisedya et al., 2019), MultiKE (Zhang et al., 2019) and HMAN 6478 (Yang et al., 2019) also utilize additional information like Wikipedia entity descriptions and attributes to improve entity representations. However, all the aforementioned methods ignore the neighborhood heterogeneity of KGs. MuGNN (Cao et al., 2019) and AliNet (Sun et al., 2020) are two most recent efforts for addressing th"
2020.acl-main.578,P19-1304,1,0.694509,"etween different KGs. By aligning entities from different knowledge graphs (KGs) to the same real-world identity, entity alignment is a powerful technique for knowledge integration. Unfortunately, entity alignment is nontrivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures. In recent years, embedding-based methods have become the dominated approach for entity alignment (Zhu et al., 2017; Pei et al., 2019a; Cao et al., 2019; Xu et al., 2019; Li et al., 2019a; Sun et al., Corresponding author. 纽约 子行政区 Introduction ∗ subdivision 最大城市 子行政区 布鲁克林区 KG2: Brooklyn # neighbors: 21 # common neighbors: 2 2020). Such approaches have the advantage of not relying on manually constructed features or rules (Mahdisoltani et al., 2015). Using a set of seed alignments, an embedding-based method models the KG structures to automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities. The vast majority of"
2020.acl-main.578,D19-1451,0,0.0505817,"(Bordes et al., 2013) to embed KG structures, including MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), NAEA (Zhu et al., 2019) and OTEA (Pei et al., 2019b). Some more recent studies use GNNs to model the structures of KGs, including GCN-Align (Wang et al., 2018), GMNN (Xu et al., 2019), RDGCN (Wu et al., 2019a), AVR-GCN (Ye et al., 2019), and HGCN-JE (Wu et al., 2019b). Besides the structural information, some recent methods like KDCoE (Chen et al., 2018), AttrE (Trisedya et al., 2019), MultiKE (Zhang et al., 2019) and HMAN 6478 (Yang et al., 2019) also utilize additional information like Wikipedia entity descriptions and attributes to improve entity representations. However, all the aforementioned methods ignore the neighborhood heterogeneity of KGs. MuGNN (Cao et al., 2019) and AliNet (Sun et al., 2020) are two most recent efforts for addressing this issue. While promising, both models still have drawbacks. MuGNN requires both pre-aligned entities and relations as training data, which can have expensive overhead for training data labeling. AliNet considers all one-hop neighbors of an entity to be equally important when aggregating inf"
2020.acl-main.578,D19-1023,1,0.824616,"raphs of two entities for robust entity alignment (Sec. 3.4). 2 Related Work Embedding-based entity alignment. In recent years, embedding-based methods have emerged as viable means for entity alignment. Early works in the area utilize TransE (Bordes et al., 2013) to embed KG structures, including MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), NAEA (Zhu et al., 2019) and OTEA (Pei et al., 2019b). Some more recent studies use GNNs to model the structures of KGs, including GCN-Align (Wang et al., 2018), GMNN (Xu et al., 2019), RDGCN (Wu et al., 2019a), AVR-GCN (Ye et al., 2019), and HGCN-JE (Wu et al., 2019b). Besides the structural information, some recent methods like KDCoE (Chen et al., 2018), AttrE (Trisedya et al., 2019), MultiKE (Zhang et al., 2019) and HMAN 6478 (Yang et al., 2019) also utilize additional information like Wikipedia entity descriptions and attributes to improve entity representations. However, all the aforementioned methods ignore the neighborhood heterogeneity of KGs. MuGNN (Cao et al., 2019) and AliNet (Sun et al., 2020) are two most recent efforts for addressing this issue. While promising, both models still hav"
2020.emnlp-main.591,Q17-1010,0,0.0118385,"ities involved. Figure 1 shows the architecture of our model. IEN consists of three levels. The bottom is for word-level language comprehension, which encodes words to distributed vectors. The middle is for sentence-level process understanding, which conducts entity state tracking. At the top, we use different classifiers to predict entities’ actions and locations, respectively. 1 http://www.nltk.org/ Word-Level Encoding Given a procedural text, our model first encodes each word wi in the paragraph to a vector wi = [emb(wi ); vi ]. Here, emb(wi ) is an embedding function, and we use fastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018) for experiments. vi is a scalar binary indicator for identifying whether wi is a verb. Then we use a BiLSTM (Hochreiter and Schmidhuber, 1997) over the whole paragraph for contextual encoding. We denote ui = BiLSTM([wi ]) as the output of BiLSTM with respect to word wi . Sentence-Level Encoding and IEN cell To track the state changes, we extract sentence features from word-level encodings by running another RNN at the sentence level. In order to take entity interactions into consideration, we propose a novel IEN cell that leverages attention mechanisms to help e"
2020.emnlp-main.591,N18-1144,0,0.262776,"ly find that carbon dioxide is 7281 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7281–7290, c November 16–20, 2020. 2020 Association for Computational Linguistics just taken away from the blood thus impossible to be in the blood. Furthermore if we look back to the first sentence, we can finally get the right answer, lung. This tells us that an entity’s location is closely related or even determined by the most recent event it involves in. However, current systems either model a general state of an entity regardless of specific tracking targets (Dalvi et al., 2018), or model different tracking targets (e.g., action, location) completely separately (Gupta and Durrett, 2019), without considering the relationship between different tracking targets. In this work, we focus on the scientific process understanding task (Dalvi et al., 2018), in which the tracking targets are action and location of entities. We propose a novel Interactive Entity Network, IEN, that explicitly models the interactions among multiple entities and explores the relationship between an entity’s action and its location. IEN is a two-layer RNN model, the bottom RNN encodes word-level inf"
2020.emnlp-main.591,N19-1423,0,0.0522405,"Missing"
2020.emnlp-main.591,N19-1244,0,0.461156,"tate changes described in each individual sentence. ProGlobal (Dalvi et al., 2018) considers the entire paragraph while predicting states for an entity. As the whole context is incorporated, more state changes are captured, and result in a higher recall. However, this may lead to over-prediction. To address these problems, several models are proposed to incorporate different constraints. ProStruct (Tandon et al., 2018) reformulate the procedural text comprehension task as a structured prediction task, and incorporates a set of commonsense constraints for globally consistent predictions. LACE (Du et al., 2019) leverages label consistency among different paragraphs on the same topic during training. NCET (Gupta and Durrett, 2019) uses a neural CRF to explicitly capture the constrains. Other interesting attempts include KG-MRC (Das et al., 2018), which constructs dynamic bipartite graphs from the procedural text, and updates the graphs 7282 Figure 1: The left is an example of the procedural text comprehension task and ProPara dataset. The paragraph describes the process of photosynthesis in five sentences, and the system needs to track the actions and locations of the three entities listed below, wat"
2020.emnlp-main.591,W19-1502,0,0.513264,"nguage Processing, pages 7281–7290, c November 16–20, 2020. 2020 Association for Computational Linguistics just taken away from the blood thus impossible to be in the blood. Furthermore if we look back to the first sentence, we can finally get the right answer, lung. This tells us that an entity’s location is closely related or even determined by the most recent event it involves in. However, current systems either model a general state of an entity regardless of specific tracking targets (Dalvi et al., 2018), or model different tracking targets (e.g., action, location) completely separately (Gupta and Durrett, 2019), without considering the relationship between different tracking targets. In this work, we focus on the scientific process understanding task (Dalvi et al., 2018), in which the tracking targets are action and location of entities. We propose a novel Interactive Entity Network, IEN, that explicitly models the interactions among multiple entities and explores the relationship between an entity’s action and its location. IEN is a two-layer RNN model, the bottom RNN encodes word-level information, and the upper RNN encodes the sentence-level information while keeping tracking entities’ states. Sp"
2020.emnlp-main.591,D15-1114,0,0.0244217,"nd their subsequent state changes. 2) We conduct intensive experiments to show how our IEN learns to encourage the synergy among different entities involved in one event, and explain how multiple tracking targets can be properly leveraged to improve context reasoning. 2 Related Work Recently, many procedural text comprehension datasets are constructed and relesed to prompt the research in this direction. bAbI (Weston et al., 2015) is a QA dataset that the questions are about movements of entities, however it is synthetically generated and the language expression is relatively simple. RECIPES (Kiddon et al., 2015) dataset introduces the task of predicting the locations of cooking ingredients. ProPara (Dalvi et al., 2018) includes scientific procedural paragraphs, and the task is to predict the entities’ actions and locations. In this paper, we continue this line of exploration using ProPara. The solutions are mainly RNN based or memory network based. Most early models are designed for QA task, e.g., bAbI, and thus researchers pay more attention to question processing. EntNet (Henaff et al., 2016) uses dynamic memories to maintain entity states, with a gated update at each step. These states are decoded"
2020.emnlp-main.591,D18-1006,0,0.408509,"erves each context sentence through time. More recently, ProPara becomes the popular testbed and methods on ProPara focus more on state tracking. ProLocal (Dalvi et al., 2018) locally predicts the state changes described in each individual sentence. ProGlobal (Dalvi et al., 2018) considers the entire paragraph while predicting states for an entity. As the whole context is incorporated, more state changes are captured, and result in a higher recall. However, this may lead to over-prediction. To address these problems, several models are proposed to incorporate different constraints. ProStruct (Tandon et al., 2018) reformulate the procedural text comprehension task as a structured prediction task, and incorporates a set of commonsense constraints for globally consistent predictions. LACE (Du et al., 2019) leverages label consistency among different paragraphs on the same topic during training. NCET (Gupta and Durrett, 2019) uses a neural CRF to explicitly capture the constrains. Other interesting attempts include KG-MRC (Das et al., 2018), which constructs dynamic bipartite graphs from the procedural text, and updates the graphs 7282 Figure 1: The left is an example of the procedural text comprehension"
2020.emnlp-main.591,P14-5010,0,0.00602419,"Missing"
2020.emnlp-main.591,D16-1147,0,0.0251317,"lj in sentence st . Then, ( e [ut i ; uvt ], if ei ∈ st ei xt = (1) 0, otherwise l xtj = ( l [utj ; uvt ], if 0, lj ∈ st otherwise (2) where uet , ult and uvt denote the contextual encodings of the entity, location candidate and the 7284 predicate verb, respectively. If the entity or location candidate consists of multiple words, mean pooling over the word representations is used. We stack all the entity representations together to get xet ∈ Rn×d , and similarly get xlt ∈ Rm×d . xet and xlt are the inputs to the tth IEN cell. Inspired by GRU (Chung et al., 2014) and keyvalue memory networks (Miller et al., 2016), we place memory slots inside IEN cells and let them recurrently update as GRU. In each IEN cell, there are n entity slots and m location slots, corresponding to the given entity set E and the extracted location candidate set L, respectively. Each memory slot represents the state of a specific entity or a location candidate. We use het ∈ Rn×d to represent all the entity memory slots in the tth IEN cell, and use hlt ∈ Rm×d to represent all the location memory slots in the tth IEN cell. The detailed structure of an IEN cell is shown in Figure 2. First, we update the entity memory slots as follo"
2020.emnlp-main.591,N18-1202,0,0.0258934,"architecture of our model. IEN consists of three levels. The bottom is for word-level language comprehension, which encodes words to distributed vectors. The middle is for sentence-level process understanding, which conducts entity state tracking. At the top, we use different classifiers to predict entities’ actions and locations, respectively. 1 http://www.nltk.org/ Word-Level Encoding Given a procedural text, our model first encodes each word wi in the paragraph to a vector wi = [emb(wi ); vi ]. Here, emb(wi ) is an embedding function, and we use fastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018) for experiments. vi is a scalar binary indicator for identifying whether wi is a verb. Then we use a BiLSTM (Hochreiter and Schmidhuber, 1997) over the whole paragraph for contextual encoding. We denote ui = BiLSTM([wi ]) as the output of BiLSTM with respect to word wi . Sentence-Level Encoding and IEN cell To track the state changes, we extract sentence features from word-level encodings by running another RNN at the sentence level. In order to take entity interactions into consideration, we propose a novel IEN cell that leverages attention mechanisms to help entities or locations get inform"
2020.emnlp-main.591,D16-1264,0,0.0682442,"Missing"
2020.findings-emnlp.350,D14-1179,0,0.00581195,"Missing"
2020.findings-emnlp.350,P16-1195,0,0.515381,"ur layers since we find using more layers does not improve the results. Each of the GAT layers has a residual connection to avoid gradient vanishment except for the last layer. We set the dropout (Srivastava et al., 2014) rate to 0.1, the weight decay rate to 0.0001 and the batch size to 20. We use Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. To reduce randomness, we run each model setting five times and take the average as the report performance. 3.5 Baseline Models We compare our model with three types of models, described as follows. The first kind of models, CodeNN (Iyer et al., 2016) and Seq2Seq (Luong et al., 2015), treat source code as a sequential sequence of words. CodeNN is a modified language model with attention mechanism, and it is among the earliest NN models for code comment generation. Seq2Seq is the most widely used model for many generation tasks. For these models, we use a bidirectional GRU as the encoder and a GRU with attention as the decoder. The second kind of models incorporate information from the AST into the model and exploit the structural information of the AST to assist comment generation. We choose two state-ofthe-art AST-based generation models:"
2020.findings-emnlp.350,W04-1013,0,0.063184,"Missing"
2020.findings-emnlp.350,D15-1166,0,0.16432,"nd guard against mismatch and obsolete comments is to automatically generate them. Classical approaches for auto-comment generation use hand-crafted templates to produce code descriptions (Sridhara et al., 2010; Cortes-Coy et al., 2014; Dawood et al., 2017), but suffer from poor scalability and high maintenance cost due to the expensive overhead of writing comment templates. More recent work takes a learning-based approach by employing neural network (NN) models developed for natural language processing tasks like machine translation to automatically generate comments (Sutskever et al., 2014; Luong et al., 2015). Compared to hand-written templates, a learningbased approach based on empirical data is more scalable and sustainable. The key to generating high-quality comments is to utilize as much relevant information as possible from the source code to infer the high-level algorithmic intents. Prior work achieves this by converting a representation of the program, e.g., an Abstract Syntax Tree (AST), into a sequential sequence where a sequential model like LSTM can be applied to translate the token sequence into natural language descriptions (Hu et al., 2018a; Alon et al., 2019; LeClair et al., 2019)."
2020.findings-emnlp.350,P02-1040,0,0.108303,"Missing"
2020.findings-emnlp.350,P17-1099,0,0.0347233,"s for an activation function, usually LeakyReLU and [·||·] means the concatenation of two matrices. We repeat this neighborhood aggregation process L times, and get the final state for each vertex of the C-Graph, represented as {giL |vi ∈ Vf }. We use the final state of the target function gtL as the global representation. 2.3 where qj and qk are the contextual embeddings of words xi and xj in the input sequence of the target function and Wla is a trainable parameter. Pointer. As source codes may contain information which can be directly used in comment, we propose to add a pointer mechanism (See et al., 2017) which can copy useful words from source codes. Pointer mechanism merges a copy distribution with a normal output prediction distribution. In the ith decoding step, Pvocab = sof tmax(Wv [hi ||ci ||cgi ] + bv ) (9) Decoder As our encoders embed both local and global information, the decoder needs to integrate information extracted at different levels and takes the integrated information into consideration during the generation of comment tokens. Once again, we adopt a GRU as decoder and use the concatenation of local representation qn and global representation gtL as its initial state. hi is th"
2021.acl-long.433,D18-1216,0,0.0180143,"regular expressions for text classification tasks. Most of these works provide effective ways to utilize word-level knowledge, but none of them formally considers the quality issues with the distantly-labeled rationales. Additionally, Poulis and Dasgupta (2017) discuss the insufficiency issue in the feature feedback framework, and try to incorporate vague feature feedback into a linear classifier. As a widely-used explanation method, the attention mechanism is often applied with constraints to guide model focus towards the significant part of inputs (Liu et al., 2017; Nguyen and Nguyen, 2018; Bao et al., 2018). Our proposed methods are currently based on gradient-based salience calculation, which is easier to obtain and model-agnostic, thus can be applied to a wider range with ease. But our methods do not depend on specific calculation methods for word salience, and can be easily transplanted to attention-based constraints, which we will leave for future work. Recent studies have provided various techniques to constrain gradient-based word salience. Ross et al. (2017) forces the gradient of features, which are annotated non-helpful, to be zero, to alter the decision boundary of the model. Liu and A"
2021.acl-long.433,N19-1404,0,0.206367,"rent rationale words may exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw correct conclusions, the rigid requirements may tu"
2021.acl-long.433,P08-1030,0,0.170866,"Missing"
2021.acl-long.433,2020.emnlp-main.258,0,0.140089,"ling to take ::::: Labeled a chance will be rewarded with two of the year’s most accomplished and riveting film per::::: formance. Table 1: An example of rationale annotation for sentiment analysis. Words in underline are rationales annotated by human experts, and words in wavy underline ::::: are annotated via sentiment lexicon matching. Numbers in [] are salience scores labeled by experts. Introduction Recent studies have shown an increasing interest in incorporating human knowledge into neural network models (Xu et al., 2018; Vashishth et al., 2018; Luo et al., 2018; Li and Srikumar, 2019; Jiang et al., 2020). For many natural language processing (NLP) tasks, such domain knowledge often refers to salient words annotated by human experts, which are also called rationales. Table 1 (top) shows an example of expert-annotated rationales for sentiment analysis, which highlight noteworthy tokens and score the contributions of these tokens. The detailed annotations reflect the importance of these words from the expert annotator’s viewpoint and are expected to help training better sentiment classification models. ∗ Corresponding author. Nonetheless, careful, case-by-case rationale annotations inevitably in"
2021.acl-long.433,D16-1011,0,0.0545267,"Missing"
2021.acl-long.433,N16-1082,0,0.028393,"indicates how much xi contributes to the model f . Prior works have explored different methods to determine word salience (Ribeiro et al., 2016; Jin et al., 2020). Among them, we choose gradientbased methods since they are model-agnostic and easy to obtain. Moreover, since gradient-based 5572 word salience is differentiable with respect to model parameters, taking it as part of the objective makes it more convenient to optimize the loss. For a function f , the magnitude (absolute value) of its gradients with respect to input x indicates how sensitive the final decision is to the change of x (Li et al., 2016). In most NLP settings, the gradient of a word is the sum of gradients for each dimension of word embeddings. Formally, the gradient of an input word xi to a function f can be calculated as: ∂f (1) gi = ∂xi 1 where k·k1 is the L1 norm that sums up the absolute value of gradients over the embedding dimensions. For gradient-based methods, we use the normalized gradients to calculate word salience, which represents the proportion of a word’s contribution in a sentence: gi si = Pn (2) j=1 gj There exist more complicated gradient-based methods for calculating word salience (Sundararajan et al., 201"
2021.acl-long.433,P19-1028,0,0.0231312,"watch, but viewers willing to take ::::: Labeled a chance will be rewarded with two of the year’s most accomplished and riveting film per::::: formance. Table 1: An example of rationale annotation for sentiment analysis. Words in underline are rationales annotated by human experts, and words in wavy underline ::::: are annotated via sentiment lexicon matching. Numbers in [] are salience scores labeled by experts. Introduction Recent studies have shown an increasing interest in incorporating human knowledge into neural network models (Xu et al., 2018; Vashishth et al., 2018; Luo et al., 2018; Li and Srikumar, 2019; Jiang et al., 2020). For many natural language processing (NLP) tasks, such domain knowledge often refers to salient words annotated by human experts, which are also called rationales. Table 1 (top) shows an example of expert-annotated rationales for sentiment analysis, which highlight noteworthy tokens and score the contributions of these tokens. The detailed annotations reflect the importance of these words from the expert annotator’s viewpoint and are expected to help training better sentiment classification models. ∗ Corresponding author. Nonetheless, careful, case-by-case rationale anno"
2021.acl-long.433,P19-1631,0,0.313689,"ay exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw correct conclusions, the rigid requirements may turn out to incorrectly"
2021.acl-long.433,P17-1164,0,0.339404,"ly helpful, given a specific context, different rationale words may exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw cor"
2021.acl-long.433,P18-1194,1,0.943051,"stantly Painful to watch, but viewers willing to take ::::: Labeled a chance will be rewarded with two of the year’s most accomplished and riveting film per::::: formance. Table 1: An example of rationale annotation for sentiment analysis. Words in underline are rationales annotated by human experts, and words in wavy underline ::::: are annotated via sentiment lexicon matching. Numbers in [] are salience scores labeled by experts. Introduction Recent studies have shown an increasing interest in incorporating human knowledge into neural network models (Xu et al., 2018; Vashishth et al., 2018; Luo et al., 2018; Li and Srikumar, 2019; Jiang et al., 2020). For many natural language processing (NLP) tasks, such domain knowledge often refers to salient words annotated by human experts, which are also called rationales. Table 1 (top) shows an example of expert-annotated rationales for sentiment analysis, which highlight noteworthy tokens and score the contributions of these tokens. The detailed annotations reflect the importance of these words from the expert annotator’s viewpoint and are expected to help training better sentiment classification models. ∗ Corresponding author. Nonetheless, careful, case"
2021.acl-long.433,2020.acl-main.419,0,0.0704263,"Missing"
2021.acl-long.433,C18-1193,0,0.310181,"a specific context, different rationale words may exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw correct conclusions, the rig"
2021.acl-long.433,D13-1170,0,0.00833621,"Missing"
2021.acl-long.433,P15-2060,0,0.0677755,"Missing"
2021.acl-long.433,D14-1162,0,0.0923824,"Missing"
2021.acl-long.433,N16-3020,0,0.0736573,"ated rationales, even in a lower quality. 2 Word Salience Before elaborating on our proposed methods, we first introduce the definition of word salience, a measure of the importance of words, which is widely applied in previous works (Luo et al., 2018; Nguyen and Nguyen, 2018; Jin et al., 2020) Given a model f and an input word sequence x = (x1 , x2 , ..., xn ), the word salience is a vector s = (s1 , s2 , ..., sn ) that denotes the importance of every word in x, where si indicates how much xi contributes to the model f . Prior works have explored different methods to determine word salience (Ribeiro et al., 2016; Jin et al., 2020). Among them, we choose gradientbased methods since they are model-agnostic and easy to obtain. Moreover, since gradient-based 5572 word salience is differentiable with respect to model parameters, taking it as part of the objective makes it more convenient to optimize the loss. For a function f , the magnitude (absolute value) of its gradients with respect to input x indicates how sensitive the final decision is to the change of x (Li et al., 2016). In most NLP settings, the gradient of a word is the sum of gradients for each dimension of word embeddings. Formally, the grad"
2021.acl-long.433,D18-1157,0,0.056442,"Missing"
2021.acl-long.433,D19-1420,0,0.033442,"Missing"
2021.acl-long.433,N07-1033,0,0.183416,"Missing"
2021.acl-short.126,D19-1498,0,0.0288646,"Missing"
2021.acl-short.126,P16-1200,0,0.101815,"Missing"
2021.acl-short.126,2020.acl-main.141,0,0.0305485,"Missing"
2021.acl-short.126,Q17-1008,0,0.0560226,"Missing"
2021.acl-short.126,D14-1162,0,0.0859413,"Missing"
2021.acl-short.126,E17-1110,0,0.0293799,"d and tail entities respectively, and S∗ represents a sentence. both the head and tail entities, the sentence itself can be seen as a path (the intra-sentence case). For more complex situations where the head and tail entities do not co-occur in one sentence, we define the following 3 types of paths which indicate how the head and tail entities can be possibly related in the context. Figure 2 provides a visualization of the three types of paths. Consecutive Paths Previous studies have shown that the majority of inter-sentence relations are often in nearby text (Swampillai and Stevenson, 2010; Quirk and Poon, 2017). We thus select the consecutive sentences to form a path when the head and tail entities are in nearby sentences. Formally, if one mention of the head entity appears in sentence Si and one mention of the tail entity is in sentence Sj , these two sentences along with the sentence in between, i.e., sentence Si+1 , . . . , Sj−1 (or Sj+1 , . . . , Si−1 when i ≥ j) forms a possible path that connects the two entities. Given that no more than 3 sentences would suffice for inference, we limit the length of these Consecutive Paths to be at most 3, which means |j − i |≤ 2. Note that this definition ca"
2021.acl-short.126,swampillai-stevenson-2010-inter,0,0.0436497,"d et stands for a mention of head and tail entities respectively, and S∗ represents a sentence. both the head and tail entities, the sentence itself can be seen as a path (the intra-sentence case). For more complex situations where the head and tail entities do not co-occur in one sentence, we define the following 3 types of paths which indicate how the head and tail entities can be possibly related in the context. Figure 2 provides a visualization of the three types of paths. Consecutive Paths Previous studies have shown that the majority of inter-sentence relations are often in nearby text (Swampillai and Stevenson, 2010; Quirk and Poon, 2017). We thus select the consecutive sentences to form a path when the head and tail entities are in nearby sentences. Formally, if one mention of the head entity appears in sentence Si and one mention of the tail entity is in sentence Sj , these two sentences along with the sentence in between, i.e., sentence Si+1 , . . . , Sj−1 (or Sj+1 , . . . , Si−1 when i ≥ j) forms a possible path that connects the two entities. Given that no more than 3 sentences would suffice for inference, we limit the length of these Consecutive Paths to be at most 3, which means |j − i |≤ 2. Note"
2021.acl-short.126,P19-1074,0,0.0159357,"−1 when i ≥ j) forms a possible path that connects the two entities. Given that no more than 3 sentences would suffice for inference, we limit the length of these Consecutive Paths to be at most 3, which means |j − i |≤ 2. Note that this definition can be naturally extended to the intra-sentence case where j = i. We thus consider the intra-sentence case as a type of the Consecutive Path. A pair of entities can correspond to multiple consecutive paths since they can be mentioned more than once. Multi-Hop Paths Another typical case for intersentence relation instances is the multi-hop relation (Yao et al., 2019; Zeng et al., 2020a). In such cases, the head and tail entities are far from each other in the document but can be connected through bridge entities, just like the entity The Espoo Cathedral in Figure 1 bridges the EC Parish and Finland in sentence 1 and 6. For these cases, we start from the head entity, go through all the bridge entities, arrive at the tail entity, and select all the corresponding sentences in this route as a path. Formally, for the head entity eh and the tail entity et , the multi-hop relation indicates that there exist a list of bridge entities eb1 , . . . , ebk such that"
2021.acl-short.126,2020.emnlp-main.127,0,0.287353,"have released our code at https://github.com/AndrewZhe/ThreeSentences-Are-All-You-Need. 1 Figure 1: A case extracted from the DocRED dataset. While the document has 6 sentences, only 1 or 2 sentences form the evidence for each relation instance. Introduction The task of relation extraction (RE) focuses on extracting relations between entity pairs in texts, and has played an important role in information extraction. While earlier works focus on extracting relations within a sentence (Lin et al., 2016; Zhang et al., 2018), recent studies begin to explore RE at document level (Peng et al., 2017; Zeng et al., 2020a; Nan et al., 2020a), which is more challenging as it often requires reasoning across multiple sentences. Compared with sentence level extraction, documents are significantly longer with useful information scattered in a larger scale. However, given a pair of entities, one may only need a few sentences, not the entire document, to infer their relationship; reading the whole document may not be necessary, since it may introduce unrelated information inevitably. As we can see in Figure 1, S[1] is sufficient to recognize Finland as the country of Espoo, and recognizing the rest two instances req"
2021.acl-short.126,D18-1244,0,0.0505205,"Missing"
2021.findings-acl.85,D14-1162,0,0.103024,"Missing"
2021.findings-acl.85,D16-1264,0,0.0515683,"The paraphrasing challenge requires MRC models to identify the same meaning represented in different words. Regarding the shortcut tricks, we study two typical kinds: question word matching (QWM) and simple matching (SpM) (Sugawara et al., 2018). For QWM, MRC models can simply obtain an answer phrase by recognizing the expected entity type confined by the wh-question words of question Q. For SpM, a model can find the answers by identifying the word overlap between answer sentences and the questions. QWM-Para Dataset: As elaborated in Algorithm 1, given an original instance (Q, P ) from SQuAD (Rajpurkar et al., 2016), we paraphrase the question Q in Qp to embed the paraphrasing challenge, and derive the corresponding shortcut version by dropping the sentences containing other entities with the matched type according to the question words from the given passage. An example is shown in the left of Figure 2. In the challenging version of Q.2, both Beyonce and Lisa are person names which match the question word who. Thus, one should at least recognize the paraphrasing relationship between named the most influential music girl and rated as the most powerful female musician to distinguish between the two names"
2021.findings-acl.85,Q19-1016,0,0.08589,"training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training. 1 Paraphrasing P: ... Begun as a claims to be the oldest continuous collegiate publication in the United States … Comprehension Challenge Shortcut Figure 1: An illustration of shortcuts in Machine Reading Comprehension. P is an excerpt of the original passage. different benchmarks. These benchmarks are designed to address challenging features, such as evidence checking in multi-document inference (Yang et al., 2018), co-reference resolution (Dasigi et al., 2019), dialog understanding (Reddy et al., 2019), symbolic reasoning (Dua et al., 2019), and so on. The task of machine reading comprehension (MRC) aims at evaluating whether a model can understand natural language texts by answering a series of questions. Recently, MRC research has seen considerable progress in terms of model performance, and many models are reported to approach or even outperform human-level performance on Corresponding author. Answer the Scholastic magazine is issued twice monthly and Introduction ∗ one-page journal in September 1876 , Coreference However, recent analysis indicates that many MRC models unintentionally le"
2021.findings-acl.85,W17-2623,0,0.0190064,"n Shortcut Version Step3 Substitute & Sentence Shuﬄe Q.3: Who was rated as the most powerful female musician? P: She released a new album with Lisa… Forbes Magazine named Beyonce as the most influential music creator. Challenging Version Figure 2: An illustration of how the instances in the synthetic datasets are constructed from original SQuAD data. Each instance has a shortcut version paired with a challenging version where comprehension skills are necessary. In this work, we focus on paraphrasing (Para) as the complex reasoning challenge, since it widely exists in many recent MRC datasets (Trischler et al., 2017; Reddy et al., 2019; Clark et al., 2019). The paraphrasing challenge requires MRC models to identify the same meaning represented in different words. Regarding the shortcut tricks, we study two typical kinds: question word matching (QWM) and simple matching (SpM) (Sugawara et al., 2018). For QWM, MRC models can simply obtain an answer phrase by recognizing the expected entity type confined by the wh-question words of question Q. For SpM, a model can find the answers by identifying the word overlap between answer sentences and the questions. QWM-Para Dataset: As elaborated in Algorithm 1, give"
2021.findings-acl.85,D19-1221,0,0.0185872,"y find that manually designed features (Chen et al., 2016) or simple model architectures (Weissenborn et al., 2017) could obtain competitive performance, indicating that complicated inference procedure may be dispensable. Even without reading the entire questions or documents, models can still correctly answer a large portion of the questions (Sugawara et al., 2018; Kaushik and Lipton, 2018; Min et al., 2019). Therefore, current MRC datasets may lack the benchmarking capacity on requisite skills (Sugawara et al., 2020), and models may be vulnerable to adversarial attacks (Jia and Liang, 2017; Wallace et al., 2019; Si et al., 2019). However, they do not formally discuss or analyze why models could learn shortcuts from the perspectives of the learning procedure. On the way of designing better MRC datasets, Jiang and Bansal (2019) construct adversarial questions to guide model learning the multi-hop reasoning skills. Bartolo et al. (2020) propose a model-inloop paradigm to annotate challenging questions. More recent works (Jhamtani and Clark, 2020; Ho et al., 2020) propose new datasets with evidence based metrics to evaluate whether the questions are solved via shortcuts. Our work aims at providing empir"
2021.findings-acl.85,K17-1028,0,0.0158608,"conomic or financial factors]Ans ... P-shortcut: ... Most of these defections occur because of [economic or financial factors]Ans . Most of these errors are caused by [economic or financial factors]Ans . ... 6 Related Works Reading documents to answer natural language questions has drawn more and more attention in recent years (Xu et al., 2016; Minjoon et al., 2017; Lai et al., 2019; Glass et al., 2020). Most previous works focus on revealing the shortcut phenomenon in MRC from different perspectives. They find that manually designed features (Chen et al., 2016) or simple model architectures (Weissenborn et al., 2017) could obtain competitive performance, indicating that complicated inference procedure may be dispensable. Even without reading the entire questions or documents, models can still correctly answer a large portion of the questions (Sugawara et al., 2018; Kaushik and Lipton, 2018; Min et al., 2019). Therefore, current MRC datasets may lack the benchmarking capacity on requisite skills (Sugawara et al., 2020), and models may be vulnerable to adversarial attacks (Jia and Liang, 2017; Wallace et al., 2019; Si et al., 2019). However, they do not formally discuss or analyze why models could learn sho"
2021.findings-acl.85,C16-1226,1,0.827302,"ining with both type of questions, BERT can learn the simple matching trick earlier than identifying the required paraphrasing between why defections occur and errors caused by. Q.6: Why do these defections occur? P-challenging: ... Most of these errors are caused by [economic or financial factors]Ans ... P-shortcut: ... Most of these defections occur because of [economic or financial factors]Ans . Most of these errors are caused by [economic or financial factors]Ans . ... 6 Related Works Reading documents to answer natural language questions has drawn more and more attention in recent years (Xu et al., 2016; Minjoon et al., 2017; Lai et al., 2019; Glass et al., 2020). Most previous works focus on revealing the shortcut phenomenon in MRC from different perspectives. They find that manually designed features (Chen et al., 2016) or simple model architectures (Weissenborn et al., 2017) could obtain competitive performance, indicating that complicated inference procedure may be dispensable. Even without reading the entire questions or documents, models can still correctly answer a large portion of the questions (Sugawara et al., 2018; Kaushik and Lipton, 2018; Min et al., 2019). Therefore, current MR"
2021.findings-acl.85,D18-1259,0,0.0227192,"ns earlier than challenging questions, and the high proportions of shortcut questions in training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training. 1 Paraphrasing P: ... Begun as a claims to be the oldest continuous collegiate publication in the United States … Comprehension Challenge Shortcut Figure 1: An illustration of shortcuts in Machine Reading Comprehension. P is an excerpt of the original passage. different benchmarks. These benchmarks are designed to address challenging features, such as evidence checking in multi-document inference (Yang et al., 2018), co-reference resolution (Dasigi et al., 2019), dialog understanding (Reddy et al., 2019), symbolic reasoning (Dua et al., 2019), and so on. The task of machine reading comprehension (MRC) aims at evaluating whether a model can understand natural language texts by answering a series of questions. Recently, MRC research has seen considerable progress in terms of model performance, and many models are reported to approach or even outperform human-level performance on Corresponding author. Answer the Scholastic magazine is issued twice monthly and Introduction ∗ one-page journal in September 187"
2021.findings-emnlp.255,N19-1423,0,0.0152983,", and the average dependency tree depth of VGaokao is 1.2 times larger than that of C3 . Longer sentences and more complicated syntactic structures usually exhibit rich linguistic phenomena, thus requiring models to learn more sophisticated language understanding skills. Lastly, the passages in VGaokao, which contain 1,159 Chinese characters on average, are approximately 3 times longer than that of RACE (Lai et al., 2017) and 10 times longer than that of C3 (Sun et al., 2020). The length of most passages even exceeds the maximum input length of general pre-trained language models such as BERT(Devlin et al., 2019). To exploit the long passages, models may need to take discourse structures into consideration so as to better integrate multiple evidence sentences. As shown in the sample articles and questions from VGaokao and C3 in Table 1, the example article from VGaokao involves domain-specific terminologies such as testosterone and estrogen. The sentences in the VGaokao example are longer and involves more complicated sentence structures such as compound sentences. Besides, the options in VGaokao seem to be more confusing: Option C and D both discuss the subtle relationship between the brains and hand"
2021.findings-emnlp.255,E17-1011,0,0.0553602,"Missing"
2021.findings-emnlp.255,2020.findings-emnlp.314,0,0.0359953,"uire two evidence sentences, we set the maximum number of iterations to 2. Considering that maintaining all evidence chains during iterative extraction has exponential complexity to the steps, we use beam search, where only top 2 evidence sentences remain in each step. We use Sentence-BERT (Reimers and Gurevych, 2019, 2020) to measure the relevance between the query and the evidence chains in the adaptive integrator. For the pairwise option competition, we use Chinese RoBERTa-wwm-ext-Large (Cui et al., 2019) with Transformers toolkit (Wolf et al., 2020). We first fine-tune our model on OCNLI (Hu et al., 2020), a Chinese natural language inference dataset before fine-tuning on VGaokao, which has 8 epochs, with maximum input length 256, batch size 64, and learning rate 2e-5. 4 Experiments We conduct experiments on our proposed VGaokao dataset and compare our ExtractIntegrate-Compete approach with several baselines. RoBERTa-Large-Chunk (Liu et al., 2019) is an end-to-end method without explicit evidence retrieval. This model splits the long passages into fix-length chunks of 200 tokens. Candidate answers are obtained from each chunk using existing MRC models, which are further aggregated over all chu"
2021.findings-emnlp.255,D19-1597,0,0.0213101,"Missing"
2021.findings-emnlp.255,Q19-1026,0,0.011953,"a novel query updating mechanism. Our hinge loss based competition component can push the model to capture fine-grained differences among different choices. 3) Experiments show that our approach outperforms a variety of baselines in both evidence retrieval F1 and QA accuracy on VGaokao while showing the merits of efficiency and explainability. 2 VGaokao: Verification Style Reading Comprehension Dataset Standardized language tests have been considered as a test-bed to harvest machine reading comprehension datasets. While most existing efforts focus on SQuAD-like QA datasets (Yang et al., 2018; Kwiatkowski et al., 2019), or cloze style questions (Zhang et al., 2018; Zheng et al., 2019), few efforts are made to verification style questions. In the Chinese Language test of Gaokao, approximately half of the reading comprehension questions instruct students to select a statement (i.e., an option from four choices) that is the most consistent or contradicting with the given passage. These questions are designed to evaluate students’ ability in extracting and integrating information from long passages, and analyzing certain linguistic phenomena or semantic relations among several similar sentences3 . According to"
2021.findings-emnlp.255,D17-1082,0,0.368168,", or verify a given statement according to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actually provides an i"
2021.findings-emnlp.255,2021.ccl-1.108,0,0.0394672,"Missing"
2021.findings-emnlp.255,2020.emnlp-main.713,0,0.0294949,"Missing"
2021.findings-emnlp.255,D19-1261,0,0.0151235,"hod for solving the long article challenge in VGaokao. Iterative evidence extraction can be seen as a sort of question decomposition method, a technique widely used in QA tasks with complex questions (Talmor and Berant, 2018; Perez et al., 2020). However, in VGaokao, the queries may interweave by implicit semantic relationship, so that models could not explicitly separate the queries into independent sub-queries. We thus adopt an iteative extractor with an adaptive integrator to decompose queries in an implicit way. Another stream of works adopt an iterative framework by updating the queries. Qi et al. (2019) iteratively generate new queries by selecting a span from the question and retrieved evidence while Xu et al. (2019); Xiong et al. (2021); Zhao et al. (2021) directly append retrieved evidence to the query. Compared with these works, we introduce two novel query updating techniques, hard masking and soft masking, together with an evidence integration module to avoid too much overlap between evidence and to dynamically determine the number of required evidence sentences. 6 Conclusion In this paper, we present a novel verification style reading comprehension dataset named VGaokao from the Chine"
2021.findings-emnlp.255,D16-1264,0,0.0515677,"er a series of questions, or verify a given statement according to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actu"
2021.findings-emnlp.255,D19-1410,0,0.0332241,"Missing"
2021.findings-emnlp.255,2020.emnlp-main.365,0,0.0223656,"Missing"
2021.findings-emnlp.255,Q19-1014,0,0.0526079,"Missing"
2021.findings-emnlp.255,2020.tacl-1.10,0,0.113313,"ng to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actually provides an ideal test-bed for natural language und"
2021.findings-emnlp.255,N18-1059,0,0.0324607,"Missing"
2021.findings-emnlp.255,N18-1074,0,0.0583002,"Missing"
2021.findings-emnlp.255,2020.emnlp-demos.6,0,0.0402704,"Missing"
2021.findings-emnlp.255,N19-1301,1,0.833831,"n decomposition method, a technique widely used in QA tasks with complex questions (Talmor and Berant, 2018; Perez et al., 2020). However, in VGaokao, the queries may interweave by implicit semantic relationship, so that models could not explicitly separate the queries into independent sub-queries. We thus adopt an iteative extractor with an adaptive integrator to decompose queries in an implicit way. Another stream of works adopt an iterative framework by updating the queries. Qi et al. (2019) iteratively generate new queries by selecting a span from the question and retrieved evidence while Xu et al. (2019); Xiong et al. (2021); Zhao et al. (2021) directly append retrieved evidence to the query. Compared with these works, we introduce two novel query updating techniques, hard masking and soft masking, together with an evidence integration module to avoid too much overlap between evidence and to dynamically determine the number of required evidence sentences. 6 Conclusion In this paper, we present a novel verification style reading comprehension dataset named VGaokao from the Chinese Language tests of Gaokao for Chinese native speakers, which embed multiple advanced language understanding skills."
2021.findings-emnlp.255,D18-1259,0,0.104276,"n statement according to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actually provides an ideal test-bed for n"
2021.findings-emnlp.255,2021.naacl-main.368,0,0.0861752,"into fix-length chunks of 200 tokens. Candidate answers are obtained from each chunk using existing MRC models, which are further aggregated over all chunks. We use the pre-trained Chinese RoBERTawwm-ext-Large (Cui et al., 2019). BM25 (Robertson and Zaragoza, 2009) is a bagof-words retrieval method, which uses sparse features to retrieve evidence sentences. We use the version implemented by Pyserini6 . Sent-BERT (Reimers and Gurevych, 2019) uses BERT to obtain contextualized dense representations for the texts and retrieve evidence sentences via cosine similarity. BeamDR (Xiong et al., 2021; Zhao et al., 2021) is an iterative evidence selection technique with beam search and dense retrieval. It updates the query by appending the newly-extracted evidence in each iteration. The BM25, Sent-BERT, and BeamDR are evidence extraction methods, which are combined with our proposed pair-wise option competition method to obtain the final question-level results. For BM25 and Sent-BERT, which cannot address the problem of multiple evidence, we report their 6 https://github.com/fxsjy/jieba 2981 https://github.com/castorini/pyserini Evidence Metrics QA Metrics P R F1 Acc. — — — 41.9 BM25 Top 1 BM25 Top 2 Sent-BER"
2021.findings-emnlp.255,P19-1075,0,0.0126622,"ent can push the model to capture fine-grained differences among different choices. 3) Experiments show that our approach outperforms a variety of baselines in both evidence retrieval F1 and QA accuracy on VGaokao while showing the merits of efficiency and explainability. 2 VGaokao: Verification Style Reading Comprehension Dataset Standardized language tests have been considered as a test-bed to harvest machine reading comprehension datasets. While most existing efforts focus on SQuAD-like QA datasets (Yang et al., 2018; Kwiatkowski et al., 2019), or cloze style questions (Zhang et al., 2018; Zheng et al., 2019), few efforts are made to verification style questions. In the Chinese Language test of Gaokao, approximately half of the reading comprehension questions instruct students to select a statement (i.e., an option from four choices) that is the most consistent or contradicting with the given passage. These questions are designed to evaluate students’ ability in extracting and integrating information from long passages, and analyzing certain linguistic phenomena or semantic relations among several similar sentences3 . According to the target language skills, we call these questions verification st"
2021.naacl-main.134,E14-1028,0,0.0519677,"Missing"
2021.naacl-main.134,W17-3531,0,0.044374,"Missing"
2021.naacl-main.134,I11-1144,0,0.036573,"words. der” information takes effects in natural language learning for neural models. On the basis of the pooling-based and memory-based approaches, we introduce the self-attention to encode the semantic dependencies between input words without considering order information, so as to enrich individual words with contextual information from different semantic aspects. We systematically compare the ability of different neural models to organize sentences from a bag of words in terms of three typical scenarios shown in Table 1. The contributions of this paper are summarized as follows: sentence (He and Liang, 2011). In dialogue systems, we need systems that are enabled to converse smoothly with people that have troubles in ordering words, such as children, language learners, and • We present an empirical study to investigate speech impaired. In image caption, the caption can the ability of neural models to organize senbe organized with a bag of attribute words extracted tences from a bag of words. from the image (Fang et al., 2015). Moreover, such a model can help non-native speakers of English to • We introduce a bag-to-sentence transformawrite a sentence just from keywords. tion model based on self-at"
2021.naacl-main.134,W04-1013,0,0.0539533,"rds in this paper. So we compare all methods under standard beam search method (with a beam size of 5) in our experiment, to highlight the differences among different encoders. N-GRAM∗ RNNLM∗ Pooling LSTM Memory AttP AttM BLEU 0.2330 0.2450 0.3118 0.3140 0.3328 0.3469 0.3489 ROUGE-L 0.5916 0.5875 0.6053 0.6169 0.6194 WAcc 0.4105 0.3873 0.4089 0.4297 0.4304 PMR 0.0863 0.0850 0.0941 0.1013 0.1059 Table 3: Results of word ordering task on PTB datasets (beam size = 5), * denotes the results reported in (Hasler et al., 2017). cal units (e.g., unigram, bigram) with the reference sentences. ROUGE-L (Lin, 2004) measures the longest common subsequence (LCS) between the reference sentence and the generated sentence. WAcc (Word Accuracy) is the negative word error rate (WER) (Mangu et al., 2000). It measures the edit distance between the generated sentence and the reference sentence (higher is better). Besides, we also conduct human evaluations to further analyze our generated results and explore the detail sort of wrong cases. 5.4 Overall Results Table 2 illustrates the performance of all models for three scenarios on the Wikipedia dataset. Firstly, we can find that Pooling shows the worse performance"
2021.naacl-main.134,D15-1043,0,0.0161204,"o, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scenario, the words of X come from a disordered sentence and are t"
2021.naacl-main.134,N15-1012,0,0.0178143,"n to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scena"
2021.naacl-main.134,J93-2004,0,0.0812258,"0.5808 0.5563 0.4369 0.6063 0.5789 0.4520 0.6613 0.6367 0.4700 0.6697 0.6461 0.4702 Table 2: Results on the test sets of three scenarios for Wikipedia dataset. We randomly generate noisy words with the number between 1 and half length of the sentence from the vocabulary for each sentence as the input of the the noise scenario. For the missing scenario, random words with number between 1 and half length of the sentence are removed from each sentence. It is worth noting that we randomly shuttle input bags with three different seeds and report the mean score of each metrics for LSTM. data (PTB) (Marcus et al., 1993), which is a widelyused dataset for word ordering task (Schmaltz et al., 2016; Hasler et al., 2017). To facilitate fair comparisons, we use the data preprocessed by (Schmaltz et al., 2016), which consists of 39, 832 training sentences, 1, 700 validation sentences and 2, 416 test sentences. 5.2 Implementation Details For all models, we set the dimension of word embedding as 128. In the LSTM-based encoder, the dimension of hidden unit is 256. In the self-attentionbased encoder, we set the number of head in Equation (6) as 8 and the hidden size of feed-forward layer in Equation (7) as 256. All pa"
2021.naacl-main.134,P02-1040,0,0.109582,"Missing"
2021.naacl-main.134,D16-1255,0,0.0833071,"al. (2018) utiIn this paper, we aim to investigate how “or- lized self-attention to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , ·"
2021.naacl-main.134,P17-1099,0,0.0479644,"e scenario by randomly introducing some To highlight the differences among different en- noisy words to the source bag, and construct the coders, we utilize the same decoder for different training data for the missing scenario by randomly encoders. removing some words from the source bag. Since the target Y corresponds to a sequence, We also compare the normal scenario of and has significant vocabulary overlap with the our model on The English Penn Treebank input bags of words, we blend a pointer-based de3 The corpus removes all links and other irrelevant material coder (Vinyals et al., 2015; See et al., 2017), which 2 It is worth noting that the current memory is composed of the word representations output by self-attention layer (e.g., navigation text, etc), and contains about one billion words, over 2 million documents. 4 http://www.nltk.org/api/nltk.tokenize.html 1685 Pooling LSTM Memory AttP AttM Normal 0.4656 0.4736 0.5030 0.5740 0.5886 BLEU Noise 0.4382 0.4327 0.4537 0.5372 0.5433 Missing 0.2636 0.2538 0.2664 0.2882 0.2914 Normal 0.6917 0.7311 0.7485 0.7860 0.7925 ROUGE-L Noise Missing 0.6587 0.5470 0.6761 0.5453 0.6939 0.5607 0.7396 0.5722 0.7465 0.5738 Perfect Matching Rate (PMR) Normal No"
2021.naacl-main.134,P17-1018,0,0.0390888,"Missing"
2021.naacl-main.134,E12-1075,0,0.0284739,"he objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scenario, the words of X come from a disorde"
2021.naacl-main.134,D11-1106,0,0.0375037,"or- lized self-attention to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence."
2021.naacl-main.137,2020.tacl-1.5,0,0.0466146,"Missing"
2021.naacl-main.137,W06-0115,0,0.112433,"Missing"
2021.naacl-main.137,2020.acl-main.611,0,0.275752,"de will be available at https://github. com/alibaba/pretrained-language-models/ LatticeBERT. However, the meanings of many Chinese words cannot be fully understood through direct compositions of their characters’ meanings. For example, 老板/boss does not mean 老/elder 板/board.2 The importance of word-level inputs in Chinese has been addressed in different tasks, including relation classification (Li et al., 2019), short text matching (Lai et al., 2019; Chen et al., 2020; Lyu et al., 2021), trigger detection (Lin et al., 2018), and named entity recognition (Zhang and Yang, 2018; Gui et al., 2019; Li et al., 2020a). The coarsegrained inputs benefit these tasks by introducing word-level semantics with multi-granularity representations, which is potentially complementary in character-level Chinese PLMs. In this work, we discuss how to pre-train a Chinese PLM over a word lattice structure to exploit multi-granularity inputs. We argue that by incorporating the coarse-grained units into PLM, models could learn to utilize the multi-granularity information for downstream tasks. Specifically, we organize characters and words in sentences as word lattices (see Figure 1), which enable the models to explore the"
2021.naacl-main.137,2020.acl-main.315,0,0.5274,"de will be available at https://github. com/alibaba/pretrained-language-models/ LatticeBERT. However, the meanings of many Chinese words cannot be fully understood through direct compositions of their characters’ meanings. For example, 老板/boss does not mean 老/elder 板/board.2 The importance of word-level inputs in Chinese has been addressed in different tasks, including relation classification (Li et al., 2019), short text matching (Lai et al., 2019; Chen et al., 2020; Lyu et al., 2021), trigger detection (Lin et al., 2018), and named entity recognition (Zhang and Yang, 2018; Gui et al., 2019; Li et al., 2020a). The coarsegrained inputs benefit these tasks by introducing word-level semantics with multi-granularity representations, which is potentially complementary in character-level Chinese PLMs. In this work, we discuss how to pre-train a Chinese PLM over a word lattice structure to exploit multi-granularity inputs. We argue that by incorporating the coarse-grained units into PLM, models could learn to utilize the multi-granularity information for downstream tasks. Specifically, we organize characters and words in sentences as word lattices (see Figure 1), which enable the models to explore the"
2021.naacl-main.137,P19-1430,0,0.0266985,"se characters — as the input, following the English PLMs’ practice (Devlin et al., 2019, BERT). ∗ investigate life Work done during an internship at Alibaba DAMO Academy. † Corresponding author. 1 Our code will be available at https://github. com/alibaba/pretrained-language-models/ LatticeBERT. However, the meanings of many Chinese words cannot be fully understood through direct compositions of their characters’ meanings. For example, 老板/boss does not mean 老/elder 板/board.2 The importance of word-level inputs in Chinese has been addressed in different tasks, including relation classification (Li et al., 2019), short text matching (Lai et al., 2019; Chen et al., 2020; Lyu et al., 2021), trigger detection (Lin et al., 2018), and named entity recognition (Zhang and Yang, 2018; Gui et al., 2019; Li et al., 2020a). The coarsegrained inputs benefit these tasks by introducing word-level semantics with multi-granularity representations, which is potentially complementary in character-level Chinese PLMs. In this work, we discuss how to pre-train a Chinese PLM over a word lattice structure to exploit multi-granularity inputs. We argue that by incorporating the coarse-grained units into PLM, models could lea"
2021.naacl-main.137,P18-1145,0,0.0605616,"Missing"
2021.naacl-main.155,C18-1041,0,0.0187589,"are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, different ways of killing are summarized as the factor kill, therefore lose valuable information. Speciﬁcally, beaten to death might occur in cases of involuntary manslaughter, while shooting cases are more likely to be associated w"
2021.naacl-main.155,D17-1042,0,0.0291644,"n contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relationship by correlation, which may introduce bias into causal inference; Ning et al. (2018) and Gao et al. (2019) merely focus on capturing causality by explicit textual features or supervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2"
2021.naacl-main.155,N19-1179,0,0.0264114,"containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relationship by correlation, which may introduce bias into causal inference; Ning et al. (2018) and Gao et al. (2019) merely focus on capturing causality by explicit textual features or supervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal rel"
2021.naacl-main.155,2020.acl-main.474,0,0.0531871,"Missing"
2021.naacl-main.155,D17-1289,1,0.786249,"pervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, di"
2021.naacl-main.155,P18-1212,0,0.0661854,"Missing"
2021.naacl-main.155,D18-1302,0,0.0340661,"Missing"
2021.naacl-main.155,K17-1018,0,0.0258579,"easurement error for the causal graphs constructed by structured data. Egami et al. (2018) focus on mapping text to a low-dimensional representation of the treatment or outcome. Veitch et al. (2019) and Yao et al. (2019) treat text as confounder and covariate, which help to make causal estimation more accurate. These works all build causal graphs manually, and regard 6.3 Effect of Integrating Causal Strength text as a whole to be one of the factors. In contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relation"
2021.naacl-main.155,W17-0903,0,0.0223768,"o be one of the factors. In contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relationship by correlation, which may introduce bias into causal inference; Ning et al. (2018) and Gao et al. (2019) merely focus on capturing causality by explicit textual features or supervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal"
2021.naacl-main.155,N18-2028,0,0.0223367,"Missing"
2021.naacl-main.155,D18-1488,0,0.0237612,"Take Violent Acquisition as an example. Although the cases are predicted correctly by BiLSTM+Att, the model tends to attend to words bag, RMB and value, which frequently occur but cannot be treated as clues for judgement. Instead, BiLSTM+Att+Cons values factors such as grab, rob and hold, which are more helpful for judgement. 7 Related Works Causal Inference with Text. Recently, a few works try to take text into account when performing causal inference. Landeiro and Culotta (2016) introduce causal inference to text classiﬁcation, and manage to remove bias from certain out-of-text confounders. Wood-Doughty et al. (2018) use text as a supplement of missing data and measurement error for the causal graphs constructed by structured data. Egami et al. (2018) focus on mapping text to a low-dimensional representation of the treatment or outcome. Veitch et al. (2019) and Yao et al. (2019) treat text as confounder and covariate, which help to make causal estimation more accurate. These works all build causal graphs manually, and regard 6.3 Effect of Integrating Causal Strength text as a whole to be one of the factors. In contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al."
2021.naacl-main.155,2020.acl-main.280,0,0.0122528,"2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, different ways of killing are summarized as the factor kill, therefore lose valuable information. Speciﬁcally, beaten to death might occur in cases of involuntary manslaughter, while shooting cases are more likely to be associated with murder. Also, factors with low frequency may be omitted in c"
2021.naacl-main.155,D18-1390,0,0.0186565,"causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, different ways of killing"
2021.naacl-main.155,P16-2034,0,0.0203501,"rom the view of graph, the nodes pointing to Yi ). The calculated scores are fed into a random forest classiﬁer (Ho, 1995) to learn thresholds between the charges. More advanced classiﬁers can also be used. B. Leverage Causal Chains Labeled Cases Neural networks (NN) are considered to be good at exploring large volumes of textual data. This motivates us to integrate the causal framework with NN, to beneﬁt each other. Here we propose two integration methods as shown in Figure 3. First, we inject the estimated causal strength to constrain the attention weights of a Bi-LSTM with attention model (Zhou et al., 2016). A Bi-LSTM layer is ﬁrst applied to the fact descriptions to obtain contextual embeddings H = {h1 , h2 , . . . , hn }, hi ∈ Rb0 , where b0 is the dimension of embeddings. Then, an attention layer assigns different weights {a1 , a2 , . . . , an } to each word, and sums the words up according to the weights to build a text embedding v: n  exp(qT · hi ) ai × hi , (5) ai = n ,v = T k=1 exp(q · hk ) i=1 A B Y1 C D Y2 ... B Y1 LSTM Chain Embeddings Linear Predictions ... strength: Lcons = n  (ai − gi )2 , i=1 (6) L = Lcross + αLcons . Note that in the validation and testing stages, the inputs do"
2021.naacl-main.155,2020.emnlp-main.612,0,0.0468084,"Missing"
C16-1226,P15-1034,0,0.00769022,"), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). To alleviate this problem, we propose a paraphrase based method that can map relational phrases to proper textual relations. Specifically, we first apply an open information extractor (Angeli et al., 2015) on the English Wikipedia to construct a repository of <argument 1 , relation, argument 2 &gt; triples, where the arguments are entity phrases found in the input sentence and the relation represents certain relationship between the arguments. By linking these arguments to KB entities, we can obtain a textual knowledge repository. Paraphrasing Once the candidate set of textual relations T R = {tr1 , tr3 , ..., tr|T R |} are constructed, given a relational phrase rp, our goal is to find the tr that has the same meaning as rp, which can be treated as a paraphrase task. Our framework accommodates any"
C16-1226,N07-4013,0,0.0109451,"structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries"
C16-1226,P14-1091,0,0.0241474,"Missing"
C16-1226,D13-1160,0,0.440177,"2) which consists of 127,811 pairs of relational phrases and DBpedia predicates involving 225 DBpedia predicates. For Freebase, we use 3,022 phrase-predicate pairs of W EB Q UESTIONS used in (Xu et al., 2016), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). To alleviate this problem, we propose a paraphrase based method that can map relational phrases to proper textual relations. Specifically, we first apply an open information extractor (Angeli et al., 2015) on the English Wikipedia to construct a repository of <argument 1 , relation, argument 2 &gt; triples, where the arguments are entity phrases found in the input sentence and the relation represents certain relationship between the arguments. By linking these arguments to KB entities, we can obtain a textual knowledge repository. Paraphrasing Once the candidate set of textual relations T R = {"
C16-1226,D14-1067,0,0.148494,"Missing"
C16-1226,P13-1042,0,0.0317011,"Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both"
C16-1226,P14-1077,1,0.827079,"ntic parsing, i.e., mapping from text to logical forms containing the predicates from the given knowledge base. However, the closed predicate vocabulary assumed by the traditional KB-QA paradigm has inherent limitations. First, a closed predicate vocabulary has limited coverage, as such vocabularies are typically powered by community efforts. Second, a closed predicate vocabulary may abstract away potentially relevant semantic differences. Third, even a logical form was produced, the answers may be incomplete due to the imperfection of the KB, which has been addressed by (Riedel et al., 2013; Chen et al., 2014). For example, no logical form could be produced for the question who is the front man of the band that wrote Coffee & TV. Because the semantics of front man cannot be adequately encoded using Freebase or DBpedia predicates. On the other hand, knowledge bases like DBpedia capture real world facts, and web resources like Wikipedia may provide a large repository of sentences that complement those facts. For instance, we can find in Wikipedia a sentence In August 2009, Debelle performed at Africa Express in Paris, an event set up by Blur and Gorillaz front-man Damon Albarn, which indicates the fr"
C16-1226,J81-4005,0,0.761527,"Missing"
C16-1226,P15-1026,0,0.160102,"Missing"
C16-1226,D11-1142,0,0.0162979,"ively. For DBpedia, we use the PATTY dataset (Nakashole et al., 2012) which consists of 127,811 pairs of relational phrases and DBpedia predicates involving 225 DBpedia predicates. For Freebase, we use 3,022 phrase-predicate pairs of W EB Q UESTIONS used in (Xu et al., 2016), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). To alleviate this problem, we propose a paraphrase based method that can map relational phrases to proper textual relations. Specifically, we first apply an open information extractor (Angeli et al., 2015) on the English Wikipedia to construct a repository of <argument 1 , relation, argument 2 &gt; triples, where the arguments are entity phrases found in the input sentence and the relation represents certain relationship between the arguments. By linking these arguments to KB entities, we can obtain a textual knowledge reposito"
C16-1226,P13-1158,0,0.0360702,"d to measure the word-wise and phrase-wise similarities between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. Then a dynamic pooling layer is introduced to compute a fixedsized representation from the variable-sized matrices. Finally the pooled representation is used as input to a classifier Cp . Learning In our experiment, we directly used the pre-trained RAE which is trained on a subset of 150,000 sentences from the NYT and AP sections of the Gigaword corpus. To train the classifier Cp , we use the PARALEX corpus (Fader et al., 2013), which is a large monolingual parallel corpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by the users of the website. 4 Joint Inference The goal of the inference step is to find a global optimal configuration of entity phrases and relational phrases with semantic components. As the result of disambiguating one phrase can influence the mapping of other phrases, we consider all phrases jointly in one disambiguation task. Now, we will first describe three key criteria that are used to evaluate the configuration in deta"
C16-1226,D15-1205,0,0.0133779,"ebase, respectively. These entities are treated as candidate entities that will be eventually disambiguated in the joint inference step. 3.2 KB-based Relation Extraction The choice of KB-based relation extraction model is also broad. In this paper, we employ the MultiChannel Convolutional Neural Networks (MCCNNs) model presented in (Xu et al., 2016) to learn a compact and robust relation representation. This is crucial since there exist thousands of relations in a KB, using lexicalized features inevitably suffers from the sparsity problem and their poor generalization ability on unseen words (Gormley et al., 2015). The MCCNN model treats the conjunction of three parts in a ungrounded triple as a sentence (subject relational phrase object). The first channel takes the shortest path between the subject and object in the dependency tree4 as input, while the other channel takes the relational phrase itself as input. Each channel uses the network structure described in (Collobert et al., 2011), which uses a convolutional layer to project the word-trigram vectors of words within a context window of 3 words to a local contextual feature vector, followed by a max pooling layer that extracts the most salient lo"
C16-1226,N10-1145,0,0.0173798,"gest river in USA involving aggregation operations. Our current assumption that free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with di"
C16-1226,D14-1117,0,0.0617263,"uctured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) who does question answering on unstructured data but enrich it with Freebase. Joint inference methods over multiple local models has been applied to KB-QA systems (Yahya et al., 2012). In contrast to this prior work concentrating on the structured KB, our constraints are more complex, as we address the joint mapping of relational phrases onto KB predicates and textual relations. 7 Conclusion and"
C16-1226,D12-1069,0,0.0135943,"rhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) wh"
C16-1226,D13-1161,0,0.0280254,"o main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is"
C16-1226,D12-1104,0,0.0317568,"g The model is learned using pairs of relational phrase and its corresponding KB predicate. Given an input phrase, the network outputs a distribution vector over the predicates o. We denote t as the target distribution vector, in which the value for gold relation is set 1, others are set 0. We compute the cross entropy error between t and o as the loss function. The model parameters can be efficiently computed via back-propagation through network structures. In experiment, we train two distinct relation extractors over DBpedia and Freebase, respectively. For DBpedia, we use the PATTY dataset (Nakashole et al., 2012) which consists of 127,811 pairs of relational phrases and DBpedia predicates involving 225 DBpedia predicates. For Freebase, we use 3,022 phrase-predicate pairs of W EB Q UESTIONS used in (Xu et al., 2016), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Be"
C16-1226,N15-1077,0,0.0471632,"for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) who does question answering on unstructured"
C16-1226,Q16-1010,0,0.0662561,"Missing"
C16-1226,N13-1008,0,0.0356622,"oblem reduces to semantic parsing, i.e., mapping from text to logical forms containing the predicates from the given knowledge base. However, the closed predicate vocabulary assumed by the traditional KB-QA paradigm has inherent limitations. First, a closed predicate vocabulary has limited coverage, as such vocabularies are typically powered by community efforts. Second, a closed predicate vocabulary may abstract away potentially relevant semantic differences. Third, even a logical form was produced, the answers may be incomplete due to the imperfection of the KB, which has been addressed by (Riedel et al., 2013; Chen et al., 2014). For example, no logical form could be produced for the question who is the front man of the band that wrote Coffee & TV. Because the semantics of front man cannot be adequately encoded using Freebase or DBpedia predicates. On the other hand, knowledge bases like DBpedia capture real world facts, and web resources like Wikipedia may provide a large repository of sentences that complement those facts. For instance, we can find in Wikipedia a sentence In August 2009, Debelle performed at Africa Express in Paris, an event set up by Blur and Gorillaz front-man Damon Albarn, wh"
C16-1226,P10-1040,0,0.031333,"Missing"
C16-1226,D07-1003,0,0.0202971,"t is the second longest river in USA involving aggregation operations. Our current assumption that free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to expl"
C16-1226,P16-1220,1,0.573515,"cannot be adequately encoded using Freebase or DBpedia predicates. On the other hand, knowledge bases like DBpedia capture real world facts, and web resources like Wikipedia may provide a large repository of sentences that complement those facts. For instance, we can find in Wikipedia a sentence In August 2009, Debelle performed at Africa Express in Paris, an event set up by Blur and Gorillaz front-man Damon Albarn, which indicates the front man of the band in the example question is Damon Albarn1 . Moreover, text corpora is also shown effective in refining the answers retrieved from the KBs (Xu et al., 2016). Motivated by these observations, we tackle the This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 1 The Blur band wrote the Coffee & TV song. Licence details: http:// 2397 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2397–2407, Osaka, Japan, December 11-17 2016. question answering task by integrating these two types of heterogeneous data, i.e., structured knowledge bases and free text, while is rarely investigated before. This task involves three main"
C16-1226,D12-1035,0,0.0169377,"al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) who does question answering on unstructured data but enrich it with Freebase. Joint inference methods over multiple local models has been applied to KB-QA systems (Yahya et al., 2012). In contrast to thi"
C16-1226,P15-1049,0,0.00860844,"g of a question using clues from two types of heterogeneous resources, we tackle the QA problem in an IE-based fashion involving entity linking and relation extraction. In particular, we simultaneously map relational phrases to KB predicates and textual relations. 3.1 Entity Linking The preliminary entity linking model can be any approach which outputs a score for each entity candidate. Note that a recall-oriented model will be more than welcome, since we expect to introduce more potentially correct local predictions into the inference step. In this paper, we adopt DBpedia Lookup3 and S-MART (Yang and Chang, 2015) to retrieve top 10 entities from DBpedia and Freebase, respectively. These entities are treated as candidate entities that will be eventually disambiguated in the joint inference step. 3.2 KB-based Relation Extraction The choice of KB-based relation extraction model is also broad. In this paper, we employ the MultiChannel Convolutional Neural Networks (MCCNNs) model presented in (Xu et al., 2016) to learn a compact and robust relation representation. This is crucial since there exist thousands of relations in a KB, using lexicalized features inevitably suffers from the sparsity problem and th"
C16-1226,D15-1237,0,0.0102964,"t free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et"
C16-1226,N13-1106,0,0.0372165,"Missing"
C16-1226,N15-3014,0,0.0666292,"Missing"
C16-1226,P13-1171,0,0.0130659,"rations. Our current assumption that free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to"
C16-1226,P15-1128,0,0.254026,"Missing"
D10-1050,P00-1041,0,0.917087,"essions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications. 1 An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Introduction Summarization is the process of condensing a source text into a shorter version while preserving its information content. Humans summarize on a daily basis and effortlessly, yet the automatic production of high-quality summaries remains a challe"
D10-1050,C08-1018,1,0.785884,"document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1 Approaches to headline generation are too numerous"
D10-1050,P09-1053,0,0.116466,"tion generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to content, style, and the task at hand. 5"
D10-1050,P02-1057,0,0.021816,"Missing"
D10-1050,W03-0501,0,0.0624012,"Missing"
D10-1050,P10-1126,1,0.528706,"that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and E"
D10-1050,N10-1125,1,0.605112,"that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and E"
D10-1050,A00-1043,0,0.014495,"i-Synchronous Grammar Kristian Woodsend, Yansong Feng and Mirella Lapata School of Informatics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captu"
D10-1050,J02-4006,0,0.0145156,"ormation content. Humans summarize on a daily basis and effortlessly, yet the automatic production of high-quality summaries remains a challenge. Most work today focuses on extractive summarization, where a summary is created by identifying and subsequently concatenating the most important sentences in a document. The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, Constituent deletion and recombination are merely two of the many rewrite operations professional editors and abstractors employ when creating summaries (Jing, 2002). Additional operations include truncating sentences, aggregating them, and paraphrasing at word or syntax level. Furthermore, professionals write summaries in a task-specific style. News headlines for example are typically short (three to six words), written in the present tense and active voice, and often leave out forms of the verb be. There are also different ways of writing a headline either directly by stating what the docu513 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Associati"
D10-1050,P03-1054,0,0.0419048,"nformation which we obtain by parsing every sentence twice, once with a phrase structure parser and once with a dependency parser. The output from the two representations is combined into a single data structure, by mapping the dependencies to the edges of the phrase structure tree. The procedure is described in detail in Woodsend and Lapata (2010). However, we do not merge the leaf nodes into phrases here, but keep the full tree structure, as we will apply compression to phrases through the QG. In our experiments, we obtain this combined representation from the output of the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. 3.2 Quasi-synchronous grammar Given an input sentence S1 or its parse tree T1, the QG constructs a monolingual grammar for parsing, or generating, the possible translation (or here, paraphrase) trees T2. A grammar node in the target tree T2 is modeled on a subset of nodes in the source tree, with a rather loose alignment between the trees. In our approach, the process of learning the grammar is unsupervised. Each sentence of the source document is compared to each sentence in the target document — headline or caption, depending on th"
D10-1050,W03-1101,0,0.0879938,"Yansong Feng and Mirella Lapata School of Informatics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as parap"
D10-1050,N03-1020,0,0.0251934,"nhances it with topic keywords. For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in Feng and Lapata (2010a). The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different surface realizations. The language model is adapted with probabilities from an image annotation model (Feng and Lapata, 2010b). Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). The DUC-04 dataset provides four reference headlines per document. We report unigram overlap (ROUGE -1) and bigram overlap (ROUGE -2) as a means of assessing informativeness, and the longest common subsequence (ROUGE -L) as a means of assessing fluency. Original DUC-04 ROUGE parameters were used. We also use ROUGE to evaluate the automatic captions with the original BBC captions as reference. In addition, we evaluated the generated headlines by eliciting human judgments. Participants were presented with a news article and its corresponding headline and were asked to rate the latter along two"
D10-1050,W09-1801,0,0.174402,"atics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming"
D10-1050,W06-3104,0,0.271471,"over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications. 1 An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and c"
D10-1050,D09-1086,0,0.0242404,"apata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints re"
D10-1050,D07-1003,0,0.120988,"utput of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to content, style, and the task at hand. 515 3 Modeling There are three components to"
D10-1050,P10-1058,1,0.791556,"upiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase"
D10-1050,P09-1094,0,0.0181334,"um´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1 Approaches to headline generation are too numerous to list in detail; see"
D10-1050,A00-2024,0,\N,Missing
D12-1076,S07-1012,0,0.278431,"Missing"
D12-1076,P98-1012,0,0.0306486,", including two steps: feature extraction and person clustering. Most research efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based metrics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the oth"
D12-1076,E06-1002,0,0.251655,"Missing"
D12-1076,D07-1074,0,0.214831,"Missing"
D12-1076,P05-1045,0,0.00760577,"ateness(David and Ian, 2008) with other concepts in the current page. Named Entity and Biographical Information Extraction Although Wikipedia concepts can provide rich background knowledge, they suffer from the limited coverage. It is common that some discriminative features are not likely to be found in Wikipedia, such as names of infamous people or organizations, email addresses, phone numbers, etc. We therefore extract two extra kinds of features, named entities that do not appear in the Wikipedia anchor text dictionary, and biographical information. We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list. We use regular expressions to extract email address, phone numbers and birth years. For convenience, we will also call concept features for Wikipedia concept features and non-concept features for the other two in the rest of this paper. 3.2 Stolen Base 0.4145 0.4228 Home Run 0.3467 Major League Baseball Graph Construction In our model, we capture the topic structure through a semantic graph. Specifically, for each name observation set, we connect all Wikipedia concepts appearing in the current observation set by their pairwise sem"
D12-1076,P10-1006,0,0.356658,"und knowledge about the semantic relatedness between entities can be leveraged to improve the disambiguation performance, and relieve the coverage problem, to some extent. Bunescu and Pasca and Cucerzan utilize Wikipedia’s category hierarchy to disambiguate entities, while Pilz uses Wikipedia’s link information. Han and Zhao adopt Wikipedia semantic relatedness to compute the similarity between name observations. They also combine multiple knowledge sources and capture explicit semantic relatedness between concepts and implicit semantic relationship embedded in a semantic graph simultaneously(Han and Zhao, 2010). Most approaches discussed above explore various features in the current page or rely on external knowledge resources to bridge the vocabulary gap, but pay less attention to the lack of clues since they ignore the person specific evidence in the current corpus level. Our model focuses on solving the data sparsity problem by utilizing other web pages in the same name observation set to provide a robust but person specific weighting for discriminative features beyond the current document alone. In terms of extra resources, the Wikipedia based model (WS) by Han and Zhao (2009) is close to our mo"
D12-1076,S07-1107,0,0.0145354,"e of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based metrics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity problem. The employed resources include raw texts available on the web and online encyclopedias. Kalashnikov et al. and Yiming et al. use extra web corpora to obtain co"
D12-1076,W03-0405,0,0.745249,"s first extracting various features from the web pages, and then grouping these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily B"
D12-1076,P04-1076,0,0.614882,"ch efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based metrics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich backgro"
D12-1076,S07-1042,0,0.035341,"Missing"
D12-1076,C98-1012,0,\N,Missing
D14-1205,D11-1071,0,0.0223021,"er, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sou"
D14-1205,P14-1077,1,0.879017,"entity pair level can be utilized here (again, a recall-oriented version will be welcome), such as Mintz++ mentioned in (Surdeanu et al., 2012), which we adapt into a Maximum Entropy version. We also include a special label, NA, to represent the case where there is no predefined relationship between an entity pair. For each sentence, we retain the relations with top q scores for the inference step, and we also call that this sentence supports those candidate relations. As for the features of RE models, we use the same features (lexical features and syntactic features) with the previous works (Chen et al., 2014; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011). 4.2 Relations’ Expectations for Argument Types In most KBs’ schemas, canonicalized relations are designed to expect specific types of entities to be their arguments. For example, in Figure 2, it is more likely that an entity Kobe Bryant takes the subject position of a relation fb:pro athlete.teams, but it is unlikely for this entity to take the subject position of a relation fb:org.headquarters. Making use of these type requirements can encourage the framework to select relation and entity candidates which are coherent with eac"
D14-1205,D13-1184,0,0.0249217,"he types of entities. In contrast, our RE models only require minimal supervision and do not need well-annotated training data. Our framework is therefore easy to adapt to new scenarios and suits real-world applications. The cold-start task aims at constructing a KB from scratch in a slot-filling style (Sun et al., 2012; Monahan and Carpenter, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries an"
D14-1205,W06-1651,0,0.12043,"ng as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick et al., 2013). Wick et al. (2013) also point out that confidence estimation should be a crucial part in the automated KB constructions and will play a key role fo"
D14-1205,C10-1032,0,0.0357622,"efine the knowledge base population task that we will address in this paper. Next we detail the proposed framework and present our experiments and results. Finally, we conclude this paper with future directions. 2 Related Work Knowledge base population (KBP), the task of extending existing KBs with entities and relations, has been studied in the TAC-KBP evaluations (Ji et al., 2011), containing three tasks. The entity linking task links entity mentions to existing KB nodes and creates new nodes for the entities absent in the current KBs, which can be considered as a kind of entity population (Dredze et al., 2010; Tamang et al., 2012; Cassidy et al., 2011). The slot-filling task populates new relations to the KB (Tamang et al., 2012; Roth et al., 2012; Liu and Zhao, 2012), but the relations are limited to a predefined sets of attributes according to the types of entities. In contrast, our RE models only require minimal supervision and do not need well-annotated training data. Our framework is therefore easy to adapt to new scenarios and suits real-world applications. The cold-start task aims at constructing a KB from scratch in a slot-filling style (Sun et al., 2012; Monahan and Carpenter, 2012). Enti"
D14-1205,D11-1142,0,0.0904765,"006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick et al., 2013). Wick et al. (2013) also point out that confidence estimation should be a crucial part in the automated KB constructions and will play a key role for the wide applications of automatically built KBs. We thus propose to model the reliability of the complete extraction process and take the argument type expectations of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zha"
D14-1205,D12-1082,0,0.0303539,"Missing"
D14-1205,P06-1060,0,0.024091,"by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fad"
D14-1205,D10-1033,0,0.0133747,"entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick"
D14-1205,P11-1095,0,0.0335071,"Missing"
D14-1205,P11-1055,0,0.743046,"ument type expectations of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zhao and Grishman, 2005; Li and Ji, 2014) to unsupervised open extractions (Fader et al., 2011; Carlson et al., 2010). There are also models, with distant supervision (DS), utilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework (Singh et al., 2013; Li and Ji, 2014; Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level whi"
D14-1205,W10-2924,0,0.251682,"ilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework (Singh et al., 2013; Li and Ji, 2014; Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level which relation this specific sentence describes (between a pair of entity mentions in this sentence). Li and Ji (2014) follow the ACE task definitions and present a neat incremental joint framework to simultaneously extract entity mentions and relations by structure perceptron. In contrast, we link entity mentions from a text corpus to their corresponding entities in an ex4 Task definition We formalize our task as follows. Given a set of entities sampled from"
D14-1205,P14-1038,0,0.402784,"osely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick et al., 2013). Wic"
D14-1205,P11-1138,0,0.0322282,"ributes according to the types of entities. In contrast, our RE models only require minimal supervision and do not need well-annotated training data. Our framework is therefore easy to adapt to new scenarios and suits real-world applications. The cold-start task aims at constructing a KB from scratch in a slot-filling style (Sun et al., 2012; Monahan and Carpenter, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly ident"
D14-1205,N13-1008,0,0.0421681,"obtain the subject’s type basketball player, and then we go through the initial matrix and find another entity Kobe Bryant with the same type taking the subject position of fb:pro athlete.teams, indicating that Jay Fletcher Vincent may take the relation fb:pro athlete.teams. The matrix Sobj is processed in the same way. Implicit Type Expectations In practice, few KBs have well-defined schemas. In order to make our framework more flexible, we need to come up with an approach to implicitly capture the relations’ type expectations, which will also be represented as preference scores. Inspired by Riedel et al. (2013) who use a matrix factorization approach to capture the association between textual patterns, relations and entities based on large text corpora, we adopt a collaborative filtering (CF) method to compute the preference scores between entities and relations based on the statistics obtained from an existing KB. In CF, the preferences between customers and items are calculated via matrix factorization over the initial customer-item matrix. In our framework, we compute the preference scores between entities and relations via the same approach over the two initialized matrices Ssubj and Sobj , resu"
D14-1205,D12-1042,0,0.733785,"of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zhao and Grishman, 2005; Li and Ji, 2014) to unsupervised open extractions (Fader et al., 2011; Carlson et al., 2010). There are also models, with distant supervision (DS), utilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework (Singh et al., 2013; Li and Ji, 2014; Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level which relation this specifi"
D14-1205,D10-1099,0,0.24755,"Missing"
D14-1205,P05-1052,0,0.0506913,"011; Wick et al., 2013). Wick et al. (2013) also point out that confidence estimation should be a crucial part in the automated KB constructions and will play a key role for the wide applications of automatically built KBs. We thus propose to model the reliability of the complete extraction process and take the argument type expectations of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zhao and Grishman, 2005; Li and Ji, 2014) to unsupervised open extractions (Fader et al., 2011; Carlson et al., 2010). There are also models, with distant supervision (DS), utilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has bee"
D14-1205,C10-1150,0,0.0170709,"Monahan and Carpenter, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the"
D14-1205,P09-1113,0,\N,Missing
D15-1062,C08-1088,0,0.0499677,"ned as follows: given a sentence S with a pair of nominals e1 and e2 , we aim to identify the relationship between e1 and e2 . RE is typically investigated in a classification style, where many features have been proposed, e.g., Hendrickx et al. (2010) designed 16 types of features including POS, WordNet, FrameNet, dependency parse features, etc. Among them, syntactic features are considered to bring significant improvements in extraction accuracy (Bunescu and Mooney, 2005a). Earlier attempts to encode syntactic information are mainly kernel-based methods, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel (Bunescu and Mooney, 2005b), and dependency tree kernel (Bunescu and Mooney, 2005a). With the recent success of neural networks in natural language processing, different neural network models are proposed to learn syntactic features from raw sequences of words or constituent 536 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 536–540, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and our model outperforms the-state-of-the-art methods on the SemEval-2010 Task 8 dataset. 2 table layer,"
D15-1062,D12-1110,0,0.370317,"parisons of our models with other methods on the SemEval 2010 task 8. Negative sampling schemes No negative examples Randomly sampled negative examples from NYT Dependency paths from the object to subject F1 81.3 83.5 85.4 Table 3: Comparisons of different negtive sampling methods on the development set. for We , W1 , W2 and W3 . The best setting was obtained with the values: 3, 200, 100, 10−4 , 10−3 , 10−4 and 2 × 10−3 , respectively. Results and Discussion Table 2 summarizes the performances of our model, depLCNN+NS(+), and state-of-the-art models, SVM (Hendrickx et al., 2010), RNN, MV-RNN (Socher et al., 2012), CNN (Zeng et al., 2014) and DepNN (Liu et al., 2015). For fair comparisons, we also add two types of lexical features, WordNet hypernyms and words around nominals, as part of input vector to the final softmax layer. We can see that our vanilla depLCNN+NS, without extra lexical features, still outperforms, by a large margin, previously reported best systems, MVRNN+ and CNN+, both of which have taken extra lexical features into account, showing that our treatment to dependency path can learn a robust and effective relation representation. When augmented with similar lexical features, our depLC"
D15-1062,H05-1091,0,0.835081,"Missing"
D15-1062,P10-1040,0,0.0177957,"y paths corresponding to two opposite subject/object directions, and then make predictions for the two paths, respectively. We choose the relation other if and only if both predictions are other. And for the rest cases, we choose the non-other relation with highest confidence as the output, since ideally, for a non-other instance, our model will output the correct label for the right subject/object direction and an other label for the wrong direction. We evaluate our models by macro-averaged F1 using the official evaluation script. We initialized We with 50-dimensional word vectors trained by Turian et al. (2010). We tuned the hyper parameters using the development set for each experimental setting. The hyper parameters include w, n1 , n2 , and regularization parameters Negative Sampling We start by presenting three pilot experiments on the development set. In the first one, we assume that the assignment of the subject and object for a relation is not given (blind), we simply extract features from e1 to e2 , and test it in a blind setting as well. In the second one, we assume that the assignment is given (sighted) during training, but still blind in the test phase. The last one is assumed to give the"
D15-1062,C14-1220,0,0.323001,"ill blind in the test phase. The last one is assumed to give the assignment during both training and test steps. The results are listed in Table 1. The third experiment can be seen as an upper bound, where we do not need to worry about the assignments of subjects and objects. By comparing the first and the second one, we can see that when adding assignment information during training, our model can be significantly improved, 1 Note that, there may be more than one relation existing between two nominals. A dependency path thus may correspond to multiple relations. 538 Method SVM RNN MVRNN CNN (Zeng et al., 2014) DepNN depCNN depLCNN depLCNN depLCNN+NS Feature Sets 16 types of features +POS, NER, WordNet +POS, NER, WordNet +WordNet,words around nominals +NER +WordNet,words around nominals +WordNet,words around nominals F1 82.2 74.8 77.6 79.1 82.4 78.9 82.7 83.6 81.3 81.9 83.7 84.0 85.6 Table 2: Comparisons of our models with other methods on the SemEval 2010 task 8. Negative sampling schemes No negative examples Randomly sampled negative examples from NYT Dependency paths from the object to subject F1 81.3 83.5 85.4 Table 3: Comparisons of different negtive sampling methods on the development set. for"
D15-1062,P14-1077,1,0.862752,"Missing"
D15-1062,W09-2415,0,0.357197,"Missing"
D15-1062,S10-1006,0,\N,Missing
D17-1233,W14-4012,0,0.00569695,"Missing"
D17-1233,J90-1003,0,0.369463,"In this work, we implement f using GRU. The decoder RNN generates each reply word conditioned on the context vector C. The probability distribution pt of candidate words at every time step t is calculated as: Technical Background Seq2Seq Model and Attention Mechanism (1) (4) where η is usually implemented as a multi-layer perceptron (MLP) with tanh as an activation function. 2191 User’s query ?? … ?? … ?? ?? Online process Cue word ?? (?? ) ?? … ?? ?? … ?? ℎ? Pre-process ?? … ?? ?? … ?? Trained model Input: ??−1 ?? … ?? ?? … ?? Pointwise Mutual Information Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association ratio based on the information theoretic concept of mutual information. Given a pair of outcomes x and y belonging to discrete random variables X and Y, the PMI quantifies the discrepancy between the probability of their coincidence based on their joint distribution and their individual distributions. Mathematically: PMI(x, y) = log p(x, y) p(x|y) = log p(x)p(y) p(x) (5) This quantity is zero if x and y are independent, positive if they are positively correlated, and negative if they are negatively correlated. 3 ?1 ?2 ?3 ?? = ℎ1 ℎ2 ℎ3 … ℎ? ?0 ?1 ?2 ??−1 Cue word pr"
D17-1233,N16-1014,0,0.449992,"ery as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-text conversation without context information. Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not controllable; it responses to a query according to the pattern learned from the training corpus. As a result, the system is likely to generate an unexpected reply even with little semantics, e.g, “I don’t know” and “Okay” due to the high frequency of these patterns in training data (Li et al., 2016a; Mou et al., 2016). To address this issue, Li et al. (2016a) proposed to increase diversity in the Seq2Seq model so that more informative utterances have a chance to stand out. Mou et al. (2016) provided a content-introducing approach that generates a reply based on a predicted word. The word is usually enlightening and drives the generated response to be more meaningful. However, this method is to some extent rigid; it requires the predicted word to explicitly occur in the generated utterance. As shown in Table 1, sometimes, it is better to generate a semantic related sentence based on the"
D17-1233,C16-1316,1,0.897694,"n open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge. Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by ∗ Corresponding author: ruiyan@pku.edu.cn query retrieval (Isbell et al., 2000; Wang et al., 2013; Yan et al., 2016; Song et al., 2016), and 2) to generate a new, tailored utterance given the userissued query (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Mou et al., 2016; Song et al., 2016). In these studies, generation-based conversation systems have shown impressive potential. Especially, the Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-text conversation without context information. Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not contr"
D17-1233,P02-1040,0,0.107195,"tic Language Generation in Dialogue (SLGD) method (Wen et al., 2015a), which added additional features in each gate of the neural cell. FGRU: To explore more fusion strategies, intuitively, we fused the cue word and hidden states by vector concatenation during the decoding process. Note that rGRU and SCGRU incorporate additional information by gating mechanisms, while SLGD and FGRU fuse the information into each gate of the neural cell directly. 4.3 Experiment Evaluation Objective metrics. To evaluate the performance of different methods for the conversation generation task, we leverage BLEU (Papineni et al., 2002) as the automatic evaluation metric, which is originally designed for machine translation and evaluates the output by using n-gram matching between the output and the reference. Here, we use BLEU-1, BLEU-2 and BLEU-3 in our experiments. Subjective metrics. Since automatic metrics may not consistently agree with human perception (Stent et al., 2005), human testing is essential to assess subjective quality. Hence, we randomly sampled 150 queries in the test set, then we invited five annotators to offer a judgment. For fairness, all of our human evaluation was conducted in a random, blind fashion"
D17-1233,D11-1054,0,0.017995,"hange the “hard” content-introducing method into a new “soft” schema. The rest of paper is organized as follows. We start by introducing the technical background. In Section 3, we describe our proposed method. In Section 4, we illustrate the experimental setup and evaluations against a variety of baselines. Section 5 briefly reviews related work. Finally, we conclude our paper in Section 6. 2 2.1 sentence as a vector by a recurrent neural network (RNN) and to decode the vector to a target sentence by another RNN. Now, the conversational generation is treated as a monolingual translation task (Ritter et al., 2011; Shang et al., 2015). Given a query Q = (x1 , ..., xn ), the encoder represents it as a context vector C and then the decoder generates a response R = (y1 , ..., ym ) word by word by maximizing the generation probability of R conditioned on Q. The objective function of Seq2Seq can be written as: p(y1 , ..., ym |x1 , ..., xn ) =p(y1 |C) T Y p(yt |C, y1 , ..., yt−1 ) t=2 To be specific, the encoder RNN calculates the context vector by: ht = f (xt , ht−1 ); C = hT st = f (yt−1 , st−1 , C); pt = softmax(st , yt−1 ) (3) where st is the hidden state of decoder RNN at time t and yt−1 is the generate"
D17-1233,P15-1152,0,0.195597,". For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. Recently, generation-based conversation systems have shownimpressive potential. Shang et al. (2015) generate replies for short-text conversation by Seq2Seq-basedneural networks with local and global attentions. 5.2 Content Introducing In vertical domains, Wen et al. (2015b) apply an additional control cell to gate the dialogue act (DA) features during the generation process to ensure the generated repliesexpressthe intended meaning. Also, the Stochastic Language Generation in Dialogue method (Wen et al., 2015a) adds additional features in each gate of the neural cell. Xu et al. (2016) introduce a new trainable gate to recall the global domain memory to enhance the ability of modeling the se"
D17-1233,P01-1066,0,0.104456,"ken as a desktop for a long while. (Screenshot) As the lockscreen? Make acquaintance and seek chances for further relations! (Freshman) I am also the new! Nice to meet you. Table 5: The implicit introducing-content cases of our HGFU model. The cue word in bold is not contained in the reply, while the response is still related to the cue word. T aemin† is a Korean singer. 5 5.1 Related work Conversation Systems Automatic human-computer conversation has attractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates (Walker et al., 2001; Misu and Kawahara, 2007; Williams et al., 2013). These approaches require no data or little data for trainingbuthuge manual effort to build the model, which is very timeconsuming. For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep"
D17-1233,D13-1096,0,0.0172018,"ers and practitioners. In particular, automatic conversation systems in open domains are attracting increasing attention due to its wide applications, such as virtual assistants and chatbots. In open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge. Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by ∗ Corresponding author: ruiyan@pku.edu.cn query retrieval (Isbell et al., 2000; Wang et al., 2013; Yan et al., 2016; Song et al., 2016), and 2) to generate a new, tailored utterance given the userissued query (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Mou et al., 2016; Song et al., 2016). In these studies, generation-based conversation systems have shown impressive potential. Especially, the Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-te"
D17-1233,W15-4639,0,0.345872,"edings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2190–2199 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Query Cue Word Reply Query Cue Word Reply 你不觉得好丑吗(Don’t you think it is ugly?) 审美(Aesthetics) 好恶心啊! (It’s disgusting!) 先放个大招(Let me use my ultimate power.) 技能(Skill) 新技能？(New skill?) Table 1: The content-introducing conversation examples. eration. 1) How to add the additional cue words during the generation process? One of the prevailing methods is modifying the neural cell with various gating mechanisms (Wen et al., 2015a,b; Xu et al., 2016). However, we need careful operation to ensure the neuron works as expected. 2) How to display the cue words in replies? As mentioned above, the explicit content-introducing approach in (Mou et al., 2016) does not fit well with all situations. In this paper, we present an implicit contentintroducing method for generative conversation systems, which incorporates cue words using our proposed hierarchical gated fusion unit (HGFU) in a flexible way. Our main contributions are as follows: • We propose the cue word GRU, another neural cell, to deal with the auxiliary information"
D17-1233,D15-1199,0,0.0607099,"Missing"
D17-1233,W13-4065,0,0.00407011,") As the lockscreen? Make acquaintance and seek chances for further relations! (Freshman) I am also the new! Nice to meet you. Table 5: The implicit introducing-content cases of our HGFU model. The cue word in bold is not contained in the reply, while the response is still related to the cue word. T aemin† is a Korean singer. 5 5.1 Related work Conversation Systems Automatic human-computer conversation has attractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates (Walker et al., 2001; Misu and Kawahara, 2007; Williams et al., 2013). These approaches require no data or little data for trainingbuthuge manual effort to build the model, which is very timeconsuming. For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural"
D17-1233,W06-1303,0,\N,Missing
D17-1289,D14-1181,0,0.00894182,"ecision making in the common law system. Rather than finding relevant cases, our work focuses on predicting specific charges, and we also emphasize the importance of law articles in decision making, which is important in the civil law system where the decisions are made based solely on statutory laws. Our work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically gene"
D17-1289,N16-1063,0,0.00409045,"words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target to label distribution during training with cross entropy as loss function (Kurata et al., 2016), and use a threshold tuned on validation set to produce the final prediction, which performs better in our pilot experiments. 3 Data Preparation Our data are collected from China Judgements Online1 , where the Chinese government has been publishing judgement documents since 2013. We randomly choose 50,000 documents for training, 5,000 for validation and 5,000 for testing. To ensure enough training data for each charge, we only classify the charges that appear more than 80 times in the training data, and treat documents with other charges as negative data. As for law articles, we consider thos"
D17-1289,O12-5004,0,0.230881,"rt) to support the decision. Even in countries using the common law system, e.g., the United States (except Louisiana), where the judgement is based mainly on decisions of previous cases, there are still some statutory laws that need to be followed when making decisions. Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al., 2012), which make those works hard to scale to more types of charges. There are also works addressing a related task, finding the law articles that are involved in a given case. A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications. Recent improvement takes a twostep approach by performing a preliminary classification first and then re-ranking the results with 2727 Procee"
D17-1289,D15-1167,0,0.0516587,"d we also emphasize the importance of law articles in decision making, which is important in the civil law system where the decisions are made based solely on statutory laws. Our work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, wh"
D17-1289,P12-2018,0,0.00966167,"e extraction task as multiple binary classifications. Specifically, we build a binary classifier for each article, focusing on its relevance to the input case, which results in 321 binary classifiers corresponding to the 321 distinct law articles appearing in our dataset. When more articles are considered, we can simply add more binary classifiers accordingly, with the existing classifiers untouched. Similar to the preliminary classification phase of (Liu et al., 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012). Specifically, we use bag-ofwords TF-IDF features, chi-square for feature selection and linear kernel for binary classification. (1) where hf t and hbt are the states of the forward and backward GRU at position t. The final sequence embedding is either the concatenation of hf T and hb1 or simply the average of ht . Attentive Sequence Encoder However, directly using [hf T , hb1 ] for sequence encoding often fails to capture all the information when the sequence is long, while using the average of ht also has the drawback of treating useless elements equally with informative ones. Inspired by ("
D17-1289,N16-1174,0,0.120055,"decisions are made based solely on statutory laws. Our work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target t"
D18-1112,P16-1154,0,0.0711736,"ble 1: Results on the WikiSQL (above) and Stackoverflow tions where S and G denotes Seq2Seq and Graph2Seq models, respectively. (below). map SELECT to which, WHERE to where, &gt; to more than. This method translates the SQL query of Figure 1 to which company where assets more than val0 and sales more than val0 and industry less than or equal to val1 and profits equals val2 . Seq2Seq. We choose two Seq2Seq models as our baselines. The first one is the attentionbased Seq2Seq model proposed by Bahdanau et al. (2014), and the second one additionally introduces the copy mechanism in the decoder side (Gu et al., 2016). To evaluate these models, we employ a template to convert the SQL query into a sequence: “SELECT + &lt;aggregation function&gt; + &lt;Split Results and Discussion Table 1 summarizes the results of our models and baselines. Although the template-based method achieves decent BLEU scores, its grammaticality score is substantially worse than other baselines. We can see that on both two datasets, our Graph2Seq models perform significantly better than the Seq2Seq and Tree2Seq baselines. One possible reason is that in our graph encoder, the node embedding retains the information of neighbor nodes within K h"
D18-1112,P16-1195,0,0.301055,"e since it helps non-expert users to understand the esoteric SQL queries that are used to retrieve the answers through the questionanswering process (Simitsis and Ioannidis, 2009) using varous text embeddings techniques (Kim, 2014; Arora et al., 2017; Wu et al., 2018a). Earlier attempts for SQL-to-text task are rulebased and template-based (Koutrika et al., 2010; Ngonga Ngomo et al., 2013). Despite requiring intensive human efforts to design temples or rules, these approaches still tend to generate rigid and stylized language that lacks the natural text of the human language. To address this, Iyer et al. (2016) proposes a sequence-to-sequence (Seq2Seq) network to model the SQL query and natural language jointly. However, since the SQL is designed ∗ Work done when the author was at IBM Research. 931 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 931–936 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics On the decoder side, we develop an RNN-based decoder which takes the graph vector representation as the initial hidden state to generate the sequences while employing an attention mechanism over all node emb"
D18-1112,P02-1040,0,0.109045,"nference phase, we use the beam search algorithm with beam size = 5. 4 Experiments We evaluate our model on two datasets, WikiSQL (Zhong et al., 2017) and Stackoverflow (Iyer et al., 2016). WikiSQL consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). StackOverflow consists of 32,337 SQL query and natural language question pairs, and we use the same train/development/test split as (Iyer et al., 2016). We use the BLEU-4 score (Papineni et al., 2002) as our automatic evaluation metric and also perform a human study. For human evaluation, we randomly sampled 1,000 predicted results and asked three native English speakers to rate each interpretation against both the correctness conforming to the input SQL and grammaticality on a scale between 1 and 5. We compare some variants of our model against the template, Seq2Seq, and Tree2Seq baselines. Graph2Seq-PGE. This method uses the Pooling method for generating Graph Embedding. Graph2Seq-NGE. This method uses the Node based Graph Embedding. Template. We implement a template-based method which f"
D18-1112,D14-1162,0,0.0903546,"Missing"
D18-1112,P18-1150,1,0.769483,"Missing"
D18-1112,D18-1482,1,0.913451,"he Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance. Interpretation: which company has both the market value and assets higher than val0, ranking in top val2 and revenue of val3 Figure 1: An example of SQL query and its interpretation. to express graph-structured query intent, the sequence encoder may need an elaborate design to fully capture the global structure information. Intuitively, varous graph encoding techniques base on deep neural network (Kipf and Welling, 2016; Hamilton et al., 2017; Song et al., 2018) or based on Graph Kernels (Vishwanathan et al., 2010; Wu et al., 2018b), whose goal is to learn the node-level or graph-level representations for a given graph, are more proper to tackle this problem. In this paper, we first introduce a strategy to represent the SQL query as a directed graph (see §2) and further make full use of a novel graphto-sequence (Graph2Seq) model (Xu et al., 2018) that encodes this graph-structured SQL query, and then decodes its interpretation (see §3). On the encoder side, we extend the graph encoding work of Hamilton et al. (2017) by encoding the edge direction information into the node embedding. Our encoder learns the representatio"
D18-1380,D17-1047,0,0.654271,"ethods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively. These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the attention weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especially for the aspect wit"
D18-1380,P14-2009,0,0.761413,"e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the"
D18-1380,P11-1016,0,0.376953,"e context and different sentiment polarities. As far as we know, we are the first to explore the interactions among the aspects with the same context. To evaluate the proposed approach, we conduct experiments on three datasets: laptop and restaurant are from the SemEval 2014 Task 4 and the third one is a tweet collection. Experimental results show that our method achieves the best performance on all three datasets. 2 Related Work Aspect-level sentiment analysis is a branch of sentiment classification, which requires considering both the sentence and aspect information. Traditional approaches (Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) regard this task as the text classification problem and design effective features, which are utilized in statistical learning algorithms for training a classifier. Kiritchenko et al. [2014] proposed to use SVM based on n-gram features, parse features and lexicon features, which achieved the best performance in SemEval 2014. Vo and Zhang [2015] designed sentiment-specific word embedding and sentiment lexicons as rich features for prediction. These methods highly depend on the effectiveness of the laborious feature engineering work and easily reach"
D18-1380,S14-2076,0,0.639666,"iment classification is a fundamental task in sentiment analysis (Pang et al., 2008; Liu, 2012), which aims to infer the sentiment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention"
D18-1380,D16-1011,0,0.0406189,"Missing"
D18-1380,E17-2091,0,0.16824,"n states to predict the polarity. TD-LSTM (Tang et al., 2016a) employs two directional LSTM networks, which estimate the left context and right context of the target aspect, respectively. Finally it takes the last hidden states of LSTM networks for prediction. MemNet (Tang et al., 2016b) applys multi-hop attentions on the word embeddings, learns the attention weights on context word vectors with respect to the averaged query vector. IAN (Ma et al., 2017) interactively learns the coarse-grained attentions between the context and aspect, and concatenate the vectors for prediction. BILSTM-ATT-G (Liu and Zhang, 2017) models left and right context with two attention-based LSTMs and utilizes gates to control the importance of left context, right context and the entire sentence for prediction. RAM(Chen et al., 2017) learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words, and proposes to use GRU network to get the aggregated vector from the attentions. Similar with MemNet, the atten3438 tion weights on context words are steered by the simple averaged aspect vector. We also list the variants of MGAN model, which are used to analyze the effects of coarsegrained attenti"
D18-1380,D15-1298,0,0.29463,"Missing"
D18-1380,D14-1162,0,0.0806935,"and each aspect ai = {wi1 , · · · , wiM } is a subsequence of sentence s, which contains M ∈ [1, N ) words. Aspect-level sentiment classification evaluates sentiment polarity of the sentence s with respect to each aspect ai . We present the overall architecture of the proposed Multi-grained Attention Network (MGAN) model in Figure 1. It consists of the Input Embedding layer, the Contextual Layer, the Multigrained Attention Layer and the Output Layer. 3.2 Input Embedding Layer Input Embedding Layer maps each word to a high dimensional vector space. We employ the pretrained word vector, GloVe (Pennington et al., 2014), to obtain the fixed word embedding of each word. Specifically, we denote the embedding lookup matrix as L ∈ Rdv ×|V |, where dv is the word vector dimension and |V |is the vocabulary size. 3.3 Contextual Layer We employ a bidirectional Long Short-Term Memory Network (BiLSTM) on top of the embedding layer to capture the temporal interactions among words. Specifically, at time step t, given the input word embedding x, the update process of forward LSTM network can be formalized as follows: − → − − → → − it = σ(W i · [ h t−1 , → x t] + b i) − → − − → → − ft = σ(W f · [ h t−1 , → x t] + b f ) −"
D18-1380,S14-2004,0,0.366679,"and update the training parameters. In addition, we utilize dropout strategy to avoid overfitting. 4 Experiments In this section, we conduct experiments to evaluate our two hypotheses: (1) whether the wordlevel interaction between aspect and context can help relieve the information loss and improve the performance. (2) whether the relationship among the aspects, which have the same context and different sentiment polarities, can bring extra useful information. 4.1 Experiment Setting We conduct experiments on three datasets, as shown in Table 1. The first two are from the SemEval 2014 Task 41 (Pontiki et al., 2014), which contains the reviews in laptop and restaurants, respectively. The third one is a tweet collection, which are gathered by (Dong et al., 2014). Each aspect with the context is labeled by three sentiment polarities, namely Positive, Neutral and Negative. In addition, we adopt Accuracy and MacroF1 as the metrics to evaluate the performance of aspect-level sentiment classification, which is widely used in previous works (Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Wang et al., 2016). In our experiments, word embeddings for both context and aspect words are initialized by Glove ("
D18-1380,D11-1014,0,0.230789,"sentiment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which inter"
D18-1380,C16-1311,0,0.287843,". Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively. These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the attention weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especial"
D18-1380,D16-1021,0,0.609338,". Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively. These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the attention weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especial"
D18-1380,S14-2036,0,0.0226756,"ion Aspect level sentiment classification is a fundamental task in sentiment analysis (Pang et al., 2008; Liu, 2012), which aims to infer the sentiment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vec"
D18-1380,D16-1058,0,0.342146,"Missing"
D19-1023,D18-1307,0,0.0230206,"al Linguistics lation alignment as a by-product. We evaluate our approach by applying it to three real-world datasets. Experimental results show that our approach delivers better and more robust results when compared with state-of-the-art methods for entity and relation alignments. The key contribution of this paper is a novel joint learning model for entity and relation alignments. Our approach reduces the human involvement and the associated cost in constructing seed alignments, but yields better performance over prior works. improve tasks like information extraction (Miwa and Bansal, 2016; Bekoulis et al., 2018). We hypothesize that this will be the case for entity alignment too; that is, the rich relation information could be useful for improving entity alignment as entities and their relations are usually closely related. Our experiments show that this is even a conservative target: by jointly learning entity and relation representations, we can promote the results of both entity and relation alignment. In this work, we aim to build a learning framework that jointly learns entity and relation representations for entity alignment; and we want to achieve this with only a small set of pre-aligned enti"
D19-1023,D18-1248,0,0.347336,", the task of linking entities with the same real-world identity from different KGs. Classical methods for entity alignment typically involve a labor-intensive and time-consuming process of feature construction (Mahdisoltani et al., 2013) or rely on external information constructed by others (Suchanek et al., 2011). Recently, efforts have been devoted to the so-called embeddingbased approaches. Representative works of this direction include JE (Hao et al., 2016), MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), and BootEA (Sun et al., 2018). More recent work (Wang et al., 2018b) uses the Graph Convolutional Network (GCN) (Kipf and Welling, 2017) to jointly embed multiple KGs. Most of the recent works (e.g., JE, MTransE, JAPE, IPTransE and BootEA) rely on the translation-based models, such as TransE (Bordes et al., 2013), which enable these approaches to encode both entities and relations of KGs. These methods often put more emphasis on the entity embeddings, but do not explicitly utilize relation embeddings to help with entity alignment. Another drawback of such approaches is that they usually rely on pre-aligned relations (JAPE and IPTransE) or triples (MTransE)."
D19-1023,D17-1159,0,0.0221954,"ed on entity representations. By linking entity representations with relation representations, they promote each other in our framework and ultimately achieve better alignment results. Graph Convolutional Networks GCNs (Duvenaud et al., 2015; Kearnes et al., 2016; Kipf and Welling, 2017) are neural networks operating on unlabeled graphs and inducing features of nodes based on the structures of their neighborhoods. Recently, GCNs have demonstrated promising performance in tasks like node classification (Kipf and Welling, 2017), relation extraction (Zhang et al., 2018a), semantic role labeling (Marcheggiani and Titov, 2017), etc. As an extension of GCNs, the R-GCNs (Schlichtkrull et al., 2018) have recently been proposed to model relational data for link prediction and entity classification. However, R-GCNs usually require a large number of parameters that are often hard to train, when applied to multi-relational graphs. In this work, we choose to use GCNs to first encode KG entities and to approximate relation representations based on entity embeddings. Our work is the first to utilize GCNs for jointly aligning entities and relations for heterogeneous KGs. 3 Our Approach 4.1 Overall Architecture As illustrated"
D19-1023,P16-1105,0,0.0203275,"ciation for Computational Linguistics lation alignment as a by-product. We evaluate our approach by applying it to three real-world datasets. Experimental results show that our approach delivers better and more robust results when compared with state-of-the-art methods for entity and relation alignments. The key contribution of this paper is a novel joint learning model for entity and relation alignments. Our approach reduces the human involvement and the associated cost in constructing seed alignments, but yields better performance over prior works. improve tasks like information extraction (Miwa and Bansal, 2016; Bekoulis et al., 2018). We hypothesize that this will be the case for entity alignment too; that is, the rich relation information could be useful for improving entity alignment as entities and their relations are usually closely related. Our experiments show that this is even a conservative target: by jointly learning entity and relation representations, we can promote the results of both entity and relation alignment. In this work, we aim to build a learning framework that jointly learns entity and relation representations for entity alignment; and we want to achieve this with only a small"
D19-1023,D18-1032,0,0.252046,", the task of linking entities with the same real-world identity from different KGs. Classical methods for entity alignment typically involve a labor-intensive and time-consuming process of feature construction (Mahdisoltani et al., 2013) or rely on external information constructed by others (Suchanek et al., 2011). Recently, efforts have been devoted to the so-called embeddingbased approaches. Representative works of this direction include JE (Hao et al., 2016), MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), and BootEA (Sun et al., 2018). More recent work (Wang et al., 2018b) uses the Graph Convolutional Network (GCN) (Kipf and Welling, 2017) to jointly embed multiple KGs. Most of the recent works (e.g., JE, MTransE, JAPE, IPTransE and BootEA) rely on the translation-based models, such as TransE (Bordes et al., 2013), which enable these approaches to encode both entities and relations of KGs. These methods often put more emphasis on the entity embeddings, but do not explicitly utilize relation embeddings to help with entity alignment. Another drawback of such approaches is that they usually rely on pre-aligned relations (JAPE and IPTransE) or triples (MTransE)."
D19-1023,P17-1132,0,0.0272134,"e them using entity embeddings learned by the GCN. We then incorporate the relation approximation into entities to iteratively learn better representations for both. Experiments performed on three real-world cross-lingual datasets show that our approach substantially outperforms state-of-the-art entity alignment methods. 1 Introduction Knowledge graphs (KGs) transform unstructured knowledge into simple and clear triples of <head entity, relation, tail entity> for rapid response and reasoning of knowledge. They are an effective way for supporting various NLP-enabled tasks like machine reading (Yang and Mitchell, 2017), information extraction (Wang et al., 2018a), and question-answering (Zhang et al., 2018b). Even though many KGs originate from the same resource, e.g., Wikipedia, they are usually created independently. Therefore, different KGs often use ∗ Corresponding author. 240 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 240–249, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics lation alignment as a by-product. We evaluate our approach by applyi"
D19-1023,P18-1187,0,0.0413936,"ions, and the black solid lines denote the process of continuing using GCNs to iteratively learn better entity and relation representations. between aligned entity pairs to be as close as possible, and the distance between positive and negative alignment pairs to be as large as possible. The loss function is defined as: ing forward propagation as: 1 1 ˜ − 2 X(l) W(l) ), ˜ − 2 A˜D X(l+1) = ReLU(D (1) where A˜ = A + I is the adjacency matrix of Ga with self-connections, I is an identity matrix, ˜ jj = P A˜jk , and W(l) ∈ Rd(l) ×d(l+1) is a layerD k specific trainable weight matrix. Inspired by (Rahimi et al., 2018) that uses highway gates (Srivastava et al., 2015) to control the noise propagation in GCNs for geographic localization, we also employ layer-wise highway gates to build a Highway-GCN (HGCN) model. Our layer-wise gates work as follow: (l) (l) T (X(l) ) = σ(X(l) WT + bT ), L= X max{0, d(p, q)−d(p0 , q 0 )+γ}, (p,q)∈L (p0 ,q 0 )∈L0 (5) where γ > 0 is a margin hyper-parameter; L0 stands for the negative alignment set of L. Rather than simply random sampling for negative instances, we look for more challenging negative samples, e.g., those with subtle differences from the positive ones, to train o"
D19-1023,D18-1244,0,0.107257,"on into entities to iteratively learn better representations for both. Experiments performed on three real-world cross-lingual datasets show that our approach substantially outperforms state-of-the-art entity alignment methods. 1 Introduction Knowledge graphs (KGs) transform unstructured knowledge into simple and clear triples of <head entity, relation, tail entity> for rapid response and reasoning of knowledge. They are an effective way for supporting various NLP-enabled tasks like machine reading (Yang and Mitchell, 2017), information extraction (Wang et al., 2018a), and question-answering (Zhang et al., 2018b). Even though many KGs originate from the same resource, e.g., Wikipedia, they are usually created independently. Therefore, different KGs often use ∗ Corresponding author. 240 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 240–249, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics lation alignment as a by-product. We evaluate our approach by applying it to three real-world datasets. Experimental results show that our approach delivers"
D19-1128,W15-4640,0,0.41228,"dely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampl"
D19-1128,W17-7301,0,0.123569,"ative sampling strategies have been studied in many machine learning tasks. In the computer vision fields, Faghri et al. (2017) studies hard negatives and introduces a simple change to common loss function on image-caption retrieval tasks. Guo et al. (2018) proposes a fast negative sampler which chooses negative examples that are most likely to meet the requirements of violation according to the latent factors of image. In natural language processing fields, Kotnis and Nastase (2017) analyses the impact of negative sampling strategies on the performance of link prediction in knowledge graphs. Saeidi et al. (2017) studies the affect of a tailored sample strategy on the performance of document retrieval task. Rao et al. (2016) uses three negative strategies to select the most informative negative samples on the pairwise ranking model for answer selection. Xu et al. (2015) introduces a straightforward negative sampling strategy to improve the assignment of subjects and objects on a convolution neural network. To our best knowledge, this is the first work to empirical study of negative sampling strategies for learning of matching models in multi-turn retrieval-based dialogue systems, which may enlighten f"
D19-1128,P19-1001,1,0.819447,"nes can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain many false negatives"
D19-1128,D13-1096,0,0.350074,"ing models in learning, we consider four strategies including minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling. Empirical studies on two benchmarks with three matching models indicate that compared with the widely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human"
D19-1128,P18-2067,1,0.725816,"igh quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain many false negatives and trivial true negatives that are very easy to distinguish from those true positives. As a result, models with advanced architectures can only reach sub-optimal performance after learning (Wu et al., 2018). In this paper, instead of configuring new architectures, we investigate how to improve the performance of existing matching models with a better learning method. A learning method usually involves choice of loss functions and construction of training data, and we are particularly interested in automatic training data construction, as data are often more crucial to the performance of models. The key problem in training data construction lies in how to properly choose negative examples, and our idea is that negative examples should adapt to the matching models at different learning stages. Fol"
D19-1128,P17-1046,1,0.961365,"wo strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn"
D19-1128,D15-1062,1,0.889697,"Missing"
D19-1128,C18-1317,0,0.0686812,"d to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training s"
D19-1128,D16-1036,1,0.902016,"Missing"
D19-1128,P18-1103,0,0.783361,"p, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain ma"
D19-1265,P15-1067,0,0.0417072,"a year. 5 Related Work KG Representation Learning aims to embed a KG into a continuous vector space, which preserves the KG structures. There are mainly two streams of researches. The first is additionbased models (also called translation-based models), which based on the principle that for every valid triple (h, r, t), their embeddings holds: h + r ≈ t. TransE(Bordes et al., 2013) is the first such model. TransH(Wang et al., 2014) projects entity embeddings into relation-specific hyperplanes. TransR(Lin et al., 2015) generalize TransH by extending projection to linear transformation. TransD (Ji et al., 2015) simplifies TransR by decomposing the transformation matrix into the product of two vectors. Another is multiplicationbased models, which comes from the idea of tensor decomposition. RESCAL(Nickel et al., 2011) is one of the earlist studies, which using a bilinear scoring function. DistMult(Yang et al., 2014) simplifies RESCAL by using a diagonal matrix. ComplEx(Trouillon et al., 2016) extend DistMult into the complex space to handle asymmetric relations. SimplE (Kazemi and Poole, 2018) learns two embeddings for each entity dependently. A direct application is KG completion. Graph Neural Netwo"
D19-1265,P17-1040,1,0.855489,"s of GUpdater. into GNN. To better capture KG information, RGCN(Schlichtkrull et al., 2018) was proposed to incorporates relation embeddings into GNN. Relation Extraction task aims to extract relations from texts. Current relation extraction models are mainly under distant supervision (Mintz et al., 2009), and are trained in bag level. PCNN (Zeng et al., 2015) divides the sentence into three pieces and applies max-pooling in a piecewise manner after the convolution layer. APCNN (Ji et al., 2017) uses sentence-level attention to select multiple valid sentences with different weights in a bag. (Luo et al., 2017) uses a transition matrix to model the noise and use curriculum for training. 6 Conclusion In this paper, we propose a new text-based KG updating task and construct a dataset, NBAtransactions, for evaluation. We design a novel GNNbased model, GUpdater, which uses text information to guide the message passing through the KG structure. Experiments show that our model can effectively handle both explicit and implicit information, and perform necessary link-adding and link-deleting operations accordingly. In the future, we will try to investigate how to update KGs when entities are involved in sev"
D19-1265,D15-1166,0,0.0257303,"ach layer. Basisdecomposition regularizes these matrices by defining each weight matrix as a linear combination of B(B &lt; R) basis matrices, which significantly decreases the parameter number. 3.2 The core idea of GNNs is to gather neighbors’ information. In our case, we propose a text-based attention mechanism to utilize the news snippet to guide the message passing along the KG structure within the R-GAT layer. We first use bi-GRU (Cho et al., 2014) to encode the given news text S into a sequence of representations {u1 , u2 , · · · , u|S |}, then we leverage the sequence attention mechanism (Luong et al., 2015) to compute the context vector: c = |S| X blr t ut (3) t=1 where blr t is the text attention weight, and is computed as follow: lr ) exp(uTt gtext blr = P|S| t T lr k=1 exp(uk gtext ) T lr attlr (hli , hlj ) = ggraph [hli ||hlj ] (4) lr is a trainable guidance vector to guide where gtext the extraction of relation-dependent context. (5) where ||denotes concatenation operation, and lr ggraph is a relation-specific trainable vector that serves as a graph guidance vector to decide which edge to pay attention to. Here, we generate the final guidance vector gflrin that combines textual information"
D19-1265,D15-1203,0,0.188549,"s the evaluation metrics. Further, to evaluate the ability of link-adding, link-deleting and link-preserving, respectively, we collect all added edges, deleted edges, and unchanged edges and compute the prediction accuracies separately, which we denote as Added Acc, Deleted Acc and Unchanged Acc, respectively. 4.4 Baseline Models Because most current text-based KG updating methods are in two steps: first extract information from texts, then add and remove links in KGs, we select non-rule-based models that perform well on these two steps and also their combination as our baseline models. PCNN (Zeng et al., 2015) is a strong baseline for relation extraction, which divides the sentence into three pieces and applies max-pooling in a piecewise manner after the convolution layer. IE-gold is a simulation of an ideal IE model that can perfectly extract explicit information from given texts. This is an upper bound for the information extraction step. DistMult (Yang et al., 2014) is a widely-used multiplication-based triple scoring function for KG completion, which computes the three-way inner-product of the triples. Its symmetric nature is suitable for NBAtransactions as the KGs are undirected. R-GCN (Schlic"
D19-1265,P09-1113,0,0.0296151,"le of one type of trade. Gtext and G 0 text represent the actual text-subgraphs before and after the trade, respectively. Selected Shortcuts represents the shortcuts selected by the attention mechanism, and the arrows indicate the directions of message passing. Results in Text-Subgraph lists the prediction errors of GUpdater. into GNN. To better capture KG information, RGCN(Schlichtkrull et al., 2018) was proposed to incorporates relation embeddings into GNN. Relation Extraction task aims to extract relations from texts. Current relation extraction models are mainly under distant supervision (Mintz et al., 2009), and are trained in bag level. PCNN (Zeng et al., 2015) divides the sentence into three pieces and applies max-pooling in a piecewise manner after the convolution layer. APCNN (Ji et al., 2017) uses sentence-level attention to select multiple valid sentences with different weights in a bag. (Luo et al., 2017) uses a transition matrix to model the noise and use curriculum for training. 6 Conclusion In this paper, we propose a new text-based KG updating task and construct a dataset, NBAtransactions, for evaluation. We design a novel GNNbased model, GUpdater, which uses text information to guide"
D19-1398,J96-1002,0,0.0760481,"network-based methods, respectively. In the rest, we first introduce constraints and redundancies in Section 2, then present our detailed approach in Section 3, followed by experimental study in Section 4, related work in Section 5, and conclusions in Section 6. 2 Constraints and Redundancies Consider a set M of mentions and a set R of predefined relations {r1 , . . . , rk }. For each mention m, a sentence level extractor produces an entity pair (subject s and object o) and a confidence score ci for each relation ri (i ∈ [1, k]), which represents the probability that s and o have relation ri (Berger et al., 1996; Hoffmann et al., 2011). Here an NA (unknown) relation is typically included in R. A mention m preprocessed by a sentence level extractor is essentially a k + 2 tuple (s, o, r1 : c1 , . . . , rk : ck ), as illustrated in Table 1, where 1 ≥ ci ≥ cj ≥ 0 for any i > j ∈ [1, k] and Pk i=1 ci = 1. Observe that an entity pair (subject s and object o) with a relation r ∈ R can be treated as an SRO triple (s, r, o), and a mention m contains k SRO triples. For convenience, given a mention m and a relation r ∈ R, we also denote the score of relation r in m as m[r].c. Given a set M of mentions preproces"
D19-1398,P07-1073,0,0.0678166,"Experimental study shows that our approach improves the efficiency and accuracy of RE, and outperforms both ILP and neural network-based methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear program"
D19-1398,P11-1055,0,0.115907,"Missing"
D19-1398,P14-1077,1,0.608857,"tence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the efficiency and accuracy of RE, and outperforms both ILP and neural network-based methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Glob"
D19-1398,D14-1205,1,0.593396,"tence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the efficiency and accuracy of RE, and outperforms both ILP and neural network-based methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Glob"
D19-1398,W10-2924,0,0.0354437,"nd the use of S-R, O-R and R redundancies improves the accuracy of RE. (3) The setting of threshold  is flexible in a range of [0.5, 0.9] for our approach eFIRE. 5 Related Work Relation extraction has been studied extensively in recent years, and can be divided into local re3858 lation extractors (Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Søgaard et al., 2015) using the lexical features, syntactic features, and other local features of sentences, and global relation extractors utilizing the corpus features and relationships among local decisions (Kate and Mooney, 2010; Yao et al., 2010; Li et al., 2011; Singh et al., 2013; Li and Ji, 2014; Nguyen et al., 2017; Su et al., 2017). Global relation extractors leverage more information to resolve conflict decisions, which typically leads to more accurate decisions than local ones. Recently, neural network-based models (Zeng et al., 2014; Ji et al., 2017) have been proposed for RE. Lin et al. (2016) proposes an attention mechanism to calculate weights for all sentences of one entity pair and selects plausible sentences from noisy sentences. Different from these are supervised methods that need label data for trai"
D19-1398,W06-1651,0,0.312995,"t al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; Ji et al., 2013; Chen et al., 2014a,b). However, global optimization remains dominated by the ILP solvers, suffering from heavy time expenses. Using integer linear programming for RE as a blackbox solver is a challenging task. First, with the increase of entity pairs and candidate relations, the variables and constraints encoded for ILP increase dramatically, which in return may consume too much computing time and memory. Second, redundant information needs to be encoded cautiously. Consider <United States, capital : 0.6, New York>, <United States, capita"
D19-1398,D14-1203,0,0.331898,"y : 0.03 locationCountry : 0.6 capital : 0.2 locationCity : 0.7 locationCountry : 0.2 nationality : 0.4 locationCountry : 0.3 nationality : 0.1 nationality : 0.05 deathPlace : 0.07 birthPlace : 0.01 largestCity : 0.2 city : 0.1 city : 0.3 Table 1: Example entity pairs and their candidate relations. When making easy decisions, we keep the consistency among candidate relations by making use of constraints (i.e., domain and uniqueness constraints) to eliminate conflicts directly, instead of implicitly encoding constraints in ILP (Yao et al., 2010; de Lacalle and Lapata, 2013; Chen et al., 2014a; Koch et al., 2014; Chen et al., 2014b). As a result, the number of variables and constraints to be encoded in ILP is significantly reduced, which speeds up the entire decision making process. In short, our approach employs an easy-first strategy with information redundancies to make most confident decisions first during which conflict candidate relations are resolved directly by domain and uniqueness constraints, and it only makes the remaining hard decisions with ILP. Our approach is an important improvement of ILPmodels, and it is not a new RE model, but an efficient and effective approach to fully exploitin"
D19-1398,D11-1142,0,0.0279925,"twork-based models (Zeng et al., 2014; Ji et al., 2017) have been proposed for RE. Lin et al. (2016) proposes an attention mechanism to calculate weights for all sentences of one entity pair and selects plausible sentences from noisy sentences. Different from these are supervised methods that need label data for training, we propose an unsupervised method in this study. Disagreements among decisions can be resolved by constraints. Yao et al. (2010) propose a relation extraction model that captures selectional preferences and functionality constraints to integrate information across documents. Fader et al. (2011) implement syntactic and lexical constraints on binary relations expressed by verbs in Open IE systems. Koch et al. (2014) impose type (or domain) constraints to only allow relations over appropriately typed mentions for relation extraction. Similar to (Chen et al., 2014a), our approach utilizes both domain and uniqueness constraints to resolve disagreements. Many global relation extractors use integer linear programming as a blackbox solver (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; Ji et al., 2013; Chen et al., 2014a,b; Wang et al., 2015). The ILP solver der"
D19-1398,W13-3809,0,0.0461121,"Missing"
D19-1398,P14-1038,0,0.0274314,"setting of threshold  is flexible in a range of [0.5, 0.9] for our approach eFIRE. 5 Related Work Relation extraction has been studied extensively in recent years, and can be divided into local re3858 lation extractors (Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Søgaard et al., 2015) using the lexical features, syntactic features, and other local features of sentences, and global relation extractors utilizing the corpus features and relationships among local decisions (Kate and Mooney, 2010; Yao et al., 2010; Li et al., 2011; Singh et al., 2013; Li and Ji, 2014; Nguyen et al., 2017; Su et al., 2017). Global relation extractors leverage more information to resolve conflict decisions, which typically leads to more accurate decisions than local ones. Recently, neural network-based models (Zeng et al., 2014; Ji et al., 2017) have been proposed for RE. Lin et al. (2016) proposes an attention mechanism to calculate weights for all sentences of one entity pair and selects plausible sentences from noisy sentences. Different from these are supervised methods that need label data for training, we propose an unsupervised method in this study. Disagreements amo"
D19-1398,P13-1008,0,0.09106,"Missing"
D19-1398,P16-1200,0,0.197328,"sed methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; Ji et al., 2013; Chen et al., 2014a,b)."
D19-1398,P09-1113,0,0.116846,"that our approach improves the efficiency and accuracy of RE, and outperforms both ILP and neural network-based methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and"
D19-1398,W04-2401,0,0.476715,"l., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; Ji et al., 2013; Chen et al., 2014a,b). However, global optimization remains dominated by the ILP solvers, suffering from heavy time expenses. Using integer linear programming for RE as a blackbox solver is a challenging task. First, with the increase of entity pairs and candidate relations, the variables and constraints encoded for ILP increase dramatically, which in return may consume too much computing time and memory. Second, redundant information needs to be encoded cautiously. Consider <United States, capital : 0.6, New York>, <Un"
D19-1398,C12-1154,0,0.0167946,"Downey et al. (2005) consider redundant extractions for judging the correctness of extractions. Li et al. (2011) take advantage of redundant information to conduct reasoning across documents based on the information network structure. We introduce four classes of information redundancies: S-O, S-R, O-R and R redundancies, and we leverage S-O redundancies for making easy decisions, and the others to aid hard decision making. Easy-first strategy relies on the intuition that “easy decisions should be made early, while harder decisions should be left for later when more information is available (Stoyanov and Eisner, 2012)”. Their method makes easy decisions first for coreference resolution, and further makes use of the information from coreference clusters in the form of features to make later decisions. In this study, we propose to make easy (most confident) decisions first for relation extraction, and then to make hard decisions with ILP, where easy decisions are distinguished from hard ones with redundance scores based on S-O redundancies. Data dependencies have well studied for improving data quality (Ma, 2011; Ma et al., 2015; Fan and Geerts, 2012), which essentially make use of data redundancies and depe"
D19-1398,D12-1042,0,0.247803,"outperforms both ILP and neural network-based methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; J"
D19-1398,D10-1099,0,0.362781,"for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; Ji et al., 2013; Chen et al., 2014a,b). However, global optimization remains dominated by the ILP solvers, suffering from heavy time expenses."
D19-1398,D15-1203,0,0.0269469,"d neural network-based methods. 1 Introduction Relation extraction (RE) has been extensively studied due to its crucial role for many knowledge based applications (Fan et al., 2012; Zhang et al., 2012; Chen et al., 2014a), such as question answering and knowledge graph. There are two types of relation extractors: local and global. The former identify relationships between entity pairs individually according to the local features of sentences, e.g., lexical and syntactic features (Bunescu and Mooney, 2007; Mintz et al., 2009; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). The latter make decisions by further considering joint features of the entire corpus (Yao et al., 2010; Li et al., 2011, 2013; de Lacalle and Lapata, 2013; Chen et al., 2014a). Global relation extractors are able to resolve conflict decisions, and to utilize the dependencies among extracted facts to improve the performance (Li et al., 2011; Chen et al., 2014a), commonly by formalizing RE as a constrained optimization problem and solving RE with integer linear programming (ILP) (Roth and Yih, 2004; Choi et al., 2006; Roth and Yih, 2007; Li et al., 2011; Ji et al., 2013; Che"
D19-1398,C14-1220,0,0.111584,"deanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Søgaard et al., 2015) using the lexical features, syntactic features, and other local features of sentences, and global relation extractors utilizing the corpus features and relationships among local decisions (Kate and Mooney, 2010; Yao et al., 2010; Li et al., 2011; Singh et al., 2013; Li and Ji, 2014; Nguyen et al., 2017; Su et al., 2017). Global relation extractors leverage more information to resolve conflict decisions, which typically leads to more accurate decisions than local ones. Recently, neural network-based models (Zeng et al., 2014; Ji et al., 2017) have been proposed for RE. Lin et al. (2016) proposes an attention mechanism to calculate weights for all sentences of one entity pair and selects plausible sentences from noisy sentences. Different from these are supervised methods that need label data for training, we propose an unsupervised method in this study. Disagreements among decisions can be resolved by constraints. Yao et al. (2010) propose a relation extraction model that captures selectional preferences and functionality constraints to integrate information across documents. Fader et al. (2011) implement syntact"
D19-1637,E17-2076,0,0.0215323,"s. Unsupervised Machine Translation Compared 6156 with supervised machine translation approaches (Cho et al., 2014; Bahdanau et al., 2015), unsupervised machine translation (Lample et al., 2018a,b) does not rely on human-labeled parallel corpora for training. This technique is proved to greatly improve the performance of low-resource languages translation systems. (e.g. English-Urdu translation). The unsupervised machine translation framework is also applied to various other tasks, e.g. image captioning (Feng et al., 2019), text style transfer (Zhang et al., 2018), speech to text translation (Bansal et al., 2017) and clinical text simplification (Weng et al., 2019). The UMT framework makes it possible to apply neural models to tasks where limited human labeled data is available. However, in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same. This is not the case for our task. Under-Translation & Over-Translation Both are troublesome problems for neural sequenceto-sequence models. Most previous related researches adopt the coverage mechanism (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016). However, as far as we know, there were no su"
D19-1637,W08-0336,0,0.107407,"Missing"
D19-1637,D18-1549,0,0.0427362,"Missing"
D19-1637,D16-1096,0,0.024037,"style transfer (Zhang et al., 2018), speech to text translation (Bansal et al., 2017) and clinical text simplification (Weng et al., 2019). The UMT framework makes it possible to apply neural models to tasks where limited human labeled data is available. However, in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same. This is not the case for our task. Under-Translation & Over-Translation Both are troublesome problems for neural sequenceto-sequence models. Most previous related researches adopt the coverage mechanism (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016). However, as far as we know, there were no successful attempt applying coverage mechanism to transformer-based models (Vaswani et al., 2017). 3 Model 3.1 Main Architecture We transform our poem generation task as an unsupervised machine translation problem. As illustrated in Figure 1, based on the recently proposed UMT framework (Lample et al., 2018b), our model is composed of the following components: • Encoder Es and decoder Ds for vernacular paragraph processing • Encoder Et and decoder Dt for classical poem processing where Es (or Et ) takes in a vernacular paragra"
D19-1637,P16-1008,0,0.0185944,"al., 2019), text style transfer (Zhang et al., 2018), speech to text translation (Bansal et al., 2017) and clinical text simplification (Weng et al., 2019). The UMT framework makes it possible to apply neural models to tasks where limited human labeled data is available. However, in previous tasks that adopt the UMT framework, the abstraction levels of source and target language are the same. This is not the case for our task. Under-Translation & Over-Translation Both are troublesome problems for neural sequenceto-sequence models. Most previous related researches adopt the coverage mechanism (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016). However, as far as we know, there were no successful attempt applying coverage mechanism to transformer-based models (Vaswani et al., 2017). 3 Model 3.1 Main Architecture We transform our poem generation task as an unsupervised machine translation problem. As illustrated in Figure 1, based on the recently proposed UMT framework (Lample et al., 2018b), our model is composed of the following components: • Encoder Es and decoder Ds for vernacular paragraph processing • Encoder Et and decoder Dt for classical poem processing where Es (or Et ) takes in a v"
D19-1637,C16-1100,0,0.0211529,"spire better poems. Human evaluation shows our models are able to generate high quality poems, which are comparable to amateur poems. 2 Related Works Classical Chinese Poem Generation Most previous works in classical Chinese poem generation focus on improving the semantic coherence of generated poems. Based on LSTM, Zhang and Lapata (2014) purposed generating poem lines incrementally by taking into account the history of what has been generated so far. Yan (2016) proposed a polishing generation schema, each poem line is generated incrementally and iteratively by refining each line one-by-one. Wang et al. (2016) and Yi et al. (2018) proposed models to keep the generated poems coherent and semantically consistent with the user’s intent. There are also researches that focus on other aspects of poem generation. (Yang et al. (2018) explored increasing the diversity of generated poems using an unsupervised approach. Xu et al. (2018) explored generating Chinese poems from images. While most previous works generate poems based on topic words, our work targets at a novel task: generating poems from vernacular Chinese paragraphs. Unsupervised Machine Translation Compared 6156 with supervised machine translati"
D19-1637,D18-1430,0,0.0190987,"nese poem generation focus on improving the semantic coherence of generated poems. Based on LSTM, Zhang and Lapata (2014) purposed generating poem lines incrementally by taking into account the history of what has been generated so far. Yan (2016) proposed a polishing generation schema, each poem line is generated incrementally and iteratively by refining each line one-by-one. Wang et al. (2016) and Yi et al. (2018) proposed models to keep the generated poems coherent and semantically consistent with the user’s intent. There are also researches that focus on other aspects of poem generation. (Yang et al. (2018) explored increasing the diversity of generated poems using an unsupervised approach. Xu et al. (2018) explored generating Chinese poems from images. While most previous works generate poems based on topic words, our work targets at a novel task: generating poems from vernacular Chinese paragraphs. Unsupervised Machine Translation Compared 6156 with supervised machine translation approaches (Cho et al., 2014; Bahdanau et al., 2015), unsupervised machine translation (Lample et al., 2018a,b) does not rely on human-labeled parallel corpora for training. This technique is proved to greatly improve"
D19-1637,2001.mtsummit-papers.68,0,0.0482613,"collected 487 seven-character quatrain poems from Tang Poems and Song Poems, as well as their corresponding high quality vernacular translations. These poems could be used as gold standards for poems generated from their corresponding vernacular translations. Table 1 shows the statistics of our training, validation and test set. 4.2 Evaluation Metrics Perplexity Perplexity reflects the probability a model generates a certain poem. Intuitively, a better model would yield higher probability (lower perplexity) on the gold poem. BLEU As a standard evaluation metric for machine translation, BLEU (Papineni et al., 2001) measures the intersection of n-grams between the generated poem and the gold poem. A better generated poem usually achieves higher BLEU score, as it shares more n-gram with the gold poem. Human evaluation While perplexity and BLEU are objective metrics that could be applied to large-volume test set, evaluating Chinese poems is after all a subjective task. We invited 30 human evaluators to join our human evaluation. The human evaluators were divided into two groups. The expert group contains 15 people who hold a bachelor degree in Chinese literature, and the amateur group contains 15 people wh"
D19-1637,D14-1074,0,0.134951,"arable to amateur poems. 1 Introduction During thousands of years, millions of classical Chinese poems have been written. They contain ancient poets’ emotions such as their appreciation for nature, desiring for freedom and concerns for their countries. Among various types of classical poetry, quatrain poems stand out. On the one hand, their aestheticism and terseness exhibit unique elegance. On the other hand, composing such poems is extremely challenging due to their phonological, tonal and structural restrictions. Most previous models for generating classical Chinese poems (He et al., 2012; Zhang and Lapata, 2014) are based on limited keywords or characters at fixed positions (e.g., acrostic poems). ∗ Equal contribution Since users could only interfere with the semantic of generated poems using a few input words, models control the procedure of poem generation. In this paper, we proposed a novel model for classical Chinese poem generation. As illustrated in Figure 1, our model generates a classical Chinese poem based on a vernacular Chinese paragraph. Our objective is not only to make the model generate aesthetic and terse poems, but also keep rich semantic of the original vernacular paragraph. Therefo"
D19-1637,P02-1040,0,\N,Missing
D19-1637,D14-1179,0,\N,Missing
N10-1011,P08-1032,1,0.555257,"|w1 ). We are not aware of any previous work that empirically assesses which measure is best at capturing semantic similarity. We undertake such an empirical comparison as it is not a priory obvious how similarity is best modeled under a multimodal representation. 4 Experimental Setup In this section we discuss our experimental design for assessing the performance of the model presented above. We give details on our training procedure and parameter estimation and present the baseline method used for comparison with our model. Data We trained the multimodal topic model on the corpus created in Feng and Lapata (2008). It contains 3,361 documents that have been downloaded from the BBC News website.2 Each document comes with an image that depicts some of its content. The images are usually 203 pixels wide 2 http://news.bbc.co.uk/ and 152 pixels high. The average document length is 133.85 words. The corpus has 542,414 words in total. Our experiments used a vocabulary of 6,253 textual words. These were words that occurred at least five times in the whole corpus, excluding stopwords. The accompanying images were preprocessed as follows. We first extracted SIFT features from each image (150 on average) which we"
N10-1011,D09-1081,0,0.0306922,"Missing"
N10-1011,D09-1066,0,0.0309453,"Missing"
N10-1125,P08-1032,1,0.542857,"as annotations for the image. These annotations are undoubt831 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831–839, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics edly noisy, but plenty and cost-free. Moreover, the collateral text is often longer and more informative in comparison to the few keywords reserved for each image in Corel. In this paper we propose a probabilistic image annotation model that learns to automatically label images under such noisy conditions. We use the database created in Feng and Lapata (2008) which consists of news articles, images, and their captions. Our model exploits the redundancy inherent in this multimodal dataset by assuming that images and their surrounding text are generated by a shared set of latent variables or topics. Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms (visiterms). Due to polysemy and synonymy many words in this vocabulary will refer to the same underlying concept. Using Latent Dirichlet Allocation (LDA, Blei and Jordan 2003), a probabilistic model of text generation, we represe"
N18-1017,D16-1032,0,0.0921085,"ional Sentence Generation: Controllable sentence generation with external information is wildly studied from different views. From the task perspective, Fan et al. (2017) utilize label information for generation, and tackle information coverage in a summarization task. He et al. (2017a) use recursive Network to represent knowledge base, and Bordes and Weston (2016) track generation states and provide information enrichment, both are in a dialog setting. In terms of network architecture, Wen et al. (2015) equip LSTM with a semantic control cell to improve informativeness of generated sentence. Kiddon et al. (2016) propose the neural checklist model to explicitly track what has been mentioned and what left to say by splitting these two into different lists. Our model is related to these models with respect to information representation and challenges from coverage and redundancy. The most closely related one is the checklist model. But it does not explicitly study information redundancy. Also, the information we track is heterogeneous, and we track it in a different way, i.e. using Cumulative attention. 4 Our Model Our model consists of the question encoder, the heterogeneous memory, and the decoder. Th"
N18-1017,D16-1147,0,0.176522,"Cameron directed the Titanic. is more favorable than the single name James Cameron. A straightforward solution to compose an answer sentence is to build a template based model, where the answer Both GenQA and CoreQA are designed to work with a structured KB as the memory, while in most real-world scenarios, we require knowledge from different resources, hence of different formats. This knowledge may come from structured KBs, documents, or even tables. It is admittedly challenging to leverage a heterogeneous memory in a neural generation framework, and it is not well studied in previous works (Miller et al., 2016). Here in our case, the memory should contain two main formats: KB triples and semi-structured en185 Proceedings of NAACL-HLT 2018, pages 185–195 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics tities from IE, forming a heterogeneous memory (HM). The former is usually organized in is a subject-predicate-object form, while, the latter is usually extracted from textual documents, in the form of keywords, sometimes associated with certain categories or tags oriented to specific tasks (Bordes and Weston, 2016). Miller et al. (2016) discuss different knowl"
N18-1017,P02-1040,0,0.101685,"he percentage of cases whose answers are perfectly covered. Here, the definition of coverage is similar in spirit with the conventional recall as both measure how many gold words are included in the output. Specifically, Cpart is essentially the same as recall with respect to its own cases. Note that perfect coverage is the most difficult, while single coverage is the easiest one. For Enrich, we measure the number of none-answer memory items included in the output. Regarding Redundancy, we calculate the times of repetition for memory values in the answer sentence. We also compute BLEU scores (Papineni et al., 2002) on the WikiMovies-Wikipedia, as an indicator of naturalness, to some extent. Table 2: The data format of WikiMovies used in our experiment. As shown in Table 2, we treat each question in WikiMovies with its original answer (usually one or more words) as a QA pair, and one of the question’s supportive sentences (either from Wikipedia or templates) as its goldstandard answer sentence. For each question, the memory will contain all knowledge triples about the question’s topic movie from the KB entries, and also include entities and keywords extracted from its IE portion. For each entry in KB ent"
N18-1017,P17-1099,0,0.443444,"y, we propose the Cumulative Attention mechanism, which uses the context of the decoder history to address the memory, thus reduces repetition at memory addressing time. We conduct experiments on two WikiMovies datasets, and experimental results show that our model is able to generate natural answer sentences composed of extra related facts about the question. 2 Related Work Natural Answer Generation with Sequence to Sequence Learning: Sequence to sequence models (with attention) have achieved successful results in many NLP tasks (Cho et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2015; See et al., 2017). Memory is an effective way to equip seq2seq systems with external information (Weston et al., 2014; Sukhbaatar et al., 2015; Miller et al., 2016; Kumar et al., 2015). GenQA (Yin et al., 2015) applies a seq2seq model to generate natural answer sentences from a knowledge base, and CoreQA (He et al., 2017b) extends it with copying mechanism (Gu et al., 2016). But they do not consider the heterogeneity of the memory, only tackle questions with one single answer word, and do not study information enrichment. Memory and Attention: There are also increasing works focusing on different memory repres"
N18-1017,P16-1154,0,0.0457845,"n. 2 Related Work Natural Answer Generation with Sequence to Sequence Learning: Sequence to sequence models (with attention) have achieved successful results in many NLP tasks (Cho et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2015; See et al., 2017). Memory is an effective way to equip seq2seq systems with external information (Weston et al., 2014; Sukhbaatar et al., 2015; Miller et al., 2016; Kumar et al., 2015). GenQA (Yin et al., 2015) applies a seq2seq model to generate natural answer sentences from a knowledge base, and CoreQA (He et al., 2017b) extends it with copying mechanism (Gu et al., 2016). But they do not consider the heterogeneity of the memory, only tackle questions with one single answer word, and do not study information enrichment. Memory and Attention: There are also increasing works focusing on different memory representations and the interaction between the decoder and memory, i.e., attention. Miller et al. (2016) propose the Key-Value style memory to explore textual knowledge (both structured and unstructured) from different sources, but they still utilize them separately, without a uniform addressing and attention mechanism. Daniluk et al. (2017) split the decoder st"
N18-1017,P17-1162,0,0.257491,"ames Cameron James Cameron directed the Titanic. James Cameron directed it. James Cameron directed it in 1999. Table 1: Answer sentences generated by different QA systems word James Cameron and topic word in the question the Titanic are filled into a pre-defined template (Chu-Carroll et al., 2004). But such systems intrinsically lack variety, hence hard to generalize to new domains. To produce more natural answer sentences, Yin et al. (2015) proposed GenQA, an encoderdecoder based model to select candidate answers from a KB styled memory during decoding to generate an answer sentence. CoreQA (He et al., 2017b) further extended GenQA with a copy mechanism to learn to copy words from the question. The application of attention mechanism enables those attempts to successfully learn sentence varieties from the memory and training data, such as usage of pronouns (A3 in Table 1). However, since they are within the encoder-decoder framework, they also encounter the well noticed repetition issue: due to loss of temporary decoder state, an RNN based decoder may repeat what has already been said during generation (Tu et al., 2016a,b). Introduction Most previous question answering systems focus on finding ca"
N18-1017,E17-2047,0,0.0164871,"hidden states st = LST Menc (qt , st−1 ). These s are later used for decoder’s attention. The last hidden state sn is used as the vector representation of the question, and is later put into the initial hidden state of the decoder. Due to loss of states across time steps, the decoder may generate duplicate outputs. Attempts have been made to address this problem. Some architectures try to utilize History attention records. See et al. (2017) introduce a coverage mechanism, and Paulus et al. (2017) use history attention weights to normalize new attention. Others are featured in network modules. Suzuki and Nagata (2017) estimate the frequency of target words and record the occurrence. Our model shows that simply attending to history decoder states can reduce redundancy. Then we use the context vector of attention to history decoder states to perform attention to the memory. Doing this enables the decoder to correctly decide what to say at mem187 ⟨N ⟩ ht ⟨K⟩ , ht ⟨V ⟩ , and ht : ht ⟨N ⟩ = W n ht ⟨K⟩ ht ⟨V ⟩ ht = Wk ht = Wv ht where the W s are initialized as identity matrix ⟨N ⟩ I = diag(1, 1...1). ht will be projected to normal word vocabulary V norm to form a distribu⟨N ⟩ ⟨K⟩ ⟨V ⟩ tion pt . ht and ht will b"
N18-1017,P17-1019,0,0.437475,"ames Cameron James Cameron directed the Titanic. James Cameron directed it. James Cameron directed it in 1999. Table 1: Answer sentences generated by different QA systems word James Cameron and topic word in the question the Titanic are filled into a pre-defined template (Chu-Carroll et al., 2004). But such systems intrinsically lack variety, hence hard to generalize to new domains. To produce more natural answer sentences, Yin et al. (2015) proposed GenQA, an encoderdecoder based model to select candidate answers from a KB styled memory during decoding to generate an answer sentence. CoreQA (He et al., 2017b) further extended GenQA with a copy mechanism to learn to copy words from the question. The application of attention mechanism enables those attempts to successfully learn sentence varieties from the memory and training data, such as usage of pronouns (A3 in Table 1). However, since they are within the encoder-decoder framework, they also encounter the well noticed repetition issue: due to loss of temporary decoder state, an RNN based decoder may repeat what has already been said during generation (Tu et al., 2016a,b). Introduction Most previous question answering systems focus on finding ca"
N18-1017,P16-5005,0,0.0137375,"rom a KB styled memory during decoding to generate an answer sentence. CoreQA (He et al., 2017b) further extended GenQA with a copy mechanism to learn to copy words from the question. The application of attention mechanism enables those attempts to successfully learn sentence varieties from the memory and training data, such as usage of pronouns (A3 in Table 1). However, since they are within the encoder-decoder framework, they also encounter the well noticed repetition issue: due to loss of temporary decoder state, an RNN based decoder may repeat what has already been said during generation (Tu et al., 2016a,b). Introduction Most previous question answering systems focus on finding candidate words, phrases or sentence snippets from many resources, and ranking them for their users (Chu-Carroll et al., 2004; Xu et al., 2016). Typically, candidate answers are collected from different resources, such as knowledge base (KB) or textual documents, which are often with heterogeneous formats, e.g., KB triples or semi-structured results from Information Extraction (IE). For factoid questions, a single answer word or phrase is chosen as the response for users, as shown in Table 1 (A1). However, in many rea"
N18-1017,P16-1008,0,0.0151795,"rom a KB styled memory during decoding to generate an answer sentence. CoreQA (He et al., 2017b) further extended GenQA with a copy mechanism to learn to copy words from the question. The application of attention mechanism enables those attempts to successfully learn sentence varieties from the memory and training data, such as usage of pronouns (A3 in Table 1). However, since they are within the encoder-decoder framework, they also encounter the well noticed repetition issue: due to loss of temporary decoder state, an RNN based decoder may repeat what has already been said during generation (Tu et al., 2016a,b). Introduction Most previous question answering systems focus on finding candidate words, phrases or sentence snippets from many resources, and ranking them for their users (Chu-Carroll et al., 2004; Xu et al., 2016). Typically, candidate answers are collected from different resources, such as knowledge base (KB) or textual documents, which are often with heterogeneous formats, e.g., KB triples or semi-structured results from Information Extraction (IE). For factoid questions, a single answer word or phrase is chosen as the response for users, as shown in Table 1 (A1). However, in many rea"
N18-1017,D15-1199,0,0.0917991,"Missing"
N18-1017,C16-1226,1,0.822053,"ism enables those attempts to successfully learn sentence varieties from the memory and training data, such as usage of pronouns (A3 in Table 1). However, since they are within the encoder-decoder framework, they also encounter the well noticed repetition issue: due to loss of temporary decoder state, an RNN based decoder may repeat what has already been said during generation (Tu et al., 2016a,b). Introduction Most previous question answering systems focus on finding candidate words, phrases or sentence snippets from many resources, and ranking them for their users (Chu-Carroll et al., 2004; Xu et al., 2016). Typically, candidate answers are collected from different resources, such as knowledge base (KB) or textual documents, which are often with heterogeneous formats, e.g., KB triples or semi-structured results from Information Extraction (IE). For factoid questions, a single answer word or phrase is chosen as the response for users, as shown in Table 1 (A1). However, in many real-world scenarios, users may prefer more natural responses rather than a single word. For example, as A2 in Table 1, James Cameron directed the Titanic. is more favorable than the single name James Cameron. A straightfor"
N19-1301,C16-1236,0,0.0294861,"fashion, making the models relatively less interpretable. Conceptually similar to our STOP strategy, Shen et al. [2017] propose the termination gate mechanism based on a random variable generated from the internal state for reading comprehension. In contrast, our model attempts to learn a general STOP key embedding based on the incrementally updated query representations, which can be learned from question-answer pairs only and lead to more explicit reasoning interpretations over structured KBs. This make our model potentially suit more real scenarios. Our work is also related to [Jain, 2016; Bao et al., 2016], which are designed to support reasoning for multi-relation questions by exploring the relation path and certain KB schema, e.g., CVT nodes, in the Freebase. The former also considers previously-addressed keys during query updating, but ignores the value representations. Thus, it still requires predefined rules and threshold to artificially add intermediate value representations to update the query. The latter also relies on a set of predefined rules to perform reasoning over Freebase. In contrast, our model incorporates both the key and value representations into the query representations,"
N19-1301,P14-1133,0,0.0160715,"in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016] further proposed the K"
N19-1301,D13-1160,0,0.133507,"trong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]."
N19-1301,P15-1026,0,0.0224465,"res the ability to properly analyze and represent the natural language questions against knowledge bases, especially for those involving multiple entities and relations, which we also call as reasoning over the KBs; (2) training such interpretable question understanding models requires considerable strong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [We"
N19-1301,N16-2016,0,0.26833,"properly analyze and represent the natural language questions against knowledge bases, especially for those involving multiple entities and relations, which we also call as reasoning over the KBs; (2) training such interpretable question understanding models requires considerable strong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al.,"
N19-1301,D13-1161,0,0.0368492,"ich is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016"
N19-1301,D16-1147,0,0.0729346,"t al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016] further proposed the Key-Value Memory Network, which generalizes the MemNN by storing facts in a key-value structured memory. Both of the two models could perform shallow reasoning over the memory, since they can find answers by consecutively making predictions over multiple memory slots. Compared to the f"
N19-1301,Q14-1030,0,0.0494423,"Missing"
N19-1301,P10-1040,0,0.218523,"Missing"
N19-1301,P14-1090,0,0.125604,"Missing"
N19-1301,N15-3014,0,0.0209391,"allenges in the open domain KB-QA task: (1) it often requires the ability to properly analyze and represent the natural language questions against knowledge bases, especially for those involving multiple entities and relations, which we also call as reasoning over the KBs; (2) training such interpretable question understanding models requires considerable strong annotations, which is expensive to obtain in practice. Existing works address these using either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over s"
N19-1301,P15-1128,0,0.127047,"sing either the information retrieval (IR) based solutions or the semantic paring (SP) based approaches. The IRbased models [Yao and Van Durme, 2014; Yao, 2015; Bast and Haussmann, 2015; Bordes et al., 2015; Dong et al., 2015; Jain, 2016; Lai et al., 2019] tackle the KB-QA task by developing various ranking models towards the candidate answers, which implicitly meet the reasoning requirements during the candidate-searching step or in designing the ranking functions. In contrast, the SP-based approaches [Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016] explicitly represent the meaning of questions as logical forms or structured queries that naturally support reasoning over structured KBs. More recently, memory based reasoning solutions [Weston et al., 2014; Miller et al., 2016] are proposed to tackle the task. [Weston et al., 2014] proposed the Memory Neural Networks (MemNNs), which enable the neural network models read/write on an external memory component, and are further extended into an End-to-End fashion [Sukhbaatar et al., 2015a]. [Miller et al., 2938 2016] further proposed the Key-Value Memory Network, which general"
P10-1126,P00-1041,0,0.196994,"or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation as follows. Given an"
P10-1126,W03-0501,0,0.008615,"manual annotation or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation a"
P10-1126,P08-1032,1,0.427614,"to the image keywords and consider only the n-best ones. Alternatively, we could consider the single most relevant sentence together with its surrounding context under the assumption that neighboring sentences are about the same or similar topics. 7 Experimental Setup In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models. Data All our experiments were conducted on the corpus created by Feng and Lapata (2008), following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing). Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. The underlying topic model was trained with 1,000 topics using only content words (i.e., nouns, verbs, and adjectives) that appeared no less than five times"
P10-1126,N10-1125,1,0.549071,"levance to the article, provide context for the picture, and ultimately draw the reader into the article. It is also worth noting that journalists often write their own captions rather than simply extract sentences from the document. In doing so they rely on general world knowledge but also expertise in current affairs that goes beyond what is described in the article or shown in the picture. 4 Image Annotation As mentioned earlier, our approach relies on an image annotation model to provide description keywords for the picture. Our experiments made use of the probabilistic model presented in Feng and Lapata (2010). The latter is well-suited to our task as it has been developed with noisy, multimodal data sets in mind. The model is based on the assumption that images and their surrounding text are generated by mixtures of latent topics which are inferred from a concatenated representation of words and visual features. Specifically, images are preprocessed so that they are represented by word-like units. Local image descriptors are computed using the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). The general idea behind the algorithm is to first sample an image with the difference-of-Ga"
P10-1126,C02-1137,0,0.0287464,"gical information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation as follows. Given an image I, and a related kno"
P10-1126,P03-1054,0,0.00216815,"similar topics. 7 Experimental Setup In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models. Data All our experiments were conducted on the corpus created by Feng and Lapata (2008), following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing). Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. The underlying topic model was trained with 1,000 topics using only content words (i.e., nouns, verbs, and adjectives) that appeared no less than five times in the corpus. For all models discussed here (extractive and abstractive) we report results with the 15 best annotation keywords. For the abstractive models, we used a trigram model trained with the SRI toolkit on a newswire corpus co"
P10-1126,J98-3004,0,0.0362029,"eral framework for generating text descriptions of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or"
P10-1126,2006.amta-papers.25,0,0.0151678,"size was set to 500 (with at least 50 states for the word-based model). For the phrase-based model, we also experimented with reducing the search scope, either by considering only the n most similar sentences to the keywords (range [2, 10]), or simply the single most similar sentence and its neighbors (range [2, 5]). The former method delivered better results with 10 sentences (and the KL divergence similarity function). Evaluation We evaluated the performance of our models automatically, and also by eliciting human judgments. Our automatic evaluation was based on Translation Edit Rate (TER, Snover et al. 2006), a measure commonly used to evaluate the quality of machine translation output. TER is defined as the minimum number of edits a human would have to perform to change the system output so that it exactly matches a reference translation. In our case, the original captions written by the BBC journalists were used as reference: Ins + Del + Sub + Shft (16) Nr where E is the hypothetical system output, Er the reference caption, and Nr the reference length. The number of possible edits include insertions (Ins), deletions (Del), substitutions (Sub) and shifts (Shft). TER is similar to word error rate"
P14-1077,P07-1073,0,0.0995413,"rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al"
P14-1077,W13-3809,0,0.197069,"Missing"
P14-1077,D11-1142,0,0.176793,"he datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracte"
P14-1077,D12-1042,0,0.574967,"es, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗ Yansong Feng is the corresponding author. 818 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations. The first layer is a multi-class classifier making local predictions for single sentences, the output of which are aggregated by the second layer into the entity pair level. Their approach only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the l"
P14-1077,P12-1076,0,0.014792,"upervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entit"
P14-1077,P11-1055,0,0.714541,"object, and in most cases, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗ Yansong Feng is the corresponding author. 818 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations. The first layer is a multi-class classifier making local predictions for single sentences, the output of which are aggregated by the second layer into the entity pair level. Their approach only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These ca"
P14-1077,D10-1099,0,0.584396,"relationship between pairs of entities is crucial for many knowledge base related applications(Suchanek et al., 2013). In the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored. Take the relation Capital as an example, we can imagine that this relation will expect a country as its subject and a city as object, and in most cases, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗ Yansong Feng is the corresponding author. 818 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among"
P14-1077,W12-3022,0,0.0222652,"ted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entity tuples to have more 3 The Framework Our framework takes a set of entity pairs and their supporting sentences as its input. We first train a p"
P14-1077,P13-1008,0,0.037412,"on dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the latent type information of the relations’ arguments, which is learnt from various data sources. In contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type clues and relation ca"
P14-1077,P09-1113,0,0.636972,"er, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a u"
P14-1077,P13-2141,0,0.012565,"local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type clues and relation cardinality expectations to discover possible inconsistencies among local predictions. the expected type and cardinality requirements for a relation’s arguments, and jointly resolve the disagreements among candidate predictions. We formalize this procedure as a constrained optimization problem, which can be solved by many optimization frameworks. We use integer linear programming (ILP) as the solver and evaluate our framework on English and Chinese datasets. The e"
P14-1077,P05-1052,0,0.141743,"work on English and Chinese datasets. The experimental results show that our framework performs better than the state-of-the-art approaches when such clues are applicable to the datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain nois"
P14-1077,N13-1008,0,0.499272,"for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations. The first layer is a multi-class classifier making local predictions for single sentences, the output of which are aggregated by the second layer into the entity pair level. Their approach only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the latent type information of the relations’ arguments, which is learnt from various data sources. In contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine exis"
P14-1077,D13-1040,0,\N,Missing
P15-2037,D13-1160,0,0.0476537,"ension of the comparison and its ranking order. Currently, our analysis suffers from the limited coverage of our WikiDiF. In the future, it would be interesting to improve our method to cover more KB predicates, and extend our NN model with more advanced structures to further improve the performances and also simultaneously characterize the target and comparison set involved. Question Answering over Freebase Acknowledgments We also investigate how our semantic analysis for superlatives can help improve question answering on two benchmark datasets, Free917(Cai and Yates, 2013) and WebQuestions(Berant et al., 2013), which contain 35 questions with attributive superlative expressions in total. We inject our formal analysis as superlative-triggered aggregation operations into an existing system, Xu14(Xu et al., 2014). Note that we leave the comparison set to be decided by Xu14’s parser. The 35 superlative-triggered complex questions can not be correctly answered by most stateof-the-art systems(Berant et al., 2013; Yao and We would like to thank Heng Ji, Benjamin Van Durme, Liwei Chen and Bingfeng Luo for their helpful discussions and three anonymous reviewers for their insightful comments that greatly imp"
P15-2037,W06-1602,0,0.458391,"lect, from Freebase predicates, the most appropriate comparison dimension for a given superlative expression, and further determine its ranking order heuristically. Experimental results show that it is possible to learn from coarsely obtained training data to semantically characterize the comparative constructions involved in attributive superlative expressions. 1 1. Target: one or more items that work as the protagonist of the utterance, and are being compared within the comparative construction, e.g., Nile; 2. Comparison set: the set of items that are being compared against in the utterance(Bos and Nissim, 2006), e.g., all rivers in the world; Introduction Superlatives are fairly common in natural languages and play an essential role in daily communications, when in conveying comparisons among a set of items or degrees of certain properties. Properly analyzing superlative expressions holds the promise for many applications such as question answering (QA), text entailment, sentiment analysis and so on. In literature, analysis of superlatives has drawn more interests from both formal linguistics and semantics(Szabolcsi, 1986; Gawron, 1995; Heim, 1999; Farkas and Kiss, 2000), but relatively less attenti"
P15-2037,S13-1045,0,0.0281309,"e construction in two aspects, the dimension of the comparison and its ranking order. Currently, our analysis suffers from the limited coverage of our WikiDiF. In the future, it would be interesting to improve our method to cover more KB predicates, and extend our NN model with more advanced structures to further improve the performances and also simultaneously characterize the target and comparison set involved. Question Answering over Freebase Acknowledgments We also investigate how our semantic analysis for superlatives can help improve question answering on two benchmark datasets, Free917(Cai and Yates, 2013) and WebQuestions(Berant et al., 2013), which contain 35 questions with attributive superlative expressions in total. We inject our formal analysis as superlative-triggered aggregation operations into an existing system, Xu14(Xu et al., 2014). Note that we leave the comparison set to be decided by Xu14’s parser. The 35 superlative-triggered complex questions can not be correctly answered by most stateof-the-art systems(Berant et al., 2013; Yao and We would like to thank Heng Ji, Benjamin Van Durme, Liwei Chen and Bingfeng Luo for their helpful discussions and three anonymous reviewers for thei"
P15-2037,P14-1090,0,0.0455063,"Missing"
P15-2037,P09-1113,0,0.0480433,"no available datasets that can be used directly for our task, especially no annotations against structured KBs. We therefore present a distantly supervised method to collect annotated training data from rich text resources of Wikipedia and the help of Freebase, without much human involvement. The key assumption behind our method is that if a superlative expression frequently appears in a context that may describe a KB predicate, then this predicate probably plays an important role in the comparative construction triggered by this superlative. Inspired by recent advances in relation extraction(Mintz et al., 2009), given a Freebase predicate, we are able to collect many sentences from Wikipedia pages, which more or less describe this predicate, without extra human annotation. These sentences in turn can be used to collect the cooccurrences between a superlative expression and this predicate. In more detail, we first find all Freebase predicates that may involve in comparative constructions, i.e., all gradable predicates, e.g., fb:geography.river.length, on which differIn this paper, we propose a novel task, semantically interpreting the comparative constructions inherent in attributive superlative expr"
P15-2037,P07-3012,0,0.0580022,"Missing"
P16-1220,N07-4013,0,0.14223,"Missing"
P16-1220,P14-1091,0,0.527771,"efine these candidate answers by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones. While the overview in Figure 1 works for questions containing single Freebase relation, it also works for questions involving multiple Freebase relations. Consider the question who plays anakin skywalker in star wars 1. The actors who are the answers to this question should satisfy the following constraints: (1) the actor played anakin skywalker; and (2) the actor played in star wars 1. Inspired by Bao et al. (2014), we design a dependency treebased method to handle such multi-relational questions. We first decompose the original question into a set of sub-questions using syntactic patterns which are listed in Appendix. The final answer set of the original question is obtained by intersecting the answer sets of all its sub-questions. For the 2327 example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer."
P16-1220,P14-1133,0,0.149467,"The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compo"
P16-1220,Q15-1039,0,0.599273,"Missing"
P16-1220,P11-1055,0,0.0472067,"Missing"
P16-1220,D13-1160,0,0.510828,"the MCCNN, and the target prediction during testing time is over these relations. 5.2 Experimental Settings We have 6 dependency tree patterns based on Bao et al. (2014) to decompose the question into subquestions (See Appendix). We initialize the word embeddings with Turian et al. (2010)’s word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of Berant et al. (2013), containing 4M entities and 5,323 relations. 5.3 Results and Discussion We use the average question-wise F1 as our evaluation metric.4 To give an idea of the impact of different configurations of our method, we compare the following with existing methods. Structured. This method involves inference on Freebase only. First the entity linking (EL) system is run to predict the topic entity. Then we run the relation extraction (RE) system and select the best relation that can occur with the topic entity. We choose this entity-relation pair to predict the answer. 4 We use the evaluation script avai"
P16-1220,D14-1067,0,0.330735,"t of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function"
P16-1220,P13-1042,0,0.372956,"Missing"
P16-1220,D14-1117,0,0.0839941,"Missing"
P16-1220,D12-1069,0,0.143,"Missing"
P16-1220,D13-1161,0,0.250474,"mprovement over the state-of-the-art. 1 Introduction Since the advent of large structured knowledge bases (KBs) like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Auer et al., 2007), answering natural language questions using those structured KBs, also known as KBbased question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities. The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate"
P16-1220,P15-2047,0,0.00839767,"ntually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and predict a single KB triple (e, r, ?) for each subquestion (here ? stands for the answer entities). The QA pr"
P16-1220,P14-5010,0,0.00510178,"use the shortest path between an entity mention and the question word in the dependency tree3 as input to the first channel. Similar to Xu et al. (2015), we treat the path as a concatenation of vectors of words, dependency edge directions and dependency labels, and feed it to the convolution layer. Note that, the entity mention and the question word are excluded from the dependency path so as to learn a more general relation representation in syntactic level. As shown in Figure 2, the dependency path between who and shaq is ← dobj – play – nsubj →. 3 We use Stanford CoreNLP dependency parser (Manning et al., 2014). 2328 Sentential Features This channel takes the words in the sentence as input excluding the question word and the entity mention. As illustrated in Figure 2, the vectors for did, first, play and for are fed into this channel. 3.2.2 Objective Function and Learning The model is learned using pairs of question and its corresponding gold relation from the training data. Given an input question x with an annotated entity mention, the network outputs a vector o(x), where the entry ok (x) is the probability that there exists the k-th relation between the entity and the expected answer. We denote t"
P16-1220,P09-1113,0,0.125939,"Missing"
P16-1220,D13-1184,0,0.0223252,"s to Wikipedia, that person might first determine that the question is about Shaquille O’Neal, then go to O’Neal ’s Wikipedia page, and search for the sentences that contain the candidate answers as evidence. By analyzing these sentences, one can figure out whether a candidate answer is correct or not. 4.1 Finding Evidence from Wikipedia As mentioned above, we should first find the Wikipedia page corresponding to the topic entity in the given question. We use Freebase API to convert Freebase entity to Wikipedia page. We extract the content from the Wikipedia page and process it with Wikifier (Cheng and Roth, 2013) which recognizes Wikipedia entities, which can further be linked to Freebase entities using Freebase API. Additionally we use Stanford CoreNLP (Manning et al., 2014) for tokenization and entity co-reference resolution. We search for the sentences containing the candidate answer entities retrieved from Freebase. For example, the Wikipedia page of O’Neal contains a sentence “O’Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft”, which is taken into account by the refinement model (our inference model on Wikipedia) to discriminate whether Orlando Magic is the"
P16-1220,N15-1077,0,0.0182235,"Missing"
P16-1220,P15-1026,0,0.785384,"ng examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select t"
P16-1220,Q14-1030,1,0.879493,"thods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions th"
P16-1220,Q16-1010,1,0.244478,"Missing"
P16-1220,P13-1158,0,0.290087,"Missing"
P16-1220,N10-1145,0,0.0208895,"Missing"
P16-1220,N13-1008,0,0.0574113,"Missing"
P16-1220,P16-1056,0,0.00682235,"Missing"
P16-1220,P10-1040,0,0.0233341,"ct to a mediator node, and the second from the mediator to the object node. For each relation candidate r, we issue the query (e, r, ?) to the KB, and label the relation that produces the answer with minimal F1 -loss against the gold answer, as the surrogate gold relation. From the training set, we collect 461 relations to train the MCCNN, and the target prediction during testing time is over these relations. 5.2 Experimental Settings We have 6 dependency tree patterns based on Bao et al. (2014) to decompose the question into subquestions (See Appendix). We initialize the word embeddings with Turian et al. (2010)’s word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of Berant et al. (2013), containing 4M entities and 5,323 relations. 5.3 Results and Discussion We use the average question-wise F1 as our evaluation metric.4 To give an idea of the impact of different configurations of our method, we compare the following with existing methods. Structured. T"
P16-1220,D07-1003,0,0.0587728,"Missing"
P16-1220,P15-1129,0,0.0159012,"gates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella’s mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer’s gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by Wang et al. (2015)). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, . . . her mother was Isabella of Barcelos . . . , can act as a further constraint to answer the question correctly. We present a novel method for question answering which infers on both structured and unstructured resources. Our method consists of two main steps as outlined in §2. In the first step we extract answers for a given question using a structured KB (here Freebase) by jointly performing entity linking and relation"
P16-1220,P14-1090,0,0.525003,"Missing"
P16-1220,N13-1106,0,0.0263742,"Missing"
P16-1220,N15-3014,0,0.252342,"eaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America bec"
P16-1220,P13-1171,0,0.0610357,"Missing"
P16-1220,P14-2105,0,0.102811,"a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North"
P16-1220,P15-1128,0,0.571937,"retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and pre"
P16-1220,D15-1062,1,0.178255,"guated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and predict a single KB triple (e, r, ?) for each subquestion (here ? stands for the answer entities). The QA problem is thus form"
P16-1220,D12-1035,0,0.0586932,"Missing"
P16-1220,P15-1049,0,0.0342182,"the 2327 example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer. 3 dobj [Who] did [shaq] first Inference on Freebase Convolution 3.1 Entity Linking For each question, we use hand-built sequences of part-of-speech categories to identify all possible named entity mention spans, e.g., the sequence NN (shaq) may indicate an entity. For each mention span, we use the entity linking tool S-MART2 (Yang and Chang, 2015) to retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answeri"
P16-1220,D15-1237,0,0.0132239,"Missing"
P17-1040,D11-1141,0,0.0360751,"he only work in neural-network-based noise modeling is to use one single global transition matrix to model the noise introduced by crosslingual projection of training data (Fang and Cohn, 2016). Our work advances them through generating a transition matrix dynamically for each instance, to avoid using one single component to characterize both reliable and unreliable data. In addition to relation extraction, distant supervision (DS) is shown to be effective in generating training data for various NLP tasks, e.g., tweet sentiment classification (Go et al., 2009), tweet named entity classifying (Ritter et al., 2011), etc. However, these early applications of DS do not well address the issue of data noise. In relation extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by (Takamatsu et al., 2012) removes potential noisy sentences by identifying bad syntactic patterns at the preprocessing stage. (Xu et al., 2013) use pseudorelevance feedback to find possible false negative data. (Riedel et al., 2010) make the at-leastone assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Followi"
P17-1040,K16-1018,0,0.0182451,"vely trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios. 1 Introduction Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks – from relation extraction (Mintz et al., 2009) and sentiment classification (Go et al., 2009) to cross-lingual semantic analysis (Fang and Cohn, 2016). By using knowledge learned from seed examples to label data, DS automatically prepares large scale training data for these tasks. While promising, DS does not guarantee perfect results and often introduces noise to the generated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple 430 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 430–439 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18"
P17-1040,Q13-1030,0,0.120447,"hanxing Zhu3 , Songfang Huang4 , Rui Yan1 and Dongyan Zhao1 1 ICST, Peking University, China 2 School of Computing and Communications, Lancaster University, UK 3 Peking University, China 4 IBM China Research Lab, China {bf luo,fengyansong,zhanxing.zhu,ruiyan,zhaody}@pku.edu.cn z.wang@lancaster.ac.uk huangsf@cn.ibm.com Abstract as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works (Takamatsu et al., 2012; Ritter et al., 2013) show that DS often mistakenly labels real positive instances as negative (false negative) or versa vice (false positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-"
P17-1040,P11-1055,0,0.0938601,"n extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by (Takamatsu et al., 2012) removes potential noisy sentences by identifying bad syntactic patterns at the preprocessing stage. (Xu et al., 2013) use pseudorelevance feedback to find possible false negative data. (Riedel et al., 2010) make the at-leastone assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Following this assumption, people further improves the original paradigm using probabilistic graphic models (Hoffmann et al., 2011; Surdeanu et al., 2012), and neural network methods (Zeng et al., 2015). Recently, (Lin et al., 2016) propose to use attention mechanism to reduce the noise within a sentence bag. Instead of characterizing the noise, these approaches only aim to alleviate the effect of noise. The at-least-one assumption is often too strong in practice, and there are still chances that the sentence bag may be false positive or false negative. Thus it is important to model the noise pattern to guide the learning procedure. (Ritter et al., 2013) and (Min et al., 2013) try to employ a set of latent variables to r"
P17-1040,D12-1042,0,0.523535,"positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-leastone assumption that at least one of the aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural ne"
P17-1040,P12-1076,0,0.121946,"Feng∗1 , Zheng Wang2 , Zhanxing Zhu3 , Songfang Huang4 , Rui Yan1 and Dongyan Zhao1 1 ICST, Peking University, China 2 School of Computing and Communications, Lancaster University, UK 3 Peking University, China 4 IBM China Research Lab, China {bf luo,fengyansong,zhanxing.zhu,ruiyan,zhaody}@pku.edu.cn z.wang@lancaster.ac.uk huangsf@cn.ibm.com Abstract as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works (Takamatsu et al., 2012; Ritter et al., 2013) show that DS often mistakenly labels real positive instances as negative (false negative) or versa vice (false positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple pre"
P17-1040,P16-1200,0,0.318747,"e aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done. In this paper, we aim to enhance DS noise modeling by providing the capability to explicitly characterize the noise in the DS-style training data Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application o"
P17-1040,N13-1095,0,0.0949015,"ong positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-leastone assumption that at least one of the aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its earl"
P17-1040,P13-2117,0,0.069312,"Missing"
P17-1040,P09-1113,0,0.0499241,"erize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios. 1 Introduction Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks – from relation extraction (Mintz et al., 2009) and sentiment classification (Go et al., 2009) to cross-lingual semantic analysis (Fang and Cohn, 2016). By using knowledge learned from seed examples to label data, DS automatically prepares large scale training data for these tasks. While promising, DS does not guarantee perfect results and often introduces noise to the generated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple 430 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 430–439 c Vancouver"
P17-1040,D15-1203,0,0.521273,"s supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done. In this paper, we aim to enhance DS noise modeling by providing the capability to explicitly characterize the noise in the DS-style training data Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervisio"
P17-1040,D14-1162,0,0.114232,"serve as gold standard, we only evaluate bag-level models on E NTITY RE, a standard practice in previous works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). Datasets We evaluate our approach on two datasets. 434 5.2 1 .0 0 Experimental Setup Hyper-parameters We use 200 convolution kernels with widow size 3. During training, we use stochastic gradient descend (SGD) with batch size 20. The learning rates for sentence-level and bag-level models are 0.1 and 0.01, respectively. Sentence level experiments are performed on T IME RE, using 100-d word embeddings pretrained using GloVe (Pennington et al., 2014) on Wikipedia and Gigaword (Parker et al., 2011), and 20-d vectors for distance embeddings. Each of the three subsets of T IME RE is added after the previous phase has run for 15 epochs. The trace regularization weights are β1 = 0.01, β2 = −0.01 and β3 = −0.1, respectively, from the reliable to the most unreliable, with the ratio of β3 and β2 fixed to 10 or 5 when tuning. Bag level experiments are performed on both T IME RE and E NTITY RE. For T IME RE, we use the same parameters as above. For E NTITY RE, we use 50-d word embeddings pre-trained on the NYT corpus using word2vec (Mikolov et al.,"
P17-2036,W14-4012,0,0.128818,"Missing"
P17-2036,D16-1230,0,0.0722005,"Missing"
P17-2036,C16-1316,1,0.792893,"end to generate longer, more meaningful and diverse replies, which sheds light on neural sequence generation. Table 2: The length, entropy, and diversity of the replies on the context-insensitive and contextaware (WSeq,concat) methods. relevant context utterances as well as weakens irrelevant contexts. RQ2: What is the effect of context on neural dialog systems? We are now curious about how context information affects neural conversational systems. In Table 2, we present three auxiliary metrics, i.e., sentence length, entropy, and diversity. The former two are used in Serban et al. (2016) and Mou et al. (2016), whereas the latter one is used in Zhang and Hurley (2008). As shown, content-aware conversational models tend to generate longer, more meaningful and diverse replies compared with content-insensitive models, given that they also improve BLEU scores.2 This shows an interesting phenomenon of neural sequence generation: an encoder-decoder framework needs sufficient source information for meaningful generation of the target; it simply does not fall into meaningful content from less meaningful input. A similar phenomenon is also reported in our previous work (Mou et al., 2016); we show that, a sa"
P17-2036,D11-1054,0,0.240659,"Missing"
P17-2036,P15-1152,0,0.283936,"rison to analyze how to use context effectively. In this paper, we conduct an empirical study to compare various models and investigate the effect of context information in dialog systems. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines. 1 Introduction Recently, human-computer conversation is attracting increasing attention due to its promising potentials and alluring commercial values. Researchers have proposed both retrieval methods (Ji et al., 2014; Yan et al., 2016) and generative methods (Ritter et al., 2011; Shang et al., 2015) for automatic conversational systems. With the success of deep learning techniques, neural networks have demonstrated powerful capability of learning human dialog patterns; given a user-issued utterance as an input query q, the network can generate a reply r, which is usually accomplished in a sequence-to-sequence (Seq2Seq) manner (Shang et al., 2015). In the literature, there are two typical research setups for dialog systems: single-turn and multiturn. Single-turn conversation is, perhaps, the simplest setting where the model only takes q into consideration when generating r (Shang et al.,"
P17-2036,N15-1020,0,0.0169469,"Missing"
P17-2036,D16-1172,0,0.0123649,"Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2036 text weighting approach, outperforming the other baselines. 2 2.1 Models Non-Hierarchical Model To model a few utterances before the current query, several studies directly concatenate these sentences together and use a single model to capture the meaning of context and the query (Yan et al., 2016; Sordoni et al., 2015). They are referred to as non-hierarchical models in our paper. Such method is also used in other NLP tasks, e.g., document-level sentiment analysis (Xu et al., 2016) and machine comprehension (Wang and Jiang, 2017). Following the classic encode-decoder framework, we use a Seq2Seq network, which transforms the query and context into a fixed-length vector venc by a recurrent neural network (RNN) during encoding; then, in the decoding phase, it generates a reply r with another RNN in a wordby-word fashion. (See Figure 1a.) In our study, we adopt RNNs with gated recurrent units (Cho et al., 2014, GRUs), which alleviates the long propagation problem of vanilla RNNs. When decoding, we apply beam search with a size of 5. 2.2 (a) Non-hierarchical model. (b) Hiera"
P18-1194,D16-1146,0,0.0368066,"Missing"
P18-1194,H90-1021,0,0.0607127,"hich matches the property of wk zk better than probability. Actually, when performing model ensemble, ensembling with logits is often empirically better than with the final probability3 . This is also the reason why we choose to operate on logits in Sec. 3.3. 4 Evaluation Methodology Our experiments aim to answer three questions: Q1: Does the use of REs enhance the learning quality when the number of annotated instances is small? Q2: Does the use of REs still help when using the full training data? Q3: How can we choose from different combination methods? 4.1 Datasets We use the ATIS dataset (Hemphill et al., 1990) to evaluate our approach. This dataset is widely used in SLU research. It includes queries of flights, meal, etc. We follow the setup of Liu and Lane (2016) by using 4,978 queries for training and 893 for testing, with 18 intent labels and 127 slot labels. We also split words like Miami’s into Miami ’s during the tokenization phase to reduce the number of words that do not have a pre-trained word embedding. This strategy is useful for fewshot learning. 3 An example can be found in the ensemble version that Juan et al. (2016) used in the Avazu Kaggle competition. To answer Q1 , we also exploit"
P18-1194,P16-1228,0,0.0358084,"Missing"
P18-1194,D16-1173,0,0.0517657,"Missing"
P18-1194,D17-1201,0,0.0204074,"s the effectiveness of our methods, and indicates that simple REs are quite costefficient since these simple REs only contain 1-2 RE groups and thus very easy to produce. We can also see that using complex REs generally leads to better results compared to using simple REs. This indicates that when considering using REs to improve a NN model, we can start with simple REs, and gradually increase the RE complexity to improve the performance over time7 . 6 Related Work Our work builds upon the following techniques, while qualitatively differing from each NN with Rules. On the initialization side, Li et al. (2017) uses important n-grams to initialize the convolution filters. On the input side, Wang et al. (2017a) uses knowledge base rules to find relevant concepts for short texts to augment input. On the output side, Hu et al. (2016a; 2016b) and Guo et al. (2017) use FOL rules to rectify the output probability of NN, and then let NN learn from the rectified distribution in a teacher-student framework. Xiao et al. (2017), on the other hand, modifies the decoding score of NN by multiplying a weight derived from rules. On the loss function side, people modify the loss function to model the relationship be"
P18-1194,D16-1197,0,0.0227221,"n initialization or in loss function often require special properties of the task, these approaches are not applicable to our problem. Our work thus offers new ways to exploit RE rules at different levels of a NN. NNs and REs. As for NNs and REs, previous work has tried to use RE to speed up the decoding phase of a NN (Strauß et al., 2016) and generating REs from natural language specifications of the 7 We do not include results of both for slot filling since its REs are different from feat and logit, and we have already shown that the attention loss method does not work for slot filling. RE (Locascio et al., 2016). By contrast, our work aims to use REs to improve the prediction ability of a NN. Few-Shot Learning. Prior work either considers few-shot learning in a metric learning framework (Koch et al., 2015; Vinyals et al., 2016), or stores instances in a memory (Santoro et al., 2016; Kaiser et al., 2017) to match similar instances in the future. Wang et al. (2017b) further uses the semantic meaning of the class name itself to provide extra information for few-shot learning. Unlike these previous studies, we seek to use the humangenerated REs to provide additional information. Natural Language Understa"
P18-1194,D14-1162,0,0.0835473,"ositive RE for intent (or slot) k can often be treated as negative REs for other intents (or slots). As such, we use the positive REs for intent (or slot) k as the negative REs for other intents (or slots) in our experiments. 4.3 Experimental Setup Hyper-parameters. Our hyper-parameters for the BLSTM are similar to the ones used by Liu and Lane (2016). Specifically, we use batch size 16, dropout probability 0.5, and BLSTM cell size 100. The attention loss weight is 16 (both positive and negative) for full few-shot learning settings and 1 for other settings. We use the 100d GloVe word vectors (Pennington et al., 2014) pre-trained on Wikipedia and Gigaword (Parker et al., 2011), and the Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001. Evaluation Metrics. We report accuracy and macro-F1 for intent detection, and micro/macroF1 for slot filling. Micro/macro-F1 are the harmonic mean of micro/macro precision and recall. Macro-precision/recall are calculated by averaging precision/recall of each label, and microprecision/recall are averaged over each prediction. Competitors and Naming Conventions. Here, a bold Courier typeface like BLSTM denotes the notations of the models that we will compare in Se"
P18-2070,W01-1605,0,0.241076,"till fall behind this state-of-the-art method. The main reason might be that MST-full follows a global graph-based dependency parsing framework, where their high order methods (in cubic time complexity) can directly analyze the relationship between any EDUs pairs in the discourse, while, we choose the transition-based local method with linear time complexity, which can only investigate the top EDUs in S and B according to the selected actions, thus usually has a lower performance than the global graph-based methods, but with a Evaluation and Results Dataset: We use the RST Discourse Treebank (Carlson et al., 2001) with the same split as in (Li et al., 2014), i.e., 312 for training, 30 for development and 38 for testing. We experiment with two set of relations, the 111 types of fine-grained relations and the 19 types of coarse-grained relations, respectively. Evaluation Metrics: In the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), head is the core of a discourse, and a dependent gives supporting evidence to its head with certain relationship. We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both"
P18-2070,W10-4327,0,0.0325967,"account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1 . 1 Introduction Discourse parsing aims to identify the structure and relationship between different element discourse units (EDUs). As a fundamental topic in natural language processing, discourse parsing can assist many down-stream applications such as summarization (Louis et al., 2010), sentiment analysis (Polanyi and van den Berg, 2011) and question-answering (Ferrucci et al., 2010). However, the performance of discourse parsing is still far from perfect, especially for EDUs that are distant to each other in the discourse. In fact, as found in (Jia et al., 2018), the discourse parsing performance drops quickly as the dependency span increases. The reason may be twofold: 1 Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers),"
P18-2070,P04-1015,0,0.130798,"rtain relationship. We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both correct heads and relations) as our evaluation metrics. Baselines: We compare our method with the following baselines and models: (1) Perceptron: We re-implement the perceptron based arc-eager style dependency discourse parser as mentioned in (Jia et al., 2018) with coarse-grained relation. The Perceptron model chooses words, POS tags, positions and length features, totally 100 feature templates, with the early update strategy (Collins and Roark, 2004). (2) Jia18: Jia et al. (2018) implement a transition-based discourse parser with stacked LSTM, where they choose a two-layer LSTM to represent EDUs by encoding four kinds of features including words, POS tags, positions and length features. (3) Basic EDU representation (Basic): Our discourse parser with the basic EDU representation method mentioned in Section 3. (4) Memory refined representation (Refined): Our full parser equipped with the basic EDU representation method and the memory networks to capture the discourse cohesion mentioned in Section 3. (5) MST-full (Li et al., 2014): a graph-b"
P18-2070,P15-1033,0,0.0222777,"hub.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast. (3) My breakfast is hamburger. (4) It is eight o'clock when I leave home. (5) So late! Model overview Memory network slot1 Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015). We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations). slot2 slot3 (9) It is nine o'clock. (10) Thank God, I am not late for work. slotn ... (6) I drive into the highway, (7) but meet a traffic jam. (8) Oh, I finally arrive at the company. (11) But the hamburger is cold, (12) order some take-away food is better, maybe. Figure 1: An i"
P18-2070,P14-5010,0,0.00567472,"Missing"
P18-2070,J91-1002,0,0.744108,"he syntactic or semantic relationship between words or phrases in a discourse, and, to some extent, can indicate the topic changing or threads in a discourse. Discourse cohesion includes five situations, including reference, substitution, ellipsis, conjunction and lexical cohesion (Halliday and Hasan, 1989). Here, lexical cohesion reflects the semantic relationship of words, and can be modeled as the recurrence of words, synonym and contextual words. However, previous works do not well model the discourse cohesion within the discourse parsing task, or do not even take this issue into account. Morris and Hirst (1991) proposes to utilize Roget thesauri to form lexical chains (sequences of semantically related words that can reflect the topic shifts within a discourse), which are used to extract features to characterize discourse structures. (Joty et al., 2013) uses lexical chain feature to model multi-sentential relation. Actually, these simplified cohesion features can already improve parsing performance, especially in long spans. Secondly, in modern neural network methods, modeling discourse cohesion as part of the networks is not a trivial task. One can still use off-the-shell tools to obtain lexical ch"
P18-2070,W03-3017,0,0.457685,"The reason may be twofold: 1 Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast. (3) My breakfast is hamburger. (4) It is eight o'clock when I leave home. (5) So late! Model overview Memory network slot1 Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015). We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations). slot2 slot3 (9) It is nine o'clock. (10) Thank God, I am not late for work. slotn ... (6) I drive into the highway, (7) but meet a traffic jam. (8) Oh, I finally arrive at the compa"
P18-2070,P13-1048,0,0.0540609,"Missing"
P19-1304,D18-1246,1,0.897995,"ic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index. This aggregator 1 Lebron James is transl"
P19-1304,D18-1032,0,0.0812748,"ultilingual knowledge graphs (KGs), such as DBpedia (Auer et al., 2007) and Yago (Suchanek et al., 2007), represent human knowledge in the structured format and have been successfully used in many natural language processing applications. These KGs encode rich monolingual knowledge but lack the cross-lingual links to bridge the language gap. Therefore, the cross-lingual KG alignment task, which automatically matches entities in a multilingual KG, is proposed to address this problem. Most recently, several entity matching based approaches (Hao et al., 2016; Chen et al., 2016; Sun et al., 2017; Wang et al., 2018) have been proposed for this task. Generally, these approaches first project entities of each KG into lowdimensional vector spaces by encoding monolingual KG facts, and then learn a similarity score function to match entities based on their vector representations. However, since some entities in different languages may have different KG To address these drawbacks, we propose a topic entity graph to represent the KG context information of an entity. Unlike previous methods that utilize entity embeddings to match entities, we formulate this task as a graph matching problem between the topic enti"
P19-1304,Q17-1010,0,0.0352098,"8 37.29 74.49 83.45 91.56 84.71 92.35 EN-FR @1 @10 14.61 37.25 21.26 50.60 32.97 65.91 36.77 73.06 81.03 90.79 84.15 91.76 66.91 67.93 67.92 64.01 65.28 65.21 72.63 73.97 73.52 69.76 71.29 70.18 87.62 89.38 88.96 87.65 88.18 88.01 77.52 78.48 78.36 78.12 79.64 79.48 85.09 87.15 86.87 83.48 84.63 84.29 94.19 95.24 94.28 93.66 94.75 94.37 Table 1: Evaluation results on the datasets. non-linearity function σ is ReLU (Glorot et al., 2011) and the parameters of aggregators are randomly initialized. Since KGs are represented in different languages, we first retrieve monolingual fastText embeddings (Bojanowski et al., 2017) for each language, and apply the method proposed in Conneau et al. (2017) to align these word embeddings into a same vector space, namely, crosslingual word embeddings. We use these embeddings to initialize word representations in the first layer of GCN1 . Results and Discussion. Following previous works, we used Hits@1 and Hits@10 to evaluate our model, where Hits@k measures the proportion of correctly aligned entities ranked in the top k. We implemented a baseline (referred as BASELINE in Table 1) that selects k closest G2 entities to a given G1 entity in the cross-lingual embedding space,"
P19-1304,D18-1223,1,0.789993,"Missing"
P19-1304,D18-1110,1,0.821566,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,D18-1112,1,0.597172,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,P18-1030,0,0.0133111,"raph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index."
P19-1370,P16-1094,0,0.0315846,"nd δ to co-teaching. Experiments are conducted with DAM on the two data sets. 5 Related Work So far, methods used to build an open domain dialogue system can be divided into two categories. The first category utilize an encoderdecoder framework to learn response generation models. Since the basic sequence-to-sequence models (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching mod"
P19-1370,W15-4640,0,0.563357,"val-based systems are often superior to their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). A key problem in response selection is how to measure the matching degree between a conversation context (a message with several turns of conversation history) and a response candidate. Existing studies have paid tremendous effort to build a matching model with neural architectures (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b), and advanced models such as the deep attention matching network (DAM) (Zhou et al., 2018b) have achieved impressive performance on benchmarks. In contrary to the progress on model architectures, there is little exploration on learning approaches of the models. On the one hand, neural matching models are becoming more and more complicated; on the other hand, all models are simply learned by distinguishing human responses from some automatically constructed negative response candidates (e.g., by random sampling). Although this heuristic"
P19-1370,C16-1316,1,0.851966,"ns on Douban 0.60 (b) 0.1 0.3 0.5 0.7 0.9 0.95 1.0 δ Data curriculum on ECD Figure 3: Effects of λ and δ to co-teaching. Experiments are conducted with DAM on the two data sets. 5 Related Work So far, methods used to build an open domain dialogue system can be divided into two categories. The first category utilize an encoderdecoder framework to learn response generation models. Since the basic sequence-to-sequence models (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al"
P19-1370,P15-1152,0,0.15213,"Missing"
P19-1370,N10-1116,0,0.0290891,"both the Douban data and the E-commerce data with SMN and DAM which achieves state-of-theart performance on benchmarks. Moreover, improvement to SMN on the Douban data from coteaching is bigger than that from weak supervision, when the ratio of the positive and the negative is 1:1 in training7 . Our work, in a broad sense, belongs to the effort on learning with noisy data. Previous studies including curriculum learning (CL) (Bengio et al., 2009) and self-paced learning (SPL) (Jiang et al., 2014, 2015) tackle the problem with heuristics, such as ordering data from easy instances to hard ones (Spitkovsky et al., 2010; Tsvetkov et al., 2016) and retaining training instances whose losses are smaller than a threshold (Jiang et al., 2015). Recently, Fan et al. (2018) propose a deep reinforcement learning framework in which a simple deep neural network is used to adaptively select and filter important data instances from the training data. Jiang et al. (2017) propose a MentorNet which learns a data-driven curriculum with a Student-Net to mitigate overfitting on corrupted labels. In parallel to curriculum learning, several studies explore sample weighting schemes where training samples are re-weighted according"
P19-1370,D19-1011,0,0.279741,"selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing models with a better approach. Probably the most related work is the weakly supervised learning approach proposed in Wu et al. (2018b). However, there is stark difference between our approach and the weak supervision approach: (1) weak supervision employs a static generative model to teach a discriminative model, while co-teaching dynamically lets two discriminative models teach each other and evolve together; (2) weak supervision needs pretraining a generative model with ex"
P19-1370,P16-1013,0,0.0575427,"Missing"
P19-1370,D13-1096,0,0.199215,"dels (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing"
P19-1370,P18-2067,1,0.90322,"epresentative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing models with a better approach. Probably the most related work is the weakly supervised learning approach proposed in Wu et al. (2018b). However, there is stark difference between our approach and the weak supervision approach: (1) weak supervision employs a static generative model to teach a discriminative model, while co-teaching dynamically lets two discriminative models teach each other and evolve together; (2) weak supervision needs pretraining a generative model with extra resources and pre-building an index for training data construction, while co-teaching does not have such request; and (3) in terms of multi-turn response selection, weak supervision is only tested on the Douban data with SMN and the multi-view match"
P19-1370,P17-1046,1,0.308055,"o their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). A key problem in response selection is how to measure the matching degree between a conversation context (a message with several turns of conversation history) and a response candidate. Existing studies have paid tremendous effort to build a matching model with neural architectures (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b), and advanced models such as the deep attention matching network (DAM) (Zhou et al., 2018b) have achieved impressive performance on benchmarks. In contrary to the progress on model architectures, there is little exploration on learning approaches of the models. On the one hand, neural matching models are becoming more and more complicated; on the other hand, all models are simply learned by distinguishing human responses from some automatically constructed negative response candidates (e.g., by random sampling). Although this heuristic approach can avoid expensive and exh"
P19-1370,P18-1205,0,0.0536553,"Missing"
P19-1370,C18-1317,0,0.375056,"d negative responses are randomly sampled. The ratio of the positive and the negative is 1:1 in training and validation. In the test set, each context has 10 response candidates retrieved from an index whose appropriateness regarding to the context is judged by human annotators. The average number of positive responses per context is 1.18. Following Wu et al. (2017), we employ R10 @1, R10 @2, R10 @5, mean average precision (MAP), mean reciprocal rank (MRR), and precision at position 1 (P@1) as evaluation metrics. In addition to the Douban data, we also choose E-commerce Dialogue Corpus (ECD) (Zhang et al., 2018b) as an experimental data set. The data consists of real-world conversations between customers and customer service staff in Taobao4 , which is the largest e-commerce platform in China. There are 1 million context-response pairs in the training set, and 10 thousand pairs in both the validation set and the test set. Each context in the training set and the validation set corresponds to one positive response candidate and one negative response candidate, while in the test set, the number of response candidates per context is 10 with only one of them positive. In the released data, human respons"
P19-1370,D16-1036,1,0.925812,"Missing"
P19-1370,P18-1103,0,0.245472,"roach can generally and significantly improve the performance of existing matching models. 1 Introduction Human-machine conversation is a long-standing goal of artificial intelligence. Recently, building a dialogue system for open domain human-machine conversation is attracting more and more attention due to both availability of large-scale human conversation data and powerful models learned with neural networks. Existing methods are either retrieval-based or generation-based. Retrievalbased methods reply to a human input by selecting a proper response from a pre-built index (Ji et al., 2014; Zhou et al., 2018b; Yan and Zhao, 2018), while generation-based methods synthesize a response with a natural language model (Shang et al., 2015; Serban et al., 2017). In this ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@pku.edu.cn). work, we study the problem of response selection for retrieval-based dialogue systems, since retrieval-based systems are often superior to their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant Ali"
W12-3312,D10-1099,0,0.114349,"ese. In this paper, we propose a new framework to build a KB in Chinese from online resources without much human involvement. Since the Chinese portion of Wikipedia is much smaller than its English part, we harvest knowledge facts from a Chinese online encyclopedia, HudongBaike2 . HudongBaike is the largest Chinese online encyclopedia and features similar managing rules and writing styles with Wikipedia. We first obtain knowledge facts by parsing the infoboxes of HudongBaike. Then we use these triples as seeds, and adopt the idea of distant supervision(Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010) to extract more facts from other HudongBaike articles and build a KB accordingly. Moreover, to make the knowledge base more up-todate, we also propose to propagate the KB with news events. The rest of this paper is organized as follows: we first introduce the related work, and briefly introduce two online encyclopedias. In Section 4 we describe our framework in detail. Our current work are discussed in Section 5. In Section 6 we conclude this paper. 2 Related Work KB construction is an important task and has attracted many research efforts from artificial intelligence, information retrieval,"
W12-3312,N10-1072,0,0.0747158,"Missing"
W12-3312,P09-1113,0,0.0209114,"used in English may not work well in Chinese. In this paper, we propose a new framework to build a KB in Chinese from online resources without much human involvement. Since the Chinese portion of Wikipedia is much smaller than its English part, we harvest knowledge facts from a Chinese online encyclopedia, HudongBaike2 . HudongBaike is the largest Chinese online encyclopedia and features similar managing rules and writing styles with Wikipedia. We first obtain knowledge facts by parsing the infoboxes of HudongBaike. Then we use these triples as seeds, and adopt the idea of distant supervision(Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010) to extract more facts from other HudongBaike articles and build a KB accordingly. Moreover, to make the knowledge base more up-todate, we also propose to propagate the KB with news events. The rest of this paper is organized as follows: we first introduce the related work, and briefly introduce two online encyclopedias. In Section 4 we describe our framework in detail. Our current work are discussed in Section 5. In Section 6 we conclude this paper. 2 Related Work KB construction is an important task and has attracted many research efforts from artifici"
