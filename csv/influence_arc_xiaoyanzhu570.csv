2020.aacl-main.16,C14-1001,0,0.0121497,"Model World Model World Model Learning DDQ Human Conversational Data Supervised Learning Acting Direct RL User Real Experience User Estimator OPPA Imitation Learning ?? ( ?$? ?? # Policy Model Direct RL Acting User Real Experience Figure 1: A comparison of dialogue policy learning a) with real/simulated user, b) with real user via DDQ and c) with real user guided by active user estimation. more efficient but also improve the target agent’s performance. In agreement with the findings from cognitive science, humans often maintain models of other people they interact with to capture their goals (Harper, 2014; Premack and Woodruff, 1978). And humans manage to use their mental process to simulate others’ behavior (Gordon, 1986; Gallese and Goldman, 1998). Therefore, to carefully treat and model the behaviors of other agents would be full of potential. For example, in competitive tasks such as chess, the player often sees a number of moves ahead by considering the possible reaction of the other player. In goal-oriented dialogues for a hotel booking task, the agent can reduce interaction numbers and improve user experience by modeling users as business travellers with strict time limit or backpackers"
2020.aacl-main.16,2020.sigdial-1.36,0,0.0249627,"selines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One principle direction for mitigating this problem is to leverage human conversation data to build a user simulator, and then to learn the di"
2020.aacl-main.16,W18-5007,0,0.0131494,"et al., 2016) is a widely applied rule-based method, which starts with a randomly generated user goal that is unknown to the system. During a dialogue session, it remains a stack data structure known as user agenda, which holds some pending user intentions to achieve. In the stack update process, machine learning or expert-defined methods can be applied. There are also some model-based methods that learn a simulator from real conversation data. The seq2seq framework has recently been introduced by encoding dialogue history and generates the next response or dialogue action (Asri et al., 2016; Kreyssig et al., 2018). By incorporating a variational step to the seq2seq network, it can introduce meaningful diversity into the simulator (G¨ur et al., 2018). Our work tackles the problem from a different point of view. We let the target agent approximate an opposite agent model to save user simulation efforts. 3 Model In this section, we introduce our proposed OPPA model. There are two agents in our framework, one is the system agent we want to optimize, and the other is the user agent. We refer to these two agents as target and opposite agents in the following sections. Note that the proposed model works at di"
2020.aacl-main.16,D17-1259,0,0.377378,"goal-oriented dialogue dataset, which contains 7 domains, 13 intents and 25 slot types. There are 10,483 sessions and 71,544 turns, which is at least one order of magnitude larger than previous annotated task-oriented dialogue dataset. Among all the dialogue sessions, we used 1,000 each for validation and test. Specifically, in the data collection stage, the user follows a specific goal to converse with the agent but is encouraged to change his/her goal dynamically during the session, which makes the dataset more challenging. For the competitive task, we used a bilateral negotiation dataset (Lewis et al., 2017), where there are 5,808 dialogues from 2,236 scenarios. In each session, there are two people negotiating to divide some items, such as books, hats and balls. Each kind of item is of different value to each person, thus they can give priority to valuable items in the negotiation. For example, a hat may worth 5 for person A and 3 for person B, so B can give up some hat in order to get other valuable items. To conduct our experiment, we further labeled the dataset with system dialogue actions. 126 4.2 We implemented the model using PyTorch (Paszke et al., 2017). The hyper-parameters were decided"
2020.aacl-main.16,P17-1045,0,0.0298629,"Missing"
2020.aacl-main.16,I17-1074,0,0.0698595,"Missing"
2020.aacl-main.16,W16-3613,0,0.0349863,"Missing"
2020.aacl-main.16,N18-1187,0,0.0128112,"ogue tasks, showing superior performance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One principle direction for mitigating this problem is to leverage"
2020.aacl-main.16,P18-1203,0,0.390173,"this problem is to leverage human conversation data to build a user simulator, and then to learn the dialogue policy by making simulated interactions with the simulator (Schatzmann et al., 2006; Li et al., 2016; G¨ur et al., 2018). However, there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model, which can lead to a sub-optimal dialogue policy (Dhingra et al., 2016). Another direction is to learn the dynamics of dialogue environment during interacting with real user, and concurrently use the learned dynamics for RL planning (Peng et al., 2018; Su et al., 2018; Wu et al., 2018; Zhang et al., 2019b). Most of these works are based on Deep Dyna-Q (DDQ) framework (Sutton, 1990), where a world model is introduced to learn the dynamics (which is much like a simulated user) from real experiences. The target agent’s policy is trained using both real experiences via direct RL and simulated experiences via a world-model. In the above methods, both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent. That is, the target agent’s knowledge about"
2020.aacl-main.16,D17-1237,0,0.0139645,"ied in dialogue policy learning, including DQN (Mnih et al., 2015) and Policy Gradient (Sutton et al., 2000) methods, which mitigate the problem of domain adaptation through function approximation and representation learning (Zhao and Eskenazi, 2016). Recently, there are some efforts focused on multi-domain dialogue policy. An intuitive way is to learn independent policies for each specific domain and aggregate them (Wang et al., 2014; Gaˇsi´c 123 et al., 2015; Cuay´ahuitl et al., 2016). There are also some works using hierarchical RL, which decomposes the complex task into several sub-tasks (Peng et al., 2017; Casanueva et al., 2018) according to pre-defined domain structure and cross-domain constraints. Nevertheless, most of the above works regard the opposite agent as part of the environment without explicitly modeling its behavior. Planning based RL methods are also introduced to make a trade-off between reducing human interaction cost and learning a more realistic simulator. Peng et al. (2018) proposed to use Deep Dynamic Q-network, in which a world model is co-trained with the target policy model. By training the world model with the real system-human interaction data, it consistently approac"
2020.aacl-main.16,D18-1416,0,0.335433,"leverage human conversation data to build a user simulator, and then to learn the dialogue policy by making simulated interactions with the simulator (Schatzmann et al., 2006; Li et al., 2016; G¨ur et al., 2018). However, there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model, which can lead to a sub-optimal dialogue policy (Dhingra et al., 2016). Another direction is to learn the dynamics of dialogue environment during interacting with real user, and concurrently use the learned dynamics for RL planning (Peng et al., 2018; Su et al., 2018; Wu et al., 2018; Zhang et al., 2019b). Most of these works are based on Deep Dyna-Q (DDQ) framework (Sutton, 1990), where a world model is introduced to learn the dynamics (which is much like a simulated user) from real experiences. The target agent’s policy is trained using both real experiences via direct RL and simulated experiences via a world-model. In the above methods, both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent. That is, the target agent’s knowledge about the simulated ag"
2020.aacl-main.16,2020.acl-main.59,1,0.806324,"Missing"
2020.aacl-main.16,D19-1010,1,0.81731,"ance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One principle direction for mitigating this problem is to leverage human conversation data to build a user s"
2020.aacl-main.16,D14-1007,0,0.0261301,"2007). However, these methods require manual work to define features and state representation, which leads to poor domain adaptation. More recently, deep learning methods are applied in dialogue policy learning, including DQN (Mnih et al., 2015) and Policy Gradient (Sutton et al., 2000) methods, which mitigate the problem of domain adaptation through function approximation and representation learning (Zhao and Eskenazi, 2016). Recently, there are some efforts focused on multi-domain dialogue policy. An intuitive way is to learn independent policies for each specific domain and aggregate them (Wang et al., 2014; Gaˇsi´c 123 et al., 2015; Cuay´ahuitl et al., 2016). There are also some works using hierarchical RL, which decomposes the complex task into several sub-tasks (Peng et al., 2017; Casanueva et al., 2018) according to pre-defined domain structure and cross-domain constraints. Nevertheless, most of the above works regard the opposite agent as part of the environment without explicitly modeling its behavior. Planning based RL methods are also introduced to make a trade-off between reducing human interaction cost and learning a more realistic simulator. Peng et al. (2018) proposed to use Deep Dyn"
2020.aacl-main.16,N07-2038,0,0.345898,"interaction data, it consistently approaches the performance of real users, which provides better simulated experience for planning. Adversarial methods are applied to dynamically control the proportion of simulated and real experience during different stages of training (Su et al., 2018; Wu et al., 2018). Still, these methods work from the opposite agents’ angle. 2.2 Dialogue User Simulation In RL-based dialogue policy learning methods, a user simulator is often required to provide affordable training environments due to the high cost of collecting real human corpus. Agenda-based simulation (Schatzmann et al., 2007; Li et al., 2016) is a widely applied rule-based method, which starts with a randomly generated user goal that is unknown to the system. During a dialogue session, it remains a stack data structure known as user agenda, which holds some pending user intentions to achieve. In the stack update process, machine learning or expert-defined methods can be applied. There are also some model-based methods that learn a simulator from real conversation data. The seq2seq framework has recently been introduced by encoding dialogue history and generates the next response or dialogue action (Asri et al., 2"
2020.aacl-main.16,P17-1062,0,0.0173839,". We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts using simulation methods to provide an affordable training environment. One prin"
2020.aacl-main.16,P19-1426,0,0.0435272,"Missing"
2020.aacl-main.16,P19-1364,0,0.112982,"to build a user simulator, and then to learn the dialogue policy by making simulated interactions with the simulator (Schatzmann et al., 2006; Li et al., 2016; G¨ur et al., 2018). However, there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model, which can lead to a sub-optimal dialogue policy (Dhingra et al., 2016). Another direction is to learn the dynamics of dialogue environment during interacting with real user, and concurrently use the learned dynamics for RL planning (Peng et al., 2018; Su et al., 2018; Wu et al., 2018; Zhang et al., 2019b). Most of these works are based on Deep Dyna-Q (DDQ) framework (Sutton, 1990), where a world model is introduced to learn the dynamics (which is much like a simulated user) from real experiences. The target agent’s policy is trained using both real experiences via direct RL and simulated experiences via a world-model. In the above methods, both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent. That is, the target agent’s knowledge about the simulated agents is still passively obtained thro"
2020.aacl-main.16,W16-3601,0,0.162786,"timation to improve the target agent by regarding it as part of the target policy. We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines. 1 Introduction In goal-oriented dialogue systems, dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state. This problem is often formulated using reinforcement learning (RL) in which the user serves as the environment (Levin et al., 1997; Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2013; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Dhingra et al., 2016; Su et al., 2016; Li et al., 2017; Williams et al., 2017; Liu and Lane, 2017; Lipton et al., 2018; Liu et al., 2018; Gao et al., 2019; Takanobu et al., 2019, 2020; Jhunjhunwala et al., 2020). However, different from symbolic-based and simulation-based RL tasks, such as chess (Silver et al., 2016) and video games (Mnih et al., ∗ Corresponding author. § {liuzitao,galehuang}@100tal.com 2015), which can get vast amounts of training interactions in low cost, dialogue systems require to learn directly from real users, which is too expensive. Therefore, there are some efforts u"
2020.aacl-main.16,D18-1547,0,\N,Missing
2020.acl-demos.19,D18-1547,0,0.234895,"Missing"
2020.acl-demos.19,P19-1360,0,0.06182,"b-2 provides a template-based method and SC-LSTM (Wen et al., 2015). 2.3.1 CamRest676 CamRest676 (Wen et al., 2017) is a Wizard-of-Oz dataset, consisting of 676 dialogues in a restaurant domain. ConvLab-2 offers an agenda-based user simulator and a complete set of models for building a traditional pipeline dialogue system on the CamRest676 dataset. 2.2.6 Word-level Policy Word-level policy directly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG compone"
2020.acl-demos.19,N19-1423,0,0.0163653,"ntegrated models in ConvLab-2 are marked in bold. Researchers can easily add their models by implementing the interface of the corresponding component. We will keep adding state-of-the-art models to reflect the latest progress in task-oriented dialogue. 2.2.1 Natural Language Understanding The natural language understanding (NLU) component, which is used to parse the other agent’s intent, takes an utterance as input and outputs the corresponding dialogue acts. ConvLab-2 provides three models: Semantic Tuple Classifier (STC) (Mairesse et al., 2009), MILU (Lee et al., 2019b), and BERTNLU. BERT (Devlin et al., 2019) has shown strong performance in many NLP tasks. Thus, ConvLab-2 proposes a new BERTNLU model. BERTNLU adds two MLPs on top of BERT for intent classification and slot tagging, respectively, and fine-tunes all parameters on the specified tasks. BERTNLU achieves the best performance on MultiWOZ in comparison with other models. 2.2.2 Dialogue State Tracking The dialogue state tracking (DST) component updates the belief state, which contains the constraints and requirements of the other agent (such as a user). ConvLab-2 provides a rule-based tracker that takes dialogue acts parsed by the NLU as in"
2020.acl-demos.19,W14-4337,0,0.245946,"Missing"
2020.acl-demos.19,P19-1546,0,0.189107,"l Intelligence, † State Key Lab of Intelligent Technology and Systems, † Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China ‡ Microsoft Research, Redmond, USA † ‡ {zhu-q18,z-zhang15,fangy17,gxly19}@mails.tsinghua.edu.cn {jincli,bapeng,jfgao}@microsoft.com † {zxy-dcs,aihuang}@tsinghua.edu.cn Abstract We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: F"
2020.acl-demos.19,P19-3011,1,0.903958,"Missing"
2020.acl-demos.19,P18-1133,0,0.311994,"improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools"
2020.acl-demos.19,D17-1259,0,0.47465,"20. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipes). ConvLab-2 inherits the flexible framework of ConvLab and imports recently proposed models that achieve state-of-the-art performance. In addition, ConvLab-2 supports several large-scale dialogue datasets including CamRest676 (Wen et al., 2017), MultiWOZ (Budzianowski et al., 2018b), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). To support end-to-end evaluation, ConvLab-2 provides user simulators for automatic evaluation and integrates Amazon Mechanical Turk for human evaluation, similar to ConvLab. Moreover, it provides an analysis tool and a human-machine interactive tool for diagnosing a dialogue system. Researchers can perform quantitative analysis using the analysis tool. It presents useful statistics extracted from the conversations between the user simulator and the dialogue system. This information helps reveal the weakness of the system and signifies the direction for furthe"
2020.acl-demos.19,D17-2014,0,0.0314309,"rating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools, namely the analysis tool and the interactive tool, are provided for in-depth error analysis. ConvLab-2 will be the development platform for Multi-domain Task-oriented Dialog Challenge II track in the 9th Dialog System Technology Challenge (DSTC9)1 . As shown in Figure 1, there are many approaches to building a task-oriented dialogue system, ranging from pipeline methods with multiple components to fully end-to-end models. Previous toolkits focus on either end-to-end models (Miller et al., 2017) or one specific component such as dialogue policy (POL) (Ultes et al., 2017), while the others toolkits that are designed for developers (Bocklisch et al., 2017; Papangelis et al., 2020) do not 1 https://sites.google.com/dstc. community/dstc9/home 142 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 142–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows rese"
2020.acl-demos.19,D17-1237,1,0.845901,"analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author."
2020.acl-demos.19,P18-2069,0,0.077924,"Missing"
2020.acl-demos.19,N07-2038,0,0.404849,"rectly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG components to assemble a complete user simulator. 2.2.8 End-to-end Model A fully end-to-end dialogue model receives the dialogue history and generates a response in natural language directly. ConvLab-2 extends Sequicity (Lei et al., 2018) to multi-domain scenarios: when the model senses that the current domain has switched, it resets the belief span, which records information of the current domain. ConvLa"
2020.acl-demos.19,D19-1010,1,0.86197,", 2019a), and TRADE (Wu et al., 2019). TRADE generates the belief state 143 from utterances using a copy mechanism and achieves state-of-the-art performance on MultiWOZ. the DealOrNoDeal dataset, we provide the ROLLOUTS RL policy proposed by Lewis et al. (2017). 2.2.4 Dialogue Policy Dialogue policy receives the belief state and outputs system dialogue acts. ConvLab-2 provides a rule-based policy, a simple neural policy that learns directly from the corpus using imitation learning, and reinforcement learning policies including REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GDPL (Takanobu et al., 2019). GDPL achieves state-of-the-art performance on MultiWOZ. Compared with ConvLab, ConvLab-2 can integrate a new dataset more conveniently. For each dataset, ConvLab-2 provides a unified data loader that can be used by all the models, thus separating data processing from the model definition. Currently, ConvLab-2 supports four task-oriented dialogue datasets, including CamRest676 (Wen et al., 2017), MultiWOZ (Eric et al., 2019), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). 2.2.5 Natural Language Generation The natural language generation (NLG) component transforms dialogue"
2020.acl-demos.19,P17-4013,0,0.0308827,"Corresponding author. two powerful tools, namely the analysis tool and the interactive tool, are provided for in-depth error analysis. ConvLab-2 will be the development platform for Multi-domain Task-oriented Dialog Challenge II track in the 9th Dialog System Technology Challenge (DSTC9)1 . As shown in Figure 1, there are many approaches to building a task-oriented dialogue system, ranging from pipeline methods with multiple components to fully end-to-end models. Previous toolkits focus on either end-to-end models (Miller et al., 2017) or one specific component such as dialogue policy (POL) (Ultes et al., 2017), while the others toolkits that are designed for developers (Bocklisch et al., 2017; Papangelis et al., 2020) do not 1 https://sites.google.com/dstc. community/dstc9/home 142 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 142–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipe"
2020.acl-demos.19,D15-1199,0,0.102409,"Missing"
2020.acl-demos.19,E17-1042,0,0.288565,"Missing"
2020.acl-demos.19,P19-1078,0,0.112045,"interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools, namely the anal"
2020.acl-demos.19,N19-1123,0,0.131839,"method and SC-LSTM (Wen et al., 2015). 2.3.1 CamRest676 CamRest676 (Wen et al., 2017) is a Wizard-of-Oz dataset, consisting of 676 dialogues in a restaurant domain. ConvLab-2 offers an agenda-based user simulator and a complete set of models for building a traditional pipeline dialogue system on the CamRest676 dataset. 2.2.6 Word-level Policy Word-level policy directly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG components to assemble a complete use"
2020.acl-demos.19,2020.tacl-1.19,1,0.84554,"nal Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipes). ConvLab-2 inherits the flexible framework of ConvLab and imports recently proposed models that achieve state-of-the-art performance. In addition, ConvLab-2 supports several large-scale dialogue datasets including CamRest676 (Wen et al., 2017), MultiWOZ (Budzianowski et al., 2018b), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). To support end-to-end evaluation, ConvLab-2 provides user simulators for automatic evaluation and integrates Amazon Mechanical Turk for human evaluation, similar to ConvLab. Moreover, it provides an analysis tool and a human-machine interactive tool for diagnosing a dialogue system. Researchers can perform quantitative analysis using the analysis tool. It presents useful statistics extracted from the conversations between the user simulator and the dialogue system. This information helps reveal the weakness of the system and signifies the direction for further improvement. With the interacti"
2020.acl-main.635,D14-1179,0,0.0186658,"Missing"
2020.acl-main.635,N19-1423,0,0.0203577,"current encoder-decoder model that has a specific context RNN to incorporate historical conversational utterances into a context state, which is used as the initial hidden state of the decoder. The adapted model generates the k-th utterance based on the past k − 1 utterances, where k was also set to 8, for fair comparison with Seq2Seq. All the generative models were trained by optimizing the cross-entropy loss: (g) L0 = − T 1X log P(ˆ xt = xt ), T t=1 where x ˆt denotes the predicted token at the time step t, while xt is the t-th token of the target sentence. 4.1.2 Retrieval-based Model BERT (Devlin et al., 2019): We adapted this deep bidirectional transformers (Vaswani et al., 2017) as a retrieval-based model. For each utterance (except the first one in a dialog), we extracted keywords in the same way as Wu et al. (2017) and retrieved 10 response candidates, including the golden truth based on the BM25 algorithm (Robertson et al., 1995). The training task is to predict whether a candidate is the correct next utterance given the context, where a sigmoid function was used to output the probability score yˆ = P(y = 1) and the cross-entropy loss was optimized: 7103 (r) L0 = −y log yˆ − (1 − y) log(1 − yˆ"
2020.acl-main.635,N16-1014,0,0.64384,"ntroducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available1 . 1 Introduction It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al."
2020.acl-main.635,P18-1138,0,0.275527,"tudied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (W"
2020.acl-main.635,W15-4640,0,0.0739565,"nt but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (Wu et al., 2019) use knowledge graphs as knowledge resources. Nevertheless, the number of topics is limited to one (Moon et al., 2019) or two (Wu et al., 2019), which is not sufficient for diversif"
2020.acl-main.635,D16-1147,0,0.0245084,"rds in the same way as Wu et al. (2017) and retrieved 10 response candidates, including the golden truth based on the BM25 algorithm (Robertson et al., 1995). The training task is to predict whether a candidate is the correct next utterance given the context, where a sigmoid function was used to output the probability score yˆ = P(y = 1) and the cross-entropy loss was optimized: 7103 (r) L0 = −y log yˆ − (1 − y) log(1 − yˆ), where y ∈ {0, 1} is the true label. For the test, we selected the candidate response with the largest probability. 4.1.3 Knowledge-aware Models A key-value memory module (Miller et al., 2016) is introduced to the aforementioned models to utilize the knowledge information. We treated all knowledge triples mentioned in a dialogue as the knowledge information in the memory module. For a triple that is indexed by i, we represented the key memory and the value memory respectively as a key vector ki and a value vector vi , where ki is the average word embeddings of the head entity and the relation, and vi is those of the tail entity. We used a query vector q to attend to the key vectors ki (i = 1, 2, ...): αi = softmaxi (q T ki ), then the weighted P sum of the value vectors vi (i = 1,"
2020.acl-main.635,D18-1255,0,0.335864,"commendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations o"
2020.acl-main.635,P19-1081,0,0.169542,"s been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialK"
2020.acl-main.635,D18-1398,0,0.0179714,"nce/character respectively). collect multi-turn conversations from scratch based on such large-scale knowledge. KdConv is proposed as one small step to achieve this goal, where we narrowed down the scale of background knowledge to several domains (film, music, and travel) and collected conversations based on the domainspecific knowledge. KdConv contains similar domains (film and music) and dissimilar domains (film and travel) so that it offers the possibility to investigate the generalization and transferability of knowledge-driven conversational models with transfer learning or meta learning(Gu et al., 2018; Mi et al., 2019). In the following subsections, we will describe the two steps in data collection: (1) Constructing the domain-specific knowledge graph; (2) Collecting conversation utterances and knowledge interactions by crowdsourcing. 3.1 Knowledge Graph Construction As the sparsity and the large scale of the knowledge were difficult to handle, we reduced the range of the domain-specific knowledge by crawling the most popular films and film stars, music and singers, and attractions as start entities, from several related websites for the film4 /music5 /travel6 domain. The knowledge of thes"
2020.acl-main.635,P02-1040,0,0.106856,"DAM (Kingma and Ba, 2014) was used to optimize all the models with the initial learning rate of 5 × 10−5 for BERT and 10−3 for others. The mini-batch sizes are set to 2 dialogues for LM and 32 pairs of post and response for Seq2Seq and HRED. 4.3 4.3.1 Metrics We measured the performance of all the retrievalbased models using Hits@1 and Hits@3, same as Zhang et al. (2018) and Wu et al. (2019). 8 We adopted several widely-used metrics to measure the quality of the generated response. We calculated Perplexity (PPL) to evaluate whether the generation result is grammatical and fluent. BLEU1/2/3/4 (Papineni et al., 2002) is a popular metric to compute the k-gram overlap between a generated sentence and a reference (Sordoni et al., 2015; Li et al., 2016b). Distinct-1/2/3/4 (Li et al., 2016b) is also provided to evaluates the diversity of generated responses. 4.3.2 Results The results are shown in Table 5. We analyze the results from the following perspectives: The influence of knowledge: after introducing the knowledge, all the models were improved in terms of all the metrics except PPL in all the domains. First, all the models obtain higher Hits@1 scores (in the music domain, BERT obtains an improvement of 0."
2020.acl-main.635,P19-1539,0,0.25324,", 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (Wu et al., 2019) use knowledge graphs a"
2020.acl-main.635,N10-1020,0,0.0602713,"n adaptation or transfer learning between similar domains (e.g., from film to music) or dissimilar domains (e.g., from music to travel). • We provide benchmark models on this corpus to facilitate further research, and conduct extensive experiments. Results show that the models can be enhanced by introducing background knowledge, but there is still much room for further research. The corpus and the models are publicly available3 . 2 Related Work Recently, open-domain conversation generation has been largely advanced due to the increase of publicly available dialogue data (Godfrey et al., 1992; Ritter et al., 2010; Shang et al., 2015; Lowe et al., 2015). However, the lack of annotation of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2019). These models produce conversations that are substantially different from those humans make, which largely rely on background knowledge. To facilitate the development of conversational models that mimic human conversations, there have been several knowledge-grounded corpora proposed. Some datasets (Zhou et al., 2018b; Ghazvininejad et al., 2018; Liu et"
2020.acl-main.635,P15-1152,0,0.143131,"can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available1 . 1 Introduction It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Gha"
2020.acl-main.635,N18-2028,0,0.0254902,"ighted sum of L0 and Latt : (l) (l) Ltot = L0 + λLatt , l ∈ {g, r}. Note that the knowledge-enhanced BERT was initialized from the fine-tuned BERT discussed in Section 4.1.2, and the parameters of the transformers were frozen during training the knowledge related modules. The purpose was to exclude the impact of the deep transformers but only examine the potential effects introduced by the background knowledge. 4.2 Implementation Details 2017) and CoTK (Huang et al., 2020). The Jieba Chinese word segmenter7 was employed for tokenization. The 200-dimensional word embeddings were initialized by Song et al. (2018), while the unmatched ones were randomly sampled from a standard normal distribution N (0, 1). The type of RNN network units was all GRU (Cho et al., 2014) and the number of hidden units of GRU cells were all set to 200. ADAM (Kingma and Ba, 2014) was used to optimize all the models with the initial learning rate of 5 × 10−5 for BERT and 10−3 for others. The mini-batch sizes are set to 2 dialogues for LM and 32 pairs of post and response for Seq2Seq and HRED. 4.3 4.3.1 Metrics We measured the performance of all the retrievalbased models using Hits@1 and Hits@3, same as Zhang et al. (2018) and"
2020.acl-main.635,N15-1020,0,0.091094,"Missing"
2020.acl-main.635,D15-1199,0,0.0119797,"ing that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available1 . 1 Introduction It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan e"
2020.acl-main.635,P19-1369,0,0.310226,"and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2"
2020.acl-main.635,P17-1046,0,0.01859,"ates the k-th utterance based on the past k − 1 utterances, where k was also set to 8, for fair comparison with Seq2Seq. All the generative models were trained by optimizing the cross-entropy loss: (g) L0 = − T 1X log P(ˆ xt = xt ), T t=1 where x ˆt denotes the predicted token at the time step t, while xt is the t-th token of the target sentence. 4.1.2 Retrieval-based Model BERT (Devlin et al., 2019): We adapted this deep bidirectional transformers (Vaswani et al., 2017) as a retrieval-based model. For each utterance (except the first one in a dialog), we extracted keywords in the same way as Wu et al. (2017) and retrieved 10 response candidates, including the golden truth based on the BM25 algorithm (Robertson et al., 1995). The training task is to predict whether a candidate is the correct next utterance given the context, where a sigmoid function was used to output the probability score yˆ = P(y = 1) and the cross-entropy loss was optimized: 7103 (r) L0 = −y log yˆ − (1 − y) log(1 − yˆ), where y ∈ {0, 1} is the true label. For the test, we selected the candidate response with the largest probability. 4.1.3 Knowledge-aware Models A key-value memory module (Miller et al., 2016) is introduced to t"
2020.acl-main.635,P18-1205,0,0.0565722,"ed by Song et al. (2018), while the unmatched ones were randomly sampled from a standard normal distribution N (0, 1). The type of RNN network units was all GRU (Cho et al., 2014) and the number of hidden units of GRU cells were all set to 200. ADAM (Kingma and Ba, 2014) was used to optimize all the models with the initial learning rate of 5 × 10−5 for BERT and 10−3 for others. The mini-batch sizes are set to 2 dialogues for LM and 32 pairs of post and response for Seq2Seq and HRED. 4.3 4.3.1 Metrics We measured the performance of all the retrievalbased models using Hits@1 and Hits@3, same as Zhang et al. (2018) and Wu et al. (2019). 8 We adopted several widely-used metrics to measure the quality of the generated response. We calculated Perplexity (PPL) to evaluate whether the generation result is grammatical and fluent. BLEU1/2/3/4 (Papineni et al., 2002) is a popular metric to compute the k-gram overlap between a generated sentence and a reference (Sordoni et al., 2015; Li et al., 2016b). Distinct-1/2/3/4 (Li et al., 2016b) is also provided to evaluates the diversity of generated responses. 4.3.2 Results The results are shown in Table 5. We analyze the results from the following perspectives: The i"
2020.acl-main.635,C16-1191,1,0.906669,"Missing"
2020.acl-main.635,D18-1076,0,0.305676,"of conversational systems (Shang et al., 2015; Li et al., 2016a; Shao et al., 2017). In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrat"
2020.acl-main.635,D19-1194,0,0.129115,", 2015; Zhou et al., 2016). In ∗ Equal contribution Corresponding author: Minlie Huang. 1 https://github.com/thu-coai/KdConv † open-domain conversational systems, it is important but challenging to leverage background knowledge, which is represented as either knowledge graphs (Zhu et al., 2017; Zhou et al., 2018a) or unstructured texts (Ghazvininejad et al., 2018), for making effective interactions. Recently, a variety of knowledge-grounded conversation corpora have been proposed (Zhou et al., 2018b; Dinan et al., 2018; Moghe et al., 2018; Moon et al., 2019; Wu et al., 2019; Liu et al., 2018; Tuan et al., 2019; Qin et al., 2019) to fill the gap where previous datasets do not provide knowledge grounding of the conversations (Godfrey et al., 1992; Shang et al., 2015; Lowe et al., 2015). CMU DoG (Zhou et al., 2018b), India DoG (Moghe et al., 2018), and Wizard of Wikipedia (Dinan et al., 2018) demonstrate attempts for generating informative responses with topic-related Wikipedia articles. However, these datasets are not suitable for modeling topic transition or knowledge planning through multi-turn dialogs based on the relations of topics. OpenDialKG (Moon et al., 2019) and DuConv (Wu et al., 2019) use"
2020.emnlp-main.54,W05-0909,0,0.397637,"Missing"
2020.emnlp-main.54,D18-1454,0,0.0573109,"Missing"
2020.emnlp-main.54,P19-1470,0,0.0376001,"rporate one-hop knowledge graph for concepts in the story context. In topic-to-essay generation, Yang et al. (2019) augmented the generator with a concept memory that updated dynamically with gate mechanism. Recently, some work also attempted to integrate external commonsense knowledge into generative pretrained language models such as GPT-2 (Radford et al., 2019). Guan et al. (2020) conducted posttraining on sythetic data constructed from commonsense knowledge bases by translating triplets into natural language texts using templates. Bhagavatula et al. (2020) transferred embeddings of COMeT (Bosselut et al., 2019), a GPT-2 model fine-tuned to generate the tail entity of a triple in commonsense knowledge graph, into another GPT-2 model for text generation. In comparison, our model utilizes both structural and semantic information of the commonsense knowledge graph during generation and does not suffers from the catastrophic forgetting problem (Kirkpatrick et al., 2016) caused by implicit knowledge transferring. Our contributions can be summarized as follows: 1) We propose GRF, a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in"
2020.emnlp-main.54,N19-1240,0,0.295331,"e commonsense-aware text generation tasks and show that our model outperforms various selective baselines. We also visualize reasoning paths inferred by the model to demonstrate the effectiveness of the multi-hop reasoning module. 2 2.1 Related Work Commonsense-Aware Neural Text Generation Incorporating commonsense knowledge is essential for text generation to augment the limited textual information. In dialogue generation, Zhou et al. (2018) enriched the context representations of the post with neighbouring concepts on ConceptNet using graph attention. In story ending generation, Guan et al. (2019) proposed incremental encoding with multi-source attention to incorporate one-hop knowledge graph for concepts in the story context. In topic-to-essay generation, Yang et al. (2019) augmented the generator with a concept memory that updated dynamically with gate mechanism. Recently, some work also attempted to integrate external commonsense knowledge into generative pretrained language models such as GPT-2 (Radford et al., 2019). Guan et al. (2020) conducted posttraining on sythetic data constructed from commonsense knowledge bases by translating triplets into natural language texts using temp"
2020.emnlp-main.54,N18-1165,0,0.0287248,"sed by implicit knowledge transferring. Our contributions can be summarized as follows: 1) We propose GRF, a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in text generation. 2) We propose the dynamic multi-hop reasoning module that aggregates evidence along relational paths for grounded gener726 2.2 Multi-Hop Reasoning on Graph Structure Performing explicit multi-hop reasoning on graph structure has been demonstrated to be an effective approach for query answering over incomplete knowledge graphs (Das et al., 2018; Chen et al., 2018; Lin et al., 2018), multi-hop question answering (Bauer et al., 2018; Cao et al., 2019; Qiu et al., 2019) and dialogue generation (Tuan et al., Decoder hidden state Layer Norm Feed Forward LD x Layer Norm Concept distribution H-hops Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). ("
2020.emnlp-main.54,P16-1154,0,0.191063,"v)), (11) where ct is the concept of the selected node at the t-th time step. Intuitively, the reasoning module learns to dynamically distribute along the paths by considering the triple evidence according to the current decoder state. 3.2.4 Generation Distribution with Gate Control The final generation distribution combines the distribution over the concepts (Eq. 11) and the distribution over the standard vocabulary (Eq. 7). We use a soft gate probability gt which denotes whether to copy a concept in the generation to control the weight of the two distributions similar to the copy mechanism (Gu et al., 2016; See et al., 2017).   D . (12) gt = σ Wgate hL t 728 The final output distribution is the linear combination of the two distributions weighted by gt and 1 − gt respectively. P (yt |y<t , x, G) = gt+N · P (ct+N |s<t+N , G) + (1 − gt+N ) · P (st+N |s<t+N ), (13) where N is the length of the input text sequence. 3.3 Training and Inference To train the proposed model, we minimize the negative log-likelihood of generating the ground truth target sequence y gold = (y1 , y2 · · · , yM , [eos]). Lgen = M +1 X gold − log P (yt gold |y<t , x, G). Explanation Generation (EG) is to generate an explanat"
2020.emnlp-main.54,2020.tacl-1.7,1,0.939847,"ch as (volcano, MadeOf, lava) besides the story context. Although pre-trained models have been demonstrated to possess commonsense reasoning ability (Trinh and Le, 2018) by implicitly learning some relational patterns from large-scale corpora, they do not fully utilize the commonsense knowledge bases that provide more explicit knowledge grounding. To address this defect, incorporating external commonsense knowledge to enhance models’ reasoning ability has been widely explored (Lin et al., 2019; Ye et al., 2019; Lv et al., 2019). In language generation, previous work (Bhagavatula et al., 2020; Guan et al., 2020) transfers commonsense knowledge into pre-trained language models by utilizing triple information in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). However, this approach has two drawbacks. 725 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 725–736, c November 16–20, 2020. 2020 Association for Computational Linguistics First, recovering knowledge triples at the posttraining stage (Guan et al., 2020) hardly enables the model to utilize the encoded knowledge in fine-tuning generation tasks whic"
2020.emnlp-main.54,N16-1014,0,0.11829,"Missing"
2020.emnlp-main.54,D19-1282,0,0.198087,"y the model that provide rationale to the generation.1 “lava” in the story ending by providing background knowledge such as (volcano, MadeOf, lava) besides the story context. Although pre-trained models have been demonstrated to possess commonsense reasoning ability (Trinh and Le, 2018) by implicitly learning some relational patterns from large-scale corpora, they do not fully utilize the commonsense knowledge bases that provide more explicit knowledge grounding. To address this defect, incorporating external commonsense knowledge to enhance models’ reasoning ability has been widely explored (Lin et al., 2019; Ye et al., 2019; Lv et al., 2019). In language generation, previous work (Bhagavatula et al., 2020; Guan et al., 2020) transfers commonsense knowledge into pre-trained language models by utilizing triple information in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). However, this approach has two drawbacks. 725 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 725–736, c November 16–20, 2020. 2020 Association for Computational Linguistics First, recovering knowledge triples at the posttraining"
2020.emnlp-main.54,W04-1013,0,0.0182468,"Missing"
2020.emnlp-main.54,D18-1362,0,0.12802,"wledge transferring. Our contributions can be summarized as follows: 1) We propose GRF, a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in text generation. 2) We propose the dynamic multi-hop reasoning module that aggregates evidence along relational paths for grounded gener726 2.2 Multi-Hop Reasoning on Graph Structure Performing explicit multi-hop reasoning on graph structure has been demonstrated to be an effective approach for query answering over incomplete knowledge graphs (Das et al., 2018; Chen et al., 2018; Lin et al., 2018), multi-hop question answering (Bauer et al., 2018; Cao et al., 2019; Qiu et al., 2019) and dialogue generation (Tuan et al., Decoder hidden state Layer Norm Feed Forward LD x Layer Norm Concept distribution H-hops Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop re"
2020.emnlp-main.54,D19-1187,0,0.0130561,"Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop reasoning module aggregates evidence from source concepts Cx along structural paths to all nodes where shade indicates the node score (§3.2.3). (d) The final generation distribution with gate control (§3.2.4). 2019; Moon et al., 2019; Liu et al., 2019). Particularly, reasoning on knowledge graphs to answer relational query typically adopts REINFORCE to learn concrete policies to search for entities or relations. In multi-hop question answering tasks, the reasoning process is augmented with entity graph (Cao et al., 2019; Qiu et al., 2019) or concept paths (Bauer et al., 2018) to enhance semantic connections among document segments. In dialogue generation, Tuan et al. (2019) modeled multiple hops on relationship graphs with a Markov transition matrix. Liu et al. (2019) proposed a twostage architecture that selected information from a knowled"
2020.emnlp-main.54,D17-1159,0,0.0309066,"graph extraction process in §4.2 and describe our proposed model in the next section. 3.2 Static Multi-Relational Graph Encoding Graph Neural Network (GNN) frameworks, such as graph convolution network (GCN) (Kipf and Welling, 2017) and graph attention network (GAT) (Velickovic et al., 2018), have been shown effective at encoding graph-structured data by aggregating node information from local neighbours. To model the relational information in the knowledge graph, R-GCN (Schlichtkrull et al., 2018) generalizes GCN with relationspecific weight matrices but is reported to be over-parameterized (Marcheggiani and Titov, 2017; Schlichtkrull et al., 2018). We follow Vashishth et al. (2020) and use a non-parametric compositional operation φ(·) to combine the node embedding and the relation embedding. Specifically, 727 3.2.1 Generation with Multi-Hop Reasoning Flow given the input graph G = (V, E) and a GCN with LG layer, for each node v ∈ V we update the node embedding at the l + 1-th layer by aggregating information from its local neighbours N (v) which consist of pairs of node u and the connected relation r. X 1 l WN φ(hlu , hlr ), (2) olv = |N (v)| (u,r)∈N (v)   l l l (3) hl+1 = ReLU o + W h v v S v , where h0v"
2020.emnlp-main.54,P02-1040,0,0.106054,"Missing"
2020.emnlp-main.54,P19-1617,0,0.0334703,"a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in text generation. 2) We propose the dynamic multi-hop reasoning module that aggregates evidence along relational paths for grounded gener726 2.2 Multi-Hop Reasoning on Graph Structure Performing explicit multi-hop reasoning on graph structure has been demonstrated to be an effective approach for query answering over incomplete knowledge graphs (Das et al., 2018; Chen et al., 2018; Lin et al., 2018), multi-hop question answering (Bauer et al., 2018; Cao et al., 2019; Qiu et al., 2019) and dialogue generation (Tuan et al., Decoder hidden state Layer Norm Feed Forward LD x Layer Norm Concept distribution H-hops Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop reasoning module aggregates evidence from source concepts Cx along structural paths to al"
2020.emnlp-main.54,P19-1081,0,0.0288194,"Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop reasoning module aggregates evidence from source concepts Cx along structural paths to all nodes where shade indicates the node score (§3.2.3). (d) The final generation distribution with gate control (§3.2.4). 2019; Moon et al., 2019; Liu et al., 2019). Particularly, reasoning on knowledge graphs to answer relational query typically adopts REINFORCE to learn concrete policies to search for entities or relations. In multi-hop question answering tasks, the reasoning process is augmented with entity graph (Cao et al., 2019; Qiu et al., 2019) or concept paths (Bauer et al., 2018) to enhance semantic connections among document segments. In dialogue generation, Tuan et al. (2019) modeled multiple hops on relationship graphs with a Markov transition matrix. Liu et al. (2019) proposed a twostage architecture that selected informa"
2020.emnlp-main.54,P17-1099,0,0.0148382,"t is the concept of the selected node at the t-th time step. Intuitively, the reasoning module learns to dynamically distribute along the paths by considering the triple evidence according to the current decoder state. 3.2.4 Generation Distribution with Gate Control The final generation distribution combines the distribution over the concepts (Eq. 11) and the distribution over the standard vocabulary (Eq. 7). We use a soft gate probability gt which denotes whether to copy a concept in the generation to control the weight of the two distributions similar to the copy mechanism (Gu et al., 2016; See et al., 2017).   D . (12) gt = σ Wgate hL t 728 The final output distribution is the linear combination of the two distributions weighted by gt and 1 − gt respectively. P (yt |y<t , x, G) = gt+N · P (ct+N |s<t+N , G) + (1 − gt+N ) · P (st+N |s<t+N ), (13) where N is the length of the input text sequence. 3.3 Training and Inference To train the proposed model, we minimize the negative log-likelihood of generating the ground truth target sequence y gold = (y1 , y2 · · · , yM , [eos]). Lgen = M +1 X gold − log P (yt gold |y<t , x, G). Explanation Generation (EG) is to generate an explanation given a counter"
2020.emnlp-main.54,N16-1098,0,0.053806,"resentations for the concepts and the relations (§3.2.1). Then, the multi-hop reasoning module performs dynamic reasoning via aggregating triple evidence along multiple relational paths to generate the salient concept under the context (§3.2.3). Finally, the generation distribution combines the probability of copying concepts from the knowledge graph and that of choosing a word from the standard vocabulary with a gate control (§3.2.4). The overall model architecture is shown in Figure 2. We conduct experiments on three commonsense-aware text generation tasks including story ending generation (Mostafazadeh et al., 2016), abductive natural language generation (Bhagavatula et al., 2020), and explanation generation for sense making (Wang et al., 2019). Results show that our model outperforms strong baselines on these tasks, thereby demonstrating the benefit of multi-hop commonsense reasoning in language generation. ation of some critical concepts. 3) We conduct extensive experiments including automatic and human evaluation on three commonsense-aware text generation tasks and show that our model outperforms various selective baselines. We also visualize reasoning paths inferred by the model to demonstrate the ef"
2020.emnlp-main.54,D19-1194,0,0.0123326,"ng structural paths to all nodes where shade indicates the node score (§3.2.3). (d) The final generation distribution with gate control (§3.2.4). 2019; Moon et al., 2019; Liu et al., 2019). Particularly, reasoning on knowledge graphs to answer relational query typically adopts REINFORCE to learn concrete policies to search for entities or relations. In multi-hop question answering tasks, the reasoning process is augmented with entity graph (Cao et al., 2019; Qiu et al., 2019) or concept paths (Bauer et al., 2018) to enhance semantic connections among document segments. In dialogue generation, Tuan et al. (2019) modeled multiple hops on relationship graphs with a Markov transition matrix. Liu et al. (2019) proposed a twostage architecture that selected information from a knowledge graph for further generating the response. Compared with these generation models that operate on knowledge graphs within a specific domain, our focus is to utilize general commonsense knowledge to supply evidence for text generation. 3 3.1 Methodology Problem Formulation In this paper, we focus on text generation tasks where reasoning over external commonsense knowledge is required. Without loss of generality, the input sou"
2020.emnlp-main.54,P19-1393,0,0.0236239,"riple evidence along multiple relational paths to generate the salient concept under the context (§3.2.3). Finally, the generation distribution combines the probability of copying concepts from the knowledge graph and that of choosing a word from the standard vocabulary with a gate control (§3.2.4). The overall model architecture is shown in Figure 2. We conduct experiments on three commonsense-aware text generation tasks including story ending generation (Mostafazadeh et al., 2016), abductive natural language generation (Bhagavatula et al., 2020), and explanation generation for sense making (Wang et al., 2019). Results show that our model outperforms strong baselines on these tasks, thereby demonstrating the benefit of multi-hop commonsense reasoning in language generation. ation of some critical concepts. 3) We conduct extensive experiments including automatic and human evaluation on three commonsense-aware text generation tasks and show that our model outperforms various selective baselines. We also visualize reasoning paths inferred by the model to demonstrate the effectiveness of the multi-hop reasoning module. 2 2.1 Related Work Commonsense-Aware Neural Text Generation Incorporating commonsens"
2020.emnlp-main.54,P19-1193,0,0.0310046,"onstrate the effectiveness of the multi-hop reasoning module. 2 2.1 Related Work Commonsense-Aware Neural Text Generation Incorporating commonsense knowledge is essential for text generation to augment the limited textual information. In dialogue generation, Zhou et al. (2018) enriched the context representations of the post with neighbouring concepts on ConceptNet using graph attention. In story ending generation, Guan et al. (2019) proposed incremental encoding with multi-source attention to incorporate one-hop knowledge graph for concepts in the story context. In topic-to-essay generation, Yang et al. (2019) augmented the generator with a concept memory that updated dynamically with gate mechanism. Recently, some work also attempted to integrate external commonsense knowledge into generative pretrained language models such as GPT-2 (Radford et al., 2019). Guan et al. (2020) conducted posttraining on sythetic data constructed from commonsense knowledge bases by translating triplets into natural language texts using templates. Bhagavatula et al. (2020) transferred embeddings of COMeT (Bosselut et al., 2019), a GPT-2 model fine-tuned to generate the tail entity of a triple in commonsense knowledge g"
2020.emnlp-main.567,C14-1151,0,0.0157364,"olarity polari from SentiWordNet for each pair (xi , posi ). In SentiWordNet, we can find m different senses for the pair (xi , posi ), each of which contains a sense number, a positive / negative score, (j) (j) (j) (j) and a gloss (SNi , P scorei , N scorei , Gi ), 1 ≤ j ≤ m, where SN indicates the rank of different senses, P score/N score is the positive / negative score assigned by SentiWordNet, and G denotes the definition of each sense. Inspired by the existing work on inferring word-level prior polarity from SentiWordNet (Guerini et al., 2013) and unsupervised word sense disambiguation (Basile et al., 2014), we propose a context-aware attention mechanism which simultaneously considers the sense rank and the context-gloss similarity to determine the attention weight of each sense: (j) αi where = sof tmax( 1 (j) SNi 1 (j) SNi (j) · sim(X, Gi )) approximates the impact of sense frequency because a smaller sense rank indicates more frequent use of this sense in natural lan(j) guage (Guerini et al., 2013), and sim(X, Gi ) denotes the textual similarity between the context and the gloss of each sense, which is commonly used as an important feature in unsupervised word sense disambiguation (Basile et a"
2020.emnlp-main.567,D19-1005,0,0.0217718,"Mo (Peters et al., 2018), GPT (Radford et al., 2018, 2019), and BERT (Devlin et al., 2019) become prevalent. These models use LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation, and explore various pre-training tasks including masked language model and next sentence prediction (Devlin et al., 2019). Thanks to the great success of BERT on various NLP tasks, many variants of BERT have been proposed, which mainly fall into four aspects: 1) Knowledge enhancement: ERNIE-Tsinghua (Zhang et al., 2019) / KnowBERT (Peters et al., 2019) explicitly introduces knowledge graph / knowledge base to BERT, while ERNIE-Baidu (Sun et al., 2019b) designs entity-specific masking strategies during pre-training. 2) Transferability: TransBERT (Li et al., 2019) conducts supervised post-training on the pre-trained BERT with transfer tasks to get a better initialization for target tasks. 3) Hyper-parameters: RoBERTa (Liu et al., 2019) measures the impact of key hyper-parameters to improve the under-trained BERT. 4) Pre-training tasks: SpanBERT (Joshi et al., 2020) masks consecutive spans randomly instead of individual tokens, while XLNet (Ya"
2020.emnlp-main.567,D13-1170,0,0.0522466,"ared with the existing work on pre-trained models for sentiment analysis, our work integrates sentiment-related linguistic knowledge from SentiWordNet (Baccianella et al., 2010) into pre-trained models to construct knowledge-aware language representation, which can benefit a wide range of downstream tasks in sentiment analysis. Linguistic Knowledge for Sentiment Analysis Linguistic knowledge such as part of speech and word-level sentiment polarity is commonly used as external features in sentiment analysis. Part of speech is shown to facilitate the parsing of the syntactic structure of texts (Socher et al., 2013). It 6976 Linguistic Knowledge Acquisition from SentiWordNet Pre-training Task: Label-aware Masked Language Model ࣦ ௪ௗ ሺሻ  ࣦ ைௌ ሺሻ  ࣦ  ሺሻ 濈濦濕濢濧濚濣濦濡濙濦 Context-aware Sentiment Attention Term ሺଵሻ ߙௗ POS good ሺଶሻ ߙௗ ڮ a SN 1 good a 2 … … … ሺሻ ߙௗ Pscore / Nscore 0.75/0 Having amount the normally … ൌ ݔܽݐݏሺ BERT Embedding ܧሾୌሿ Having desirable or positive qualities especially those suitable for a thing specified 0/0 ͳ 濮濖濟濦濰 Gloss (G) POS Embedding Word-level Polarity Embedding Sentence-level Sentiment Embedding expected … ሺሻ  ܵܰௗ ݏ ڄሺݒ  ܽ"
2020.emnlp-main.567,D19-1569,0,0.116784,"ent. These models use LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation, and explore various pre-training tasks including masked language model and next sentence prediction (Devlin et al., 2019). Thanks to the great success of BERT on various NLP tasks, many variants of BERT have been proposed, which mainly fall into four aspects: 1) Knowledge enhancement: ERNIE-Tsinghua (Zhang et al., 2019) / KnowBERT (Peters et al., 2019) explicitly introduces knowledge graph / knowledge base to BERT, while ERNIE-Baidu (Sun et al., 2019b) designs entity-specific masking strategies during pre-training. 2) Transferability: TransBERT (Li et al., 2019) conducts supervised post-training on the pre-trained BERT with transfer tasks to get a better initialization for target tasks. 3) Hyper-parameters: RoBERTa (Liu et al., 2019) measures the impact of key hyper-parameters to improve the under-trained BERT. 4) Pre-training tasks: SpanBERT (Joshi et al., 2020) masks consecutive spans randomly instead of individual tokens, while XLNet (Yang et al., 2019) designs a training objective combining both reconstruction and autoregressive langu"
2020.emnlp-main.567,S14-2004,0,0.0365378,"s that for sentiment analysis tasks, linguistic knowledge can be used to enhance the state-of-the-art pre-trained model via the well-designed pre-training task. 4.5 Table 2: Statistics of sentence-level sentiment classification (SSC) datasets. We first evaluated our model on sentenceAspect-level Sentiment Analysis Aspect-level sentiment analysis includes aspect term extraction, aspect term sentiment classification, aspect category detection, and aspect category sentiment classification. For aspect term based tasks, we chose SemEval2014 Task 4 for laptop (Lap14) and restaurant (Res14) domains (Pontiki et al., 2014) as the benchmarks, while for aspect category based tasks, we used SemEval2014 Task 6980 Dataset Lap14 Res14 Res16 # sentences # terms # categories # sentiment Amount of ATE Amount of ATSC Amount of ACD Amount of ACSC (Train/Test) (Train/Test) (Train/Test) classes (Train/Valid/Test) (Train/Valid/Test) (Train/Valid/Test) (Train/Valid/Test) 3,045 / 800 2,358 / 654 3 1,338 / 150 / 422 2,163 / 150 / 638 3,041 / 800 3,693 / 1,134 3,711 / 1,025 3 1,871 / 150 / 606 3,452 / 150 / 1,120 2,891 / 150 / 800 3,366 / 150 / 973 2,000 / 676 2,507 / 859 3 1,850 / 150 / 676 2,150 / 150 / 751 Table 4: Statistics"
2020.emnlp-main.567,P17-1154,1,0.934047,"19a). Despite the great success of pre-trained models, existing pre-training tasks like masked language model and next sentence prediction (Devlin et al., ∗ Equal contribution Corresponding author 1 The data, codes, and model parameters are available at https://github.com/thu-coai/SentiLARE. † 2019) neglect to consider the linguistic knowledge. Such knowledge is important for some NLP tasks, particularly for sentiment analysis. For instance, existing work has shown that linguistic knowledge including part-of-speech tag (Qian et al., 2015; Huang et al., 2017) and word-level sentiment polarity (Qian et al., 2017) is closely related to the sentiment of longer texts. We argue that pre-trained models enriched with the linguistic knowledge of words will facilitate the understanding of the sentiment of the whole texts, thereby resulting in better performance on sentiment analysis. There are two major challenges to construct knowledge-aware pre-trained language representation models which can promote the downstream tasks in sentiment analysis: 1) Knowledge acquisition across different contexts. Most of the existing work has adopted static sentiment lexicons as linguistic resource (Qian et al., 2017; Chen et"
2020.emnlp-main.567,P15-1132,1,0.930297,"state-of-the-art performance on various downstream tasks (Wang et al., 2019a). Despite the great success of pre-trained models, existing pre-training tasks like masked language model and next sentence prediction (Devlin et al., ∗ Equal contribution Corresponding author 1 The data, codes, and model parameters are available at https://github.com/thu-coai/SentiLARE. † 2019) neglect to consider the linguistic knowledge. Such knowledge is important for some NLP tasks, particularly for sentiment analysis. For instance, existing work has shown that linguistic knowledge including part-of-speech tag (Qian et al., 2015; Huang et al., 2017) and word-level sentiment polarity (Qian et al., 2017) is closely related to the sentiment of longer texts. We argue that pre-trained models enriched with the linguistic knowledge of words will facilitate the understanding of the sentiment of the whole texts, thereby resulting in better performance on sentiment analysis. There are two major challenges to construct knowledge-aware pre-trained language representation models which can promote the downstream tasks in sentiment analysis: 1) Knowledge acquisition across different contexts. Most of the existing work has adopted s"
2020.emnlp-main.567,P18-1215,0,0.0131144,"sks. Task-specific Pre-trained Models: We used BERT-PT (Xu et al., 2019), TransBERT (Li et al., 2019), and SentiBERT (Yin et al., 2020) as taskspecific pre-trained baselines. Since TransBERT is not originally designed to deal with sentiment analysis tasks, we chose review-level sentiment classification on Yelp Dataset Challenge 2019 as the transfer task, and the downstream tasks in sentiment analysis as the target tasks. Task-specific Models without Pre-training: We also chose some task-specific baselines without pre-training for corresponding tasks, including SCSNN (Chen et al., 2019), DRNN (Wang, 2018), ML (Sachan et al., 2019) for sentence-level sentiment classification, DE-CNN (Xu et al., 2018) for aspect term extraction, CDT (Sun et al., 2019a) for aspect term sentiment classification, TAN (Movahedi et al., 2019) for aspect category detection, and ASCapsules (Wang et al., 2019b) for aspect category sentiment classification. We evaluated all the pre-trained baselines based on the codes and the model parameters provided by the original papers. For a fair comparison, all the pre-trained models were set to the base version, which possess a similar number of parameters (about 110M). The exper"
2020.emnlp-main.567,H05-1044,0,0.0524276,"tained by weighting the matched senses with context-aware sentiment attention. During pre-training, the model is trained based on label-aware masked language model including early fusion and late supervision. Red dotted boxes denote that the linguistic knowledge is used in input embedding or pre-training loss function. can also be incorporated into all layers of RNN as tag embeddings (Qian et al., 2015). Huang et al. (2017) shows that part of speech can help to learn sentiment-favorable representations. Word-level sentiment polarity is mostly derived from sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Guerini et al. (2013) obtains the prior sentiment polarity by weighting the sentiment scores over all the senses of a word in SentiWordNet (Esuli and Sebastiani, 2006; Baccianella et al., 2010). Teng et al. (2016) proposes a context-aware lexicon-based weighted sum model, which weights the prior sentiment scores of sentiment words to derive the sentiment label of the whole sentence. Qian et al. (2017) models the linguistic role of sentiment, negation and intensity words via linguistic regularizers in the training objective of LSTM. 3 3.1 Model Task Definition and Model Overview Our task is d"
2020.emnlp-main.567,P18-2094,0,0.0192082,"al., 2019), and SentiBERT (Yin et al., 2020) as taskspecific pre-trained baselines. Since TransBERT is not originally designed to deal with sentiment analysis tasks, we chose review-level sentiment classification on Yelp Dataset Challenge 2019 as the transfer task, and the downstream tasks in sentiment analysis as the target tasks. Task-specific Models without Pre-training: We also chose some task-specific baselines without pre-training for corresponding tasks, including SCSNN (Chen et al., 2019), DRNN (Wang, 2018), ML (Sachan et al., 2019) for sentence-level sentiment classification, DE-CNN (Xu et al., 2018) for aspect term extraction, CDT (Sun et al., 2019a) for aspect term sentiment classification, TAN (Movahedi et al., 2019) for aspect category detection, and ASCapsules (Wang et al., 2019b) for aspect category sentiment classification. We evaluated all the pre-trained baselines based on the codes and the model parameters provided by the original papers. For a fair comparison, all the pre-trained models were set to the base version, which possess a similar number of parameters (about 110M). The experimental results were presented with mean values over 5 runs. As for the task-specific baselines"
2020.tacl-1.19,W17-5526,0,0.0936601,"Missing"
2020.tacl-1.19,W17-5506,0,0.163439,"er simulation, etc. 1 Introduction Recently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures (Yao et al., 2013; Wen et al., 2015; Mrkˇsi´c et al., 2017; Peng et al., 2017; Lei et al., 2018; G¨ur et al., 2018). However, research is still largely limited by the lack of large-scale highquality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WOZ 2.0 (Wen et al., 2017), and M2M (Shah et al., 2018). 1. The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains ∗ Corresponding author. 281 Transactions of the Association for Computational Linguistics, vol. 8, pp. 281–295, 2020. https://doi.org/10.1162/tacl a 00314 Action Editor: Bonnie Webber. Submission batch: 10/2019; Revision batch: 1/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. in CrossWOZ. As shown in Figure 1 and Table 2, the hotel must be nea"
2020.tacl-1.19,H90-1021,0,0.827971,"f tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc. 1 Introduction Recently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures (Yao et al., 2013; Wen et al., 2015; Mrkˇsi´c et al., 2017; Peng et al., 2017; Lei et al., 2018; G¨ur et al., 2018). However, research is still largely limited by the lack of large-scale highquality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WOZ 2.0 (Wen et al., 2017), and M2M (Shah et al., 2018). 1. The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains ∗ Corresponding author. 281 Transactions of the Association for Computational Linguistics, vol. 8, pp. 281–295, 2020. https://doi.org/10.1162/tacl a 00314 Action Editor: Bonnie Webber. Submission batch: 10/2019; Revision batch: 1/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed unde"
2020.tacl-1.19,D18-1547,0,0.119842,"Missing"
2020.tacl-1.19,W14-4337,0,0.146229,"modeling, such as dialogue state tracking, policy learning, user simulation, etc. 1 Introduction Recently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures (Yao et al., 2013; Wen et al., 2015; Mrkˇsi´c et al., 2017; Peng et al., 2017; Lei et al., 2018; G¨ur et al., 2018). However, research is still largely limited by the lack of large-scale highquality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WOZ 2.0 (Wen et al., 2017), and M2M (Shah et al., 2018). 1. The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains ∗ Corresponding author. 281 Transactions of the Association for Computational Linguistics, vol. 8, pp. 281–295, 2020. https://doi.org/10.1162/tacl a 00314 Action Editor: Bonnie Webber. Submission batch: 10/2019; Revision batch: 1/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. in CrossWO"
2020.tacl-1.19,N19-1423,0,0.0180028,"in transition is a key factor for a cross-domain dialogue system, natural language understanding models need to utilize context information more effectively. 6.1 Natural Language Understanding Task: The natural language understanding component in a task-oriented dialogue system takes an utterance as input and outputs the corresponding semantic representation, namely, a dialogue act. The task can be divided into two sub-tasks: intent classification that decides the intent type of an utterance, and slot tagging which identifies the value of a slot. Model: We adapted BERTNLU from ConvLab2. BERT (Devlin et al., 2019) has shown strong performance in many NLP tasks. We use Chinese pre-trained BERT1 (Cui et al., 2019) for initial6.2 Dialogue State Tracking Task: Dialogue state tracking is responsible for recognizing user goals from the dialogue context and then encoding the goals into the pre-defined 1 BERT-wwm-ext model in https://github.com/ ymcui/Chinese-BERT-wwm. 289 S M M+T CM CM+T Overall BERTNLU – context Dialogue act F1 96.69 96.01 96.15 94.99 95.38 94.55 93.05 93.70 90.66 90.82 95.53 91.85 RuleDST TRADE Joint state accuracy (single turn) 84.17 78.17 81.93 63.38 67.86 Joint state accuracy 71.67 45.29"
2020.tacl-1.19,P18-1133,0,0.134956,"vide a user simulator and several benchmark models for pipelined taskoriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc. 1 Introduction Recently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures (Yao et al., 2013; Wen et al., 2015; Mrkˇsi´c et al., 2017; Peng et al., 2017; Lei et al., 2018; G¨ur et al., 2018). However, research is still largely limited by the lack of large-scale highquality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WOZ 2.0 (Wen et al., 2017), and M2M (Shah et al., 2018). 1. The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains ∗ Corresponding author. 281 Transactions"
2020.tacl-1.19,D17-1259,0,0.0440343,"user simulator, which will facilitate the development and evaluation of dialogue models on this corpus. The corpus and the benchmark models are publicly available at https://github.com/ thu-coai/CrossWOZ. 2 Related Work According to whether the dialogue agent is human or machine, we can group the collection methods of existing task-oriented dialogue datasets into three categories. The first one is human-to-human dialogues. One of the earliest and well-known is the ATIS dataset (Hemphill et al., 1990) used this setting, followed by El Asri et al. (2017), Eric et al. (2017), Wen et al. (2017), Lewis et al. (2017), Wei et al. (2018), and Budzianowski et al. (2018b). Though this setting requires many human efforts, it can collect natural and diverse dialogues. The second one is human-to-machine dialogues, which need a ready dialogue system to converse with humans. The famous Dialogue State Tracking Challenges provided a set of human-to-machine dialogue data (Williams et al., 2013; Henderson Figure 1: A dialogue example. The user state is initialized by the user goal: Finding an attraction and one of its nearby hotels, then booking a taxi to commute between these two places. In addition to expressing pre"
2020.tacl-1.19,P17-1163,0,0.123031,"Missing"
2020.tacl-1.19,D17-1237,0,0.201526,"sation. We also provide a user simulator and several benchmark models for pipelined taskoriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc. 1 Introduction Recently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures (Yao et al., 2013; Wen et al., 2015; Mrkˇsi´c et al., 2017; Peng et al., 2017; Lei et al., 2018; G¨ur et al., 2018). However, research is still largely limited by the lack of large-scale highquality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WOZ 2.0 (Wen et al., 2017), and M2M (Shah et al., 2018). 1. The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains ∗ Corresponding author."
2020.tacl-1.19,D15-1199,0,0.110348,"Missing"
2020.tacl-1.19,N07-2038,0,0.287802,"Missing"
2020.tacl-1.19,D19-1010,1,0.862571,"are underlined. Some turns are omitted to save space. Names of hotels are replaced by A,B,C for simplicity. Cross-domain constraints are pre-specified in MultiWOZ and Schema, while determined dynamically in CrossWOZ. In CrossWOZ, the choice in one domain will greatly affect related domains. much attention recently, due to its large size and multi-domain characteristics. It is at least one order of magnitude larger than previous datasets, amounting to 8,438 dialogues and 115K turns in the training set. It greatly promotes the research on multi-domain dialogue modeling, such as policy learning (Takanobu et al., 2019), state tracking (Wu et al., 2019), and context-to-text generation (Budzianowski et al., 2018a). Recently the Schema dataset has been collected in a machine-to-machine fashion, resulting in 16,142 dialogues and 330K turns for 16 domains in the training set. However, the multi-domain dependency in these two datasets is only embodied in imposing the same pre-specified constraints on system simulators to generate dialogue outlines, then use templates (Peng et al., 2017) to generate dialogues or further use people to paraphrase the dialogues to make them more natural (Shah et al., 2018; Rastogi et"
2020.tacl-1.19,W13-4065,0,0.0807343,"ee categories. The first one is human-to-human dialogues. One of the earliest and well-known is the ATIS dataset (Hemphill et al., 1990) used this setting, followed by El Asri et al. (2017), Eric et al. (2017), Wen et al. (2017), Lewis et al. (2017), Wei et al. (2018), and Budzianowski et al. (2018b). Though this setting requires many human efforts, it can collect natural and diverse dialogues. The second one is human-to-machine dialogues, which need a ready dialogue system to converse with humans. The famous Dialogue State Tracking Challenges provided a set of human-to-machine dialogue data (Williams et al., 2013; Henderson Figure 1: A dialogue example. The user state is initialized by the user goal: Finding an attraction and one of its nearby hotels, then booking a taxi to commute between these two places. In addition to expressing prespecified informable slots and filling in requestable slots, users need to consider and modify cross-domain informable slots (bold) that vary through conversation. We only show a few turns (turn number on the left), each with either user or system state of the current domain, which are shown above each utterance. et al., 2014). The performance of the dialogue system wil"
2020.tacl-1.19,P19-1078,0,0.27026,"o save space. Names of hotels are replaced by A,B,C for simplicity. Cross-domain constraints are pre-specified in MultiWOZ and Schema, while determined dynamically in CrossWOZ. In CrossWOZ, the choice in one domain will greatly affect related domains. much attention recently, due to its large size and multi-domain characteristics. It is at least one order of magnitude larger than previous datasets, amounting to 8,438 dialogues and 115K turns in the training set. It greatly promotes the research on multi-domain dialogue modeling, such as policy learning (Takanobu et al., 2019), state tracking (Wu et al., 2019), and context-to-text generation (Budzianowski et al., 2018a). Recently the Schema dataset has been collected in a machine-to-machine fashion, resulting in 16,142 dialogues and 330K turns for 16 domains in the training set. However, the multi-domain dependency in these two datasets is only embodied in imposing the same pre-specified constraints on system simulators to generate dialogue outlines, then use templates (Peng et al., 2017) to generate dialogues or further use people to paraphrase the dialogues to make them more natural (Shah et al., 2018; Rastogi et al., 2019). It needs much less hu"
2020.tacl-1.19,2020.acl-demos.19,1,\N,Missing
2020.tacl-1.7,P19-1470,0,0.0616403,"et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin et al., 2018) to enhance language representation for reading comprehension (Yang et al., 2019a) and other knowledge-driven NLP tasks like entity typing and relation classification (Zhang et al., 2019). Besides, Sun et al. (2019) improved BERT on Chinese NLP tasks by multi-stage knowledge masking strategy to integrate phrase and entity level knowledge into the language representation. Moreover, Bosselut et al. (2019) transferred the implicit knowledge from GPT-2 by fine-tuning the model to generate an object given the subject and a relation as input in commonsense knowledge graphs, that is, automatic knowledge base construction. However, the low novelty of the generated objects showed that it could still be difficult for GPT-2 to generate commonsense texts solely based on its implicit knowledge. Therefore, we target integrating external knowledge into GPT-2 for generating more reasonable commonsense stories. 2.2 Pretraining Recently, large-scale pretraining models have been widely developed in various NLP"
2020.tacl-1.7,D15-1075,0,0.0379736,"e dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge to enrich the limited source information. Commonsense knowledge has been demonstrated to significantly improve dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin et al., 2018) to enha"
2020.tacl-1.7,N18-1204,0,0.0395001,"github.com/thu-coai/CommonsenseStoryGen, and demo is available at http://coai.cs.tsinghua. edu.cn/static/CommonsenseStoryGen. 94 et al., 2014), we build our model based on GPT-2 because of its simplicity and broad applicability. short text descriptions (Jain et al., 2017). Different from these studies, we consider the setting of open-ended story generation from only a limited leading context in this paper. For this task, prior studies have attempted to build specific sentence representations by modeling story entities and events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mi"
2020.tacl-1.7,N19-1423,0,0.0269491,"Missing"
2020.tacl-1.7,P18-1082,0,0.410495,"the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. 1 Introduction Story generation is a strong indicator of machine understanding of natural language. It is often approached as selecting a sequence of events to form a story with a reasonable logic or plot. Although existing generative models (Roemmele, 2016; Fan et al., 2018; Fan et al., 2019) can generate stories with good local coherence, they ∗ Corresponding author: Minlie Huang. 93 Transactions of the Association for Computational Linguistics, vol. 8, pp. 93–108, 2020. https://doi.org/10.1162/tacl a 00302 Action Editor: Noah Smith. Submission batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. these knowledge bases, which can provide additional crucial information for story generation. Empirical experiments demonstrate that training with millions of such examples hel"
2020.tacl-1.7,P19-1254,0,0.347048,"a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. 1 Introduction Story generation is a strong indicator of machine understanding of natural language. It is often approached as selecting a sequence of events to form a story with a reasonable logic or plot. Although existing generative models (Roemmele, 2016; Fan et al., 2018; Fan et al., 2019) can generate stories with good local coherence, they ∗ Corresponding author: Minlie Huang. 93 Transactions of the Association for Computational Linguistics, vol. 8, pp. 93–108, 2020. https://doi.org/10.1162/tacl a 00302 Action Editor: Noah Smith. Submission batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. these knowledge bases, which can provide additional crucial information for story generation. Empirical experiments demonstrate that training with millions of such examples helps improve the cohe"
2020.tacl-1.7,P19-1134,0,0.0494618,"Missing"
2020.tacl-1.7,K17-1034,0,0.0131358,"is difficult to match the events extracted from the training data with those stored in KB. (2) Learning and utilizing multi-hop triples in knowledge graphs is costly in time because of the large-scale size. (3) Most of KB triples do not appear in the task-specific training data, so that those absent triples are not fully utilized in existing models. Fortunately, our model is trained on the knowledge bases directly, which can effectively ease these limitations. We transform the commonsense triples in ConceptNet and ATOMIC into readable natural language sentences using a template-based method (Levy et al., 2017), as illustrated in Table 2. We do not use roughly concatenated triples in order to avoid introducing additional special tokens (e.g., UsedFor in ConceptNet and oEffect in ATOMIC), or break the syntactic features contained in the pretrained language model (Alt et al., 2019), which are essential for following story generation. And then the language model is post-trained on the transformed sentences to learn commonsense knowledge between entities and events by minimizing the negative likelihood of predicting the next token: LKG = − |r | X logP (rt |r<t ), Figure 2: An example of fake story const"
2020.tacl-1.7,N16-1014,0,0.158784,"n and fully connected layers. Radford et al. (2019) used a 12-layer decoder-only transformer (GPT-2) (i.e., a left-to-right language model) with masked self-attention heads which are constrained in that every token can only attend to its left context. Formally, the objective in this stage is to minimize the following negative likelihood: LGP T = − |u| X logP (ut |u<t ), (1) P (ut |u<t ) = softmax(HL t W + b), (2) notable work for dialog generation (Zhou et al., 2018). To leverage commonsense knowledge in pretrained language models, we resort to existing large-scale knowledge bases ConceptNet (Li et al., 2016b) and ATOMIC (Sap et al., 2019). The ConceptNet dataset2 consists of triples obtained from the Open Mind Common Sense entries in ConceptNet 5 (Speer and Havasi, 2012). It contains 34 relations in total and represents each knowledge triple by R = (h, r, t), meaning that head concept h has the relation r with tail concept t for example, (cross street, Causes, accident). And the ATOMIC dataset3 is an atlas of everyday commonsense reasoning containing a mass of textual description of inferential knowledge organized as typed if-then triples. For example, a typical if-then triple is (PersonX pays P"
2020.tacl-1.7,P16-1137,0,0.15463,"n and fully connected layers. Radford et al. (2019) used a 12-layer decoder-only transformer (GPT-2) (i.e., a left-to-right language model) with masked self-attention heads which are constrained in that every token can only attend to its left context. Formally, the objective in this stage is to minimize the following negative likelihood: LGP T = − |u| X logP (ut |u<t ), (1) P (ut |u<t ) = softmax(HL t W + b), (2) notable work for dialog generation (Zhou et al., 2018). To leverage commonsense knowledge in pretrained language models, we resort to existing large-scale knowledge bases ConceptNet (Li et al., 2016b) and ATOMIC (Sap et al., 2019). The ConceptNet dataset2 consists of triples obtained from the Open Mind Common Sense entries in ConceptNet 5 (Speer and Havasi, 2012). It contains 34 relations in total and represents each knowledge triple by R = (h, r, t), meaning that head concept h has the relation r with tail concept t for example, (cross street, Causes, accident). And the ATOMIC dataset3 is an atlas of everyday commonsense reasoning containing a mass of textual description of inferential knowledge organized as typed if-then triples. For example, a typical if-then triple is (PersonX pays P"
2020.tacl-1.7,P11-2057,0,0.017414,"d events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge to enrich the limited source information. Commonsense knowledge has been demonstrated to significantly improve dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin"
2020.tacl-1.7,P19-1373,0,0.0670514,"e pretraining models have been widely developed in various NLP tasks. Some work leveraged pretraining to provide better language representations at the word level (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) or sentence level (Le and Mikolov, 2014; Kiros et al., 2015) for various downstream task-specific architectures. However, Radford et al. (2018) and Devlin et al. (2018) suggest that these complex task-specific architectures are no longer necessary, and it is sufficient to merely fine-tune pretrained task-independent transformer language models for downstream tasks. Mehri et al. (2019) explored different pretraining methods based on language models for dialogue context representation learning. Furthermore, Radford et al. (2019) demonstrate pretrained language models (i.e., GPT-2) can perform downstream tasks better than state-of-the-art models even in a zero-shot setting (i.e., without any finetuning on task-specific data). Wolf et al. (2019) fine-tuned GPT-2 for personalized conversation generation, which obtains very competitive results in the challenge. However, as previous studies (See et al., 2019; Holtzman et al., 2019) observed, transferring GPT-2 directly to open-en"
2020.tacl-1.7,D17-1195,0,0.0376794,"able at https:// github.com/thu-coai/CommonsenseStoryGen, and demo is available at http://coai.cs.tsinghua. edu.cn/static/CommonsenseStoryGen. 94 et al., 2014), we build our model based on GPT-2 because of its simplicity and broad applicability. short text descriptions (Jain et al., 2017). Different from these studies, we consider the setting of open-ended story generation from only a limited leading context in this paper. For this task, prior studies have attempted to build specific sentence representations by modeling story entities and events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), read"
2020.tacl-1.7,N16-1098,0,0.503158,"as trying to get home but the other passengers would not let her go. He thought she was going to die because of her weight. She was trying to get home but the other passengers would not let her go. The bus driver would not let her go. Fine-tuned GPT-2: I was on my way to a party. I was on my way to a party and I ’d gotten out of my seat, and started driving. I got a flat tire, so I stopped driving. I drove to the party and had a great time. Table 1: Story examples generated by human and GPT-2 models. The stories written by the pretrained GPT-2 and fine-tuned GPT-2 (post-trained on ROCStories [Mostafazadeh et al., 2016b]) suffer from repetition (in italic), bad inter-sentence coherence to the context (e.g., ignoring key entities such as accident in bold), as well as conflicting logic (underlined, e.g., first stopped driving and then drove to the party), in spite of their good fluency and intra-sentence coherence. • We propose a knowledge-enhanced pretraining model for commonsense story generation by extending GPT-2 with external commonsense knowledge. The model is post-trained on the knowledge examples constructed from ConceptNet and ATOMIC, thereby improving long-range coherence of generated stories. • To"
2020.tacl-1.7,W16-2505,0,0.0960944,"as trying to get home but the other passengers would not let her go. He thought she was going to die because of her weight. She was trying to get home but the other passengers would not let her go. The bus driver would not let her go. Fine-tuned GPT-2: I was on my way to a party. I was on my way to a party and I ’d gotten out of my seat, and started driving. I got a flat tire, so I stopped driving. I drove to the party and had a great time. Table 1: Story examples generated by human and GPT-2 models. The stories written by the pretrained GPT-2 and fine-tuned GPT-2 (post-trained on ROCStories [Mostafazadeh et al., 2016b]) suffer from repetition (in italic), bad inter-sentence coherence to the context (e.g., ignoring key entities such as accident in bold), as well as conflicting logic (underlined, e.g., first stopped driving and then drove to the party), in spite of their good fluency and intra-sentence coherence. • We propose a knowledge-enhanced pretraining model for commonsense story generation by extending GPT-2 with external commonsense knowledge. The model is post-trained on the knowledge examples constructed from ConceptNet and ATOMIC, thereby improving long-range coherence of generated stories. • To"
2020.tacl-1.7,P02-1040,0,0.107429,"ch sentence with RAKE algorithm (Rose et al., 2010). Skeleton-based Model with Reinforcement Learning (SKRL): The model first generates a compressed story including the most critical phrases, called skeleton, and then generates a story conditioned upon the skeleton. The skeleton is automatically learned by reinforcement learning (Xu et al., 2018). 4.4 Automatic Evaluation Evaluation Metrics We adopted the following automatic metrics to evaluate the generation performance in the entire test set. (1) Perplexity (PPL). Smaller perplexity scores indicate better fluency in general. (2) BLEU. BLEU (Papineni et al., 2002) evaluates n-gram overlap between a generated story and a human-written story. However, BLEU is usually inappropriate for openended text generation (Fan et al., 2018) because there are multiple plausible stories for the same input but only one story is given in the dataset. And BLEU scores will become extremely low for large n. We thus experimented with n = 1,2. (3) Coverage. To access the effect of incorporating commonsense knowledge, we calculated the coverage score as the average number of commonsense triples matched in each generated story, which requires both head and tail entities/events"
2020.tacl-1.7,D14-1162,0,0.0837457,"on as input in commonsense knowledge graphs, that is, automatic knowledge base construction. However, the low novelty of the generated objects showed that it could still be difficult for GPT-2 to generate commonsense texts solely based on its implicit knowledge. Therefore, we target integrating external knowledge into GPT-2 for generating more reasonable commonsense stories. 2.2 Pretraining Recently, large-scale pretraining models have been widely developed in various NLP tasks. Some work leveraged pretraining to provide better language representations at the word level (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) or sentence level (Le and Mikolov, 2014; Kiros et al., 2015) for various downstream task-specific architectures. However, Radford et al. (2018) and Devlin et al. (2018) suggest that these complex task-specific architectures are no longer necessary, and it is sufficient to merely fine-tune pretrained task-independent transformer language models for downstream tasks. Mehri et al. (2019) explored different pretraining methods based on language models for dialogue context representation learning. Furthermore, Radford et al. (2019) demonstrate pretrained language models (i.e."
2020.tacl-1.7,N18-1202,0,0.015855,"e knowledge graphs, that is, automatic knowledge base construction. However, the low novelty of the generated objects showed that it could still be difficult for GPT-2 to generate commonsense texts solely based on its implicit knowledge. Therefore, we target integrating external knowledge into GPT-2 for generating more reasonable commonsense stories. 2.2 Pretraining Recently, large-scale pretraining models have been widely developed in various NLP tasks. Some work leveraged pretraining to provide better language representations at the word level (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) or sentence level (Le and Mikolov, 2014; Kiros et al., 2015) for various downstream task-specific architectures. However, Radford et al. (2018) and Devlin et al. (2018) suggest that these complex task-specific architectures are no longer necessary, and it is sufficient to merely fine-tune pretrained task-independent transformer language models for downstream tasks. Mehri et al. (2019) explored different pretraining methods based on language models for dialogue context representation learning. Furthermore, Radford et al. (2019) demonstrate pretrained language models (i.e., GPT-2) can perform d"
2020.tacl-1.7,K19-1079,0,0.320773,"a University, Beijing 100084, China 2 School of Software, Beihang University, Beijing, China 1 Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems 1 Beijing National Research Center for Information Science and Technology j-guan19@mails.tsinghua.edu.cn,f-huang18@mails.tsinghua.edu.cn, extsuioku@gmail.com, zxy-dcs@tsinghua.edu.cn, aihuang@tsinghua.edu.cn Abstract are still struggling to plan a coherent plot and maintain a reasonable event sequence throughout the story, or they are often biased towards generating a limited set of stories with generic plots (See et al., 2019) (e.g., I have a great time), even when using the powerful generative model OpenAI’s GPT-2 (Radford et al., 2019), as shown in Table 1. Pretrained GPT-2 has been shown to capture useful semantic and syntactic features (Alt et al., 2019), as demonstrated by state-of-theart performance on some generation tasks such as machine translation and text summarization (Radford et al., 2019). However, compared with such tasks whose source inputs have contained sufficient information to generate desired target texts, story generation is a typical openended generation task, where only very limited informat"
2020.tacl-1.7,D19-1321,1,0.78663,"re are multiple plausible stories for the same input but only one story is given in the dataset. And BLEU scores will become extremely low for large n. We thus experimented with n = 1,2. (3) Coverage. To access the effect of incorporating commonsense knowledge, we calculated the coverage score as the average number of commonsense triples matched in each generated story, which requires both head and tail entities/events appears in the same story. (4) Repetition. We measured the redundancy of stories by computing repetition-4, the percentage of generated stories that repeat at least one 4-gram (Shao et al., 2019). (5) Distinct. To measure the generation diversity, we adopted distinct-4 (Li et al., 2016a), the ratio of distinct 4-grams to all the generated 4-grams. Decomposed Model with Semantic Role Labeling (DSRL): It first generates a predicateargument structure conditioned upon the beginning and then generates a story by surface realization on top of the structure. The structures are identified by semantic role labelling (Fan et al., 2019). We also made comparisons with GPT-2 in different settings as follows: GPT-2 (Scratch): The network architecture is the same as GPT-2, but the model is only trai"
2020.tacl-1.7,speer-havasi-2012-representing,0,0.235598,"e knowledge for expanding a reasonable story, handling the causal relationships, as well as deciding the temporal orders between entities and events in context. Explicitly introducing external commonsense knowledge has been shown helpful to improve language understanding and long-range coherence of generated texts (Zhou et al., 2018; Guan et al., 2019; Yang et al., 2019b). For example, for the entities in the given context of Table 1, many potentially related concepts (e.g., run over, cross street) can be inferred and predicted based on external commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). These knowledge bases contain abundant semantic knowledge of concepts and inferential knowledge for commonsense reasoning. We enhance GPT-2 with such knowledge by post-training the model on the knowledge examples constructed from • We conduct extensive experiments with automatic and manual evaluation. Results show that our model can generate more reasonable stories than strong baselines, particularly in terms of logicality and global coherence.1 2 Related Work 2.1 Neural Story Generation Many existing neural story generation models generated stories by condition"
2020.tacl-1.7,P18-1213,0,0.0457733,"Missing"
2020.tacl-1.7,P18-1043,0,0.0289463,"n sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge to enrich the limited source information. Commonsense knowledge has been demonstrated to significantly improve dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin et al., 2018) to enhance language represent"
2020.tacl-1.7,C16-1100,0,0.0422215,"Missing"
2020.tacl-1.7,D18-1462,0,0.0892502,"build our model based on GPT-2 because of its simplicity and broad applicability. short text descriptions (Jain et al., 2017). Different from these studies, we consider the setting of open-ended story generation from only a limited leading context in this paper. For this task, prior studies have attempted to build specific sentence representations by modeling story entities and events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge"
2020.tacl-1.7,D16-1023,0,0.0130188,"previous studies (See et al., 2019; Holtzman et al., 2019) observed, transferring GPT-2 directly to open-ended text generation still suffers from several issues such as repetition or lack of knowledge and inter-sentence coherence with different decoding algorithms. Besides, although Song et al. (2019) and Dong et al. (2019) extended the language model to support an encoder-decoder framework (Sutskever 2.4 Multi-Task Learning Incorporating other auxiliary task objectives to complement the primary goal has been shown to improve the performance in many NLP tasks such as sentiment classification (Yu and Jiang, 2016) and conversation generation (Zhao et al., 2017). Recently, multi-task learning was also used to pretrain language models to capture dependencies in context (Devlin et al., 2018; Mehri et al., 2019) and further improve pretrained models’ representation power during fine-tuning (Wolf et al., 2019). 95 3 Methodology The task in this work can be defined as follows: Given a one-sentence story beginning X as the leading context, the model should continue to complete a K -sentence story Y with a reasonable plot. The sentences in a generated story should have reasonable logical connections, causal re"
2020.tacl-1.7,P19-1139,0,0.0511831,"Missing"
2020.tacl-1.7,P19-1226,0,0.376298,"istinguish true stories from auto-constructed fake stories. The auxiliary task makes the model implicitly capture the causal, temporal dependencies between sentences and inter-sentence coherence, and lead to less repetition. dependent commonsense knowledge for expanding a reasonable story, handling the causal relationships, as well as deciding the temporal orders between entities and events in context. Explicitly introducing external commonsense knowledge has been shown helpful to improve language understanding and long-range coherence of generated texts (Zhou et al., 2018; Guan et al., 2019; Yang et al., 2019b). For example, for the entities in the given context of Table 1, many potentially related concepts (e.g., run over, cross street) can be inferred and predicted based on external commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). These knowledge bases contain abundant semantic knowledge of concepts and inferential knowledge for commonsense reasoning. We enhance GPT-2 with such knowledge by post-training the model on the knowledge examples constructed from • We conduct extensive experiments with automatic and manual evaluation. Results show t"
2020.tacl-1.7,P17-1061,0,0.0273739,"l., 2019) observed, transferring GPT-2 directly to open-ended text generation still suffers from several issues such as repetition or lack of knowledge and inter-sentence coherence with different decoding algorithms. Besides, although Song et al. (2019) and Dong et al. (2019) extended the language model to support an encoder-decoder framework (Sutskever 2.4 Multi-Task Learning Incorporating other auxiliary task objectives to complement the primary goal has been shown to improve the performance in many NLP tasks such as sentiment classification (Yu and Jiang, 2016) and conversation generation (Zhao et al., 2017). Recently, multi-task learning was also used to pretrain language models to capture dependencies in context (Devlin et al., 2018; Mehri et al., 2019) and further improve pretrained models’ representation power during fine-tuning (Wolf et al., 2019). 95 3 Methodology The task in this work can be defined as follows: Given a one-sentence story beginning X as the leading context, the model should continue to complete a K -sentence story Y with a reasonable plot. The sentences in a generated story should have reasonable logical connections, causal relationships, and temporal dependencies with each"
2020.tacl-1.7,P19-1193,0,0.423439,"istinguish true stories from auto-constructed fake stories. The auxiliary task makes the model implicitly capture the causal, temporal dependencies between sentences and inter-sentence coherence, and lead to less repetition. dependent commonsense knowledge for expanding a reasonable story, handling the causal relationships, as well as deciding the temporal orders between entities and events in context. Explicitly introducing external commonsense knowledge has been shown helpful to improve language understanding and long-range coherence of generated texts (Zhou et al., 2018; Guan et al., 2019; Yang et al., 2019b). For example, for the entities in the given context of Table 1, many potentially related concepts (e.g., run over, cross street) can be inferred and predicted based on external commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). These knowledge bases contain abundant semantic knowledge of concepts and inferential knowledge for commonsense reasoning. We enhance GPT-2 with such knowledge by post-training the model on the knowledge examples constructed from • We conduct extensive experiments with automatic and manual evaluation. Results show t"
2021.acl-long.237,D19-1243,0,0.0558251,"Missing"
2021.acl-long.237,2020.acl-main.679,0,0.124454,"tion (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020). Table 1 lists several typical score functions. However, these scores can be easily influenced by word frequencies, sentence structures, and other 3037 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3037–3049 August 1–6, 2021. ©2021 Association for Computational Linguistics factors, which can mislead the models and make existing methods oversensitive to lexical perturbations (Abdou et al., 2020; Tamborrino et al., 2020). Figure 1 shows two examples. The correct choices are paraphrased via synonym replacement or structure transformation. In these examples, the baseline (Pro-A) produces much lower scores for the paraphrased choices and chooses the wrong choices. Since existing methods can be easily distracted by irrelevant factors such as lexical perturbations, we argue that a commonsense question answering method should focus on the answers’ semantics and assign similar scores to synonymous choices. To this end, we introduce a novel SEmantic-based Question Answering model, SEQA, whic"
2021.acl-long.237,2020.emnlp-main.11,0,0.0152985,"ch has no restriction on the query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations. Besides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks. 3 And it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al."
2021.acl-long.237,2020.coling-main.467,0,0.0294948,"e query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations. Besides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks. 3 And it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al., 2020) for our method and"
2021.acl-long.237,N16-1098,0,0.0192474,"Evaluation results, including the original selection accuracy before attack, the accuracy after attack, the attack success rate, the percentage of perturbed words with respect to the original sentence length in successful attacks, and the semantic similarity between the original and paraphrased choices. GPT-2, RoBERTa and SRoBERTa refer to GPT-2-xlarge, RoBERTa-large (Liu et al., 2019) and SentenceRoBERTa-large, respectively. 4 4.1 Experiments 4.3 Datasets We conducted experiments on four multiplechoice commonsense question answering tasks, COPA (Roemmele et al., 2011), StoryClozeTest (SCT) (Mostafazadeh et al., 2016), SocialIQA (Sap et al., 2019b) and CosmosQA (Huang et al., 2019). For each instance, only one choice is correct. See Appendix for more description about datasets. For COPA, we reported the results on its test set. As the test sets of another three datasets are hidden, for convenience of analysis, we reported the experiment results on their development sets. 4.2 Baselines We employed five strong baselines. Table 1 shows three of them, Pro-A, Pro-Q and MI-QA. There is no explicit auxiliary information used in these three methods, while another two baselines rely on explicit information suppleme"
2021.acl-long.237,2020.findings-emnlp.369,0,0.020767,"fore, it is vital to study unsupervised commonsense question answering without relying on any labeled downstream task data. In this paper, we investigate multiple-choice commonsense question answering tasks in an unsupervised setting: given a question and a set of answer choices, a model is required to predict the most reasonable answer choice for the question, but without access to any labeled task data. Introduction Pre-trained language models have been widely used for commonsense question answering. Finetuning pre-trained models on task-specific data produces many state-of-the-art results (Wang et al., 2020; * Equal † contribution Corresponding author: Minlie Huang. Many existing unsupervised methods tackle these tasks by scoring each answer choice using a language model, e.g., estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020). Table 1 lists several typical score functions. However, these scores can be easily influenced by word frequencies, sentence structures, and other 3037 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and t"
2021.acl-long.237,D14-1162,0,0.0871353,"Missing"
2021.acl-long.237,N19-1094,0,0.021734,"generalize to different domains. Self-Talk (Shwartz et al., 2020) breaks the limit by extracting knowledge from GPT-2 (Radford et al., 2019), which has no restriction on the query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations. Besides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks. 3 And it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the t"
2021.acl-long.237,D19-1250,0,0.0572008,"Missing"
2021.acl-long.237,P19-1487,0,0.013742,"2018; Tamborrino et al., 2020) , which is denoted as Probability-Q (Pro-Q) in Table 1. Some recent work claims that external knowledge can benefit commonsense reasoning. Besides static knowledge bases (KBs), such as ConceptNet (Speer et al., 2017) and Atomic (Sap et al., 2019a), there are also numerous studies treating LMs as dynamic KBs. Petroni et al. (2019) shows that LMs can be used for KB completion. And Davison et al. (2019) shows that BERT can distinguish true and fake ConceptNet triplets. Further, the extracted knowledge can work as complementary information for answering a question. Rajani et al. (2019) proposes a model for Com3038 1 PBERT (Q|A) , Q|Q| i PBERT (Qi |Q/i , A). monSenseQA (Talmor et al., 2019) that generates explanations for questions, which are then used as additional inputs. The shortcoming of this approach is that it requires collecting human explanations for each new dataset to fine-tune LMs. Some following researches explore unsupervised explanation/knowledge generator. CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al., 2019) to generate intermediate inferences which are then used to score the choice. However, COMET is limited by a small set of question types so"
2021.acl-long.237,D19-1410,0,0.0762639,"probability of observing the choice’s semantics. A choice’s semantic score can be obtained by summing the generative probabilities of sentences that have the same semantic meanings with the choice, where the sentences are called the choice’s supporters. However, it is hard to obtain the supporters which have exactly the same semantic meanings with the choice, so we reformulate the semantic score into a soft version as explained in Section 3.2. Each supporter is weighed by the semantic similarity to the answer choice, which can be computed with some off-the-shelf models, such as SentenceBERT (Reimers and Gurevych, 2019). Since the supporters and their weights depend on the semantics rather than the surface form of the answer choice, by this means, the effects of the distracting factors can be largely suppressed. Moreover, synonymous choices are likely to share the same set of supporters, so their scores are expected to be stably close. Our contributions in this paper are summarized as follows: • We propose a semantic-based question answering model (SEQA) for robust commonsense question answering in an unsupervised setting. Instead of directly scoring the answer choices, our method first generates some plausi"
2021.acl-long.237,D19-1454,0,0.0543383,"Missing"
2021.acl-long.237,2020.emnlp-main.373,0,0.0376012,"Missing"
2021.acl-long.237,N19-1421,0,0.0193007,"laims that external knowledge can benefit commonsense reasoning. Besides static knowledge bases (KBs), such as ConceptNet (Speer et al., 2017) and Atomic (Sap et al., 2019a), there are also numerous studies treating LMs as dynamic KBs. Petroni et al. (2019) shows that LMs can be used for KB completion. And Davison et al. (2019) shows that BERT can distinguish true and fake ConceptNet triplets. Further, the extracted knowledge can work as complementary information for answering a question. Rajani et al. (2019) proposes a model for Com3038 1 PBERT (Q|A) , Q|Q| i PBERT (Qi |Q/i , A). monSenseQA (Talmor et al., 2019) that generates explanations for questions, which are then used as additional inputs. The shortcoming of this approach is that it requires collecting human explanations for each new dataset to fine-tune LMs. Some following researches explore unsupervised explanation/knowledge generator. CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al., 2019) to generate intermediate inferences which are then used to score the choice. However, COMET is limited by a small set of question types so that CGA is difficult to generalize to different domains. Self-Talk (Shwartz et al., 2020) breaks the lim"
2021.acl-long.237,2020.acl-main.357,0,0.0465758,"Missing"
2021.emnlp-main.184,P19-1081,0,0.0234423,"dels EARL can generate informative responses with both seen knowledge graphs and unseen Some prior works introduce high-quality structured knowledge graphs in two benchmark datasets. knowledge graph for conversation generation. Zhu Ablation studies demonstrate the influence of et al. (2017) presented an end-to-end knowledge different mechanisms and conversation frame- grounded conversation model using a copy networks. work (Gu et al., 2016). A large-scale commonsense 2384 knowledge graph is introduced to open-domain conversation generation by graph attention mechanisms in (Zhou et al., 2018). Moon et al. (2019) proposed a knowledge graph walker to select relevant entities of the knowledge graph to improve the performance of retrieval-based conversation models. The adjacency matrix (Tuan et al., 2019) is introduced to modeling the dynamic knowledge graph in conversation generation. However, these studies adopt pre-trained knowledge graph embeddings (Zhou et al., 2018), word embeddings (Wu et al., 2019), or the adjacency matrix (Tuan et al., 2019) to represent knowledge triples, making them not applicable for large-scale and unseen knowledge graphs. By contrast, our model addresses this issue by repre"
2021.emnlp-main.184,Y14-1039,0,0.0305778,"ion and the structure informaEARL consists of three modules: an encoder to tion of the knowledge graph. Entity-agnostic repreconvert the context to the hidden representations, sentations are defined as category representations a knowledge interpreter to represent each subject for entities sharing the same context and structure and object entity based on the context and structure information, including two major circumstances. information, and a decoder to generate a token or One is caused by the one-to-many mapping propselect an entity from the knowledge graph deter- erty of knowledge graphs (Fan et al., 2014; Xiao mined by a knowledge selector. The overview of et al., 2016), where a subject has multiple objects EARL is presented in Figure 3. with the same relation. As shown in the left knowlInstead of parameterizing specific representa- edge graph in Figure 1, (Chuck Palahniuk, Write, tions for entities of knowledge graphs as used in Pygmy) and (Chuck Palahniuk, Write, Tell-All) have prior studies (Zhou et al., 2018; Wu et al., 2019; the one-to-many mapping property, and EARL Tuan et al., 2019), EARL learns entity-agnostic rep- learns the same category representation for Pygmy resentations condit"
2021.emnlp-main.184,C16-1316,0,0.0150829,"2 2.1 Related Work Open-domain Conversation Models Recently, Sequence-to-Sequence (Seq2Seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have been applied to large-scale open-domain conversation generation, including neural responding machine (Shang et al., 2015), hierarchical recurrent models (Serban et al., 2015), and many others (Sordoni et al., 2015; Li et al., 2016; Shao et al., 2017). Some models are proposed to improve the content quality of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative responses, incorporate external knowledge into conversation generation, including unstructured texts (Ghazvininejad et al., 2018; Long et al., 2017), and structured knowledge graphs (Han et al., 2015; Xu et al., 2017; Zhou et al., 2018). 2.2 Knowledge Graph Enhanced • Automatic and manual evaluations show that Conversation Models EARL can ge"
2021.emnlp-main.184,P16-1154,0,0.0249314,"k Open-domain Conversation Models Recently, Sequence-to-Sequence (Seq2Seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have been applied to large-scale open-domain conversation generation, including neural responding machine (Shang et al., 2015), hierarchical recurrent models (Serban et al., 2015), and many others (Sordoni et al., 2015; Li et al., 2016; Shao et al., 2017). Some models are proposed to improve the content quality of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative responses, incorporate external knowledge into conversation generation, including unstructured texts (Ghazvininejad et al., 2018; Long et al., 2017), and structured knowledge graphs (Han et al., 2015; Xu et al., 2017; Zhou et al., 2018). 2.2 Knowledge Graph Enhanced • Automatic and manual evaluations show that Conversation Models EARL can generate informative"
2021.emnlp-main.184,W15-4616,0,0.0158617,"lity of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative responses, incorporate external knowledge into conversation generation, including unstructured texts (Ghazvininejad et al., 2018; Long et al., 2017), and structured knowledge graphs (Han et al., 2015; Xu et al., 2017; Zhou et al., 2018). 2.2 Knowledge Graph Enhanced • Automatic and manual evaluations show that Conversation Models EARL can generate informative responses with both seen knowledge graphs and unseen Some prior works introduce high-quality structured knowledge graphs in two benchmark datasets. knowledge graph for conversation generation. Zhu Ablation studies demonstrate the influence of et al. (2017) presented an end-to-end knowledge different mechanisms and conversation frame- grounded conversation model using a copy networks. work (Gu et al., 2016). A large-scale commonsense"
2021.emnlp-main.184,D16-1230,0,0.0290225,"p performances in automatic evaluation. For each response pair, three judges were hired to give a preference between the two responses in terms of the following two metrics. The tie was allowed. Notice that system identifiers were masked during annotation. Metrics: We adopted two widely used metrics, Appropriateness and Informativeness as proposed in (Zhou et al., 2018). Appropriateness measures the quality of the generated response at the 4 BLEU (Papineni et al., 2002) is not adopted due to its low content level (whether the response is appropricorrelation with human judgment, as proposed by Liu et al. (2016). ate in relevance, coherence, and adequacy). In2389 Dataset Model Entity Precision Recall F1 Distinct-3 Distinct-4 PPL DuConv Seen Test Set Seq2Seq DIALOGPT MemNet PostKS CopyNet CCM EARL 0.068 0.141 0.195 0.131 0.650 0.655 1.269 0.020 0.054 0.084 0.051 0.399 0.376 0.435 0.013 0.036 0.062 0.036 0.396 0.392 0.478 0.015 0.041 0.068 0.040 0.379 0.365 0.422 0.128 0.078 0.179 0.135 0.255 0.239 0.379 0.201 0.125 0.278 0.232 0.378 0.350 0.519 20.54 9.94 19.88 25.30 15.63 20.71 17.00 DuConv Unseen Test Set Seq2Seq DIALOGPT MemNet PostKS CopyNet CCM EARL 0.062 0.133 0.195 0.110 0.684 0.686 1.310 0.020"
2021.emnlp-main.184,P02-1040,0,0.114287,"e generated by EARL and the one by a baseline for the same context. In total, there are 1,200 pairs since we chose three baselines, which achieve top performances in automatic evaluation. For each response pair, three judges were hired to give a preference between the two responses in terms of the following two metrics. The tie was allowed. Notice that system identifiers were masked during annotation. Metrics: We adopted two widely used metrics, Appropriateness and Informativeness as proposed in (Zhou et al., 2018). Appropriateness measures the quality of the generated response at the 4 BLEU (Papineni et al., 2002) is not adopted due to its low content level (whether the response is appropricorrelation with human judgment, as proposed by Liu et al. (2016). ate in relevance, coherence, and adequacy). In2389 Dataset Model Entity Precision Recall F1 Distinct-3 Distinct-4 PPL DuConv Seen Test Set Seq2Seq DIALOGPT MemNet PostKS CopyNet CCM EARL 0.068 0.141 0.195 0.131 0.650 0.655 1.269 0.020 0.054 0.084 0.051 0.399 0.376 0.435 0.013 0.036 0.062 0.036 0.396 0.392 0.478 0.015 0.041 0.068 0.040 0.379 0.365 0.422 0.128 0.078 0.179 0.135 0.255 0.239 0.379 0.201 0.125 0.278 0.232 0.378 0.350 0.519 20.54 9.94 19.88"
2021.emnlp-main.184,P15-1152,0,0.0217111,"te The Selfish Gene. Figure 1: Conversation samples generated with welltrained knowledge graphs (left) and unseen knowledge graphs (right). Grey nodes are well-trained entities, and white nodes are unseen nodes in the training data. Entities in the blue rectangle share the same entity-agnostic representation (see Section 3.2 for more details). 2 2.1 Related Work Open-domain Conversation Models Recently, Sequence-to-Sequence (Seq2Seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have been applied to large-scale open-domain conversation generation, including neural responding machine (Shang et al., 2015), hierarchical recurrent models (Serban et al., 2015), and many others (Sordoni et al., 2015; Li et al., 2016; Shao et al., 2017). Some models are proposed to improve the content quality of generated responses by copy mechanisms, diversified beam search algorithms, and various techniques (Shao et al., 2017; Li et al., 2016; Mou et al., 2016; Gu et al., 2016). However, the lack of background information or related knowledge results in significantly degenerated conversations, where the text is bland and strangely repetitive (Holtzman et al., 2020). Other studies, aiming to generate informative r"
2021.emnlp-main.184,N15-1020,0,0.0775017,"Missing"
2021.emnlp-main.184,P16-1008,0,0.0217122,"object entity, respectively. Although aforementioned methods is able to represent the relevant entities related to the context, it cannot represent entities, which are not mentioned in the context or not connected to the subject entity in the context with any path in the knowledge graph. In this case, we resort to represent the entity i with Nri relations connected to it by graph attention based on the hidden state hX of the context, which is formulated as follows: Nri X is designed to allow the decoder to select object entities from knowledge graphs or words from the vocabulary. Inspired by Tu et al., 2016, we also introduce a coverage mechanism to facilitate the decoder to avoid generating repetitive entities. The decoding process is formulated as follows: (11) where gt ∈ [0, 1] is a scalar to balance the choice between an entity obj i and a generic word wg , Pg /Pe is the distribution over generic words / entities respectively, and P (yt ) is the final word decoding distribution. 3.6 Loss Function The loss function is the cross entropy between the predicted token distribution P (yt ) and the groundtruth distribution pt in the training corpus. Additionally, we apply supervised signals on the k"
2021.emnlp-main.184,D19-1194,0,0.200814,"incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models. KG Freebase Wikidata ConceptNet # Entities # Triples # Relations 40M 18M 8M 637M 66M 21M 35,000 1,632 36 Table 1: Statistics of some widely used knowledge graphs (KG, Knowledge Graph; M, million). Prior studies adopt either pre-trained knowledge graph embeddings (Zhou et al., 2018), e.g. TransE (Bordes et al., 2013), word embeddings (Wu et al., 2019), or adjacency matrix (Tuan et al., 2019) to model entities and relations in knowledge graphs and incorporate them to conversation generation. These models face two major challenges when applied to introduce large-scale knowledge graphs. First, there is a significant gap in representations between knowledge and text (Buitelaar and Cimiano, 2008; Zhou et al., 2018), which requires model training to apply knowledge in conversation generation based on different knowledge representations. However, the training corpus of knowledgegrounded conversations only contains a small subset of entities for applying knowledge, while the 1 Introducti"
2021.emnlp-main.184,P19-1369,0,0.356449,"for entities, which is generalized to incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models. KG Freebase Wikidata ConceptNet # Entities # Triples # Relations 40M 18M 8M 637M 66M 21M 35,000 1,632 36 Table 1: Statistics of some widely used knowledge graphs (KG, Knowledge Graph; M, million). Prior studies adopt either pre-trained knowledge graph embeddings (Zhou et al., 2018), e.g. TransE (Bordes et al., 2013), word embeddings (Wu et al., 2019), or adjacency matrix (Tuan et al., 2019) to model entities and relations in knowledge graphs and incorporate them to conversation generation. These models face two major challenges when applied to introduce large-scale knowledge graphs. First, there is a significant gap in representations between knowledge and text (Buitelaar and Cimiano, 2008; Zhou et al., 2018), which requires model training to apply knowledge in conversation generation based on different knowledge representations. However, the training corpus of knowledgegrounded conversations only contains a small subset of entities for a"
2021.emnlp-main.184,P16-1219,1,0.873385,"Missing"
2021.emnlp-main.184,2020.acl-demos.30,0,0.202847,"ns some noisy data, e.g. empty utterances in the dialogue. After filtering the noisy data, we randomly split the corpus in the same way as DuConv. The statistics is presented in Table 2. Conversations Training 14,845 Validation 1,800 Seen 900 Test Unseen 900 Training 10,583 Validation 1,200 Seen 600 Test Unseen 600 Knowledge Graphs Entity 12,909 Relation 39 Triple 113,959 Entity Relation 100,717 1,380 Triple 1,172,552 Table 2: Statistics of datasets and knowledge graphs. (Mikolov et al., 2010), which is widely used in open-domain conversation systems. • DIALOGPT: a pre-trained dialogue model (Zhang et al., 2020; Wang et al., 2020) based on transformers, which is widely adopted in dialogue generation. • MemNet: a knowledge-grounded model adapted from (Ghazvininejad et al., 2018), of which the memory units store word embeddings of knowledge triples. • PostKS: a knowledge-grounded model selecting knowledge by prior and posterior distributions proposed by Wu et al. (2019), where we adopt word embeddings, instead of the RNN knowledge encoder, to represent knowledge triples. • CopyNet: a copy network model (Zhu et al., 2017), which represents knowledge triples by word embeddings, and can copy words from k"
2021.findings-acl.138,2021.ccl-1.108,0,0.0263718,"Missing"
2021.findings-acl.138,W17-4902,0,0.0258789,"(b) An example of word alignments between the source and target sentences. Arrows connect aligned words (identical or relevant), and blue words are not aligned. (c) NAST’s generation process. Step 1: generate the index of aligned words. [Mask] is a placeholder for unaligned words. Step 2: generate the transferred sentence non-autoregressively. Text style transfer aims at changing the text style while preserving the style-irrelevant contents, which has a wide range of applications, e.g., sentiment transfer (Shen et al., 2017), text formalization (Rao and Tetreault, 2018), and author imitation (Jhamtani et al., 2017). Due to the lack of parallel training data, most works focus on unsupervised text style transfer using non-parallel stylistic data. The cycle consistency loss (Zhu et al., 2017), a.k.a. the back-translation loss (Lample et al., 2018, author: Minlie Huang. Not great, but good atmosphere and great service (b) Observation of Word Alignment Source: Not terrible , but Introduction * Corresponding Not terrible, but not very good 2019), has been widely adopted by unsupervised text style transfer models (Dai et al., 2019; He et al., 2020; Yi et al., 2020). Specifically, the cycle loss minimizes the r"
2021.findings-acl.138,P19-1041,0,0.370615,"based models. To the best of our knowledge, we are the first to introduce a non-autoregressive generator to an unsupervised generation task. • Experiments show that incorporating NAST in cycle-loss-based models significantly improves the overall performance and the speed of training and inference. In further analysis, we find that NAST provides better optimization of the cycle loss and learns explainable word alignments. 2 Related Work Unsupervised Text Style Transfer We categorize style transfer models into three types. The first type (Shen et al., 2017; Zhao et al., 2018; Yang et al., 2018; John et al., 2019) disentangles the style and content representations, and then combines the content representations with the target style to generate the transferred sentence. However, the disentangled representations are limited in capacity and thus hardly scalable for long sentences (Dai et al., 2019). The second type is the editing-based method (Li et al., 2018; Wu et al., 2019a,b), which edits the source sentence with several discrete operations. The operations are usually trained separately and then constitute a pipeline. These methods are highly explainable, but they usually need to locate and replace th"
2021.findings-acl.138,2020.acl-main.639,0,0.0266396,"ation seems similar to a pipeline, NAST is trained in an end-to-end fashion with the cycle loss. All transferred words in NAST are generated, not copied, which is essentially different from these methods. The third type is based on the cycle loss. Zhang et al. (2018); Lample et al. (2019) introduce the back translation method into style transfer, where the model is directly trained with the cycle loss after a proper initialization. The following works (Dai et al., 2019; Luo et al., 2019; He et al., 2020; Yi et al., 2020) further adopt a style loss to improve the style control. A recent study (Zhou et al., 2020) explores the word-level information for style transfer, which is related to our motivation. However, they focus on word-level style relevance in designing novel objectives, while we focus on modeling word alignments and the non-autoregressive architecture. Non-Autoregressive Generation Non-AutoRegressive (NAR) generation is first introduced in machine translation for parallel decoding with low latency (Gu et al., 2018). The NAR generator assumes that each token is generated independently of each other conditioned on the input sentence, which sacrifices the generation quality in exchange for t"
2021.findings-acl.223,2020.webnlg-1.8,0,0.151112,"on (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fi"
2021.findings-acl.223,2020.emnlp-main.54,1,0.690434,"o-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-t"
2021.findings-acl.223,2020.coling-main.217,0,0.0283643,"ers with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives to jointly learn the tasks of graph-to-text and textto-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-trained NLG models such as GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), recent work directly fine-tunes these models on graph-totext datasets and reports impressive performance (Ribeiro et al., 2020a; Kale and Rastogi, 2020; Chen et al., 2020b; Mager et al., 2020). Compared with the existing work on pre-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning textto-text pre-trained models on"
2021.findings-acl.223,2020.inlg-1.14,0,0.196457,"arameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Devlin et al., 2019), which i"
2021.findings-acl.223,N19-1238,0,0.0375841,"Missing"
2021.findings-acl.223,D18-2012,0,0.017748,"Lewis et al., 2020) and T5 (Raffel et al., 2020) as the base model in this paper, which are denoted by JointGT (BART) and JointGT (T5), respectively. The hyper-parameters of the Transformer blocks were the same as BARTbase and T5-base because of the limited computational resources. We initialized our model parameters with the pre-trained checkpoint of BARTbase / T5-base except for the structure-aware semantic aggregation module, which was randomly initialized. We followed BART / T5 to use BytePair Encoding (BPE) vocabulary (Radford et al., 2019) with the size of 50,265 / WordPiece vocabulary (Kudo and Richardson, 2018) with the size of 32,000. The batch size was 42 / 32 for JointGT (BART) / JointGT (T5). The maximum length of linearized input graphs was 600, while the maximum length of text sequences was 64. We adopted Adam (Kingma and Ba, 2015) as the optimizer and set the learning rate to be 3e-5. The warmup ratio was 0.1. JointGT was pre-trained on KGTEXT for 1 epoch with the proposed pre-training tasks. It took 44 / 69 hours for JointGT (BART) / JointGT (T5) on 3 NVIDIA Quadro RTX 6000 GPUs. 4.2 Fine-Tuning Settings We adopted WebNLG, WebQuestions and Path Questions as the benchmark datasets during fine"
2021.findings-acl.223,2020.acl-main.703,0,0.127151,"’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained m"
2021.findings-acl.223,2020.acl-main.167,0,0.0456604,"Missing"
2021.findings-acl.223,W18-6501,0,0.0153687,"of KG-to-text generation including WebNLG, WebQuestions and PathQuestions. Results show that JointGT achieves new state-of-theart performance on KG-to-text generation. 2 Related Work KG-to-Text Generation Recent studies on KG-to-text generation tasks mainly fall into three aspects: 1) Encoder modification: To alleviate the structural information loss of sequence encoders with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives to jointly learn the tasks of graph-to-text and textto-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-trained NLG models such as GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), recent work directly fine-tunes these models on graph-totext datasets and"
2021.findings-acl.223,N19-1236,0,0.0570043,"Missing"
2021.findings-acl.223,P02-1040,0,0.109804,"each dataset as our baselines, including Seq2Seq with copying or delexicalisation (Shimorina and Gardent, 2018) for WebNLG v2.0, and G2S (Chen et al., 2020d) for WebQuestions and PathQuestions. We directly re-printed the results of baselines if they use the same datasets as ours. Otherwise, we implemented the baselines based on the codes and model parameters released by the original papers. We reported all the results of our implemented models with the mean values over 5 runs. 4.4 Automatic Evaluation We followed the existing work (Shimorina and Gardent, 2018; Chen et al., 2020d) to use BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004) as our automatic metrics. The main results on WebNLG, WebQuestions and PathQuestions are shown in Table 1. We can observe that JointGT based on BART / T5 can outperform vanilla BART / T5 on most of the metrics, respectively, and obtain the state-of-the-art performance on all the datasets. This indicates that our method can promote graph-text alignments and further enhance the performance of the state-of-theart pre-trained models on KG-to-text datasets. 4.5 Human Evaluation To further evaluate the quality of generated results, we condu"
2021.findings-acl.223,D19-1005,0,0.0242072,"re-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning textto-text pre-trained models on KG-to-text datasets. KG-Enhanced Pre-Trained Models Another line of related studies is pre-trained models enhanced by knowledge graphs for natural language understanding (NLU). The motivation of these models is to incorporate knowledge graphs into pre-trained models to facilitate the understanding of entities and relations in natural language. Early work including ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) directly uses fixed entity embeddings based on TransE (Bordes et al., 2013) or word vectors (Mikolov et al., 2013) during pre-training. Recent work like KEPLER (Wang et al., 2021) and JAKET (Yu et al., 2020) resorts to jointly pre-training graph-text representations. Specifically, they encode the textual descriptions of entities with pre-trained language models as entity embeddings and jointly optimize the knowledge embedding objective and the masked language modeling objective. In comparison, our model focuses on joint pretraining methods on knowledge graph encoding and sequence decoding in"
2021.findings-acl.223,2020.tacl-1.38,0,0.336959,"ata, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Dev"
2021.findings-acl.223,2020.emnlp-main.577,0,0.0368561,"Missing"
2021.findings-acl.223,P16-1056,0,0.0453243,"Missing"
2021.findings-acl.223,N18-2074,0,0.183227,"Graph-Text Embedding Alignment ? Figure 3: Overview of our proposed pre-training tasks: (a) Graph enhanced text reconstruction: reconstructing the text sequence given the complete graph. (b) Text enhanced graph reconstruction: predicting the masked entities and relations of the corrupted graph conditioned on the complete text. (c) Graph-text embedding alignment: matching the embedding vectors of the knowledge graph and the text via Optimal Transport. The special token &lt;SEP&gt; is to separate the linearized graph and the text, while &lt;M&gt; denotes the placeholder for masked tokens. attention layer (Shaw et al., 2018): z˜il = |V| X l l βij (zjl W V S + qij W V R) j=1 exp(ulij ) l βij = P|V| l p=1 exp(uip ) ulij = zil W QS  l W KR zjl W KS + qij √ dk (3) 3.3 &gt; i = 1, 2, · · · , |V| where W QS , W KS , W V S , W KR , W V R are the weight matrices in the structure-aware selfattention. This layer integrates the contextual semantic representation of entities and relations based on the graph structure, thereby injecting the structural information into the vanilla Transformer layer. Finally, we use a residual layer to fuse semantic and structural representations of entities, and obtain the hidden states for th"
2021.findings-acl.223,W18-6543,0,0.295351,"OUGE BLEU METEOR ROUGE † † ‡ 36.00 65.00 29.45 30.96‡ 55.45‡ 44.51 70.94 29.61 31.48 55.42 46.04 73.06 28.78 30.55 55.12 76.10** 58.55 75.91 61.01** 72.31 30.02* 73.57** 28.95 45.01 46.32** 32.05** 31.29 55.60 54.47 BLEU 61.48‡ 63.74 58.95 65.89** 60.45 PathQuestions METEOR ROUGE 44.57‡ 77.72‡ 47.23 77.76 44.72 76.58 48.25** 45.38 78.87** 77.59 Table 1: Results on WebNLG, WebQuestions and PathQuestions. SOTA-NPT indicates the state-of-the-art performance from the baselines without pre-training. #Param means the number of model parameters. The results marked with †, ‡ and ] are re-printed from Shimorina and Gardent (2018), Chen et al. (2020d) and Chen et al. (2020b), respectively. - means that the results are not reported in the corresponding references. * indicates that our model significantly outperforms BART and T5 on the corresponding datasets (t-test, p &lt; 0.05), while ** means p &lt; 0.01. Wikipedia hyperlinks of entities in the sentences. The detailed statistics of KGTEXT are shown in Table 2. Dataset KGTEXT WebNLG(U) WebNLG(C) WebQuestions PathQuestions #Ent #Rel 1.8M 3,114 3,129 25,703 7,250 1,210 373 373 672 378 #Instances #Triples Length (Train / Valid / Test) 6.98M / 10K / 10K 27.2 20.2 34,352 / 4,316"
2021.findings-acl.223,2020.acl-main.712,1,0.718322,"our proposed structureaware semantic aggregation module, we fixed the pre-training tasks and compared our encoder with two Transformer-based encoders commonly used in the existing work: SeqEnc: This sequence encoder takes linearized graphs as input and ignores structural information (Ribeiro et al., 2020a; Kale and Rastogi, 2020). RelEnc: This relation-aware encoder regards the entity sequence as input and leverages the relation embedding into the self-attention layer. Both the entity and relation embedding vectors are directly learned as model parameters (Shaw et al., 2018; Zhu et al., 2019; Song et al., 2020). 4.6.2 Pre-Training Task Model JointGT (BART) w/o TextRecon w/o GraphRecon w/o OT w/ BARTPretrain w/ KGPTPretrain BLEU METEOR ROUGE 65.92 47.15 76.10 64.22 46.56 74.96 65.37 47.09 75.97 65.03 47.09 75.83 64.60 46.78 75.74 65.14 46.94 75.72 Table 6: Ablation test of three pre-training tasks on WebNLG(U), including text / graph reconstruction and graph-text alignments via OT. BARTPretrain / KGPTPretrain means using the pre-training tasks of BART / KGPT instead of our tasks on KGTEXT. Model #Param BLEU METEOR ROUGE JointGT (BART) 160M 65.92 47.15 76.10 w/ SeqEnc 140M 64.82 46.87 75.37 160M 65.17"
2021.findings-acl.223,N18-1059,0,0.0186059,"lits: the traditional split (Unconstrained) which guarantees that there is no overlap of input graphs among train / validation / test sets, and a more challenging split (Constrained) where the non-overlap constraint is applied to the triples of input graphs. We denoted these two data splits as WebNLG(U) and WebNLG(C) in our paper. We followed the preprocessing steps of the existing work (Chen et al., 2020b) to replace the underlines in the entities and relations with spaces, and split the entities and relations in a camel case into multiple words. WebQuestions: This dataset (Yih et al., 2016; Talmor and Berant, 2018) is the benchmark for question generation over knowledge bases (KBQG), whose purpose is to generate natural language questions about the corresponding knowledge graphs (Serban et al., 2016). It is constructed from two question answering datasets, i.e., WebQuestionsSP (Yih et al., 2016) and ComplexWebQuestions (Talmor and Berant, 2018). These two datasets contain natural language questions, SPARQL queries and answer entities. We converted the SPARQL query to return a subgraph, and used the same preprocessing steps and data splits as the existing work (Kumar et al., 2019; Chen et al., 2020d). Pa"
2021.findings-acl.223,P18-1151,0,0.0238869,"layer, and utilizes three pre-training tasks to explicitly learn graph-text alignments in the discrete and continuous spaces. • We conduct experiments on the datasets of KG-to-text generation including WebNLG, WebQuestions and PathQuestions. Results show that JointGT achieves new state-of-theart performance on KG-to-text generation. 2 Related Work KG-to-Text Generation Recent studies on KG-to-text generation tasks mainly fall into three aspects: 1) Encoder modification: To alleviate the structural information loss of sequence encoders with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives to jointly learn the tasks of graph-to-text and textto-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-train"
2021.findings-acl.223,P16-2033,0,0.0165938,"o official data splits: the traditional split (Unconstrained) which guarantees that there is no overlap of input graphs among train / validation / test sets, and a more challenging split (Constrained) where the non-overlap constraint is applied to the triples of input graphs. We denoted these two data splits as WebNLG(U) and WebNLG(C) in our paper. We followed the preprocessing steps of the existing work (Chen et al., 2020b) to replace the underlines in the entities and relations with spaces, and split the entities and relations in a camel case into multiple words. WebQuestions: This dataset (Yih et al., 2016; Talmor and Berant, 2018) is the benchmark for question generation over knowledge bases (KBQG), whose purpose is to generate natural language questions about the corresponding knowledge graphs (Serban et al., 2016). It is constructed from two question answering datasets, i.e., WebQuestionsSP (Yih et al., 2016) and ComplexWebQuestions (Talmor and Berant, 2018). These two datasets contain natural language questions, SPARQL queries and answer entities. We converted the SPARQL query to return a subgraph, and used the same preprocessing steps and data splits as the existing work (Kumar et al., 201"
2021.findings-acl.223,P19-1139,0,0.0260899,"mpared with the existing work on pre-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning textto-text pre-trained models on KG-to-text datasets. KG-Enhanced Pre-Trained Models Another line of related studies is pre-trained models enhanced by knowledge graphs for natural language understanding (NLU). The motivation of these models is to incorporate knowledge graphs into pre-trained models to facilitate the understanding of entities and relations in natural language. Early work including ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) directly uses fixed entity embeddings based on TransE (Bordes et al., 2013) or word vectors (Mikolov et al., 2013) during pre-training. Recent work like KEPLER (Wang et al., 2021) and JAKET (Yu et al., 2020) resorts to jointly pre-training graph-text representations. Specifically, they encode the textual descriptions of entities with pre-trained language models as entity embeddings and jointly optimize the knowledge embedding objective and the masked language modeling objective. In comparison, our model focuses on joint pretraining methods on knowledge graph"
2021.findings-acl.223,2020.acl-main.224,0,0.0321197,"re-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new stateof-the-art performance on various KG-to-text datasets1 . 1 Introduction Knowledge-graph-to-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / rela"
2021.findings-acl.223,C18-1171,1,0.845721,"rious KG-to-text datasets1 . 1 Introduction Knowledge-graph-to-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences"
2021.findings-acl.223,D19-1548,0,0.0634937,"ated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Devlin et al., 2019), which include full attention connections. This model structure may neglect the structural information when encoding knowledge graphs since the relation between each pair of input entities is not explicitly considered (Zhu et al., 2019). 2) Absence of explicit graph-text alignments. Existing work on pre-trained models for text generation commonly adopts auto-encoding or auto-regressive text reconstruction to learn texttext alignments, which encodes the corrupted text sequence and decodes the original sequence (Lewis et al., 2020; Raffel et al., 2020). Since knowledge graphs may possess more complex structures than text sequences, it’s hard to explicitly learn graphtext alignments by directly using the pre-training tasks based on text reconstruction. Thus, we propose a graph-text joint represen2526 Findings of the Association"
2021.insights-1.9,2020.lrec-1.53,0,0.0227251,"the top of the tokens’ representations to predict BIO format tags. Semantic Parsing (SP) aims at identifying both intents and slots’ values in an utterance. We use TOP dataset (Gupta et al., 2018) that has 45K utterances spanning 25 intents and 36 slots and M ulti WOZ 2.3 dataset (Han et al., 2020) that has 10K dialogs and 143K utterances spanning 7 domains, 13 intents, and 25 slots. We use two linear layers to predict intent and tokens’ tags respectively. Dialog State Tracking (DST) is the task of recognizing user constraints throughout the conversation. We use MultiWOZ dataset version 2.1 (Eric et al., 2020) that has 30 domain-slot pairs to track. We adopt two BERT-based models: TripPy (Heck et al., 2020) and TOD - DST (Wu et al., 2020). Both models use BERT to encode dialog history. Dialog Act Prediction (DAP) is a multi-label sequence classification problem, where models predict the intents of the system response given the dialog history. We use two datasets: MultiWOZ and GSIM (Shah et al., 2018) that contains 6 intents and 3K dialogs. For each intent, we feed the representation of [CLS] token to a linear layer and predict whether the intent is in the response. As for evaluation metrics, we use"
2021.insights-1.9,E17-1042,0,0.0620316,"Missing"
2021.insights-1.9,2020.emnlp-main.66,0,0.26302,"ning MLM Help? An Empirical Study on Task-Oriented Dialog Pre-training Qi Zhu1 , Yuxian Gu1 , Lingxiao Luo1 , Bing Li1 , Cheng Li2 , Wei Peng2 , Xiaoyan Zhu1 , Minlie Huang1∗ 1 CoAI Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China 2 Artificial Intelligence Application Research Center, Huawei Technologies, Shenzhen, China zhu-q18@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn Abstract that DAPT masked LM leads to performance gains under both high- and low-resource settings and TAPT is beneficial with or without DAPT . DAPT has shown effectiveness for task-oriented dialog modeling. Wu et al. (2020) further pretrained BERT on 9 task-oriented dialog corpora and outperformed BERT on four downstream tasks, especially in the few-shot setting. Gu et al. (2020) further pre-trained GPT-2 on 13 dialog corpora ranging from chitchats to task-oriented dialogs, leading to better results on three task-oriented datasets. However, does further pre-training always help? Mehri et al. (2020) performed DAPT on 700M opendomain dialogs and TAPT, but the resulting model only outperforms BERT in 4 out of 7 task-oriented dialog datasets. We also observe that replacing BERT with TOD-BERT-mlm (Wu et al., 2020) th"
2021.insights-1.9,2020.acl-demos.19,1,0.841597,"nt et al. (2020), we encode samples from a test dataset and randomly select the same n = 5000 tokens as stimuli, whose contextual representations at each layer are used to compute an n × n pairwise cosine similarity matrix. The final similarity score between two models’ representations at a certain layer is computed as the Pearson correlation between the flattened upper triangular of the two similarity matrices. Evaluation We conduct comprehensive evaluations on 5 downstream tasks. Models on these tasks are adapted from TOD-BERT (Wu et al., 2020), DialoGLUE (Mehri et al., 2020), or ConvLab-2 (Zhu et al., 2020). See Appendix B for fine-tuning details. Intent Classification (IC) is a sequence classification problem, where models take an utterance as input and predict its intent. We use three datasets: HWU (Liu et al., 2019) that has 64 intents and 26K utterances, BANKING (Casanueva et al., 2020) that has 77 intents and 13K utterances, and OOS (Larson et al., 2019) that has 151 intents and 24K utterances. We pass the representation of [CLS] token to a linear layer for prediction. Slot Filling (SF) requires models to extract slots’ values in an utterance, which is often formulated 3 3.1 Empirical Analy"
2021.insights-1.9,2020.blackboxnlp-1.4,0,0.0220859,"ce respectively, we randomly pick a dialog D and sample a turn t ∈ [1, T ] uniformly, where T is the length of D. Then all the utterances are concatenated into a sequence as the model input: &quot;[CLS] [USR] U1 [SEP] [SYS] S1 [SEP] ... [USR] Ut [SEP]&quot;, where [USR] and [SYS] are two special tokens prepended to user’s and system’s utterances respectively. See Appendix A for the hyper-parameter setting. 2.2 2.3 Representational Similarity Analysis Representational similarity analysis (RSA) is a technique to measure the similarity between models’ representations (Laakso and Cottrell, 2000). Following Merchant et al. (2020), we encode samples from a test dataset and randomly select the same n = 5000 tokens as stimuli, whose contextual representations at each layer are used to compute an n × n pairwise cosine similarity matrix. The final similarity score between two models’ representations at a certain layer is computed as the Pearson correlation between the flattened upper triangular of the two similarity matrices. Evaluation We conduct comprehensive evaluations on 5 downstream tasks. Models on these tasks are adapted from TOD-BERT (Wu et al., 2020), DialoGLUE (Mehri et al., 2020), or ConvLab-2 (Zhu et al., 2020"
2021.insights-1.9,P17-1163,0,0.0623124,"Missing"
C08-1061,H05-1059,0,0.0159026,"e most probable label sequence yˆ for input sequence x . In the training set, we label all the noun words with semantic question categories, and other words will be labeled by “O”. We suppose Feature Sets Syntactic Features The questions, which have similar syntactic style, intend to belong to the same category. Besides words, part-of-speech, chunker, parser information and question length are used as syntactic features. All the words are lemmatized to root forms, and a window size (here is 4) is set to utilize the surrounding words. The part-of-speech (POS) tagging is completed by SS Tagger (Tsuruoka and Tsujii, 2005), with our own improvement. The noun phrase chunking (NP chunking) module uses the basic NP chunker software from 483 (Ramshaw and Marcus, 1995) to recognize the noun phrases in the question. The importance of question syntactic structure is reported in (Zhang and Lee, 2002; Nguyen et al. 2007). They used complex machine learning method to capture the tree architecture. The LDCRFs based approach just selects parent node, relation with parent and governor for each target word generated from Minipar(Lin, 1999). The length of question is another important syntactic feature. In our experiment, a t"
C08-1061,W95-0107,0,0.0534389,"other words will be labeled by “O”. We suppose Feature Sets Syntactic Features The questions, which have similar syntactic style, intend to belong to the same category. Besides words, part-of-speech, chunker, parser information and question length are used as syntactic features. All the words are lemmatized to root forms, and a window size (here is 4) is set to utilize the surrounding words. The part-of-speech (POS) tagging is completed by SS Tagger (Tsuruoka and Tsujii, 2005), with our own improvement. The noun phrase chunking (NP chunking) module uses the basic NP chunker software from 483 (Ramshaw and Marcus, 1995) to recognize the noun phrases in the question. The importance of question syntactic structure is reported in (Zhang and Lee, 2002; Nguyen et al. 2007). They used complex machine learning method to capture the tree architecture. The LDCRFs based approach just selects parent node, relation with parent and governor for each target word generated from Minipar(Lin, 1999). The length of question is another important syntactic feature. In our experiment, a threshold is set to denote the length as “high” or “low”. 4.2 between a hypernym h and category c j . It is defined as: χ 2 ( h, c j ) = ( A + B"
C08-1061,P05-1045,0,0.00516825,"Missing"
C08-1061,C02-1150,0,0.758401,"OC city, country, mountain, other, state NUM code, count, date, distance, money, order, other, percent, period, speed, temperature, size, weight Table 1. Question Ontology Introduction Question classification is a crucial component of modern question answering system. It classifies questions into several semantic categories which indicate the expected semantic type of answers to the questions. The semantic category helps to filter out irrelevant answer candidates, and determine the answer selection strategies. 1 The widely used question category criteria is a two-layered taxonomy developed by Li and Roth (2002) from UIUC. The hierarchy contains 6 coarse classes and 50 fine classes as shown in Table 1. In this paper, we focus on fine-category classification. Each fine category will be denoted as “Coarse:fine”, such as “HUM:individual”. A what-type question is defined as the one whose question word is “what”, “which”, “name” or “list”. It is a dominant type in question answering system. Li and Roth (2006) find © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. Coarse that the d"
C10-1014,W01-0513,0,\N,Missing
C10-1014,J90-1003,0,\N,Missing
C10-1014,P05-2003,0,\N,Missing
C10-1014,W03-0430,0,\N,Missing
C10-1014,P02-1054,0,\N,Missing
C10-1014,P05-1045,0,\N,Missing
C10-1074,J96-1002,0,0.0338033,"Feature Beginning CB Negative Beginning FI Feature Inside CI Negative Inside PB Positive Beginning N Negation Word PI Positive Inside O Other Table 1. Basic Tag Set for Review Mining 3.2 Structure Aware Model In this section, we describe how to encode different linguistic structure into model representation based on our CRFs framework. 3.2.1 Using Linear CRFs. For each sentence in a review, our task is to extract all the object features, positive opinions and negative opinions. This task can be modeled as a classification problem. Traditional classification tools, e.g. Maximum Entropy model (Berger et al, 1996), can be employed, where each word or phrase will be treated as an instance. However, they independently consider each word or phrase, and ignore the dependency relationship among them. Actually, the context information plays an important role for review mining. For example, given two continuous words with same part of speech, if the previous word is a positive opinion, the next word is more likely a positive opinion. Another example is that if the previous word is an adjective, and it is an opinion, the next noun word is more likely an object feature. To this end, we formulate the review mini"
C10-1074,H05-1045,0,0.00632958,"is a generative model, which is hard to integrate rich, overlapping features. It may encounter sparse data problem, especially when simultaneously integrating multiple features. Our framework is based on Conditional Random Fields (CRFs). CRFs is a discriminative model, which can easily integrate various features. These are some studies on opinion mining with Conditional Random Fields. For example, with CRFs, Zhao et al (2008) and McDonald et al. (2007) performed sentiment classification in sentence and document level; Breck et al (2007) identified opinion expressions from newswire documents; Choi et al. (2005) determined opinion holders to opinions also from newswire data. None of previous work focuses on jointly extracting object features, positive opinions and negative opinions simultaneously from review data. More importantly, we also show how to encode the linguistic structure, such as conjunction structure and syntactic tree structure, into model representation in our framework. This is significantly different from most of previous studies, which consider the structure information as heuristic rules (Hu and Liu, 2004) or input features (Wilson et al. 2009). Recently, there are some studies on"
C10-1074,esuli-sebastiani-2006-sentiwordnet,0,0.0325539,"ns in the review sentence, with a collected conjunction set, including “and”, “but”, “or”, “however”, “although” etc. For each conjunction, we extract its connected two text sequences. The nearest two words with same part of speech from the two text sequences are connected with the skip-edge. Here, we just consider the noun, adjective, and adverb. For example, in “good pictures and beautiful music”, there are two skip-edges: one connects two adjective words “good” and “beautiful”; the other connects two nouns “pictures” and “music”. We also employ the general sentiment lexicons, SentiWordNet (Esuli and Sebastiani, 2006), to connect opinions. Two nearest opinion words, detected by sentiment lexicon, from two sequences, will also be connected by skip-edge. If the nearest distance exceeds the threshold, this skip edge will be discarded. Here, we consider the threshold as nine. Skip-chain CRFs improve the performance of review mining, because it naturally encodes the conjunction structure into model representation with skip-edges. 3.2.3 Leveraging Syntactic Tree Structure Besides the conjunction structure, the syntactic tree structure also helps for review mining. The tree denotes the syntactic relationship amon"
C10-1074,P97-1023,0,0.0166859,"tinuous words, as discussed above. It views each word in the sentence as a node, and adjacent nodes are connected by an edge. The graphical representation is shown in Figure 2(a). Linear CRFs can make use of dependency relationship among adjacent words. 3.2.2 Leveraging Conjunction Structure We observe that the conjunctions play important roles on review mining: If the words or phrases are connected by conjunction “and”, they mostly belong to the same opinion polarity. If the words or phrases are connected by conjunction “but”, they mostly belong to different opinion polarity, as reported in (Hatzivassiloglou and McKeown, 1997; Ding and Liu, 2007). For example, “This phone has a very cool and useful feature – the speakerphone”, if we only detect “cool”, it is hard to determine its opinion polarity. But if we see “cool” is connected with “useful” by conjunction “and”, we can easily acquire the polarity of “cool” as positive. This conjunction structure not only helps to determine the opinions, but also helps to recognize object features. For example, “I like the special effects and music in this movie”, with word “music” and conjunction “and”, we can easily detect that “special effects” as an object feature. To model"
C10-1074,H05-1043,0,0.90566,"ject (movie) features, such as “movie”, “actor”, with their corresponding positive opinions and negative opinions, are listed in a structured way. The opinions are ranked by their frequencies. This provides a concise view for reviews. To accomplish this goal, we need to do three tasks: 1), extract all the object features and opinions; 2), determine the sentiment polarities for opinions; 3), for each object feature, determine the relevant opinions, i.e. object feature-opinion pairs. For the first two tasks, most previous studies employ linguistic rules or statistical methods (Hu and Liu, 2004; Popescu and Etzioni 2005). They mainly use unsupervised learning methods, which lack an effective way to address infrequent object features and opinions. They are also hard to incorporate rich overlapping features. 653 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653–661, Beijing, August 2010 Actually, there are many useful features, which have not been fully exploited for review mining. Meanwhile, most of previous methods extract object features, opinions, and determine the polarities for opinions separately. In fact, the object features, positive opinions and neg"
C10-1074,N07-1038,0,0.0084283,"ious work focuses on jointly extracting object features, positive opinions and negative opinions simultaneously from review data. More importantly, we also show how to encode the linguistic structure, such as conjunction structure and syntactic tree structure, into model representation in our framework. This is significantly different from most of previous studies, which consider the structure information as heuristic rules (Hu and Liu, 2004) or input features (Wilson et al. 2009). Recently, there are some studies on joint sentiment/topic extraction (Mei et al. 2007; Titov and McDonald, 2008; Snyder and Barzilay, 2007). These methods represent reviews as several coarse-grained topics, which can be considered as clusters of object features. They are hard to indentify the low-frequency object features and opinions. While in this paper, we will extract all the present object features and corresponding opinions with their polarities. Besides, the joint sentiment/topic methods are mainly based on review document for topic extraction. In our framework, we focus on sentence-level review extraction. 3 3.1 Structure Aware Review Mining Problem Definition To produce review summaries, we need to first finish two tasks"
C10-1074,P02-1053,0,0.0155192,"Missing"
C10-1074,J09-3003,0,0.168825,"rk can naturally encode the linguistic structure. Besides the neighbor context with linear-chain CRFs, we propose to use Skip-chain CRFs and Tree CRFs to utilize the conjunction structure and syntactic tree structure. We also propose a new unified model, Skip-Tree CRFs to integrate these structures. Here, “structure-aware” refers to the output structure, which model the relationship among output labels. This is significantly different from the previous input structure methods, which consider the linguistic structure as heuristic rules (Ding and Liu, 2007) or input features for classification (Wilson et al. 2009). Our proposed framework has the following advantages: First, it can employ rich features for review mining. We will analyze the effect of features for review mining in this framework. Second, the framework can utilize the relationship among object features, positive opinions and negative opinions. It jointly extracts these three types of expressions in a unified way. Third, the linguistic structure information can be naturally integrated into model representation, which provides more semantic dependency for output labels. Through extensive experiments on movie review and product review, we sh"
C10-1074,P07-1055,0,0.0181301,"and negative opinions, in a unified framework. Recently, Jin and Ho (2009) propose to use Lexicalized HMM for review mining. Lexicalized HMM is a variant of HMM. It is a generative model, which is hard to integrate rich, overlapping features. It may encounter sparse data problem, especially when simultaneously integrating multiple features. Our framework is based on Conditional Random Fields (CRFs). CRFs is a discriminative model, which can easily integrate various features. These are some studies on opinion mining with Conditional Random Fields. For example, with CRFs, Zhao et al (2008) and McDonald et al. (2007) performed sentiment classification in sentence and document level; Breck et al (2007) identified opinion expressions from newswire documents; Choi et al. (2005) determined opinion holders to opinions also from newswire data. None of previous work focuses on jointly extracting object features, positive opinions and negative opinions simultaneously from review data. More importantly, we also show how to encode the linguistic structure, such as conjunction structure and syntactic tree structure, into model representation in our framework. This is significantly different from most of previous stu"
C10-1074,D08-1013,0,\N,Missing
C10-1074,H05-2017,0,\N,Missing
C10-1074,P08-1036,0,\N,Missing
C10-2060,W04-3247,0,0.0527343,"experiment, only bigrams are used as concepts. The selection problem requires systems improve diversity or remove redundancy so that more relevant information can be covered by the summary as its length is limited. As our paper focuses on extractive summarization, the selection problem refers to selecting sentences. However, the selection framework presented here is universal for selecting arbitrary textual units, as discussed in Section 4. There have been a variety of studies to approach the ranking problem. These include both unsupervised sentence ranking (Luhn, 1958; Radev and Jing, 2004, Erkan and Radev, 2004), and supervised methods (Ouyang et al., 2007; Shen et al., 2007; Li et al., 2009). Even given a list of ranked sentences, it is not trivial to select a subset of sentences to form a good summary which includes diverse information within a length limit. Three common selection strategies have been studied to address this problem: Maximum Marginal Relevance (MMR) (Carbonell and Goldstein, 1998), Diversity Penalty (DiP) (Wan, 2007), and integer linear programming (ILP) (McDonald, 2007; Gillick and Favre, 2009). As different methods were often evaluated on different datasets, it is of great value"
C10-2060,W04-1017,0,0.0496688,"and TextRank (Mihalcea and Tarau, 2004). There are also a variety of studies on supervised learning methods for sentence ranking and selection. Kupiec et al. (1995) developed a naive Bayes classifier to decide whether a sentence is worthy to extract. Recently, Conditional Random Field (CRF) and Structural SVM have been employed for single document summarization (Shen et al., 2007; Li et al., 2009). Besides ranking sentences directly, there are some approaches that select sentences based on 526 concept ranking. Radev et al. (2004) used centroid words whose tf*idf scores are above a threshold. Filatova and Hatzivassiloglou (2004) used atomic event as concept. Moreover, summarization evaluation metrics such as Basic Element (Hovy et al., 2006), ROUGE (Lin and Hovy, 2003) and Pyramid (Passonneau et al., 2005) are all counting the concept overlap between generated summaries and human-written summaries. Another important issue existing in extractive summarization is to find an optimal sentence subset which can cover diverse information. Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and Diversity Penalty (Wan, 2007) are most widely used approaches to reduce redundancy. The two methods are essentially bas"
C10-2060,W09-1802,0,0.476772,"Marginal Relevance and Diversity Penalty strategies. 1 Introduction As the rapid development of the Internet, document summarization has become an important task since document collections are growing larger and larger. Document summarization, which aims at producing a condensed version of the original document(s), helps users to acquire information that is both important and relevant to their information need. So far, researchers have mainly focused on extractive methods which choose a set of salient textual units to form a summary. Such textual units are typically sentences, sub-sentences (Gillick and Favre, 2009), or excerpts (Sauper and Barzilay, 2009). Almost all extractive summarization methods face two key problems: the first problem is how to rank textual units, and the second one is how to select a subset of those ranked units. The ranking problem requires systems model the relevance of a textual unit to a topic or a query. In this paper, the ranking problem refers to either sentence ranking or concept ranking. Concepts can be unigrams, bigrams, semantic content units, etc., although in our experiment, only bigrams are used as concepts. The selection problem requires systems improve diversity or"
C10-2060,hovy-etal-2006-automated,0,0.015235,"selection. Kupiec et al. (1995) developed a naive Bayes classifier to decide whether a sentence is worthy to extract. Recently, Conditional Random Field (CRF) and Structural SVM have been employed for single document summarization (Shen et al., 2007; Li et al., 2009). Besides ranking sentences directly, there are some approaches that select sentences based on 526 concept ranking. Radev et al. (2004) used centroid words whose tf*idf scores are above a threshold. Filatova and Hatzivassiloglou (2004) used atomic event as concept. Moreover, summarization evaluation metrics such as Basic Element (Hovy et al., 2006), ROUGE (Lin and Hovy, 2003) and Pyramid (Passonneau et al., 2005) are all counting the concept overlap between generated summaries and human-written summaries. Another important issue existing in extractive summarization is to find an optimal sentence subset which can cover diverse information. Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and Diversity Penalty (Wan, 2007) are most widely used approaches to reduce redundancy. The two methods are essentially based on greedy search. By contrast, ILP based approaches view summary generation as a global optimization problem. Mc"
C10-2060,N03-1020,0,0.302308,"1995) developed a naive Bayes classifier to decide whether a sentence is worthy to extract. Recently, Conditional Random Field (CRF) and Structural SVM have been employed for single document summarization (Shen et al., 2007; Li et al., 2009). Besides ranking sentences directly, there are some approaches that select sentences based on 526 concept ranking. Radev et al. (2004) used centroid words whose tf*idf scores are above a threshold. Filatova and Hatzivassiloglou (2004) used atomic event as concept. Moreover, summarization evaluation metrics such as Basic Element (Hovy et al., 2006), ROUGE (Lin and Hovy, 2003) and Pyramid (Passonneau et al., 2005) are all counting the concept overlap between generated summaries and human-written summaries. Another important issue existing in extractive summarization is to find an optimal sentence subset which can cover diverse information. Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and Diversity Penalty (Wan, 2007) are most widely used approaches to reduce redundancy. The two methods are essentially based on greedy search. By contrast, ILP based approaches view summary generation as a global optimization problem. McDonald (2007) proposed a sen"
C10-2060,W09-1801,0,0.0221996,"Goldstein, 1998) and Diversity Penalty (Wan, 2007) are most widely used approaches to reduce redundancy. The two methods are essentially based on greedy search. By contrast, ILP based approaches view summary generation as a global optimization problem. McDonald (2007) proposed a sentence-level ILP solution. Sauper and Barzilay (2009) presented an excerpt-level ILP method to generate Wikipedia articles. Gillick and Favre (2009) proposed a concept-level ILP, but they used document frequency to score concepts (bigrams), without any learning process. Some recent studies (Gillick and Favre, 2009; Martins and Smith, 2009) also modeled sentence selection and compression jointly using ILP. Our ILP framework proposed here is based on these studies. Although various selection strategies have been proposed, there is no work to systematically compare these strategies yet. Learning to rank attracts much attention in the information retrieval community recently. Pointwise, pairwise and listwise learning-torank approaches have been extensively studied (Liu, 2009). Some of those have been applied to document summarization, such as SVR (Ouyang et al., 2007), classification SVM (Wang et al., 2007), and RankNet (Svore et a"
C10-2060,P09-1024,0,0.0583753,"ty strategies. 1 Introduction As the rapid development of the Internet, document summarization has become an important task since document collections are growing larger and larger. Document summarization, which aims at producing a condensed version of the original document(s), helps users to acquire information that is both important and relevant to their information need. So far, researchers have mainly focused on extractive methods which choose a set of salient textual units to form a summary. Such textual units are typically sentences, sub-sentences (Gillick and Favre, 2009), or excerpts (Sauper and Barzilay, 2009). Almost all extractive summarization methods face two key problems: the first problem is how to rank textual units, and the second one is how to select a subset of those ranked units. The ranking problem requires systems model the relevance of a textual unit to a topic or a query. In this paper, the ranking problem refers to either sentence ranking or concept ranking. Concepts can be unigrams, bigrams, semantic content units, etc., although in our experiment, only bigrams are used as concepts. The selection problem requires systems improve diversity or remove redundancy so that more relevant"
C10-2060,D07-1047,0,0.056092,"ith, 2009) also modeled sentence selection and compression jointly using ILP. Our ILP framework proposed here is based on these studies. Although various selection strategies have been proposed, there is no work to systematically compare these strategies yet. Learning to rank attracts much attention in the information retrieval community recently. Pointwise, pairwise and listwise learning-torank approaches have been extensively studied (Liu, 2009). Some of those have been applied to document summarization, such as SVR (Ouyang et al., 2007), classification SVM (Wang et al., 2007), and RankNet (Svore et al., 2007). Again, there is no work to systematically compare these ranking algorithms. To the best of our knowledge, this is the first time that a listwise learning-to-rank algorithm, ListNet (Cao et al., 2007), is adapted to document summarization in this paper. Moreover, pairwise and listwise learning-to-rank algorithms have never been used to perform concept ranking for extractive summarization. 3 Ranking Sentences or Concepts Given a query and a collection of relevant documents, an extractive summarization system is required to generate a summary consisting of a set of text units (usually sentences"
C10-2060,E09-1089,0,0.00336336,"∑ exp( f j =1 w ( x j )) (8) ∂f w ( x j ) ∂w During training, w is updated in a gradient descent manner: w=w -η∆w and η is the learning rate. For details, refer to (Cao et al., 2007). 4 ∑ z * |u u j s.t. ILP-based Selection Framework After we have a way of ranking sentences or concepts, we face a sentence selection problem: selecting an optimal subset of sentences as the final summary. To integrate sentence/concept ranking, we adopted an integer linear programming (ILP) framework to find the optimal sentence subset (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Gillick and Favre, 2009; Takamura and Okumura, 2009). ILP is a global 528 j |≤ Lim j ∑z (5) It is easy to prove that (Ps(1), Ps(2), …, Ps(n)) is a probability distribution, as the sum of them equals to 1. Therefore, the cross entropy can be used to define the loss between the gold standard distribution Py(j) and the distribution Pf(j), as follows: Δw = optimization problem whose objective and constraints are linear in a set of integer variables. Formally, we define the problem of sentence selection as follows: ⎧ ⎫ (9) maximize: ⎨∑ f ( xi )* zix ⎬ ⎩ i ⎭ u j * I (i, j ) ≥ zix , ∀i j ( z + z xj ) * sim( xi , x j ) < δ ∀i, j x i zix , z uj ∈ {0,1},"
C10-2060,W04-3252,0,\N,Missing
C10-2088,de-marneffe-etal-2006-generating,0,0.0164696,"Missing"
C10-2088,W04-3253,0,0.0360036,"rating of the service. Our approach is therefore verified to be a successful step towards accurate feature rating estimation. 2 Related Work Our work aims at estimating feature ratings of a service based on its textual reviews. It is related to sentiment classification. The task of sentiment classification is to determine the semantic orientations of words, sentences or documents. (Pang et al., 2002) is the earliest work of automatic sentiment classification at document level, using several machine learning approaches with common textual features to classify movie reviews. Mullen and Collier (Mullen and Collier, 2004) integrated PMI values, Osgood semantic factors and some syntactic relations into the features of SVM. Pang and Lee (Pang and Lee, 2004) proposed another machine learning method based on subjectivity detection and minimum-cut in graph. However, these approaches focus only on binary classification of reviews. In 2005, Pang and Lee extended their earlier work in (Pang and Lee, 2004) to determine a reviewer’s evaluation with respect to multiscales (Pang and Lee, 2005). The rating estimation is viewed as multi-class sentiment categorization on documents. They used SVM regression as the multi-class"
C10-2088,P04-1035,0,0.0132875,"ur work aims at estimating feature ratings of a service based on its textual reviews. It is related to sentiment classification. The task of sentiment classification is to determine the semantic orientations of words, sentences or documents. (Pang et al., 2002) is the earliest work of automatic sentiment classification at document level, using several machine learning approaches with common textual features to classify movie reviews. Mullen and Collier (Mullen and Collier, 2004) integrated PMI values, Osgood semantic factors and some syntactic relations into the features of SVM. Pang and Lee (Pang and Lee, 2004) proposed another machine learning method based on subjectivity detection and minimum-cut in graph. However, these approaches focus only on binary classification of reviews. In 2005, Pang and Lee extended their earlier work in (Pang and Lee, 2004) to determine a reviewer’s evaluation with respect to multiscales (Pang and Lee, 2005). The rating estimation is viewed as multi-class sentiment categorization on documents. They used SVM regression as the multi-class classifier, and also applied a meta-algorithm based on a metric labeling formulation of the problem, which alters a given n-ary classif"
C10-2088,P05-1015,0,0.38481,"rate results than that for other reviews. This statistical evidence also allows us to use the average of the estimated feature ratings to better represent an overall opinion of experts for the feature of the service, which will be particularly useful for assisting other users to correctly make their consumption decisions. We verify our approach and arguments based on real data collected from the TripAdvisor website. First, our approach is shown to be able to effectively select reviews that comprehensively talk about features of a service. We then adopt the machine learning method proposed in (Pang and Lee, 2005) and the Bayesian Network classifier (Russell and Norvig, 2002) for feature rating estimation. Our experimental results show that the accuracy of estimating feature ratings for these selected reviews is higher than that for other reviews, for both the machine learning 766 Coling 2010: Poster Volume, pages 766–774, Beijing, August 2010 methods. And, the average of these estimated ratings is testified to closely represent the overall feature rating of the service. Our approach is therefore verified to be a successful step towards accurate feature rating estimation. 2 Related Work Our work aims a"
C10-2088,W02-1011,0,0.0141748,"reviews, for both the machine learning 766 Coling 2010: Poster Volume, pages 766–774, Beijing, August 2010 methods. And, the average of these estimated ratings is testified to closely represent the overall feature rating of the service. Our approach is therefore verified to be a successful step towards accurate feature rating estimation. 2 Related Work Our work aims at estimating feature ratings of a service based on its textual reviews. It is related to sentiment classification. The task of sentiment classification is to determine the semantic orientations of words, sentences or documents. (Pang et al., 2002) is the earliest work of automatic sentiment classification at document level, using several machine learning approaches with common textual features to classify movie reviews. Mullen and Collier (Mullen and Collier, 2004) integrated PMI values, Osgood semantic factors and some syntactic relations into the features of SVM. Pang and Lee (Pang and Lee, 2004) proposed another machine learning method based on subjectivity detection and minimum-cut in graph. However, these approaches focus only on binary classification of reviews. In 2005, Pang and Lee extended their earlier work in (Pang and Lee,"
C16-1062,D15-1038,0,0.279837,"r of different entities and relations. The rest of this paper are organized as follows. In Section 2, we introduce some related works. In Section 3, we detail the proposed method of graph aware knowledge embedding. Section 4 describes the data and presents experimental results to validate our method. Section 5 concludes the paper. 642 Table 1: A summary of different knowledge embedding methods. Method NTN(Socher et al., 2013) TransE(Bordes et al., 2013) TransH(Wang et al., 2014) TransR(Lin et al., 2015b) TransD(Ji et al., 2015) TranSparse(Ji et al., 2016) PTransE(Lin et al., 2015a) Traversing(Gu et al., 2015) GAKE(ours) 2 Triple X X X X X X X X X Path × × × × × × X X X Edge × × × × × × × × X Related Work In this section, we review some existing work relevant to our paper. Generally, our work is closely related to the following two topics: (1) knowledge base embedding (2) Graph embedding. 2.1 Knowledge Base Embedding A variety of approaches have been explored for knowledge base embedding, such as general linear based models, such as SE (Bordes et al., 2011), bilinear based models, like LFM (Jenatton et al., 2012; Sutskever et al., 2009), neural network based models, like SLM (Socher et al., 2013),"
C16-1062,P15-1009,0,0.0146091,"there are still some works(Xiao et al., 2016b; Xiao et al., 2016a) follow the principle h + r ≈ t, although they do not share the same form of score function. Particularly, (Xiao et al., 2016b) proposes to use a generative model to deal with multiple semantic meanings of a relation. To accommondate more flexible knowledge embedding, (Xiao et al., 2016a) proposes a manifold principle instead of a point-wise estimation of entity and relation embeddings. There are some other works incorporate additional information, such as text(Toutanova and Chen, 2015; Toutanova et al., 2015) and entity types(Guo et al., 2015). Above knowledge base embedding models all treat the knowledge base as a set of triples. However, in fact, knowledge base is a graph with its graph structure which can be used to better embed the entities and relations in knowledge base. Although (Gu et al., 2015) and PTransE(Lin et al., 2015a) introduce the relation path instead of only considering the direct relations between entities, they just treat the relation path as a new relation and the path length is limited to the model complexity. However, (Feng et al., 2016) claims the principle h + r ≈ t is too strict to model the complex and d"
C16-1062,P15-1067,0,0.672745,"edding models. 1 Introduction Knowledge bases, such as DBpedia, YAGO, and Freebase, are important resources to store complex structured facts about the real world in the form of triplets as (head entity, relation, tail entity). These knowledge bases have benefited many applications, such as web search and question answer. In the meanwhile, knowledge base embedding, which aims to learn a D-dimensional vector for each subject (i.e., an entity or a relation) in a given knowledge base, has attracted considerable research efforts recently (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015). For instance, TransE method (Bordes et al., 2013) regards the relation in a triplet as a translation between the embedding of the two entities. In other words, TransE learns a preference of h + r = t for each triple, where h, r, and t are the representation vector of head entity, relation, and tail entity respectively. Similar ideas are also proposed in TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransSparse (Ji et al., 2016), etc. Despite the success of above methods in learning knowledge representations, most of them mainly consider knowledge base as a set of triples and models"
C16-1062,D15-1082,0,0.609577,"f-art knowledge embedding models. 1 Introduction Knowledge bases, such as DBpedia, YAGO, and Freebase, are important resources to store complex structured facts about the real world in the form of triplets as (head entity, relation, tail entity). These knowledge bases have benefited many applications, such as web search and question answer. In the meanwhile, knowledge base embedding, which aims to learn a D-dimensional vector for each subject (i.e., an entity or a relation) in a given knowledge base, has attracted considerable research efforts recently (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015). For instance, TransE method (Bordes et al., 2013) regards the relation in a triplet as a translation between the embedding of the two entities. In other words, TransE learns a preference of h + r = t for each triple, where h, r, and t are the representation vector of head entity, relation, and tail entity respectively. Similar ideas are also proposed in TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransSparse (Ji et al., 2016), etc. Despite the success of above methods in learning knowledge representations, most of them mainly consider knowledge base as a set of"
C16-1062,D15-1161,0,0.0208951,"ily. 3.3 Attention Mechanism So far, the translation of a graph context, π(·), takes the embedding results of each subject contained in the context equally. However, in reality, different subjects may have different power of influence to represent the target subject. As an example shown in Figure 1, in edge context, “SingSong” relation is more unique and preventative than “Nationality” as only few people like singers will connect with this “SingSong” while everyone has “Nationality”. In this work, we model representative powers of different subjects in graph context by an attention mechanism (Ling et al., 2015; Hermann et al., 2015). The basic idea of the attention mechanism is using an attention model a(si ) to represent how subject si selectively focuses on representing another subject sj when si is a part of sj ’s context (Kelvin Xu, 2015). In this work, we define the attention model a(si ) as exp(θi ) sj ∈C(si ) exp(θj ) a(si ) = P (6) where θ is the parameters we aim to estimate. Figure 2 illustrates the attention for a path context when predicting the entity “English”, where darker color indicates a greater attention. We see that entities like “Washington” and relations like “LocateInCountry”"
C16-1062,W15-4007,0,0.116253,"ned by the number of entities linked by the relations. In addition, there are still some works(Xiao et al., 2016b; Xiao et al., 2016a) follow the principle h + r ≈ t, although they do not share the same form of score function. Particularly, (Xiao et al., 2016b) proposes to use a generative model to deal with multiple semantic meanings of a relation. To accommondate more flexible knowledge embedding, (Xiao et al., 2016a) proposes a manifold principle instead of a point-wise estimation of entity and relation embeddings. There are some other works incorporate additional information, such as text(Toutanova and Chen, 2015; Toutanova et al., 2015) and entity types(Guo et al., 2015). Above knowledge base embedding models all treat the knowledge base as a set of triples. However, in fact, knowledge base is a graph with its graph structure which can be used to better embed the entities and relations in knowledge base. Although (Gu et al., 2015) and PTransE(Lin et al., 2015a) introduce the relation path instead of only considering the direct relations between entities, they just treat the relation path as a new relation and the path length is limited to the model complexity. However, (Feng et al., 2016) claims the"
C16-1062,D15-1174,0,0.0613821,"ies linked by the relations. In addition, there are still some works(Xiao et al., 2016b; Xiao et al., 2016a) follow the principle h + r ≈ t, although they do not share the same form of score function. Particularly, (Xiao et al., 2016b) proposes to use a generative model to deal with multiple semantic meanings of a relation. To accommondate more flexible knowledge embedding, (Xiao et al., 2016a) proposes a manifold principle instead of a point-wise estimation of entity and relation embeddings. There are some other works incorporate additional information, such as text(Toutanova and Chen, 2015; Toutanova et al., 2015) and entity types(Guo et al., 2015). Above knowledge base embedding models all treat the knowledge base as a set of triples. However, in fact, knowledge base is a graph with its graph structure which can be used to better embed the entities and relations in knowledge base. Although (Gu et al., 2015) and PTransE(Lin et al., 2015a) introduce the relation path instead of only considering the direct relations between entities, they just treat the relation path as a new relation and the path length is limited to the model complexity. However, (Feng et al., 2016) claims the principle h + r ≈ t is to"
C16-1062,P16-1219,1,0.589397,"are projected into a relationspecific hyperplane wr , say hr = h − wr&gt; hwr , tr = t − wr&gt; twr . In TransR (Lin et al., 2015b), hr = hMr , tr = tMr , where entities are projected from the entity space to the relation space by Mr . In TransD (Ji et al., 2015), hr = Mrh h, tr = Mrt t, where the mapping matrices Mrh and Mrt are both related to the entity and relation. In TransSparse (Ji et al., 2016), hr = Mr (θr )h , tr = Mr (θr )t, where Mr is an adaptive sparse matrix, whose sparse degrees are determined by the number of entities linked by the relations. In addition, there are still some works(Xiao et al., 2016b; Xiao et al., 2016a) follow the principle h + r ≈ t, although they do not share the same form of score function. Particularly, (Xiao et al., 2016b) proposes to use a generative model to deal with multiple semantic meanings of a relation. To accommondate more flexible knowledge embedding, (Xiao et al., 2016a) proposes a manifold principle instead of a point-wise estimation of entity and relation embeddings. There are some other works incorporate additional information, such as text(Toutanova and Chen, 2015; Toutanova et al., 2015) and entity types(Guo et al., 2015). Above knowledge base embed"
C16-1106,P15-1153,0,0.0487759,"them make the ﬁnal decision. However, the vast availability of such reviews becomes overwhelming to users when there is just too much to digest. Product review summarization is the task to address this problem. It summarizes the large number of reviews and generates a short readable summary which contains the overall rating of the opinions in the reviews. Traditional extractive summarization has been studied for a long time, such as (Hovy and Lin, 1999; Kupiec et al., 1995; Paice, 1990). Recently, there are also a number of studies on abstractive summarization, such as (Banerjee et al., 2015; Bing et al., 2015; Liu et al., 2015). However, applying traditional summarization methods directly on product reviews doesn’t yield satisfying results. This is due to that product review summarization is quite different from traditional extractive summarization. From the perspective of data size, the number of reviews of a product is often much larger than that of traditional data such as news articles. Another important difference is that sentences in product reviews are usually colloquial and contain lots of noises. Directly extractive summaries may contain a large number of undesired information. A number o"
C16-1106,W14-4408,0,0.225569,"Missing"
C16-1106,C12-1047,0,0.0603627,"Missing"
C16-1106,C12-1056,0,0.0590086,"Missing"
C16-1106,C10-1039,0,0.401842,"on methods directly on product reviews doesn’t yield satisfying results. This is due to that product review summarization is quite different from traditional extractive summarization. From the perspective of data size, the number of reviews of a product is often much larger than that of traditional data such as news articles. Another important difference is that sentences in product reviews are usually colloquial and contain lots of noises. Directly extractive summaries may contain a large number of undesired information. A number of researchers have studied the task of review summarization. (Ganesan et al., 2010) proposed a graph-based method for generating ultra concise opinion summaries of products. They used predeﬁned rules for ﬁnding valid sub-paths in the graph and converted those sub-paths into sentences. Since the sentence generation was rule-based, their method didn’t provide a well-formed grammatical summary. (Gerani et al., 2014) generated product review summaries by using discourse structure. After simplifying the discourse graph, they used a template-based NLG framework to generate natural language summaries. Their summary produced a statistical overview of the product but lacked detailed"
C16-1106,D14-1168,0,0.314393,"r important difference is that sentences in product reviews are usually colloquial and contain lots of noises. Directly extractive summaries may contain a large number of undesired information. A number of researchers have studied the task of review summarization. (Ganesan et al., 2010) proposed a graph-based method for generating ultra concise opinion summaries of products. They used predeﬁned rules for ﬁnding valid sub-paths in the graph and converted those sub-paths into sentences. Since the sentence generation was rule-based, their method didn’t provide a well-formed grammatical summary. (Gerani et al., 2014) generated product review summaries by using discourse structure. After simplifying the discourse graph, they used a template-based NLG framework to generate natural language summaries. Their summary produced a statistical overview of the product but lacked detailed information. (Ganesan et al., 2012) proposed some heuristic rules to generate phrases, they used a modiﬁed mutual information function and an n-gram language model to ensure the representativeness and readability of the phrases. However, their method didn’t consider the descriptiveness of the phrases. This work is licensed under a"
C16-1106,E09-1059,0,0.385012,"Missing"
C16-1106,C10-1074,1,0.937337,"Missing"
C16-1106,N15-1114,0,0.0132115,"decision. However, the vast availability of such reviews becomes overwhelming to users when there is just too much to digest. Product review summarization is the task to address this problem. It summarizes the large number of reviews and generates a short readable summary which contains the overall rating of the opinions in the reviews. Traditional extractive summarization has been studied for a long time, such as (Hovy and Lin, 1999; Kupiec et al., 1995; Paice, 1990). Recently, there are also a number of studies on abstractive summarization, such as (Banerjee et al., 2015; Bing et al., 2015; Liu et al., 2015). However, applying traditional summarization methods directly on product reviews doesn’t yield satisfying results. This is due to that product review summarization is quite different from traditional extractive summarization. From the perspective of data size, the number of reviews of a product is often much larger than that of traditional data such as news articles. Another important difference is that sentences in product reviews are usually colloquial and contain lots of noises. Directly extractive summaries may contain a large number of undesired information. A number of researchers have"
C16-1106,P04-1035,0,0.0479276,"Missing"
C16-1106,W02-1011,0,0.0218297,"Missing"
C16-1106,N07-1038,0,0.109955,"Missing"
C16-1106,P08-1036,0,0.0297951,"04b) used an effective method based on WordNet. (Ku et al., 2006) also used a set of positive and negative words from GI and CNSD to predict sentiments of aspects. (Zhuang et al., 2006) used dependency relationships to identify opinions associated with feature words. Summary generation involves aggregating the results of aspect extraction and sentiment detection and generate the ﬁnal opinion summary in an effective and easy to understand format. Statistical summary 1121 is the most commonly adopted format, such as (Hu and Liu, 2004a; Hu and Liu, 2004b; Hu and Liu, 2006; Zhuang et al., 2006). (Titov and McDonald, 2008b) used a topic modeling method to provide a word level summary for each topic. (Popescu and Etzioni, 2007) also provided a word level summary by ranking opinion words associated to features and showing the strongest opinionated word for each aspect. (Mei et al., 2007) scored the probability of each sentence using TSM model and generated a sentence level summary. (Ku et al., 2006) used TF-IDF to score sentences and select the most relevant and discriminative sentence to be shown as summary. Besides texts, aggregated ratings can also be shown for summary, such as (Lu et al., 2009). (Ku et al.,"
C16-1106,C14-1157,0,0.0592707,"Missing"
C16-1106,D15-1012,0,0.0559966,"Missing"
C16-1191,P06-4018,0,0.00511708,"s. The act types and slot-value pairs are labeled in the dataset. The details about the dialogue act are provided in Table 1. The training, validation and testing set are partitioned in the ratio of 3:1:1. And upsampling w.r.t act type is applied to make the corpus more uniform similar to (Wen et al., 2015). 4.2 Implementation Details We use Theano (Bergstra et al., 2010) to implement the proposed model. For each dialogue act and input question, we generate 20 responses and select the top 5 responses as the output after reranking. The BLEU-4 metric (Papineni et al., 2002) implemented by NLTK (Bird, 2006) is used for quantitative evaluation. And the references set of the BLEU-4 metric are built by grouping the references of the same dialogue acts after delexicalising the responses and lexicalizing them by the correct values. Since the performance of CA-LSTM depends on initialisation, the results shown below are averaged over 5 randomly initialised CA-LSTM and the corpus are partitioned after random shuffle as well. 4.3 Quantitative Evaluation We compare our proposed model with several baselines including: the handcrafted generator (hdc), k-nearest neighbour (kNN), class-based LMs (classlm) as"
C16-1191,D16-1127,0,0.0494517,"open domain dialogue. In addition to the input text, the hierarchical model encodes the context information to generate the response. Data-driven statistical approaches have also been studied for the text planning phase. In the text planning phase, NLG chooses the proper information of every sentence to be presented to users. The generation models mentioned above are trained by predicting the system response in a given conversational context using the maximum-likelihood estimation (MLE) objective so that they tend to generate nonsense responses such as “I dont know”. To address this problem, Li et al. (2016) applies deep reinforcement learning to model long-term reward in chatbot dialogue which can plan the information in the response and avoid generating the nonsense responses in the dialogue. 2.2 Task-oriented NLG for Specific Domain The statistical methods mentioned above are designed for open-domain chatbots, which emphasize on generating relevant and fluent responses according to the input text. While these methods are not suitable for task-solving scenarios (for instance, dialogue systems for restaurant and hotel reservation), which aims at providing correct answers to the input questions,"
C16-1191,W00-0306,0,0.133133,"quantitative evaluation. And the references set of the BLEU-4 metric are built by grouping the references of the same dialogue acts after delexicalising the responses and lexicalizing them by the correct values. Since the performance of CA-LSTM depends on initialisation, the results shown below are averaged over 5 randomly initialised CA-LSTM and the corpus are partitioned after random shuffle as well. 4.3 Quantitative Evaluation We compare our proposed model with several baselines including: the handcrafted generator (hdc), k-nearest neighbour (kNN), class-based LMs (classlm) as proposed by Oh and Rudnicky (2000), the 2-hidder-layer semantically conditioned LSTM network (SC-LSTM) proposed by Wen et al. (2015). For our own method, we experiment with several settings: the basic setting (denoted by CA-LSTM), the Context-Aware LSTM with attention (CA-LSTM+att) which encodes the question vector with an attention mechanism, and the Context-Aware LSTM with attention and act type embeddings (CALSTM+att+emb). The result is shown in Table 2. As we can see, the performances of our methods have been greatly improved compared to the baselines shown in the first block (hdc,kNN,classlm and SC-LSTM). By combining mor"
C16-1191,P02-1040,0,0.103079,"ponse turns sampled from about 1000 dialogues. The act types and slot-value pairs are labeled in the dataset. The details about the dialogue act are provided in Table 1. The training, validation and testing set are partitioned in the ratio of 3:1:1. And upsampling w.r.t act type is applied to make the corpus more uniform similar to (Wen et al., 2015). 4.2 Implementation Details We use Theano (Bergstra et al., 2010) to implement the proposed model. For each dialogue act and input question, we generate 20 responses and select the top 5 responses as the output after reranking. The BLEU-4 metric (Papineni et al., 2002) implemented by NLTK (Bird, 2006) is used for quantitative evaluation. And the references set of the BLEU-4 metric are built by grouping the references of the same dialogue acts after delexicalising the responses and lexicalizing them by the correct values. Since the performance of CA-LSTM depends on initialisation, the results shown below are averaged over 5 randomly initialised CA-LSTM and the corpus are partitioned after random shuffle as well. 4.3 Quantitative Evaluation We compare our proposed model with several baselines including: the handcrafted generator (hdc), k-nearest neighbour (kN"
C16-1191,D14-1162,0,0.0788637,"Missing"
C16-1191,D11-1054,0,0.0382846,"ogue or question answering system. NLG can be treated as a single-turn dialogue generation. Traditional approaches to NLG problem are mostly rule-based or template-based (Bateman and Henschel, 1999; Busemann and Horacek, 2002). However, these methods tend to generate rigid and stylised language without the natural variation of human language. In addition, they need a heavy workload to design the templates or rules. Recently due to the growth of artificial neural networks and the increase of labeled data available on the Internet, data-driven approaches are developed to attack the NLG problem (Ritter et al., 2011; Shang et al., 2015). Shang et al. (2015) and Serban et al. (2015) apply the RNN-based general encoder-decoder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answ"
C16-1191,P15-1152,0,0.0253425,"ering system. NLG can be treated as a single-turn dialogue generation. Traditional approaches to NLG problem are mostly rule-based or template-based (Bateman and Henschel, 1999; Busemann and Horacek, 2002). However, these methods tend to generate rigid and stylised language without the natural variation of human language. In addition, they need a heavy workload to design the templates or rules. Recently due to the growth of artificial neural networks and the increase of labeled data available on the Internet, data-driven approaches are developed to attack the NLG problem (Ritter et al., 2011; Shang et al., 2015). Shang et al. (2015) and Serban et al. (2015) apply the RNN-based general encoder-decoder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answer information. Howev"
C16-1191,D15-1199,0,0.46474,"rules. Recently due to the growth of artificial neural networks and the increase of labeled data available on the Internet, data-driven approaches are developed to attack the NLG problem (Ritter et al., 2011; Shang et al., 2015). Shang et al. (2015) and Serban et al. (2015) apply the RNN-based general encoder-decoder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answer information. However the context information, such as the input question and dialogue act, is ignored. Yin et al. (2016) propose a neural network model that can generate answers to simple factoid questions based on a knowledge base. But a large error rate is observed due to the complex architecture introduced. In this paper, we deal with the NLG problem in this setting: given a question, the correspondin"
C16-1191,W16-0106,0,0.176671,"coder framework to the open-domain dialogue response generation task. Although their model can generate the relevant and variant responses according to the input text in a statistical manner, the quality and content of responses depend on the quality and quantum of the training corpus. Wen et al. (2015) propose a taskoriented NLG model that can generate the responses providing the correct answers given the dialogue act (for instance, confirm or request some information), including the answer information. However the context information, such as the input question and dialogue act, is ignored. Yin et al. (2016) propose a neural network model that can generate answers to simple factoid questions based on a knowledge base. But a large error rate is observed due to the complex architecture introduced. In this paper, we deal with the NLG problem in this setting: given a question, the corresponding dialogue act, and the semantic slots to be addressed in the response, how to generate a natural language response in a dialogue. We present a statistical task-oriented NLG model based on a Context-Aware Long Short-term Memory network (CA-LSTM), which adopts the general encoder-decoder framework to incorporate"
C16-1191,W98-1425,0,\N,Missing
C18-1091,P11-1049,0,0.0316708,"Marcu (2000) proposed statistical approaches to mimic the sentence compression process, they used both noisy-channel and decision-tree to solve the problem. McDonald (2006) presented a discriminative large-margin learning framework coupled with a feature set and syntactic representations for sentence compression. Clarke and Lapata (2006) compared different models for sentence compression across domains and assessed a number of automatic evaluation measures. Clarke and Lapata (2008) used integer linear programming to infer globally optimal compression with linguistically motivated constraints. Berg-Kirkpatrick et al. (2011) proposed a joint model of sentence extraction and compression for multi-document summarization. Filippova and Altun (2013) presented a method for automatically building delete-based sentence compression corpus and proposed an compression method which used structured prediction. Abstractive sentence compression. Abstractive sentence compression extends delete-based compression methods with additional operations, such as substitution, reordering and insertion. Cohn and Lapata (2008) proposed a discriminative tree-to-tree transduction model which incorporated a grammar extraction method and used"
C18-1091,D14-1179,0,0.00895027,"Missing"
C18-1091,N16-1012,0,0.26706,"ess of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon the Seq2Seq architecture with attention mechanism, such as (Chopra et al., 2016; Wubben et al., 2016; Nallapati et al., 2016; See et al., 2017). These abstractive models (which will be termed generate-based models hereafter) have the ability to reorder words or rephrase. However, none of these models consider explicit word deletion. As Coster and Kauchak (2011b) pointed out, deletion is a frequently occurring phenomena in sentence compression dataset. Coster and Kauchak (2011a) imposed delete operation on their sentence compression model and improved the performance significantly. Thus, deletion is also very important for abstractive sentence compression task. Inspired b"
C18-1091,P06-1048,0,0.603936,"on-redundancy aspect. 1073 6 Related Work Delete-based sentence compression. A large number of work is devoted to delete-based sentence compression. Jing (2000) presented a system that used multiple sources of knowledge to decide which phrases in a sentence can be removed. Knight and Marcu (2000) proposed statistical approaches to mimic the sentence compression process, they used both noisy-channel and decision-tree to solve the problem. McDonald (2006) presented a discriminative large-margin learning framework coupled with a feature set and syntactic representations for sentence compression. Clarke and Lapata (2006) compared different models for sentence compression across domains and assessed a number of automatic evaluation measures. Clarke and Lapata (2008) used integer linear programming to infer globally optimal compression with linguistically motivated constraints. Berg-Kirkpatrick et al. (2011) proposed a joint model of sentence extraction and compression for multi-document summarization. Filippova and Altun (2013) presented a method for automatically building delete-based sentence compression corpus and proposed an compression method which used structured prediction. Abstractive sentence compress"
C18-1091,D07-1008,0,0.0378157,"e compression, since it needs deeper understanding of the source sentence. Delete-based sentence compression treats the task as a word deletion problem: given an input source sentence x = x1 , x2 , ..., xn (where xi stands for the ith word in the sentence x), the goal is to produce a target sentence by removing any subset of words in the source sentence x (Knight and Marcu, 2002). Delete-based sentence compression has been widely explored across different modeling paradigms, such as noisy-channel model (Knight and Marcu, 2002; Turner and Charniak, 2005), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), integer linear programming (Clarke and Lapata, 2008) and variational auto-encoder (Miao and Blunsom, 2016). In delete-based sentence compression models, only delete operations are allowed, thus the order of the remaining words can not be changed. These constraints make delete-based sentence compression a relatively easier task. However, in spite of the strong ability of deleting undesired words, delete-based models are not able to rephrase the words, which is far † Corresponding author: Minlie Huang (aihuang@tsinghua.edu.cn) This work is licensed under a Creative Commons Attribution 4.0 Inte"
C18-1091,C08-1018,0,0.61979,"Conference on Computational Linguistics, pages 1065–1076 Santa Fe, New Mexico, USA, August 20-26, 2018. from human sentence compression. For example, in human sentence compression, the word “remove” can replace the phrase “get rid of” under some particular circumstance, but this substitution can not be accomplished by delete-only models. Abstractive sentence compression has the ability to reorder, substitute words or rephrase, thus it is more coherent to human sentence compression. Due to the difficulty of abstractive sentence compression, there was only a limited number of work on the task (Cohn and Lapata, 2008; Cohn and Lapata, 2013; Galanis and Androutsopoulos, 2011; Coster and Kauchak, 2011a). However, with the recent success of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of a"
C18-1091,W11-1601,0,0.0244585,"SA, August 20-26, 2018. from human sentence compression. For example, in human sentence compression, the word “remove” can replace the phrase “get rid of” under some particular circumstance, but this substitution can not be accomplished by delete-only models. Abstractive sentence compression has the ability to reorder, substitute words or rephrase, thus it is more coherent to human sentence compression. Due to the difficulty of abstractive sentence compression, there was only a limited number of work on the task (Cohn and Lapata, 2008; Cohn and Lapata, 2013; Galanis and Androutsopoulos, 2011; Coster and Kauchak, 2011a). However, with the recent success of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon the Seq2Seq architecture wi"
C18-1091,P11-2117,0,0.0248599,"SA, August 20-26, 2018. from human sentence compression. For example, in human sentence compression, the word “remove” can replace the phrase “get rid of” under some particular circumstance, but this substitution can not be accomplished by delete-only models. Abstractive sentence compression has the ability to reorder, substitute words or rephrase, thus it is more coherent to human sentence compression. Due to the difficulty of abstractive sentence compression, there was only a limited number of work on the task (Cohn and Lapata, 2008; Cohn and Lapata, 2013; Galanis and Androutsopoulos, 2011; Coster and Kauchak, 2011a). However, with the recent success of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon the Seq2Seq architecture wi"
C18-1091,D13-1155,0,0.337619,"on-tree to solve the problem. McDonald (2006) presented a discriminative large-margin learning framework coupled with a feature set and syntactic representations for sentence compression. Clarke and Lapata (2006) compared different models for sentence compression across domains and assessed a number of automatic evaluation measures. Clarke and Lapata (2008) used integer linear programming to infer globally optimal compression with linguistically motivated constraints. Berg-Kirkpatrick et al. (2011) proposed a joint model of sentence extraction and compression for multi-document summarization. Filippova and Altun (2013) presented a method for automatically building delete-based sentence compression corpus and proposed an compression method which used structured prediction. Abstractive sentence compression. Abstractive sentence compression extends delete-based compression methods with additional operations, such as substitution, reordering and insertion. Cohn and Lapata (2008) proposed a discriminative tree-to-tree transduction model which incorporated a grammar extraction method and used a language model for coherent output. Galanis and Androutsopoulos (2011) presented a dataset for extractive and abstractiv"
C18-1091,D15-1042,0,0.304499,"ng and insertion. Cohn and Lapata (2008) proposed a discriminative tree-to-tree transduction model which incorporated a grammar extraction method and used a language model for coherent output. Galanis and Androutsopoulos (2011) presented a dataset for extractive and abstractive sentence compression and proposed a SVR based abstractive sentence compressor which utilized additional PMI-based and LDA-based features. Shafieibavani et al. (2016) proposed a word graph-based model which can improve both informativeness and grammaticality of the sentence at the same time. Neural sentence compression. Filippova et al. (2015) proposed a delete-based sentence compression system which took as input a sentence and output a binary sequence corresponding to word deletion decisions in the sentence. The model was trained on a set of 2 millions sentence pairs which was constructed by the same approach used in Filippova and Altun (2013). There are also some neural approaches for abstractive sentence compression. Rush et al. (2015) proposed a fully data-driven approach which utilized neural language models for abstractive sentence compression. They tried different kinds of encoders to encode the input sentence into vector r"
C18-1091,W11-2701,0,0.126061,"s 1065–1076 Santa Fe, New Mexico, USA, August 20-26, 2018. from human sentence compression. For example, in human sentence compression, the word “remove” can replace the phrase “get rid of” under some particular circumstance, but this substitution can not be accomplished by delete-only models. Abstractive sentence compression has the ability to reorder, substitute words or rephrase, thus it is more coherent to human sentence compression. Due to the difficulty of abstractive sentence compression, there was only a limited number of work on the task (Cohn and Lapata, 2008; Cohn and Lapata, 2013; Galanis and Androutsopoulos, 2011; Coster and Kauchak, 2011a). However, with the recent success of the sequence-to-sequence (Seq2Seq) model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon t"
C18-1091,P16-1154,0,0.0225553,"erate decoder for further calculation. 4.2 Copy-Generate Decoder The Copy-Generate decoder produces output the compressed sentence word by word, and the output words are either copied from the input words which are filtered by the delete decoder, or generated with a fixed vocabulary. In other words, our model integrates copy, generate, delete operations together to produce the output sequence. We implement the Copy-Generate decoder as a hybrid network between the basic Seq2Seq network and a pointer network (Vinyals et al., 2015). The structure of the Copy-Generate decoder is close to CopyNet (Gu et al., 2016), Pointer Softmax (Gulcehre et al., 2016) and Pointer-Generator (See et al., 2017). However, the Copy-Generate decoder model has some unique characteristic designed for abstractive sentence compression task. The major difference is that our Copy-Generate decoder incorporates the result of the delete decoder, to make sure that unnecessary words will not be copied back by accident. Another difference from the other models is that our model generates explicit operations. During the 1068 0 0 He said 1 0 companies engaged 0 1 1 1 to set up offices Context Vectors Figure 2: Delete Decoder. training"
C18-1091,P16-1014,0,0.014366,"on. 4.2 Copy-Generate Decoder The Copy-Generate decoder produces output the compressed sentence word by word, and the output words are either copied from the input words which are filtered by the delete decoder, or generated with a fixed vocabulary. In other words, our model integrates copy, generate, delete operations together to produce the output sequence. We implement the Copy-Generate decoder as a hybrid network between the basic Seq2Seq network and a pointer network (Vinyals et al., 2015). The structure of the Copy-Generate decoder is close to CopyNet (Gu et al., 2016), Pointer Softmax (Gulcehre et al., 2016) and Pointer-Generator (See et al., 2017). However, the Copy-Generate decoder model has some unique characteristic designed for abstractive sentence compression task. The major difference is that our Copy-Generate decoder incorporates the result of the delete decoder, to make sure that unnecessary words will not be copied back by accident. Another difference from the other models is that our model generates explicit operations. During the 1068 0 0 He said 1 0 companies engaged 0 1 1 1 to set up offices Context Vectors Figure 2: Delete Decoder. training procedure, we also add supervision on the"
C18-1091,A00-1043,0,0.140282,"results in Table 2 show that combined with the delete decoder, our model can effectively delete unimportant contents without losing much grammar quality of the text. This exactly matches what we expect from our model. Pointer-Generator model performs better on grammaticality aspect than our model, however, our model outperforms the other baselines on non-redundancy aspect. The basic Seq2Seq model performs poorly on both grammaticality aspect and non-redundancy aspect. 1073 6 Related Work Delete-based sentence compression. A large number of work is devoted to delete-based sentence compression. Jing (2000) presented a system that used multiple sources of knowledge to decide which phrases in a sentence can be removed. Knight and Marcu (2000) proposed statistical approaches to mimic the sentence compression process, they used both noisy-channel and decision-tree to solve the problem. McDonald (2006) presented a discriminative large-margin learning framework coupled with a feature set and syntactic representations for sentence compression. Clarke and Lapata (2006) compared different models for sentence compression across domains and assessed a number of automatic evaluation measures. Clarke and La"
C18-1091,W04-1013,0,0.0610612,"periment Metric In the experiments, we compare our model and the baselines with the following metrics. Compression Ratio: The common assumption in compression research is that the system can make the determination of the optimal compression length. Thus, compression ratios can vary drastically across systems. Different systems can be compared only when they are compressing at similar ratios (Napoles et al., 2011). Compression ratio is defined as: CompRatio = # of tokens in compressed text # of tokens in source text (20) ROUGE: We evaluated our models with the standard ROUGE metric proposed by Lin (2004). ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is commonly used for measuring the quality of the summary by comparing computer-generated summaries to reference summaries generated by humans. The basic idea of ROUGE is to count the number of overlapping units such as n-grams, word sequences, and word pairs between computer-generated summaries and the reference summaries. In our experiments, we considered ROUGE-1, ROUGE-2 and ROUGE-L (which respectively measures the word-overlap, bigram-overlap, and longest common sequence between the reference summary and the summary t"
C18-1091,E06-1038,0,0.263039,"te-based sentence compression, since it needs deeper understanding of the source sentence. Delete-based sentence compression treats the task as a word deletion problem: given an input source sentence x = x1 , x2 , ..., xn (where xi stands for the ith word in the sentence x), the goal is to produce a target sentence by removing any subset of words in the source sentence x (Knight and Marcu, 2002). Delete-based sentence compression has been widely explored across different modeling paradigms, such as noisy-channel model (Knight and Marcu, 2002; Turner and Charniak, 2005), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), integer linear programming (Clarke and Lapata, 2008) and variational auto-encoder (Miao and Blunsom, 2016). In delete-based sentence compression models, only delete operations are allowed, thus the order of the remaining words can not be changed. These constraints make delete-based sentence compression a relatively easier task. However, in spite of the strong ability of deleting undesired words, delete-based models are not able to rephrase the words, which is far † Corresponding author: Minlie Huang (aihuang@tsinghua.edu.cn) This work is licensed under a Creative Comm"
C18-1091,D16-1031,0,0.0349658,"n treats the task as a word deletion problem: given an input source sentence x = x1 , x2 , ..., xn (where xi stands for the ith word in the sentence x), the goal is to produce a target sentence by removing any subset of words in the source sentence x (Knight and Marcu, 2002). Delete-based sentence compression has been widely explored across different modeling paradigms, such as noisy-channel model (Knight and Marcu, 2002; Turner and Charniak, 2005), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), integer linear programming (Clarke and Lapata, 2008) and variational auto-encoder (Miao and Blunsom, 2016). In delete-based sentence compression models, only delete operations are allowed, thus the order of the remaining words can not be changed. These constraints make delete-based sentence compression a relatively easier task. However, in spite of the strong ability of deleting undesired words, delete-based models are not able to rephrase the words, which is far † Corresponding author: Minlie Huang (aihuang@tsinghua.edu.cn) This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1065 Proceedings of the 27th"
C18-1091,K16-1028,0,0.0828148,"model, the task of abstractive sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon the Seq2Seq architecture with attention mechanism, such as (Chopra et al., 2016; Wubben et al., 2016; Nallapati et al., 2016; See et al., 2017). These abstractive models (which will be termed generate-based models hereafter) have the ability to reorder words or rephrase. However, none of these models consider explicit word deletion. As Coster and Kauchak (2011b) pointed out, deletion is a frequently occurring phenomena in sentence compression dataset. Coster and Kauchak (2011a) imposed delete operation on their sentence compression model and improved the performance significantly. Thus, deletion is also very important for abstractive sentence compression task. Inspired by previous work, we propose an Operation Netw"
C18-1091,W11-1611,0,0.0532476,"Missing"
C18-1091,P02-1040,0,0.104339,"mparing computer-generated summaries to reference summaries generated by humans. The basic idea of ROUGE is to count the number of overlapping units such as n-grams, word sequences, and word pairs between computer-generated summaries and the reference summaries. In our experiments, we considered ROUGE-1, ROUGE-2 and ROUGE-L (which respectively measures the word-overlap, bigram-overlap, and longest common sequence between the reference summary and the summary to be evaluated). BLEU: We also report BLEU scores of the baseline models and Operation Network on the test dataset. BLEU is proposed by Papineni et al. (2002), and is usually used for automatic evaluation of statistical machine translation systems. However, it can also be used for evaluating sentence compression task (Napoles et al., 2011). We use the multi-bleu script3 for BLEU score calculation. 5.5 Result Analysis We present the average ratios of the operations in Operation Network’s outputs and the human-written references on the test dataset in Figure 3. Note that the delete ratio is calculated as the number of delete operations divide by the number of tokens in source text, and the copy and generate ratio is calculated as the number of copy a"
C18-1091,D15-1044,0,0.0752913,"atures. Shafieibavani et al. (2016) proposed a word graph-based model which can improve both informativeness and grammaticality of the sentence at the same time. Neural sentence compression. Filippova et al. (2015) proposed a delete-based sentence compression system which took as input a sentence and output a binary sequence corresponding to word deletion decisions in the sentence. The model was trained on a set of 2 millions sentence pairs which was constructed by the same approach used in Filippova and Altun (2013). There are also some neural approaches for abstractive sentence compression. Rush et al. (2015) proposed a fully data-driven approach which utilized neural language models for abstractive sentence compression. They tried different kinds of encoders to encode the input sentence into vector representation of fixed dimensions. Chopra et al. (2016) further improved the model with Recurrent Neural Networks. However, both works used vocabularies of fixed size for target sentence generation. Wubben et al. (2016) used a Seq2Seq model with bi-directional LSTMs for abstractive compression of captions. Toutanova et al. (2016) manually created a multi-reference dataset for sentence and short paragr"
C18-1091,P17-1099,0,0.425321,"active sentence compression has become viable. Seq2Seq has an encoder-decoder architecture where the encoder encodes the input sequence into hidden states, and the decoder then generates the output sequence from the hidden states. The attention mechanism (Bahdanau et al., 2014), which can align the output sequence with the input sequence automatically, boosts the performance of Seq2Seq significantly. A number of abstractive sentence compression work has been built upon the Seq2Seq architecture with attention mechanism, such as (Chopra et al., 2016; Wubben et al., 2016; Nallapati et al., 2016; See et al., 2017). These abstractive models (which will be termed generate-based models hereafter) have the ability to reorder words or rephrase. However, none of these models consider explicit word deletion. As Coster and Kauchak (2011b) pointed out, deletion is a frequently occurring phenomena in sentence compression dataset. Coster and Kauchak (2011a) imposed delete operation on their sentence compression model and improved the performance significantly. Thus, deletion is also very important for abstractive sentence compression task. Inspired by previous work, we propose an Operation Network for abstractive"
C18-1091,D16-1033,0,0.251834,"TY 1 X copy gen loss = (switch losst + copy losst + gen losst ) TY (19) t=1 where TY is the length of the target word sequence Y . 5 Experiments In this section, we introduce the experiments for abstractive sentence compression with our proposed Operation Network. First, we present a brief description of the dataset, and the pre-processing procedure in our experiments. Then, we introduce baselines that are compared with our model. Next, we introduce parameters of our models in the experiments. At last we present and analyze the experiment results. 5.1 Dataset We adopt the dataset provided by Toutanova et al. (2016) for our experiments. It’s a manually-created, multi-reference dataset for sentence and short paragraph compression. It contains 6,169 source texts with multiple compressions (26,423 pairs of source and compressed texts), consisting of business letters, 1070 news journals, and technical documents sampled from the Open American National Corpus (OANC1 ). Of all the source texts, 3,769 are single sentences and the rest are 2-sentence short paragraphs. Each pair of the source and compressed text is aligned by the state-of-the-art monolingual aligner Jacana (Yao et al., 2013). The dataset is split"
C18-1091,P05-1036,0,0.226013,"ctive sentence compression is much harder than delete-based sentence compression, since it needs deeper understanding of the source sentence. Delete-based sentence compression treats the task as a word deletion problem: given an input source sentence x = x1 , x2 , ..., xn (where xi stands for the ith word in the sentence x), the goal is to produce a target sentence by removing any subset of words in the source sentence x (Knight and Marcu, 2002). Delete-based sentence compression has been widely explored across different modeling paradigms, such as noisy-channel model (Knight and Marcu, 2002; Turner and Charniak, 2005), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), integer linear programming (Clarke and Lapata, 2008) and variational auto-encoder (Miao and Blunsom, 2016). In delete-based sentence compression models, only delete operations are allowed, thus the order of the remaining words can not be changed. These constraints make delete-based sentence compression a relatively easier task. However, in spite of the strong ability of deleting undesired words, delete-based models are not able to rephrase the words, which is far † Corresponding author: Minlie Huang (aihuang@tsinghua.edu.cn) This"
C18-1091,W16-6608,0,0.720592,"Missing"
C18-1091,P13-2123,0,0.071311,"Missing"
C18-1171,D17-2011,0,0.413853,"questions with multiple triples in KB. Results show that our model obtains state-of-the-art performance. 2. Our model is more interpretable than existing reasoning networks in that the intermediate entities and relations predicted by the hop-by-hop reasoning process construct traceable reasoning paths to clearly reveal how the answer is derived. 2 Related Works Recent works on QA can be roughly classified into two types: one is semantic-parsing-based and the other is embedding-based. Semantic parsing approaches map questions to logical form queries (Pasupat and Liang, 2015; Yih et al., 2016; Abujabal et al., 2017). These systems are effective but at the cost of heavy data annotation and pattern/grammar engineering. What’s more, parsing systems are often constrained on a specific domain and broken down when executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models (Bordes et al., 2014b; Dong et al., 2015; Xu et al., 2016; Hao et al., 2017; Yavuz et al., 2017) which are recently introduced into the QA community where questions and KB entities are represented by distributed vectors, and QA is formulated as a problem of matching between vectors of questions and answer"
C18-1171,N16-1181,0,0.0161629,"rsing the questions. Other studies applying hop-by-hop inference into QA can be seen in Neural Programmer (Neelakantan et al., 2015; Neelakantan et al., 2016) and Neural Enquirer (Yin et al., 2015), where deep networks are proposed to parse a question and execute a query on tables. However, Neural Programmer needs to predefine symbolic operations, while Neural Enquirer lacks explicit interpretation. Mou et al. (2017) proposed a model coupling distributed and symbolic execution with REINFORCE algorithm, however, training such a model is challenging. Neural Module Network (Andreas et al., 2015; Andreas et al., 2016) customized network architectures for different patterns of reasoning, making the reasoning network interpretable. However, a dependency parser and the REINFORCE algorithm are required. 2011 3 Interpretable Reasoning Network 3.1 Task Definition Our goal is to offer an interpretable reasoning network to answer multi-relation questions. Given a question q and its topic entity or subject es which can be annotated by some NER tools, the task is to find an entity a in KB as the answer. In this work, we consider two typical categories of multi-relation questions, a path question (Guu et al., 2015) a"
C18-1171,D13-1160,0,0.0791093,"016). 4.1 PathQuestion We adopted two subsets of Freebase (Bollacker et al., 2008) as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) datasets. We extracted paths between two enr1 r2 r1 r2 r3 tities which span two hops (es −→ e1 −→ a, denoted by -2H) or three hops (es −→ e1 −→ e2 −→ a, denoted by -3H) and then generated natural language questions with templates. To make the generated questions analogical to real-world questions, we included paraphrasing templates and synonyms for relations by searching the Internet and two real-world datasets, WebQuestions (Berant et al., 2013) and WikiAnswers (Fader et al., 2013). In this way, the syntactic structure and surface wording of the generated questions have been greatly enriched. PQL is more challenging than PQ in that PQL utilizes larger KB and provides less training instances. The statistics are shown in Table 1 and more details are described in the Appendix 6. 4.2 WorldCup2014 We also evaluated our model on the WorldCup2014 (WC2014) dataset constructed by (Zhang et al., 2016). The dataset contains single-relation questions (denoted by WC-1H), two-hop path questions (WC2H), and conjunctive questions (WC-C). WC-M is the"
C18-1171,D14-1067,0,0.256853,"ons such as “Name a soccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability. Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document. However, transferring existing RC methods to KBQA is not trivial. For one reason, the focus of reasoning in RC is usually on understanding the document rather than parsing questions. For another reason, existing reasoning netw"
C18-1171,P15-1026,0,0.04849,"derived. 2 Related Works Recent works on QA can be roughly classified into two types: one is semantic-parsing-based and the other is embedding-based. Semantic parsing approaches map questions to logical form queries (Pasupat and Liang, 2015; Yih et al., 2016; Abujabal et al., 2017). These systems are effective but at the cost of heavy data annotation and pattern/grammar engineering. What’s more, parsing systems are often constrained on a specific domain and broken down when executing logical queries on incomplete KBs. Our work follows the line of Embedding-based models (Bordes et al., 2014b; Dong et al., 2015; Xu et al., 2016; Hao et al., 2017; Yavuz et al., 2017) which are recently introduced into the QA community where questions and KB entities are represented by distributed vectors, and QA is formulated as a problem of matching between vectors of questions and answer entities. These models need less grammars as well as annotated data, and are more flexible to deal with incomplete KBs. To make better matching, subgraphs of an entity in KB (Bordes et al., 2014a), answer aspects (Dong et al., 2015; Hao et al., 2017) and external contexts (Xu et al., 2016) can be used to enrich the representation o"
C18-1171,P13-1158,0,0.0259847,"subsets of Freebase (Bollacker et al., 2008) as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) datasets. We extracted paths between two enr1 r2 r1 r2 r3 tities which span two hops (es −→ e1 −→ a, denoted by -2H) or three hops (es −→ e1 −→ e2 −→ a, denoted by -3H) and then generated natural language questions with templates. To make the generated questions analogical to real-world questions, we included paraphrasing templates and synonyms for relations by searching the Internet and two real-world datasets, WebQuestions (Berant et al., 2013) and WikiAnswers (Fader et al., 2013). In this way, the syntactic structure and surface wording of the generated questions have been greatly enriched. PQL is more challenging than PQ in that PQL utilizes larger KB and provides less training instances. The statistics are shown in Table 1 and more details are described in the Appendix 6. 4.2 WorldCup2014 We also evaluated our model on the WorldCup2014 (WC2014) dataset constructed by (Zhang et al., 2016). The dataset contains single-relation questions (denoted by WC-1H), two-hop path questions (WC2H), and conjunctive questions (WC-C). WC-M is the mixture of WC-1H and WC-2H. The stat"
C18-1171,D15-1038,0,0.0261814,"dreas et al., 2016) customized network architectures for different patterns of reasoning, making the reasoning network interpretable. However, a dependency parser and the REINFORCE algorithm are required. 2011 3 Interpretable Reasoning Network 3.1 Task Definition Our goal is to offer an interpretable reasoning network to answer multi-relation questions. Given a question q and its topic entity or subject es which can be annotated by some NER tools, the task is to find an entity a in KB as the answer. In this work, we consider two typical categories of multi-relation questions, a path question (Guu et al., 2015) and a conjunctive question (Zhang et al., 2016), while the former is our major focus. A path question contains only one topic entity (subject es ) and its answer (object a) can be found by walking down an answer path consisting of a few relations and the corresponding intermediate entities. We define an answer path as a sequence of entities and relations in KB which starts from the subject r1 r2 rn and ends with the answer like es −→ e1 −→ ... −→ a. Relations (ri ) are observable (in various natural language forms) in the question, however, the intermediate entities (e1 · · · eH ) are not. Fo"
C18-1171,P17-1021,0,0.161121,"ccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability. Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document. However, transferring existing RC methods to KBQA is not trivial. For one reason, the focus of reasoning in RC is usually on understanding the document rather than parsing questions. For another reason, existing reasoning networks are usually de"
C18-1171,D16-1147,0,0.201485,"Missing"
C18-1171,P17-2047,0,0.0719795,"s task has recently been facilitated by large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008). However, due to the variety and complexity of language and knowledge, open-domain question answering over knowledge bases (KBQA) is still a challenging task. Question answering over knowledge bases falls into two types, namely single-relation QA and multirelation QA, as argued by Yin et al. (2016). Single-relation questions, such as “How old is Obama?”, can be answered by finding one fact triple in KB, and this task has been widely studied (Bordes et al., 2015; Xu et al., 2016; Savenkov and Agichtein, 2017). In comparison, reasoning over multiple fact triples is required to answer multi-relation questions such as “Name a soccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annota"
C18-1171,P17-1018,0,0.277386,"dge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability. Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document. However, transferring existing RC methods to KBQA is not trivial. For one reason, the focus of reasoning in RC is usually on understanding the document rather than parsing questions. For another reason, existing reasoning networks are usually designed in a black-box style, making the models less interpretable. While in multi-relation question answering, we believe that an interpretable reasoning process is essential. In this paper, we propose a novel Interpretable Reasoning Netwo"
C18-1171,P16-1220,0,0.192007,"pic in AI and this task has recently been facilitated by large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008). However, due to the variety and complexity of language and knowledge, open-domain question answering over knowledge bases (KBQA) is still a challenging task. Question answering over knowledge bases falls into two types, namely single-relation QA and multirelation QA, as argued by Yin et al. (2016). Single-relation questions, such as “How old is Obama?”, can be answered by finding one fact triple in KB, and this task has been widely studied (Bordes et al., 2015; Xu et al., 2016; Savenkov and Agichtein, 2017). In comparison, reasoning over multiple fact triples is required to answer multi-relation questions such as “Name a soccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-cra"
C18-1171,D17-1094,0,0.216687,"ays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability. Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document. However, transferring existing RC methods to KBQA is not trivial. For one reason, the focus of reasoning in RC is usually on understanding the document rather than parsing questions. For another reason, existing reasoning networks are usually designed in a black-box"
C18-1171,P14-2105,0,0.0254562,"triple in KB, and this task has been widely studied (Bordes et al., 2015; Xu et al., 2016; Savenkov and Agichtein, 2017). In comparison, reasoning over multiple fact triples is required to answer multi-relation questions such as “Name a soccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability. Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document. Howev"
C18-1171,P16-2033,0,0.261631,"this task has been widely studied (Bordes et al., 2015; Xu et al., 2016; Savenkov and Agichtein, 2017). In comparison, reasoning over multiple fact triples is required to answer multi-relation questions such as “Name a soccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categorized into two lines: semantic parsing and embedding-based models. Semantic parsing models (Yih et al., 2014; Yih et al., 2016) obtain competitive performance at the cost of hand-crafted features and manual annotations, but lack the ability to generalize to other domains. In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability. Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document. However, transferring ex"
C18-1171,C16-1164,0,0.0524942,"easoning analysis and failure diagnosis, thereby allowing manual manipulation in predicting the final answer. 1 Introduction Open-domain Question Answering (QA) has always been a hot topic in AI and this task has recently been facilitated by large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008). However, due to the variety and complexity of language and knowledge, open-domain question answering over knowledge bases (KBQA) is still a challenging task. Question answering over knowledge bases falls into two types, namely single-relation QA and multirelation QA, as argued by Yin et al. (2016). Single-relation questions, such as “How old is Obama?”, can be answered by finding one fact triple in KB, and this task has been widely studied (Bordes et al., 2015; Xu et al., 2016; Savenkov and Agichtein, 2017). In comparison, reasoning over multiple fact triples is required to answer multi-relation questions such as “Name a soccer player who plays at forward position at the club Borussia Dortmund.” where more than one entity and relation are mentioned. Compared to single-relation QA, multi-relation QA is yet to be addressed. Previous studies on QA over knowledge bases can be roughly categ"
C18-1171,D14-1071,0,\N,Missing
D10-1109,C02-1150,0,0.743254,"of time, researches on question answering are mainly focused on finding short and concise answers from plain text for factoid questions driven by annual trackes such as CLEF, TREC and NTCIR. However, people usually ask more complex questions in real world which cannot be handled by these QA systems tailored to factoid questions. Question classification (QC) in factoid QA is to provide constraints on answer types that allows further processing to pinpoint and verify the answer (Li and Roth, 2004). Usually, questions are classified into a fine grained content-based taxonomy(e.g. UIUC taxonomy (Li and Roth, 2002)). We cannot use these taxonomies directly. To guide question distribution and answer summarization, questions are classified according to their functions instead of contents. Motivated by related work on user goal classification(Broder, 2002; Rose and Levinson, 2004) , we propose a function-based question classification category tailored to general QA. The category contain six types, namely Fact, List, Reason, Solution, Definition and Navigation. We will introduced this 1119 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1119–1128, c MIT, Massach"
D10-1109,C08-1063,0,0.0309741,"rks for understanding goals of user searches. Generally, web queries are classified into four types: Navigational, Informational, Transactional (Broder, 2002) and Resource (Rose and Levinson, 2004). Lee et al. (2005) automatically classify Navigational and Informational queries based on past user-click behavior and anchor-link distribution. Jansen and Booth (2010) investigate the correspondence between three user intents and eighteen topics. The result shows that user intents distributed unevenly among different topics. Inspired by Rose and Levinson (2004)’s work in user goals classification, Liu et al. (2008) describe a three-layers cQA oriented question taxonomy and use it to determine the expected best answer types and summarize answers. Other than Navigational, Informational and Transactional, the first layer contains a new Social category which represents the questions that do not intend to get an answer but to elicit interaction with other people. Informational contains two subcategories Constant and Dynamic. Dynamic is further divided into Opinion, ContextDependent and Open. Markov logic network (MLN) (Richardson and Domingos, 2006) is a general model combining first-order logic and probabil"
D10-1109,P07-1098,0,0.0338526,"hese taxonomies are for open-domain question answering. With the booming of internet, researches on question answering are becoming more practical. Most taxonomies proposed are focused on factoid questions, such as UIUC taxonomy (Li and Roth, 2002). UIUC taxonomy contains 6 coarse classes (Abbreviation, Entity, Description, Human, Location and Numeric Value) and 50 fine classes. All coarse classes are factoid oriented except Description. To classify questions effectively, Researchers have proposed features of different levels, such as lexical features, syntactic features (Nguyen et al., 2007; Moschitti et al., 2007) and semantic features (Moschitti et al., 2007; Li and Roth, 2004). Zhang and Lee (2003) compared five machine learning methods and found SVM outperformed the others. In information retrieval community, researchers have described frameworks for understanding goals of user searches. Generally, web queries are classified into four types: Navigational, Informational, Transactional (Broder, 2002) and Resource (Rose and Levinson, 2004). Lee et al. (2005) automatically classify Navigational and Informational queries based on past user-click behavior and anchor-link distribution. Jansen and Booth (20"
D10-1109,D09-1001,0,0.0252968,"Missing"
D10-1109,P09-1023,0,0.0197369,"hotmail.com haoyu@mail.tsinghua.edu.cn zxy-dcs@tsinghua.edu.cn Abstract During recent years, social collaborative applications begin to flourish, such as Wikipedia, Facebook, Yahoo! Answers and etc. A large amount of semistructured data, which has been accumulated from these services, becomes new sources for question answering. Previous researches show that different sources are suitable for answering different questions. For example, the answers for factoid questions can be extracted from webpages with high accuracy, definition questions can be answered by corresponding articles in wikipedia(Ye et al., 2009) while community question answering services provide comprehensive answers for complex questions(Jeon et al., 2005). It will greatly enhance the overall performance if we can classify questions into several types, distribute each type of questions to suitable sources and trigger corresponding strategy to summarize returned answers. In contrast with the booming increase of internet data, state-of-art QA (question answering) systems, otherwise, concerned data from specific domains or resources such as search engine snippets, online forums and Wikipedia in a somewhat isolated way. Users may welco"
D14-1169,P08-1031,0,0.0321242,"owledge to provide a better initialization for EM. In Zhai et al. (2011a), an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping feature expressions that share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-bas"
D14-1169,D13-1172,0,0.150438,"ovel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and Figure 1: A semi-structured Review. This new angle is inspired by this simple observation (as illustrated in Fig. 1): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Co"
D14-1169,P09-1028,0,0.0604815,"Missing"
D14-1169,P12-1036,0,0.217566,"onstraint, we propose a novel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably. 1 Introduction Aspect-level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product’s properties, and provide much detailed, complete, and in-depth summaries of a large number of reviews. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has attracted more and more attentions (Mukherjee and Liu, 2012; Chen et al., 2013; Zhai et al., 2011a; Zhai et al., 2010). Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while semantically close, which makes the task more challenging. For example, “price”, “money” , “worth” and Figure 1: A semi-structured Review. This new angle is inspired by this simple observation (as illustrated in Fig. 1): two phrases within the same cluster are not likely to be simultaneously pl"
D14-1169,D11-1088,0,0.0132666,"n criteria (Druck et al., 2008) (GE) is a framework for incorporating preferences about model expectations into parameter estimation objective functions. Liang et al. (2009) developed a Bayesian decision-theoretic framework to learn an exponential family model using general measurements on the unlabeled data. In this paper, we model our problem in the framework of posterior regularization. Many works promoted the performance of sentiment analysis by incorporating prior knowledge as weak supervision. Li and Zhang (2009) injected lexical prior knowledge to non-negative matrix tri-factorization. Shen and Li (2011) further extended the matrix factorization framework to model dual supervision from both document and word labels. Vikas Sindhwani (2008) proposed a general framework for incorporating lexical information as well as unlabeled data within standard regularized least squares for sentiment prediction tasks. Fang (2013)proposed a structural learning model with a handful set of aspect signature terms that are encoded as weak supervision to extract latent sentiment explanations. 5 Conclusions Aspect finding and clustering is an important task for aspect-level sentiment analysis. In order to cluster a"
D14-1169,C10-1143,0,0.258662,"Missing"
D14-1169,D10-1006,0,0.0828585,"at share words in common, and then merged the groups by lexical similarity. Zhai et al. (2011b) proposed a LDA-based method that incorporates must-link and cannot-link constraints. Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling. Titov and McDonald (2008) proposed the multi-grain topic models to discover global and local aspects. Branavan et al. (2008) proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. Zhao et al. (2010) proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspectspecific opinion words, which can leverage syntactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used userprovided seeds to discover aspects. Chen et al. (2013) proposed a knowledge-based topic model to incorporate must-link and cannot-link information. Their model can adjust topic numbers automatically by leveraging cannot-link. Our work is also related to general constraintdriven(or knowledge-"
D16-1058,P14-2009,0,0.803983,"Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qian et al., 2015), Recursive Neural Tensor Network (Socher et al., 2013), Recurrent Neural Network (Mikolov et al., 2010; Tang et al., 2015b), LSTM (Hochreiter and Schmidhuber, 1997) and Tree-LSTMs (Tai et al., 2015) were applied into sentiment analysis currently. By utilizing syntax structures of sentences, tree-based LSTMs have been proved to be quite effective for many NLP tasks. However, such methods may suffer from syntax parsing errors which are common in resourcelacking languages. LSTM has achieved a great success in various NLP tasks. TD-LSTM and TC-LSTM (Tang et al., 2015a), which"
D16-1058,D16-1166,0,0.0948827,"sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification. 1 Neural networks have achieved state-of-the-art performance in a variety of NLP tasks such as machine translation (Lample et al., 2016), paraphrase identification (Yin et al., 2015), question answering (Golub and He, 2016) and text summarization (Rush et al., 2015). However, neural network models are still in infancy to deal with aspectlevel sentiment classification. In some works, target dependent sentiment classification can be benefited from taking into account target information, such as in Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM (TC-LSTM) (Tang et al., 2015a). However, those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect-level classification. Introduction Sentiment analysis (Nasukawa and Yi, 2003), also known as opin"
D16-1058,D07-1115,0,0.0074877,"tion briefly. 2.1 Sentiment Classification at Aspect-level Aspect-level sentiment classification is typically considered as a classification problem in the liter607 ature. As we mentioned before, aspect-level sentiment classification is a fine-grained classification task. The majority of current approaches attempt to detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. Traditional approaches to solve those problems are to manually design a set of features. With the abundance of sentiment lexicons (Rao and Ravichandran, 2009; Perez-Rosas et al., 2012; Kaji and Kitsuregawa, 2007), the lexicon-based features were built for sentiment analysis (Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical"
D16-1058,N16-1030,0,0.173152,"fore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification. 1 Neural networks have achieved state-of-the-art performance in a variety of NLP tasks such as machine translation (Lample et al., 2016), paraphrase identification (Yin et al., 2015), question answering (Golub and He, 2016) and text summarization (Rush et al., 2015). However, neural network models are still in infancy to deal with aspectlevel sentiment classification. In some works, target dependent sentiment classification can be benefited from taking into account target information, such as in Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM (TC-LSTM) (Tang et al., 2015a). However, those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect-level clas"
D16-1058,S13-2053,0,0.0134082,"ation is typically considered as a classification problem in the liter607 ature. As we mentioned before, aspect-level sentiment classification is a fine-grained classification task. The majority of current approaches attempt to detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. Traditional approaches to solve those problems are to manually design a set of features. With the abundance of sentiment lexicons (Rao and Ravichandran, 2009; Perez-Rosas et al., 2012; Kaji and Kitsuregawa, 2007), the lexicon-based features were built for sentiment analysis (Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qia"
D16-1058,W04-3253,0,0.0108208,"assification task. The majority of current approaches attempt to detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. Traditional approaches to solve those problems are to manually design a set of features. With the abundance of sentiment lexicons (Rao and Ravichandran, 2009; Perez-Rosas et al., 2012; Kaji and Kitsuregawa, 2007), the lexicon-based features were built for sentiment analysis (Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qian et al., 2015), Recursive Neural Tensor Network (Socher et al., 2013), Recurrent Neural Network (Mikolov et al., 2010; Tang et al., 2015b), LSTM (Hochreiter and Schm"
D16-1058,D14-1162,0,0.121008,"Missing"
D16-1058,perez-rosas-etal-2012-learning,0,0.0156931,"s for sentiment classification briefly. 2.1 Sentiment Classification at Aspect-level Aspect-level sentiment classification is typically considered as a classification problem in the liter607 ature. As we mentioned before, aspect-level sentiment classification is a fine-grained classification task. The majority of current approaches attempt to detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. Traditional approaches to solve those problems are to manually design a set of features. With the abundance of sentiment lexicons (Rao and Ravichandran, 2009; Perez-Rosas et al., 2012; Kaji and Kitsuregawa, 2007), the lexicon-based features were built for sentiment analysis (Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analy"
D16-1058,S14-2004,0,0.8804,"o a specific aspect. We design an aspect-tosentence attention mechanism that can concentrate 606 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606–615, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the key part of a sentence given the aspect. We explore the potential correlation of aspect and sentiment polarity in aspect-level sentiment classification. In order to capture important information in response to a given aspect, we design an attentionbased LSTM. We evaluate our approach on a benchmark dataset (Pontiki et al., 2014), which contains restaurants and laptops data. The main contributions of our work can be summarized as follows: • We propose attention-based Long Short-Term memory for aspect-level sentiment classification. The models are able to attend different parts of a sentence when different aspects are concerned. Results show that the attention mechanism is effective. • Since aspect plays a key role in this task, we propose two ways to take into account aspect information during attention: one way is to concatenate the aspect vector into the sentence hidden representations for computing attention weight"
D16-1058,P15-1132,1,0.401601,"13). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qian et al., 2015), Recursive Neural Tensor Network (Socher et al., 2013), Recurrent Neural Network (Mikolov et al., 2010; Tang et al., 2015b), LSTM (Hochreiter and Schmidhuber, 1997) and Tree-LSTMs (Tai et al., 2015) were applied into sentiment analysis currently. By utilizing syntax structures of sentences, tree-based LSTMs have been proved to be quite effective for many NLP tasks. However, such methods may suffer from syntax parsing errors which are common in resourcelacking languages. LSTM has achieved a great success in various NLP tasks. TD-LSTM and TC-LSTM (Tang et al., 2015a), which took target informat"
D16-1058,E09-1077,0,0.00451146,"ification and neural networks for sentiment classification briefly. 2.1 Sentiment Classification at Aspect-level Aspect-level sentiment classification is typically considered as a classification problem in the liter607 ature. As we mentioned before, aspect-level sentiment classification is a fine-grained classification task. The majority of current approaches attempt to detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. Traditional approaches to solve those problems are to manually design a set of features. With the abundance of sentiment lexicons (Rao and Ravichandran, 2009; Perez-Rosas et al., 2012; Kaji and Kitsuregawa, 2007), the lexicon-based features were built for sentiment analysis (Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networ"
D16-1058,D15-1044,0,0.0568654,"on-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification. 1 Neural networks have achieved state-of-the-art performance in a variety of NLP tasks such as machine translation (Lample et al., 2016), paraphrase identification (Yin et al., 2015), question answering (Golub and He, 2016) and text summarization (Rush et al., 2015). However, neural network models are still in infancy to deal with aspectlevel sentiment classification. In some works, target dependent sentiment classification can be benefited from taking into account target information, such as in Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM (TC-LSTM) (Tang et al., 2015a). However, those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect-level classification. Introduction Sentiment analysis (Nasukawa and Yi, 2003), also known as opinion mining (Liu, 2012), is a key NLP task t"
D16-1058,D11-1014,0,0.00578556,"sentiment analysis (Mohammad et al., 2013). Most of these studies focus on building sentiment classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qian et al., 2015), Recursive Neural Tensor Network (Socher et al., 2013), Recurrent Neural Network (Mikolov et al., 2010; Tang et al., 2015b), LSTM (Hochreiter and Schmidhuber, 1997) and Tree-LSTMs (Tai et al., 2015) were applied into sentiment analysis currently. By utilizing syntax structures of sentences, tree-based LSTMs have been proved to be quite effective for many NLP tasks. However, such methods may suffer from syntax parsing errors which are common in resourcelacking languages. LSTM has achieved a great success in various NLP tasks. TD-LSTM and TC-LSTM (Tang et"
D16-1058,D13-1170,0,0.0441153,"t classifiers with features, which include bag-of-words and sentiment lexicons, using SVM (Mullen and Collier, 2004). However, the results highly depend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qian et al., 2015), Recursive Neural Tensor Network (Socher et al., 2013), Recurrent Neural Network (Mikolov et al., 2010; Tang et al., 2015b), LSTM (Hochreiter and Schmidhuber, 1997) and Tree-LSTMs (Tai et al., 2015) were applied into sentiment analysis currently. By utilizing syntax structures of sentences, tree-based LSTMs have been proved to be quite effective for many NLP tasks. However, such methods may suffer from syntax parsing errors which are common in resourcelacking languages. LSTM has achieved a great success in various NLP tasks. TD-LSTM and TC-LSTM (Tang et al., 2015a), which took target information into consideration, achieved state-of-the-art perfo"
D16-1058,P15-1150,0,0.0439382,"epend on the quality of features. In addition, feature engineering is labor intensive. 2.2 Sentiment Classification with Neural Networks Since a simple and effective approach to learn distributed representations was proposed (Mikolov et al., 2013), neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network (Socher et al., 2011; Dong et al., 2014; Qian et al., 2015), Recursive Neural Tensor Network (Socher et al., 2013), Recurrent Neural Network (Mikolov et al., 2010; Tang et al., 2015b), LSTM (Hochreiter and Schmidhuber, 1997) and Tree-LSTMs (Tai et al., 2015) were applied into sentiment analysis currently. By utilizing syntax structures of sentences, tree-based LSTMs have been proved to be quite effective for many NLP tasks. However, such methods may suffer from syntax parsing errors which are common in resourcelacking languages. LSTM has achieved a great success in various NLP tasks. TD-LSTM and TC-LSTM (Tang et al., 2015a), which took target information into consideration, achieved state-of-the-art performance in target-dependent sentiment classification. TCLSTM obtained a target vector by averaging the vectors of words that the target phrase co"
D16-1058,D15-1167,0,0.614855,"el sentiment classification. 1 Neural networks have achieved state-of-the-art performance in a variety of NLP tasks such as machine translation (Lample et al., 2016), paraphrase identification (Yin et al., 2015), question answering (Golub and He, 2016) and text summarization (Rush et al., 2015). However, neural network models are still in infancy to deal with aspectlevel sentiment classification. In some works, target dependent sentiment classification can be benefited from taking into account target information, such as in Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM (TC-LSTM) (Tang et al., 2015a). However, those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect-level classification. Introduction Sentiment analysis (Nasukawa and Yi, 2003), also known as opinion mining (Liu, 2012), is a key NLP task that receives much attention these years. Aspect-level sentiment analysis is a fine-grained task that can provide complete and in-depth results. In this paper, we deal with aspect-level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect. For exampl"
D19-1321,K16-1002,0,0.102899,"Missing"
D19-1321,W14-4012,0,0.0301727,"Missing"
D19-1321,D16-1032,0,0.363241,"te natural language texts from structured data (Gatt and Krahmer, 2018), which has a wide range of applications (for weather forecast, game report, product description, advertising document, etc.). Most neural methods focus on devising encoding scheme and attention mechanism, namely, (1) exploiting input structure to learn better representation of input data (Lebret et al., 2016; Liu et al., 2018), and (2) devising attention mechanisms to better employ input data (Mei et al., 2016; Liu et al., 2018; Nema et al., 2018) or to dynamically trace which part of input has been covered in generation (Kiddon et al., 2016). These models are able to pro∗ *Corresponding author: Minlie Huang. duce fluent and coherent short texts in some applications. However, to generate long and diverse texts such as product descriptions, existing methods are still unable to capture the complex semantic structures and diversified surface forms of long texts. First, existing methods are not good at modeling input data dynamically during generation. Some neural methods (Kiddon et al., 2016; Feng et al., 2018) propose to record the accumulated attention devoted to each input item. However, these records may accumulate errors in repr"
D19-1321,W04-3250,0,0.0379699,"Missing"
D19-1321,J05-1002,0,0.0980966,"Missing"
D19-1321,W03-1016,0,0.102778,"an be better captured. To capture expression diversity, we devise a hierarchical latent structure which injects variations at both high-level planning and low-level realization. 2 Related Work Traditional methods (Reiter and Dale, 1997; Stent et al., 2004) for data-to-text generation consist of three components: content planning, sentence planning, and surface realization. Content planning and sentence planning are responsible for what to say and how to say respectively; they are typically based on hand-crafted (Kukich, 1983; Dalianis and Hovy, 1993; Hovy, 1993) or automatically-learnt rules (Duboue and McKeown, 2003). Surface realization generates natural language by carrying out the plan, which is template-based (McRoy et al., 2003; van Deemter et al., 2005) or grammar-based (Bateman, 1997; Espinosa et al., 2008). As these models are shallow and the two stages (planning and realization) often function separately, traditional methods are unable to capture rich variations of texts. Recently, neural methods have become the mainstream models for data-to-text generation due to their strong ability of representation learning and scalability. These methods perform well in generating weather forecasts (Mei et al"
D19-1321,P08-1022,0,0.0119656,"methods (Reiter and Dale, 1997; Stent et al., 2004) for data-to-text generation consist of three components: content planning, sentence planning, and surface realization. Content planning and sentence planning are responsible for what to say and how to say respectively; they are typically based on hand-crafted (Kukich, 1983; Dalianis and Hovy, 1993; Hovy, 1993) or automatically-learnt rules (Duboue and McKeown, 2003). Surface realization generates natural language by carrying out the plan, which is template-based (McRoy et al., 2003; van Deemter et al., 2005) or grammar-based (Bateman, 1997; Espinosa et al., 2008). As these models are shallow and the two stages (planning and realization) often function separately, traditional methods are unable to capture rich variations of texts. Recently, neural methods have become the mainstream models for data-to-text generation due to their strong ability of representation learning and scalability. These methods perform well in generating weather forecasts (Mei et al., 2016) or very short biographies (Lebret et al., 2016; Liu 3258 Input Encoder gt p ~z h1 h2 h3 d1 d2 d3 … &lt;SG&gt; bow(g1) dN g1 g2 … group … … hN Plan Decoder bow(gT-1) bow(gT-1) gt … d1 d3 Plan Encoder"
D19-1321,D13-1157,0,0.0233552,"k the ability to model diversity of expressions. As for long text generation, recent studies tackle the incoherence problem from different perspectives. To keep the decoder aware of the crucial information in the already generated prefix, Shao et al. (2017) appended the generated prefix to the encoder, and Guo et al. (2018) leaked the extracted features of the generated prefix from the discriminator to the generator in a Generative Adversarial Nets (Goodfellow et al., 2014). To model dependencies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder. Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data. Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible for large inputs. As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017), post-processing with beam search and rerank (Li et al., 2016), and designing effective models (Xu et al., 2018). Some text-to-text generation models (Serban et al., 2017; Zhao et al., 2017) inject high-level variations wit"
D19-1321,P83-1022,0,0.156408,"n sub-tasks. Thus, input data can be better modeled and inter-sentence coherence can be better captured. To capture expression diversity, we devise a hierarchical latent structure which injects variations at both high-level planning and low-level realization. 2 Related Work Traditional methods (Reiter and Dale, 1997; Stent et al., 2004) for data-to-text generation consist of three components: content planning, sentence planning, and surface realization. Content planning and sentence planning are responsible for what to say and how to say respectively; they are typically based on hand-crafted (Kukich, 1983; Dalianis and Hovy, 1993; Hovy, 1993) or automatically-learnt rules (Duboue and McKeown, 2003). Surface realization generates natural language by carrying out the plan, which is template-based (McRoy et al., 2003; van Deemter et al., 2005) or grammar-based (Bateman, 1997; Espinosa et al., 2008). As these models are shallow and the two stages (planning and realization) often function separately, traditional methods are unable to capture rich variations of texts. Recently, neural methods have become the mainstream models for data-to-text generation due to their strong ability of representation"
D19-1321,D16-1128,0,0.0527432,"Missing"
D19-1321,N16-1014,0,0.128154,"Nets (Goodfellow et al., 2014). To model dependencies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder. Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data. Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible for large inputs. As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017), post-processing with beam search and rerank (Li et al., 2016), and designing effective models (Xu et al., 2018). Some text-to-text generation models (Serban et al., 2017; Zhao et al., 2017) inject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) (Park et al., 2018) is a most similar model to ours, which also adopts a hierarchical latent structure. Our method differs from VHCR in two aspects: (1) VHCR has no planning mechanism, and the global latent variable is mainly designed to address the KL collapse problem, while our global latent variable captures the diversity of reasonable planning; (2) VHCR injects di"
D19-1321,P15-1107,0,0.0287674,": they often generate incoherent texts. In fact, these methods also lack the ability to model diversity of expressions. As for long text generation, recent studies tackle the incoherence problem from different perspectives. To keep the decoder aware of the crucial information in the already generated prefix, Shao et al. (2017) appended the generated prefix to the encoder, and Guo et al. (2018) leaked the extracted features of the generated prefix from the discriminator to the generator in a Generative Adversarial Nets (Goodfellow et al., 2014). To model dependencies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder. Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data. Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible for large inputs. As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017), post-processing with beam search and rerank (Li et al., 2016), and designing effective models (Xu et al., 2018). Some text-to-text g"
D19-1321,N16-1086,0,0.172296,"odel outperforms state-of-theart baselines in long and diverse text generation. 1 Introduction Data-to-text generation is to generate natural language texts from structured data (Gatt and Krahmer, 2018), which has a wide range of applications (for weather forecast, game report, product description, advertising document, etc.). Most neural methods focus on devising encoding scheme and attention mechanism, namely, (1) exploiting input structure to learn better representation of input data (Lebret et al., 2016; Liu et al., 2018), and (2) devising attention mechanisms to better employ input data (Mei et al., 2016; Liu et al., 2018; Nema et al., 2018) or to dynamically trace which part of input has been covered in generation (Kiddon et al., 2016). These models are able to pro∗ *Corresponding author: Minlie Huang. duce fluent and coherent short texts in some applications. However, to generate long and diverse texts such as product descriptions, existing methods are still unable to capture the complex semantic structures and diversified surface forms of long texts. First, existing methods are not good at modeling input data dynamically during generation. Some neural methods (Kiddon et al., 2016; Feng et"
D19-1321,N19-1236,0,0.0457418,"Missing"
D19-1321,N18-1139,0,0.100053,"Missing"
D19-1321,P02-1040,0,0.104109,"the plan decoder, and the plan encoder all have a hidden size of 100. Recipe Text Generation We embedded a multiword title (ingredient) by taking average pooling 4 Our corpus and code are available https://github.com/ZhihongShao/Planning-basedHierarchical-Variational-Model. at of the embeddings of its constituent words. Embedding dimensions for title word and ingredient word are 100 and 200 respectively. The input encoder, the plan decoder, and the plan encoder all have a hidden size of 200. 5.4 Automatic Evaluation Metrics We adopted the following automatic metrics. (1) Corpus BLEU: BLEU-4 (Papineni et al., 2002). (2) Coverage: This metric measures the average proportion of input items that are covered by a generated text. We recognized attribute values (ingredients) with string match heuristics. For the advertising text generation task, synonyms were also considered. (3) Length: The average length of the generated texts. (4) Distinct-4: Distinctn (Li et al., 2016) is a common metric for diversity which measures the ratio of distinct n-grams in generated tokens. We adopted distinct-4. (5) Repetition-4: This metric measures redundancy with the percentage of generated texts that repeat at least one 4-gr"
D19-1321,N18-1162,0,0.0214811,"ully et al. (2019) planned by reordering input data. Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible for large inputs. As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017), post-processing with beam search and rerank (Li et al., 2016), and designing effective models (Xu et al., 2018). Some text-to-text generation models (Serban et al., 2017; Zhao et al., 2017) inject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) (Park et al., 2018) is a most similar model to ours, which also adopts a hierarchical latent structure. Our method differs from VHCR in two aspects: (1) VHCR has no planning mechanism, and the global latent variable is mainly designed to address the KL collapse problem, while our global latent variable captures the diversity of reasonable planning; (2) VHCR injects distinct local latent variables without direct dependencies, while our method explicitly models the dependencies among local latent variables to better capture inter-sentence connections. Shen et al. (2019) proposed ml-VAE-D with multi-level latent va"
D19-1321,D17-1235,0,0.016175,"inter-sentence coherence. et al., 2018; Sha et al., 2018; Nema et al., 2018) using well-designed data encoder and attention mechanisms. However, as demonstrated in Wiseman et al. (2017) (a game report generation task), existing neural methods are still problematic for long text generation: they often generate incoherent texts. In fact, these methods also lack the ability to model diversity of expressions. As for long text generation, recent studies tackle the incoherence problem from different perspectives. To keep the decoder aware of the crucial information in the already generated prefix, Shao et al. (2017) appended the generated prefix to the encoder, and Guo et al. (2018) leaked the extracted features of the generated prefix from the discriminator to the generator in a Generative Adversarial Nets (Goodfellow et al., 2014). To model dependencies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder. Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data. Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible f"
D19-1321,P19-1200,0,0.0801873,"Missing"
D19-1321,P04-1011,0,0.0956437,"ith attractive wording. The goal of writing such texts is to advertise a product and attract users to buy it. • We propose a novel planning mechanism which segments the input data into a sequence of groups, thereby decomposing long text generation into dependent sentence generation sub-tasks. Thus, input data can be better modeled and inter-sentence coherence can be better captured. To capture expression diversity, we devise a hierarchical latent structure which injects variations at both high-level planning and low-level realization. 2 Related Work Traditional methods (Reiter and Dale, 1997; Stent et al., 2004) for data-to-text generation consist of three components: content planning, sentence planning, and surface realization. Content planning and sentence planning are responsible for what to say and how to say respectively; they are typically based on hand-crafted (Kukich, 1983; Dalianis and Hovy, 1993; Hovy, 1993) or automatically-learnt rules (Duboue and McKeown, 2003). Surface realization generates natural language by carrying out the plan, which is template-based (McRoy et al., 2003; van Deemter et al., 2005) or grammar-based (Bateman, 1997; Espinosa et al., 2008). As these models are shallow"
D19-1321,D17-1239,0,0.071664,"Missing"
D19-1321,D18-1428,0,0.0298353,"ies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder. Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data. Most recently, Moryossef et al. (2019) proposed to select plans from all possible ones, which is infeasible for large inputs. As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017), post-processing with beam search and rerank (Li et al., 2016), and designing effective models (Xu et al., 2018). Some text-to-text generation models (Serban et al., 2017; Zhao et al., 2017) inject high-level variations with latent variables. Variational Hierarchical Conversation RNN (VHCR) (Park et al., 2018) is a most similar model to ours, which also adopts a hierarchical latent structure. Our method differs from VHCR in two aspects: (1) VHCR has no planning mechanism, and the global latent variable is mainly designed to address the KL collapse problem, while our global latent variable captures the diversity of reasonable planning; (2) VHCR injects distinct local latent variables without direct depen"
D19-1321,P17-1061,0,0.0524088,"Missing"
D19-1321,P11-1015,0,\N,Missing
D19-1321,P09-1011,0,\N,Missing
D19-1321,P17-1017,0,\N,Missing
D19-1436,P18-1060,0,0.0137772,"the ability to generate more diverse sentences. We also provide standard deviation of each metric in Table 4, reflecting the stability of each model’s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training. 80 60 40 4.5 Dialogue Generation on WeiboDial 20 0 50 100 150 200 250 Epoch Dialogue evaluation is an open problem and existing works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the models. For each pair of responses (one from ARAML and the other from a baseline, given the same input post), five annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gramFigure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training process. The shade area indicates the standard deviation at each data p"
D19-1436,W14-4012,0,0.0815704,"Missing"
D19-1436,P82-1020,0,0.82805,"Missing"
D19-1436,P18-1139,1,0.835699,"ML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likelihood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by refining the objective function (Li et al., 2016; Mou et al., 2016) or modifying the generation distribution with external information like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actorcritic (Bahdanau et al., 2017). (Norouzi et al., 2016) proposed an efficient and stable approach called Reward Augmented Maximum Likelihood (RAML), which connects the log-likelihood and expected rewards to incorporate MLE training objective into RL framework. Since some text generation tasks ha"
D19-1436,N16-1014,0,0.0372014,"butions are mainly as follows: • We analyze the fundamental issue of current GANs for text generation from the perspectives of training instability. • We propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML), which incorporates stable RAML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likelihood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by refining the objective function (Li et al., 2016; Mou et al., 2016) or modifying the generation distribution with external information like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actorcritic (Bahdanau et al."
D19-1436,D17-1230,0,0.15585,"sues of training GANs on discrete data are more severe than exposure bias (Semeniuta1 et al., 2018; Caccia et al., 2018). One of the fundamental issues when generating discrete text samples with GANs is training instability. Updating the generator with policy gradient always leads to an unstable training process because it’s difficult for the generator to derive positive and stable reward signals from the discriminator even with careful pretraining (Che et al., 2017). As a result, the generator gets lost due to the high variance of reward signals and the training process may finally collapse (Li et al., 2017). In this paper, we propose a novel adversarial training framework called Adversarial Reward Augmented Maximum Likelihood (ARAML) to deal with the instability issue of training GANs for text generation. At each iteration of adversarial training, we first train the discriminator to assign higher rewards to real data than to generated samples. Then, inspired by reward augmented maximum likelihood (RAML) (Norouzi et al., 2016), the generator is updated on the samples acquired from a stationary distribution with maximum likelihood estimation (MLE), weighted by the discriminator’s rewards. This sta"
D19-1436,D16-1230,0,0.0162778,"elp of the MLE training objective and has the ability to generate more diverse sentences. We also provide standard deviation of each metric in Table 4, reflecting the stability of each model’s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training. 80 60 40 4.5 Dialogue Generation on WeiboDial 20 0 50 100 150 200 250 Epoch Dialogue evaluation is an open problem and existing works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the models. For each pair of responses (one from ARAML and the other from a baseline, given the same input post), five annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gramFigure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training process. The shade area"
D19-1436,C12-1173,0,0.0538465,"Missing"
D19-1436,C16-1316,0,0.0159788,"ability. • We propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML), which incorporates stable RAML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likelihood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by refining the objective function (Li et al., 2016; Mou et al., 2016) or modifying the generation distribution with external information like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actorcritic (Bahdanau et al., 2017). (Norouzi et al., 2016) proposed an efficient and stable approach called Reward Augmented Maximum Likelihood (RAML), which connects t"
D19-1436,D17-1238,0,0.0155499,"ining objective and has the ability to generate more diverse sentences. We also provide standard deviation of each metric in Table 4, reflecting the stability of each model’s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training. 80 60 40 4.5 Dialogue Generation on WeiboDial 20 0 50 100 150 200 250 Epoch Dialogue evaluation is an open problem and existing works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the models. For each pair of responses (one from ARAML and the other from a baseline, given the same input post), five annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gramFigure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training process. The shade area indicates the standard"
D19-1436,D18-1428,0,0.21573,"Although widely used, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016): during test, the model sequentially predicts the next word conditioned on its previous generated words while during training conditioned on ground-truth words. To tackle this ∗ † Equal contribution Corresponding author: Minlie Huang problem, generative adversarial networks (GAN) with reinforcement learning (RL) training approaches have been introduced to text generation tasks (Yu et al., 2017; Che et al., 2017; Lin et al., 2017; Fedus et al., 2018; Guo et al., 2018; Shi et al., 2018; Xu et al., 2018), where the discriminator is trained to distinguish real and generated text samples to provide reward signals for the generator, and the generator is optimized via policy gradient (Yu et al., 2017). However, recent studies have shown that potential issues of training GANs on discrete data are more severe than exposure bias (Semeniuta1 et al., 2018; Caccia et al., 2018). One of the fundamental issues when generating discrete text samples with GANs is training instability. Updating the generator with policy gradient always leads to an unstable training process because it’s difficult for the gene"
I11-1042,N10-1020,0,0.0237827,"2010) proposed a real-time earthquake detection framework by treating each Twitter user as a sensor. Petrovic et al. (2010) addressed the problem of detecting new events from a stream of Twitter posts and adopted a method based on localitysensitive hashing to make event detection feasible on web-scale corpora. To facilitate fine-grained information extraction on news tweets, Liu et al. (2010) presented a work on semantic role labeling for such texts. Corvey et al. (2010) proposed a work for entity detection and entity class annotation on tweets that were posted during times of mass emergency. Ritter et al. (2010) proposed a topic model to detect conversational threads among tweets. Since a large amount of tweets are posted every day, ranking strategies is extremely important for users to find information quickly. Current ranking strategy on Twitter considers relevance to an input query, information recency (the latest tweets are preferred), and popularity (the retweet times by other users). The recency information, which is useful for real-time web search, has also been explored by Dong et al. (2010) who used fresh URLs present in tweets to rank documents in response to recency sensitive queries. Duan"
I11-1042,N10-1021,0,0.0848092,"Missing"
I11-1042,C10-1034,0,0.253764,"010) proposed a topic model to detect conversational threads among tweets. Since a large amount of tweets are posted every day, ranking strategies is extremely important for users to find information quickly. Current ranking strategy on Twitter considers relevance to an input query, information recency (the latest tweets are preferred), and popularity (the retweet times by other users). The recency information, which is useful for real-time web search, has also been explored by Dong et al. (2010) who used fresh URLs present in tweets to rank documents in response to recency sensitive queries. Duan et al. (2010) proposed a ranking SVM approach to rank tweets with various features. 3 Problem Formulation and Methodology Given a set of queries Q = {q1 , q2 , · · · , qn }, for each query qk , we have a set of short documents Dk = {d1k , d2k , · · · } which are retrieved by our builtin search engine. The document set Dk is partially labeled, i.e., a small portion of documents in Dk 1 2 were annotated with a category set C={1, 2, 3, 4, 5} where 5 means the highest quality and 1 lowest. Therefore, we denote Dk = DkU ∪ DkL , where DkU indicates the unlabeled documents, and DkL the labeled documents. Each doc"
I11-1042,W06-1650,0,0.0386186,"ed several factors on assessing review helpfulness including reviewer characteristics, reviewer history, and review readability and subjectivity. Lu et al. (2010) proposed a linear regression model with various social contexts for review quality prediction. The authors employed author consistency, trust consistency and co-citation consistency hypothesis to predict more consistently. Liu et al. (2008) studied three factors, i.e., reviewer expertise, writing style, and timeliness, and proposed a non-linear regression model with radial basis functions to predict the helpfulness of movie reviews. Kim et al. (2006) used SVM regression with various features to predict review helpfulness. Finding high-quality content and reliable users is also very important for question answering. Agichtein et al. (2008) proposed a classification framework of estimating answer quality. They studied content-based features (e.g. the answer length) and usage-based features derived from question answering communities. Jeon et al. (2006) used nontextual features extracted from the Naver Q&A service to predict the quality of answers. Bian et al. (2009) proposed a mutual reinforcement learning framework to simultaneously predic"
I11-1042,W10-0513,0,0.359402,"lity prediction has been a very important problem in many tasks. In review mining, quality prediction has two lines of research: one line is to detect spam reviews (Jindal and Liu, 2008) or spam reviewers (Lim et al., 2010), which is helpful to exclude misleading information; the other is to identify high-quality reviews, on which we will focus in this survey. Various factors and contexts have been studied to produce reliable and consistent quality prediction. Danescu-Niculescu-Mizil et al. (2009) stud374 ied several factors on helpfulness voting of Amazon product reviews. Ghose and Ipeirotis (2010) studied several factors on assessing review helpfulness including reviewer characteristics, reviewer history, and review readability and subjectivity. Lu et al. (2010) proposed a linear regression model with various social contexts for review quality prediction. The authors employed author consistency, trust consistency and co-citation consistency hypothesis to predict more consistently. Liu et al. (2008) studied three factors, i.e., reviewer expertise, writing style, and timeliness, and proposed a non-linear regression model with radial basis functions to predict the helpfulness of movie rev"
I11-1042,C10-1079,0,\N,Missing
I11-1106,P09-1116,0,0.0131569,"osting her intent generate different questions. In this example, let the corresponding ﬁnal questions be What are hot research topics in NLP and Which research topics are hot in NLP. The replaceable part (such as [subject areas]) can be considered as slots for concrete words. Generally, a slot can be interpreted as a word or a word cluster. A word cluster is a set of words which can be used in similar contexts. For example, Beijing and Paris are in the same cluster since they are both suitable to be used in the context in [cities]. We obtain the word clusters by using k-means as was shown in (Lin and Wu, 2009). Here, we propose a User Inquiry Intent model to describe the process of generating queries and questions from the user intent. Figure 1: The plate representation of the UII model. (as in Probabilistic Graphical Models) The plate representation of UII model is shown in Figure 1. In the model: User Inquiry Intent Model ∙ ? is the index of user intents, ranging from 1 to ? (? is the number of different user intents). A user intent not only corresponds to a distribution over all words but also corresponds to a distribution over the slots ?. ∙ ? and ? are both indices of words, ranging from 1 to"
N10-1072,P98-1012,0,0.133866,"next section we will briefly review the related work. We present our framework for entity linking in section 3. We then describe in section 4 learning to rank methods and features for entity linking. A top1 candidate validation module will be explained in section 5. Experiment results will be discussed in section 6. Finally, we conclude the paper and discusses the future work in section 7. 2 Infoboxes are tables with semi-structured information in some pages of Wikipedia 484 2 Related Work There are a number of studies on named entity disambiguation, which is quite relevant to entity linking. Bagga and Baldwin (1998) used a Bag of Words (BOW) model to resolve ambiguities among people. Mann and Yarowsky (2003) improved the performance of personal names disambiguation by adding biographic features. Fleischman (2004) trained a Maximum Entropy model with Web Features, Overlap Features, and some other features to judge whether two names refer to the same individual. Pedersen (2005) developed features to represent the context of an ambiguous name with the statistically significant bigrams. These methods determined to which entity a specific name refer by measuring the similarity between the context of the speci"
N10-1072,W03-0405,0,0.0202154,"ing in section 3. We then describe in section 4 learning to rank methods and features for entity linking. A top1 candidate validation module will be explained in section 5. Experiment results will be discussed in section 6. Finally, we conclude the paper and discusses the future work in section 7. 2 Infoboxes are tables with semi-structured information in some pages of Wikipedia 484 2 Related Work There are a number of studies on named entity disambiguation, which is quite relevant to entity linking. Bagga and Baldwin (1998) used a Bag of Words (BOW) model to resolve ambiguities among people. Mann and Yarowsky (2003) improved the performance of personal names disambiguation by adding biographic features. Fleischman (2004) trained a Maximum Entropy model with Web Features, Overlap Features, and some other features to judge whether two names refer to the same individual. Pedersen (2005) developed features to represent the context of an ambiguous name with the statistically significant bigrams. These methods determined to which entity a specific name refer by measuring the similarity between the context of the specific name and the context of the entities. They measured similarity with a BOW model. Since the"
N10-1072,W04-0701,0,0.053274,"Missing"
N10-1072,D07-1074,0,0.67023,"ekkerman and McCallum (2005) disambiguated web appearances of people based on the link structure of Web pages. These methods tried to add background knowledge via social networks. Social networks can capture the relatedness between terms, so the problem of a BOW model can be solved to some extent. Xianpei and Jun (2009) proposed to use Wikipedia as the background knowledge for disambiguation. By leveraging Wikipedia’s semantic knowledge like social relatedness between named entities and associative relatedness between concepts, they can measure the similarity between entities more accurately. Cucerzan (2007) and Bunescu (2006) used Wikipedia’s category information in the disambiguation process. Using different background knowledge, researcher may find different efficient features for disambiguation. Hence researchers have proposed so many efficient features for disambiguation. It is important to integrate these features to improve the system performance. Some researchers combine features by manual rules or weights. However, it is not convenient to directly use these rules or weights in another data set. Some researchers also try to use machine learning methods to combine the features. Milne and W"
N10-1072,E06-1002,0,0.421818,"Missing"
N10-1072,P05-1045,0,0.00689089,"the queries which are expansions of Q’s nameString. ∙ ExactEqualSurface. The feature value is 1 if there is a string s in set C’s titleExpand same as the Q’s nameString, or the Candidate C is extracted from the disambiguation page. In other case, the feature value is set to 0. ∙ C’s title represents the title of corresponding Wikipedia article of C. C’s titleExpand represents the union set of the redirect set of C and the anchor text set of C. C’s article represents the Wikipedia article of C. ∙ C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stanford NER (Finkel et al., 2005). Q’s nameEntitySet represents the set of all named entities in Q’s sourceText. ∙ C’s countrySet represents the set of all countries in C’s article, and we detect the countries from text via a manual edited country list. Q’s countrySet represents the set of all countries in Q’s sourceText. C’s countrySetInTitle represents the set of countries exist in one of the string s from C’s titleExpand. ∙ C’s citySetInTitle represents the set of all cities exist in one of the string s from C’s titleExpand, and we detect the cities from text via a manual edited list of famous cities. Q’s citySet represent"
N10-1072,C98-1012,0,\N,Missing
P08-1081,N06-1027,0,0.0136922,". In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and dependency between answer sentences. We also note the existing work on extracting knowledge from discussion threads. Huang et al.(2007) used SVM to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006a) used cosine similarity to match students’ query with reply posts for discussion-bot. Feng et al. (2006b) identified the most important message in online classroom discussion board. Our problem is quite different from the above work. Detecting context for question in forums is related to the context detection problem raised in the QA roadmap paper commissioned by ARDA (Burger et al., 2006). To our knowledge, none of the previous work addresses the problem of context detection. The method of finding follow-up questions (Yang et al., 2006) from TREC context track could be adapted for context d"
P08-1081,W06-1643,0,0.106336,"Missing"
P08-1081,P06-1114,0,0.0132137,"ur knowledge, none of the previous work addresses the problem of context detection. The method of finding follow-up questions (Yang et al., 2006) from TREC context track could be adapted for context detection. However, the followup relationship is limited between questions while context is not. In our other work (Cong et al., 2008), we proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering context detection. Extensive research has been done in questionanswering, e.g. (Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007). They mainly focus on con712 structing answer for certain types of question from a large document collection, and usually apply sophisticated linguistic analysis to both questions and the documents in the collection. Soricut and Brill (2006) used statistical translation model to find the appropriate answers from their QA pair collections from FAQ pages for the posted question. In our scenario, we not only need to find answers for various types of questions in forum threads but also their contexts. 3 Context and Answer Detection A question is a linguistic expression used by"
P08-1081,W03-0430,0,0.00402016,"ndency relationship between sentences. To this end, we proposed a general framework to detect contexts and answers based on Conditional Random Fields (Lafferty et al., 2001) (CRFs) which are able to model the sequential dependencies between contiguous nodes. A CRF is an undirected graphical model G of the conditional distribution P (Y|X). Y are the random variables over the labels of the nodes that are globally conditioned on X, which are the random variables of the observations. (See Section 3.4 for more about CRFs) Linear CRF model has been successfully applied in NLP and text mining tasks (McCallum and Li, 2003; Sha and Pereira, 2003). However, our problem cannot be modeled with Linear CRFs in the same way as other NLP tasks, where one node has a unique label. In our problem, each node (sentence) might have multiple labels since one sentence could be the context of multiple questions in a thread. Thus, it is difficult to find a solution to tag context sentences for all questions in a thread in single pass. Here we assume that questions in a given thread are independent and are found, and then we can label a thread with m questions one-by-one in mpasses. In each pass, one question Qi is selected as f"
P08-1081,N04-4027,0,0.0138336,"ance for context detection. The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed techniques. We evaluate our techniques in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work There is some research on summarizing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and dependency between answer s"
P08-1081,N03-1028,0,0.0615982,"ween sentences. To this end, we proposed a general framework to detect contexts and answers based on Conditional Random Fields (Lafferty et al., 2001) (CRFs) which are able to model the sequential dependencies between contiguous nodes. A CRF is an undirected graphical model G of the conditional distribution P (Y|X). Y are the random variables over the labels of the nodes that are globally conditioned on X, which are the random variables of the observations. (See Section 3.4 for more about CRFs) Linear CRF model has been successfully applied in NLP and text mining tasks (McCallum and Li, 2003; Sha and Pereira, 2003). However, our problem cannot be modeled with Linear CRFs in the same way as other NLP tasks, where one node has a unique label. In our problem, each node (sentence) might have multiple labels since one sentence could be the context of multiple questions in a thread. Thus, it is difficult to find a solution to tag context sentences for all questions in a thread in single pass. Here we assume that questions in a given thread are independent and are found, and then we can label a thread with m questions one-by-one in mpasses. In each pass, one question Qi is selected as focus and each other sent"
P08-1081,C04-1128,0,0.62306,"ing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and dependency between answer sentences. We also note the existing work on extracting knowledge from discussion threads. Huang et al.(2007) used SVM to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006a) used cosine similarity to match students’ query with reply posts for discussion-bot. Feng et al. (2006b) identifie"
P08-1081,C04-1079,0,0.0221809,"achieves better performance for context detection. The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed techniques. We evaluate our techniques in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work There is some research on summarizing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and depen"
P08-1081,P94-1019,0,0.00466449,"BFGS (limited memory Broyden-Fletcher-Goldfarb-Shanno) can be used to optimize objective function Lλ , while for complicated CRFs, Loopy BP are used instead to calculate the marginal probability. 3.5 Features used in CRF models The main features used in Linear CRF models for context detection are listed in Table 3. The similarity feature is to capture the word similarity and semantic similarity between candidate contexts and answers. The word similarity is based on cosine similarity of TF/IDF weighted vectors. The semantic similarity between words is computed based on Wu and Palmer’s measure (Wu and Palmer, 1994) using WordNet (Fellbaum, 1998).1 The similarity between contiguous sentences will be used to capture the dependency for CRFs. In addition, to bridge the lexical gaps between question and context, we learned top-3 context terms for each question term from 300,000 question-description pairs obtained from Yahoo! Answers using mutual information (Berger et al., 2000) ( question description in Yahoo! Answers is comparable to contexts in fo1 The semantic similarity between sentences is calculated as in (Yang et al., 2006). 715 Similarity features: · Cosine similarity with the question · Similarity"
P08-1081,W06-3005,0,0.0882293,"ct input-reply pairs from forums for chatbot knowledge. Feng et al. (2006a) used cosine similarity to match students’ query with reply posts for discussion-bot. Feng et al. (2006b) identified the most important message in online classroom discussion board. Our problem is quite different from the above work. Detecting context for question in forums is related to the context detection problem raised in the QA roadmap paper commissioned by ARDA (Burger et al., 2006). To our knowledge, none of the previous work addresses the problem of context detection. The method of finding follow-up questions (Yang et al., 2006) from TREC context track could be adapted for context detection. However, the followup relationship is limited between questions while context is not. In our other work (Cong et al., 2008), we proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering context detection. Extensive research has been done in questionanswering, e.g. (Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007). They mainly focus on con712 structing answer for certain types of question from a large document c"
P08-1081,P05-1037,0,0.0156498,"ection; 2) Skip-chain CRFs outperform Linear CRFs for answer finding, which demonstrates that context improves answer finding; 3) 2D CRF model improves the performance of Linear CRFs and the combination of 2D CRFs and Skipchain CRFs achieves better performance for context detection. The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed techniques. We evaluate our techniques in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work There is some research on summarizing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarizati"
P09-1083,W05-0620,0,0.0136438,"j |vi ∈ Vs , oj ∈ Vo } corresponds to the Auth(n+1) (vi ) = sen X (n) γ· twij · topic score(j) · Hubtopic (tj ) (2) twij &gt;0 + (1 − γ) · (n) X owij · Hubopinion (oj ) owij &gt;0 (n+1) Hubtopic (ti ) = X twki · Auth(n) sen (vi ) (3) owki · Auth(n) sen (vi ) (4) twki &gt;0 (n+1) Hubopinion (oi ) = X owki &gt;0 740 types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yu and Hatzivassiloglou, 2003)). 2).Topic Set Expansion: The opinion question asks opinions about a particular target. Semantic role labeling based (Carreras and Marquez, 2005) and rule based techniques can be employed to extract this target as topic word. We also expand the topic word with several external knowledge bases: Since all the entity synonyms are redirected into the same page in Wikipedia (Rodrigo et al., 2007), we collect these redirection synonym words to expand topic set. We also collect some related lists as topic words. For example, given question “What reasons did people give for liking Ed Norton’s movies?”, we collect all the Norton’s movies from IMDB as this question’s topic words. Document Retrieval: The PRISE search engine, supported by NIST (Da"
P09-1083,P06-1136,0,0.0610266,"Missing"
P09-1083,W04-3247,0,0.049984,"r, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are"
P09-1083,esuli-sebastiani-2006-sentiwordnet,0,0.00569774,"on of positive/negative Chinese words Words with a positive or negative score above 0.6 Words appeared in both 1 and 2 Words appeared in 1 or 2 All words appeared in 1 or 2 without distinguishing pos or neg Table 1: Sentiment lexicon description For lexicon-based opinion analysis, the selection of opinion thesaurus plays an important role in the final performance. HowNet2 is a knowledge database of the Chinese language, and provides an online word list with tags of positive and negative polarity. We use the English translation of those sentiment words as the sentimental lexicon. SentiWordNet (Esuli and Sebastiani, 2006) is another popular lexical resource for opinion mining. Table 1 shows the detail information of our used sentiment lexicons. In our models, the positive opinion words are used only for positive questions, and negative opinion words just for negative questions. We initially set parameter λ in Opinion PageRank as 0 as (Liu and Ma, 2005), and other parameters simply as 0.5, including µ in Opinion PageRank, γ in Opinion HITS, and α in baseline. The experiment results are shown in Figure 4. We can make three conclusions from Figure 4: 1. Opinion PageRank and Opinion HITS are both effective. The be"
P09-1083,O07-1013,0,0.108741,"ludes this paper and provides possible directions for future work. 2 Related Work Few previous studies have been done on opinion QA. To our best knowledge, (Stoyanov et al., 2005) first created an opinion QA corpus OpQA. They find that opinion QA is a more challenging task than factual question answering, and they point out that traditional fact-based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering"
P09-1083,H05-1115,0,0.156503,"Missing"
P09-1083,H05-1116,0,0.271644,"ihuang,zxy-dcs}@tsinghua.edu.cn Abstract lies?”(Stoyanov et al., 2005) and “Why do people like Subway Sandwiches?” from TAC 2008 (Dang, 2008). Systems designed to deal with such questions are called opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005), directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performances. Similar to other complex QA tasks (Chen et al., 2006; Cui et al., 2007), the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the o"
P09-1083,W03-1017,0,0.27067,"based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders. 3 Our Models for Opinion Sentence Ranking In this section, we formulate the opinion question answering problem as a topic and sentiment based sentence ranking task. In order to naturally integrate the topic and opinion information into the graph based sentence ranking framework, we propose two random walk based models for solving the"
P09-1083,W04-3252,0,\N,Missing
P12-1043,P07-1056,0,0.961927,"assume any predefined rules and 411 labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better res"
P12-1043,P11-1014,0,0.0725534,"domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different"
P12-1043,P07-1033,0,0.0474057,"Missing"
P12-1043,esuli-sebastiani-2006-sentiwordnet,0,0.0825648,"Missing"
P12-1043,P11-1013,0,0.0773877,"tation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different from a recent wo"
P12-1043,D10-1101,0,0.61694,"various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our experiments, we compare our method with theirs, and find that ours can achieve much better results on cross-domain lexicon extraction. Note that our work is also different from a recent work (Du et al., 2010), which focused on identifying the polarity of adjective words by using crossdomain knowledge. While we extract both topic and sentiment words and allow non-adjective sentiment words, which is more practic"
P12-1043,P07-1034,0,0.0851362,". However, their method requires to manually define some general syntactic rules among sentiment and topic words. In addition, it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and 411 labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain to"
P12-1043,P08-2065,0,0.0148754,"Missing"
P12-1043,C10-1074,1,0.375054,"e or negative). The sentiment lexicon is domain dependent as users may use different sentiment words to express their opinion in different domains (e.g., different products). A topic lexicon is a list of topic expressions, on which the sentiment words are expressed. Extracting the topic lexicon from a specific domain is important because users not only care about the overall sentiment polarity of a review but also care about which aspects are mentioned in review. Note that, similar to sentiment lexicons, different domains may have very different topic lexicons. Recently, Jin and Ho (2009) and Li et al. (2010a) showed that supervised learning methods can achieve state-of-the-art results for lexicon extraction. However, the performance of these methods highly relies on manually annotated training data. In most cases, the labeling work may be timeconsuming and expensive. It is impossible to annotate each domain of interest to build precise domaindependent lexicons. It is more desirable to automatically construct precise lexicons in domains of interest by transferring knowledge from other domains. In this paper, we focus on the co-extraction task of sentiment and topic lexicons in a target domain whe"
P12-1043,P04-1035,0,0.0135605,"esults on varying values of M . We also test the sensitivity of the parameter k1 and find that the proposed methods work well and robust when k1 falls in the range from 10 to 20. Figures 5(a) and 5(b) show the results under varying numbers of iterations in the “product vs. movie” task. As we can see, our proposed methods converge well when M ≥ 40. 7 Application: Sentiment Classification To further verify the usefulness of the lexicons extracted by the RAP method, we apply the extracted sentiment lexicon for sentiment classification. 7.1 Experiment Setting Our work is motivated by the work of (Pang and Lee, 2004), which only used subjective sentences for document-level sentiment classification, instead of using all sentences. In this experiment, we only use sentiment related words as features to represent opinion documents for classification, instead of using all words. Our goal is compare the sentiment lexicon constructed by the RAP method with other general lexicons on the impact of for sentiment classification. The general lexicons used for comparison are described in Table 4. We use the dataset from (Blitzer et al., 2007) for sentiment classification. It contains a collection of product reviews fr"
P12-1043,H05-1043,0,0.229081,"emonstrate the effectiveness of our methods. 2 Related Work 2.1 Sentiment or Topic Lexicon Extraction Sentiment or topic lexicon extraction is to identify the sentiment or topic words from text. In the past, many machine learning techniques have been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-"
P12-1043,W03-0404,0,0.0603623,"Missing"
P12-1043,P08-1036,0,0.056369,"e been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. H"
P12-1043,J04-3002,0,0.0107833,"-step domain adaptation framework, with a novel RAP algorithm for seed expansion, is proposed. 3) We conduct extensive evaluation, and the experimental results demonstrate the effectiveness of our methods. 2 Related Work 2.1 Sentiment or Topic Lexicon Extraction Sentiment or topic lexicon extraction is to identify the sentiment or topic words from text. In the past, many machine learning techniques have been proposed for this task. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both prop"
P12-1043,H05-1044,0,0.0445773,"Missing"
P12-1043,D09-1158,0,0.0229503,"it still requires some annotated words in the target domain. In this paper, we do not assume any predefined rules and 411 labeled data be available in the target domain. 2.2 Domain Adaptation Domain adaptation aims at transferring knowledge across domains where data distributions may be different (Pan and Yang, 2010). In the past few years, domain adaptation techniques have been widely applied to various NLP tasks, such as part-of-speech tagging (Ando and Zhang, 2005; Jiang and Zhai, 2007; Daum´e III, 2007), named-entity recognition and shallow parsing (Daum´e III, 2007; Jiang and Zhai, 2007; Wu et al., 2009). There are also lots of studies for cross-domain sentiment analysis (Blitzer et al., 2007; Tan et al., 2007; Li et al., 2009; Pan et al., 2010; Bollegala et al., 2011; He et al., 2011; Glorot et al., 2011). However, most of them focused on coarse-grained document-level sentiment classification, which is different from our fine-grained word-level extraction. Our work is similar to Jakob and Gurevych (2010) which proposed a Conditional Random Field (CRF) for cross-domain topic word extraction. However, the performance of their method highly depends on the manually designed features. In our expe"
P12-1043,D10-1006,0,0.0468868,"ask. Hu and Liu et al. (2004) proposed an association-rule-based method to extract topic words and a dictionary-based method to identify sentiment words, independently. Wiebe et al. (2004) and Rioff et al. (2003) proposed to identify subjective adjectives and nouns using word clustering based on their distributional similarity. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection. Some researchers also proposed to use topic modeling to identify implicit topics and sentiment words (Mei et al., 2007; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010b), where a topic is a cluster of words, which is different from our fine-grained topic-word extraction. Jin and Ho (2009) and Li et al. (2010a) both proposed to use supervised sequential labeling methods for topic and opinion extraction. Experimental results showed that the supervised learning methods can achieve state-of-the-art performance on lexicon extraction. However, these methods need to manually annotate a lot of training data in each domain. Recently, Qiu et al. (2009) proposed a rule-based semi-supervised learning methods for lexicon extraction. However, their metho"
P12-1043,H05-2017,0,\N,Missing
P12-1047,N03-1003,0,0.206645,"natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inner product between them in the feature space induced by all the re-writing"
P12-1047,P08-1077,0,0.0171766,"ncluding the wildcard kernel to facilitate inexact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by t"
P12-1047,I05-5002,0,0.164065,"English Stemmer (http://www.iveonik.com/ ). 455 window sizes k. We also tried to combine the kernels with two lexical features “unigram precision and recall” proposed in (Wan et al., 2006), referred to as PR. For each kernel K, we tested the window size settings of K1 + ... + Kkmax (kmax ∈ {1, 2, 3, 4}) together with the combination with PR and we report the best accuracies of them in Tab 1 and Tab 2. 6.1 Paraphrase Identification The task of paraphrase identification is to examine whether two sentences have the same meaning. We trained and tested all the methods on the MSR Paraphrase Corpus (Dolan and Brockett, 2005; Quirk et al., 2004) consisting of 4,076 sentence pairs for training and 1,725 sentence pairs for testing. The experimental results on different SRKs are shown in Table 1. It can be seen that kb-SRK outperforms ps-SRK and pw-SRK. The results by the state-of-the-art methods reported in previous work are also included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results"
P12-1047,W07-1401,0,0.0352189,"Missing"
P12-1047,W07-1423,0,0.106742,"9) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la450 bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training data consisting of re-writings of strings and their responses ((s1 ,t1 ), y1 ), ..., ((sn ,tn ), yn ) ∈ (Σ∗ × Σ∗ ) × Y i where Σ denot"
P12-1047,N10-1145,0,0.659008,"e. Introduction Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inne"
P12-1047,C08-1066,0,0.0153732,"cation task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la450 bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training dat"
P12-1047,W06-1603,0,0.011707,"of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la450 bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual e"
P12-1047,W04-3219,0,0.0541647,"Missing"
P12-1047,U06-1019,0,0.733099,"Missing"
P12-1047,W07-1412,0,0.4284,"Missing"
P12-1047,U05-1023,0,0.130747,"ritten by Shakespeare. (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. Introduction Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), c"
P12-1047,P09-1053,0,\N,Missing
P14-1050,P98-1010,0,0.147938,"Missing"
P14-1050,C10-1014,1,0.670234,"Missing"
P14-1050,P05-2003,0,0.213804,"Missing"
P14-1050,C02-1049,0,0.0497514,"Missing"
P14-1050,C04-1081,0,0.0372318,"Missing"
P14-1050,W03-1721,0,0.0714455,"Missing"
P14-1050,J11-1002,0,0.181446,"Missing"
P14-1050,W01-0513,0,0.0220238,"Missing"
P14-1050,W03-1719,0,0.0171807,"Missing"
P14-1050,J90-1003,0,0.446175,"Missing"
P14-1050,P12-1027,0,0.0401365,"Missing"
P14-1050,J93-1003,0,0.364587,"Missing"
P14-1050,W03-1730,0,0.0476908,"Missing"
P14-1050,I05-1047,0,0.66772,"Missing"
P14-1050,C98-1010,0,\N,Missing
P14-1050,W03-1726,0,\N,Missing
P15-1132,P13-1088,0,0.0722726,"e of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an adjective phrase. There are two notable works mentioned here: (Socher et al., 2013a) presented to combine the parsing and composition processes, but the purpose is for parsing; (Hermann and Blunsom, 2013) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal improvement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes. As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP). This simple strategy obtains remarkable improvements against strong baselines. ing/JJ’ apparently contributes more to sentiment expression. To address this issue,"
P15-1132,P14-1062,0,0.00735143,"RNN (ours) TE-RNN (ours) TE-RNTN (ours) CNN DCNN Para-Vec for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • RNN. The first Recursive Neural Network model proposed by"
P15-1132,D14-1181,0,0.0442612,"01 and a constant learning rate of 0.005. Method SVM MNB bi-MNB RNN MV-RNN RNTN AdaMC-RNN AdaMC-RNTN DRNN TG-RNN (ours) TE-RNN (ours) TE-RNTN (ours) CNN DCNN Para-Vec for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information i"
P15-1132,D13-1054,1,0.596359,"ie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China *Samsung R&D Institute Beijing, China qianqiaodecember29@126.com , smxtianbo@gmail.com aihuang@tsinghua.edu.cn , yang.liu@samsung.com xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn Abstract semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learning models. Recursive neural network is one of the most successful deep learning models f"
P15-1132,P08-1028,0,0.0144891,"are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursi"
P15-1132,J07-2002,0,0.0165882,"Missing"
P15-1132,P10-1093,0,0.037432,"e making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is perfo"
P15-1132,D11-1014,0,0.70505,"node ‘is very interesting’ is composed from the phrase node ‘very interesting’ and the word node ‘is’ . Introduction Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) may be one of the most popular models. Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including There are various attempts to design the composition function in RNN (or related models). In RNN (Socher et al., 2011), a global matrix is used to linearly combine the elements of vectors. In RNTN (Socher et al., 2013b), a global tensor is used to compute the tensor products of dimensions to favor the association between different el1365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ements of the vectors. Sometimes it is challenging to find a single function to model the composition process. As a"
P15-1132,D12-1110,0,0.459358,"er et al., 2013b), a global tensor is used to compute the tensor products of dimensions to favor the association between different el1365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ements of the vectors. Sometimes it is challenging to find a single function to model the composition process. As an alternative, multiple composition functions can be used. For instance, in MV-RNN (Socher et al., 2012), different matrices is designed for different words though the model is suffered from too much parameters. In AdaMC RNN/RNTN (Dong et al., 2014), a fixed number of composition functions is linearly combined and the weight for each function is adaptively learned. In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ sinc"
P15-1132,P13-1045,0,0.103785,"and Tag-specific Composition Functions in Recursive Neural Network Qiao Qian, Bo Tian, Minlie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China *Samsung R&D Institute Beijing, China qianqiaodecember29@126.com , smxtianbo@gmail.com aihuang@tsinghua.edu.cn , yang.liu@samsung.com xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn Abstract semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learni"
P15-1132,D13-1170,0,0.432546,"and Tag-specific Composition Functions in Recursive Neural Network Qiao Qian, Bo Tian, Minlie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China *Samsung R&D Institute Beijing, China qianqiaodecember29@126.com , smxtianbo@gmail.com aihuang@tsinghua.edu.cn , yang.liu@samsung.com xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn Abstract semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learni"
P15-1132,P12-2018,0,0.00831717,"valuated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Pos./Neg. 79.4 81.8 83.1 82.4 82.9 85.4 87.1 88.5 87.7 86.3 86.8 87.7 88.1 86.8 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level ("
P15-1132,D11-1016,0,0.00905229,"ciently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which e"
P16-1219,P11-1055,0,0.0266318,"Missing"
P16-1219,D15-1082,0,0.288185,"Missing"
P16-1219,Y14-1039,0,0.34255,"Missing"
P16-1219,1993.eamt-1.1,0,0.673487,"Missing"
P16-1219,P15-1009,0,0.161379,"Missing"
P17-1154,W12-3802,0,0.0183136,"of the sentiment value of the modified text (Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006). The shifting hyothesis assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Since each negator can affect the modified text in different ways, the constant amount can be extended to be negatorspecific (Zhu et al., 2014), and further, the effect of negators could also depend on the syntax and semantics of the modified text (Zhu et al., 2014). Other approaches to negation modeling can be seen in (Jia et al., 2009; Wiegand et al., 2010; Benamara et al., 2012; Lapponi et al., 2012). Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensity words can change the valence degree (i.e., sentiment intensity) of the modified text. In (Wei et al., 2011) the authors propose a linear regression model to predict the valence value for content words. In (Malandrakis et al., 2013), a kernel-based model is proposed to combine semantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, a learningto-rank model with a pair-wis"
P17-1154,P14-2063,0,0.0155399,"dge and sentiment resources, such as sentiment lexicons, negation words (not, never, neither, etc.) or negators, and intensity words (very, extremely, etc.) or intensifiers, are useful for sentiment analysis in general. Sentiment lexicon (Hu and Liu, 2004; Wilson et al., 2005) usually defines prior polarity of a lexical entry, and is valuable for lexicon-based models (Turney, 2002; Taboada et al., 2011), and machine learning approaches (Pang and Lee, 2008). There are recent works for automatic construction of sentiment lexicons from social data (Vo and Zhang, 2016) and for multiple languages (Chen and Skiena, 2014). A noticeable work that ultilizes sentiment lexicons can be seen in (Teng et al., 2016) which treats the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words, where the weights are learned by a neural network. Negation words play a critical role in modifying sentiment of textual expressions. Some early negation models adopt the reversing assumption that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006). The shifting hyothesis assumes that negators change the se"
P17-1154,J15-2004,0,0.0184039,"valence value for content words. In (Malandrakis et al., 2013), a kernel-based model is proposed to combine semantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, a learningto-rank model with a pair-wise strategy is proposed to predict sentiment intensity scores (Wang 1680 et al., 2016). Linguistic intensity is not limited to sentiment or intensity words, and there are works that assign low/medium/high intensity scales to adjectives such as okay, good, great (Sharma et al., 2015) or to gradable terms (e.g. large, huge, gigantic) (Shivade et al., 2015). In (Dong et al., 2015), a sentiment parser is proposed, and the authors studied how sentiment changes when a phrase is modified by negators or intensifiers. Applying linguistic regularization to text classification can be seen in (Yogatama and Smith, 2014) which introduces three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. Our work differs in that (Yogatama and Smith, 2014) applies group lasso regularizers to logistic regression on model parameters while our regularizers are applied on intermediate outputs with KL divergence."
P17-1154,P16-1047,0,0.0774501,"cross entropy loss: ∑ ∑∑ L(θ) = − yˆi log yi + α Lt,i + β||θ||2 i i t (4) where yˆi is the gold distribution for sentence i, yi is the predicted distribution, Lt,i is one of the above regularizers or combination of these regularizers on sentence i, α is the weight for the regularization term, and t is the word position in a sentence. Note that we do not consider the modification span of negation and intensity words to preserve the simplicity of the proposed models. Negation scope resolution is another complex problem which has been extensively studied (Zou et al., 2013; Packard et al., 2014; Fancellu et al., 2016), which is beyond the scope of this work. Instead, we resort to sequence LSTMs for encoding surrounding contexts at a given position. 4.1 Non-Sentiment Regularizer (NSR) This regularizer constrains that the sentiment distributions of adjacent positions should not vary much if the additional input word xt is not a sentiment word, formally as follows: (N SR) Lt = max(0, DKL (pt ||pt−1 ) − M ) (5) where M is a hyperparameter for margin, pt is the predicted distribution at state of position t, (i.e., ht ), and DKL (p||q) is a symmetric KL divergence defined as follows: C 1∑ p(l) log q(l) + q(l) lo"
P17-1154,P82-1020,0,0.822845,"Missing"
P17-1154,P15-1162,0,0.0165846,"Missing"
P17-1154,P14-1062,0,0.0819215,"apture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree structures and expensive phrase-level annotation, whose performance drops substantially when only trained with sentence-level annotation. S"
P17-1154,D14-1181,0,0.016407,"e able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree structures and expensive phrase-level annotation, whose performance drops substantially when only trained with s"
P17-1154,D15-1180,0,0.12824,"of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree structures and expensive phrase-level annotation, whose performance drops substantially when only trained with sentence-level annotation. Second, linguistic k"
P17-1154,P14-1007,0,0.0674053,"term into the original cross entropy loss: ∑ ∑∑ L(θ) = − yˆi log yi + α Lt,i + β||θ||2 i i t (4) where yˆi is the gold distribution for sentence i, yi is the predicted distribution, Lt,i is one of the above regularizers or combination of these regularizers on sentence i, α is the weight for the regularization term, and t is the word position in a sentence. Note that we do not consider the modification span of negation and intensity words to preserve the simplicity of the proposed models. Negation scope resolution is another complex problem which has been extensively studied (Zou et al., 2013; Packard et al., 2014; Fancellu et al., 2016), which is beyond the scope of this work. Instead, we resort to sequence LSTMs for encoding surrounding contexts at a given position. 4.1 Non-Sentiment Regularizer (NSR) This regularizer constrains that the sentiment distributions of adjacent positions should not vary much if the additional input word xt is not a sentiment word, formally as follows: (N SR) Lt = max(0, DKL (pt ||pt−1 ) − M ) (5) where M is a hyperparameter for margin, pt is the predicted distribution at state of position t, (i.e., ht ), and DKL (p||q) is a symmetric KL divergence defined as follows: C 1∑"
P17-1154,P05-1015,0,0.676356,"tic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree structures and expensi"
P17-1154,W02-1011,0,0.0268907,"o model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree s"
P17-1154,D14-1162,0,0.112451,"those words that have conflicting sentiment labels, and produce a lexicon of 9, 750 words with 4 sentiment labels. For negation and intensity words, we collect them manually since the number is small, some of which can be seen in Table 2. Dataset # sentences in total #sen containing sentiment word #sen containing negation word #sen containing intensity word MR 10,662 10,446 1,644 2,687 SST 11,885 11,211 1,832 2,472 Table 1: The data statistics. 5.2 The Details of Experiment Setting In order to let others reproduce our results, we present all the details of our models. We adopt Glove vectors (Pennington et al., 2014) as the initial setting of word embeddings V . The shifting vector for each sentiment class (sc ), and the transformation matrices for negation and intensity (Tm ) are initialized with a prior value. The other parameters for hidden layers (W (∗) , U (∗) , S) are initialized with U nif orm(0, 1/sqrt(d)), where d is the dimension of hidden representation, and we set d=300. We adopt adaGrad to train the models, and the learning rate is 0.1. It’s worth noting that, we adopt stochastic gradient descent to update the word embeddings (V ), with a learning rate of 0.2 but without momentum. The optimal"
P17-1154,P15-1132,1,0.127763,"k. In Section 3, we briefly introduce the background of LSTM and bidirectional LSTM, and then describe in detail the lingistic regularizers for sentiment/negation/intensity words in Section 4. Experiments are presented in Section 5, and Conclusion follows in Section 6. 2 Related Work 2.1 Neural Networks for Sentiment Classification There are many neural networks proposed for sentiment classification. The most noticeable models may be the recursive autoencoder neural network which builds the representation of a sentence from subphrases recursively (Socher et al., 2011, 2013; Dong et al., 2014; Qian et al., 2015). Such recursive models usually depend on a tree structure of input text, and in order to obtain competitive results, usually require annotation of all subphrases. Sequence models, for instance, convolutional neural network (CNN), do not require tree-structured data, which are widely adopted for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015). Long short-term memory models are also common for learning sentence-level representation due to its capability of modeling the prefix or suffix context (Hochreiter and Schmidhuber, 1997). LSTM can be commonly applied to"
P17-1154,D15-1300,0,0.0127721,"e modified text. In (Wei et al., 2011) the authors propose a linear regression model to predict the valence value for content words. In (Malandrakis et al., 2013), a kernel-based model is proposed to combine semantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, a learningto-rank model with a pair-wise strategy is proposed to predict sentiment intensity scores (Wang 1680 et al., 2016). Linguistic intensity is not limited to sentiment or intensity words, and there are works that assign low/medium/high intensity scales to adjectives such as okay, good, great (Sharma et al., 2015) or to gradable terms (e.g. large, huge, gigantic) (Shivade et al., 2015). In (Dong et al., 2015), a sentiment parser is proposed, and the authors studied how sentiment changes when a phrase is modified by negators or intensifiers. Applying linguistic regularization to text classification can be seen in (Yogatama and Smith, 2014) which introduces three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. Our work differs in that (Yogatama and Smith, 2014) applies group lasso regularizers to logistic regression on"
P17-1154,D11-1014,0,0.690416,"ensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree structures and expensive phrase-level annotation, whose performance drops substantially when only trained with sentence-level annotation. Second, linguistic knowledge such as sentiment lexicon, negation"
P17-1154,D09-1017,0,0.0241467,"e seen in (Teng et al., 2016) which treats the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words, where the weights are learned by a neural network. Negation words play a critical role in modifying sentiment of textual expressions. Some early negation models adopt the reversing assumption that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006). The shifting hyothesis assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Since each negator can affect the modified text in different ways, the constant amount can be extended to be negatorspecific (Zhu et al., 2014), and further, the effect of negators could also depend on the syntax and semantics of the modified text (Zhu et al., 2014). Other approaches to negation modeling can be seen in (Jia et al., 2009; Wiegand et al., 2010; Benamara et al., 2012; Lapponi et al., 2012). Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensity words can change the v"
P17-1154,D13-1170,0,0.073158,"individual words. Negation/Intensity effect also depends on the syntax and semantics of the modified text, however, for simplicity we resort to sequence LSTM for encoding surrounding contexts in this paper. We partially address the modification scope issue by applying the minimization operator in Eq. 11 and Eq. 14, and the bidirectional LSTM. 5 Experiment 5.1 Dataset and Sentiment Lexicon Two datasets are used for evaluating the proposed models: Movie Review (MR) (Pang and Lee, 2005) where each sentence is annotated with two classes as negative, positive and Stanford Sentiment Treebank (SST) (Socher et al., 2013) with five classes { very negative, negative, neutral, positive, very positive}. Note that SST has provided phrase-level annotation on all inner nodes, but we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation. The sentiment lexicon contains two parts. The first part comes from MPQA (Wilson et al., 2005), which contains 5, 153 sentiment words, each with polarity rating. The second part consists of the leaf nodes of the SST dataset (i.e., all sentiment words) and there are 6, 886 polar words except neural ones. We combine the two parts an"
P17-1154,J11-2001,0,0.643048,"models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM ("
P17-1154,P15-1150,0,0.232855,"y text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), depend on parsing tree structures and expensive phrase-level annotation, whose performance drops substantially when only trained with sentence-level annotation. Second, linguistic knowledge such as sentiment lexicon, negation words or negators (e.g., not, never), and intensity words or intensifiers (e.g., very, abs"
P17-1154,D16-1169,0,0.0339412,"Missing"
P17-1154,P02-1053,0,0.0708854,"ropose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. 1 Introduction Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011), and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005), and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), recursive autoencoders (Socher et al., 2011, 2013), Long ShortTerm Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015), and many more. ∗ Corresponding Author: Minlie Huang In spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoe"
P17-1154,P16-2036,0,0.00635839,"e for Sentiment Classification Linguistic knowledge and sentiment resources, such as sentiment lexicons, negation words (not, never, neither, etc.) or negators, and intensity words (very, extremely, etc.) or intensifiers, are useful for sentiment analysis in general. Sentiment lexicon (Hu and Liu, 2004; Wilson et al., 2005) usually defines prior polarity of a lexical entry, and is valuable for lexicon-based models (Turney, 2002; Taboada et al., 2011), and machine learning approaches (Pang and Lee, 2008). There are recent works for automatic construction of sentiment lexicons from social data (Vo and Zhang, 2016) and for multiple languages (Chen and Skiena, 2014). A noticeable work that ultilizes sentiment lexicons can be seen in (Teng et al., 2016) which treats the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words, where the weights are learned by a neural network. Negation words play a critical role in modifying sentiment of textual expressions. Some early negation models adopt the reversing assumption that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006). The shi"
P17-1154,S16-1080,0,0.0302136,"Missing"
P17-1154,W10-3111,0,0.0157848,"tor reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006). The shifting hyothesis assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Since each negator can affect the modified text in different ways, the constant amount can be extended to be negatorspecific (Zhu et al., 2014), and further, the effect of negators could also depend on the syntax and semantics of the modified text (Zhu et al., 2014). Other approaches to negation modeling can be seen in (Jia et al., 2009; Wiegand et al., 2010; Benamara et al., 2012; Lapponi et al., 2012). Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensity words can change the valence degree (i.e., sentiment intensity) of the modified text. In (Wei et al., 2011) the authors propose a linear regression model to predict the valence value for content words. In (Malandrakis et al., 2013), a kernel-based model is proposed to combine semantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, a learningto-ran"
P17-1154,H05-1044,0,0.433297,"mmon for learning sentence-level representation due to its capability of modeling the prefix or suffix context (Hochreiter and Schmidhuber, 1997). LSTM can be commonly applied to sequential data but also tree-structured data (Zhu et al., 2015; Tai et al., 2015). 2.2 Applying Linguistic Knowledge for Sentiment Classification Linguistic knowledge and sentiment resources, such as sentiment lexicons, negation words (not, never, neither, etc.) or negators, and intensity words (very, extremely, etc.) or intensifiers, are useful for sentiment analysis in general. Sentiment lexicon (Hu and Liu, 2004; Wilson et al., 2005) usually defines prior polarity of a lexical entry, and is valuable for lexicon-based models (Turney, 2002; Taboada et al., 2011), and machine learning approaches (Pang and Lee, 2008). There are recent works for automatic construction of sentiment lexicons from social data (Vo and Zhang, 2016) and for multiple languages (Chen and Skiena, 2014). A noticeable work that ultilizes sentiment lexicons can be seen in (Teng et al., 2016) which treats the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words, where the weights are learned by a n"
P17-1154,P14-1074,0,0.0180569,"h a pair-wise strategy is proposed to predict sentiment intensity scores (Wang 1680 et al., 2016). Linguistic intensity is not limited to sentiment or intensity words, and there are works that assign low/medium/high intensity scales to adjectives such as okay, good, great (Sharma et al., 2015) or to gradable terms (e.g. large, huge, gigantic) (Shivade et al., 2015). In (Dong et al., 2015), a sentiment parser is proposed, and the authors studied how sentiment changes when a phrase is modified by negators or intensifiers. Applying linguistic regularization to text classification can be seen in (Yogatama and Smith, 2014) which introduces three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. Our work differs in that (Yogatama and Smith, 2014) applies group lasso regularizers to logistic regression on model parameters while our regularizers are applied on intermediate outputs with KL divergence. 3 where g (LST M ) is the same as that in Eq (1). Particularly, parameters in the two LSTMs are shared. The representation of the entire sentence − → ← − is [ h n , h 1 ], where n is the length of the sentence. At each position t, the"
P17-1154,P14-1029,0,0.140699,"n. Firstly, we attempts to develop simple models that do not depend on parsing trees and do not require phrase-level annotation which is too expensive in real-world applications. Secondly, in order to obtain competitive performance, simple models can benefit from linguistic resources. Three types of resources will be addressed in this paper: sentiment lexicon, negation words, and intensity words. Sentiment lexicon offers the prior polarity of a word which can be useful in determining the sentiment polarity of longer texts such as phrases and sentences. Negators are typical sentiment shifters (Zhu et al., 2014), which constantly change the polarity of sentiment expression. Intensifiers change the valence degree of the modified text, which is important for fine-grained sentiment classification. In order to model the linguistic role of sentiment, negation, and intensity words, our central idea is to regularize the difference between the predicted sentiment distribution of the current position 1 , and that of the previous or next positions, in a sequence model. For instance, if the cur1 Note that in sequence models, the hidden state of the current position also encodes forward or backward contexts. 167"
P17-1154,D13-1099,0,0.0294612,"e plug a new loss term into the original cross entropy loss: ∑ ∑∑ L(θ) = − yˆi log yi + α Lt,i + β||θ||2 i i t (4) where yˆi is the gold distribution for sentence i, yi is the predicted distribution, Lt,i is one of the above regularizers or combination of these regularizers on sentence i, α is the weight for the regularization term, and t is the word position in a sentence. Note that we do not consider the modification span of negation and intensity words to preserve the simplicity of the proposed models. Negation scope resolution is another complex problem which has been extensively studied (Zou et al., 2013; Packard et al., 2014; Fancellu et al., 2016), which is beyond the scope of this work. Instead, we resort to sequence LSTMs for encoding surrounding contexts at a given position. 4.1 Non-Sentiment Regularizer (NSR) This regularizer constrains that the sentiment distributions of adjacent positions should not vary much if the additional input word xt is not a sentiment word, formally as follows: (N SR) Lt = max(0, DKL (pt ||pt−1 ) − M ) (5) where M is a hyperparameter for margin, pt is the predicted distribution at state of position t, (i.e., ht ), and DKL (p||q) is a symmetric KL divergence de"
P18-1139,K16-1002,0,0.200612,"coding position, and the type distribution will be used in the mixture model of the decoder for final word generation. During the decoding process, the decoder’s state st and the latent variable z are taken as input to estimate the type distribution as follows: P (wt|st , z) = sof tmax(W0 · MLPtype (st , z)) (10) Noticeably, the latent variable z introduced to the RNN encoder-decoder framework often fails to learn a meaningful representation and has little influence on language generation, because the RNN decoder may ignore z during generation, known as the issue of vanishing latent variable (Bowman et al., 2016). By contrast, our model allows z to directly control the word type at each decoding position, which has more influence on language generation. 1502 3.6 Decoder Compared with the traditional decoder described in Section 3.2, our decoder updates the hidden state st with both the input information c and the latent variable z, and generates the response in a mixture form which is combined with the type distribution obtained from the type controller: st = GRU(st−1 , e(yt−1 ), cvt−1 , c, z) (11) P (yt |y&lt;t , c, z) = P (yt |yt−1 , st , c, z) = 3 X P (wt = i|st , z)P (yt |yt−1 , st , c, z, wt = i) i="
P18-1139,W14-4012,0,0.0699369,"Missing"
P18-1139,J90-1003,0,0.596855,"P (Y |z, c)] X = −Eqφ (z|Y,c) [ log P (yt |y&lt;t , z, c)] (16) t 3.7 Loss Function The overall loss L is a linear combination of the KL term L1 , the classification loss of the discriminator L2 , and the generation loss of the decoder L3 : L = αL1 + L2 + L3 (17) We let α gradually increase from 0 to 1. This technique of KL cost annealing can address the optimization challenges of vanishing latent variables in the RNN encoder-decoder (Bowman et al., 2016). 3.8 Topic Word Prediction Topic words play a key role in generating an informative response. We resort to pointwise mutual information (PMI) (Church and Hanks, 1990) for predicting a list of topic words that are relevant to a post. Let x and y indicate a word in a post X and its response Y respectively, and PMI is computed as follows: P M I(x, y) = log P (x, y) P (x)P (y) (18) Then, the relevance score of a topic word to a given post x1 x2 · · · xn can be approximated as follows, similar to (Mou et al., 2016): REL(x1 , ..., xn , y) ≈ n X P M I(xi , y) (19) i=1 During training, the words in a response with high REL scores to the post are treated as topic words. During test, we use REL to select the top ranked words as topic words for a post. 4 Experiment 4"
P18-1139,E17-1059,0,0.0200574,"n be used to avoid stalemates (Li et al., 2016b), which can be viewed as important proactive behaviors in conversation (Yu et al., 2016). Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for different purposes within different contexts, behave more proactively, and may lead the dialogue to go further. Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation (Hu et al., 2017; Ficler and Goldberg, 2017; Asghar et al., 2017; Ghosh et al., 2017; Zhou and Wang, 2017; Dong et al., 2017; Murakami et al., 2017). These studies, involving the control of sentiment polarity, emotion, or tense, fall into local control, more or less, because the controllable variable can be locally re1 Note that we did not include the exclamatory category in this paper because an exclamatory sentence in conversation is only a strong emotional expression of the original sentence with few changes. 1499 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1499–1508 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Lin"
P18-1139,W17-4912,0,0.240126,"make statements to state or explain something.1 Interrogative and imperative responses can be used to avoid stalemates (Li et al., 2016b), which can be viewed as important proactive behaviors in conversation (Yu et al., 2016). Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for different purposes within different contexts, behave more proactively, and may lead the dialogue to go further. Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation (Hu et al., 2017; Ficler and Goldberg, 2017; Asghar et al., 2017; Ghosh et al., 2017; Zhou and Wang, 2017; Dong et al., 2017; Murakami et al., 2017). These studies, involving the control of sentiment polarity, emotion, or tense, fall into local control, more or less, because the controllable variable can be locally re1 Note that we did not include the exclamatory category in this paper because an exclamatory sentence in conversation is only a strong emotional expression of the original sentence with few changes. 1499 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1499–1508 c"
P18-1139,P17-1059,0,0.19224,"Interrogative and imperative responses can be used to avoid stalemates (Li et al., 2016b), which can be viewed as important proactive behaviors in conversation (Yu et al., 2016). Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for different purposes within different contexts, behave more proactively, and may lead the dialogue to go further. Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation (Hu et al., 2017; Ficler and Goldberg, 2017; Asghar et al., 2017; Ghosh et al., 2017; Zhou and Wang, 2017; Dong et al., 2017; Murakami et al., 2017). These studies, involving the control of sentiment polarity, emotion, or tense, fall into local control, more or less, because the controllable variable can be locally re1 Note that we did not include the exclamatory category in this paper because an exclamatory sentence in conversation is only a strong emotional expression of the original sentence with few changes. 1499 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1499–1508 c Melbourne, Australia, July 15 - 20, 2018"
P18-1139,P82-1020,0,0.846242,"Missing"
P18-1139,P15-1152,0,0.318681,"Missing"
P18-1139,N16-1014,0,0.681453,"u have at breakfast? Let’s have dinner together! tions, but surprisingly, this problem is rather untouched in dialogue systems. As shown in Figure 1, responses with different functions can be used to achieve different conversational purposes: Interrogative responses can be used to acquire further information from the user; imperative responses are used to make requests, directions, instructions or invitations to elicit further interactions; and declarative responses commonly make statements to state or explain something.1 Interrogative and imperative responses can be used to avoid stalemates (Li et al., 2016b), which can be viewed as important proactive behaviors in conversation (Yu et al., 2016). Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for different purposes within different contexts, behave more proactively, and may lead the dialogue to go further. Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation (Hu et al., 2017; Ficler and Goldberg, 2017; Asghar et al., 2017; Ghosh et al., 2017; Zhou and Wang, 2017; Dong et al., 2017; Murakami et al., 2017). The"
P18-1139,C16-1316,0,0.566909,"., 2018; Zhou and Wang, 2017), and was for past tense (Hu et al., 2017). By contrast, sentence function is a global attribute of text, and controlling sentence function is more challenging in that it requires to adjust the global structure of the entire text, including changing word order and word patterns. Controlling sentence function in conversational systems faces another challenge: in order to generate informative and meaningful responses, it has to deal with the compatibility of the sentence function and the content. Similar to most existing neural conversation models (Li et al., 2016a; Mou et al., 2016; Xing et al., 2017), we are also struggling with universal and meaningless responses for different sentence functions, e.g., “Is that right?” for interrogative responses, “Please!” for imperative responses and “Me, too.” for declarative responses. The lack of meaningful topics in responses will definitely degrade the utility of the sentence function so that the desired conversational purpose can not be achieved. Thus, the task needs to generate responses with both informative content and controllable sentence functions. In this paper, we propose a conversation generation model to deal with th"
P18-1139,W16-3649,0,0.0204493,"rather untouched in dialogue systems. As shown in Figure 1, responses with different functions can be used to achieve different conversational purposes: Interrogative responses can be used to acquire further information from the user; imperative responses are used to make requests, directions, instructions or invitations to elicit further interactions; and declarative responses commonly make statements to state or explain something.1 Interrogative and imperative responses can be used to avoid stalemates (Li et al., 2016b), which can be viewed as important proactive behaviors in conversation (Yu et al., 2016). Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for different purposes within different contexts, behave more proactively, and may lead the dialogue to go further. Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation (Hu et al., 2017; Ficler and Goldberg, 2017; Asghar et al., 2017; Ghosh et al., 2017; Zhou and Wang, 2017; Dong et al., 2017; Murakami et al., 2017). These studies, involving the control of sentiment polarity, emotion, or tense, fall into loca"
P18-1139,P17-1061,0,0.293087,"ed from the recognition network which is supervised by the function label in the discriminator. In the type controller, the latent variable and the decoder’s state are used to estimate a type distribution which modulates the final generation distribution. During test, z is sampled from the prior network whose input is only the post. The response encoder in the dotted box appears only in training. plan the words globally to realize the function type to be controlled. (2) The compatibility of controllable variables and content quality is less studied in the literature. The most similar work in (Zhao et al., 2017) proposed to control the dialogue act of a response, which is also a global attribute. However, the model controls dialog act by directly feeding a latent variable into the decoder, instead, our model has a stronger control on the generation process via a type controller in which words of different types are concretely modeled. 3 3.1 Model Task Definition and Model Overview Our problem is formulated as follows: given a post X = x1 x2 · · · xn and a sentence function category l, our task is to generate a response Y = y1 y2 · · · ym that is not only coherent with the specified function category"
W04-1204,J95-4004,0,0.212793,"Missing"
W08-1806,P06-1112,0,0.161553,"tsinghua.edu.cn Abstract search. With automatic answer validation, the system will carry out different refinements of its searching criteria to check the relevance of new candidate answers. In addition, since most of QA systems rely on complex architectures and the evaluation of their performances requires a huge amount of work, the automatic assessment of candidates with respect to a given question will speed up both algorithm refinement and testing. Currently, answer validation is mainly viewed as a classification problem or ranking problem. Different models, such as Support Vector Machine (Shen and Klakow, 2006) and Maximum Entropy Model (Ittycheriah et al., 2001), are used to integrate sophisticated linguistic features to determine the correctness of candidates. The answer validation exercise (Penas et al. , 2007) aims at developing systems able to decide whether the answer is correct or not. They formulate answer validation as a text entailment problem. These approaches are dependent on sophisticated linguistic analysis of syntactic and semantic relations between question and candidates. It is quite expensive to use deep analysis for automatic answer validation, especially in large scale data set."
W08-1806,N01-1005,0,0.0380267,"swer validation, the system will carry out different refinements of its searching criteria to check the relevance of new candidate answers. In addition, since most of QA systems rely on complex architectures and the evaluation of their performances requires a huge amount of work, the automatic assessment of candidates with respect to a given question will speed up both algorithm refinement and testing. Currently, answer validation is mainly viewed as a classification problem or ranking problem. Different models, such as Support Vector Machine (Shen and Klakow, 2006) and Maximum Entropy Model (Ittycheriah et al., 2001), are used to integrate sophisticated linguistic features to determine the correctness of candidates. The answer validation exercise (Penas et al. , 2007) aims at developing systems able to decide whether the answer is correct or not. They formulate answer validation as a text entailment problem. These approaches are dependent on sophisticated linguistic analysis of syntactic and semantic relations between question and candidates. It is quite expensive to use deep analysis for automatic answer validation, especially in large scale data set. Thus it is appropriate to find an alternative solutio"
W08-1806,W02-1033,0,0.0911347,"Missing"
W08-1806,N04-1007,0,0.0450021,"Missing"
W09-1312,W97-0702,0,0.0923059,"c evaluation by measuring the amount of overlap between the machine-selected sentences and human-written summaries. Our metric for the evaluation was ROUGE 1 , a widely used intrinsic summarization evaluation metric. 2 Related Work Summarization systems aim to extract salient text fragments, especially sentences, from the original documents to form a summary. A number of methods for sentence scoring and ranking have been developed. Approaches based on sentence position (Edmundson, 1969), cue phrase (McKeown and Radev, 1995), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997) have been reported. Radev et al. (Radev et al., 2004) developed an extractive multidocument summarizer, MEAD, which extracts a summary from multiple documents based on the document cluster centroid, position and firstsentence overlap. Recently, graph-based ranking methods, such as LexPageRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), 1 http://haydn.isi.edu/ROUGE/ 98 have been proposed for multi-document summarization. Similar to the original PageRank algorithm, these methods make use of similarity relationships between sentences and then rank sentences according to the"
W09-1312,W04-3247,0,0.248938,"by this gene is a receptor for interleukin 20 (IL20), a cytokine that may be involved in epidermal function. The receptor of IL20 is a heterodimeric receptor complex consisting of this protein and interleukin 20 receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The expression of both genes is found to be upregulated in Psoriasis. Table1. Two examples of human-written gene summaries not include enough informative words for gene summaries. Next, the remaining sentences are ranked by the sum of two individual scores: a) an authority score from a lexical PageRank algorithm (Erkan and Radev, 2004) and b) a similarity score between the sentence and the Gene Ontology (GO) terms with which the gene is annotated (To date, over 190,000 genes have two or more associated GO terms). Finally, redundant sentences are removed and top ranked sentences are nominated for the target gene. In order to evaluate our system, we assembled a gold standard dataset consisting of handwritten summaries for 7,294 human genes and conducted an intrinsic evaluation by measuring the amount of overlap between the machine-selected sentences and human-written summaries. Our metric for the evaluation was ROUGE 1 , a wi"
W09-1312,P07-2049,0,0.272715,"Missing"
W09-1312,C00-1072,0,0.448319,"d an extractive multidocument summarizer, MEAD, which extracts a summary from multiple documents based on the document cluster centroid, position and firstsentence overlap. Recently, graph-based ranking methods, such as LexPageRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), 1 http://haydn.isi.edu/ROUGE/ 98 have been proposed for multi-document summarization. Similar to the original PageRank algorithm, these methods make use of similarity relationships between sentences and then rank sentences according to the “votes” or “recommendations” from their neighboring sentences. Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. Afterwards, this technique was successfully used in a number of summarization systems (Hickl et al., 2007, Gupta and Nenkova et al., 2007). In order to improve sentence selection, we adopted the idea in a similar way to identify terms that tend to appear frequently in gene summaries and subsequently filter sentences that include none or few such terms. Compared with newswire document summarization, much less attention has been paid to summarizing MEDLINE documents for genic information. Ling et al. (Ling et al"
W09-1312,N03-1020,0,0.0890458,"with a preexisting handwritten summary downloaded from the NCBI’s FTP site5. The handwritten summaries were used as reference summaries (i.e. a gold standard) to compare with the automatically generated summaries. Although the length of reference summaries varies, the majority of these summaries contain 80 to 120 words. To produce a summary of similar length, we decided to select five sentences consisting of about 100 words. For the intrinsic evaluation of a large number of summaries, we made use of the ROUGE metrics that has been widely used in automatic evaluation of summarization systems (Lin and Hovy, 2003; Hickl et al., 2007). It provides a set of evaluation metrics to measure the quality of a summary by counting overlapping units such as n-grams or word sequences between the generated summary and its reference summary. 5 102 Evaluation Metrics ftp://ftp.ncbi.nih.gov/gene/DATA/ASN_BINARY/ We computed three ROUGE measures for each summary, namely ROUGE-1 (unigram based), ROUGE-2 (bigram based) and ROUGE-SU4 (skip-bigram and unigram) (Lin and Hovy, 2003). Among them, ROUGE-1 has been shown to agree most with human judgments (Lin and Hovy, 2003). However, as biomedical concepts usually contain mo"
W09-1312,W04-3252,0,\N,Missing
W10-1902,P08-1081,1,0.838227,"2) improving matching 11 to enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs. To the best of our knowledge, no previous work has been done on using non-linear-chain CRF in the biomedical NER task. Beyond the biomedical domain, skip-chain CRF has been used in several studies to model long distance dependency. In (Galley, 2006), skip edges were linked between sentences with nonlocal pragmatic dependencies to rank meetings. In (Ding et al., 2008), skip-chain CRF was used to detect the context and answers from online forums. The most close work to ours was in (Sutton and McCallum, 2004), which used skip-chain CRF to extract information from email messages announcing seminars. By linking the same words whose initial letter is capital, the method obtained improvements on extracting speakers’ name. Our work is in the spirit of this idea, but we approach it in a different way. We found that the problem is much more difficult in the biomedical NER task: that is why we systematically studied the principles of linking skip edges and the quali"
W10-1902,P05-1045,0,0.0502115,"rom the two directions. Huang et al. (2007) combines a linear-chain CRF and two SVM models Related work NER is a widely studied topic in text mining research, and many new challenges are seen in domain-specific applications, such as biomedical NER (Zhou et al., 2004). The dictionary based method is a common technique as biomedical thesauruses play a key role in understanding such text. Most dictionary based NER systems focused on: (1) integrating and normalizing different biomedical databases to improve the quality of the dictionary to be used; (2) improving matching 11 to enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction. However, the CRF models used in these systems were all linear-chain CRFs. To the best of our knowledge, no previous work has been done on using non-linear-chain CRF in the biomedical NER task. Beyond the biomedical domain, skip-chain CRF has been used in several studies to model long distance dependency. In (Galley, 2006), skip edges were linked between sentences with nonlocal pragmatic dependencies to rank meetings. In (Ding et al., 2008), skip-chain CRF was used to detect the context an"
W10-1902,N03-1028,0,0.0606232,"Department of Computer Science and Technology Tsinghua University, Beijing 100084, China liu-jc04@mails.tsinghua.edu.cn {aihuang, zxy-dcs}@tsinghua.edu.cn 2004 and BioCreAtIvE II in 20062 . The overview reports from these competitions, presenting stateof-the-art of biomedical NER studies, show that linear-chain Conditional Random Fields (CRF) is one of the most commonly used models and has the most competitive results (Yeh et al., 2005; Smith et al., 2008). Linear-chain CRF has also been successfully applied to other NLP tasks such as POS-tagging (Lafferty et al., 2001) and sentence chunking (Sha and Pereira, 2003). However, in most of these applications, only linear-chain CRF was fully exploited, assuming that only adjacent words are inter-dependent. The dependency between distant words, which occurs frequently in the biomedical literature, is yet to be captured. In the biomedical literature, the repeated appearance of same or similar words in one sentence is a common type of long distance dependencies. This phenomenon is due to the complicated syntactic structures and the various biomedical terminologies in nature. See the following example: Abstract Linear-chain Conditional Random Fields (CRF) has be"
W10-1902,W06-1643,0,\N,Missing
W10-1902,W02-0301,0,\N,Missing
