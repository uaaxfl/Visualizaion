2021.teachingnlp-1.11,A Balanced and Broadly Targeted Computational Linguistics Curriculum,2021,-1,-1,2,1,793,emma manning,Proceedings of the Fifth Workshop on Teaching NLP,0,"This paper describes the primarily-graduate computational linguistics and NLP curriculum at Georgetown University, a U.S. university that has seen significant growth in these areas in recent years. We reflect on the principles behind our curriculum choices, including recognizing the various academic backgrounds and goals of our students; teaching a variety of skills with an emphasis on working directly with data; encouraging collaboration and interdisciplinary work; and including languages beyond English. We reflect on challenges we have encountered, such as the difficulty of teaching programming skills alongside NLP fundamentals, and discuss areas for future growth."
2021.scil-1.34,{CCG} Supertagging as Top-down Tree Generation,2021,-1,-1,2,1,2245,jakob prange,Proceedings of the Society for Computation in Linguistics 2021,0,None
2021.scil-1.56,{SNACS} Annotation of Case Markers and Adpositions in {H}indi,2021,-1,-1,3,1,2276,aryaman arora,Proceedings of the Society for Computation in Linguistics 2021,0,None
2021.scil-1.58,Supersense and Sensibility: Proxy Tasks for Semantic Annotation of Prepositions,2021,-1,-1,3,0.897436,2266,luke gessler,Proceedings of the Society for Computation in Linguistics 2021,0,None
2021.mwe-1.6,Lexical Semantic Recognition,2021,-1,-1,4,0,4927,nelson liu,Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021),0,"In lexical semantics, full-sentence segmentation and segment labeling of various phenomena are generally treated separately, despite their interdependence. We hypothesize that a unified lexical semantic recognition task is an effective way to encapsulate previously disparate styles of annotation, including multiword expression identification / classification and supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence tagger and evaluate its performance along various axes of annotation. As the label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally evaluate how well the model generalizes to those test sets, finding that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes baseline models and evaluation metrics for integrated and accurate modeling of lexical semantics, facilitating future work in this area."
2021.law-1.6,Classifying Divergences in Cross-lingual {AMR} Pairs,2021,-1,-1,2,1,2283,shira wein,Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,0,"Translation divergences are varied and widespread, challenging approaches that rely on parallel text. To annotate translation divergences, we propose a schema grounded in the Abstract Meaning Representation (AMR), a sentence-level semantic framework instantiated for a number of languages. By comparing parallel AMR graphs, we can identify specific points of divergence. Each divergence is labeled with both a type and a cause. We release a small corpus of annotated English-Spanish data, and analyze the annotations in our corpus."
2021.law-1.10,Subcategorizing Adverbials in {U}niversal {C}onceptual {C}ognitive {A}nnotation,2021,-1,-1,3,0,5444,zhuxin wang,Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,0,"Universal Conceptual Cognitive Annotation (UCCA) is a semantic annotation scheme that organizes texts into coarse predicate-argument structure, offering broad coverage of semantic phenomena. At the same time, there is still need for a finer-grained treatment of many of the categories. The Adverbial category is of special interest, as it covers a wide range of fundamentally different meanings such as negation, causation, aspect, and event quantification. In this paper we introduce a refinement annotation scheme for UCCA{'}s Adverbial category, showing that UCCA Adverbials can indeed be subcategorized into at least 7 semantic types, and doing so can help clarify and disambiguate the otherwise coarse-grained labels. We provide a preliminary set of annotation guidelines, as well as pilot annotation experiments with high inter-annotator agreement, confirming the validity of the scheme."
2021.findings-emnlp.423,Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets,2021,-1,-1,3,1,4928,michael kranzlein,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"For interpreting the behavior of a probabilistic model, it is useful to measure a model{'}s calibration{---}the extent to which it produces reliable confidence scores. We address the open problem of calibration for tagging models with sparse tagsets, and recommend strategies to measure and reduce calibration error (CE) in such models. We show that several post-hoc recalibration techniques all reduce calibration error across the marginal distribution for two existing sequence taggers. Moreover, we propose tag frequency grouping (TFG) as a way to measure calibration error in different frequency bands. Further, recalibrating each group separately promotes a more equitable reduction of calibration error across the tag frequency spectrum."
2021.eval4nlp-1.12,Referenceless Parsing-Based Evaluation of {AMR}-to-{E}nglish Generation,2021,-1,-1,2,1,793,emma manning,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2021.emnlp-main.806,Putting Words in {BERT}{'}s Mouth: Navigating Contextualized Vector Spaces with Pseudowords,2021,-1,-1,3,0,9530,taelin karidi,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized {``}pseudoword{''} vector as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a set of carefully constructed sentences targeting highly ambiguous English words, we find substantial regularity in the contextualized space, with regions that correspond to distinct word senses; but between these regions there are occasionally {``}sense voids{''}{---}regions that do not correspond to any intelligible sense."
2021.blackboxnlp-1.43,{BERT} Has Uncommon Sense: Similarity Ranking for Word Sense {BERT}ology,2021,-1,-1,2,0.897436,2266,luke gessler,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"An important question concerning contextualized word embedding (CWE) models like BERT is how well they can represent different word senses, especially those in the long tail of uncommon senses. Rather than build a WSD system as in previous work, we investigate contextualized embedding neighborhoods directly, formulating a query-by-example nearest neighbor retrieval task and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations."
2021.acl-long.257,"Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of {AMR} Alignments",2021,-1,-1,2,1,13081,austin blodgett,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation."
2020.lrec-1.733,A Corpus of Adpositional Supersenses for {M}andarin {C}hinese,2020,39,0,6,0,2267,siyao peng,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Adpositions are frequent markers of semantic relations, but they are highly ambiguous and vary significantly from language to language. Moreover, there is a dearth of annotated corpora for investigating the cross-linguistic variation of adposition semantics, or for building multilingual disambiguation systems. This paper presents a corpus in which all adpositions have been semantically annotated in Mandarin Chinese; to the best of our knowledge, this is the first Chinese corpus to be broadly annotated with adposition semantics. Our approach adapts a framework that defined a general set of supersenses according to ostensibly language-independent semantic criteria, though its development focused primarily on English prepositions (Schneider et al., 2018). We find that the supersense categories are well-suited to Chinese adpositions despite syntactic differences from English. On a Mandarin translation of The Little Prince, we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext."
2020.law-1.10,{PASTRIE}: A Corpus of Prepositions Annotated with Supersense Tags in {R}eddit International {E}nglish,2020,-1,-1,6,1,4928,michael kranzlein,Proceedings of the 14th Linguistic Annotation Workshop,0,"We present the Prepositions Annotated with Supsersense Tags in Reddit International English ({``}PASTRIE{''}) corpus, a new dataset containing manually annotated preposition supersenses of English data from presumed speakers of four L1s: English, French, German, and Spanish. The annotations are comprehensive, covering all preposition types and tokens in the sample. Along with the corpus, we provide analysis of distributional patterns across the included L1s and a discussion of the influence of L1s on L2 preposition choice."
2020.law-1.11,Supersense and Sensibility: Proxy Tasks for Semantic Annotation of Prepositions,2020,-1,-1,3,0.897436,2266,luke gessler,Proceedings of the 14th Linguistic Annotation Workshop,0,"Prepositional supersense annotation is time-consuming and requires expert training. Here, we present two sensible methods for obtaining prepositional supersense annotations indirectly by eliciting surface substitution and similarity judgments. Four pilot studies suggest that both methods have potential for producing prepositional supersense annotations that are comparable in quality to expert annotations."
2020.law-1.12,Sprucing up Supersenses: Untangling the Semantic Clusters of Accompaniment and Purpose,2020,-1,-1,2,0.59271,7749,jena hwang,Proceedings of the 14th Linguistic Annotation Workshop,0,"We reevaluate an existing adpositional annotation scheme with respect to two thorny semantic domains: accompaniment and purpose. {`}Accompaniment{'} broadly speaking includes two entities situated together or participating in the same event, while {`}purpose{'} broadly speaking covers the desired outcome of an action, the intended use or evaluated use of an entity, and more. We argue the policy in the SNACS scheme for English should be recalibrated with respect to these clusters of interrelated meanings without adding complexity to the overall scheme. Our analysis highlights tradeoffs in lumping vs. splitting decisions as well as the flexibility afforded by the construal analysis."
2020.dmr-1.6,K-{SNACS}: Annotating {K}orean Adposition Semantics,2020,-1,-1,4,0.59271,7749,jena hwang,Proceedings of the Second International Workshop on Designing Meaning Representations,0,"While many languages use adpositions to encode semantic relationships between content words in a sentence (e.g., agentivity or temporality), the details of how adpositions work vary widely across languages with respect to both form and meaning. In this paper, we empirically adapt the SNACS framework (Schneider et al., 2018) to Korean, a language that is typologically distant from English{---}the language SNACS was based on. We apply the SNACS framework to annotate the highly popular novellaThe Little Prince with semantic supersense labels over allKorean postpositions. Thus, we introduce the first broad-coverage corpus annotated with Korean postposition semantics and provide a detailed analysis of the corpus with an apples-to-apples comparison between Korean and English annotations"
2020.coling-tutorials.1,Cross-lingual Semantic Representation for {NLP} with {UCCA},2020,-1,-1,5,0,3239,omri abend,Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts,0,"This is an introductory tutorial to UCCA (Universal Conceptual Cognitive Annotation), a cross-linguistically applicable framework for semantic representation, with corpora annotated in English, German and French, and ongoing annotation in Russian and Hebrew. UCCA builds on extensive typological work and supports rapid annotation. The tutorial will provide a detailed introduction to the UCCA annotation guidelines, design philosophy and the available resources; and a comparison to other meaning representations. It will also survey the existing parsing work, including the findings of three recent shared tasks, in SemEval and CoNLL, that addressed UCCA parsing. Finally, the tutorial will present recent applications and extensions to the scheme, demonstrating its value for natural language processing in a range of languages and domains."
2020.coling-main.264,Comparison by Conversion: Reverse-Engineering {UCCA} from Syntax and Lexical Semantics,2020,-1,-1,2,0.590435,375,daniel hershcovich,Proceedings of the 28th International Conference on Computational Linguistics,0,"Building robust natural language understanding systems will require a clear characterization of whether and how various linguistic meaning representations complement each other. To perform a systematic comparative analysis, we evaluate the mapping between meaning representations from different frameworks using two complementary methods: (i) a rule-based converter, and (ii) a supervised delexicalized parser that parses to one framework using only information from the other as features. We apply these methods to convert the STREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a graph-structured full-sentence meaning representation). Both methods yield surprisingly accurate target representations, close to fully supervised UCCA parser quality{---}indicating that UCCA annotations are partially redundant with STREUSLE annotations. Despite this substantial convergence between frameworks, we find several important areas of divergence."
2020.coling-main.420,A Human Evaluation of {AMR}-to-{E}nglish Generation Systems,2020,23,0,3,1,793,emma manning,Proceedings of the 28th International Conference on Computational Linguistics,0,"Most current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as BLEU, which are known to be problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems."
2020.acl-main.462,({R}e)construing Meaning in {NLP},2020,115,0,4,0,13493,sean trott,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Human speakers have an extensive toolkit of ways to express themselves. In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding{---}namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed. We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics. We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP."
2020.acl-main.696,Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in {H}indi and {P}unjabi,2020,13,0,3,1,2276,aryaman arora,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification."
W19-3316,Preparing {SNACS} for Subjects and Objects,2019,0,2,3,0,24475,adi shalev,Proceedings of the First International Workshop on Designing Meaning Representations,0,"Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by morphosyntax. Here, we ask whether this approach can be generalized beyond adpositions and possessives to cover all scene participants{---}including subjects and objects{---}directly, without reference to a frame lexicon. We present new guidelines for English and the results of an interannotator agreement study."
W19-3319,Semantically Constrained Multilayer Annotation: The Case of Coreference,2019,28,1,2,1,2245,jakob prange,Proceedings of the First International Workshop on Designing Meaning Representations,0,"We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions. We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes."
W19-0405,An Improved Approach for Semantic Graph Composition with {CCG},2019,29,0,2,1,13081,austin blodgett,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new semantics for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators: these require that the two constituents being combined overlap in one AMR relation. We also provide a new semantics for type raising, which is necessary for certain constructions. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG."
K19-1017,Made for Each Other: Broad-Coverage Semantic Structures Meet Preposition Supersenses,2019,45,0,2,1,2245,jakob prange,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is a typologically-informed, broad-coverage semantic annotation scheme that describes coarse-grained predicate-argument structure but currently lacks semantic roles. We argue that lexicon-free annotation of the semantic roles marked by prepositions, as formulated by Schneider et al. (2018), is complementary and suitable for integration within UCCA. We show empirically for English that the schemes, though annotated independently, are compatible and can be combined in a single semantic graph. A comparison of several approaches to parsing the integrated representation lays the groundwork for future research on this task."
W18-4903,Leaving no token behind: comprehensive (and delicious) annotation of {MWE}s and supersenses,2018,0,0,1,1,794,nathan schneider,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"I will describe an unorthodox approach to lexical semantic annotation that prioritizes corpus coverage, democratizing analysis of a wide range of expression types. I argue that a lexicon-free lexical semantics{---}defined in terms of units and supersense tags{---}is an appetizing direction for NLP, as it is robust, cost-effective, easily understood, not too language-specific, and can serve as a foundation for richer semantic structure. Linguistic delicacies from the STREUSLE and DiMSUM corpora, which have been multiword- and supersense-annotated, attest to the veritable sm{\""o}rg{\aa}sbord of noncanonical constructions in English, including various flavors of prepositions, MWEs, and other curiosities. Bio: Nathan Schneider is an annotation schemer and computational modeler for natural language. As Assistant Professor of Linguistics and Computer Science at Georgetown University, he looks for synergies between practical language technologies and the scientific study of language. He specializes in broad-coverage semantic analysis: designing linguistic meaning representations, annotating them in corpora, and automating them with statistical natural language processing techniques. A central focus in this research is the nexus between grammar and lexicon as manifested in multiword expressions and adpositions/case markers. He has inhabited UC Berkeley (BA in Computer Science and Linguistics), Carnegie Mellon University (Ph.D. in Language Technologies), and the University of Edinburgh (postdoc). Now a Hoya and leader of NERT, he continues to play with data and algorithms for linguistic meaning."
W18-4912,Annotation of Tense and Aspect Semantics for Sentential {AMR},2018,0,2,4,0,5431,lucia donatelli,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"Although English grammar encodes a number of semantic contrasts with tense and aspect marking, these semantics are currently ignored by Abstract Meaning Representation (AMR) annotations. This paper extends sentence-level AMR to include a coarse-grained treatment of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for NLP tasks and applications that require sophisticated reasoning about time and event structure."
W18-4921,Constructing an Annotated Corpus of Verbal {MWE}s for {E}nglish,2018,0,1,5,0,14283,abigail walsh,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"This paper describes the construction and annotation of a corpus of verbal MWEs for English, as part of the PARSEME Shared Task 1.1 on automatic identification of verbal MWEs. The criteria for corpus selection, the categories of MWEs used, and the training process are discussed, along with the particular issues that led to revisions in edition 1.1 of the annotation guidelines. Finally, an overview of the characteristics of the final annotated corpus is presented, as well as some discussion on inter-annotator agreement."
W18-4925,Edition 1.1 of the {PARSEME} Shared Task on Automatic Identification of Verbal Multiword Expressions,2018,0,2,22,0,12002,carlos ramisch,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"This paper describes the PARSEME Shared Task 1.1 on automatic identification of verbal multiword expressions. We present the annotation methodology, focusing on changes from last year{'}s shared task. Novel aspects include enhanced annotation guidelines, additional annotated data for most languages, corpora for some new languages, and new evaluation settings. Corpora were created for 20 languages, which are also briefly discussed. We report organizational principles behind the shared task and the evaluation metrics employed for ranking. The 17 participating systems, their methods and obtained results are also presented and analysed."
P18-1018,Comprehensive Supersense Disambiguation of {E}nglish Prepositions and Possessives,2018,46,3,1,1,794,nathan schneider,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Semantic relations are often signaled with prepositional or possessive marking{---}but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker{'}s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task."
P18-1210,Discourse Coherence: Concurrent Explicit and Implicit Relations,2018,0,3,3,1,16894,hannah rohde,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text. Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature. Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions (e.g., BUT and SO) to express the link they see between two segments. These apparent divergences are not symptomatic of participant naivety or bias, but arise reliably from the concurrent availability of multiple relations between segments {--} some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing."
N18-1088,Parsing Tweets into {U}niversal {D}ependencies,2018,35,7,5,0,3664,yijia liu,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We study the problem of analyzing tweets with universal dependencies (UD). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-of-speech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (Tweebank v2) that is four times larger than the (unlabeled) Tweebank v1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome the annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-of-the-art on other treebanks in both accuracy and speed."
N18-1106,A Structured Syntax-Semantics Interface for {E}nglish-{AMR} Alignment,2018,0,2,3,0,19680,ida szubert,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested. To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs. Our annotation explains how 97{\%} of AMR edges are evoked by words or syntax. Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23{\%}. While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMR-to-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax."
L18-1242,Semantic Supersenses for {E}nglish Possessives,2018,0,4,2,1,13081,austin blodgett,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1266,"{A}bstract {M}eaning {R}epresentation of Constructions: The More We Include, the Better the Representation",2018,0,1,8,0.221187,5184,claire bonial,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-6814,Exploring Substitutability through Discourse Adverbials and Multiple Judgments,2017,-1,-1,3,1,16894,hannah rohde,{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers,0,None
S17-1022,Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions,2017,0,7,6,0.7455,7749,jena hwang,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that an adposition{'}s lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition{'}s lexical function so they can be annotated at scale{---}supporting automatic, statistical processing of domain-general language{---}and discuss how this representation would allow for a simpler inventory of labels."
D17-2001,The {NLTK} {F}rame{N}et {API}: Designing for Discoverability with a Rich Linguistic Resource,2017,0,1,1,1,794,nathan schneider,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt."
W16-1707,"Filling in the Blanks in Understanding Discourse Adverbials: Consistency, Conflict, and Context-Dependence in a Crowdsourced Elicitation Task",2016,20,2,3,1,16894,hannah rohde,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,"The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isnxe2x80x99t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to a larger set of 20 discourse adverbials by eliciting 28K conjunction completions via crowdsourcing. Our data replicate and extend Rohde et al.xe2x80x99s findings that discourse adverbials do indeed license inferred conjunctions. Further, the diverse patterns observed for the adverbials include cases in which more than one valid connection can be inferred, each one endorsed by a substantial number of participants; such differences in annotation might otherwise be written off as annotator error or bias, or just a low level of inter-annotator agreement. These results will inform future discourse annotation endeavors by revealing where it is necessary to entertain implicit relations and elicit several judgments to fully characterize discourse relationships."
W16-1712,A Corpus of Preposition Supersenses,2016,47,11,1,1,794,nathan schneider,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,"We present the first corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). The preposition supersenses are organized hierarchically and designed to facilitate comprehensive manual annotation. Our dataset is publicly released on the web. 1"
S16-1084,{S}em{E}val-2016 Task 10: Detecting Minimal Semantic Units and their Meanings ({D}i{MSUM}),2016,38,15,1,1,794,nathan schneider,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1629,Inconsistency Detection in Semantic Annotation,2016,18,9,2,0,3242,nora hollenstein,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,Inconsistencies are part of any manually annotated corpus. Automatically finding these inconsistencies and correcting them (even manually) can increase the quality of the data. Past research has focused mainly on detecting inconsistency in syntactic annotation. This work explores new approaches to detecting inconsistency in semantic annotation. Two ranking methods are presented in this paper: a discrepancy ranking and an entropy ranking. Those methods are then tested and evaluated on multiple corpora annotated with multiword expressions and supersense labels. The results show considerable improvements in detecting inconsistency candidates over a random baseline. Possible applications of methods for inconsistency detection are improving the annotation procedure as well as the guidelines and correcting errors in completed annotations.
W15-1612,"A Hierarchy with, of, and for Preposition Supersenses",2015,52,22,1,1,794,nathan schneider,Proceedings of The 9th Linguistic Annotation Workshop,0,"English prepositions are extremely frequent and extraordinarily polysemous. In some usages they contribute information about spatial, temporal, or causal roles/relations; in other cases they are institutionalized, somewhat arbitrarily, as case markers licensed by a particular governing verb, verb class, or syntactic construction. To facilitate automatic disambiguation, we propose a general-purpose, broadcoverage taxonomy of preposition functions that we call supersenses: these are coarse and unlexicalized so as to be tractable for efficient manual annotation, yet capture crucial semantic distinctions. Our resource, including extensive documentation of the supersenses, many example sentences, and mappings to other lexical resources, will be publicly released. Prepositions are perhaps the most beguiling yet pervasive lexicosyntactic class in English. They are everywhere; their functional versatility is dizzying and largely idiosyncratic (1). They are nearly invisible, yet indispensable for situating the where, when, why, and how of events. In a way, prepositions are the bastard children of lexicon and grammar, rising to the occasion almost whenever a noun-noun or verbnoun relation is needed and neither subject nor object is appropriate. Consider the many uses of the word to, just a few of which are illustrated in (1):1 (1) a. My cake is to die for. b. If you want I can treat you to some. c. How about this: you go to the store d. to buy ingredients. e. Then if you give the recipe to me f. Ixe2x80x99m happy to make the batter g. and put it in the oven for 30 to 40 minutes h. so youxe2x80x99ll arrive to the sweet smell of chocolate. i. That sounds good to me. j. Thatxe2x80x99s all there is to it. 1Though infinitival to is traditionally not considered a preposition, we allow it to be labeled with a supersense if the infinitival clause serves as a PURPOSE (as in (1d)) or FUNCTION. See xc2xa72. Sometimes a preposition specifies a relationship between two entities or quantities, as in (1g). In other scenarios it serves a case-marking sort of function, marking a complement or adjunctxe2x80x94principally to a verb (1bxe2x80x931e, 1h, 1i), but also to an argument-taking noun or adjective (1f). Further, it is not always possible to separate the semantic contribution of the preposition from that of other words in the sentence. As amply demonstrated in the literature, prepositions play a key role in multiword expressions (Baldwin and Kim, 2010), as in (1a, 1b, 1j). An adequate descriptive annotation scheme for prepositions must deal with these messy facts. Following a brief discussion of existing approaches to preposition semantics (xc2xa71), this paper offers a new approach to characterizing their functions at a coarsegrained level. Our scheme is intended to apply to almost all preposition tokens, though some are excluded on the grounds that they belong to a larger multiword expression or are purely syntactic (xc2xa72). The rest of the paper is devoted to our coarse semantic categories, supersenses (xc2xa73).2 Many of these categories are based on previous proposalsxe2x80x94primarily, Srikumar and Roth (2013a) (so-called preposition relations) and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)xe2x80x94but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable."
W15-1618,What {I}{'}ve learned about annotating informal text (and why you shouldn{'}t take my word for it),2015,20,2,1,1,794,nathan schneider,Proceedings of The 9th Linguistic Annotation Workshop,0,"In conjunction with this yearxe2x80x99s LAW theme, xe2x80x9cSyntactic Annotation of Non-canonical Languagexe2x80x9d (NCL), I have been asked to weigh in on several important questions faced by anyone wishing to create annotated resources of NCLs. My experience with syntactic annotation of noncanonical language falls under an effort undertaken at Carnegie Mellon University with the aim of building an NLP pipeline for syntactic analysis of Twitter text. We designed a linguistically-grounded annotation scheme, applied it to tweets, and then trained statistical analyzersxe2x80x94first for part-of-speech (POS) tags (Gimpel et al., 2011; Owoputi et al., 2012), then for parses (Schneider et al., 2013; Kong et al., 2014). I will review some of the salient points from this work in addressing the broader questions about annotation methodology."
P15-2036,Frame-Semantic Role Labeling with Heterogeneous Annotations,2015,32,23,3,0,37420,meghana kshirsagar,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model.
N15-4003,"The Logic of {AMR}: Practical, Unified, Graph-Based Sentence Semantics for {NLP}",2015,0,0,1,1,794,nathan schneider,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"The Abstract Meaning Representation formalism is rapidly emerging as an important practical form of structured sentence semantics which, thanks to the availability of largescale annotated corpora, has potential as a convergence point for NLP research. This tutorial unmasks the design philosophy, data creation process, and existing algorithms for AMR semantics. It is intended for anyone interested in working with AMR data, including parsing text into AMRs, generating text from AMRs, and applying AMRs to tasks such as machine translation and summarization. The goals of this tutorial are twofold. First, it will describe the nature and design principles behind the representation, and demonstrate that it can be practical for annotation. In Part I: The AMR Formalism, participants will be coached in the basics of annotation so that, when working with AMR data in the future, they will appreciate the benefits and limitations of the process by which it was created. Second, the tutorial will survey the state of the art for computation with AMRs. Part II: Algorithms and Applications will focus on the task of parsing English text into AMR graphs, which requires algorithms for alignment, for structured prediction, and for statistical learning. The tutorial will also address graph grammar formalisms that have been recently developed, and future applications such as AMR-based machine translation and summarization. Participants with laptops are encouraged to bring them to the tutorial. Instructors Part I: The AMR Formalism Nathan Schneider is an annotation schemer and computational modeler for natural language. He has been involved in the design of the AMR formalism since 2012, when he interned with Kevin Knight at ISI. His 2014 dissertation introduced a coarse-grained representation for lexical semantics that facilitates rapid annotation and is practical for broad-coverage statistical NLP. He has also worked on semantic parsing for the FrameNet representation and other forms of syntactic/semantic annotation and processing for social media text. For most of these projects, he led the design of the annotation scheme, guidelines, and workflows, and the training and supervision of annotators."
N15-4006,Getting the Roles Right: Using {F}rame{N}et in {NLP},2015,7,0,2,0,1267,collin baker,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"The FrameNet lexical database (Fillmore & Baker 2010; Ruppenhofer et al. 2006) http://framenet.icsi. berkeley.edu), covers roughly 13,000 lexical units (word senses) for the core Engish lexicon, associating them with roughly 1,200 fully defined semantic frames; these frames and their roles cover the majority of event types in everyday, non-specialist text, and they are documented with 200,000 manually annotated examples. This tutorial will teach attendees what they need to know to start using the FrameNet lexical database as part of an NLP system. We will cover the basics of Frame Semantics, explain how the database was created, introduce the Python API and the state of the art in automatic frame semantic role labeling systems; and we will discuss FrameNet collaboration with commercial partners. Time permitting, we will present new research on frames and annotation of locative relations, as well as corresponding metaphorical uses, along with information about how frame semantic roles can aid the interpretation of metaphors."
N15-1177,A Corpus and Model Integrating Multiword Expressions and Supersenses,2015,25,30,1,1,794,nathan schneider,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper introduces a task of identifying and semantically classifying lexical expressions in running text. We investigate the online reviews genre, adding semantic supersense annotations to a 55,000 word English corpus that was previously annotated for multiword expressions. The noun and verb supersenses apply to full lexical expressions, whether single- or multiword. We then present a sequence tagging model that jointly infers lexical expressions and their supersenses. Results show that even with our relatively small training corpus in a noisy domain, the joint task can be performed to attain 70% class labeling F1."
K15-1009,"Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representations on Sequence Labelling Tasks",2015,37,19,5,0,7403,lizhen qu,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Word embeddings -- distributed word representations that can be learned from unlabelled data -- have been shown to have high utility in many natural language processing applications. In this paper, we perform an extrinsic evaluation of five popular word embedding methods in the context of four sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE identification. A particular focus of the paper is analysing the effects of task-based updating of word representations. We show that when using word embeddings as features, as few as several hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over OOV words and out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider."
S14-2027,"{CMU}: Arc-Factored, Discriminative Semantic Dependency Parsing",2014,16,7,7,1,3932,sam thomson,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We present an arc-factored statistical model for semantic dependency parsing, as defined by the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing. Our entry in the open track placed second in the competition."
Q14-1016,Discriminative Lexical Semantic Segmentation with Gaps: Running the {MWE} Gamut,2014,64,44,1,1,794,nathan schneider,Transactions of the Association for Computational Linguistics,0,"We present a novel representation, evaluation measure, and supervised models for the task of identifying the multiword expressions (MWEs) in a sentence, resulting in a lexical semantic segmentation. Our approach generalizes a standard chunking representation to encode MWEs containing gaps, thereby enabling efficient sequence tagging algorithms for feature-rich discriminative models. Experiments on a new dataset of English web text offer the first linguistically-driven evaluation of MWE identification with truly heterogeneous expression types. Our statistical sequence model greatly outperforms a lookup-based segmentation procedure, achieving nearly 60{\%} F1 for MWE identification."
P14-5021,Simplified Dependency Annotations with {GFL}-Web,2014,14,4,2,0,39116,michael mordowanec,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present GFL-Web, a web-based interface for syntactic dependency annotation with the lightweight FUDG/GFL formalism. Syntactic attachments are specified in GFL notation and visualized as a graph. A one-day pilot of this workflow with 26 annotators established that even novices were, with a bit of training, able to rapidly annotate the syntax of English Twitter messages. The open-source tool is easily installed and configured; it is available at: https://github.com/ Mordeaux/gfl _ web"
tsvetkov-etal-2014-augmenting-english,Augmenting {E}nglish Adjective Senses with Supersenses,2014,19,22,2,0.47619,3965,yulia tsvetkov,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We develop a supersense taxonomy for adjectives, based on that of GermaNet, and apply it to English adjectives in WordNet using human annotation and supervised classification. Results show that accuracy for automatic adjective type classification is high, but synsets are considerably more difficult to classify, even for trained human annotators. We release the manually annotated data, the classifier, and the induced supersense labeling of 12,304 WordNet adjective synsets."
schneider-etal-2014-comprehensive,Comprehensive Annotation of Multiword Expressions in a Social Web Corpus,2014,26,37,1,1,794,nathan schneider,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Multiword expressions (MWEs) are quite frequent in languages such as English, but their diversity, the scarcity of individual MWE types, and contextual ambiguity have presented obstacles to corpus-based studies and NLP systems addressing them as a class. Here we advocate for a comprehensive annotation approach: proceeding sentence by sentence, our annotators manually group tokens into MWEs according to guidelines that cover a broad range of multiword phenomena. Under this scheme, we have fully annotated an English web corpus for multiword expressions, including those containing gaps."
J14-1002,Frame-Semantic Parsing,2014,101,152,4,0.258509,6251,dipanjan das,Computational Linguistics,0,"Frame semantics is a linguistic theory that has been instantiated for English in the FrameNet lexicon. We solve the problem of frame-semantic parsing using a two-stage statistical model that takes lexical targets i.e., content words and phrases in their sentential contexts and predicts frame-semantic structures. Given a target in context, the first stage disambiguates it to a semantic frame. This model uses latent variables and semi-supervised learning to improve frame disambiguation for targets unseen at training time. The second stage finds the target's locally expressed semantic arguments. At inference time, a fast exact dual decomposition algorithm collectively predicts all the arguments of a frame at once in order to respect declaratively stated linguistic constraints, resulting in qualitatively better structures than nave local predictors. Both components are feature-based and discriminatively trained on a small set of annotated frame-semantic parses. On the SemEval 2007 benchmark data set, the approach, along with a heuristic identifier of frame-evoking targets, outperforms the prior state of the art by significant margins. Additionally, we present experiments on the much larger FrameNet 1.5 data set. We have released our frame-semantic parser as open-source software."
D14-1108,A Dependency Parser for Tweets,2014,56,119,2,0,12752,lingpeng kong,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions. Our dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP."
C14-1100,Automatic Classification of Communicative Functions of Definiteness,2014,45,2,3,0.46875,11651,archna bhatia,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Definiteness expresses a constellation of semantic, pragmatic, and discourse propertiesxe2x80x94the communicative functionsxe2x80x94of an NP. We present a supervised classifier for English NPs that uses lexical, morphological, and syntactic features to predict an NPxe2x80x99s communicative function in terms of a language-universal classification scheme. Our classifiers establish strong baselines for future work in this neglected area of computational semantic analysis. In addition, analysis of the features and learned parameters in the model provides insight into the grammaticalization of definiteness in English, not all of which is obvious a priori."
W13-2307,A Framework for (Under)specifying Dependency Syntax without Overloading Annotators,2013,25,5,1,1,794,nathan schneider,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We introduce a framework for lightweight dependency syntax annotation. Our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workflow. Moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. We demonstrate the e cacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations."
W13-2322,{A}bstract {M}eaning {R}epresentation for Sembanking,2013,23,386,10,0,40939,laura banarescu,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it."
W13-1736,Identifying the {L}1 of non-native writers: the {CMU}-Haifa system,2013,29,13,3,0.47619,3965,yulia tsvetkov,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We show that it is possible to learn to identify, with high accuracy, the native language of English test takers from the content of the essays they write. Our method uses standard text classification techniques based on multiclass logistic regression, combining individually weak indicators to predict the most probable native language from a set of 11 possibilities. We describe the various features used for classification, as well as the settings of the classifier that yielded the highest accuracy."
N13-1039,Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters,2013,34,452,5,0,41595,olutobi owoputi,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the xe2x80x9cCMU Twitter Part-of-Speech Taggerxe2x80x9d and annotated data. [This paper is forthcoming in Proceedings of NAACL 2013; Atlanta, GA, USA.]"
N13-1076,Supersense Tagging for {A}rabic: the {MT}-in-the-Middle Attack,2013,33,9,1,1,794,nathan schneider,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We consider the task of tagging Arabic nouns with WordNet supersenses. Three approaches are evaluated. The first uses an expertcrafted but limited-coverage lexicon, Arabic WordNet, and heuristics. The second uses unsupervised sequence modeling. The third and most successful approach uses machine translation to translate the Arabic into English, which is automatically tagged with English supersenses, the results of which are then projected back into Arabic. Analysis shows gains and remaining obstacles in four Wikipedia topical domains."
J13-2006,Book Review: Design Patterns in Fluid Construction Grammar edited by Luc Steels,2013,-1,-1,1,1,794,nathan schneider,Computational Linguistics,0,None
P12-2050,Coarse Lexical Semantic Annotation with Supersenses: An {A}rabic Case Study,2012,24,22,1,1,794,nathan schneider,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Lightweight semantic annotation of text calls for a simple representation, ideally without requiring a semantic lexicon to achieve good coverage in the language and domain. In this paper, we repurpose WordNet's supersense tags for annotation, developing specific guidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains. The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement."
E12-1017,Recall-Oriented Learning of Named Entities in {A}rabic {W}ikipedia,2012,51,39,2,0,35024,behrang mohit,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We consider the problem of NER in Arabic Wikipedia, a semisupervised domain adaptation setting for which we have no labeled training data in the target domain. To facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-specific entity types in addition to standard categories. Standard supervised learning on newswire text leads to poor target-domain recall. We train a sequence model and show that a simple modification to the online learner---a loss function encouraging it to arrogantly favor recall over precision---substantially improves recall and F1. We then adapt our model with self-training on unlabeled target-domain data; enforcing the same recall-oriented bias in the self-training stage yields marginal gains."
P11-2008,"Part-of-Speech Tagging for {T}witter: Annotation, Features, and Experiments",2011,15,673,2,0,4023,kevin gimpel,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets."
S10-1059,{SEMAFOR}: Frame Argument Resolution with Log-Linear Models,2010,7,40,2,0,39978,desai chen,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper describes the SEMAFOR system's performance in the SemEval 2010 task on linking events and their participants in discourse. Our entry is based upon SEMAFOR 1.0 (Das et al., 2010a), a frame-semantic probabilistic parser built from log-linear models. The extended system models null instantiations, including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted systems with respect to recall and F1."
N10-1138,Probabilistic Frame-Semantic Parsing,2010,32,118,2,0,6251,dipanjan das,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an English sentence into a frame-semantic representation. It finds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame. The system uses two feature-based, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words. The parser is demonstrated to significantly outperform previously published results."
