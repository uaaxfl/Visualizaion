2020.emnlp-main.218,N19-1423,0,0.00654858,"ts cannot have such information in the UD v2 framework.6 • Our proposed sequence alignment algorithm is a novel one that can impose the orderpreserving and non-overlapping constraints on the resulting alignment. This feature is required to deal with gapping constructions in PTB constituent trees. 4 Experiment We conducted an experiment using the PTB to evaluate the performance of our proposed method.7 We used the standard PTB training, development, and test data split (i.e., sections 02– 21, 22, and 23, respectively) and the Kitaev and Klein parser (Kitaev and Klein, 2018)8 that can use BERT (Devlin et al., 2019). We trained the parsing model by simply replacing the training and 6 In the UD v2 framework, one remnant is treated as a head, and the others are attached to it with the special orphan dependency. 7 The code is available at https://github.com/ yosihide/ptb2cf. 8 https://github.com/nikitakit/ self-attentive-parser rec. 6.9 20.7 58.6 86.2 F 12.9 31.6 70.8 89.3 Table 2: Alignment performance on the test data. 13: end for 14: end for 15: return T [0, 0] Our method is similar to that of Schuster et al. (2018) in that both rely on sequence alignment. The following differences, however, exist: pre."
2020.emnlp-main.218,P16-2012,0,0.0182073,"nct node. The number assigned to a correlate and a remnant indicates a correspondence relation. For example, NP-SBJ-1 and NP-SBJ=1 in this tree are a correlate and a remnant, respectively, and correspond to each other. We can obtain the constituent tree for “the six-month bills will still mature on May 3, 1990” by replacing NP-SBJ-1 with the tree whose root is NP-SBJ=1 and NP-TMP-2 with that whose root is PP-TMP=2. In other words, “will still mature” is elided from the second conjunct. 2.2 Previous Work This section gives an overview of previous approaches to analyzing sentences with gapping. Ficler and Goldberg (2016) proposed a new representation for argument-cluster coordination, which is one kind of gapping constructions. They converted PTB trees by coordinating correlates and remnants. This conversion can be applied only when the correlates and the remnants are all to1 In English, the ﬁrst conjunct is ungapped. 2747 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2747–2752, c November 16–20, 2020. 2020 Association for Computational Linguistics ^  ^ ǁŝůů sWͲdDW Ɛƚŝůů ^ EWͲ^:сϭ sW EWͲ^:Ͳϭ dŚĞƚŚƌĞĞͲŵŽŶƚŚďŝůůƐ ĂŶĚ sW WWͲdDWсϮ ŽŶ ŵĂƚƵƌĞ EWͲdDWͲϮ EW ƚŚĞƐŝ"
2020.emnlp-main.218,N06-1024,0,0.0794748,"parser is to analyze traces, such as wh-movement, and not gapping constructions. 3 The grammatical tag PRD can be used together with a semantic tag. We only used the PRD tag in this case. 3 Proposed Method 2748 DTV LGS PRD PUT SBJ grammatical Dative Logical subject Predicate Locative complement of ‘put’ Surface subject semantic BNF Benefactive DIR Direction EXT Extent LOC Locative MNR Manner PRP Purpose TMP Temporal Table 1: Grammatical and semantic role tags. grammatical and semantic tag information; thus, we can learn a model that identiﬁes the tags by simply retaining them in the treebank (Gabbard et al., 2006). 3.2 How to identify correspondences between correlates and remnants This section explains how to identify correspondences between correlates and remnants using the tag-annotated constituent trees described in the previous section. The procedure consists of the following two steps: 1. Extract remnant candidates R and correlate candidates C when gapped conjuncts exist. where, rl(x) stands for the role tag of x. If x has no role tag, rl(x) = null. hd(x) is the head preposition of x. Furthermore, we impose the following structural constraints to follow the PTB annotation scheme: Uniqueness of re"
2020.emnlp-main.218,P19-1340,0,0.0122929,"2 pre. 66.7 97.4 82.0 88.6 73.7 50.0 89.7 88.4 71.1 76.9 76.2 87.5 0.0 w/o BERT rec. F1 25.0 36.4 96.8 97.1 78.8 80.4 88.6 88.6 73.7 73.7 40.0 44.4 91.9 90.7 80.9 84.5 45.4 55.4 55.4 64.4 77.5 76.9 80.0 83.6 0.0 0.0 with BERT pre. rec. F1 84.6 68.8 75.9 98.1 98.1 98.1 88.3 85.1 86.6 91.5 90.4 90.9 87.5 73.7 80.0 72.7 80.0 76.2 91.8 94.3 93.0 91.2 84.5 87.7 82.8 62.5 71.2 84.3 71.1 77.1 72.5 79.8 75.9 88.9 83.8 86.3 0.0 0.0 0.0 Table 3: Accuracy of tag identiﬁcation on the test data. development data with those annotated by our annotation scheme. The hyperparameters were identical to those of Kitaev et al. (2019). The test data were parsed by the trained model to obtain tagannotated trees. The correspondences between the correlates and the remnants were identiﬁed by our proposed alignment algorithm. The alignment accuracy was evaluated by the metric of Kummerfeld and Klein (2017). That is, we represent a correspondence between a correlate c and a remnant r as a tuple (ct(r), s(r), e(r), ct(c), s(c), e(c)), and measure the precision and recall using tuples. Table 2 shows the alignment performance of our method and the previous one. The previous method struggled to generate arcs between correlates and r"
2020.emnlp-main.218,P18-1249,0,0.0182802,"d semantic roles unlike ours, because remnants cannot have such information in the UD v2 framework.6 • Our proposed sequence alignment algorithm is a novel one that can impose the orderpreserving and non-overlapping constraints on the resulting alignment. This feature is required to deal with gapping constructions in PTB constituent trees. 4 Experiment We conducted an experiment using the PTB to evaluate the performance of our proposed method.7 We used the standard PTB training, development, and test data split (i.e., sections 02– 21, 22, and 23, respectively) and the Kitaev and Klein parser (Kitaev and Klein, 2018)8 that can use BERT (Devlin et al., 2019). We trained the parsing model by simply replacing the training and 6 In the UD v2 framework, one remnant is treated as a head, and the others are attached to it with the special orphan dependency. 7 The code is available at https://github.com/ yosihide/ptb2cf. 8 https://github.com/nikitakit/ self-attentive-parser rec. 6.9 20.7 58.6 86.2 F 12.9 31.6 70.8 89.3 Table 2: Alignment performance on the test data. 13: end for 14: end for 15: return T [0, 0] Our method is similar to that of Schuster et al. (2018) in that both rely on sequence alignment. The fol"
2020.emnlp-main.218,Q17-1031,0,0.296223,"es and remnants. This conversion can be applied only when the correlates and the remnants are all to1 In English, the ﬁrst conjunct is ungapped. 2747 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2747–2752, c November 16–20, 2020. 2020 Association for Computational Linguistics ^  ^ ǁŝůů sWͲdDW Ɛƚŝůů ^ EWͲ^:сϭ sW EWͲ^:Ͳϭ dŚĞƚŚƌĞĞͲŵŽŶƚŚďŝůůƐ ĂŶĚ sW WWͲdDWсϮ ŽŶ ŵĂƚƵƌĞ EWͲdDWͲϮ EW ƚŚĞƐŝǆͲŵŽŶƚŚďŝůůƐ DĂǇϯϭϵϵϬ &ĞďϭϭϵϵϬ Figure 1: Gapping construction in the PTB. gether on the right. Therefore, it cannot handle the tree shown in Figure 1. Kummerfeld and Klein (2017) developed a parser that adopts a graph representation for syntactic structure. They discussed how to represent a correspondence between a remnant and a correlate with an arc in their graph representation. However, the parser struggled to generate such arcs, and the recall was very low.2 Schuster et al. (2018) proposed two methods based on dependency structure. One represents gapping constructions using complex relation labels (Seeker et al., 2012), and the other adopts a sequence alignment algorithm to assign remnant words to correlate words. The latter is similar to ours. We will discuss the"
2020.emnlp-main.218,J93-2004,0,0.0742087,"ng. Our proposed method uses constituent trees annotated with grammatical and semantic tags and a special tag indicating gapped conjuncts. The method parses a sentence to obtain a tag-annotated constituent tree and analyzes gapping constructions using the resulting tree when it includes gapped conjuncts. The analysis is based on a sequence alignment algorithm using grammatical and semantic tags. An experiment shows that our method outperforms the previous method in terms of F-measure and recall. 2 Gapping Construction This section ﬁrst explains gapping constructions in the Penn Treebank (PTB, Marcus et al., 1993), on which our proposed method is based, and summarizes the previous work on analyzing sentences with gapping. A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. The constituents remaining in a gapped conjunct are called remnants. The remnants have a corresponding constituent, called a correlate, in the ungapped conjunct.1 We can obtain the ungapped version of the conjunct by replacing each correlate with its corresponding remnant. In the PTB, the correspondences between remnants and correlates are annotated. Figure 1 show"
2020.emnlp-main.218,C12-2105,0,0.0335934,"ϯϭϵϵϬ &ĞďϭϭϵϵϬ Figure 1: Gapping construction in the PTB. gether on the right. Therefore, it cannot handle the tree shown in Figure 1. Kummerfeld and Klein (2017) developed a parser that adopts a graph representation for syntactic structure. They discussed how to represent a correspondence between a remnant and a correlate with an arc in their graph representation. However, the parser struggled to generate such arcs, and the recall was very low.2 Schuster et al. (2018) proposed two methods based on dependency structure. One represents gapping constructions using complex relation labels (Seeker et al., 2012), and the other adopts a sequence alignment algorithm to assign remnant words to correlate words. The latter is similar to ours. We will discuss the differences between the latter method and ours in the later section. Another approach is the one that does not depend on syntactic representation. In the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) (Ponomareva et al., 2019), the prepared dataset marked each element comprising gapping constructions. Most participants treated this task as a sequence labeling problem. correspondences between correlates and remnants, but from the"
2020.emnlp-main.218,W19-3705,0,0.0165253,"rser struggled to generate such arcs, and the recall was very low.2 Schuster et al. (2018) proposed two methods based on dependency structure. One represents gapping constructions using complex relation labels (Seeker et al., 2012), and the other adopts a sequence alignment algorithm to assign remnant words to correlate words. The latter is similar to ours. We will discuss the differences between the latter method and ours in the later section. Another approach is the one that does not depend on syntactic representation. In the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) (Ponomareva et al., 2019), the prepared dataset marked each element comprising gapping constructions. Most participants treated this task as a sequence labeling problem. correspondences between correlates and remnants, but from the following tags easily obtained from the PTB: • a special tag indicating gapped conjuncts • grammatical and semantic role tags Correspondences between correlates and remnants are identiﬁed by a sequence alignment algorithm using the tag-annotated constituent tree. We ﬁrst explain our tag annotation and describe the sequence alignment algorithm. 3.1 Annotation Our method uses a special tag to"
2020.emnlp-main.218,L16-1376,0,0.040206,"Missing"
2020.lrec-1.87,W17-5516,0,0.0261156,"tives for the purpose of showing an attitude of attentive listening, is simply called attentive listening response. Attentive listening responses show empathy to narrative speech and enhance speaker’s motivation to speak. A representative attentive listening response is a back-channel, whose generation methods have already been proposed (Kamiya et al., 2010; Yamaguchi et al., 2016). Besides the back-channel, there are various types of attentive listening responses such as admiration response or evaluation response, and there are some works on generating such responses (Kobayashi et al., 2010; Lala et al., 2017; Meguro et al., 2011; Shitaoka et al., 2017). The degree of empathy shown by attentive listening responses are thought to depend on their type. Empathy to narratives encourages a speaker to speak more only when the degree of the empathy is appropriate. On the other hand, when the degree of the empathy is not appropriate for the narrative, such empathy discourages the speaker. In order to enhance the speaker’s motivation to speak, it is necessary to utter an attentive listening response which can show the appropriate degree of empathy. Nevertheless, the relation between types of attentive list"
2021.emnlp-main.826,2020.repl4nlp-1.23,0,0.0249963,"er with our representation is comparable with previous CCG parsers. 1 Introduction X/Y Y |1 Z1 · · · |d Zd ⇒ X|1 Z1 · · · |d Zd Y |1 Z1 · · · |d Zd XY ⇒ X|1 Z1 · · · |d Zd (&gt;d ) (<d ) Figure 1: CCG rule schemata. sentation decomposes CCG derivations into several independent pieces and can prevent the spanbased parsing models from violating the CCG rule schemata. Furthermore, as a by-product of our representation, the parsing models can assign outof-vocabulary (OOV) categories, which have not appeared in training data. This characteristic has been attracting attention in CCG parsing research (Bhargava and Penn, 2020; Prange et al., 2021; Liu et al., 2021). Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers and can generate correct OOV categories. Combinatory Categorial Grammar (CCG) (Steed2 CCG and Span-based Parsing man, 2000) is a mildly context-sensitive grammar formalism. Several neural CCG parsing methods This section gives an overview of Combinatory have been proposed so far (Lewis and Steedman, Categorial Grammar (CCG) (Steedman, 2000) and 2014; Xu et al., 2015; Lewis et al., 2016; Vaswani explains why we cannot dir"
2021.emnlp-main.826,J07-4004,0,0.263003,"Missing"
2021.emnlp-main.826,D16-1001,0,0.0146042,"context-sensitive grammar formalism. Several neural CCG parsing methods This section gives an overview of Combinatory have been proposed so far (Lewis and Steedman, Categorial Grammar (CCG) (Steedman, 2000) and 2014; Xu et al., 2015; Lewis et al., 2016; Vaswani explains why we cannot directly apply the spanet al., 2016; Lee et al., 2016; Xu, 2016; Yoshikawa based approach to CCG parsing. et al., 2017; Stanojevi´c and Steedman, 2019, 2020; Bhargava and Penn, 2020; Tian et al., 2020; Prange 2.1 Combinatory Categorial Grammar et al., 2021; Liu et al., 2021). Currently, neural span-based models (Cross and Huang, 2016; Stern CCG represents syntactic information by basic catet al., 2017; Gaddy et al., 2018; Kitaev and Klein, egories (e.g., S, NP) and complex categories. Com2018) have been successful in the field of con- plex categories are in the form of X/Y or XY , stituency parsing. However, we cannot directly where X and Y are categories. Intuitively, each apply this technique to CCG parsing. Span-based category X/Y means that it receives a category models assume that each node label in parse trees Y from its right and returns a category X. In the can be predicted independently, while, in CCG, case of X"
2021.emnlp-main.826,N19-1423,0,0.0720924,"Missing"
2021.emnlp-main.826,N18-1091,0,0.047682,"Missing"
2021.emnlp-main.826,J07-3004,0,0.0946399,"Missing"
2021.emnlp-main.826,P19-1340,0,0.041608,"Missing"
2021.emnlp-main.826,P18-1249,0,0.0144407,"Conference on Empirical Methods in Natural Language Processing, pages 10579–10584 c November 7–11, 2021. 2021 Association for Computational Linguistics rule schema: X/Y Y α ⇒ Xα. (1) We define |α |= d and the arity of a category Y = Xα where X is a basic category is defined as follows: arity(Y ) = |α| (2) 2.2 1. Convert CCG derivations into SBRs (Section 3.2). 2. Train a span-based parsing model using SBRs and parse sentences to generate SBRs. 3. Convert the output SBRs into CCG derivations. (Section 3.3). Span-based Parsing A span-based parsing model (Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018) has a single scoring function s(i, j, l) that scores each label l for each span (i, j). The score of a tree T is defined as follows: s(T ) = X s(i, j, l). (3) (i,j,l)∈T The parsing problem is formulated as finding the tree T ∗ with the highest score: T ∗ = arg max s(T ) (4) The basic idea behind our method is that each node label in an SBR represents a constraint on the categories of nodes in a CCG derivation. Our method recovers a CCG derivation from its SBR version by satisfying such constraints. Because constraints encoded in SBR’s labels are independent, a span-based model using SBRs does"
2021.emnlp-main.826,Q14-1032,0,0.0253802,"se trees Y from its right and returns a category X. In the can be predicted independently, while, in CCG, case of XY , the direction is from its left. Foreach node label (category) is strictly restricted by mally, categories are combined using CCG rule CCG rule schemata. The independence assumption schemata. Figure 1 shows CCG rule schemata. of span-based models implies that the models are Here, X, Y and Zi (1 ≤ i ≤ d) are categories, and not guaranteed to generate valid CCG derivations. |i ∈ {/, }. |1 Z1 · · · |d Zd is called an argument To solve this problem, we propose a method of stack (Kuhlmann and Satta, 2014), and we use a representing CCG derivations in a way suitable for Greek letter to represent an argument stack. For span-based parsing models. Our proposed repre- example, we use the following notation for the first 10579 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10579–10584 c November 7–11, 2021. 2021 Association for Computational Linguistics rule schema: X/Y Y α ⇒ Xα. (1) We define |α |= d and the arity of a category Y = Xα where X is a basic category is defined as follows: arity(Y ) = |α| (2) 2.2 1. Convert CCG derivations into SBRs (Sectio"
2021.emnlp-main.826,D16-1262,0,0.0382228,"Missing"
2021.emnlp-main.826,Q14-1026,0,0.0667087,"Missing"
2021.emnlp-main.826,N19-1020,0,0.0272556,"Missing"
2021.emnlp-main.826,2020.acl-main.378,0,0.0259336,"Missing"
2021.emnlp-main.826,P17-1076,0,0.0194285,"the first 10579 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10579–10584 c November 7–11, 2021. 2021 Association for Computational Linguistics rule schema: X/Y Y α ⇒ Xα. (1) We define |α |= d and the arity of a category Y = Xα where X is a basic category is defined as follows: arity(Y ) = |α| (2) 2.2 1. Convert CCG derivations into SBRs (Section 3.2). 2. Train a span-based parsing model using SBRs and parse sentences to generate SBRs. 3. Convert the output SBRs into CCG derivations. (Section 3.3). Span-based Parsing A span-based parsing model (Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018) has a single scoring function s(i, j, l) that scores each label l for each span (i, j). The score of a tree T is defined as follows: s(T ) = X s(i, j, l). (3) (i,j,l)∈T The parsing problem is formulated as finding the tree T ∗ with the highest score: T ∗ = arg max s(T ) (4) The basic idea behind our method is that each node label in an SBR represents a constraint on the categories of nodes in a CCG derivation. Our method recovers a CCG derivation from its SBR version by satisfying such constraints. Because constraints encoded in SBR’s labels are in"
2021.emnlp-main.826,2020.emnlp-main.487,0,0.0167945,"can generate correct OOV categories. Combinatory Categorial Grammar (CCG) (Steed2 CCG and Span-based Parsing man, 2000) is a mildly context-sensitive grammar formalism. Several neural CCG parsing methods This section gives an overview of Combinatory have been proposed so far (Lewis and Steedman, Categorial Grammar (CCG) (Steedman, 2000) and 2014; Xu et al., 2015; Lewis et al., 2016; Vaswani explains why we cannot directly apply the spanet al., 2016; Lee et al., 2016; Xu, 2016; Yoshikawa based approach to CCG parsing. et al., 2017; Stanojevi´c and Steedman, 2019, 2020; Bhargava and Penn, 2020; Tian et al., 2020; Prange 2.1 Combinatory Categorial Grammar et al., 2021; Liu et al., 2021). Currently, neural span-based models (Cross and Huang, 2016; Stern CCG represents syntactic information by basic catet al., 2017; Gaddy et al., 2018; Kitaev and Klein, egories (e.g., S, NP) and complex categories. Com2018) have been successful in the field of con- plex categories are in the form of X/Y or XY , stituency parsing. However, we cannot directly where X and Y are categories. Intuitively, each apply this technique to CCG parsing. Span-based category X/Y means that it receives a category models assume that ea"
2021.emnlp-main.826,N16-1027,0,0.0350596,"Missing"
2021.emnlp-main.826,P15-2041,0,0.0189286,"n attracting attention in CCG parsing research (Bhargava and Penn, 2020; Prange et al., 2021; Liu et al., 2021). Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers and can generate correct OOV categories. Combinatory Categorial Grammar (CCG) (Steed2 CCG and Span-based Parsing man, 2000) is a mildly context-sensitive grammar formalism. Several neural CCG parsing methods This section gives an overview of Combinatory have been proposed so far (Lewis and Steedman, Categorial Grammar (CCG) (Steedman, 2000) and 2014; Xu et al., 2015; Lewis et al., 2016; Vaswani explains why we cannot directly apply the spanet al., 2016; Lee et al., 2016; Xu, 2016; Yoshikawa based approach to CCG parsing. et al., 2017; Stanojevi´c and Steedman, 2019, 2020; Bhargava and Penn, 2020; Tian et al., 2020; Prange 2.1 Combinatory Categorial Grammar et al., 2021; Liu et al., 2021). Currently, neural span-based models (Cross and Huang, 2016; Stern CCG represents syntactic information by basic catet al., 2017; Gaddy et al., 2018; Kitaev and Klein, egories (e.g., S, NP) and complex categories. Com2018) have been successful in the field of con- plex c"
2021.emnlp-main.826,P17-1026,0,0.0397704,"Missing"
C02-1107,W01-1603,0,0.0264105,"Missing"
C02-1107,W00-1001,0,0.0672571,"Missing"
C02-1107,C02-1136,1,\N,Missing
C02-1136,W98-1511,0,0.2521,"to left and the bunsetsu which doesn’t depend on any bunsetsu. The parsing results are expressed by partial dependency structures. The method acquires in advance the probabilities of dependencies from a spoken dialogue corpus tagged with dependency structures, and provides the most plausible dependency structure for each utterance on the basis of the probabilities. Several techniques for dependency parsing based on stochastic approaches have been proposed so far. Fujio and Matsumoto have used the probability based on the frequency of cooccurrence between two bunsetsus for dependency parsing (Fujio and Matsumoto, 1998). Uchimoto et al. have proposed a technique for learning the dependency probability model based on a maximum entropy method (Uchimoto et al., 1999). However, since these 1 A bunsetsu is one of the linguistic units in Japanese, and roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and more than zero ancillary words. A dependency is a modification relation between two bunsetsus. techniques are for written language, whether they are available for spoken language or not is not clear. As the technique for stochastic parsing of spoken language, Den has sug"
C02-1136,kawaguchi-etal-2002-multi,1,0.816174,"able for robust parsing of spontaneously spoken language. 2 Linguistic Analysis of Spontaneous Speech We have investigated spontaneously spoken utterances in an in-car speech dialogue corpus which is constructed at the Center for Integrated Acoustic Information Research (CIAIR), Nagoya University (Kawaguchi et al., 2001) The corpus contains speeches of dialogue between drivers and navigators (humans, a Wizard of OZ system, or a spoken dialogue system) and their transcripts. 2.1 CIAIR In-car Speech Dialogue Corpus Data collection project of in-car speech dialogues at CIAIR has started in 1999 (Kawaguchi et al., 2002). The project has developed a private car, and been collecting a total of about 140 hours of multimodal data such as speeches, images, locations and so on. These data would be available for investigating in-car speech dialogues. The speech ﬁles are transcribed into ASCII text ﬁles by hand. The example of a transcript is shown in Figure 1. As an advance analysis, discourse tags are assigned to ﬁllers, hesitations, and so on. Furthermore, each speech is segmented into utterance units by a pause, and the exact start time and end time are provided for them. The environmental information about sex"
C02-1136,E99-1026,0,0.231381,"in advance the probabilities of dependencies from a spoken dialogue corpus tagged with dependency structures, and provides the most plausible dependency structure for each utterance on the basis of the probabilities. Several techniques for dependency parsing based on stochastic approaches have been proposed so far. Fujio and Matsumoto have used the probability based on the frequency of cooccurrence between two bunsetsus for dependency parsing (Fujio and Matsumoto, 1998). Uchimoto et al. have proposed a technique for learning the dependency probability model based on a maximum entropy method (Uchimoto et al., 1999). However, since these 1 A bunsetsu is one of the linguistic units in Japanese, and roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and more than zero ancillary words. A dependency is a modification relation between two bunsetsus. techniques are for written language, whether they are available for spoken language or not is not clear. As the technique for stochastic parsing of spoken language, Den has suggested a new idea for detecting and parsing self-repaired expressions, however, the phenomena with which the framework can cope are restricted (Den"
C08-2030,kozawa-etal-2008-automatic,1,0.880303,"Missing"
C14-1112,J03-1005,0,0.0385958,"s for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order differences between them. Therefore, their problem setting is different from that for improving the readability of a single language. This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes easier to read for revision support. Our proposed method concurrently performs dependency parsing and word reordering for an input sentence of which the dependency structure is still unknown. Our method can identify more suitable word ord"
C14-1112,W08-0406,0,0.0116689,"exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order differences between them. Therefore, their problem setting is different from that for improving the readability of a single language. This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes easier to read for revision support. Our proposed method concurrently performs dependency parsing and word reordering for an input sentence of which the dependency structure is still unknown. Our met"
C14-1112,P07-1041,0,0.301812,"word order is not completely arbitrary and has some sort of preference. Since such preference is incompletely understood, even native Japanese writers often write Japanese sentences which are grammatically well-formed but not easy to read. The word reordering of such sentences enables the readability to be improved. There have been proposed some methods for reordering words in a Japanese sentence so that the sentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004). In addition, there exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to b"
C14-1112,N10-1127,0,0.0173082,"researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order differences between them. Therefore, their problem setting is different from that for improving the readability of a single language. This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes easier to read for revision support. Our proposed method concurrently performs dependency parsing and word reordering for an input sentence of which the dependency structure is still unknown. Our method can id"
C14-1112,W01-0810,0,0.829671,"Missing"
C14-1112,P12-2061,0,0.0260616,"In addition, there exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order differences between them. Therefore, their problem setting is different from that for improving the readability of a single language. This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes easier to read for revision support. Our proposed method concurrently performs dependency parsing and word reordering for an input sentence of which the dependency structure is still un"
C14-1112,W06-1402,0,0.249225,"Missing"
C14-1112,W02-2016,0,0.147999,"Missing"
C14-1112,2007.mtsummit-papers.29,0,0.0503154,"word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order differences between them. Therefore, their problem setting is different from that for improving the readability of a single language. This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes easier to read for revision support. Our proposed method concurrently performs dependency parsing and word reordering for an input sentence of which the dependency structure is still unknown. Our method can identify more suitable word order than conven"
C14-1112,C04-1097,0,0.142273,"ince such preference is incompletely understood, even native Japanese writers often write Japanese sentences which are grammatically well-formed but not easy to read. The word reordering of such sentences enables the readability to be improved. There have been proposed some methods for reordering words in a Japanese sentence so that the sentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004). In addition, there exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order di"
C14-1112,C00-2109,0,0.114084,"Missing"
C14-1112,P99-1018,0,0.554271,"s incompletely understood, even native Japanese writers often write Japanese sentences which are grammatically well-formed but not easy to read. The word reordering of such sentences enables the readability to be improved. There have been proposed some methods for reordering words in a Japanese sentence so that the sentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004). In addition, there exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010; Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source language and a target language to handle word order differences between them. Therefore,"
C14-1112,E99-1026,0,0.325932,"hat the dependency relation whose modifier bunsetsu is bi is di when the sentence generated by reordering B according to O is provided, P (di |B) is the probability that the dependency relation whose modifier bunsetsu is bi is di when B is provided, and P (oi,j |D, B) is the probability that the order between bi and bj is oi,j when B where the dependency relation is D is provided. These probabilities are estimated by the maximum entropy method. remain comparatively high. This is because their processings use mainly local information. 1188 To estimate P (di |O, B), we used the features used in Uchimoto et al. (1999) except when eliminating features about Japanese commas (called toten, which is a kind of punctuation) and quotation marks. To estimate P (di |B), we used the features which can be obtained without information about the order of input bunsetsus among the features used in estimating P (di |O, B). To estimate P (oi,j |D, B), if bi and bj modifies the same bunsetsu, we used the features used in Uchimoto et al. (2000), except when eliminating features about parallel relations and semantic features. Otherwise, we used the features left after eliminating features about modified bunsetsus from those"
C14-1112,C00-2126,0,0.925811,"relatively free word order, and thus Japanese sentences which make sense can be written without having a strong awareness of word order. However, Japanese word order is not completely arbitrary and has some sort of preference. Since such preference is incompletely understood, even native Japanese writers often write Japanese sentences which are grammatically well-formed but not easy to read. The word reordering of such sentences enables the readability to be improved. There have been proposed some methods for reordering words in a Japanese sentence so that the sentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004). In addition, there exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999). Although most of these previous researches used syntactic information, the sentences they used there were what had been previously parsed. It is a problem that word reordering suffers the influence of parsing errors. Furthermore, as the related works, there are various researches on word reordering for improving the performance of stat"
D10-1087,C92-2100,0,0.0542418,"stics on comma insertion into non-Japanese written texts (White and Rajkumar, 2008; Guo et al., 2010). In Japanese, there are several usages of commas, and some usages are specific to Japanese due to its linguistic nature. Therefore, just adopting the above mentioned methods, which have been developed to process non-Japanese texts, is not sufficient to enable high-quality comma insertion into Japanese sentences. Development of a method based on the detailed analysis of Japanese commas is required. Furthermore, there have been some investigations on comma insertion into Japanese written texts (Hayashi, 1992; Suzuki et al., 1995). These investigations have adopted rule-based methods. However, the number of their rules is not necessarily sufficient, and no quantitative evaluation has been performed. 3 Table 1: Categorization of usages of commas # 1 2 3 usage of comma commas between clauses commas indicating clear dependency relations commas for avoiding reading mistakes and reading difficulty commas indicating the subject commas inserted after a conjunction or adverb at the beginning of a sentence commas inserted between parallel words or phrases commas inserted after an adverbial phrase to indica"
D10-1087,W08-1703,0,0.0213384,"since pause information cannot be obtained from texts, we cannot use this approach because our targets are written texts. In addition, there have been some investigations 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. 892 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 892–901, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics on comma insertion into non-Japanese written texts (White and Rajkumar, 2008; Guo et al., 2010). In Japanese, there are several usages of commas, and some usages are specific to Japanese due to its linguistic nature. Therefore, just adopting the above mentioned methods, which have been developed to process non-Japanese texts, is not sufficient to enable high-quality comma insertion into Japanese sentences. Development of a method based on the detailed analysis of Japanese commas is required. Furthermore, there have been some investigations on comma insertion into Japanese written texts (Hayashi, 1992; Suzuki et al., 1995). These investigations have adopted rule-based"
I05-4006,maekawa-etal-2000-spontaneous,0,0.0387294,"䉡䊄䊮䊉 䈍ᐫ [restaurant] &䉥䊚䉶 ⴕ䈐䈢䈇䉖䈪䈜䈏&lt;SB&gt; [want to go] &䉟䉨䉺䉟䊮䊂䉴䉧&lt;SB&gt; 0023 - 01:42:368-01:49:961 F:O:I:C: 䈲䈇 [well] &䊊䉟 䈖䈱 [this area] &䉮䊉 ㄭ䈒䈪䈜䈫 [near] &䉼䉦䉪䊂䉴䊃 ⺪⸰ደ [SUWAYA] &䉴䊪䊟 ජ⒳⼾䈏 [“CHIKUSA HOUGETSU”]&䉼䉪䉰䊖䊷䉭䉿䉧 䈗䈙䈇䉁䈜䈏&lt;SB&gt; [there are ] &䉯䉱䉟䊙䉴䉧&lt;SB&gt; In this project, a system was specially built in a Data Collection Vehicle (DCV), shown in Figure 1, and was used for the synchronous recording of multi-channel audio data, multi-channel video data, and vehicle related data. All dialogue data were transcribed according to transcription standards in compliance with CSJ (Corpus of Spontaneous Japanese) (Maekawa, 2000) and were assigned discourse tags such as fillers, hesitations, and slips. An example of a transcript is shown in Figure 2. Utterances were divided into utterance units by a pause of 200 ms or more. Figure 2: Transcription of in-car dialogue speech Discourse act Express (Exp) Propose (Pro) Action Confirm (Con) Exhibit (Exh) Guide (Gui) Request (Req) Statement (Sta) ReSearch (ReS) Reserve (Rev) Search (Sea) Suggest (Sug) Select (Sel) Object ExhibitDetail (ExD) Genre (Gen) IntentDetail (InD) Parking (Par) ParkingInfo (PaI) RequestDetail (ReD) ReserveInfo (ReI) SearchResult (SeR) SelectDetail (Se"
irie-etal-2006-layered,W00-1001,0,\N,Missing
irie-etal-2006-layered,H01-1015,0,\N,Missing
irie-etal-2006-layered,kawaguchi-etal-2002-multi,1,\N,Missing
irie-etal-2006-layered,maekawa-etal-2000-spontaneous,0,\N,Missing
kato-etal-2006-corpus,J93-2004,0,\N,Missing
kato-etal-2006-corpus,J03-4003,0,\N,Missing
kato-etal-2006-corpus,P05-3009,0,\N,Missing
kawaguchi-etal-2002-multi,matsubara-etal-2002-bilingual,1,\N,Missing
kozawa-etal-2008-automatic,A00-2018,0,\N,Missing
kozawa-etal-2008-automatic,C92-2083,0,\N,Missing
kozawa-etal-2008-automatic,Y04-1012,0,\N,Missing
kozawa-etal-2008-automatic,P07-1112,0,\N,Missing
kozawa-etal-2008-automatic,bouillon-etal-2002-acquisition,0,\N,Missing
kozawa-etal-2008-automatic,C96-1079,0,\N,Missing
kozawa-etal-2010-collection,kozawa-etal-2008-automatic,1,\N,Missing
kozawa-etal-2010-collection,quasthoff-etal-2006-corpus,0,\N,Missing
kozawa-etal-2010-collection,biemann-etal-2004-web,0,\N,Missing
kozawa-etal-2010-collection,tohyama-etal-2008-construction-metadata,1,\N,Missing
kozawa-etal-2010-collection,dalli-etal-2004-web,0,\N,Missing
L16-1244,P03-2041,0,0.0858869,"e obtained 1,987 rules which are not constructed by the previous method, and the rules achieved good precision. This paper is organized as follows: Section 2 introduces the previous method of correcting errors in a treebank. Section 3 explains our method which is based on tree mining. Section 4 reports experimental results using the Penn Treebank. NP S PP VP TO VP NP VP TO VB VP VB PP Figure 1: An example of STSG rule 2. Previous Work Kato and Matsubara (2010) propose a method of correcting annotation errors in a treebank. Their method is based on synchronous tree substitution grammar (STSG) (Eisner, 2003). An STSG defines a tree-to-tree mapping, and consists of rules each of which is defined as a pair of trees called elementary trees. The one tree is called source, and the other is called target. Figure 1 shows an example of STSG rule. The rule transforms the structure which matches the source into the target’s structure. To correct annotation errors in a treebank, the method constructs STSG rules which transform incorrect structures to correct one and applies them to the treebank. The STSG rules are constructed as follows: 1. Make a pseudo parallel corpus, which is a collection of pairs of pa"
L16-1244,P10-2014,1,0.768693,"annotation process. So, detecting and correcting errors in annotated corpora are important tasks. Many studies suggest methods of detecting or correcting errors in various kinds of annotated corpora (see (Dickinson, 2015) for a survey). There are several methods of detecting annotation errors in a phrase structure treebank (Dickinson and Meurers, 2003; Ule and Simov, 2004; Dickinson and Meurers, 2005; Boyd et al., 2007; Dickinson, 2009; Przepi´orkowski and Lenart, 2012; Kulick et al., 2013; Faria, 2014). However, there is little work on treebank error correction. One exception is the work of Kato and Matsubara (2010). Their method constructs a pseudo parallel corpus where incorrect parse trees are paired with correct ones, and extracts error correction rules from the parallel corpus. The rules transform incorrect tree patterns to correct ones. By applying these rules to a treebank, the method corrects errors. However, this method does not achieve wide coverage of error correction. To solve this problem, we propose another approach to construct error correction rules. Our method does not construct a pseudo parallel corpus. In our method, we consider that an infrequent tree pattern which can be transformed"
L16-1244,N13-1061,0,0.0249493,"EQT 1. Introduction source target S It is inevitable for annotated corpora to contain errors caused by manual or semi-manual annotation process. So, detecting and correcting errors in annotated corpora are important tasks. Many studies suggest methods of detecting or correcting errors in various kinds of annotated corpora (see (Dickinson, 2015) for a survey). There are several methods of detecting annotation errors in a phrase structure treebank (Dickinson and Meurers, 2003; Ule and Simov, 2004; Dickinson and Meurers, 2005; Boyd et al., 2007; Dickinson, 2009; Przepi´orkowski and Lenart, 2012; Kulick et al., 2013; Faria, 2014). However, there is little work on treebank error correction. One exception is the work of Kato and Matsubara (2010). Their method constructs a pseudo parallel corpus where incorrect parse trees are paired with correct ones, and extracts error correction rules from the parallel corpus. The rules transform incorrect tree patterns to correct ones. By applying these rules to a treebank, the method corrects errors. However, this method does not achieve wide coverage of error correction. To solve this problem, we propose another approach to construct error correction rules. Our method"
L16-1244,J93-2004,0,0.0544997,"ration of Infrequent Source Patterns To seek infrequent source patterns efficiently, we focus on leaf elements of patterns. According as pattern expansion proceeds from left to right, it is determined whether or not a grammar rule is applied to a leaf element. Once a leaf element is skipped, it never has a grammar rule. We call such element determined leaf. In Figure 4 and 6, determined leaves are marked with an asterisk. Our method expands a pattern τ only if there exists a frequent pattern τt which fulfills the following conditions: 1. root(τ ) = root(τt ). nal section of the Penn Treebank (Marcus et al., 1993). We implemented our method in Java. The experiment was run on a PC (Intel core i7 3.40GHz) with 8GB main memory, running Windows 7 Professional. The threshold σ was set to 100. We obtained 2,379 rules. This took about 34 minutes1 . In these rules, 1,987 rules can not be obtained by Kato and Matsubara’s method. To measure the precision of the rules, we applied rules to the WSJ section. Because it is time-consuming and expensive to evaluate all rules, we only evaluated the rules with the 300 highest scores. A person (not the authors) manually checks whether or not each rule corrects errors. The"
L16-1244,W12-3615,0,0.0394889,"Missing"
L16-1244,ule-simov-2004-unexpected,0,0.0296191,"ous method, and the rules achieved good precision. Keywords: error correction, synchronous tree substitution grammar, FREQT 1. Introduction source target S It is inevitable for annotated corpora to contain errors caused by manual or semi-manual annotation process. So, detecting and correcting errors in annotated corpora are important tasks. Many studies suggest methods of detecting or correcting errors in various kinds of annotated corpora (see (Dickinson, 2015) for a survey). There are several methods of detecting annotation errors in a phrase structure treebank (Dickinson and Meurers, 2003; Ule and Simov, 2004; Dickinson and Meurers, 2005; Boyd et al., 2007; Dickinson, 2009; Przepi´orkowski and Lenart, 2012; Kulick et al., 2013; Faria, 2014). However, there is little work on treebank error correction. One exception is the work of Kato and Matsubara (2010). Their method constructs a pseudo parallel corpus where incorrect parse trees are paired with correct ones, and extracts error correction rules from the parallel corpus. The rules transform incorrect tree patterns to correct ones. By applying these rules to a treebank, the method corrects errors. However, this method does not achieve wide coverage"
L18-1676,maekawa-etal-2000-spontaneous,0,0.115308,"he SIDB (Matsubara et al., 2002). 3.1.1. Scale and Features of Analyzed Data The SIBD includes monologue data (lectures) and dialogue data, and their corresponding J-E and E-J interpretations. In our analyses, 22 English lectures interpreted by four interpreters (i.e., 88 E-J interpretations) were used. The data statistics are shown in Table 1. The recorded speech data of both the source speakers and the interpreters were separated into utterance units of 200millisecond or longer pauses. All utterance units were transcribed manually in compliance with the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000), and each utterance unit was assigned a start and end time. Spontaneous language phenomena, such as fillers, repetitions, and mistakes, were tagged with discourse tags. 3.1.2. Word-level Translation Alignment Word-level translation alignment is essential for analyses of missing translations in simultaneous interpretation. The data used in this research include translation alignment at an utterance unit level (Takagi et al., 2002). The analyzed data comprise 14,679 utterance unit level alignments. In addition, word translation correspondences were aligned • Phrases and idioms are aligned as a"
L18-1676,ono-etal-2008-construction,1,0.595027,"Missing"
L18-1676,tohyama-matsubara-2006-collection,1,0.555285,"no correspondence” are defined as missing translations in the simultaneous interpretation. As described previously, omissions in interpretations can be classified into different categories. In addition to the four categories defined by Barik (1994), omissions can be classified as conscious strategic omissions, conscious intentional omissions, conscious unintentional omissions, conscious receptive omissions and unconscious omissions (Napier, 2004). For example, interpreters can omit unnecessary words and summarize content to increase interpreter simultaneity in E-J simultaneous interpretation (Tohyama and Matsubara, 2006). Note that determining the type of missing translation and whether a word is unnecessary are subjective operations (Barik, 1994). In addition, it is impossible to classify missing translations automatically. However, to analyze missing translations as defined in previous studies, aligned words that satisfy the following conditions are excluded from the analyses: • Determiners, existential there words, and prepositions (i.e., not content words). • Pronouns. In E-J translations, English pronouns are usually omitted to obtain a more natural Japanese translation (Anzai, 2008). • Words tagged as r"
L18-1676,matsubara-etal-2002-bilingual,1,0.538339,"d and several factors related to omissions have been studied (Barik, 1994). However, these analyses, which were based on observation, did not clarify the correlation between the identified factors and the occurrence of omissions. In this paper, to detect occasions where interpreters would encounter difficulties, we statistically analyzed the correlation between source speech features and interpretation conditions and the occurrence of missing translations. In the analyses, we used 88 lectures of English-to-Japanese (EJ) interpretation data from the Simultaneous Interpretation Database (SIDB) (Matsubara et al., 2002). Note that wordlevel alignments were created manually. 2. Missing Translations in Simultaneous Interpretation In simultaneous interpretation, departures from the source speech in interpreters’ renditions include omissions, additions, and errors. Omissions refer to items that are present in the source speech but not included in the translation (Barik, 1994). However, if an interpreter does not translate a lexically irrelevant repetition or a mistake in the source speech, such as a false start, it is not considered an omission because these are phenomena in spontaneous language. Barik classifie"
murata-etal-2010-construction,ono-etal-2008-construction,1,\N,Missing
murata-etal-2010-construction,P09-1060,1,\N,Missing
murata-etal-2010-construction,W06-3711,0,\N,Missing
murata-etal-2010-construction,P06-2088,1,\N,Missing
murata-etal-2010-construction,matsubara-etal-2002-bilingual,1,\N,Missing
murata-etal-2010-construction,frederking-etal-2002-field,0,\N,Missing
ohno-etal-2006-syntactically,J93-2004,0,\N,Missing
ohno-etal-2006-syntactically,P92-1041,0,\N,Missing
ohno-etal-2006-syntactically,maekawa-etal-2000-spontaneous,0,\N,Missing
ono-etal-2008-construction,A00-2018,0,\N,Missing
ono-etal-2008-construction,matsubara-etal-2002-bilingual,1,\N,Missing
ono-etal-2008-construction,maekawa-etal-2000-spontaneous,0,\N,Missing
P06-1022,W02-2016,0,0.031121,"of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel probability P (bik → bil |Bi ) is calculated as follows: rel P (bik →"
P06-1022,J94-4001,0,0.0440763,"` our method ``` correct conv. method ```` correct 1037 103 incorrect total 1,140 incorrect total 63 534 597 1,100 637 1,737 Since monologue sentences tend to be long and have complex structures, it is important to consider the features. Although there have been very few studies on parsing monologue sentences, some studies on parsing written language have dealt with long-sentence parsing. To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the “clause.” First, there are the studies that focused attention on compound clauses (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994). These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures. Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the “scope-embedding preference” of Japanese subordinate clauses (Shirai et al., 1995; Utsuro et al., 2000). Furthermore, Kim et al. (Kim and Lee, 2004) divided a sentence into “S(ubject)-clauses,” which were defined as a group of words containing several predicates and their common subject. The above studies have attempted to reduc"
P06-1022,maekawa-etal-2000-spontaneous,0,0.0946367,"Missing"
P06-1022,P92-1003,0,0.0521559,"a clause boundary unit) ``` our method ``` correct conv. method ```` correct 1037 103 incorrect total 1,140 incorrect total 63 534 597 1,100 637 1,737 Since monologue sentences tend to be long and have complex structures, it is important to consider the features. Although there have been very few studies on parsing monologue sentences, some studies on parsing written language have dealt with long-sentence parsing. To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the “clause.” First, there are the studies that focused attention on compound clauses (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994). These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures. Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the “scope-embedding preference” of Japanese subordinate clauses (Shirai et al., 1995; Utsuro et al., 2000). Furthermore, Kim et al. (Kim and Lee, 2004) divided a sentence into “S(ubject)-clauses,” which were defined as a group of words containing several predicates and their common subject. The above stu"
P06-1022,W97-0301,0,0.0218029,"unsetsu has one or more ancillary words, the type of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attribute"
P06-1022,A00-2018,0,0.0125219,"e lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel probability P (bik → bil |Bi ) is calculate"
P06-1022,P96-1025,0,0.0724292,"f a dependent bunsetsu has one or more ancillary words, the type of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using"
P06-1022,P99-1053,0,0.0325274,", Nagoya University, Japan ‡Information Technology Center, Nagoya University, Japan §ATR Spoken Language Communication Research Laboratories, Japan ]The National Institute for Japanese Language, Japan Faculty of Information Science and Technology, Aichi Prefectural University, Japan a)ohno@el.itc.nagoya-u.ac.jp Abstract logues. Spontaneously spoken monologues include a lot of grammatically ill-formed linguistic phenomena such as fillers, hesitations and selfrepairs. In order to robustly deal with their extragrammaticality, some techniques for parsing of dialogue sentences have been proposed (Core and Schubert, 1999; Delmonte, 2003; Ohno et al., 2005b). On the other hand, monologues also have the characteristic feature that a sentence is generally longer and structurally more complicated than a sentence in dialogues which have been dealt with by the previous researches. Therefore, for a monologue sentence the parsing time would increase and the parsing accuracy would decrease. It is thought that more effective, high-performance spoken monologue parsing could be achieved by dividing a sentence into suitable language units for simplicity. This paper proposes a method for dependency parsing of monologue sen"
P06-1022,C04-1159,0,0.0355551,"od, the transcribed sentence on which morphological analysis, clause boundary detection, and bunsetsu segmentation are performed is considered the input 4 . The dependency 3 Asu-Wo-Yomu is a collection of transcriptions of a TV commentary program of the Japan Broadcasting Corporation (NHK). The commentator speaks on some current social issue for 10 minutes. 4 It is difficult to preliminarily divide a monologue into sentences because there are no clear sentence breaks in monologues. However, since some methods for detecting sentence boundaries have already been proposed (Huang and Zweig, 2002; Shitaoka et al., 2004), we assume that they can be detected automatically before dependency parsing. P (Si |Bi ) = nY i −1 k=1 171 rel P (bik → bil |Bi ), (1) rel Note that F is a co-occurrence frequency function. In order to resolve the sparse data problems rel caused by estimating P (bik → bil |Bi ) with formula (2), we adopted the smoothing method described by Fujio and Matsumoto (Fujio and Matsumoto, i 1998): if F (hik , hil , tik , til , rki , dii kl , el ) in formula (2) where P (bik → bil |Bi ) is the probability that a bunsetsu bik depends on a bunsetsu bil when the sequence of bunsetsus Bi is provided. Unl"
P06-1022,W98-1511,0,0.0175422,"more ancillary words, the type of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel prob"
P06-1022,E99-1026,0,0.0206787,"ype of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel probability P (bik → bil |B"
P06-1022,A00-2015,0,0.0164835,"nce parsing. To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the “clause.” First, there are the studies that focused attention on compound clauses (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994). These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures. Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the “scope-embedding preference” of Japanese subordinate clauses (Shirai et al., 1995; Utsuro et al., 2000). Furthermore, Kim et al. (Kim and Lee, 2004) divided a sentence into “S(ubject)-clauses,” which were defined as a group of words containing several predicates and their common subject. The above studies have attempted to reduce the parsing ambiguity between specific types of clauses in order to improve the parsing accuracy of an entire sentence. On the other hand, our method utilizes all types of clauses without limiting them to specific types of clauses. To improve the accuracy of longsentence parsing, we thought that it would be more effective to cyclopaedically divide a sentence into all t"
P06-1022,C02-1136,1,\N,Missing
P06-2088,J93-2004,0,0.0430125,"sh phrase structure” in Fig. 6). When we parse the phrase structure for input words incrementally, there arises the problem of ambiguity; our method needs to determine only one parsing result at a time. To resolve this problem our system selects the phrase structure of the maximum likelihood at that time by using PCFG (Probabilistic Context-Free Grammar) rules. To resolve the problem of the processing time our system sets a cut-off value. 686 PentiumM PC with 512 MB of RAM. The OS was Windows XP. The experiment used all 578 sentences in the ATIS corpus with a parse tree, in the Penn Treebank (Marcus et al. 1993). In addition, we used 533 syntax rules, which were extracted from the corpus’ parse tree. The position of the head child in the grammatical rule was defined according to Collins’ method (Collins, 1999). generation module the system has to recognize the predicate of an input sentence. This system recognizes the chunk (e.g. “want to fly”) on which the subject chunk (e.g. “I”) depends as a predicate. 4.2 Incremental Transfer In the transfer module, structure and lexicon transfer rules transform the English dependency structure into the Japanese case structure (“Japanese case structure” in Fig. 6"
P06-2088,P98-2141,0,0.0376483,"s in conversations. One effective method of improving these problems is that a translation system begins to translate the words without waiting for the end of the speaker’s utterance, much as a simultaneous interpreter does. This has been verified as possible by a study on comparing simultaneous interpretation with consecutive interpretation from the viewpoint of efficiency and smoothness of cross-language conversations (Ohara et al., 2003). There has also been some research on simultaneous machine interpretation with the aim of developing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken"
P06-2088,W02-0706,0,0.0255424,"One effective method of improving these problems is that a translation system begins to translate the words without waiting for the end of the speaker’s utterance, much as a simultaneous interpreter does. This has been verified as possible by a study on comparing simultaneous interpretation with consecutive interpretation from the viewpoint of efficiency and smoothness of cross-language conversations (Ohara et al., 2003). There has also been some research on simultaneous machine interpretation with the aim of developing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such"
P06-2088,hoge-2002-project,0,0.0213495,"ing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such as IWSLT have been held. Though some speech 683 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683–690, c Sydney, July 2006. 2006 Association for Computational Linguistics our method utilizes dependency structures and Japanese dependency constraints to determine the word order of a translation. Moreover, by considering the fact that the inversion of predicate expressions occurs more frequently in Japanese spoken language, our method employs predicate inversion to resolve the pr"
P06-2088,J03-4003,0,\N,Missing
P06-2088,frederking-etal-2002-field,0,\N,Missing
P06-2088,C98-2136,0,\N,Missing
P09-1060,W02-2016,0,0.339023,"Missing"
P09-1060,matsubara-etal-2002-bilingual,1,0.526722,"accordance with only the width of a screen without considering the proper points of linefeeds, the caption is not easy to read. Especially, since the audience is forced to read the caption in synchronization with the speaker’s utterance speed, it is important that linefeeds are properly inserted into the displayed text in consideration of the readability as shown in Figure 3. To investigate whether the line insertion facilitates the readability of the displayed texts, we conducted an experiment using the transcribed text of lecture speeches in the Simultaneous Interpretation Database (SIDB) (Matsubara et al., 2002). We randomly selected 50 sentences from the data, and then created the following two texts for each sentence based on two different concepts about linefeed insertion. (For example, environmental problem) (population problem) (AIDS problem and so on) a lot of global-scale problems have occurred (and unfortunately, these problems) (to continue during also 21st century) (or if we look through blue glasses) (seems to become worse) 49 1 50 2 49 3 37 40 4 5 36 6 43 7 49 48 34 8 9 10 subject ID Figure 4: Result of investigation of effect of linefeed insertion into transcription （2）Text into which li"
P09-2011,C00-1052,0,0.0324821,"een adopted by Lombardo and Sturt (1997) and Kato et al. (2004). However, this raises another problem that their adjoining operations cannot preserve lexical dependencies of partial parse trees. This paper proposes a restricted 1 More precisely, the chain is attached after attaching endof-constituent # under the NP node. 41 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 41–44, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP S (a) (c) S (b) S VP NP NP PRP PRP VBP PRP We We describe We (d) VBP NP describe DT a S VP NP PRP We with a selective left-corner transformation (Johnson and Roark, 2000) before inducing the allowable chains and allowable triples (Collins and Roark, 2004). The first and second approaches can prevent the parser from infinitely producing partial parse trees, but the parser has to produce partial parse trees as shown in Figure 1. The local ambiguity still remains. In the third approach, no left recursive structure exists in the transformed grammar, but the parse trees defined by the grammar are different from those defined by the original grammar. It is not clear if partial parse trees defined by the transformed grammar represent syntactic relations correctly. As"
P09-2011,W04-0302,1,0.893561,"trees on a word-by-word basis. However, they suffer from the problem of infinite local ambiguity, i.e., they may produce an infinite number of candidates of partial parse trees. This problem is caused by the fact that partial parse trees can have arbitrarily nested left-recursive structures and there is no information to predict the depth of nesting. To solve the problem, this paper proposes an incremental parsing method based on an adjoining operation. By using the operation, we can avoid the problem of infinite local ambiguity. This approach has been adopted by Lombardo and Sturt (1997) and Kato et al. (2004). However, this raises another problem that their adjoining operations cannot preserve lexical dependencies of partial parse trees. This paper proposes a restricted 1 More precisely, the chain is attached after attaching endof-constituent # under the NP node. 41 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 41–44, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP S (a) (c) S (b) S VP NP NP PRP PRP VBP PRP We We describe We (d) VBP NP describe DT a S VP NP PRP We with a selective left-corner transformation (Johnson and Roark, 2000) before inducing the allowable chains a"
P09-2011,J01-2004,0,0.67477,"→ describe⟩ to the partial parse tree (a) 1 . The attachment is possible when the allowable triple ⟨S, NP, VP⟩ exists in B. Introduction Incremental parser reads a sentence from left to right, and produces partial parse trees which span all words in each initial fragment of the sentence. Incremental parsing is useful to realize real-time spoken language processing systems, such as a simultaneous machine interpretation system, an automatic captioning system, or a spoken dialogue system (Allen et al., 2001). Several incremental parsing methods have been proposed so far (Collins and Roark, 2004; Roark, 2001; Roark, 2004). In these methods, the parsers can produce the candidates of partial parse trees on a word-by-word basis. However, they suffer from the problem of infinite local ambiguity, i.e., they may produce an infinite number of candidates of partial parse trees. This problem is caused by the fact that partial parse trees can have arbitrarily nested left-recursive structures and there is no information to predict the depth of nesting. To solve the problem, this paper proposes an incremental parsing method based on an adjoining operation. By using the operation, we can avoid the problem of"
P09-2011,J03-4003,0,\N,Missing
P09-2011,P04-1015,0,\N,Missing
P10-2014,E03-1068,0,0.499398,". Section 3 explains our method of correcting errors in a treebank. Section 4 reports an experimental result using the Penn Treebank. Introduction 2 Previous Work Annotated corpora play an important role in the fields such as theoretical linguistic researches or the development of NLP systems. However, they often contain annotation errors which are caused by a manual or semi-manual mark-up process. These errors are problematic for corpus-based researches. To solve this problem, several error detection and correction methods have been proposed so far (Eskin, 2000; Nakagawa and Matsumoto, 2002; Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b; Ule and Simov, 2004; Murata et al., 2005; Dickinson and Meurers, 2005; Boyd et al., 2008). These methods detect corpus positions which are marked up incorrectly, and find the correct labels (e.g. pos-tags) for those positions. However, the methods cannot correct errors in structural annotation. This means that they are insufficient to correct annotation errors in a treebank. This paper proposes a method of correcting errors in structural annotation. Our method is based on a synchronous grammar formalism, called synchronous tree substitution grammar (STSG) (Eisn"
P10-2014,P03-2041,0,0.544347,"2003a; Dickinson and Meurers, 2003b; Ule and Simov, 2004; Murata et al., 2005; Dickinson and Meurers, 2005; Boyd et al., 2008). These methods detect corpus positions which are marked up incorrectly, and find the correct labels (e.g. pos-tags) for those positions. However, the methods cannot correct errors in structural annotation. This means that they are insufficient to correct annotation errors in a treebank. This paper proposes a method of correcting errors in structural annotation. Our method is based on a synchronous grammar formalism, called synchronous tree substitution grammar (STSG) (Eisner, 2003), which defines a tree-to-tree transforThis section summarizes previous methods for correcting errors in corpus annotation and discusses their problem. Some research addresses the detection of errors in pos-annotation (Nakagawa and Matsumoto, 2002; Dickinson and Meurers, 2003a), syntactic annotation (Dickinson and Meurers, 2003b; Ule and Simov, 2004; Dickinson and Meurers, 2005), and dependency annotation (Boyd et al., 2008). These methods only detect corpus positions where errors occur. It is unclear how we can correct the errors. Several methods can correct annotation errors (Eskin, 2000; Mu"
P10-2014,A00-2020,0,0.879466,"ection 2 gives an overview of previous work. Section 3 explains our method of correcting errors in a treebank. Section 4 reports an experimental result using the Penn Treebank. Introduction 2 Previous Work Annotated corpora play an important role in the fields such as theoretical linguistic researches or the development of NLP systems. However, they often contain annotation errors which are caused by a manual or semi-manual mark-up process. These errors are problematic for corpus-based researches. To solve this problem, several error detection and correction methods have been proposed so far (Eskin, 2000; Nakagawa and Matsumoto, 2002; Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b; Ule and Simov, 2004; Murata et al., 2005; Dickinson and Meurers, 2005; Boyd et al., 2008). These methods detect corpus positions which are marked up incorrectly, and find the correct labels (e.g. pos-tags) for those positions. However, the methods cannot correct errors in structural annotation. This means that they are insufficient to correct annotation errors in a treebank. This paper proposes a method of correcting errors in structural annotation. Our method is based on a synchronous grammar formalism"
P10-2014,J93-2004,0,0.0335891,"ere f (·) is the occurrence frequency in the treebank. The score function ranges from 0 to 1. We assume that the occurrence frequency of an elementary tree matching incorrect parse trees is very low. According to this assumption, the score function Score(⟨t, t′ ⟩) is high when the source elementary tree t matches incorrect parse trees and the target elementary tree t′ matches correct parse trees. Therefore, STSG rules with high scores are regarded to be useful for error correction. 4 An Experiment To evaluate the effectiveness of our method, we conducted an experiment using the Penn Treebank (Marcus et al., 1993). We used 49208 sentences in Wall Street Journal sections. We induced STSG rules by applying our method to the corpus. We obtained 8776 rules. We 2 This also means that we cannot measure the recall of the rules. 77 (a) PRN S , (c) NP VP I think (b) PRN , VP I think NP VP NP NP PP NP , S , S S VP all you need is one good one is one good one all you need PP IN DT of the respondents NNS IN NP DT of NNS the respondents (d) NP NP NP NP NP only two or three other major banks IN NP PP only two or three other major banks in IN NP in the U.S. the U.S. Figure 7: Examples of correcting syntactic annotati"
P10-2014,C02-1101,0,0.107045,"s an overview of previous work. Section 3 explains our method of correcting errors in a treebank. Section 4 reports an experimental result using the Penn Treebank. Introduction 2 Previous Work Annotated corpora play an important role in the fields such as theoretical linguistic researches or the development of NLP systems. However, they often contain annotation errors which are caused by a manual or semi-manual mark-up process. These errors are problematic for corpus-based researches. To solve this problem, several error detection and correction methods have been proposed so far (Eskin, 2000; Nakagawa and Matsumoto, 2002; Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b; Ule and Simov, 2004; Murata et al., 2005; Dickinson and Meurers, 2005; Boyd et al., 2008). These methods detect corpus positions which are marked up incorrectly, and find the correct labels (e.g. pos-tags) for those positions. However, the methods cannot correct errors in structural annotation. This means that they are insufficient to correct annotation errors in a treebank. This paper proposes a method of correcting errors in structural annotation. Our method is based on a synchronous grammar formalism, called synchronous tree subs"
P10-2014,ule-simov-2004-unexpected,0,0.283066,"eebank. Section 4 reports an experimental result using the Penn Treebank. Introduction 2 Previous Work Annotated corpora play an important role in the fields such as theoretical linguistic researches or the development of NLP systems. However, they often contain annotation errors which are caused by a manual or semi-manual mark-up process. These errors are problematic for corpus-based researches. To solve this problem, several error detection and correction methods have been proposed so far (Eskin, 2000; Nakagawa and Matsumoto, 2002; Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b; Ule and Simov, 2004; Murata et al., 2005; Dickinson and Meurers, 2005; Boyd et al., 2008). These methods detect corpus positions which are marked up incorrectly, and find the correct labels (e.g. pos-tags) for those positions. However, the methods cannot correct errors in structural annotation. This means that they are insufficient to correct annotation errors in a treebank. This paper proposes a method of correcting errors in structural annotation. Our method is based on a synchronous grammar formalism, called synchronous tree substitution grammar (STSG) (Eisner, 2003), which defines a tree-to-tree transforThis"
P16-1088,H91-1060,0,0.423857,"son (2002) and Campbell (2004) used the output of Charniak’s parser. Table 3: Baseline feature templates. feature templates s0 .n0 .c, s0 .n1 .c, s1 .n0 .c, s1 .n1 .c, rest2 .n0 .c, rest2 .n1 .c Table 5: Comparison for constituent parsing. Table 4: Nonlocal dependency feature templates. length of action sequences (= N ) was set to 7n, where n is the length of input sentence. This maximum length was determined to deal with the sentences in the training data. Table 5 presents the constituent parsing performances of our system and previous systems. We used the labeled bracketing metric PARSEVAL (Black et al., 1991). Here, “CF” is the parser which was learned from the training data where nonlocal dependencies are removed. This result demonstrates that our nonlocal dependency identification does not have a bad influence on constituent parsing. From the viewpoint of transitionbased constituent parsing, our left-corner parser is somewhat inferior to other perceptron-based shiftreduce parsers. On the other hand, our parser outperforms the parsers which identify nonlocal dependency based on in-processing approach. We use the metric proposed by Johnson (2002) to evaluate the accuracy of nonlocal dependency ide"
P16-1088,K15-1029,0,0.0213126,"ns, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies. An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal dependency"
P16-1088,P04-1042,0,0.763683,"A-movement in passives, topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics terminal symbol of empty element is indexed, its filler exists in the parse tree. The filler has"
P16-1088,P04-1082,0,0.811154,"topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics terminal symbol of empty element is indexed, its filler exists in the parse tree. The filler has the same number."
P16-1088,P15-1116,0,0.0584225,"y elements with their fillers using simple heuristic rules, which are developed for our transition system. 4.1 Empty Element Detection • L EFT C ORNER and ATTACH are not allowed when s0 has no head child. We introduce the following action to deal with empty elements: • ATTACH-H is not allowed when s1 has a head child. E-S HIFT(E, t) : Table 2 shows the first several transition actions which derive the parse tree shown in Figure 1. Head children are indicated by the superscript ∗ . Previous transition-based constituent parsing does not handle nonlocal dependencies. One exception is the work of Maier (2015), who proposes shift-reduce constituent parsing with swap action. The parser can handle nonlocal dependencies represented as discontinuous constituents. In this framework, discontinuities are directly annotated by allowing crossing branches. Since the annotation style is quite different from the PTB annotation, the parser is not suitable for identifying (⟨sm , . . . , s0 ⟩, i) ⇒ (⟨sm , . . . , s0 , [t]E ⟩, i) This action simply inserts an empty element at an arbitrary position and pops up no element from the buffer (see the transition from #11 to #12 shown in Table 2 as an example). 4.2 Annota"
P16-1088,A00-2018,0,0.543084,"Missing"
P16-1088,J93-2004,0,0.0590189,"s in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies. An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal dependency identification. 1 Introduction Many constituent parsers based on the Penn Treebank (Marcus et al., 1993) are available, but most of them do not deal with nonlocal dependencies. Nonlocal dependencies represent syntactic phenomenon such as wh-movement, A-movement in passives, topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-pro"
P16-1088,W02-1001,0,0.429377,"in-processing approach uses a probabilistic context-free grammar, which makes it difficult to use global features. Postprocessing approach performs constituent parsing and nonlocal dependency identification separately. This means that the constituent parser cannot use any kind of information about nonlocal dependencies. This paper proposes a parser which integrates nonlocal dependency identification into constituent parsing. Our method adopts an inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballester"
P16-1088,W03-1005,0,0.676337,"n Treebank (Marcus et al., 1993) are available, but most of them do not deal with nonlocal dependencies. Nonlocal dependencies represent syntactic phenomenon such as wh-movement, A-movement in passives, topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Be"
P16-1088,N15-1108,0,0.0780437,"inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which enables our parser to utilize global fea"
P16-1088,P03-1055,0,0.809962,"n Treebank (Marcus et al., 1993) are available, but most of them do not deal with nonlocal dependencies. Nonlocal dependencies represent syntactic phenomenon such as wh-movement, A-movement in passives, topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Be"
P16-1088,W05-1513,0,0.175246,"a parser which integrates nonlocal dependency identification into constituent parsing. Our method adopts an inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into"
P16-1088,P06-2089,0,0.103317,"es nonlocal dependency identification into constituent parsing. Our method adopts an inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based syst"
P16-1088,W11-2913,0,0.117644,"dencies represented as discontinuous constituents. In this framework, discontinuities are directly annotated by allowing crossing branches. Since the annotation style is quite different from the PTB annotation, the parser is not suitable for identifying (⟨sm , . . . , s0 ⟩, i) ⇒ (⟨sm , . . . , s0 , [t]E ⟩, i) This action simply inserts an empty element at an arbitrary position and pops up no element from the buffer (see the transition from #11 to #12 shown in Table 2 as an example). 4.2 Annotations For empty element resolution, we augment the Penn Treebank. For nonlocal dependency types 2 In (Evang and Kallmeyer, 2011), the PTB-style annotation of types ∗EXP, ∗ICH∗, ∗RNR∗ and ∗T∗ is transformed into an annotation with crossing branches. 932 action (initial state) S HIFT(DT) L EFT C ORNER-∅(NP) S HIFT(NNP) ATTACH-∅ S HIFT(NN) ATTACH-H L EFT C ORNER-H(NP) S HIFT(WDT) L EFT C ORNER-H(WHNP-∗T∗-NP-L) L EFT C ORNER-H(SBAR) E-S HIFT(-NONE-NP-L, ∗T∗) # 1 2 3 4 5 6 7 8 9 10 11 12 state (⟨⟩, 0) (⟨[the]DT ⟩, 1) (⟨[[the]DT ]NP ⟩, 1) (⟨[[the]DT ]NP , [U.N.]NNP ⟩, 2) (⟨[[the]DT [U.N.]NNP ]NP ⟩, 2) (⟨[[the]DT [U.N.]NNP ]NP , [group]NN ⟩, 3) (⟨[[the]DT [U.N.]NNP [group]NN∗ ]NP ⟩, 3) (⟨[[[the]DT [U.N.]NNP [group]NN∗ ]NP∗ ]N"
P16-1088,P06-1023,0,0.768656,"l with nonlocal dependencies. Nonlocal dependencies represent syntactic phenomenon such as wh-movement, A-movement in passives, topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computatio"
P16-1088,N03-1014,0,0.0608956,", s2 , [σ1 s0 ]X ⟩, i) Figure 2: Transition actions for left-corner parsing. the PTB style nonlocal dependencies.2 difference between ATTACH and R EDUCE. The R EDUCE action cannot deal with any node with more than two children. For this reason, the previous work converts parse trees into binarized ones. The conversion makes it difficult to capture the hierarchical structure of the parse trees. On the other hand, ATTACH action can handle more than two children. Therefore, our parser does not require such kind of tree binarization. These transition actions are similar to the ones described in (Henderson, 2003), although his parser uses right-binarized trees and does not identify headchildren. Figure 2 summarizes the transition actions for our parser. To guarantee that every non-terminal node has exactly one head child, our parser uses the following constraints: 4 Nonlocal Dependency Identification Nonlocal dependency identification consists of two subtasks: • empty element detection. • empty element resolution, which coindexes empty elements with their fillers. Our parser can insert empty elements at an arbitrary position to realize empty element detection. This is in a similar manner as the in-pro"
P16-1088,W08-2121,0,0.0651163,"Missing"
P16-1088,N12-1015,0,0.0246357,"Missing"
P16-1088,D15-1156,0,0.0167606,"d so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics terminal symbol of empty element is indexed, its filler exists in the parse tree. The filler has the same number. For example, ∗T∗-1 means that the node WHNP-1 is the correspo"
P16-1088,P02-1018,0,0.75808,"as wh-movement, A-movement in passives, topicalization, raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics terminal symbol of empty element is indexed, its filler exists in the par"
P16-1088,P15-1148,0,0.0469212,"ch, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which enables our parser to utilize global features captured by no"
P16-1088,P14-1069,0,0.256602,"Our method adopts an inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which enables our parser t"
P16-1088,P15-1113,0,0.0470485,"a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies. An expe"
P16-1088,P13-1081,0,0.0172155,"ight node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics terminal symbol of empty element is indexed, its filler exists in the parse tree. The filler has the same number. For example, ∗T∗-1 means that the node"
P16-1088,N13-1125,0,0.0179221,"raising, control, right node raising and so on. Nonlocal dependencies play an important role on semantic interpretation. In the Penn Treebank, a nonlocal dependency is represented as a pair of an empty element and a filler. Several methods of identifying nonlocal dependencies have been proposed so far. These methods can be divided into three approaches: pre-processing approach (Dienes and Dubey, 2003b), in-processing approach (Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Kato and Matsubara, 2015) and post-processing approach (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015).1 In preprocessing approach, a tagger called “trace tagger” detects empty elements. The trace tagger uses 1 The methods of (Cai et al., 2011; Xue and Yang, 2013; Xiang et al., 2013; Takeno et al., 2015) only detect empty elements. 930 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 930–940, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics terminal symbol of empty element is indexed, its filler exists in the parse tree. The filler has the same number. For example, ∗T∗-1"
P16-1088,W09-3825,0,0.401283,"identification into constituent parsing. Our method adopts an inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured"
P16-1088,P13-1043,0,0.248186,"stituent parsing. Our method adopts an inprocessing approach, but does not use a probabilistic context-free grammar. Our parser is based on a transition system with structured perceptron (Collins, 2002), which can easily introduce global features to its parsing model. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role on nonlocal dependency identification. Previous work on transition-based constituent parsing adopts a shift-reduce strategy with a tree binarization (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhang and Clark, 2009; Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015; Thang et al., 2015; Watanabe and Sumita, 2015), or convert constituent trees to “spinal trees”, which are similar to dependency trees (Ballesteros and Carreras, 2015). These conversions make it difficult for their parsers to capture c-command relations in the parsing process. On the other hand, our parser does not require such kind of conversion. This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We use a structured perceptron which"
P16-1088,P11-2037,0,\N,Missing
P19-1530,P16-2016,0,0.104212,"Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ ϯϭϭ ƌĞŵŽǀŝŶŐE EW ^ t,EWͲϭ ƚŚĞĨĂƐƚďĂůů ͲEKEͲ E"
P19-1530,P02-1018,0,0.620714,"PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ ϯϭϭ ƌĞŵŽǀŝŶŐE EW ^ t,EWͲϭ ƚŚĞĨĂƐƚďĂůů ͲEKEͲ EWͲ^: Ϭ WZW ^Z EW ^Z EW ϯϭϮ ƌĞĐŽǀĞƌŝŶŐE s EW WW ϯϮϮ ƌĞĐŽǀĞƌŝŶŐ ^ ͲEKEͲ EWͲ^: Ϭ WZW ŚĞ ŚĞ ďƌŽƵŐŚƚ ͲEKE"
P19-1530,P16-1088,1,0.841407,"t al., 1995). 2.2 Previous Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ ϯϭϭ ƌĞŵŽǀŝŶŐE EW ^ t,"
P19-1530,P18-1249,0,0.0342473,"n the test data. Algorithm 3 Recovering empty element decode(x) creates a tree by decoding a string assigned by encode and returns its root. Input: a node x C ← children(x) C 0 ← hi while C 6= hi do pop the first element c from C if c is an inserted node and c has the tag L then C ← hdecode(c)i · children(c) · C else if c is an inserted node and has the tag R then C ← children(c) · hdecode(c)i · C else C 0 ← C 0 · hci end if end while return node(label(x), C 0 ) method6 , and a parsing model was trained using the PTB augmented trees. The hyperparameters for training were identical to those of Kitaev and Klein (2018a). We selected the model that maximizes the F1 score on the development data, where we treated the node labels of PTB augmented trees as constituent labels. For the test data (section 23), PTB graphs were recovered from the PTB augmented trees generated by the parser. The accuracy of the nonlocal dependency identification was evaluated by the metric proposed by Johnson (2002). First, we evaluated the performance of our approximation method. We recovered PTB graphs from not the parser output but the gold PTB augmented trees in the development data. We ob6 The conversion code is available at ht"
P19-1530,P11-2037,0,0.112837,"out PTB nonlocal dependencies, we refer readers to (Bies et al., 1995). 2.2 Previous Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1530,P04-1082,0,0.181079,"ile such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ ϯϭϭ ƌĞŵŽǀŝŶŐE EW ^ t,EWͲϭ ƚŚĞĨĂƐƚďĂůů ͲEKEͲ EWͲ^: Ϭ WZW ^Z EW ^Z EW ϯϭϮ ƌĞĐŽǀĞƌŝŶŐE s EW WW ϯϮϮ ƌĞĐŽǀĞƌŝŶŐ ^ ͲEKEͲ EWͲ^: Ϭ WZW ŚĞ ŚĞ ďƌŽƵŐŚƚ ͲEKEͲ EW EW t,EWͲΎdΎ"
P19-1530,W03-1005,0,0.381877,"ing filler WHNP-1. For more details about PTB nonlocal dependencies, we refer readers to (Bies et al., 1995). 2.2 Previous Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 -"
P19-1530,W11-2913,0,0.025425,"ependencies, we refer readers to (Bies et al., 1995). 2.2 Previous Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguist"
P19-1530,Q17-1031,0,0.20303,"rs deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ ϯϭϭ ƌĞŵŽǀŝŶŐE EW ^ t,EWͲϭ ƚŚĞĨĂƐƚďĂůů ͲEKEͲ EWͲ^: Ϭ WZW ^Z EW ^Z EW ϯ"
P19-1530,P04-1042,0,0.0770772,"are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ ϯϭϭ ƌĞŵŽǀŝŶŐE EW ^ t,EWͲϭ ƚŚĞĨĂƐƚďĂůů ͲEKEͲ EWͲ^: Ϭ WZW ^Z EW ^Z EW ϯϭϮ ƌĞĐŽǀĞƌŝŶŐE s EW WW ϯϮϮ ƌĞĐŽǀĞƌŝŶŐ ^ ͲEKEͲ EWͲ^: Ϭ WZW ŚĞ ŚĞ ďƌŽƵŐŚƚ ͲEKEͲ EW EW t,EWͲΎdΎͲEWͲ> ƚŚĞĨĂƐƚďĂůů sW Wd"
P19-1530,P15-1116,0,0.0151149,"rs to (Bies et al., 1995). 2.2 Previous Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics WdŐƌĂƉŚ"
P19-1530,J93-2004,0,0.0640874,"cts.nagoya-u.ac.jp Abstract The Penn Treebank (PTB) represents syntactic structures as graphs due to nonlocal dependencies. This paper proposes a method that approximates PTB graph-structured representations by trees. By our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single treebased parsing. An experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification. 1 Introduction In the Penn Treebank (PTB) (Marcus et al., 1993), syntactic structures are represented as graphs due to nonlocal dependencies, which capture syntactic discontinuities. This paper proposes a method that approximates PTB graph-structured representations by trees. By our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing. The information loss of our approximation method is slight, and we can easily recover original PTB graphs from the output of a parser trained using the approximated ones. An experimental result demonstrates that our approximation method with an off-th"
P19-1530,N18-1202,0,0.0747206,"Missing"
P19-1530,P06-1023,0,0.185944,"ore details about PTB nonlocal dependencies, we refer readers to (Bies et al., 1995). 2.2 Previous Work Most PTB-based parsers deal with the trees obtained by removing nonlocal dependencies and empty elements (we call such trees PTB trees). While such parsers are simple, efficient and accurate, they cannot handle nonlocal dependencies. To fill this gap, several methods have been proposed so far. They can be classified into the following two categories: the methods that introduce special operations handling nonlocal dependencies or empty elements into parsing algorithm (Dienes and Dubey, 2003; Schmid, 2006; Cai et al., 2011; Evang and Kallmeyer, 2011; Maier, 2015; Kato and Matsubara, 2016; Hayashi and Nagata, 2016; Kummerfeld and Klein, 2017), and the ones that recover PTB graphs from PTB trees generated by a parser (Johnson, 2002; Campbell, 2004; Levy and Manning, 2004). The former approach is required to design a parsing model that is suitable for the algorithm. In the latter post-processing approach, the pre-processing parser cannot reflect 5344 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5344–5349 c Florence, Italy, July 28 - August 2, 2019"
S15-1032,W08-2222,0,0.0719234,"man, 2000). CCG represents the 269 syntactic process as a derivation which is a tree structure. Our method constructs a CCG derivation by applying operations used in incremental phrase structure parsing. Each intermediate data structure constructed by the operations represents partial information of some derivation. Our method obtains a semantic representation from the intermediate structure. Since the obtained semantic representations conform to the CCG semantic construction, we can expect that incremental semantic interpretation is realized by applying a CCG-based semantic analysis such as (Bos, 2008). This paper is organized as follows: Section 2 briefly explains Combinatory Categorial Grammar. Section 3 gives an overview of previous work of CCG-based incremental parsing and discusses its problem. Section 4 proposes our CCG-based method of incrementally constructing semantic representations. Section 5 reviews related work and Section 6 concludes this paper. 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a grammar formalism which has a transparent correspondence between the syntax and semantics. Syntactic information is represented using basic cat"
S15-1032,P04-1015,0,0.389962,"0). 271 placed with semantic representations as soon as they are determined. In the rest of this section, we first describe incremental parsing which is the basis of our method. Next, we explain how to obtain a semantic representation from a partial parse tree constructed by incremental parsing. 4.1 Incremental Construction of CCG Derivation Our method considers a CCG derivation as a tree structure. We call this parse tree. Our method constructs a parse tree according to an incremental parsing formalism proposed in (Kato and Matsubara, 2009). This formalism extends the incremental parsing of (Collins and Roark, 2004) by introducing adjoining operation used in Tree Adjoining Grammar (Joshi, 1985). The incremental parsing assigns partial parse trees for any initial fragments of a sentence. Adjoining operation reduces local ambiguity caused by left-recursive structure, and improves the parsing accuracy (Kato and Matsubara, 2009). Furthermore, in the field of psycholinguistics, adjoining operation is introduced to a human sentence processing model (e.g., (Sturt and Lombardo, 2005; Mazzei et al., 2007; Demberg et al., 2013)). 4.1.1 A Formal Description of Incremental Parsing This section gives a formal descrip"
S15-1032,W07-1210,0,0.0228678,"similar to these functions. However, our method is more general than that of Milward. Milward’s method cannot produce CCG derivations, since it can deal with only function application. There are other approaches to incremental semantic construction, which use different formalism. Purver et al. (2011) have developed a dialogue system based on Dynamic Syntax (DS) (Kempson et al., 2001), which provides an incremental framework of constructing semantic representations. Peldszus and Schlangen (2012) have proposed incremental semantic construction based on Robust Minimal Recursion Semantics (RMRS) (Copestake, 2007). Sayeed and Demberg (2012) have proposed incremental semantic construction for PLTAG (Demberg et al., 2013). It is unclear how to construct a wide coverage grammar (with semantic annotation) in these frameworks.5 On the other hand, our method can use 5 DS grammar induction method (Eshghi et al., 2013) was only applied to a small artificial corpus (200 sentences, max sentence length is 6.). Peldszus and Schlangen (2012) manually assigned semantic annotations to a small set of context-free rules (30 rules). Sayeed and Demberg (2012) only provided small examples. CCG-based lexicon (e.g., (Bos, 2"
S15-1032,J13-4008,0,0.0679546,"ed in (Kato and Matsubara, 2009). This formalism extends the incremental parsing of (Collins and Roark, 2004) by introducing adjoining operation used in Tree Adjoining Grammar (Joshi, 1985). The incremental parsing assigns partial parse trees for any initial fragments of a sentence. Adjoining operation reduces local ambiguity caused by left-recursive structure, and improves the parsing accuracy (Kato and Matsubara, 2009). Furthermore, in the field of psycholinguistics, adjoining operation is introduced to a human sentence processing model (e.g., (Sturt and Lombardo, 2005; Mazzei et al., 2007; Demberg et al., 2013)). 4.1.1 A Formal Description of Incremental Parsing This section gives a formal description of incremental parsing of (Kato and Matsubara, 2009). The parsing grammar consists of three types of elements: allowable tuples, allowable chains and auxiliary trees. Each allowable tuple is a 3-tuple ⟨X, Y, Z⟩ which means that the grammar allows a node labelled with Z to follow a node labelled with Y under its parent labelled with X. Each allowable chain is a sequence of labels. This corresponds to a sequence of labels on a path from a node to its leftmost descendant leaf in a parse tree. Each auxilia"
S15-1032,W12-4623,0,0.106159,"nna′ ) ∧ meet′ manny′ anna′ . 270 Incremental parsing methods based on CCG have been proposed so far (Reitter et al., 2006; Hassan et al., 2008; Hefny et al., 2011). By using the property that CCG allows non-standard constituents, previous CCG-based incremental parsers assign a syntactic category to each initial fragment of an input sentence. The obtained derivations are most leftbranching ones which are called incremental derivations. Figure 3 shows two examples of incremental derivations. In Figure 3(a), the fragment “Anna met” is a non-phrase, but it has a syntactic category S/NP. However, Demberg (2012) has demonstrated that some kinds of sentences cannot have strictly leftbranching derivations. This means that previous approaches have the case where the parser cannot assign any syntactic categories to an initial fragment. This also means that such initial fragments do not have any semantic representations. A typical example is coordinate structure. In CCG, a coordinate structure is derived by combining conjuncts and a conjunction using coordination rule. This prevents the first conjunct from combining with its left constituent. As an example, let us consider the incremental derivation shown"
S15-1032,P96-1011,0,0.279732,"ed.2 We consider a derivation as a parse tree and construct it based on incremental phrase structure parsing. For each initial fragment of a sentence, incremental parsing can construct a partial parse tree which connects all words in the fragment. Our method obtains a semantic representation from the partial parse tree. In the constructed partial parse tree, some parts of the derivation are underspecified. Our method introduces variables to denote underspecified parts of the semantic representation. These variables are re2 Several variants of normal form have been presented. For example, see (Eisner, 1996) and (Hockenmaier and Bisk, 2010). 271 placed with semantic representations as soon as they are determined. In the rest of this section, we first describe incremental parsing which is the basis of our method. Next, we explain how to obtain a semantic representation from a partial parse tree constructed by incremental parsing. 4.1 Incremental Construction of CCG Derivation Our method considers a CCG derivation as a tree structure. We call this parse tree. Our method constructs a parse tree according to an incremental parsing formalism proposed in (Kato and Matsubara, 2009). This formalism exten"
S15-1032,W13-0110,0,0.0192991,"1) have developed a dialogue system based on Dynamic Syntax (DS) (Kempson et al., 2001), which provides an incremental framework of constructing semantic representations. Peldszus and Schlangen (2012) have proposed incremental semantic construction based on Robust Minimal Recursion Semantics (RMRS) (Copestake, 2007). Sayeed and Demberg (2012) have proposed incremental semantic construction for PLTAG (Demberg et al., 2013). It is unclear how to construct a wide coverage grammar (with semantic annotation) in these frameworks.5 On the other hand, our method can use 5 DS grammar induction method (Eshghi et al., 2013) was only applied to a small artificial corpus (200 sentences, max sentence length is 6.). Peldszus and Schlangen (2012) manually assigned semantic annotations to a small set of context-free rules (30 rules). Sayeed and Demberg (2012) only provided small examples. CCG-based lexicon (e.g., (Bos, 2009)) directly. Although our method requires a set of allowable chains and auxiliary trees in addition to such a lexicon, we can easily extract it from CCGbank (Hockenmaier and Steedman, 2007) by using the method proposed in (Kato and Matsubara, 2009). 6 Conclusion This paper proposed a CCG-based metho"
S15-1032,C10-1053,0,0.0211021,"derivation as a parse tree and construct it based on incremental phrase structure parsing. For each initial fragment of a sentence, incremental parsing can construct a partial parse tree which connects all words in the fragment. Our method obtains a semantic representation from the partial parse tree. In the constructed partial parse tree, some parts of the derivation are underspecified. Our method introduces variables to denote underspecified parts of the semantic representation. These variables are re2 Several variants of normal form have been presented. For example, see (Eisner, 1996) and (Hockenmaier and Bisk, 2010). 271 placed with semantic representations as soon as they are determined. In the rest of this section, we first describe incremental parsing which is the basis of our method. Next, we explain how to obtain a semantic representation from a partial parse tree constructed by incremental parsing. 4.1 Incremental Construction of CCG Derivation Our method considers a CCG derivation as a tree structure. We call this parse tree. Our method constructs a parse tree according to an incremental parsing formalism proposed in (Kato and Matsubara, 2009). This formalism extends the incremental parsing of (Co"
S15-1032,J07-3004,0,0.0438625,"mar (with semantic annotation) in these frameworks.5 On the other hand, our method can use 5 DS grammar induction method (Eshghi et al., 2013) was only applied to a small artificial corpus (200 sentences, max sentence length is 6.). Peldszus and Schlangen (2012) manually assigned semantic annotations to a small set of context-free rules (30 rules). Sayeed and Demberg (2012) only provided small examples. CCG-based lexicon (e.g., (Bos, 2009)) directly. Although our method requires a set of allowable chains and auxiliary trees in addition to such a lexicon, we can easily extract it from CCGbank (Hockenmaier and Steedman, 2007) by using the method proposed in (Kato and Matsubara, 2009). 6 Conclusion This paper proposed a CCG-based method of incrementally constructing semantic representations. Our approach is based on normal form derivations unlike previous ones. In this paper, we focused on the formal aspect of our method. We defined semantic transition function to obtain semantic representations for each initial fragment of an input sentence. Another important issue is how to interpret intermediate semantic representations for initial fragments. To our knowledge, there is little work to this direction. In future wo"
S15-1032,P09-2011,1,0.937302,"been presented. For example, see (Eisner, 1996) and (Hockenmaier and Bisk, 2010). 271 placed with semantic representations as soon as they are determined. In the rest of this section, we first describe incremental parsing which is the basis of our method. Next, we explain how to obtain a semantic representation from a partial parse tree constructed by incremental parsing. 4.1 Incremental Construction of CCG Derivation Our method considers a CCG derivation as a tree structure. We call this parse tree. Our method constructs a parse tree according to an incremental parsing formalism proposed in (Kato and Matsubara, 2009). This formalism extends the incremental parsing of (Collins and Roark, 2004) by introducing adjoining operation used in Tree Adjoining Grammar (Joshi, 1985). The incremental parsing assigns partial parse trees for any initial fragments of a sentence. Adjoining operation reduces local ambiguity caused by left-recursive structure, and improves the parsing accuracy (Kato and Matsubara, 2009). Furthermore, in the field of psycholinguistics, adjoining operation is introduced to a human sentence processing model (e.g., (Sturt and Lombardo, 2005; Mazzei et al., 2007; Demberg et al., 2013)). 4.1.1 A"
S15-1032,E95-1017,0,0.808828,"ext-free rules an4 The initial fragment “Anna met” can have the semantic representation λx.meet′ xanna′ as shown in Figure 3(a). However, the derivation which has this semantic representation is not a partial structure of incremental derivation shown in Figure 3(b). That is, the derivation is not consistent with that of “Anna met and might marry Manny.” 276 notated with semantic representations. The parsing process proceeds on a word-by-word basis, but its intermediate structure is a stack, that is, the parser does not assign a fully-connected semantic representation to each initial fragment. Milward (1995) has proposed an incremental semantic construction method based on Categorial Grammar. The method uses two types of transition functions: state-application and state-prediction. Our semantic transition function is similar to these functions. However, our method is more general than that of Milward. Milward’s method cannot produce CCG derivations, since it can deal with only function application. There are other approaches to incremental semantic construction, which use different formalism. Purver et al. (2011) have developed a dialogue system based on Dynamic Syntax (DS) (Kempson et al., 2001)"
S15-1032,W12-4705,0,0.168574,"ch and previous ones. Previous approaches use most left-branching derivation called incremental derivation, but they cannot process coordinate structures incrementally. Our method overcomes this problem. 1 Introduction By incremental interpretation, we mean that a sentence is analyzed from left to right, and a semantic representation is assigned to each initial fragment of the sentence. These properties enable NLP systems to analyze unfinished sentences. Moreover, incremental interpretation is useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007; Purver et al., 2011; Peldszus and Schlangen, 2012). Furthermore, in the field of psycholinguistics, incremental interpretation has been explored as a human sentence processing model. This paper proposes a method of constructing a semantic representation for each initial fragment of a sentence in an incremental fashion. The proposed method is based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). CCG represents the 269 syntactic process as a derivation which is a tree structure. Our method constructs a CCG derivation by applying operations used in incremental phrase structure parsing. Each intermediate data structure constructed by th"
S15-1032,E85-1019,0,0.615598,"tic representations for sentences including coordinate structures. In comparison with our incremental semantic construction, incremental derivation approaches have the case where no semantic representations are assigned to initial fragments. Table 2 shows semantic representations which are assigned using incremental derivations. There exist initial fragments which have no semantic representations as discussed in Section 3.4 5 Related Work Our incremental semantic construction is based on the λ-calculus. There have been several methods of incremental semantic construction using the λ-calculus. Pulman (1985) has developed an incremental parser which uses context-free rules an4 The initial fragment “Anna met” can have the semantic representation λx.meet′ xanna′ as shown in Figure 3(a). However, the derivation which has this semantic representation is not a partial structure of incremental derivation shown in Figure 3(b). That is, the derivation is not consistent with that of “Anna met and might marry Manny.” 276 notated with semantic representations. The parsing process proceeds on a word-by-word basis, but its intermediate structure is a stack, that is, the parser does not assign a fully-connecte"
S15-1032,W11-0144,0,0.153238,"ce between our approach and previous ones. Previous approaches use most left-branching derivation called incremental derivation, but they cannot process coordinate structures incrementally. Our method overcomes this problem. 1 Introduction By incremental interpretation, we mean that a sentence is analyzed from left to right, and a semantic representation is assigned to each initial fragment of the sentence. These properties enable NLP systems to analyze unfinished sentences. Moreover, incremental interpretation is useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007; Purver et al., 2011; Peldszus and Schlangen, 2012). Furthermore, in the field of psycholinguistics, incremental interpretation has been explored as a human sentence processing model. This paper proposes a method of constructing a semantic representation for each initial fragment of a sentence in an incremental fashion. The proposed method is based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). CCG represents the 269 syntactic process as a derivation which is a tree structure. Our method constructs a CCG derivation by applying operations used in incremental phrase structure parsing. Each intermediate d"
S15-1032,W06-1637,0,0.0321781,"on of syntactic categories has a corresponding semantic composition of their semantic representations. Figure 2 shows an example of CCG derivation, which is taken from (Steedman, 2000).1 Here, 1 For simplicity, we use a symbol for a semantic representation of a word. Note that it is allowed to use complex semantic representations. For example, by assigning λpx.3(px) (3 is possibility operator.) and λpq.p∧q to “might” and “and” respectively, we can obtain a modal logic formula 3(marry′ manny′ anna′ ) ∧ meet′ manny′ anna′ . 270 Incremental parsing methods based on CCG have been proposed so far (Reitter et al., 2006; Hassan et al., 2008; Hefny et al., 2011). By using the property that CCG allows non-standard constituents, previous CCG-based incremental parsers assign a syntactic category to each initial fragment of an input sentence. The obtained derivations are most leftbranching ones which are called incremental derivations. Figure 3 shows two examples of incremental derivations. In Figure 3(a), the fragment “Anna met” is a non-phrase, but it has a syntactic category S/NP. However, Demberg (2012) has demonstrated that some kinds of sentences cannot have strictly leftbranching derivations. This means th"
S15-1032,W12-4608,0,0.125304,"unctions. However, our method is more general than that of Milward. Milward’s method cannot produce CCG derivations, since it can deal with only function application. There are other approaches to incremental semantic construction, which use different formalism. Purver et al. (2011) have developed a dialogue system based on Dynamic Syntax (DS) (Kempson et al., 2001), which provides an incremental framework of constructing semantic representations. Peldszus and Schlangen (2012) have proposed incremental semantic construction based on Robust Minimal Recursion Semantics (RMRS) (Copestake, 2007). Sayeed and Demberg (2012) have proposed incremental semantic construction for PLTAG (Demberg et al., 2013). It is unclear how to construct a wide coverage grammar (with semantic annotation) in these frameworks.5 On the other hand, our method can use 5 DS grammar induction method (Eshghi et al., 2013) was only applied to a small artificial corpus (200 sentences, max sentence length is 6.). Peldszus and Schlangen (2012) manually assigned semantic annotations to a small set of context-free rules (30 rules). Sayeed and Demberg (2012) only provided small examples. CCG-based lexicon (e.g., (Bos, 2009)) directly. Although ou"
tohyama-etal-2008-construction-metadata,kozawa-etal-2008-automatic,1,\N,Missing
tohyama-matsubara-2006-collection,matsubara-etal-2002-bilingual,1,\N,Missing
tohyama-matsubara-2006-collection,P98-2141,0,\N,Missing
tohyama-matsubara-2006-collection,C98-2136,0,\N,Missing
W01-1825,P96-1025,0,0.280617,"n particular, such spoken language processing systems as real-time dialogue system, simultaneous machine interpreting system, etc. are being studied, and thus dependency parsing needs to be discussed from the viewpoint of incrementality. An approach based on context-free phrase structure grammars is known as one of the dependency parsing techniques. In this approach, dependency grammar is described as a set of phrase structure rules with head-dependent relations between syntactic categories, and the dependencies are extracted from parse trees according to Collins’ structures mapping procedure [1]. This means that we can not acquire the dependency structures before getting parse tree completely. If such the technique is used for incremental spoken language processing, it is strongly required not to damage the incrementality. This paper describes an efficient method of incremental dependency parsing based on CFG with the dependency relation. In order to connect a head word with a dependent word simultaneously with the inputs, the reachability relation between syntactic categories is utilized. Our method, which is different from the standard one of incremental parsing such as incremental"
W01-1825,J93-2004,0,0.0230758,"same as the ones extracted from the term (5). S(saw) NP(boy) VP*(saw) h DET(the) N*(boy) The boy * V*(saw) saw The boy saw Figure 5: An example of propagation of head-word average parsing time(sec) 1000 100 10 1 0.1 0.01 0.001 0.0001 2 4 6 usual method our method 10 12 sentence length(word) Figure 6: Parsing time per sentence 4 8 Experimental Result In order to evaluate the performance of our technique, an parsing experiment has been made. The parsing was implemented in GNU Common Lisp on a Linux PC with Pentium III 1GHz processor and 1GB main memory. We used the ATIS corpus of Penn Treebank [3]. The corpus contains 578 sentences with CFG parse tree tags, and 509 phrase structure rules in them are used for the test. The parsing time has been compared between the technique explained in Sect. 2 (i.e. usual method) and the one utilizing the reachability relation in Sect. 3 (i.e. our method). Fig. 6 shows the relation between the sentence length and the processing time1 . The average time of our method is 0.03 sec., while that of the usual method is 23.49 sec. This result shows the technique of utilizing the reachability to be effective for reducing processing time. 5 Concluding Remarks"
W03-2112,C00-1004,0,0.0227945,"(WCDB): Consists of words essential to the task in question and classes given to them according to meaning. Word classes are determined empirically based on dialogue within the dialogue corpus. Shop Information Database (SIDB): Consists of a collection of information on about 800 restaurants and shops in Nagoya, the same as that used in the WOZ system. Speech Recognition: Uses “Japanese Dictation Toolkit(Kawahara et al., 2000)”. The language model was created from the previously collected human-to-human dialogue corpus. 3 Using ChaSen morphological-analysis software for the Japanese language (Asahara and Matsumoto, 2000). Speech Input Speech Recognition Query Generation Search Speech Output Speech Synthesis Reply Generation Shop Information Database (SIDB) Dialogue Example Database (DEDB) Word class Database (WCDB) Figure 7: Configuration of example-based dialogue system Query Generation: Extracts from the DEDB the example closest to current input speech and conditions, modifies the query in that example according to current conditions, and outputs the result. Search execution: Accesses the SIDB using the generated query and obtains search results. Reply Generation: Extracts from the DEDB the example closest"
W03-2112,kawaguchi-etal-2002-multi,1,0.778098,"synthesis section of the system. Switching back and forth between the query and reply parts can be performed as needed using a switch button. The reply part also includes buttons for instantly generating words and short phrases of confirmation and encouragement (e.g., ”yes,” ”I see”) while the user is speaking to create as natural a dialogue as possible. 3.3 Collecting dialogue data by the WOZ system We targeted shop-information retrieval while driving a car as an information-retrieval application based on spoken dialogue, and collected dialogue data between the WOZ system and human subjects (Kawaguchi et al., 2002). This data was collected within an automobile driven by subjects each of whom acted as a user searching for information. A personal computer running the WOZ software was placed in the automobile with the ”wizard” sitting in the back seat. All spoken dialogue was recorded using another personal computer. Data collection was performed according to the following procedure for a duration of about five min• A prompting panel such as shown in Fig. 5 is presented to the subject. • The subject converses freely with WOZ based on the prompting panel shown. The wizard operates the WOZ system while liste"
W03-2112,H92-1003,0,0.0537142,"example database while collecting speech data. 3.1 WOZ system When carrying on a dialogue using the WOZ system, the user feels that he or she is talking to a completely mechanical system despite the fact that a human being is actually being used for some of the elements making up the dialogue system. Collecting dialogue data by WOZ should therefore result in Figure 2: Configuration of Wizard of OZ system data that is closer to dialogue that would occur between a human and a machine. Collecting spoken dialogue data using the WOZ system has actually been performed a number of times in the past (MADCOW, 1992; Bertenstam et al., 1995; Life et al., 1996; Eskenazi et al., 1999; San-Segundo et al., 2001; Lemmela and Boda, 2002; Yoma et al., 2002). The objective of those studies, however, was to collect, analyze, and evaluate dialogue data between people and artificial objects, and in many cases, only one of the artificial-object’s functions was taken over by a human, for example, the speech recognition function. Our study, however, goes further than the above. In particular, we create special software (called WOZ software) that allows a human being to perform the functions of interpreting user speech"
W03-2112,W01-1619,0,0.0278543,"dialogue using the WOZ system, the user feels that he or she is talking to a completely mechanical system despite the fact that a human being is actually being used for some of the elements making up the dialogue system. Collecting dialogue data by WOZ should therefore result in Figure 2: Configuration of Wizard of OZ system data that is closer to dialogue that would occur between a human and a machine. Collecting spoken dialogue data using the WOZ system has actually been performed a number of times in the past (MADCOW, 1992; Bertenstam et al., 1995; Life et al., 1996; Eskenazi et al., 1999; San-Segundo et al., 2001; Lemmela and Boda, 2002; Yoma et al., 2002). The objective of those studies, however, was to collect, analyze, and evaluate dialogue data between people and artificial objects, and in many cases, only one of the artificial-object’s functions was taken over by a human, for example, the speech recognition function. Our study, however, goes further than the above. In particular, we create special software (called WOZ software) that allows a human being to perform the functions of interpreting user speech, generating queries and executing searches, and generating replies. We then propose a framew"
W04-0302,2000.iwpt-1.9,0,0.0330375,"bol as η1 , split the parse tree at η1 and ηp , and combine the upper tree and the lower tree. η1 of intermediate tree is a foot node. • for each node η2 having only one left-sibling, if the parent ηp does not have the same nonterminal symbol as the left-sibling η1 of η2 , split the parse tree at η2 . • for the other node η in the parse tree, split the parse tree at η. For example, The initial trees α1 , α2 , α5 , α7 α8 and α10 and the auxiliary tree β2 are extracted from the parse tree #18 in Table 1. Our proposed tree extraction is similar to the TAG extractions proposed in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999). The main difference between these methods is the position of nodes at which parse trees are split. While the methods in the literatures (Chen and Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) utilize a head percolation rule to split the parse trees at complement nodes, our method splits the parse trees Table 1: Incremental parsing process of “I found a dime in the wood.” word I found a dime in the wood # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 partial parse tree s [[[I]prp ]np vp]s [[[I]prp ]np [[f ound]vb np]vp ]s [[[I]prp ]np [[f ound]vb np adjp]vp ]s [[[I"
W04-0302,J93-2004,0,0.0299756,"Missing"
W04-0302,C94-2120,0,0.0288431,"or each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed"
W04-0302,E95-1017,0,0.178915,"or improving the accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid partial parse trees"
W04-0302,P99-1054,0,0.0462477,"Missing"
W04-0302,J01-2004,0,0.720493,"e accuracy of incremental parsing. 1 Introduction Real-time spoken language processing systems, such as simultaneous machine interpretation systems, are required to quickly respond to users’ utterances. To fulfill the requirement, the system needs to understand spoken language at least incrementally (Allen et al., 2001; Inagaki and Matsubara, 1995; Milward and Cooper, 1994), that is, to analyze each input sentence from left to right and acquire the content. Several incremental parsing methods have been proposed to date (Costa et al., 2001; Haddock, 1987; Matsubara et al., 1997; Milward, 1995; Roark, 2001). These methods construct candidate partial parse trees for initial fragments of the input sentence on a word-by-word basis. However, these methods contain local ambiguity problems that partial parse trees representing valid syntactic relations can not be determined without using information from the rest of the input sentence. On the other hand, Marcus proposed a method of deterministically constructing valid partial parse trees by looking ahead several words (Marcus, 1980), while Kato et al. proposed an incremental parsing which delays the decision of valid partial parse trees (Kato et al.,"
W04-0302,P00-1058,0,\N,Missing
W10-4335,kamiya-etal-2010-construction,1,0.697107,"ed correlate with the cues provided by earlier researchers. Then we will develop a system which can detect back-channel feedback timings comprehensively. P (O) − P (E) 1 − P (E) where P (O) is the observed agreement between annotators, and P (E) is the hypothetical probability of chance agreement. A subject who has a certain level of knowledge annotated 673 dialogue turns. The kappa value was 0.731 (P (O) = 0.975, P (E) = 0.907), and thus we can see the substantial agreement between annotators. As the target for comparison, we used the kappa value in the existing back-channel feedback corpus (Kamiya et al., 2010). The corpus had been constructed by the way that the recorded driver’s voice was replayed and 4 subjects independently produced back-channel feedbacks for the same sound. This means that the policies for tagging the existing corpus differ from those of our corpus, and are “on-line tagging,” “tagging on the time axis” and “tagging without elaborating.” In the exisiting corpus, 297 dialogue turns were used as driver’s sound. Table 2 shows the kappa value between two among the 4 subjects. The kappa value of our corpus was higher than that between any subjects of the existing corpus, substantiati"
W13-5710,D07-1123,0,0.0289597,"re, this paper shows its application in the simultaneous linefeed insertion for real-time generation of more readable captions, and then describes the effectiveness of our proposed dependency structure. 1 Introduction In applications such as simultaneous machine interpretation, spoken dialogue and real-time captioning, it is necessary to execute processing simultaneously with speech input, and thus, the questions of what syntactic information is acquirable and when the information can be acquired become issues. However, previous researches on incremental dependency parsing (Kato et al., 2005; Johansson and Nugues, 2007; Nivre, 2008) have not discussed how a parser should output the information about dependency relation where the modified word has not been inputted. This paper proposes a dependency structure which a Japanese incremental dependency parser should produce in the middle of an input sentence. Our proposed Shigeki Matsubara Graduate Schoool of Information Science, Nagoya University Nagoya, Japan matubara@nagoya-u.jp dependency structure makes a parser clarify the fact that the modified bunsetsu1 of a bunsetsu will be inputted later. This enables a higher layer application to use the information th"
W13-5710,W10-4335,1,0.830245,"ucture outputted in the middle of an input sentence Next, we discuss the output contents of incremental dependency parsing. When applications use incremental dependency parsing, the information about whether or not the dependency structure of a sequence of busnetsus is closed, that is, the information about a syntactically sufficient unit is also useful. In fact, when identifying the timing of the start of interpreting in the simultaneous machine interpretation (Ryu et al., 2006; F¨ugen et al., 2007; Paulik and Waibel, 2010), the timing of inserting back-channel feedbacks (Fujie et al., 2004; Kamiya et al., 2010), and the appropriate position of a linefeed in captioning (Ohno et al., 2009), the information about a syntactically sufficient unit is used as an important clue. In our research, an incremental dependency parsing shall clearly output the fact that a bunsetsu depends on a bunsetsu which will be later inputted. If it becomes clear that the modified bunsetsu of a bunsetsu has not been inputted yet, the higher layer applications can identify the syntactically sufficient units in the inputted sequence of bunsetsus and use the information effectively. In what follows, we describe the dependency st"
W13-5710,P06-2088,1,0.80861,"-te-kite (had come back)” “non-inputted bunsetsu” “non-inputted bunsetsu” “terebi-de (on TV)” &gt; Figure 1: Example of a dependency structure outputted in the middle of an input sentence Next, we discuss the output contents of incremental dependency parsing. When applications use incremental dependency parsing, the information about whether or not the dependency structure of a sequence of busnetsus is closed, that is, the information about a syntactically sufficient unit is also useful. In fact, when identifying the timing of the start of interpreting in the simultaneous machine interpretation (Ryu et al., 2006; F¨ugen et al., 2007; Paulik and Waibel, 2010), the timing of inserting back-channel feedbacks (Fujie et al., 2004; Kamiya et al., 2010), and the appropriate position of a linefeed in captioning (Ohno et al., 2009), the information about a syntactically sufficient unit is used as an important clue. In our research, an incremental dependency parsing shall clearly output the fact that a bunsetsu depends on a bunsetsu which will be later inputted. If it becomes clear that the modified bunsetsu of a bunsetsu has not been inputted yet, the higher layer applications can identify the syntactically s"
W13-5710,2000.iwpt-1.43,0,0.581004,"utted in the middle of an input sentence. This example shows the dependency structure which is outputted right after the bunsetsu “terebi-de (on TV)” was inputted when a parser parses the sentence “watashi-mo nihon-ni kaet-te-kite sugu ie-no terebide warudokappu-wo mita-wake-desu (I also watched the world cup on TV at home soon after I had come back to Japan.)” 3 Incremental dependency parsing This section describes a method to produce the dependency structure proposed in Section 2 whenever receiving a bunsetsu. This method is implemented by improving the dependency parsing method proposed by Uchimoto et al. (2000). Generally, stochastic dependency parsing methods identify the dependency structure which maximizes the conditional probability P (D|S), which means the probability that the dependency structure for a sentence S is D. In usual methods, a relationship between two bunsetsus is tagged with a “1” or “0” to indicate whether or not there is a dependency relation between the bunsetsus, respectively. On the other 92 hand, in the Uchimoto et al.’s method, a relationship between two bunsetsus is tagged with a “between,” “depend,” or “beyond” to indicate the following three cases, respectively. The ante"
W13-5710,W02-2016,0,0.179422,"Missing"
W13-5710,matsubara-etal-2002-bilingual,1,0.722068,"When b5 is inputted： D5i = １ ２ ３ ４ 4.2 Evaluation index Our method is designed to output the dependency structure defined in Section 2 whenever a bunsetsu is inputted. Therefore, we introduced the following new evaluation index to evaluate the accuracy of incremental dependency parsing: acc. = ∑N ∑ni i i i j=1 DepN um(M atch(Dj , Gj )) ∑N ∑ni i i j=1 DepN um(Dj ) (0/0) (0/1) (1/2) Acc. =(8/10) (3/3) (4/4) ５ Figure 3: Calculation of accuracy 4.1 Outline of experiment As the experimental data, we used the transcribed data of Japanese discourse speech in the Simultaneous Interpretation Database (Matsubara et al., 2002). All the data had been manually annotated with information on morphological analysis, bunsetsu boundary detection, clause boundary detection and dependency analysis (Ohno et al., 2009). We performed a cross-validation experiment by using 16 discourses. That is, we repeated the experiment, in which we used one discourse from among 16 discourses as the test data and the others as the learning data, 16 times. However, since we used 2 discourses among 16 discourses as the preliminary analysis data, we evaluated the experimental results for the remaining 14 discourses (1,714 sentences, 20,707 buns"
W13-5710,J08-4003,0,0.037384,"lication in the simultaneous linefeed insertion for real-time generation of more readable captions, and then describes the effectiveness of our proposed dependency structure. 1 Introduction In applications such as simultaneous machine interpretation, spoken dialogue and real-time captioning, it is necessary to execute processing simultaneously with speech input, and thus, the questions of what syntactic information is acquirable and when the information can be acquired become issues. However, previous researches on incremental dependency parsing (Kato et al., 2005; Johansson and Nugues, 2007; Nivre, 2008) have not discussed how a parser should output the information about dependency relation where the modified word has not been inputted. This paper proposes a dependency structure which a Japanese incremental dependency parser should produce in the middle of an input sentence. Our proposed Shigeki Matsubara Graduate Schoool of Information Science, Nagoya University Nagoya, Japan matubara@nagoya-u.jp dependency structure makes a parser clarify the fact that the modified bunsetsu1 of a bunsetsu will be inputted later. This enables a higher layer application to use the information that the bunsets"
W13-5710,P09-1060,1,0.866213,"contents of incremental dependency parsing. When applications use incremental dependency parsing, the information about whether or not the dependency structure of a sequence of busnetsus is closed, that is, the information about a syntactically sufficient unit is also useful. In fact, when identifying the timing of the start of interpreting in the simultaneous machine interpretation (Ryu et al., 2006; F¨ugen et al., 2007; Paulik and Waibel, 2010), the timing of inserting back-channel feedbacks (Fujie et al., 2004; Kamiya et al., 2010), and the appropriate position of a linefeed in captioning (Ohno et al., 2009), the information about a syntactically sufficient unit is used as an important clue. In our research, an incremental dependency parsing shall clearly output the fact that a bunsetsu depends on a bunsetsu which will be later inputted. If it becomes clear that the modified bunsetsu of a bunsetsu has not been inputted yet, the higher layer applications can identify the syntactically sufficient units in the inputted sequence of bunsetsus and use the information effectively. In what follows, we describe the dependency structure which incremental dependency parsing in our research outputs in the mi"
W15-4709,W11-2832,0,0.0175801,"reference is incompletely understood, even Japanese natives often write Japanese sentences which are grammatically well-formed but not easy to read. For example, in Figure 1, the word order of S1 is less readable than that of S2 because the distance between the bunsetsu “Suzuki-san-ga (Mr. Suzuki)” and its modified bunsetsu “toi-te-shimatta (solved)” is large and thus the loads on working memory become large (Nihongo Kijutsu Bunpo Kenkyukai, 2009; Uchimoto et al., 2000) There have been some conventional researches for reordering words in a sentence so that the sentence becomes easier to read (Belz et al., 2011; Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999; Uchimoto et al., 2000; Yokobayashi et al., 2004). Most of the conventional researches used syntactic information by assuming that an input sentence for word reordering 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. A dependency relation in Japanese is a modification relation in which a modifier bunsetsu depends on a modified bunsetsu. Tha"
W15-4709,C14-1112,1,0.322147,"difference between S1 and S2 is just in their word orders in Japanese. Figure 1: Example of less-readable/readable word order has been already parsed. There is a problem that the errors of dependency parsing increase when an input sentence is less-readable, and the parsing errors cause negative effects on word reordering. To solve the problem, we previously proposed a method for concurrently performing word reordering and dependency parsing and confirmed the effectiveness of their proposed method using evaluation data created by randomly changing the word order in newspaper article sentences (Yoshida et al., 2014). However, since some of the just automatically created sentences are unlikely to be spontaneously written by a native, the evaluation is thought to be not enough. In addition, the probablistic model has room for improvement in targeting at sentences which a native is likely to spontaneously write. This paper proposes a new method on Japanese word reordering based on concurrent execution with dependency parsing by extending the probablistic model proposed by Yoshida et al. (2014), and describes an evaluation experiment using our Introduction Although Japanese has relatively free word order, Ja"
W15-4709,P07-1041,0,0.0211959,"letely understood, even Japanese natives often write Japanese sentences which are grammatically well-formed but not easy to read. For example, in Figure 1, the word order of S1 is less readable than that of S2 because the distance between the bunsetsu “Suzuki-san-ga (Mr. Suzuki)” and its modified bunsetsu “toi-te-shimatta (solved)” is large and thus the loads on working memory become large (Nihongo Kijutsu Bunpo Kenkyukai, 2009; Uchimoto et al., 2000) There have been some conventional researches for reordering words in a sentence so that the sentence becomes easier to read (Belz et al., 2011; Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999; Uchimoto et al., 2000; Yokobayashi et al., 2004). Most of the conventional researches used syntactic information by assuming that an input sentence for word reordering 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. A dependency relation in Japanese is a modification relation in which a modifier bunsetsu depends on a modified bunsetsu. That is, the modifier bunsetsu"
W15-4709,W06-1402,0,0.0601657,"Missing"
W15-4709,W01-0810,0,0.0424583,"Missing"
W15-4709,C04-1097,0,0.0479721,"hich are grammatically well-formed but not easy to read. For example, in Figure 1, the word order of S1 is less readable than that of S2 because the distance between the bunsetsu “Suzuki-san-ga (Mr. Suzuki)” and its modified bunsetsu “toi-te-shimatta (solved)” is large and thus the loads on working memory become large (Nihongo Kijutsu Bunpo Kenkyukai, 2009; Uchimoto et al., 2000) There have been some conventional researches for reordering words in a sentence so that the sentence becomes easier to read (Belz et al., 2011; Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999; Uchimoto et al., 2000; Yokobayashi et al., 2004). Most of the conventional researches used syntactic information by assuming that an input sentence for word reordering 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. A dependency relation in Japanese is a modification relation in which a modifier bunsetsu depends on a modified bunsetsu. That is, the modifier bunsetsu and the modified bunsetsu work as modifier and modifyee, respective"
W15-4709,P99-1018,0,0.0561983,"well-formed but not easy to read. For example, in Figure 1, the word order of S1 is less readable than that of S2 because the distance between the bunsetsu “Suzuki-san-ga (Mr. Suzuki)” and its modified bunsetsu “toi-te-shimatta (solved)” is large and thus the loads on working memory become large (Nihongo Kijutsu Bunpo Kenkyukai, 2009; Uchimoto et al., 2000) There have been some conventional researches for reordering words in a sentence so that the sentence becomes easier to read (Belz et al., 2011; Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999; Uchimoto et al., 2000; Yokobayashi et al., 2004). Most of the conventional researches used syntactic information by assuming that an input sentence for word reordering 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. A dependency relation in Japanese is a modification relation in which a modifier bunsetsu depends on a modified bunsetsu. That is, the modifier bunsetsu and the modified bunsetsu work as modifier and modifyee, respectively. Proceedings of the 15th Europ"
W15-4709,E99-1026,0,0.070588,"ength of sentences. and pair agreement (the percentage of the pairs of bunsetsus whose word order agrees with that in the original sentence), which are defined by Uchimoto et al. (2000). Here, when deciding α using the held-out data, we calculate the α to two places of decimals which maximizes the pair agreement. In the evaluation of dependency parsing, we obtained the dependency accuracy (the percentage of correctly analyzed dependencies out of all dependencies) and sentency accuracy (the percentage of the sentences in which all the dependencies are analyzed correctly), which were defined by Uchimoto et al. (1999). We compared our method to Yoshida’s method (Yoshida et al., 2014) and two conventional sequential methods. Both the sequential methods execute the dependency parsing primarily, and then, perform the word reordering by using the conventional word reordering method (Uchimoto et al., 1999). The difference between the two is the method of dependency parsing. The sequential methods 1 and 2 use the dependency parsing method proposed by Uchimoto et al. (2000) and the dependency parsing tool CaboCha5 , respectively. All of the methods used the same training features as those described in Yoshida et"
W15-4709,C00-2126,0,0.283543,"tion Although Japanese has relatively free word order, Japanese word order is not completely arbitrary and has some sort of preference. Since such preference is incompletely understood, even Japanese natives often write Japanese sentences which are grammatically well-formed but not easy to read. For example, in Figure 1, the word order of S1 is less readable than that of S2 because the distance between the bunsetsu “Suzuki-san-ga (Mr. Suzuki)” and its modified bunsetsu “toi-te-shimatta (solved)” is large and thus the loads on working memory become large (Nihongo Kijutsu Bunpo Kenkyukai, 2009; Uchimoto et al., 2000) There have been some conventional researches for reordering words in a sentence so that the sentence becomes easier to read (Belz et al., 2011; Filippova and Strube, 2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999; Uchimoto et al., 2000; Yokobayashi et al., 2004). Most of the conventional researches used syntactic information by assuming that an input sentence for word reordering 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more"
Y18-1030,W08-2222,0,0.0131347,"e hαβi and E2 is an expression of type α, then (E1 E2 ) is an expression of type β. Below, we impose the following constraints on all expressions. • All discourse referents are declared at most once. Here, we say that x is declared in E if E takes the form [. . . , x, . . . |C]. • For all function types hαβi, β 6= e. Let E be an expression in β-normal form.1 We say that E is complete if E does not include any variables. Otherwise, E is partial. If E is of type t, we call E a DRS. We can compositionally build up a DRS for a sentence in bottom-up fashion. In this paper, we adopt the approach of Bos (2008), which is based on Combinatory Categorial Grammar (Steedman, 2000). As an example, let us consider constructing a semantic expression for the noun phrase “a student” using the lexicon shown in Table 1. The categories of “a” and “student” are NP/N and N, respectively. We can combine these, since NP/N means that it receives an expression of category N from the right and returns one of category NP. The corresponding semantic expression can be obtained by function application as follows2 :    λP Q. ([x1 |]; P x1 ); Qx1 (λX.[ |stu(X)]) β λQ.([x1 |stu(x1 )]; Qx1 ) Here, β is the reflexive tran"
Y18-1030,W14-1410,0,0.021308,"et us consider the sentence prefix A red laptop . . . (18) Table 4 shows the partial DRSs, the sub-expressions, and their 3-interpretations. In 3-interpretations, the entities to which discourse referents can refer are incrementally specified. This example demonstrates that our semantics has a potential to be useful for incremental reference resolution (Schlangen et al., 2009). 5 Comparisons with Previous Work Unlike incremental semantic construction, there has been little work on how to interpret partial semantic representations incrementally, with two exceptions: (Schuler et al., 2009) and (Hough and Purver, 2014). These papers proposed an incremental referential interpretation where noun phrase prefixes are interpreted as entities to which the noun phrase derived from the prefix can refer. In their interpretation process, such entities are incrementally specified. Our 3-interpretaion provides a similar mechanism, as shown in Section 4.1.3. In addition, our approach provides a method of determining the truth values of sentence prefixes (Theorem 2), whereas that of the previous studies has no way to deal with truth values, and thus cannot offer sentential interpretations. Furthermore, their semantics ca"
Y18-1030,S15-1032,1,0.86178,"dates. One denotes possible updates and the other denotes necessary updates. With the proposed semantics, we can assign truth values to sentence prefixes. 1 Introduction Incremental semantic parsers construct semantic representations for each sentence prefix, and are useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007). While most research on incremental semantic parsers has focused on how to construct such representations incrementally (Pulman, 1985; Milward, 1995; Poesio and Rieser, 2010; Purver et al., 2011; Peldszus and Schlangen, 2012; Sayeed and Demberg, 2012; Kato and Matsubara, 2015), there has been little work on how to formally interpret them. An important issue with incremental interpretation, from a formal semantic viewpoint, is that sentence prefixes do not have propositional interpretations (Chater et al., 1995). In other words, standard formal semantics cannot be applied to incremental interpretation directly. This paper proposes a model-theoretic approach to incremental interpretation where each sentence prefix has semantic values. The proposed semantics is an extension of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993). In DRT, semantic representatio"
Y18-1030,E95-1017,0,0.521718,"es. In our semantics, a partial DRS of a sentence prefix is interpreted as two sets which stipulate the assignment updates. One denotes possible updates and the other denotes necessary updates. With the proposed semantics, we can assign truth values to sentence prefixes. 1 Introduction Incremental semantic parsers construct semantic representations for each sentence prefix, and are useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007). While most research on incremental semantic parsers has focused on how to construct such representations incrementally (Pulman, 1985; Milward, 1995; Poesio and Rieser, 2010; Purver et al., 2011; Peldszus and Schlangen, 2012; Sayeed and Demberg, 2012; Kato and Matsubara, 2015), there has been little work on how to formally interpret them. An important issue with incremental interpretation, from a formal semantic viewpoint, is that sentence prefixes do not have propositional interpretations (Chater et al., 1995). In other words, standard formal semantics cannot be applied to incremental interpretation directly. This paper proposes a model-theoretic approach to incremental interpretation where each sentence prefix has semantic values. The p"
Y18-1030,W12-4705,0,0.132481,"nterpreted as two sets which stipulate the assignment updates. One denotes possible updates and the other denotes necessary updates. With the proposed semantics, we can assign truth values to sentence prefixes. 1 Introduction Incremental semantic parsers construct semantic representations for each sentence prefix, and are useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007). While most research on incremental semantic parsers has focused on how to construct such representations incrementally (Pulman, 1985; Milward, 1995; Poesio and Rieser, 2010; Purver et al., 2011; Peldszus and Schlangen, 2012; Sayeed and Demberg, 2012; Kato and Matsubara, 2015), there has been little work on how to formally interpret them. An important issue with incremental interpretation, from a formal semantic viewpoint, is that sentence prefixes do not have propositional interpretations (Chater et al., 1995). In other words, standard formal semantics cannot be applied to incremental interpretation directly. This paper proposes a model-theoretic approach to incremental interpretation where each sentence prefix has semantic values. The proposed semantics is an extension of Discourse Representation Theory (DRT) ("
Y18-1030,E85-1019,0,0.800354,"signment updates. In our semantics, a partial DRS of a sentence prefix is interpreted as two sets which stipulate the assignment updates. One denotes possible updates and the other denotes necessary updates. With the proposed semantics, we can assign truth values to sentence prefixes. 1 Introduction Incremental semantic parsers construct semantic representations for each sentence prefix, and are useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007). While most research on incremental semantic parsers has focused on how to construct such representations incrementally (Pulman, 1985; Milward, 1995; Poesio and Rieser, 2010; Purver et al., 2011; Peldszus and Schlangen, 2012; Sayeed and Demberg, 2012; Kato and Matsubara, 2015), there has been little work on how to formally interpret them. An important issue with incremental interpretation, from a formal semantic viewpoint, is that sentence prefixes do not have propositional interpretations (Chater et al., 1995). In other words, standard formal semantics cannot be applied to incremental interpretation directly. This paper proposes a model-theoretic approach to incremental interpretation where each sentence prefix has semanti"
Y18-1030,W11-0144,0,0.0261833,"sentence prefix is interpreted as two sets which stipulate the assignment updates. One denotes possible updates and the other denotes necessary updates. With the proposed semantics, we can assign truth values to sentence prefixes. 1 Introduction Incremental semantic parsers construct semantic representations for each sentence prefix, and are useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007). While most research on incremental semantic parsers has focused on how to construct such representations incrementally (Pulman, 1985; Milward, 1995; Poesio and Rieser, 2010; Purver et al., 2011; Peldszus and Schlangen, 2012; Sayeed and Demberg, 2012; Kato and Matsubara, 2015), there has been little work on how to formally interpret them. An important issue with incremental interpretation, from a formal semantic viewpoint, is that sentence prefixes do not have propositional interpretations (Chater et al., 1995). In other words, standard formal semantics cannot be applied to incremental interpretation directly. This paper proposes a model-theoretic approach to incremental interpretation where each sentence prefix has semantic values. The proposed semantics is an extension of Discourse"
Y18-1030,W12-4608,0,0.0232653,"tipulate the assignment updates. One denotes possible updates and the other denotes necessary updates. With the proposed semantics, we can assign truth values to sentence prefixes. 1 Introduction Incremental semantic parsers construct semantic representations for each sentence prefix, and are useful for incremental dialogue systems (Allen et al., 2001; Aist et al., 2007). While most research on incremental semantic parsers has focused on how to construct such representations incrementally (Pulman, 1985; Milward, 1995; Poesio and Rieser, 2010; Purver et al., 2011; Peldszus and Schlangen, 2012; Sayeed and Demberg, 2012; Kato and Matsubara, 2015), there has been little work on how to formally interpret them. An important issue with incremental interpretation, from a formal semantic viewpoint, is that sentence prefixes do not have propositional interpretations (Chater et al., 1995). In other words, standard formal semantics cannot be applied to incremental interpretation directly. This paper proposes a model-theoretic approach to incremental interpretation where each sentence prefix has semantic values. The proposed semantics is an extension of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993). In"
Y18-1030,W09-3905,0,0.0382076,"e that [ |¬S5 ] is false in Mex . 4.1.3 Referential interpretation By applying the 3-interpretation function to DRS’s sub-expressions that include discourse referents, we can identify the entities to which the discourse referents refer. Here, let us consider the sentence prefix A red laptop . . . (18) Table 4 shows the partial DRSs, the sub-expressions, and their 3-interpretations. In 3-interpretations, the entities to which discourse referents can refer are incrementally specified. This example demonstrates that our semantics has a potential to be useful for incremental reference resolution (Schlangen et al., 2009). 5 Comparisons with Previous Work Unlike incremental semantic construction, there has been little work on how to interpret partial semantic representations incrementally, with two exceptions: (Schuler et al., 2009) and (Hough and Purver, 2014). These papers proposed an incremental referential interpretation where noun phrase prefixes are interpreted as entities to which the noun phrase derived from the prefix can refer. In their interpretation process, such entities are incrementally specified. Our 3-interpretaion provides a similar mechanism, as shown in Section 4.1.3. In addition, our appro"
Y18-1030,J09-3001,0,0.0314346,"se referents refer. Here, let us consider the sentence prefix A red laptop . . . (18) Table 4 shows the partial DRSs, the sub-expressions, and their 3-interpretations. In 3-interpretations, the entities to which discourse referents can refer are incrementally specified. This example demonstrates that our semantics has a potential to be useful for incremental reference resolution (Schlangen et al., 2009). 5 Comparisons with Previous Work Unlike incremental semantic construction, there has been little work on how to interpret partial semantic representations incrementally, with two exceptions: (Schuler et al., 2009) and (Hough and Purver, 2014). These papers proposed an incremental referential interpretation where noun phrase prefixes are interpreted as entities to which the noun phrase derived from the prefix can refer. In their interpretation process, such entities are incrementally specified. Our 3-interpretaion provides a similar mechanism, as shown in Section 4.1.3. In addition, our approach provides a method of determining the truth values of sentence prefixes (Theorem 2), whereas that of the previous studies has no way to deal with truth values, and thus cannot offer sentential interpretations. Fu"
