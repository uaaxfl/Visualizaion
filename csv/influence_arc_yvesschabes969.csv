1991.iwpt-1.4,P90-1036,0,0.023881,"ese dependencies can be captured by a stack mechanism since trees are embedded by ad j unction. However, if one would like to maintain the valid prefix property, which requires travers ing the output tree in a prefix fashion , the depen dencies are more complex than a context-free lan guage and the complexity of the parsing algorithm mcreases. For example, consider the trees a , /3 and , in Figure J. When , is adjoined into /3 at the B node, and the result is adjoined into a at the A node, the resulting tree yields the string ux &apos;zx&quot;vy &quot;ty &apos;w ( see Figure 1 ) . 6 1n a recent paper, Kai;en Harbusch (1990) claimed to have defined an O (n4 log (n)) worst time general TAG parser based on the C KY parser for CFGs. However, since the paper does not include a proof of correctness and com plexity, the relationship between the parser and the set of 3 � &quot; li !i i z A. z t � I A. t � � Left to Right Recognition u x&apos; z x&quot; v y&quot; t y&apos; w 1&apos;&quot;LYj rrr�r 1 Figure 1 : A bove, a sequence of adjunctions; be low left, bottom-up recognition of the derived tree; right, left to right recognition of the derived tree. If this TAG derived tree is recognized purely b ottom-up from leaf to root ( and therefore with out"
1991.iwpt-1.4,W89-0235,1,0.884325,"Missing"
1993.iwpt-1.20,P92-1017,1,0.908125,"Missing"
1993.iwpt-1.20,C92-2065,0,0.113627,"bilities apply only to single rules. It has been observed in practice that SCF G performs worse than non-hierarchical approaches. This has lead many researchers to believe that simple distributional information about adjacent words is the most important single source of in formation. In the absence of a formalism that adequately combines this information with other kinds of information, the emphasis in research has been on simple non-hierarchical statistical models of words, such as word N-gram models. Recently, it has been suggested that stochastic lexicalized tree-adjoining grammar (SLTAG) [8, 9] may be able to capture both distributional and hierarchical information. An SLTAG grammar consists of a set of trees each of which contains one or more lexical items. These elementary trees can be viewed as the elementary clauses (including their transformational variants) in which the lex ical items participate. The elementary trees are combined by substitution and adjunction. Each possible way of combining two trees is associated with a probability. Since it is based on tree-adjoining grammar (TAG), SLTAG can capture some kinds of hier25 7 25 8 S CHABES - WATERS archical information that c"
1993.iwpt-1.20,C92-2066,1,0.665112,"bilities apply only to single rules. It has been observed in practice that SCF G performs worse than non-hierarchical approaches. This has lead many researchers to believe that simple distributional information about adjacent words is the most important single source of in formation. In the absence of a formalism that adequately combines this information with other kinds of information, the emphasis in research has been on simple non-hierarchical statistical models of words, such as word N-gram models. Recently, it has been suggested that stochastic lexicalized tree-adjoining grammar (SLTAG) [8, 9] may be able to capture both distributional and hierarchical information. An SLTAG grammar consists of a set of trees each of which contains one or more lexical items. These elementary trees can be viewed as the elementary clauses (including their transformational variants) in which the lex ical items participate. The elementary trees are combined by substitution and adjunction. Each possible way of combining two trees is associated with a probability. Since it is based on tree-adjoining grammar (TAG), SLTAG can capture some kinds of hier25 7 25 8 S CHABES - WATERS archical information that c"
1993.iwpt-1.20,E93-1040,1,0.898229,"Missing"
A92-1030,W90-0203,0,0.0424511,"Missing"
A92-1030,C88-2121,1,0.931373,"Missing"
A92-1030,1991.iwpt-1.4,1,0.808348,"Missing"
A92-1030,W90-0102,1,0.901398,"Missing"
A92-1030,C90-3045,1,0.900667,"Missing"
A92-1030,C88-2147,0,0.0976216,"Missing"
A92-1030,C90-3001,1,\N,Missing
A92-1030,C92-3145,1,\N,Missing
C88-2121,C88-1002,1,0.829428,"Missing"
C88-2121,P84-1058,0,0.272853,"5-K0018, NSF grants MCS-82-191169 and DGR-84-10413. The second author is also partially supported by J.W. Zellldja grant. The authors would llke to thank Mitch Marcus for his helpful conunents about this work. Thanks are also due to Ellen Hays. **Visiting from University of Paris VII. 57,~ 1 'Lexicalization' of g r a m m a r formalisms Most of the current linguistics theories tend to give lexical accounts of several phenomena that used to be considered purely syntactic. The information put in the lexicon is therefore increased and complexified (e.g. lexical rules in LFG, used also by HPSG, or Gross 1984's lexicongrammar). But the question of what it means to 'lexicalize' a grammar is seldom addressed. The possible consequences of this question for parsing are not fully investigated. We present how to 'lexicalize' grammars such as CFGs in a radical way, while possibly keeping the rules in their full generality. If one assumes that the input sentence is finite and that it cannot be syntactically infinitely ambiguous, the 'lexicalization' simplifies the task of a parser. We say that a grammar formalism is 'lexicalized' if it consists of: • a finite set of structures to be associated with lexica"
C88-2121,P88-1032,1,0.908713,"s Earley-type a sentence to be parsed. It places no restrictions on the grammar. The algorithm is a bottom-up parser that uses top-down filtering. It is able to parse constraints on adjunction, substitution and feature structures for TAGs as defined by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). It is able to parse directly CFGs and TAGs. Thus it embeds the essential aspects of PATR-II as defined by Shieber (1984 and 1986). Its correctness was proven in Sehabes and Joshi (1988b). The concepts of dotted rule and states have been extended to TAG trees. The algorithm as described by Schabes and Joshi (1988a) manipulates states of the form: s = [a, dot, side, pos, l, fl, fi, star, t[, b[, snbst?] where a is a tree, dot is the address of the dot in the tree, side is the side of the symbol the dot is on (left or right), pos is the position of the dot (above or below), star is an address in a and l, f~, fr, star, t~, b~ are indices of positions in the input string. The variable subst? is a boolean that indicates whether the tree has been predicted for substitution. The algorithm uses nine processes: • The S c a n n e r allows lexical items to be recognized. • M o v e d o t d o w n and M o v e d o t"
C88-2121,P84-1075,0,0.00573075,"----+ S} Also, one can state a necessary condition on the correctness of a sentence similar to the category count theorem of van Benthem (1985 and 1986). 5 Extending the parser for TAGs Earley-type a sentence to be parsed. It places no restrictions on the grammar. The algorithm is a bottom-up parser that uses top-down filtering. It is able to parse constraints on adjunction, substitution and feature structures for TAGs as defined by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). It is able to parse directly CFGs and TAGs. Thus it embeds the essential aspects of PATR-II as defined by Shieber (1984 and 1986). Its correctness was proven in Sehabes and Joshi (1988b). The concepts of dotted rule and states have been extended to TAG trees. The algorithm as described by Schabes and Joshi (1988a) manipulates states of the form: s = [a, dot, side, pos, l, fl, fi, star, t[, b[, snbst?] where a is a tree, dot is the address of the dot in the tree, side is the side of the symbol the dot is on (left or right), pos is the position of the dot (above or below), star is an address in a and l, f~, fr, star, t~, b~ are indices of positions in the input string. The variable subst? is a boolean that indic"
C88-2121,P85-1018,0,0.0162873,"lied to execute the second step, since the number of structures produced js finite and since each of them corresponds to a token in the input string, the search space is finite and termination is guaranteed. In principle, one can proceed inside out, left to right or in any other way. Of course, standard parsing algorithm can be used too. In particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion. Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist. The use of restrictors as proposed by Shieber (1985) is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem. By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous. Since the trees are produced by the input string, the parser can use information that might be non-local to guide the search. For example, consider the language generated by the following CFG (example due to Mitch Marcus):"
C88-2121,C88-2147,0,\N,Missing
C88-2121,C69-4701,1,\N,Missing
C88-2121,P88-1033,0,\N,Missing
C90-3001,C88-1007,0,0.0563856,"Missing"
C90-3001,E89-1037,0,0.0632686,"Missing"
C90-3001,C88-2121,1,0.910376,"Missing"
C90-3001,C88-1002,1,0.877286,"Missing"
C90-3001,P90-1037,1,0.779933,"Missing"
C90-3001,C90-3045,1,0.538116,"Missing"
C90-3001,E89-1001,1,\N,Missing
C90-3001,C86-1071,0,\N,Missing
C90-3001,W90-0102,1,\N,Missing
C90-3045,J87-1005,1,0.295617,"Missing"
C90-3045,P88-1032,1,0.741633,"Missing"
C90-3045,C88-2121,1,0.346774,"Missing"
C90-3045,C88-2128,1,0.185062,"one-to-one mapping between the source and target derivation. When partial source derivations are recognized by the parser, the corresponding partial target derivation (for example semantic inteq)retation) can be incrementally compuled: as the input is read from left to right, interpretations of the partial target derivations corresponding to partial source derivations can be combined in one step to buikl a larger partial target derivation. In previous work, one of us noted that generation according to an augmented context-free grammar can be made more efficient by requiring the grammar to be (Shieber, 1988); the derived semantics for an expression must include, in an appropriate sense, the semantic material of all its subconstituents. It is interesting to note that synchronous ""FAGs are inherently semantically monotonic. Furthermore, it is reasonable to require that the semantic component of a synchronous TAG t~ (in the sense of Schabes et manticallymonotonic selexicalized 5 257 When the synchronous TAG is order-sensitive, however, there may be a many-to-many correspondence between source derivations and target derivations. This is the case, for instance, in a grammar in which alternative quanti"
C90-3045,P85-1011,0,0.0107604,"Missing"
C90-3045,H86-1020,0,\N,Missing
C90-3045,C90-3001,1,\N,Missing
C90-3045,E89-1001,1,\N,Missing
C90-3045,P89-1027,0,\N,Missing
C92-1034,P92-1010,1,0.778242,"fication. We allow for overwriting inherited attributes by assuming that the local specification has a higher precedence. Figure 2 shows a fragment of the hierarchy for verbs. The lexicon associates lexicaJ items with a set of classes. Entries specify relationships and properties of sets of nodes in trees which will be associated with the lexical items. The framework for describing the tree that will be associated with the lexicai item is very similar to unification based tree-adjoining grammar (Vijay-Shanker, 1992) in which the trees are described with partial descriptions of their topology (Rogers and Vijay-Shanker, 1992) using statements of domination Paoc. OFCOLING-92, NANTES,AUO. 23-28, 1992 different classes e.g. in NP-IOBJ or used in tile syntactic rides such as for Wll-movement. • linear precedence (LP) statements which define precedence an&apos;long nodes within the framework of I D / L P TAG proposed by Jo~hi (1987). give d~at~ ~t Figure 2: 1,¥agment of the Lexicon • anchm; anchor --- xspecifies that the node x is tile aalchor node of the tree being described. and linear precedence. We do not discuss tile description language in which these trees are stated. Instead, we will pictorially represent these part"
C92-1034,C88-2121,1,0.902782,"ssociated elementary trees of extended domain of locality. Furthermore, the lexical and syntactic rules can be used to derive new elementary trees from the default structures specified in the hierarchical lexicon. In the envisaged scheme, tbe use of a hierarchical lexicon and of lexical and syntactic rules for lexicalized tree-adjoining grammars will capture important linguistic generalizations and also allows for a space efficient representation of the grammar. This will allow for easy maintenance and facilitate updates to the grammar. 1 Motivations Lexicalized tree-adjoining grammar (LTAG) (Schabes et al., 1988; Schabes, 1990) is a tree-rewriting formalism used for specifying the syntax of natural languages. It combines elementary lexical trees with two operations, adjoining and substitution. In a LTAG, lexical itenm are associated with complex syntactic structures (in the form of trees) that define the various phrase structures they can participate in. LTAG allows for unique syntactic and semantic properties: • The domain of locality in LTAG is larger than for other formalisms, and • Most syntactic dependencies (such as filler-gap, verb-subject, verb-objects) and some semantic *The first author is"
C92-1034,C88-1002,0,0.0359946,"Missing"
C92-1034,W90-0203,0,0.0252095,"987) and Pollard and Sag (1987) use a hierarchical lexicon aud rules for implementing Headdriven Phrase Structure Grammars. Shieber (1986) proposed the use of default inheritance combined with templates and of transformation rules in the PATRII system for organizing a unification based grammar. Lexical redundancy rules have been used in LFG (Bresnan and Kaplan, 1983) to capture relations among lexicai items. Gazdar et al. (1985) proposed the use of meta-rules for expressing transformational relationships. There has been suggestions for compacting the size of a tree-adjoining grammar lexicons (Becker, 1990; ttabert, 1991). However, they only partially solve the problem since they fail to combine in a uniform way a compact representation of the lexicon and, at the same time, of their associated elementary trees. In this paper, we present a scheme to efficiently represent a LTAG and illustrate this scheme by examples. We examine the information that needs to be associated with the classes in our hierarchical organization in order that we can represent the elementary trees of a LTAG. Our main concern in this paper is the proposal for organizing a LTAG. In order to be concrete, we consider the repr"
C92-1034,C90-3045,1,0.844589,"Missing"
C92-1034,J92-4004,1,\N,Missing
C92-1034,W90-0102,1,\N,Missing
C92-2066,H90-1021,0,0.0544897,"Missing"
C92-2066,C88-2121,1,\N,Missing
C92-3145,A92-1030,1,\N,Missing
C92-3145,H89-2024,0,\N,Missing
C92-3145,C88-1069,0,\N,Missing
E89-1001,E87-1034,0,0.059133,"Missing"
E89-1001,P88-1032,1,0.882475,"Missing"
E89-1001,C88-2121,1,0.909651,"Missing"
E89-1001,P87-1008,0,0.0241186,"of 'free' structures) but also with elements of the idiom that are directly attached. Unification equations, such as those constraining agreement, are the same for trees selected by idioms and trees selected by 'free' structures. Thus only grande that is feminine singular, and not grand for example, can adjoin to majorit~ that is feminine singular. In il falloir NP, the frozen subject il is marked 3rd person singular, and only an auxiliary like va (that is 3rd person singular) and not vont (3rd person plural) will be allowed Parsing flexible idioms has received only partial solutions so far (Stock 1987, Laporte 1988). Since TAGs factor recursion from dependencies, discontinuities are captured straightforwardly without special devices (as opposed to Johnson 1985 or Bunt et al. 1987). We distinguish two kinds of discontinuities: discontinuities that come from internal structures and discontinuities that come from the insertion of modifiers. 5.2 No VP Figure 11: Jean a cassg sa pipe Figure 10: Tree for NPo takes NP1 into account Internal NPI I A V DtA N1 I I I I a casse sa pipe I I 5.1 V Jean Aux into N2/VA 5 NP o VP -7- Therefore parsing flexible idioms is reduced to the general parsing of TA"
E89-1001,P85-1011,0,0.146169,"Missing"
E89-1001,C88-2147,0,0.0244285,"Missing"
E89-1001,C88-1002,1,0.899204,"Missing"
E89-1001,P85-1015,0,\N,Missing
E93-1040,P92-1024,0,0.113666,"Missing"
E93-1040,H90-1055,0,0.0113302,"he fact that the extended inside-outside ~dgorithm (as described in Pereira and Schabes, 1992) behaves in linear time when the text is fully bracketed. Then, the syntactic labels are ignored. This allows the reestimation algorithm to distribute its own set of labels based on their actual distribution. We later suggest a method for recovering these labels. The following is ,an ex~unple of a partially parsed sentence found in the Penn Treeb~mk: S NP I Training Corpus The experiments use texts from the Wall Street Journ~d Corpus ,and its partially bracketed version provided by the Penn Treebank (Brill et al., 1990). Out of 38 600 bracketed sentences (914 000 words), we extracted 34500 sentences (817 000 words) as possible source of training material ,and 4100 sentences (97 000 words) as source for testing. We experimented with several subsets (350, 1095, 8000 ,and 34500 sentences) of the available training materi~d. I No price IN has VBN I been NP f°r D~T JIJ VP I VBN I NI~IS sel t e new shares The above parse corresponds to the fully bracketed unlabeled parse VBZ DT After having described the training material used, we report experiments using several subsets of the available training material ,and eva"
E93-1040,W89-0209,0,0.478731,"Missing"
E93-1040,P92-1017,1,0.71194,"Missing"
H89-1036,P84-1058,0,0.0311519,"ed above. 1 L e x i c a l i z e d Tree A d j o i n i n g G r a m m a r Most current linguistic theories give lexical accounts of several phenomena that used to be considered purely syntactic. The information put in the lexicon is thereby increased both in amount and complexity: for example, lexical rules in LFG (Kaplan and Bresnan, 1983), GPSG (Gazdar, Klein, Pullum and Sag, 1985), HPSG (Pollard and Sag, 1987), Comhinatory Categoriai Grammars (Steedman 1988), Karttunen&apos;s version of Categorial G r a m m a r (Karttunen 1986, 1988), some versions of GB theory (Chomsky 1981), and LexiconGrammars (Gross 1984). We say that a grammar is &apos;lexicalized&apos; if it consists of: 1 • a finite set of structures associated with each lexical item, which is intended to be the head of these structures; • an operation or operations for composing the structures. The finite set of structures define the domain of locality over which constraints are specified, and these are local with respect to their lexical heads. Context free grammars cannot be in general be lexicalized. However TAGs are &apos;naturally&apos; lexicalized because they use an extended domain of locality (Schabes, Abeilld and Joshi, 1988). TAGs were first introdu"
H89-1036,C73-1005,0,0.221334,"Missing"
H89-1036,P88-1032,1,0.671987,"finite and termination is guaranteed. In principle, one can proceed inside out, left to right or in any other way. Of course, standard parsing algorithms can be used, too. In particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion. By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous. An Earley-type parser for TAGs has been investigated by Schabes and Joshi (1988). The algorithm has a linear best time behavior and an O(n 9) worst time behavior. This is the first practical parser for TAGs because as is well known for CFGs, the average behavior of Earley-type parsers is superior to its worst time behavior. We extended it to deal with substitution and feature structures for TAGs. By doing this, we have built a system that parses unification formalisms that have a CFG skeleton and also those that have a TAG skeleton. The Earley-type parser for TAGs can be extended to take advantage of lexicalized TAGs. Once the first pass has been performed, a subset of th"
H89-1036,C88-2121,1,0.86168,"Missing"
H89-1036,C69-4701,1,\N,Missing
H89-2053,C88-1002,0,0.0224637,"Missing"
H89-2053,P84-1058,0,0.0607668,"Missing"
H89-2053,C73-1005,0,0.139804,"Missing"
H89-2053,J86-2006,0,0.0323485,"Missing"
H89-2053,P88-1032,1,0.9276,"stics. First, the amount of filtering on the entire grammar is evaluated: once the first pass is performed, the parser uses only a subset of the grammar. Second, we evaluate the use of non-local information: the structures selected during the first pass encode the morphological value (and therefore the position in the string) of their anchor; this enables the parser to use non-local information to guide its search. We take Lexicalized Tree Adjoining Grammars as an instance of lexicallzed grammar. We illustrate the organization of the grammar. Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization. Empirical d a t a show that the filtering of the grammar and the non-local information provided by the two-pass strategy improve the performance of the parser. 1 LEXICALIZED GRAMMARS M o s t c u r r e n t linguistic theories give lexical a c c o u n t s o f several p h e n o m e n a t h a t used t o be c o n s i d e r e d p u r e l y s y n t a c t i c . T h e i n f o r m a t i o n p u t in t h e lexicon is t h e r e b y i n c r e a s e d in b o t h a m o u n t a n d c o m p l e x i t y : see, for e x a m p l e , lexical rules in L F G ( K a p l a n a n d"
H89-2053,W89-0235,1,0.354792,"Missing"
H89-2053,C88-2121,1,0.835073,"Missing"
H89-2053,E89-1001,1,\N,Missing
H89-2053,C69-4701,1,\N,Missing
H90-1010,P90-1035,1,0.784394,"need to use a more powerful configuration then a finite state automaton driving a push down stack. The design of deterministic left to right bottom up parsers for TAGs in which a finite state control drives the moves of a Bottom-up Embedded Push Down Stack has been investigated. The class of corresponding non-deterministic automata recognizes exactly the set of TALs. Due to the lack of space, we focus our attention on the bottom-up embedded pushdown automaton. The moves of the parser are sequences of moves of the automaton. The complete construction of LR-style parser for TAGs can be found in Schabes and Vijay-Shanker (1990). A u t o m a t a M o d e l s of Tags By virtue of their extended domain of locality, Tree Adjoining Grammars allow regular correspondences between larger structures to be stated without a mediating interlingual representation. The mapping of derivation trees from source to target languages, using the formalism of synchronous TAGs, makes possible to state such direct correspondences. By doing so, we are able to match linguistic units with quite different internal structures. Furthermore, the fact that the grammars are lexicalized enables capturing some idiosyncrasies of each language. Before w"
H90-1010,C88-2121,1,0.884344,"Missing"
H90-1010,C90-3045,1,0.866342,"Missing"
H90-1010,W90-0102,1,0.873911,"Missing"
H90-1010,J87-1004,0,0.166907,"t Free Grammars (Knuth, 1965) consist of a finite state control (constructed given a CFG) that drives deterministically with k lookahead symbols a push down stack, while scanning the input from left to right. It has been shown that they recognize exactly the set of languages recognized by deterministic push down automata. LR(k) parsers for CFGs have been proven useful for compilers as well as recently for natural language processing. For natural language processing, although LR(k) parsers are not powerful enough, conflicts between multiple choices are solved by pseudo-parallelism (Lang, 1974, Tomita, 1987). This gives rise to a class of powerful yet efficient parsers for natural languages. It is in this context that deterministic (LR(k)-style) parsing of TAGs is studied (this work has been done in collaboration with Vijay-Shanker). The set of Tree Adjoining Languages is a strict superset of the set of Context Free Languages (CFLs). For example, the cross serial dependency construction in Dutch can be generated by a TAG. Walters (1970), R6v6sz (1971), Turnbull and Lee (1979) investigated deterministic parsing of the class of context-sensitive languages. However they used Turing machines which re"
H90-1010,C90-3001,1,\N,Missing
H90-1010,E89-1001,1,\N,Missing
H91-1035,C90-3045,1,\N,Missing
H91-1035,C88-2121,1,\N,Missing
H91-1035,H89-1038,0,\N,Missing
H91-1035,W90-0102,1,\N,Missing
H92-1024,W89-0209,0,0.0250144,"ble with a bracketing B of w if no span of a symbol occurrence in the derivation overlaps a span in B. 3. THE INSIDE-OUTSIDE ALGORITHM The inside-outside algorithm [2] is a reestimation procedure for the rule probabilities of a Chomsky normal-form (CNF) SCFG. It takes as inputs an initial CNF SCFG and a training corpus of sentences and it iteratively reestimates rule probabilities to maximize the probability that the grammar used as a stochastic generator would produce the corpus. A reestimation algorithm can be used both to refine the parameter estimates for a CNF SCFG derived by other means [7] or to infer a grammar from scratch. In the latter case, the initial grammar for the inside-outside algorithm consists of all possible CNF rules over given sets 123 N of nonterminals and E of terminals, with suitable assigned nonzero probabilities. In what follows, we will take N, E as fixed, n = [NI, t = I~1, and assume enumerations N = { A 1 , . . . , A n } and E = { b l , . . . , b t } , with A1 the grammar start symbol. A CNF SCFG over N, E can then be specified by the n s + nt probabilities Bp,q,r of each possible binary rule Ap ~ Aq Ar and Up,m of each possible unary rule A n ~ bin. Sinc"
H92-1024,H90-1021,0,0.029291,"raw text decreases rapidly (1.57 initially, 0.87 after 22 iterations). Similarly, the cross entropy estimate of the bracketed text with respect to the grammar improves rapidly (2.85 initially, 0.87 after 22 iterations). The inferred grammar models correctly the palindrome language. Its high probability rules (p > 0.1, p / p &apos; > 30 125 4.2. Experiments on the ATIS Corpus We also conducted an experiment on inferring grammars for the language consisting of part-of-speech sequences of spoken-language transcriptions in the Texas Instruments subset of the Air Travel Information System (ATIS) corpus [8]. We take advantage of the availability of the hand-parsed version of the ATIS corpus provided by the Penn Treebank project [9] and use the corresponding bracketed corpus over parts of speech as training data. Out of the 770 bracketed sentences (7812 words) in the corpus, we used 700 as training data and 70 (901 words) as test set. The following is an example training string ( ( ( VB ( DT ~NS ( Im ( ( ~ CD) ) ) ) ) ) . ) ) ( n corresponding to the parsed sentence As a first example, GB gives the following bracketings: ( ( ( I (would (like (to (take (((Delta ((([List (the fares (for ((flight) ("
H92-1024,H90-1055,0,0.0335164,"ext with respect to the grammar improves rapidly (2.85 initially, 0.87 after 22 iterations). The inferred grammar models correctly the palindrome language. Its high probability rules (p > 0.1, p / p &apos; > 30 125 4.2. Experiments on the ATIS Corpus We also conducted an experiment on inferring grammars for the language consisting of part-of-speech sequences of spoken-language transcriptions in the Texas Instruments subset of the Air Travel Information System (ATIS) corpus [8]. We take advantage of the availability of the hand-parsed version of the ATIS corpus provided by the Penn Treebank project [9] and use the corresponding bracketed corpus over parts of speech as training data. Out of the 770 bracketed sentences (7812 words) in the corpus, we used 700 as training data and 70 (901 words) as test set. The following is an example training string ( ( ( VB ( DT ~NS ( Im ( ( ~ CD) ) ) ) ) ) . ) ) ( n corresponding to the parsed sentence As a first example, GB gives the following bracketings: ( ( ( I (would (like (to (take (((Delta ((([List (the fares (for ((flight) (number 891)))))) .) (flight number)) 83) (to The initial grammar consists of all possible CNF rules (4095 rules) over 15 nonter"
H92-1024,H91-1060,0,0.0173671,"Missing"
H92-1024,C92-2066,1,0.729367,"Missing"
H92-1027,H91-1046,0,0.0905429,"tree associated with a lexical item and rooted by No. Then, for all node ~1 in an elementary tree, the following rules are generated. • If r/if/2 are the 2 children of a node N such that N2 is on where Xk E VN, a G VT and Po E Vi, Pl,P2 G V~; 3The constructed LIG generates the same language as the given tree-adjoining grammar. 4The algorithms explained in this paper can be generalized to lexi2 LIGs have been shown to b e weakly equivalent to Tree-Adjoining G r a m m a r s [16]. calized tree-adjoining grammars t h a t need not be in Chomsky Normal Form using techniques similar the one found in [17]. 141 is 01, say 6, at the node r / o f a lexicalized elementary tree, say o~.6 the spine (i.e. subsumes the foot node), include: bE-0] P -I (2) Since (2) encodes an immediate domination link defined by t]he tree-adjoining grammar, its associated probability is one. The SLIG constructed as above is well defined if the following equalities hold for all nodes r/: P(t[-.y] ~ b[..T/]) + ~ • Similarly, if ~7102 are the 2 children of a node r/such that P(t[..y] ---~ t[..0Th] ) = 1 (9) r/1 is on the spine (i.e. subsumes the foot node), include: bill (3) Since (3) encodes an immediate domination link"
H92-1027,P85-1011,0,0.0355311,".g. John ~hlnks ~hat H a r r y is sick). 142 P(t[$y] ~ t[$yl]) = 1 (10) P(t[$] ~ t[$y0]) = 1 (11) rh Yo A grammar satisfying (12) is called consistent. 7 E P(t[$]=~w) = 1 (12) wE~* Beside the distributional phenomena that we mentioned earlier, SLTAG also captures the effect of adjoining constraints (selective, obligatory or null adjoining) which are required for tree-adjoining grammar, s 3. PROBABILITY OF A SENTENCE We now define an bottom-up algorithm for SLTAG which computes the probability of an input string. T h e algorithm is an extension of the CKY-type parser for tree-adjoining grammar [18]. The extended algorithm parses all spans of the input string and also computes their probability in a bottom-up fashion. Since the string on the frontier of an auxiliary is broken up into two substrings by the foot node, for the purpose of computing the probability of the sentence, we will consider the probability that a node derives two substrings of the input string. This entity will be called the inside probability. Its exact definition is given below. We will refer to the subsequence of the input string w = a l . . . aN from position i to j , w~. It is defined as follows: &quot;del f a i + l •"
H92-1027,P92-1017,1,0.79372,"Missing"
H92-1027,H86-1020,0,\N,Missing
H92-1027,H92-1024,1,\N,Missing
H92-1027,C88-2121,1,\N,Missing
J94-1004,C69-0101,0,0.73786,"Missing"
J94-1004,P83-1021,0,0.0102503,"use the following notations in this and later sections. The symbol P will serve as a variable over the two LIG grammar nonterminals t and b. The substring of the string wl ... Wn being parsed between indices i and j will be notated as wi+t &quot; . wj, which we take to be the empty string when i is greater than or equal to j. We will use p, A, and {9 for sequences containing terminals and LIG nonterminals with their stack specifications. For instance, F might be t[rll]t[..rl2]t[rl3]. The parsing algorithm can be seen as a tabular parsing method based on deduction of items, as in Earley deduction (Pereira and Warren 1983). We will so describe it, by presenting inference rules over items of the form (e[r/] --* r • A , i , j , k , l ) . Such items play the role of the items of Earley's algorithm. Unlike the items of Earley's algorithm, however, an item of this form does not embed a grammar rule proper; that is, P[7/] --+ pA is not necessarily a rule of the grammar. Rather, it is what we will call a reduced rule; for reasons described below, the nonterminals in F and A as well as the nonterminal P[~/] record only the top element of each stack of indices. We will use the notation P[~] --+ pA for the unreduced form"
J94-1004,C92-2065,0,0.133512,"Kosaraju, and Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber In summary, the interpretation of adjoining constraints particular notion of derivation that is used. Therefore, it test for an appropriate definition of derivation. As such, it independent notion of derivation for modifier auxiliary trees notion for predicative trees. Tree-Adjoining Derivation in TAG is sensitive to the can be used as a litmus argues for a nonstandard and a standard dependent 3.2 Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the"
J94-1004,1991.iwpt-1.4,1,0.811146,"Missing"
J94-1004,C92-2066,1,0.68126,"Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber In summary, the interpretation of adjoining constraints particular notion of derivation that is used. Therefore, it test for an appropriate definition of derivation. As such, it independent notion of derivation for modifier auxiliary trees notion for predicative trees. Tree-Adjoining Derivation in TAG is sensitive to the can be used as a litmus argues for a nonstandard and a standard dependent 3.2 Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the standard defin"
J94-1004,P92-1022,1,0.188092,"ees incrementally as parsing proceeds. Once this has been demonstrated, it should be obvious that these derivation trees could be transferred to target derivation trees during the parsing process and immediately generated from. Thus, incremental interpretation is demonstrated to be possible in the synchronous TAG framework. In fact, the technique presented in this section has allowed for the first implementation of synchronous TAG processing, by Onnig Dombalagian. This implementation was directly based on the inference-based TAG parser mentioned in Section 6.5 and presented in full elsewhere (Schabes and Shieber 1992). We associate with each item a set of operations that have been implicitly carried out by the parser in recognizing the substring covered by the item. An operation can be characterized by a derivation tree and a tree address at which the derivation tree is 113 Computational Linguistics Volume 20, N u m b e r 1 • Scanner: (b[r/] --+ F • aA, i,j, k, I) (b[z/] ~ r a . A , i , j , k , l + l ) a ~ Wl+l (P[r/] --, P • P' [r/lA, i,j, k, I) (P'[r/']--~ • O,/, - , - , l ) P' [~/'] ~ 0 • Predictor: Type 1 and 2 Completor: (b[rh] -+ r • t[r/]A, m,j', k', i) (t[rl] -+ 0 • , i,j, k, l) (b[rh] -+ Pt[r/] •"
J94-1004,J92-2002,0,0.0380292,"ernatively, the axioms can be stated as if there were extra rules S --* t[r/s] for each ~/s a start-nonterminal-labeled root node of an initial tree. In this case, the axioms are items of the form (S --~ • t[~s], 0, - , - , 0) and the string is accepted u p o n proving IS --+ t[~/s] • , 0 , - , - , n). In this case, an extra prediction and completion rule is n e e d e d just for these rules, since the normal rules do not allow S on the left-hand side. This point is taken u p further in Section 6.4. Generation of items can be cached in the standard w a y for inference-based parsing algorithms (Shieber 1992); this leads to a tabular or chart-based parsing algorithm. 6.2 The Algorithm Invariant The algorithm maintains an invariant that holds of all items a d d e d to the chart. We will describe the invariant using some additional notational conventions. Recall that P[~] -+ 1~ is the LIG production in the g r a m m a r w h o s e reduced form is P[~] --+ P. The notation F[7] where 7 is a sequence of stack symbols (i.e., nodes), specifies the sequence F with 7 replacing the occurrence of .. in the stack specifications. For example, if P is the sequence t[rll]t[..rl2]t[~13 ], then F[3,] = t[r]l]t['yrl"
J94-1004,C90-3045,1,0.920026,"g,&quot; whereas the appropriate term for the corresponding cooking process applied to peppers is &quot;roasting,&quot; would be more determining of the expected overall probabilities. Note again that the distinction between modifier and predicative trees is important. The standard definition of derivation is entirely appropriate for adjunction probabilities for predicative trees, but not for modifier trees. 3.3 Adding Semantics Finally, the formation of synchronous TAGs has been proposed to allow use of TAGs in semantic interpretation, natural language generation, and machine translation. In previous work (Shieber and Schabes 1990), the definition of synchronous TAG derivation is given in a manner that requires multiple adjunctions at a single node. The need for such derivations follows from the fact that synchronous derivations are intended to model semantic relationships. In cases of multiple adjunction of modifier trees at 9 Intuition is an appropriate guide in the design of the SLTAG framework, as the idea is to set up a linguistically plausible infrastructure on top of which a lexically based statistical model can be built. In addition, suggestive (though certainly not conclusive) evidence along these lines can be"
J94-1004,P85-1011,0,0.0109463,"r. However, tree-adjoining grammars are almost universally extended with augmentations that make the issue apposite. We discuss three such variations here, all of which argue for the use of independent derivations under certain circumstances. 4 3.1 Adding Adjoining Constraints Already in very early work on tree-adjoining grammars (Joshi, Levy, and Takahashi 1975) constraints were allowed to be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi 1985). As an application of this capability, we consider the traditional grammatical view that directional adjuncts can be used only with certain verbs. 5 This would account 4 The formulation of derivation for tree-adjoining grammars is also of significance for other grammatical formalisms based on weaker forms of adjunction such as lexicalized context-free grammar (Schabes and Waters 1993a) and its stochastic extension (Schabes and Waters 1993b), though we do not discuss these arguments here. 5 For instance, Quirk, Greenbaum, Leech, and Svartvik (1985, page 517) remark that &quot;direction adjuncts of"
J94-1004,C88-2147,0,0.0524701,"of the original node, rather than those of the root and foot of the modifier tree, are manifest in the corresponding nodes in the derived tree, the adjoining constraints would propagate appropriately to handle the examples above. This alternative leads, however, to a formalism for which derivation trees are no longer context-free, with concomitant difficulties in designing parsing algorithms. Instead, the extended definition of derivation effectively allows use of a Kleene-* in the &quot;grammar&quot; of derivation trees. Adjoining constraints can also be implemented using feature structure equations (Vijay-Shanker and Joshi 1988). It is possible that judicious use of such techniques might prevent the particular problems noted here. Such an encoding of a solution requires consideration of constraints that pass among many trees just to limit the cooccurrence of a pair of trees. However, it more closely follows the spirit of TAGs to state such intuitively local limitations locally. 7 We use the term 'predication' in its logical sense, that is, for auxiliary trees that serve as logical predicates over the trees into which they adjoin, in contrast to the term's linguistic sub-sense in which the argument of the predicate is"
J94-1004,C90-3001,1,\N,Missing
J94-1004,P93-1017,1,\N,Missing
J94-1004,W90-0102,1,\N,Missing
J95-1007,J91-1004,0,0.0296905,"ologies. Modern programming language design relies heavily on grammatical frameworks and their parsing algorithms for syntactic and semantic analysis. Similarly, natural language systems rely on formalisms and their parsing algorithms. As a computational linguist, one should be well aware of the parsing methods of the 1960s and onward. Functional programming has recently gained popularity in the programming language community. In natural language processing, the statement of parsing algorithms in a functional notation and the use of memoization to handle non-determinism in parsing algorithms (Norvig 1991) are analogous to this direction. This book presents a functional treatment of parsing. Most well-known parsing algorithms for programming languages and natural languages are represented uniformly in a functional notation. The presentation relies heavily on new notations and simple mathematical concepts. The first chapters introduce a notation, ""bunch"" notation, for sets and their constructs. Then, an algorithmic interpretation for the previously defined language is defined. Leermakers achieves a tour de force: each (possibly non-deterministic) parsing algorithm is represented in a compact fun"
J95-1007,P80-1024,0,0.0277494,"ls, and q is the number of items of the form C --* X. 6 with the same X. This is bigger than O(IGIn3), which is the complexity of standard context-free grammar parsers, such as the CKY parser (Kasami 1965), the parser described by Graham, Harrison, and Ruzzo (1980), or efficient implementations of Earley's algorithm (Earley 1970). Most of the references to previous work are found in the notes of the last chapter. Although they are adequate, some important references are missing. For example, the grammar transformation used for eliminating left recursion is very similar to the one described by Rosenkrantz and Lewis (1970) and Rosenkrantz (1967). Similarly, the interpretation of context-free grammars given in the book is similar to that using equations on languages (Chomsky and Sch/itzenberger 1963), which is also described in numerous textbooks, such as Salomaa and Soittola 1978, and Gross and Lentin 1970. The book is mostly free of editorial inconsistencies or errors. Some slipped through, however. For example, on page 55, the caption of Figure 5.3 should refer to Figure 3.1 instead of Figure 1.2. Moreover, three different notations for context-free grammars are used. On page 16, a grammar rule is represented"
J95-1007,P79-1022,0,0.0255482,"minal A. Then, a grammar rule (A, ~p) ~ (X, ~/)(Y, p) states that the non-terminal A can be rewritten as X Y and that the derived string from A (~p) is the concatenation of the one from X (7) and the one from Y (p). Later, a context-free rule is written as A ~ X Y. Although most of the book is mathematically rigorous, there are a few cases where more rigor would have been appropriate. On pages 18 and 19, the existence and the uniqueness of the smallest solution for the interpretation of context-free grammars are proven very casually. Checking the applicability of Tarski's fixed-point theorem (Tarski 1955) on complete partial orders would have been one way to solve this question. The existence of a complete partial order in which the functions used are continuous would have guaranteed the existence and the uniqueness of the solution. Another way to prove this fact would have been to check the applicability of Banach's fixedpoint theorem for contracting mapping in complete metric spaces (Banach 1922), as is usually done for formal power series (Chomsky and Sch/itzenberger 1963). 113 Computational Linguistics Volume 21, Number 1 Although most of the book follows a rigorous mathematical argumentat"
J95-2004,H92-1022,0,0.080019,"utilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: rocbe/schabes@merl.com. (~) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 quired. In addition, the tagger requires drastically less space than stochastic taggers. However, current implementations of Brill's tagger are considerably slowe"
J95-2004,H94-1049,0,0.0290111,"Missing"
J95-2004,A88-1019,0,0.404594,"ve been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: rocbe/schabes@merl.com. (~) 1995 Association for Computationa"
J95-2004,A92-1018,0,0.0525488,"ed to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: rocbe/schabes@merl.com. (~) 1995 Association for Computational Linguistics Computational Lingui"
J95-2004,J88-1003,0,0.0281299,"anguage processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: rocbe/schabes@merl.com. (~) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 qu"
J95-2004,J94-3001,0,0.229981,"ation of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as we"
J95-2004,C92-1025,0,0.0386902,"Missing"
J95-2004,H94-1050,0,0.0157243,"Missing"
J95-2004,E93-1046,0,0.0273478,"recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) de"
J95-2004,J93-2006,0,0.0120164,"ssing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993), parsing (Roche 1993; Tapanainen and Voutilainen 1993), phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994). Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139. E-mail: rocbe/schabes@merl.com. (~) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 quired. In addition, the ta"
J95-2004,E93-1069,0,\N,Missing
J95-4002,T75-2001,0,0.516885,"Missing"
J95-4002,P81-1022,0,0.239041,"G in the style of the CKY parser for CFG can be straightforwardly constructed following the approach shown in Schabes and Waters (1993a). As shown below, one can obtain a more efficient left-to-right parsing algorithm for TIG that maintains the valid prefix property and requires O(n 3) time in the worst case, by combining top-down prediction as in Earley&apos;s algorithm for parsing CFGs 487 Computational Linguistics Volume 21, Number 4 S ,1__ 4. 2 , 4 S ~A A B a A D,I. S* b 5 6 7 ""A---~""D ""b L e f t A u x ( , 1) Subst(, 6) F o o t ( , 8) Figure 8 An auxiliary tree and its textual representation. (Earley 1970) with bottom-up recognition. The algorithm is a general recognizer for TIGs, which requires no condition on the grammar. 6 4.1 A n Earley-Style C u b i c - T i m e Parser For TIG Notation. Suppose that G = (G, NT, L A, S) is a TIG and that al ... an is an input string. The Greek letters #, v, and p are used to designate nodes in elementary trees. Subscripts are used to indicate the label on a node, e.g., #x. Superscripts are sometimes used to distinguish between nodes. A layer of an elementary tree is represented textually in a style similar to a production rule, e.g., #x--*~&apos;Ypz. For instance"
J95-4002,P84-1058,0,0.0807172,"Missing"
J95-4002,C92-2066,1,0.650945,"nal Linguistics Computational Linguistics Volume 21, Number 4 and Rosenkrantz (1967) often produce very large output grammars--so large that they can be awkward or even impossible to parse with. Procedures that convert CFGs into lexicalized CFGs provide only a weak lexicalization, because while they preserve the strings derived, they do not preserve the trees derived. Parsing with the resulting grammar can be fast, but it does not produce the right trees. Strong lexicalization that preserves the trees derived is possible using context-sensitive formalisms such as tree adjoining grammar (TAG) (Joshi and Schabes 1992; Schabes 1990). However, these context-sensitive formalisms entail larger computation costs than CFGs--O(n6)-time in the case of TAG (Vijay-Shanker and Joshi 1985), instead of O(n 3) for CFG. Tree Insertion Grammar (TIG) is a compromise between CFG and TAG that combines the efficiency of the former with the strong lexicalizing power of the latter. As discussed in Section 2, TIG is the same as TAG except that adjunction is restricted so that it no longer generates context-sensitive languages. In section 3, we compare TIG with CFG and TAG, showing how it is related to both. Like CFG, TIG can be"
J95-4002,W90-0207,0,0.0342209,"ree), and so on. By relying on the fact that the intersection of two regular languages must be regular, it is easy to show that L is not a regular language. In particular, consider: L N {SA}*S{BS}*x = {SA}nS{BS}nx This intersection corresponds to all the paths from root to x in the trees that are generated by recursively embedding the elementary auxiliary tree in Figure 7 into the middle of its spine. Since this intersection is not a regular language, L cannot be a regular language. 4. Parsing TIG Since TIG is a restricted case of tree-adjoining grammar (TAG), standard O(n6)-time TAG parsers (Lang 1990; Schabes 1991; Vijay-Shanker 1987; Vijay-Shanker and Weir 1993; Vijay-Shanker and Joshi 1985) can be used for parsing TIG. Further, they can be easily optimized to require at most O(n4)-time when applied to a TIG. However, this still does not take full advantage of the context-freeness of TIG. A simple O(nB)-time bottom-up recognizer for TIG in the style of the CKY parser for CFG can be straightforwardly constructed following the approach shown in Schabes and Waters (1993a). As shown below, one can obtain a more efficient left-to-right parsing algorithm for TIG that maintains the valid prefix"
J95-4002,1991.iwpt-1.4,1,0.856678,"o on. By relying on the fact that the intersection of two regular languages must be regular, it is easy to show that L is not a regular language. In particular, consider: L N {SA}*S{BS}*x = {SA}nS{BS}nx This intersection corresponds to all the paths from root to x in the trees that are generated by recursively embedding the elementary auxiliary tree in Figure 7 into the middle of its spine. Since this intersection is not a regular language, L cannot be a regular language. 4. Parsing TIG Since TIG is a restricted case of tree-adjoining grammar (TAG), standard O(n6)-time TAG parsers (Lang 1990; Schabes 1991; Vijay-Shanker 1987; Vijay-Shanker and Weir 1993; Vijay-Shanker and Joshi 1985) can be used for parsing TIG. Further, they can be easily optimized to require at most O(n4)-time when applied to a TIG. However, this still does not take full advantage of the context-freeness of TIG. A simple O(nB)-time bottom-up recognizer for TIG in the style of the CKY parser for CFG can be straightforwardly constructed following the approach shown in Schabes and Waters (1993a). As shown below, one can obtain a more efficient left-to-right parsing algorithm for TIG that maintains the valid prefix property and"
J95-4002,C88-2121,1,0.817035,"Missing"
J95-4002,J94-1004,1,0.714246,"auxiliary trees are both and cause infinite ambiguity.) TIG does not allow a left (right) auxiliary tree to be adjoined on any node that is on the spine of a right (left) auxiliary tree. Further, no adjunction w h a t e v e r is permitted on a node # that is to the right (left) of the spine of an elementary left (right) auxiliary tree T. Note that for T to be a left (right) auxiliary tree, every frontier node d o m i n a t e d b y # must be labeled with ~. TIG allows arbitrarily m a n y simultaneous adjunctions on a single node in a manner similar to the alternative TAG derivation defined in Schabes and Shieber (1994). Simultaneous adjunction is specified b y two sequences, one of left auxiliary trees and the other of right auxiliary trees that specify the order of the strings corresponding to the trees combined. A TIG derivation starts with an initial tree rooted at S. This tree is repeatedly extended using substitution and adjunction. A derivation is complete w h e n e v e r y frontier node in the tree(s) derived is labeled with a terminal symbol. By means of adjunction, complete derivations can be extended to bigger complete derivations. 2 In Schabes and Waters (1993a) these three kinds of auxiliary tre"
J95-4002,P85-1011,0,0.0203319,"ard or even impossible to parse with. Procedures that convert CFGs into lexicalized CFGs provide only a weak lexicalization, because while they preserve the strings derived, they do not preserve the trees derived. Parsing with the resulting grammar can be fast, but it does not produce the right trees. Strong lexicalization that preserves the trees derived is possible using context-sensitive formalisms such as tree adjoining grammar (TAG) (Joshi and Schabes 1992; Schabes 1990). However, these context-sensitive formalisms entail larger computation costs than CFGs--O(n6)-time in the case of TAG (Vijay-Shanker and Joshi 1985), instead of O(n 3) for CFG. Tree Insertion Grammar (TIG) is a compromise between CFG and TAG that combines the efficiency of the former with the strong lexicalizing power of the latter. As discussed in Section 2, TIG is the same as TAG except that adjunction is restricted so that it no longer generates context-sensitive languages. In section 3, we compare TIG with CFG and TAG, showing how it is related to both. Like CFG, TIG can be parsed in O(IGInB)-time. Section 4 presents an Earley-style parser for TIG that maintains the valid prefix property. Section 5 presents a procedure that converts C"
J95-4002,J93-4002,0,0.0101409,"the intersection of two regular languages must be regular, it is easy to show that L is not a regular language. In particular, consider: L N {SA}*S{BS}*x = {SA}nS{BS}nx This intersection corresponds to all the paths from root to x in the trees that are generated by recursively embedding the elementary auxiliary tree in Figure 7 into the middle of its spine. Since this intersection is not a regular language, L cannot be a regular language. 4. Parsing TIG Since TIG is a restricted case of tree-adjoining grammar (TAG), standard O(n6)-time TAG parsers (Lang 1990; Schabes 1991; Vijay-Shanker 1987; Vijay-Shanker and Weir 1993; Vijay-Shanker and Joshi 1985) can be used for parsing TIG. Further, they can be easily optimized to require at most O(n4)-time when applied to a TIG. However, this still does not take full advantage of the context-freeness of TIG. A simple O(nB)-time bottom-up recognizer for TIG in the style of the CKY parser for CFG can be straightforwardly constructed following the approach shown in Schabes and Waters (1993a). As shown below, one can obtain a more efficient left-to-right parsing algorithm for TIG that maintains the valid prefix property and requires O(n 3) time in the worst case, by combin"
J95-4002,C86-1048,0,0.0297963,"ree is referred to as wrapping adjunction. This is illustrated in Figure 6. The key force of the restrictions applied to TIG, in 483 Computational Linguistics Volume 21, Number 4 w2~w4 w~ Figure 6 Wrapping adjunction. comparison with TAG, is that they prevent wrapping adjunction from occurring, by preventing the creation of wrapping auxiliary trees. 3 Wrapping adjunction yields context-sensitive languages because two strings that are mutually constrained by being in the same auxiliary tree are wrapped around another string. This observation stems from the equivalence of TAG and head grammars (Vijay-Shanker et al. 1986). In contrast, every operation allowed by a TIG inserts a string into another string. Simultaneous adjunction merely specifies multiple independent insertions. Simultaneous left and right adjunction is not an instance of wrapping, because TIG does not allow there to be any constraints between the adjoinability of the trees in question. There are many ways that the TIG formalism could be extended. First, adjoining constraints could be used to prohibit the adjunction of particular auxiliary trees (or all auxiliary trees) at a given node. Second, one can easily imagine variants of TIG where simul"
J95-4002,H86-1020,0,\N,Missing
J95-4002,H90-1055,0,\N,Missing
J95-4002,P93-1017,1,\N,Missing
P88-1032,P81-1022,0,0.065742,"uxiliary tree whose root node is also labeled by X. Then the adjunction of fl to a at node n will be the tree 7 shown in Figure 2. The resulting tree, 7, is built as follows: * The sub-tree of a dominated by n, call it t, is excised, leaving a copy of n behind. • The auxiliary tree fl is attached at n and its root node is identified with n. • The sub-tree t is attached to the foot node of # and the root node n of t is identified with the foot node of ft. $ runs much better than its worst time complexity, we decided to try to adapt Earley's parser for CFGs to TAGs. Earley's algorithm for CFGs (Earley, 1970, Aho and Ullman, 1973) is a bottomup parser which uses top-down information. It manipulates states of the form A -* a.fl[i] while using three processors: the predictor, the completot and the scanner. The algorithm for CFGs runs in O(IGl2n s) time and in O(IGI n2) space in all cases, and parses unambiguous grammars in O(n 2) time (n being the length of the input, IGI the size of the grammar). Given a context-free grammar in any form and an input string al &quot; ' a n , Earley's parser for CFGs maintains the following invariant: The state A --* a./3[i] is in states set Skiff $ S ::b 6A'r, 6 :bal &quot;"
P88-1032,P84-1075,0,0.0184666,"a(dot) is a non-terminal on the frontier of ~ .hieh is marked for subst itut ion: It adds the states {[fl, O, left, above, i, - , - , - , - , - , true] 4.3 ]/~ i s an L n i t i a l tree s . t . # ( O ) -- or(dot)} to Si. P a r s i n g f e a t u r e s t r u c t u r e s for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. Substitution Completor Suppose that the initial tree that we predicted for substitution has been recognized (see Figure 18). Then the algorithm should try to recognize the rest of the tree in which we predicted a substitution. This operation is performed by the s u b s t i tution completor. 267 t br A tUu&quot; m NP br f I PRO tf ..- I, Ubr (a) Vp / V / PP to go to the movies S.top::gtsnsed> = + S,bottom::<tensed> = V.boRom::<tensed> V.bottom::<tensed> = - Figure 19: Updating of features $ F e a t u r e s"
P88-1032,P85-1011,1,0.639221,"nals, S is a distinguished nonterminal, I is a finite set of trees called initial t r e e s and A is a finite set of trees called a u x i l i a r y t r e e s . The trees in I U A are called e l e m e n t a r y trees. I n i t i a l t r e e s (see left tree in Figure 1) are characterized as follows: internal nodes are labeled by non-terminals; leaf nodes are labeled by either terminal symbols or the empty string. Introduction S Although formal properties of Tree Adjoining Grammars (TAGs) have been investigated (VijayShanker, 1987)--for example, there is an O(ns)time CKY-like algorithm for TAGs (Vijay-Shanker and Joshi, 1985)--so far there has been no attempt to develop an Earley-type parser for TAGs. This paper presents an Earley parser for TAGs and discusses modifications to the parsing algorithm that make it possible to handle extensions of TAGs such as constraints on adjunction, subx /x Li~minill$ tofnflnldJ$ Ltef rntnll|$ Figure h Schematic initial and auxiliary trees A u x i l i a r y t r e e s (see right tree in Figure 1) are characterized as follows: internal nodes are labeled by non-terminals; leaf nodes are labeled by a terminal or by the empty string except for exactly one node (called the f o o t n o"
P88-1032,C88-2147,0,0.0897224,"trees rooted by A and tries to recognize the initial tree. This operation is performed by the s u b s t i t u t i o n predictor. It applies t o s[~, dot, left, above, l, f l, fr , star, t~ i b~ , subst?] such that a(dot) is a non-terminal on the frontier of ~ .hieh is marked for subst itut ion: It adds the states {[fl, O, left, above, i, - , - , - , - , - , true] 4.3 ]/~ i s an L n i t i a l tree s . t . # ( O ) -- or(dot)} to Si. P a r s i n g f e a t u r e s t r u c t u r e s for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. Substitution Completor Suppose that the initial tree that we predicted for substitution has been recognized (see Figure 18). Then the algorithm should try to recognize the rest of the tree in which we predicted a substitution. This operation is performed by the s u b s t i tution completor. 267 t br A tUu&quot; m"
P88-1032,C88-1002,0,\N,Missing
P90-1035,P88-1032,1,0.828796,"e adjunctions between the above and below positions of the dot of interior nodes. STAa : .ao • By keeping track of the auxiliary trees being reduced, it is possible to output a parse instead of acceptance or an error. The parser recognizes the derived tree inside out: it extracts recursively the innermost auxiliary tree that has no adjunction performed in it. E F G 2.1 2.2 2.3 H I 3.1 3.2 Figure 3: Left to Right Tree Traversal 5 LR(0) Parsing Tables This section explain how to construct an LR(0) parsing table given a TAG. The construction is an extension of the one used for CFGs. Similarly to Schabes and Joshi (1988), we extend the notion of dotted rules to trees. We define the closure operations that correspond to adjunction. Then we explain how transitions between states are defined. We give in Figure 5 an example of a finite state automaton used to build the parsing table for a TAG (see Figure 5) generating a context-sensitive language. We first explain preliminary concepts (originally defined to construct an Earley-type parser for TAGs) that will be used by the algorithm. Dotted rules are extended to trees. Then we recall a tree traversal that the algorithm will mimic in order to scan the input from l"
P90-1035,J87-1004,0,0.0484334,"shown that they recognize exactly the set of languages recognized by deterministic push down automata. LR(k) parsers for CFGs have been proven useful for compilers as well as recently for natural language processing. For natural language processing, although LR(k) parsers are not powerful enough, *The first author is partially supported by Darpa grant N0014-85K0018, ARO grant DAAL03-89-C-003iPRI NSF grant-IRIS4-10413 A02. We are extremely grateful to Bernard Lang and David Weir for their valuable suggestions. 276 conflicts between multiple choices are solved by pseudoparallelism (Lang, 1974, Tomita, 1987). This gives rise to a class of powerful yet efficient parsers for natural languages. It is in this context that we study deterministic (LR(k)-style) parsing of TAGs. The set of Tree Adjoining Languages is a strict superset of the set of Context Free Languages (CFLs). For example, the cross serial dependency constmction in Dutch can be generated by a TAG. 1 Waiters (1970), R~v6sz (1971), Turnbull and Lee (1979) investigated deterministic parsing of the class of context-sensitive languages. However they used Turing machines which recognize languages much more powerful than Tree Adjoining Langua"
P91-1014,P81-1022,0,0.514065,"ch the predictive behavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart. The recognizer behaves in the worst case in O(IGI2n3)-time and O(IGIn2)-space. However in practice it is always superior to Earley&apos;s parser since the prediction steps have been compiled before runtime. Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portionsof the basic non-deterministic pushdown machine while still using the same pseudoparallel driver. 1 Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none of these approaches have been formally proven to have a behavior superior to well known parsers such as Earley&apos;s parser. Earley himself ([1968] pages 69-89) proposed to precompile the state sets generated by his algorithm to make it as efficient as LR(k) parsers (Knuth, 1965) when used on LR(k) grammars by precomputing all possible states sets that the parser could create."
P91-1014,W89-0221,0,0.137927,"o show that the parser behaves in practice better than Earley&apos;s parser (which is proven to take in the worst case O([G[2n3)-time), the duplication of the same experiments shows no conclusive outcome. Modifications to Tomita&apos;s algorithm have been proposed in order to alleviate the exponential complexity with respect to the input length (Kipps, 1989) but, according to Kipps, the modified algorithm does not lead to a practical parser. Furthermore, the algorithm is doomed to behave in the worst case in exponential time with respect to the g r a m m a r size for some ambiguous grammars and inputs (Johnson, 1989). 2 So far, there is no formal proof showing that the Tomita&apos;s parser can be superior for some grammars and inputs to Earley&apos;s parser, and its worst case complexity seems to contradict the experimental data. As explained, the previous attempts to compile the predictive component are not general and achieve a worst case complexity (with respect to the grammar size and the input length) worse than standard parsers. The methodology we follow in order to compile the predictive component of Earley&apos;s parser i s to define a predictive bottom-up pushdown machine equivalent to the given g r a m m a r w"
P91-1014,W89-0220,0,0.138175,"shdown automaton suffers from an exponential time and space worst case complexity with respect to the input length and also with respect to the grammar size (Johnson [1989] and also page 72 in Tomita [1985]). Although Tomita reports experimental data that seem to show that the parser behaves in practice better than Earley&apos;s parser (which is proven to take in the worst case O([G[2n3)-time), the duplication of the same experiments shows no conclusive outcome. Modifications to Tomita&apos;s algorithm have been proposed in order to alleviate the exponential complexity with respect to the input length (Kipps, 1989) but, according to Kipps, the modified algorithm does not lead to a practical parser. Furthermore, the algorithm is doomed to behave in the worst case in exponential time with respect to the g r a m m a r size for some ambiguous grammars and inputs (Johnson, 1989). 2 So far, there is no formal proof showing that the Tomita&apos;s parser can be superior for some grammars and inputs to Earley&apos;s parser, and its worst case complexity seems to contradict the experimental data. As explained, the previous attempts to compile the predictive component are not general and achieve a worst case complexity (wit"
P91-1014,P85-1018,0,0.0832781,"Missing"
P91-1014,J87-1004,0,0.224019,"iled out. At run time, the machine is driven in pseudo-parallel with the help of a chart. The recognizer behaves in the worst case in O(IGI2n3)-time and O(IGIn2)-space. However in practice it is always superior to Earley&apos;s parser since the prediction steps have been compiled before runtime. Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portionsof the basic non-deterministic pushdown machine while still using the same pseudoparallel driver. 1 Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none of these approaches have been formally proven to have a behavior superior to well known parsers such as Earley&apos;s parser. Earley himself ([1968] pages 69-89) proposed to precompile the state sets generated by his algorithm to make it as efficient as LR(k) parsers (Knuth, 1965) when used on LR(k) grammars by precomputing all possible states sets that the parser could create. However, some context-free grammars, incl"
P92-1017,W89-0209,0,0.263064,"Missing"
P92-1017,H90-1021,0,0.0354795,"Missing"
P92-1017,H91-1060,0,0.019785,"Missing"
P92-1017,C92-2066,1,0.737415,"Missing"
P92-1017,H90-1055,0,0.0301975,"ext W with respect to the grammar G, not The first experiment involves an artificial example used by Lari and Young (1990) in a previous evaluation of the inside-outside algorithm. In this (8). 131 case, training on a bracketed corpus can lead to a good solution while no reasonable solution is found training on raw text only. The initial grammar consists of all possible CNF rules over five nonterminals and the terminals a and b (135 rules), with random rule probabilities. The second experiment uses a naturally occurring corpus and its partially bracketed version provided by the Penn Treebank (Brill et al., 1990). We compare the bracketings assigned by grammars inferred from raw and from bracketed training material with the Penn Treebank bracketings of a separate test set. As shown in Figure 1, with an unbracketed training set W the cross-entropy estimate H(W, GR) remains almost unchanged after 40 iterations (from 1.57 to 1.44) and no useful solution is found. In contrast, with a fully bracketed version C of the same training set, the cross-entropy estimate /~(W, GB) decreases rapidly (1.57 initially, 0.88 after 21 iterations). Similarly, the cross-entropy estimate H ( C , GB) of the bracketed text wi"
P92-1022,P85-1011,0,0.245597,"th respect to the roles of the two auxiliary trees (by inspection), whereas the derived tree is not. By symmetry, therefore, it must be the case that the same independent derivation tree specifies the alternative derived tree in Figure 2(b). Motivation Derivations for Adjoining Constraints Already in very early work on tree-adjoining grammars (Joshi et al., 1975) constraints were allowed to be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi, 1985). As an application of this capability, we consider the remark by Quirk et al. (1985, page 517) that &quot;direction adjuncts of both goal and source can normally be used only with verbs of motion&quot;, which accounts for the distinction between the following sentences: (b) Figure 3: Derivation trees for the derived tree of Figure 2(a) according to the grammar of Figure 1 3 Adding Brockway escorted his sister to the annual cotillion. b. #Brockway resembled his sister to the annual cotillion. This could be modeled by disallowing through selective adjoining constraints the adjunction of the elementary tr"
P92-1022,P92-1022,1,0.0528665,"the foot of a modifier auxiliary tree to the top (rather than the bottom) of the node at which it adjoined (Figure 6b). Recognition and Parsing Following Schabes (1991), the LIG generated by compiling a TAG can be used as the basis for EarIcy recognition. Schabes&apos;s original method must be modified to respect the differences in compilation engendered by extended derivations. Such parsing rules, along with an extension that allows building of explicit derivation trees on-line as a basis for incremental interpretation, have been developed, and are presented in an extended version of this paper (Schabes and Shieber, 1992). In summary, the algorithm operates as a variant of Earley parsing on the corresponding LIG. The set of extended derivations can subsequently be recovered from the set of Earley items generated by the algorithm. The resultant algorithm can be further modified so as to build an explicit derivation tree incrementally as parsing proceeds; this modification, which is a novel result in its own right, allows the parsing algorithm to be used by systems that require incremental processing with respect to tree-adjoining grammars. As a proof of concept, the parsing algorithm just described was implemen"
P92-1022,C92-2066,1,0.80066,"be used as a litmus test for an appropriate definition of derivation. As such, it argues for a nonstandard, independent, notion of derivation for modifier auxiliary trees and a standard, dependent, notion for predicative trees. Brockway conjectured that Harrison wanted to escort his sister. 3.2 [Brockway conjectured that] [Harrison wanted] [to escort his sister] (5)a. Brockway wanted to try to escort his sister. b. [Srockway wanted] [to try] [to escort his sister] Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik, 1992; Schabes, 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the standard defin"
P92-1022,C90-3045,1,0.851113,"er and predicative trees is important. The standard definition of derivation is entirely appropriate for adjunction probabilities for predicative trees, but not for modifier trees. 3.3 Adding (7)a. Brockway paid for the tickets twice intentionally. b. Brockway paid for the tickets intentionally twice. We hope to address this issue in greater detail in future work on synchronous tree-adjoining grammars. Semantics 4 Finally, the formation of synchronous TAGs has been proposed to allow use of TAGs in semantic interpretation, natural language generation, and machine translation. In previous work (Shieber and Schabes, 1990), the definition of synchronous TAG derivation is given in a manner that requires multiple adjunctions at a single node. The need for such derivations follows from the fact that synchronous derivations are intended to model semantic relationships. In cases of multiple adjunction of modifier trees at a single node, the appropriate semantic relationships comprise separate modifications rather than cascaded ones, and this is reflected in the definition of synchronous TAG derivation. 6 Because of this, a parser for synchronous TAGs must recover, at least implicitly, the extended derivations of TAG"
P92-1022,H86-1020,0,\N,Missing
P92-1022,C88-2147,0,\N,Missing
P92-1022,C92-2065,0,\N,Missing
P92-1022,P93-1017,1,\N,Missing
P92-1022,P83-1021,0,\N,Missing
P92-1022,W90-0102,1,\N,Missing
P93-1017,1991.iwpt-1.4,1,0.908773,"tationally efficient, O(n3)-time in the worst case. Recently there has been a gain in interest in the so-called 'mildly' context-sensitive formalisms (Vijay-Shanker, 1987; Weir, 1988; Joshi, VijayShanker, and Weir, 1991; Vijay-Shanker and Weir, 1993a) that generate only a small superset of context-free languages. One such formalism is lexicalized tree-adjoining grammar (LTAG) (Schabes, Abeill~, and Joshi, 1988; Abeillfi et al., 1990; Joshi and Schabes, 1992), which provides a number of attractive properties at the cost of decreased efficiency, O(n6)-time in the worst case (VijayShanker, 1987; Schabes, 1991; Lang, 1990; VijayShanker and Weir, 1993b). In particular, most of the current LTAG grammar for English (Abeilld et al., 1990) follows the restrictions of LCFG. This is of significant practical interest because it means that the processing of these analyses does not require more computational resources than CFGs. In addition, any CFG can be transformed into an equivalent LCFC that generates the same trees (and therefore the same strings). This result breaks new ground, because heretofore every method of lexicalizing CFCs required contextsensitive operations (Joshi and Schabes, 1992). The foll"
P93-1017,J93-4002,0,0.0417819,"to be parsed in O(n3) time in the worst ease. However, LCFC retains most of the key features of LTAG enumerated above. Motivation Context-free grammar (CFG) has been a well accepted framework for computational linguistics for a long time. While it has drawbacks, including the inability to express some linguistic constructions, it has the virtue of being computationally efficient, O(n3)-time in the worst case. Recently there has been a gain in interest in the so-called 'mildly' context-sensitive formalisms (Vijay-Shanker, 1987; Weir, 1988; Joshi, VijayShanker, and Weir, 1991; Vijay-Shanker and Weir, 1993a) that generate only a small superset of context-free languages. One such formalism is lexicalized tree-adjoining grammar (LTAG) (Schabes, Abeill~, and Joshi, 1988; Abeillfi et al., 1990; Joshi and Schabes, 1992), which provides a number of attractive properties at the cost of decreased efficiency, O(n6)-time in the worst case (VijayShanker, 1987; Schabes, 1991; Lang, 1990; VijayShanker and Weir, 1993b). In particular, most of the current LTAG grammar for English (Abeilld et al., 1990) follows the restrictions of LCFG. This is of significant practical interest because it means that the proces"
P93-1017,W90-0207,0,0.285245,"cient, O(n3)-time in the worst case. Recently there has been a gain in interest in the so-called 'mildly' context-sensitive formalisms (Vijay-Shanker, 1987; Weir, 1988; Joshi, VijayShanker, and Weir, 1991; Vijay-Shanker and Weir, 1993a) that generate only a small superset of context-free languages. One such formalism is lexicalized tree-adjoining grammar (LTAG) (Schabes, Abeill~, and Joshi, 1988; Abeillfi et al., 1990; Joshi and Schabes, 1992), which provides a number of attractive properties at the cost of decreased efficiency, O(n6)-time in the worst case (VijayShanker, 1987; Schabes, 1991; Lang, 1990; VijayShanker and Weir, 1993b). In particular, most of the current LTAG grammar for English (Abeilld et al., 1990) follows the restrictions of LCFG. This is of significant practical interest because it means that the processing of these analyses does not require more computational resources than CFGs. In addition, any CFG can be transformed into an equivalent LCFC that generates the same trees (and therefore the same strings). This result breaks new ground, because heretofore every method of lexicalizing CFCs required contextsensitive operations (Joshi and Schabes, 1992). The following sectio"
P93-1017,C88-2121,1,0.947292,"en a well accepted framework for computational linguistics for a long time. While it has drawbacks, including the inability to express some linguistic constructions, it has the virtue of being computationally efficient, O(n3)-time in the worst case. Recently there has been a gain in interest in the so-called 'mildly' context-sensitive formalisms (Vijay-Shanker, 1987; Weir, 1988; Joshi, VijayShanker, and Weir, 1991; Vijay-Shanker and Weir, 1993a) that generate only a small superset of context-free languages. One such formalism is lexicalized tree-adjoining grammar (LTAG) (Schabes, Abeill~, and Joshi, 1988; Abeillfi et al., 1990; Joshi and Schabes, 1992), which provides a number of attractive properties at the cost of decreased efficiency, O(n6)-time in the worst case (VijayShanker, 1987; Schabes, 1991; Lang, 1990; VijayShanker and Weir, 1993b). In particular, most of the current LTAG grammar for English (Abeilld et al., 1990) follows the restrictions of LCFG. This is of significant practical interest because it means that the processing of these analyses does not require more computational resources than CFGs. In addition, any CFG can be transformed into an equivalent LCFC that generates the s"
P93-1017,P92-1022,1,0.814405,"t is independent of sentence length.) if (7/, z) e c[0, 7q and 71 is labeled by ,5' and 71 is the root node of an initial tree in I then return acceptance otherwise return rejection end By recording how each pair was introduced in each cell of the array C, one can easily extend the recognizer to produce all derivations of the input. Note t h a t the sole purl)ose of the codes t and b is to insure t h a t only one auxiliary tree can adjoin on a node. T h e procedure could easily be modified to account for other constraints on the way derivation should proceed such as those suggested for LTAGs (Schabes and Shieber, 1992). T h e procedure A d d puts a pair into the array C. If the pair is already present, nothing is (lone. However, if it is new, it is added to (7 and other pairs may be added as well. These correspond to cases where the coverage is not increased: when a node is the only child of its parent, when the Conclusion LCFG combines much of the power of LTAG with tile c o m p u t a t i o n a l efficiency of C F G . It supports most of the same linguistic analysis supported by LTAC. In particular, most of the current LTAG for English falls into LCFG. In addition, L C F C can lexicalize C F G without alte"
P96-1010,P94-1013,0,0.381583,"on context-sensitive spelling correction and related lexical disambiguation tasks has its limitations. Word-trigram methods (Mays, Damerau, and Mercer, 1991) require an extremely large body of text to train the word-trigram model; even with extensive training sets, the problem of sparse data is often acute. In addition, huge word-trigram tables need to be available at run time. Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense. Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of success for the problem of context-sensitive spelling correction. However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they&apos;re} and {than, then}, where the predominant distinction to be made among the words is syntactic. Introduction Spelling correction has become a very common technology and is often not perceived as a problem where progress can be made. However, conventional spelling checkers, such as Unix spell, are concerned only with spelling errors that"
P96-1010,A88-1019,0,0.012458,"iplicative weight-updating algorithm (Golding and Roth, 1996). We adopt the Bayesian hybrid method, which we will call Bayes, having experimented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; however, the version used here uses an improved smoothing technique, which is mentioned briefly below. P(W, T) T where T is a tag sequence for sentence W. The above probabilities are estimated as is traditionally done in trigram-based part-of-speech tagging (Church, 1988; DeRose, 1988): P(W,T) = P(WIT)P(T ) = HP(wi[ti) i Bayes (1) HP(t, lt,_2t,_l)(2) i where T = tl ...tn, and P(ti]tl-2ti-1) is the prob ability of seeing a part-of-speech tag tl given the two preceding part-of-speech tags ti-2 and ti-1. Equations 1 and 2 will also be used to tag sentences W and W ~ with their most likely part-of-speech sequences. This will allow us to determine the tag that ~In the experiments reported here, the trigram method was run using the tag inventory derived from the Brown corpus, except that a handful of common function words were tagged as themselves, namely: except,"
P96-1010,J88-1003,0,0.017396,"ght-updating algorithm (Golding and Roth, 1996). We adopt the Bayesian hybrid method, which we will call Bayes, having experimented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; however, the version used here uses an improved smoothing technique, which is mentioned briefly below. P(W, T) T where T is a tag sequence for sentence W. The above probabilities are estimated as is traditionally done in trigram-based part-of-speech tagging (Church, 1988; DeRose, 1988): P(W,T) = P(WIT)P(T ) = HP(wi[ti) i Bayes (1) HP(t, lt,_2t,_l)(2) i where T = tl ...tn, and P(ti]tl-2ti-1) is the prob ability of seeing a part-of-speech tag tl given the two preceding part-of-speech tags ti-2 and ti-1. Equations 1 and 2 will also be used to tag sentences W and W ~ with their most likely part-of-speech sequences. This will allow us to determine the tag that ~In the experiments reported here, the trigram method was run using the tag inventory derived from the Brown corpus, except that a handful of common function words were tagged as themselves, namely: except, than, then, to,"
P96-1010,W95-0104,1,0.701876,"n and related lexical disambiguation tasks has its limitations. Word-trigram methods (Mays, Damerau, and Mercer, 1991) require an extremely large body of text to train the word-trigram model; even with extensive training sets, the problem of sparse data is often acute. In addition, huge word-trigram tables need to be available at run time. Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense. Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of success for the problem of context-sensitive spelling correction. However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they&apos;re} and {than, then}, where the predominant distinction to be made among the words is syntactic. Introduction Spelling correction has become a very common technology and is often not perceived as a problem where progress can be made. However, conventional spelling checkers, such as Unix spell, are concerned only with spelling errors that result in words that cannot be found"
W90-0102,E89-1032,0,0.0231195,"Missing"
W90-0102,P85-1012,0,0.0167701,"so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic information. Semantichead-driven generation (Shieber et al., forthcoming; Calder et al., 1989) uses semantic heads and their complements as a locus of semantic locality. Joshi (1987) points out that tree-adjoining grammars may be an especially appropriate formalism for generation because of their syntactic locality properties, which, intuitively at least, ought to correlate with some notion of semantic locality. The same observation runs as an undercurrent in the work of McDonald and Pustejovsky (1985), who apply TAGs to the task of generaScope of the Paper The portion of the full-blown generation problem that we address here is what might be referred to as the tactical as opposed to the strategic generation problem. T h a t is, we are concerned only with how to compute instances of a well-defined relation between strings and canonical logical forms 1 in the direction from logical forms to strings, a problem that is sometimes referred to as ""reversing"" a grammar. This aspect of the generation problem, which ignores the crucial issues in determining what content to communicate, what predicat"
W90-0102,W89-0235,1,0.535065,"ars, the semantic monotonicity requirement precludes ""lexicaliza13 tion"" of the semantics. It is not possible to require nontrivial semantics to be associated with each lexical item. This fact, and the inefficiencies of generation thatfollow from it, was the initial motivation for the move to semantic-head-driven generation (Shieber et al., forthcoming). The efficiencies that that algorithmgains for augmented-context-free generation inhere in the synchronous TAG generation process if the semantic gramamr is lexicalized. In summary, just as lexicalization of the syntactic grammar aids parsing (Schabes and Joshi, 1989), so lexicalization of the semantic grammar aids generation. The simple generation algorithm that we have just presented seems to require that we completely analyze the logical form before generating the target string, as the process is a cascade of three subprocesses: parsing the logical form to a source derivation, mapping from source to target derivation, and computing the target derivation yield. As is common in such cases, portions of these computations can be interleaved, so that generation of the target string can proceed incrementally while traversing the source logical form. To what e"
W90-0102,C88-2121,1,0.323021,"Missing"
W90-0102,C88-2128,1,0.602073,"ation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation grammars as a computational aid is seen to be an inherent property of synchronous TAGs. Introduction The recent history of grammar reversing can be viewed as an effort to recover some notion of semantic locality on which to base a generation process. For instance, Wedekind (1988) requires a property of a grammar that he refers to as connectedness, which specifies that complements be semantically connected to their head. Shieber (1988) defines a notion of semantic monoLonicity, a kind of compositionality property that guarantees that it can be locally determined whether phrases can contribute to forming an expression with a given meaning. Generation schemes that reorder top-down generation (Dymetman and Isabelle, 1988; Strzalkowski, 1989) so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic information. Semantichead-driven generation (Shieber et al., forthcoming; Calder et al., 1989) uses semantic heads and their complements as a locus of semantic localit"
W90-0102,C88-2150,0,0.0334042,"s. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation grammars as a computational aid is seen to be an inherent property of synchronous TAGs. Introduction The recent history of grammar reversing can be viewed as an effort to recover some notion of semantic locality on which to base a generation process. For instance, Wedekind (1988) requires a property of a grammar that he refers to as connectedness, which specifies that complements be semantically connected to their head. Shieber (1988) defines a notion of semantic monoLonicity, a kind of compositionality property that guarantees that it can be locally determined whether phrases can contribute to forming an expression with a given meaning. Generation schemes that reorder top-down generation (Dymetman and Isabelle, 1988; Strzalkowski, 1989) so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic informati"
W90-0102,H86-1020,0,\N,Missing
W90-0102,C90-3001,1,\N,Missing
W90-0102,E89-1001,1,\N,Missing
W90-0102,P88-1032,1,\N,Missing
W90-0102,J87-1005,1,\N,Missing
W90-0102,P89-1027,0,\N,Missing
W90-0102,P85-1011,0,\N,Missing
