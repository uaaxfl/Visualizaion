N15-1186,Unsupervised Morphology Induction Using Word Embeddings,2015,27,60,2,0,4002,radu soricut,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a language agnostic, unsupervised method for inducing morphological transformations between words. The method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages."
2012.amta-papers.18,Improved Domain Adaptation for Statistical Machine Translation,2012,11,19,4,0,4596,wei wang,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We present a simple and effective infrastructure for domain adaptation for statistical machine translation (MT). To build MT systems for different domains, it trains, tunes and deploys a single translation system that is capable of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability."
W11-2102,A Lightweight Evaluation Framework for Machine Translation Reordering,2011,23,31,6,0,25889,david talbot,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the system's reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.
P11-1140,Language-independent compound splitting with morphological operations,2011,8,26,5,1,43878,klaus macherey,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Translating compounds is an important problem in machine translation. Since many compounds have not been observed during training, they pose a challenge for translation systems. Previous decompounding methods have often been restricted to a small set of languages as they cannot deal with more complex compound forming processes. We present a novel and unsupervised method to learn the compound parts and morphological operations needed to split compounds into their compound parts. The method uses a bilingual corpus to learn the morphological operations required to split a compound into its parts. Furthermore, monolingual corpora are used to learn and filter the set of compound part candidates. We evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach."
D11-1017,Training a Parser for Machine Translation Reordering,2011,40,46,4,0,44211,jason katzbrown,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress."
D11-1126,Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation.,2011,12,8,4,0.539042,44842,ashish venugopal,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose a general method to watermark and probabilistically identify the structured outputs of machine learning algorithms. Our method is robust to local editing operations and provides well defined trade-offs between the ability to identify algorithm outputs and the quality of the watermarked output. Unlike previous work in the field, our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one's own algorithm. We present an application in statistical machine translation, where machine translated output is watermarked at minimal loss in translation quality and detected with high recall."
N10-1141,Model Combination for Machine Translation,2010,21,32,4,0,817,john denero,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Machine translation benefits from two types of decoding techniques: consensus decoding over multiple hypotheses under a single model and system combination over hypotheses from different models. We present model combination, a method that integrates consensus decoding and system combination into a unified, forest-based technique. Our approach makes few assumptions about the underlying component models, enabling us to combine systems with heterogenous structure. Unlike most system combination techniques, we reuse the search space of component models, which entirely avoids the need to align translation hypotheses. Despite its relative simplicity, model combination improves translation quality over a pipelined approach of first applying consensus decoding to individual systems, and then applying system combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments."
D10-1016,{``}Poetic{''} Statistical Machine Translation: Rhyme and Meter,2010,16,32,3,0,13188,dmitriy genzel,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"As a prerequisite to translation of poetry, we implement the ability to produce translations with meter and rhyme for phrase-based MT, examine whether the hypothesis space of such a system is flexible enough to accomodate such constraints, and investigate the impact of such constraints on translation quality."
P09-1019,Efficient Minimum Error Rate Training and Minimum {B}ayes-Risk Decoding for Translation Hypergraphs and Lattices,2009,16,74,4,1,12217,shankar kumar,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-the-art Statistical Machine Translation (SMT) systems. The algorithms were originally developed to work with N-best lists of translations, and recently extended to lattices that encode many more hypotheses than typical N-best lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs."
N09-1028,Using a Dependency Parser to Improve {SMT} for Subject-Object-Verb Languages,2009,31,104,4,0.784314,2485,peng xu,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems."
D08-1065,Lattice {M}inimum {B}ayes-{R}isk Decoding for Statistical Machine Translation,2008,29,113,3,0,47432,roy tromble,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabic-to-English, Chinese-to-English and English-to-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance."
D08-1076,Lattice-based Minimum Error Rate Training for Statistical Machine Translation,2008,20,348,2,1,18847,wolfgang macherey,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT."
C08-1144,"A Systematic Comparison of Phrase-Based, Hierarchical and Syntax-Augmented Statistical {MT}",2008,18,89,3,0,44639,andreas zollmann,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Probabilistic synchronous context-free grammar (PSCFG) translation models define weighted transduction rules that represent translation and reordering operations via nonterminal symbols. In this work, we investigate the source of the improvements in translation quality reported when using two PSCFG translation models (hierarchical and syntax-augmented), when extending a state-of-the-art phrase-based baseline that serves as the lexical support for both PSCFG models. We isolate the impact on translation quality for several important design decisions in each model. We perform this comparison on three NIST language translation tasks; Chinese-to-English, Arabic-to-English and Urdu-to-English, each representing unique challenges."
D07-1005,Improving Word Alignment with Bridge Languages,2007,25,45,2,1,12217,shankar kumar,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We describe an approach to improve Statistical Machine Translation (SMT) performance using multi-lingual, parallel, sentence-aligned corpora in several bridge languages. Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a procedure for combining word alignment systems from multiple bridge languages. The final translation is obtained by consensus decoding that combines hypotheses obtained using all bridge language word alignments. We present experiments showing that multilingual, parallel text in Spanish, French, Russian, and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task."
D07-1090,Large Language Models in Machine Translation,2007,44,476,4,0,47297,thorsten brants,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
D07-1105,An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems,2007,17,47,2,1,18847,wolfgang macherey,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the bestranked machine translation engines in the 2006 NIST machine translation evaluation.
2005.mtsummit-tutorials.1,Statistical Machine Translation: Foundations and Recent Advances,2005,7,30,1,1,37712,franz och,Proceedings of Machine Translation Summit X: Tutorial notes,0,None
P04-1077,Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics,2004,16,305,2,0,12609,chinyew lin,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically. The second method relaxes strict n-gram matching to skip-bigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency.
N04-1021,A Smorgasbord of Features for Statistical Machine Translation,2004,12,254,1,1,37712,franz och,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation.
N04-1023,Discriminative Reranking for Machine Translation,2004,25,163,3,0,6930,libin shen,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation."
J04-4002,The Alignment Template Approach to Statistical Machine Translation,2004,43,829,1,1,37712,franz och,Computational Linguistics,0,"A phrase-based statistical machine translation approach xe2x80x94 the alignment template approach xe2x80x94 is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used sourcexe2x80x93channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the Germanxe2x80x93English speech VERBMOBIL task, we analyze the effect of various system components. On the Frenchxe2x80x93English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinesexe2x80x93English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems."
C04-1072,{ORANGE}: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation,2004,10,188,2,0,12609,chinyew lin,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson's product moment correlation coefficient or Spearman's rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, Orange, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using Orange."
2004.iwslt-evaluation.9,The {ISI}/{USC} {MT} system,2004,2,8,5,0,47735,emil ettelaie,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
W03-1209,Statistical {QA} - Classifier vs. Re-ranker: What{'}s the difference?,2003,7,39,3,0,47397,deepak ravichandran,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"In this paper, we show that we can obtain a good baseline performance for Question Answering (QA) by using only 4 simple features. Using these features, we contrast two approaches used for a Maximum Entropy based QA system. We view the QA problem as a classification problem and as a re-ranking problem. Our results indicate that the QA system viewed as a re-ranker clearly outperforms the QA system used as a classifier. Both systems are trained using the same data."
W03-0420,Maximum Entropy Models for Named Entity Recognition,2003,4,114,2,0,47115,oliver bender,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"In this paper, we describe a system that applies maximum entropy (ME) models to the task of named entity recognition (NER). Starting with an annotated corpus and a set of features which are easily obtainable for almost any language, we first build a baseline NE recognizer which is then used to extract the named entities and their context information from additional non-annotated data. In turn, these lists are incorporated into the final recognizer to further improve the recognition accuracy."
P03-1021,Minimum Error Rate Training in Statistical Machine Translation,2003,18,2362,1,1,37712,franz och,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
N03-1017,Statistical Phrase-Based Translation,2003,15,2805,2,0,4417,philipp koehn,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems."
J03-1002,A Systematic Comparison of Various Statistical Alignment Models,2003,37,3252,1,1,37712,franz och,Computational Linguistics,0,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."
E03-1032,Efficient Search for Interactive Statistical Machine Translation,2003,11,48,1,1,37712,franz och,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The goal of interactive machine translation is to improve the productivity of human translators. An interactive machine translation system operates as follows: the automatic system proposes a translation. Now, the human user has two options: to accept the suggestion or to correct it. During the post-editing process, the human user is assisted by the interactive system in the following way: the system suggests an extension of the current translation prefix. Then, the user either accepts this extension (completely or partially) or ignores it. The two most important factors of such an interactive system are the quality of the proposed extensions and the response time. Here, we will use a fully fledged translation system to ensure the quality of the proposed extensions. To achieve fast response times, we will use word hypotheses graphs as an efficient search space representation. We will show results of our approach on the Verbmobil task and on the Canadian Hansards task."
E03-1055,Comparison of Alignment Templates and Maximum Entropy Models for {NLP},2003,0,0,3,0,47115,oliver bender,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
2003.mtsummit-papers.37,Rapid-response machine translation for unexpected languages,2003,14,12,2,0,12879,douglas oard,Proceedings of Machine Translation Summit IX: Papers,0,"Statistical techniques for machine translation offer promise for rapid development in response to unexpected requirements, but realizing that potential requires rapid acquisition of required resources as well. This paper reports the results of experiments with resources collected in ten days; about 1.3 million words of parallel text from five types of sources and a bilingual term list with about 20,000 term pairs. Systems were trained with resources individually and in combination, using an approach based on alignment templates. The use of all available resources was found to yield the best results in an automatic evaluation using the BLEU measure, but a single resource (the Bible) coupled with a small amount of in-domain manual translation (less than 6,000 words) achieved more than 85{\%} of that upper baseline. With a concerted effort, such a system could be built in a single day."
W02-1021,Generation of Word Graphs in Statistical Machine Translation,2002,7,143,2,0,28492,nicola ueffing,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,Statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. We describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. The advantage is that these hypotheses can be rescored using a refined language or translation model. Results are presented on the German-English Verbmobil corpus.
P02-1038,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,2002,15,974,1,1,37712,franz och,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach."
C02-1032,Improving Alignment Quality in Statistical Machine Translation Using Context-dependent Maximum Entropy Models,2002,9,16,2,0,49655,ismael varea,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Typically, statistical alignment models are based on single-word dependencies. These models do not include contextual information, which can lead to inadequate alignments. In this paper, we present an approach to include contextual dependencies in the statistical alignment model by using a refined lexicon model. Unlike previous work, we directly integrate this in the EM algorithm of statistical alignment models. Experimental results are given for the French-English Canadian Parliament Hansards task and the Verbmobil task. The evaluation is performed by comparing the obtained alignments with a manually annotated reference alignment."
garcia-varea-etal-2002-efficient,Efficient integration of maximum entropy lexicon models within the training of statistical alignment models,2002,10,5,2,1,43571,ismael garciavarea,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Maximum entropy (ME) models have been successfully applied to many natural language problems. In this paper, we show how to integrate ME models efficiently within a maximum likelihood training scheme of statistical machine translation models. Specifically, we define a set of context-dependent ME lexicon models and we present how to perform an efficient training of these ME models within the conventional expectation-maximization (EM) training of statistical translation models. Experimental results are also given in order to demonstrate how these ME models improve the results obtained with the traditional translation models. The results are presented by means of alignment quality comparing the resulting alignments with manually annotated reference alignments."
W01-1408,An Efficient {A}* Search Algorithm for Statistical Machine Translation,2001,10,87,1,1,37712,franz och,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"In this paper, we describe an efficient A* search algorithm for statistical machine translation. In contrary to beam-search or greedy approaches it is possible to guarantee the avoidance of search errors with A*. We develop various so-phisticated admissible and almost admissible heuristic functions. Especially our newly developped method to perform a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sentences. We compare the A* search algorithm with a beam-search approach on the Hansards task."
P01-1027,Refined Lexicon Models for Statistical Machine Translation using a Maximum Entropy Approach,2001,17,29,2,1,43571,ismael garciavarea,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Typically, the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information, which often leads to problems in performing a correct word sense disambiguation. One way to deal with this problem within the statistical framework is to use maximum entropy methods. In this paper, we present how to use this type of information within a statistical machine translation system. We show that it is possible to significantly decrease training and test corpus perplexity of the translation models. In addition, we perform a rescoring of N-Best lists using our maximum entropy model and thereby yield an improvement in translation quality. Experimental results are presented on the so-called Verbmobil Task."
2001.mtsummit-road.6,What can machine translation learn from speech recognition?,2001,-1,-1,1,1,37712,franz och,Workshop on MT2010: Towards a Road Map for MT,0,"The performance of machine translation technology after 50 years of development leaves much to be desired. There is a high demand for well performing and cheap MT systems for many language pairs and domains, which automatically adapt to rapidly changing terminology. We argue that for successful MT systems it will be crucial to apply data-driven methods, especially statistical machine translation. In addition, it will be very important to establish common test environments. This includes the availability of large parallel training corpora, well defined test corpora and standardized evaluation criteria. Thereby research results can be compared and this will open the possibility for more competition in MT research."
2001.mtsummit-papers.46,Statistical multi-source translation,2001,-1,-1,1,1,37712,franz och,Proceedings of Machine Translation Summit VIII,0,"We describe methods for translating a text given in multiple source languages into a single target language. The goal is to improve translation quality in applications where the ultimate goal is to translate the same document into many languages. We describe a statistical approach and two specific statistical models to deal with this problem. Our method is generally applicable as it is independent of specific models, languages or application domains. We evaluate the approach on a multilingual corpus covering all eleven official European Union languages that was collected automatically from the Internet. In various tests we show that these methods can significantly improve translation quality. As a side effect, we also compare the quality of statistical machine translation systems for many European languages in the same domain."
P00-1056,Improved Statistical Alignment Models,2000,8,930,1,1,37712,franz och,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies."
niessen-etal-2000-evaluation,An Evaluation Tool for Machine Translation: Fast Evaluation for {MT} Research,2000,6,234,2,0,52339,sonja niessen,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper we present a tool for the evaluation of translation quality. First, the typical requirements of such a tool in the framework of machine translation (MT) research are discussed. We define evaluation criteria which are more adequate than pure edit distance and we describe how the measurement along these quality criteria is performed semi-automatically in a fast, convenient and above all consistent way using our tool and the corresponding graphical user interface."
C00-2163,A Comparison of Alignment Models for Statistical Machine Translation,2000,8,197,1,1,37712,franz och,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper, we present and compare various alignment models for statistical machine translation. We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments. We also compare the impact of different alignment models on the translation quality of a statistical machine translation system."
2000.eamt-1.5,Statistical Machine Translation,2000,-1,-1,1,1,37712,franz och,5th EAMT Workshop: Harvesting Existing Resources,0,None
W99-0604,Improved Alignment Models for Statistical Machine Translation,1999,7,488,1,1,37712,franz och,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,None
E99-1010,An Efficient Method for Determining Bilingual Word Classes,1999,12,177,1,1,37712,franz och,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,In statistical natural language processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximum-likelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
P98-2162,Improving Statistical Natural Language Translation with Categories and Rules,1998,6,33,1,1,37712,franz och,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes an all level approach on statistical natural language translation (SNLT). Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA). The translation process itself is realized as a beam search. In our method example-based techniques enter an overall statistical approach leading to about 50 percent correctly translated sentences applied to the very difficult English-German VERBMOBIL spontaneous speech corpus."
C98-2157,Improving Statistical Natural Language Translation with Categories and Rules,1998,6,33,1,1,37712,franz och,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes an all level approach on statistical natural language translation (SNLT). Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA). The translation process itself is realized as a beam search. In our method example-based techniques enter an overall statistical approach leading to about 50 percent correctly translated sentences applied to the very difficult English-German VERBMOBIL spontaneous speech corpus."
