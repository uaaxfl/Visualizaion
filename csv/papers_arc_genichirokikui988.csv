P11-2128,Entity Set Expansion using Topic information,2011,19,13,4,0,32868,kugatsu sadamitsu,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper proposes three modules based on latent topics of documents for alleviating semantic drift in bootstrapping entity set expansion. These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsupervised way. Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain."
I11-1103,Identification of relations between answers with global constraints for Community-based Question Answering services,2011,18,1,3,0,18349,hikaru yokono,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Community-based Question Answering services contain many threads consisting of a question and its answers. When there are many answers for a question, it is hard for a user to understand them all. To address this problem, we focus on logi- cal relations between answers in a thread and present a model for identifying the relations between the answers. We con- sider that there are constraints among the relations, such as a transitive law, and that it might be useful to take these con- straints into account. To consider these constraints, we propose the model based on a Markov logic network. We also in- troduce super-relations to give additional information for logical relation identifica- tion into our model. Through the experi- ment, we show that global constraints and super-relations make it easier to identify the relations."
W10-3709,Standardizing Complex Functional Expressions in {J}apanese Predicates: Applying Theoretically-Based Paraphrasing Rules,2010,13,7,3,0,39511,tomoko izumi,Proceedings of the 2010 Workshop on Multiword Expressions: from Theory to Applications,0,"In order to accomplish the deep semantic understanding of a language, it is essential to analyze the meaning of predicate phrases, a content word plus functional expressions. In agglutinating languages such as Japanese, however, sentential predicates are multi-morpheme expressions and all the functional expressions including those unnecessary to the meaning of the predicate are merged into one phrase. This triggers an increase in surface forms, which is problematic for NLP systems. We solve this by introducing simplified surface forms of predicates that retain only the crucial meaning of the functional expressions. We construct paraphrasing rules based on syntactic and semantic theories in linguistics. The results of experiments show that our system achieves the high accuracy of 77% while reducing the differences in surface forms by 44%, which is quite close to the performance of manually simplified predicates."
P10-2060,Optimizing Informativeness and Readability for Sentiment Summarization,2010,18,20,4,0,18348,hitoshi nishikawa,Proceedings of the {ACL} 2010 Conference Short Papers,0,"We propose a novel algorithm for sentiment summarization that takes account of informativeness and readability, simultaneously. Our algorithm generates a summary by selecting and ordering sentences taken from multiple review texts according to two scores that represent the informativeness and readability of the sentence order. The informativeness score is defined by the number of sentiment expressions and the readability score is learned from the target corpus. We evaluate our method by summarizing reviews on restaurants. Our method outperforms an existing algorithm as indicated by its ROUGE score and human readability experiments."
C10-2046,Learning to Model Domain-Specific Utterance Sequences for Extractive Summarization of Contact Center Dialogues,2010,24,5,7,0,1442,ryuichiro higashinaka,Coling 2010: Posters,0,"This paper proposes a novel extractive summarization method for contact center dialogues. We use a particular type of hidden Markov model (HMM) called Class Speaker HMM (CSHMM), which processes operator/caller utterance sequences of multiple domains simultaneously to model domain-specific utterance sequences and common (domain-wide) sequences at the same time. We applied the CSHMM to call summarization of transcripts in six different contact center domains and found that our method significantly outperforms competitive baselines based on the maximum coverage of important words using integer linear programming."
C10-2047,Recognizing Relation Expression between Named Entities based on Inherent and Context-dependent Features of Relational words,2010,18,3,4,1,33720,toru hirano,Coling 2010: Posters,0,"This paper proposes a supervised learning method to recognize expressions that show a relation between two named entities, e.g., person, location, or organization. The method uses two novel features, 1) whether the candidate words inherently express relations and 2) how the candidate words are influenced by the past relations of two entities. These features together with conventional syntactic and contextual features are organized as a tree structure and are fed into a boosting-based classification algorithm. Experimental results show that the proposed method outperforms conventional methods."
C10-2105,Opinion Summarization with Integer Linear Programming Formulation for Sentence Extraction and Ordering,2010,28,45,4,0,18348,hitoshi nishikawa,Coling 2010: Posters,0,"In this paper we propose a novel algorithm for opinion summarization that takes account of content and coherence, simultaneously. We consider a summary as a sequence of sentences and directly acquire the optimum sequence from multiple review documents by extracting and ordering the sentences. We achieve this with a novel Integer Linear Programming (ILP) formulation. Our proposed formulation is a powerful mixture of the Maximum Coverage Problem and the Traveling Salesman Problem, and is widely applicable to text generation and summarization tasks. We score each candidate sequence according to its content and coherence. Since our research goal is to summarize reviews, the content score is defined by opinions and the coherence score is developed in training against the review document corpus. We evaluate our method using the reviews of commodities and restaurants. Our method outperforms existing opinion summarizers as indicated by its ROUGE score. We also report the results of human readability experiments."
P07-2040,Detecting Semantic Relations between Named Entities in Text Using Contextual Features,2007,9,13,3,1,33720,toru hirano,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"This paper proposes a supervised learning method for detecting a semantic relation between a given pair of named entities, which may be located in different sentences. The method employs newly introduced contextual features based on centering theory as well as conventional syntactic and word-based features. These features are organized as a tree structure and are fed into a boosting-based classification algorithm. Experimental results show the proposed method outperformed prior methods, and increased precision and recall by 4.4% and 6.7%."
P07-2057,{J}apanese Dependency Parsing Using Sequential Labeling for Semi-spoken Language,2007,5,14,2,0.587992,324,kenji imamura,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"The amount of documents directly published by end users is increasing along with the growth of Web 2.0. Such documents often contain spoken-style expressions, which are difficult to analyze using conventional parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse self-dependent (independent) segments using sequential labeling."
O07-5005,Multilingual Spoken Language Corpus Development for Communication Research,2007,0,16,2,1,43353,toshiyuki takezawa,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 12, Number 3, September 2007: Special Issue on Invited Papers from {ISCSLP} 2006",0,None
P06-2123,Subword-Based Tagging for Confidence-Dependent {C}hinese Word Segmentation,2006,13,23,2,1,46412,ruiqiang zhang,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging. The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods. We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments. In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971)."
N06-2049,Subword-based Tagging by Conditional Random Fields for {C}hinese Word Segmentation,2006,5,54,2,1,46412,ruiqiang zhang,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005."
kageura-kikui-2006-self,A Self-Referring Quantitative Evaluation of the {ATR} Basic Travel Expression Corpus ({BTEC}),2006,9,5,2,0,17687,kyo kageura,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we evaluate the Basic Travel Expression Corpus (BTEC), developed by ATR (Advanced Telecommunication Research Laboratory), Japan. BTEC was specifically developed as a wide-coverage, consistent corpus containing basic Japanese travel expressions with English counterparts, for the purpose of providing basic data for the development of high quality speech translation systems. To evaluate the corpus, we introduce a quantitative method for evaluating the sufficiency of qualitatively well-defined corpora, on the basis of LNRE methods that can estimate the potential growth patterns of various sparse data by fitting various skewed distributions such as the Zipfian group of distributions, lognormal distribution, and inverse Gauss-Poisson distribution to them. The analyses show the coverage of lexical items of BTEC vis-a-vis the possible targets implicitly defined by the corpus itself, and thus provides basic insights into strategies for enhancing BTEC in future."
2005.mtsummit-papers.27,Assessing Degradation of Spoken Language Translation by Measuring Speech Recognizer{'}s Output against Non-native Speakers{'} Listening Capabilities,2005,-1,-1,4,1,43353,toshiyuki takezawa,Proceedings of Machine Translation Summit X: Papers,0,None
2005.iwslt-1.2,A decoding algorithm for word lattice translation in speech translation,2005,9,8,2,1,46412,ruiqiang zhang,Proceedings of the Second International Workshop on Spoken Language Translation,0,None
2005.iwslt-1.3,Using multiple recognition hypotheses to improve speech translation,2005,11,1,2,1,46412,ruiqiang zhang,Proceedings of the Second International Workshop on Spoken Language Translation,0,"This paper describes our recent work on integrating speech recognition and machine translation for improving speech translation performance. Two approaches are applied and their performance are evaluated in the workshop of IWSLT 2005. The first is direct N-best hypothesis translation, and the second, a pseudo-lattice decoding algorithm for translating word lattice, can dramatically reduce computation cost incurred by the first approach. We found in the experiments that both of these approaches could improve speech translation significantly."
W04-3244,Learning Nonstructural Distance Metric by Minimum Cluster Distortion,2004,20,9,2,0,21449,daichi mochihashi,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-1708,Automatic Measuring of {E}nglish Language Proficiency using {MT} Evaluation Technology,2004,6,3,5,1,4955,keiji yasuda,Proceedings of the Workshop on e{L}earning for Computational Linguistics and Computational Linguistics for e{L}earning,0,"Assisting in foreign language learning is one of the major areas in which natural language processing technology can contribute. This paper proposes a computerized method of measuring communicative skill in English as a foreign language. The proposed method consists of two parts. The first part involves a test sentence selection part to achieve precise measurement with a small test set. The second part is the actual measurement, which has three steps. Step one asks proficiency-known human subjects to translate Japanese sentences into English. Step two gauges the match between the translations of the subjects and correct translations based on the n-gram overlap or the edit distance between translations. Step three learns the relationship between proficiency and match. By regression it finds a straight-line fitting for the scatter plot representing the proficiency and matches of the subjects. Then, it estimates proficiency of proficiency-unknown users by using the line and the match. Based on this approach, we conducted experiments on estimating the Test of English for International Communication (TOEIC) score. We collected two sets of data consisting of English sentences translated from Japanese. The first set consists of 330 sentences, each translated to English by 29 subjects with varied English proficiency. The second set consists of 510 sentences translated in a similar manner by a separate group of 18 subjects. We found that the estimated scores correlated with the actual scores."
takezawa-kikui-2004-comparative,A Comparative Study on Human Communication Behaviors and Linguistic Characteristics for Speech-to-Speech Translation,2004,8,8,2,1,43353,toshiyuki takezawa,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"A large bilingual corpus of English and Japanese is being built at ATR Spoken Language Translation Research Laboratories in order to improve speech translation technology to the level where people can use a portable translation system for traveling abroad, dining and shopping, and hotel situations. As a part of these corpus construction activities, we have been collecting spoken dialogue data by using an experimental translation system between English and Japanese. In a previous study, we found that humans communicate as part of their daily social life, so they prefer using complex sentences and saying than one sentence per utterance. However, corpus-based machine translation systems for conversational expressions tend to be limited to dealing with short simple sentences. To find a way to bridge the gap between human communication behaviors and system performance, we examined the relationship between instructions and linguistic expressions. The experimental results suggest that a state-of-the-art translation system may be useful for subjects who can make their utterance length short by following instructions."
C04-1168,A Unified Approach in Speech-to-Speech Translation: Integrating Features of Speech recognition and Machine Translation,2004,13,46,2,1,46412,ruiqiang zhang,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Based upon a statistically trained speech translation system, in this study, we try to combine distinctive features derived from the two modules: speech recognition and statistical machine translation, in a loglinear model. The translation hypotheses are then rescored and translation performance is improved. The standard translation evaluation metrics, including BLEU, NIST, multiple reference word error rate and its position independent counterpart, were optimized to solve the weights of the features in the log-linear model. The experimental results have shown significant improvement over the baseline IBM model 4 in all automatic translation evaluation metrics. The largest was for BLEU, by 7.9% absolute."
2004.iwslt-papers.8,Multi-lingual speech recognition system for speech-to-speech translation,2004,13,2,6,0,1441,satoshi nakamura,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,"This paper describes the speech recognition module of the speech-to-speech translation system being currently developed at ATR. It is a multi-lingual large vocabulary continuous speech recognition system supporting Japanese, English and Chinese languages. A corpusbased statistical approach was adopted for the system design. The database we collected consists of more than 600 000 sentences covering broad range of travel related conversations in each of the three languages. The recognition system is based on language-dependent acoustic and language models, and pronunciation dictionaries. The models are built using the latest training methods developed at ATR as the Minimum Description Length Successive State Splitting (MDL-SSS) and Multi-dimensional Composite N-gram techniques. The specifics of each language are taken into account in order to achieve high recognition performance. The speech recognition system is under constant improvement and enhancement, and although the models for the different languages are at different development stages, the recent evaluation experiments showed that the recognition performance is above 92% for every language."
W02-0704,Finding Translation Pairs from {E}nglish-{J}apanese Untokenized Aligned Corpora,2002,6,4,1,1,44638,genichiro kikui,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"This paper proposes a new algorithm for finding translation pairs in an English-Japanese parallel aligned corpus. Unlike previous methods, our algorithm does not presuppose a separate tokenizer for Japanese, but finds translation pairs as side-effects of unsupervised tokenization of Japanese sentences by using information from the English sentences. The algorithm is based on the observation that two Japanese sentences tend to have a common word when their English mates (i.e., aligned sentences) contain the same word. We implemented this idea as an unsupervised tokenization of Japanese with extended Hidden-Markov-Models (HMMs), where hidden n-gram probabilities (i.e., state transition probabilities) are affected by co-occurring words in the English part. Our experiment on finding noun-noun translation pairs achieved 76.3% accuracy, which was 0.4 points lower than the result using supervised tokenization."
suyaga-etal-2002-proposal,Proposal of a very-large-corpus acquisition method by cell-formed registration,2002,4,4,3,0,53548,fumiaki suyaga,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"One promising way to improve the performance of a speech translation system is to collect a large volume of data in the target tasks/domains. However, a naive expansion of the traditional data collection scheme consumes valuable resources. Advanced speech recognition technology can provide a highly accurate recognizer if a machine-friendly speech is permitted. We propose a new data collection scheme that is supported by this speaking style. The preliminary results of data collection show that the proposed scheme has a three-digit efficiency."
W99-0905,Resolving Translation Ambiguity using Non-parallel Bilingual Corpora,1999,10,25,1,1,44638,genichiro kikui,Unsupervised Learning in Natural Language Processing,0,"This paper presents an unsupervised method for choosing the correct translation of a word in context. It learns disambiguation information from nonparallel bilinguM corpora (preferably in the same domain) free from tagging. Our method combines two existing unsupervised disambiguation algorithms: a word sense disambiguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpora. For the given word in context, the former algorithm identifies its meaning as one of a number of predefined usage classes derived by clustering a large amount of usages in the source language corpus. The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that is most relevant to the usage. This paper also shows preliminary results of translation experiments."
1999.mtsummit-1.24,A scalable cross-language metasearch architecture for multilingual information access on the Web,1999,-1,-1,2,0,18065,yoshihiko hayashi,Proceedings of Machine Translation Summit VII,0,"This position paper for the special session on ``Multilingual Information Access'' comprises of three parts. The first part reviews possible demands for Multilingual Information Access (hereafter, MLIA) on the Web, and examines required technical elements. Among those, we, in the second part, focus on Cross-Language Information Retrieval (hereafter, CLIR), particularly a scalable architecture which enables CLIR in a number of language combinations. Such a distributed architecture developed around XIRCH project (an international joint experimental project currently involves NTT, KRDL, and KAIST) is then described in a certain detail. The final part discusses some NLP/MT related issues associated with such a CLIR architecture."
P98-1110,Term-list Translation using Mono-lingual Word Co-occurrence Vectors,1998,8,10,1,1,44638,genichiro kikui,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"A term-list is a list of content words that characterize a consistent text or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most 'coherent' combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations)."
C98-1106,Term-list Translation using Mono-lingual Word Co-occurrence Vectors,1998,8,10,1,1,44638,genichiro kikui,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"A term-list is a list of content words that characterize a consistent text or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most 'coherent' combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations)."
C94-1058,Default Handling in Incremental Generation,1994,8,2,2,0,14013,karin harbusch,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,Natural language generation must work with insufficient input. Underspecifications can be caused by shortcomings of the component providing the input or by the preliminary state of incrementally given input. The paper aims to escape from such dead-end situations by making assumptions. We discuss global aspects of default handling. Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion.
C92-3164,A Spoken Language Translation System: {SL-TRANS}2,1992,9,13,4,0,55960,tsuyoshi morimoto,{COLING} 1992 Volume 3: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
C92-1009,Feature Structure Based Semantic Head Driven Generation,1992,9,4,1,1,44638,genichiro kikui,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper proposes a generation method for feature-structure-based unification grammars. As compared with fixed arity term notation. feature structure notation is more flexible for representing knowledge needed to generate idiomatic structures as well as general constructions. The method enables feature structure retrieval via multiple indices. The indexing mechanism, when used with a semantic head driven generation algorithm, attains efficient generation even when a large amount of generation knowledge must be considered. Our method can produce all possible structures in parallel, using structure sharing among ambiguous substructures."
