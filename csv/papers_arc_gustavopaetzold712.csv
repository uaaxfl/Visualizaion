2021.semeval-1.1,{S}em{E}val-2021 Task 1: Lexical Complexity Prediction,2021,-1,-1,3,0,1608,matthew shardlow,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper presents the results and main findings of SemEval-2021 Task 1 - Lexical Complexity Prediction. We provided participants with an augmented version of the CompLex Corpus (Shardlow et al. 2020). CompLex is an English multi-domain corpus in which words and multi-word expressions (MWEs) were annotated with respect to their complexity using a five point Likert scale. SemEval-2021 Task 1 featured two Sub-tasks: Sub-task 1 focused on single words and Sub-task 2 focused on MWEs. The competition attracted 198 teams in total, of which 54 teams submitted official runs on the test data to Sub-task 1 and 37 to Sub-task 2."
2021.semeval-1.78,{UTFPR} at {S}em{E}val-2021 Task 1: Complexity Prediction by Combining {BERT} Vectors and Classic Features,2021,-1,-1,1,1,1610,gustavo paetzold,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"We describe the UTFPR systems submitted to the Lexical Complexity Prediction shared task of SemEval 2021. They perform complexity prediction by combining classic features, such as word frequency, n-gram frequency, word length, and number of senses, with BERT vectors. We test numerous feature combinations and machine learning models in our experiments and find that BERT vectors, even if not optimized for the task at hand, are a great complement to classic features. We also find that employing the principle of compositionality can potentially help in phrase complexity prediction. Our systems place 45th out of 55 for single words and 29th out of 38 for phrases."
2020.winlp-1.6,{SIMPLEX}-{PB} 2.0: A Reliable Dataset for Lexical Simplification in {B}razilian {P}ortuguese,2020,-1,-1,2,0,14003,nathan hartmann,Proceedings of the The Fourth Widening Natural Language Processing Workshop,0,"Most research on Lexical Simplification (LS) addresses non-native speakers of English, since they are numerous and easy to recruit. This makes it difficult to create LS solutions for other languages and target audiences. This paper presents SIMPLEX-PB 2.0, a dataset for LS in Brazilian Portuguese that, unlike its predecessor SIMPLEX-PB, accurately captures the needs of Brazilian underprivileged children. To create SIMPLEX-PB 2.0, we addressed all limitations of the old SIMPLEX-PB through multiple rounds of manual annotation. As a result, SIMPLEX-PB 2.0 features much more reliable and numerous candidate substitutions to complex words, as well as word complexity rankings produced by a group underprivileged children."
2020.semeval-1.140,{UTFPR} at {S}em{E}val-2020 Task 7: Using Co-occurrence Frequencies to Capture Unexpectedness,2020,-1,-1,1,1,1610,gustavo paetzold,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"We describe the UTFPR system for SemEval-2020{'}s Task 7: Assessing Humor in Edited News Headlines. Ours is a minimalist unsupervised system that uses word co-occurrence frequencies from large corpora to capture unexpectedness as a mean to capture funniness. Our system placed 22nd on the shared task{'}s Task 2. We found that our approach requires more text than we used to perform reliably, and that unexpectedness alone is not sufficient to gauge funniness for humorous content that targets a diverse target audience."
2020.semeval-1.297,{UTFPR} at {S}em{E}val 2020 Task 12: Identifying Offensive Tweets with Lightweight Ensembles,2020,-1,-1,2,0,15402,marcos boriola,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Offensive language is a common issue on social media platforms nowadays. In an effort to address this issue, the SemEval 2020 event held the OffensEval 2020 shared task where the participants were challenged to develop systems that identify and classify offensive language in tweets. In this paper, we present a system that uses an Ensemble model stacking a BOW model and a CNN model that led us to place 29th in the ranking for English sub-task A."
W19-1423,Experiments in Cuneiform Language Identification,2019,26,0,1,1,1610,gustavo paetzold,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the first writing systems in the world. We report the results obtained by the PZ team in the Cuneiform Language Identification (CLI) shared task organized within the scope of the VarDial Evaluation Campaign 2019. The task included two languages, Sumerian and Akkadian. The latter is divided into six dialects: Old Babylonian, Middle Babylonian peripheral, Standard Babylonian, Neo Babylonian, Late Babylonian, and Neo Assyrian. We approach the task using a meta-classifier trained on various SVM models and we show the effectiveness of the system for this task. Our submission achieved 0.738 F1 score in discriminating between the seven languages and dialects and it was ranked fourth in the competition among eight teams."
S19-2093,{UTFPR} at {S}em{E}val-2019 Task 5: Hate Speech Identification with Recurrent Neural Networks,2019,15,0,1,1,1610,gustavo paetzold,Proceedings of the 13th International Workshop on Semantic Evaluation,0,In this paper we revisit the problem of automatically identifying hate speech in posts from social media. We approach the task using a system based on minimalistic compositional Recurrent Neural Networks (RNN). We tested our approach on the SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (HatEval) shared task dataset. The dataset made available by the HatEval organizers contained English and Spanish posts retrieved from Twitter annotated with respect to the presence of hateful content and its target. In this paper we present the results obtained by our system in comparison to the other entries in the shared task. Our system achieved competitive performance ranking 7th in sub-task A out of 62 systems in the English track.
S19-2140,{UTFPR} at {S}em{E}val-2019 Task 6: Relying on Compositionality to Find Offense,2019,0,2,1,1,1610,gustavo paetzold,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present the UTFPR system for the OffensEval shared task of SemEval 2019: A character-to-word-to-sentence compositional RNN model trained exclusively over the training data provided by the organizers. We find that, although not very competitive for the task at hand, it offers a robust solution to the orthographic irregularity inherent to tweets."
W18-6483,{UTFPR} at {WMT} 2018: Minimalistic Supervised Corpora Filtering for Machine Translation,2018,0,0,1,1,1610,gustavo paetzold,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present the UTFPR systems at the WMT 2018 parallel corpus filtering task. Our supervised approach discerns between good and bad translations by training classic binary classification models over an artificially produced binary classification dataset derived from a high-quality translation set, and a minimalistic set of 6 semantic distance features that rely only on easy-to-gather resources. We rank translations by their probability for the {``}good{''} label. Our results show that logistic regression pairs best with our approach, yielding more consistent results throughout the different settings evaluated."
W18-6224,{UTFPR} at {IEST} 2018: Exploring Character-to-Word Composition for Emotion Analysis,2018,0,2,1,1,1610,gustavo paetzold,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"We introduce the UTFPR system for the Implicit Emotions Shared Task of 2018: A compositional character-to-word recurrent neural network that does not exploit heavy and/or hard-to-obtain resources. We find that our approach can outperform multiple baselines, and offers an elegant and effective solution to the problem of orthographic variance in tweets."
W18-0507,A Report on the Complex Word Identification Shared Task 2018,2018,16,0,4,0,282,seid yimam,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT{'}2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks: English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks: binary classification and probabilistic classification. A total of 12 teams submitted their results in different task/track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings."
L18-1553,Text Simplification from Professionally Produced Corpora,2018,0,3,2,0.310911,7140,carolina scarton,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1685,{S}im{PA}: A Sentence-Level Simplification Corpus for the Public Administration Domain,2018,0,1,2,0.310911,7140,carolina scarton,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1021,"{L}exi: A tool for adaptive, personalized text simplification",2018,0,2,2,0.428571,24953,joachim bingel,Proceedings of the 27th International Conference on Computational Linguistics,0,"Most previous research in text simplification has aimed to develop generic solutions, assuming very homogeneous target audiences with consistent intra-group simplification needs. We argue that this assumption does not hold, and that instead we need to develop simplification systems that adapt to the individual needs of specific users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users."
W17-5910,Complex Word Identification: Challenges in Data Annotation and System Performance,2017,9,0,3,0,622,marcos zampieri,Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017),0,"This paper revisits the problem of complex word identification (CWI) following up the SemEval CWI shared task. We use ensemble classifiers to investigate how well computational methods can discriminate between complex and non-complex words. Furthermore, we analyze the classification performance to understand what makes lexical complexity challenging. Our findings show that most systems performed poorly on the SemEval CWI dataset, and one of the reasons for that is the way in which human annotation was performed."
W17-4765,Feature-Enriched Character-Level Convolutions for Text Regression,2017,14,0,1,1,1610,gustavo paetzold,Proceedings of the Second Conference on Machine Translation,0,None
I17-5005,"The Ultimate Presentation Makeup Tutorial: How to {P}olish your Posters, Slides and Presentations Skills",2017,0,0,1,1,1610,gustavo paetzold,"Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",0,"There is no question that our research community have, and still has been producing an insurmountable amount of interesting strategies, models and tools to a wide array of problems and challenges in diverse areas of knowledge. But for as long as interesting work has existed, we{'}ve been plagued by a great unsolved mystery: how come there is so much interesting work being published in conferences, but not as many interesting and engaging posters and presentations being featured in them? In this tutorial, we present practical step-by-step makeup solutions for poster, slides and oral presentations in order to help researchers who feel like they are not able to convey the importance of their research to the community in conferences."
I17-3001,{MASSA}lign: Alignment and Annotation of Comparable Documents,2017,7,6,1,1,1610,gustavo paetzold,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"We introduce MASSAlign: a Python library for the alignment and annotation of monolingual comparable documents. MASSAlign offers easy-to-use access to state of the art algorithms for paragraph and sentence-level alignment, as well as novel algorithms for word-level annotation of transformation operations between aligned sentences. In addition, MASSAlign provides a visualization module to display and analyze the alignments and annotations performed."
I17-1030,Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs,2017,0,6,3,0,1658,fernando alvamanchego,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that generalization becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a method that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of transformations that different approaches can model."
E17-2006,Lexical Simplification with Neural Ranking,2017,0,8,1,1,1610,gustavo paetzold,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We present a new Lexical Simplification approach that exploits Neural Networks to learn substitutions from the Newsela corpus - a large set of professionally produced simplifications. We extract candidate substitutions by combining the Newsela corpus with a retrofitted context-aware word embeddings model and rank them using a new neural regression model that learns rankings from annotated data. This strategy leads to the highest Accuracy, Precision and F1 scores to date in standard datasets for the task."
W16-2381,{SHEF}-{MIME}: Word-level Quality Estimation Using Imitation Learning,2016,4,1,3,0,5907,daniel beck,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2388,{S}imple{N}ets: Quality Estimation with Resource-Light Neural Networks,2016,5,3,1,1,1610,gustavo paetzold,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
S16-1085,{S}em{E}val 2016 Task 11: Complex Word Identification,2016,6,21,1,1,1610,gustavo paetzold,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1149,{SV}000gg at {S}em{E}val-2016 Task 11: Heavy Gauge Complex Word Identification with System Voting,2016,8,2,1,1,1610,gustavo paetzold,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
N16-1050,Inferring Psycholinguistic Properties of Words,2016,12,7,1,1,1610,gustavo paetzold,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1491,Benchmarking Lexical Simplification Systems,2016,0,4,1,1,1610,gustavo paetzold,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Lexical Simplification is the task of replacing complex words in a text with simpler alternatives. A variety of strategies have been devised for this challenge, yet there has been little effort in comparing their performance. In this contribution, we present a benchmarking of several Lexical Simplification systems. By combining resources created in previous work with automatic spelling and inflection correction techniques, we introduce BenchLS: a new evaluation dataset for the task. Using BenchLS, we evaluate the performance of solutions for various steps in the typical Lexical Simplification pipeline, both individually and jointly. This is the first time Lexical Simplification systems are compared in such fashion on the same data, and the findings introduce many contributions to the field, revealing several interesting properties of the systems evaluated."
C16-3004,Quality Estimation for Language Output Applications,2016,9,0,2,0.310911,7140,carolina scarton,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Tutorial Abstracts",0,"Quality Estimation (QE) of language output applications is a research area that has been attracting significant attention. The goal of QE is to estimate the quality of language output applications without the need of human references. Instead, machine learning algorithms are used to build supervised models based on a few labelled training instances. Such models are able to generalise over unseen data and thus QE is a robust method applicable to scenarios where human input is not available or possible. One such a scenario where QE is particularly appealing is that of Machine Translation, where a score for predicted quality can help decide whether or not a translation is useful (e.g. for post-editing) or reliable (e.g. for gisting). Other potential applications within Natural Language Processing (NLP) include Text Summarisation and Text Simplification. In this tutorial we present the task of QE and its application in NLP, focusing on Machine Translation. We also introduce QuEst++, a toolkit for QE that encompasses feature extraction and machine learning, and propose a practical activity to extend this toolkit in various ways."
C16-2017,{A}nita: An Intelligent Text Adaptation Tool,2016,6,0,1,1,1610,gustavo paetzold,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"We introduce Anita: a flexible and intelligent Text Adaptation tool for web content that provides Text Simplification and Text Enhancement modules. Anita{'}s simplification module features a state-of-the-art system that adapts texts according to the needs of individual users, and its enhancement module allows the user to search for a word{'}s definitions, synonyms, translations, and visual cues through related images. These utilities are brought together in an easy-to-use interface of a freely available web browser extension."
C16-1069,Understanding the Lexical Simplification Needs of Non-Native Speakers of {E}nglish,2016,7,1,1,1,1610,gustavo paetzold,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We report three user studies in which the Lexical Simplification needs of non-native English speakers are investigated. Our analyses feature valuable new insight on the relationship between the non-natives{'} notion of complexity and various morphological, semantic and lexical word properties. Some of our findings contradict long-standing misconceptions about word simplicity. The data produced in our studies consists of 211,564 annotations made by 1,100 volunteers, which we hope will guide forthcoming research on Text Simplification for non-native speakers of English."
C16-1157,Collecting and Exploring Everyday Language for Predicting Psycholinguistic Properties of Words,2016,15,4,1,1,1610,gustavo paetzold,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Exploring language usage through frequency analysis in large corpora is a defining feature in most recent work in corpus and computational linguistics. From a psycholinguistic perspective, however, the corpora used in these contributions are often not representative of language usage: they are either domain-specific, limited in size, or extracted from unreliable sources. In an effort to address this limitation, we introduce SubIMDB, a corpus of everyday language spoken text we created which contains over 225 million words. The corpus was extracted from 38,102 subtitles of family, comedy and children movies and series, and is the first sizeable structured corpus of subtitles made available. Our experiments show that word frequency norms extracted from this corpus are more effective than those from well-known norms such as Kucera-Francis, HAL and SUBTLEXus in predicting various psycholinguistic properties of words, such as lexical decision times, familiarity, age of acquisition and simplicity. We also provide evidence that contradict the long-standing assumption that the ideal size for a corpus can be determined solely based on how well its word frequencies correlate with lexical decision times."
W15-4940,{O}kapi+{Q}u{E}st: Translation Quality Estimation within Okapi,2015,0,0,1,1,1610,gustavo paetzold,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-3041,{SHEF}-{NN}: Translation Quality Estimation with Neural Networks,2015,16,10,3,0,695,kashif shah,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We describe our systems for Tasks 1 and 2 of the WMT15 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model to extract additional features for Task 1 (SHEFGP, SHEF-SVM), (ii) a continuous bagof-words model to produce word embeddings as features for Task 2 (SHEF-W2V) and (iii) a combination of features produced by QuEst and a feature produced with word embedding models (SHEFQuEst). Our systems outperform the baseline as well as many other submissions. The results are especially encouraging for Task 2, where our best performing system (SHEF-W2V) only uses features learned in an unsupervised fashion."
W15-1835,Using Positional Suffix Trees to Perform Agile Tree Kernel Calculation,2015,10,0,1,1,1610,gustavo paetzold,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"Tree kernels have been used as an efficient solution for many tasks, but are difficult to calculate. To address this problem, in this paper we introduce the Positional Suffix Trees: a novel data structure devised to store tree structures, as well as the MFTK and EFTK algorithms, which use them to estimate Subtree and Subspace Tree Kernels. Results show that the Positional Suffix Tree can store large amounts of trees in a scalable fashion, and that our algorithms are up to 22 times faster than the state-ofthe-art approach."
P15-4015,{LEX}enstein: A Framework for Lexical Simplification,2015,17,10,1,1,1610,gustavo paetzold,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"Lexical Simplification consists in replacing complex words in a text with simpler alternatives. We introduce LEXenstein, the first open source framework for Lexical Simplification. It covers all major stages of the process and allows for easy benchmarking of various approaches. We test the toolxe2x80x99s performance and report comparisons on different datasets against the state of the art approaches. The results show that combining the novel Substitution Selection and Substitution Ranking approaches introduced in LEXenstein is the most effective approach to Lexical Simplification."
P15-4020,Multi-level Translation Quality Prediction with {Q}u{E}st++,2015,12,41,2,0,2509,lucia specia,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"This paper presents QUEST , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). QUEST allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST achieves state-of-the-art performance."
N15-2002,Reliable Lexical Simplification for Non-Native Speakers,2015,31,9,1,1,1610,gustavo paetzold,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Lexical Simplification is the task of modifying the lexical content of complex sentences in order to make them simpler. Due to the lack of reliable resources available for the task, most existing approaches have difficulties producing simplifications which are grammatical and that preserve the meaning of the original text. In order to improve on the state-of-the-art of this task, we propose user studies with nonnative speakers, which will result in new, sizeable datasets, as well as novel ways of performing Lexical Simplification. The results of our first experiments show that new types of classifiers, along with the use of additional resources such as spoken text language models, produce the state-of-the-art results for the Lexical Simplification task of SemEval-2012."
2015.eamt-1.41,{O}kapi+{Q}u{E}st: Translation Quality Estimation within Okapi,2015,0,0,1,1,1610,gustavo paetzold,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W13-4813,Text Simplification as Tree Transduction,2013,18,12,1,1,1610,gustavo paetzold,Proceedings of the 9th {B}razilian Symposium in Information and Human Language Technology,0,"Lexical and syntactic simplification aim to make texts more accessible to certain audiences. Syntactic simplification uses either hand-crafted linguis- tic rules for deep syntactic transformations, or machine learning techniques to model simpler transformations. Lexical simplification performs a lookup for synonyms followed by context and/or frequency-based models. In this paper we investigate modelling both syntactic and lexical simplification through the learning of general tree transduction rules. Experiments with the Simple En- glish Wikipedia corpus show promising results but highlight the need for clever filtering strategies to remove noisy transformations."
