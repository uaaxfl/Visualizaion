2020.wnut-1.80,N19-1423,0,0.127138,"i-Task binary classification problem. As before, the same text span may fill different Event Slots, pertaining to different event mentions. Because event extraction in this challenge is based on a mapping operation from tweet text spans to Event Slots, we contemplated methods of representing these spans that could take into account more than deep contextual information. We hypothesized that the representation of the tweet text spans should be specific to each of the Event Slot recognition tasks depicted in Figure 1(b). While contextual span representations based on the widely used BERT model (Devlin et al., 2019) have been successful in many applications, e.g. end-to-end relation extraction (Eberts and Ulges, 2020) or coreference resolution (Joshi et al., 2020), we believe that representing tweet text spans could be improved when considering modern Hopfield Networks in which the update rule is the attention mechanism used in the transformer and BERT, an idea recently advocated in Ramsauer et al. (2020). These ideas led the design of our Multi-Task, Event-specific Ex531 Figure 2: The Multi-Task Event-specific Extraction system using BERT and Hopfield Pooling (MT-EsE.BHP). traction system using BERT and"
2020.wnut-1.80,2020.tacl-1.5,0,0.0235116,"t extraction in this challenge is based on a mapping operation from tweet text spans to Event Slots, we contemplated methods of representing these spans that could take into account more than deep contextual information. We hypothesized that the representation of the tweet text spans should be specific to each of the Event Slot recognition tasks depicted in Figure 1(b). While contextual span representations based on the widely used BERT model (Devlin et al., 2019) have been successful in many applications, e.g. end-to-end relation extraction (Eberts and Ulges, 2020) or coreference resolution (Joshi et al., 2020), we believe that representing tweet text spans could be improved when considering modern Hopfield Networks in which the update rule is the attention mechanism used in the transformer and BERT, an idea recently advocated in Ramsauer et al. (2020). These ideas led the design of our Multi-Task, Event-specific Ex531 Figure 2: The Multi-Task Event-specific Extraction system using BERT and Hopfield Pooling (MT-EsE.BHP). traction system using BERT and Hopfield Pooling (MT-EsE.BHP). 2 Related Work Zong et al. (2020) introduced the collection of tweets used for training in this shared-task along with"
2020.wnut-1.80,P18-1093,0,0.0160597,"a typo with “1st respknders” which likely causes the language model to miss the contextual clues that the author of the tweet has some relation with first responders and nurses who have contracted and died from complications due to COVID-19. Example 4 and 5 demonstrate one of the primary reasons we believe our system performs poorly on the C URE Event Type, and specifically the who cure Event Slot. We see sarcasm demonstrated in Example 4, where the author of the tweet is sarcastically stating that Biden’s recommendation of flying a flag at half-staff is an actual potential cure for COVID-19. Tay et al. (2018) discuss the ”sophisticated speech act” of sarcasm in the context of social communities such as Twitter and Reddit, and they note that sarcasm can severely disrupt opinion mining systems. Sarcasm can be very difficult to identify (Joshi et al., 2017), and many of the tweets discussing COVID-19 cures contain sarcastic content which can be difficult to distinguish. Our system mistakenly identified Biden as who cure, while the annotator picked up on the sarcasm and did not annotate this instance. Example 5 demonstrates a debatable instance of sarcasm on the other side, where the annotation states"
A00-1020,W97-1306,0,0.034554,"euristics for coreference. Our experiments show that SWIZZLE outperformed COCKTAILon both English and Romanian test documents. The rest of the paper is organized as follows. Section 2 presents COCKTAIL,a monolingnai coreference resolution system used separately on both the English and Romanian texts. Section 3 details the data-driven approach used in SWIZZLEand presents some of its resources. Section 4 reports and discusses the experimental results. Section 5 summarizes the 2The name of COCKTAIL is a pun on CogNIAC because COCKTAILcombines a larger number of heuristics than those reported in (Baldwin, 1997). SWIZZLE,moreover, adds new heuristics, discovered from the bilingual aligned corpus. conclusions. 2 COCKTAIL Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIAC (Ba"
A00-1020,P99-1048,0,0.0242037,"Missing"
A00-1020,H92-1022,0,0.0281983,"9-0143. Table 3 also shows the original MUC coreference SGML annotations. Whenever present, the REF tag indicates the ID of the antecedent, whereas the MIN tag indicates the minimal reference expression. 3.2 Lexical Resources The multilingual coreference resolution m e t h o d implemented in SWIZZLE incorporates the heuristics derived from COKCTAIL&apos;s monolingual coreference resolution processing in both languages. To this end, COCKTAIL required both sets of texts to be tagged for part-of-speech and to recognize the noun phrases. The English texts were parsed with Brill&apos;s part-ofspeech tagger (Brill 1992) and the noun phrases were identified by the g r a m m a r rules implemented in the phrasal parser of FASTUS (Appelt et al., 1993). Corresponding resources are not available in Romanian. To minimize COCKTAIL&apos;s configuration for processing Romanian texts, we implemented a R o m a n i a n part-of-speech rule-based tagger t h a t used the same 146 Economic adviser Gene Sperling described &lt;COREF ID=&quot;29&quot; T Y P E = &apos; I D E N T &quot; R E F - &quot; 3 0 &quot; &gt; i t &lt; / C O R E F &gt; as &quot;a true full-court press&quot; to pass &lt;COREF ID=&quot;31&quot; TYPE=&quot;IDENT&quot; REF=&quot;26&quot; MIN=&apos;bilr&apos; &gt; t h e &lt;COREF ID=&quot;32&quot; TYPE=&quot;IDENT&quot; REF-----&quot;10&quot; M"
A00-1020,W99-0611,0,0.0884387,"Missing"
A00-1020,W98-1119,0,0.105928,"Missing"
A00-1020,A94-1006,0,0.0164238,"method for coreference resolution as implemented in the SWIZZLE system. The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts, outperformed coreference resolution in each of the individual languages. 1 Introduction The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing. Most of the research has focused on bilingual terminology identification, either as parallel multiwords forms (e.g. the ChampoUion system (Smadja et a1.1996)), technical terminology (e.g. the Termight system (Dagan and Church, 1994) or broad-coverage translation lexicons (e.g. the SABLE system (Resnik and Melamed, 1997)). In addition, the Multilingual Entity Task (MET) from the TIPSTER program 1 (http://www-nlpir.nist.gov/relatedprojeets/tipster/met.htm) challenged the participants in the Message Understanding Conference (MUC) to extract named entities across several foreign language corpora, such as Chinese, Japanese and Spanish. In this paper we present a new application of aligned multilinguai texts. Since coreference resolution is a pervasive discourse phenomenon causing performance impediments in current IE systems,"
A00-1020,W99-0104,1,0.822126,"ts into Romanian using native speakers. The training data set for Romanian coreference used, wherever possible, the same coreference identifiers as the English data and incorporated additional tags as needed. Our claim is that by adding the wealth of coreferential features provided by multilingual data, new powerful heuristics for coreference resolution can be developed that outperform monolingual coreference resolution systems. For both languages, we resolved coreference by using SWIZZLE, our implementation of a bilingual coreference resolver. SWIZZLEis a multilingual enhancement of COCKTAIL(Harabagiu and Maiorano, 1999), a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information2. When COCKTAIL was applied separately on the English and the Romanian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuristics for coreference. Our experiments show that SWIZZLE outperformed COCKTAILon both English and Romanian test documents. The rest of the paper is organized as follows. Section 2 presents COCKTAIL,a monoling"
A00-1020,W97-0319,0,0.0506879,"Missing"
A00-1020,J94-4002,0,0.091152,"Missing"
A00-1020,W97-1307,0,0.0599282,"Missing"
A00-1020,J96-1001,0,0.0673932,"Missing"
A00-1020,A97-1050,0,\N,Missing
A00-1020,C96-1021,0,\N,Missing
A00-1020,C88-1021,0,\N,Missing
A00-1020,P98-2143,0,\N,Missing
A00-1020,C98-2138,0,\N,Missing
bejan-harabagiu-2008-linguistic,P98-1013,0,\N,Missing
bejan-harabagiu-2008-linguistic,C98-1013,0,\N,Missing
C00-1043,P96-1025,0,0.266005,"mantic and logic formulae quite simple. In addition, the acquisition of question taxonomies is alleviated by machine learning techniques inspired from bootstrapping methods that learn linguistic patterns and semantic dictionaries for IE (cf. (Rilo and Jones, 1999)). World knowledge axioms can also be easily derived by processing the gloss de nitions of WordNet (Fellbaum 1998). 3.1 Semantic and Logic Transformations Semantic Transformations Instead of producing only a phrasal parse for the question and answer, we make use of one of the new statistical parsers of large real-world text coverage (Collins, 1996). The parse trees produced by such a parser can be easily translated into a semantic representation that (1) comprises all the phrase heads and (2) captures their inter-relationships by anonymous links. Figure 2 illustrates both the parse tree and the associated semantic representation of a TREC-8 question. Question: Why did David Koresh ask the FBI for a word processor? Parse: SBARQ SQ VP PP WHADVP WRB NP VBD NNP Why did David NP NP NNP VB DT NNP IN Koresh ask the FBI DT NN for a NN word processor Semantic representation: ask REASON word David processor Koresh FBI Figure 2: Question semantic"
C00-1043,H86-1003,0,0.0159972,"f a question; (2) the question type; (3) the answer type; (4) the question focus and (5) the question keywords. By using over 1500 questions provided by Remedia, as well as other 2000 questions retrieved from FAQFinder, we have been able to learn classi cation rules and build a complex question taxonomy. NP processor NNP VB DT NNP IN Koresh ask the FBI DT NN for a NN word processor Figure 3: Parse tree traversal Reason Definition Date Instance Number Product Name Logical Transformations The logical formulae in which questions or answers are translated are inspired by the notation proposed in (Hobbs, 1986-1) and implemented in TACITUS (Hobbs, 1986-2). Based on the davidsonian treatment of action sentences, in which events are treated as individuals, every question and every answer are transformed in a rst-order predicate formula for which (1) verbs are mapped in predicates verb(e,x,y,z,...) with the convention that variable e represents the eventuality of that action or event to take place, whereas the other arguments (e.g. x, y, z, ...) represent the predicate arguments of the verb; (2) nouns are mapped into their lexicalized predicates; and, (3) modi ers Currency Name Person Organization Loc"
C02-1169,C00-1043,1,\N,Missing
C02-1169,P01-1037,1,\N,Missing
C02-1169,P98-1035,0,\N,Missing
C02-1169,C98-1035,0,\N,Missing
C02-1169,P96-1025,0,\N,Missing
C04-1084,J97-1003,0,\N,Missing
C04-1084,C00-2136,0,\N,Missing
C04-1084,C00-1072,0,\N,Missing
C04-1084,N01-1006,0,\N,Missing
C04-1084,harabagiu-etal-2002-multidocument,1,\N,Missing
C04-1100,J97-1003,0,\N,Missing
C04-1100,C02-1156,1,\N,Missing
C04-1100,N03-1022,1,\N,Missing
C04-1100,C04-1084,1,\N,Missing
C04-1100,C00-1072,0,\N,Missing
C04-1100,P02-1005,1,\N,Missing
C04-1100,P96-1025,0,\N,Missing
C04-1100,P98-1013,0,\N,Missing
C04-1100,C98-1013,0,\N,Missing
C04-1100,P03-1002,1,\N,Missing
C04-1100,J02-3001,0,\N,Missing
C04-1100,P02-1031,0,\N,Missing
D11-1048,P04-1053,0,0.438126,"erence and parameter estimation of our method. Section 5 details our experiments, Section 6 discusses our findings. Section 7 summarizes the conclusions. 2 Related Work Previous methods for unsupervised relation discovery have also relied on clustering techniques. One technique uses the context of entity arguments to cluster, while another is to perform a postprocessing step to cluster relations found using an existing relation extraction system. The approaches most similar to ours have taken features from the context of pairs of entities and used those features to form a clustering space. In Hasegawa et al. (2004), those features are tokens found within a context window of the entity pair. Distance between entity pairs is then computed using cosine similarity. In another approach, Rosenfeld and Feldman (2007) use hierarchical agglomerative clustering along with features based on token patterns seen in the context, again compared by cosine similarity. Other approaches to unsupervised relation discovery have relied on a two-step process where a number of relations are extracted, usually from a predicate-argument structure. Then similar relations are clustered together since synonymous predicates should b"
D11-1048,P06-1015,0,0.0141499,"(e.g. “ac520 quire” and “purchase”). Yates (2009) considers the output from an open information extraction system (Yates et al., 2007) and clusters predicates and arguments using string similarity and a combination of constraints. Syed and Viegas (2010) also perform a clustering on the output of an existing relation extraction system by considering the number of times two relations share the same exact arguments. Similar relations are expected to have the same pairs of arguments (e.g. “Ford produces cars” and “Ford manufactures cars”). These approaches and others (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006) rely on an assumption that relations are context-independent, such as when a person is born, or the capital of a nation. Our method will discover relations that can depend on the context as well. For instance, “penicillin” may be causally related to “allergic reaction” in one patient’s medical record but not in another. The relation between the two entities is not globally constant and should be considered only within the scope of one patient’s records. Additionally, these two-step approaches tend to rely on predicate-argument structures such as subject-verb-object triples to detect arbitrary"
D11-1048,N04-1041,0,0.0237885,"del. 6 Discussion The relation and argument clusters determined by the RDM provide a better unsupervised relation discovery method than the baselines. The RDM does this using no knowledge about syntax or semantics outside of that used to determine concepts. The analysis shows that words highly indicative of relations are detected and clustered automatically, without the need for prior annotation of relations or even the choice of a predetermined set of relation types. The discovered relations can be interpreted by a human or labeled automatically using a technique such as the one presented in Pantel and Ravichandran (2004). The fact that the discovered relations and argument classes align well with those chosen by annotators on the same data justify our assumptions about relations being present and discoverable by the way they are expressed in text. Table 1 shows that the model does not perform as well when many of the pairs of entities do not have a relation, but it still performs better than the baselines. While the RDM relies in large part on trigger words for making clustering decisions it is also capable of including examples which do not contain any contextual words between the arguments. In addition to m"
D11-1048,W10-0913,0,0.0179227,"based on token patterns seen in the context, again compared by cosine similarity. Other approaches to unsupervised relation discovery have relied on a two-step process where a number of relations are extracted, usually from a predicate-argument structure. Then similar relations are clustered together since synonymous predicates should be considered the same relation (e.g. “ac520 quire” and “purchase”). Yates (2009) considers the output from an open information extraction system (Yates et al., 2007) and clusters predicates and arguments using string similarity and a combination of constraints. Syed and Viegas (2010) also perform a clustering on the output of an existing relation extraction system by considering the number of times two relations share the same exact arguments. Similar relations are expected to have the same pairs of arguments (e.g. “Ford produces cars” and “Ford manufactures cars”). These approaches and others (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006) rely on an assumption that relations are context-independent, such as when a person is born, or the capital of a nation. Our method will discover relations that can depend on the context as well. For instance, “penicillin"
D11-1048,N07-4013,0,0.0138655,"ty. In another approach, Rosenfeld and Feldman (2007) use hierarchical agglomerative clustering along with features based on token patterns seen in the context, again compared by cosine similarity. Other approaches to unsupervised relation discovery have relied on a two-step process where a number of relations are extracted, usually from a predicate-argument structure. Then similar relations are clustered together since synonymous predicates should be considered the same relation (e.g. “ac520 quire” and “purchase”). Yates (2009) considers the output from an open information extraction system (Yates et al., 2007) and clusters predicates and arguments using string similarity and a combination of constraints. Syed and Viegas (2010) also perform a clustering on the output of an existing relation extraction system by considering the number of times two relations share the same exact arguments. Similar relations are expected to have the same pairs of arguments (e.g. “Ford produces cars” and “Ford manufactures cars”). These approaches and others (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006) rely on an assumption that relations are context-independent, such as when a person is born, or the ca"
D11-1048,N07-1016,0,\N,Missing
D11-1091,E09-1013,0,0.0265757,"his allows separate induced predicates to each select a separate argument class. Consider the verb fire, which has at least two distinct common senses: (1) to shoot or propel an object (e.g., to fire a gun), and (2) to lay someone off (e.g., to fire an employee). The first sense selects a weapon (e.g., gun, bullet, rocket), while the second sense selects a person (e.g., employee, coach, apprentice). Specifically, we employ tiered clustering (Reisinger and Mooney, 2010) using the words in the predicate’s context. Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. Tiered clustering has several advantages over other discrete clustering approaches. First, tiered clustering learns a background word distribution in addition to the clusters. This reduces the impact that words common to most senses have on the clustering process and allow clusters to form around only the most salient words. Second, tiered clustering Cluster 1 (18,391) shots gun Israeli missiles rockets officers soldiers rounds bullets weapons Cluster 2 (16,651) ball puck hired owner shots coaches net circle Johnson Williams Clu"
D11-1091,de-marneffe-etal-2006-generating,0,0.00843372,"Missing"
D11-1091,J03-2004,0,0.22375,", 2010; O an unsupervised model for learning selectional restrictions. The model assumes that (1) arguments have a single selected class exemplified by the selectional restriction, and (2) the selected class can be inferred from the data, in part by modeling how coercive each predicate is. The model is capable of operating with both ambiguous and disambiguated predicates, producing superior results for predicates that have been disambiguated. The selectional restrictions and coercions detected by the model reported in this paper can be used to enhance the logical metonymy approach reported in Lapata and Lascarides (2003). The experimental results show a significant improvement in the ranking of interpretations. 980 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 980–990, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 details unsupervised models that inform detection of metonymies. Section 4 outlines a method for disambiguating ambiguous predicates. Section 5 describes the enhanced interpretation of logical metonymies when convent"
D11-1091,P10-1045,0,0.0243169,"Missing"
D11-1091,D10-1114,0,0.267989,"l, which we denote cLDA, is: For each argument class k = 1..K: 1. Choose φk ∼ Dirichlet(β) For each unique predicate v = 1..V : 2. Choose sv ∼ Uniform(1,K) 3. Choose θv ∼ Dirichlet(α)2 4. Choose τ v ∼ Beta(γ0 , γ1 ) For every argument i = 1..nv : 5. Choose cvi ∼ Multinomial(θv ) 6. Choose xvi ∼ Bernoulli(τ v ) v 7. If xvi = 1, Choose avi ∼ Multinomial(φci ) v Else Choose avi ∼ Multinomial(φs ) for each argument i by cvi , where xvi chooses between the selected and coerced class. The variable xvi is similar to switching variables in other graphical models such as Chemudugunta et al. (2007) and Reisinger and Mooney (2010), where switching variables are used to choose between a background distribution and a document-specific distribution. In this case, the switching variable chooses between a specific class and a predicate-specific distribution. The graphical model for cLDA is shown in Figure 1(b). Note that cLDA is virtually equivalent to LDA when τ v is 1 and γ1 is small because the selected class will be ignored. In this way, highly coercive predicates have less of an impact on the argument clustering because they are more reliant on the multinomial θ. We use Gibbs sampling to perform model inference and col"
D11-1091,P10-1044,0,0.0406414,"Missing"
D11-1091,P99-1014,0,0.0287693,"ible to providing interpretations that are themselves logical metonymies (e.g., finish book). In this paper, we propose an enhancement to resolving logical metonymies by ruling out event-invoking predicates in order to provide more semantically valid interpretations. Recently, the resolution of several linguistic problems has benefited from Latent Dirichlet Alloca´ S´eaghdha tion (LDA) (Blei et al., 2003) models. O (2010) examines several selectional preference models based on LDA in predicting human judgements on predicate-argument plausibility. Both LDA and an extension, ROOTH-LDA (based on Rooth et al. (1999)), perform well at predicting plausibility on unseen predicate-argument pairs. Inspired by these results, we propose to extend selectional preference models in order to learn selectional restrictions. Alternatively, unsupervised algorithms exist that both induce semantic classes (Rooth et al., 1999; Lin and Pantel, 2001) and cluster predicates by their 981 selectional restrictions (Rumshisky et al., 2007) but none of these provide a sufficient framework for determining if a specific argument violates its predicate’s selectional restriction. 3 Unsupervised Learning of Selectional Restrictions I"
D11-1091,P09-3001,0,0.193134,"edicates. Section 5 describes the enhanced interpretation of logical metonymies when conventional constraints are known. Section 6 outlines our implementation and experimental design. Section 7 presents our experimental results in three broad tasks: (i) semantic class induction, (ii) coercion detection, and (iii) logical metonymy interpretation. Section 8 summarizes the conclusions. 2 Previous Work Lapata and Lascarides (2003) propose a probabilistic ranking model for logical metonymies. They estimate these probabilities using co-occurrence frequencies of predicate-argument pairs in a corpus. Shutova (2009) extends this approach to provide sense-disambiguated interpretations from WordNet (Fellbaum, 1998) by using the alternative interpretations to disambiguate polysemous words. Shutova and Teufel (2009) extend this approach further by clustering these sense-disambiguated interpretations into distinct groups of meaning (e.g., {read, browse, look through} and {write, produce, work on} for “enjoy book”). Not only do these approaches assume logical metonymies have already been identified, but they are susceptible to providing interpretations that are themselves logical metonymies (e.g., finish book)"
D11-1091,M95-1005,0,0.144938,"WordNet classes: artifact, person, plant, animal, location, and food. We consider each of these a cluster and compare them to clusters composed of the top ten non-polysemous words (according to WordNet) in each of the classes generated by both the baseline (LDA) and our model (cLDA). Words not in both sets of clusters are removed. The result of this evaluation, compared with six clustering metrics, is shown in Table 2. For descriptions of NMI, Rand, and cluster F-measure, see Manning et al. (2008); for the B3 metrics (Cluster and Element), see Bagga and Baldwin (1998); for the MUC metric, see Vilain et al. (1995). Each metric has different strengths and biases in regards to the number and distribution of clusters, so all are provided to give a general picture of class induction performance. The best performing model on all metrics is cLDA with induced predicates using 10 classes. However, as the number of classes is increased and the granularity of the induced classes becomes more finegrained, LDA (predictably) outperforms cLDA on most metrics. This is consistent with our intuition induced predicates? # classes LDA C1 C1 cLDA C2 C3 10 74.4 80.6 75.4 67.8 N 25 78.7 81.2 75.9 70.8 50 80.5 80.9 78.9 67.4"
D11-1091,S10-1005,0,\N,Missing
harabagiu-bejan-2006-answer,H05-1088,0,\N,Missing
harabagiu-bejan-2006-answer,P04-1072,0,\N,Missing
harabagiu-bejan-2006-answer,P01-1037,1,\N,Missing
harabagiu-bejan-2006-answer,W02-1028,0,\N,Missing
harabagiu-bejan-2006-answer,W04-2403,1,\N,Missing
harabagiu-bejan-2006-answer,N04-1020,0,\N,Missing
harabagiu-etal-2002-multidocument,W98-1106,0,\N,Missing
harabagiu-etal-2002-multidocument,J98-3005,0,\N,Missing
harabagiu-etal-2002-multidocument,C00-2136,0,\N,Missing
harabagiu-etal-2002-multidocument,C00-1072,0,\N,Missing
harabagiu-etal-2002-multidocument,harabagiu-maiorano-2000-acquisition,1,\N,Missing
harabagiu-etal-2002-multidocument,P91-1032,0,\N,Missing
harabagiu-etal-2002-multidocument,H01-1065,0,\N,Missing
harabagiu-maiorano-2000-acquisition,A88-1032,0,\N,Missing
harabagiu-maiorano-2000-acquisition,P96-1025,0,\N,Missing
harabagiu-maiorano-2000-acquisition,M92-1024,0,\N,Missing
harabagiu-maiorano-2000-acquisition,P91-1032,0,\N,Missing
harabagiu-maiorano-2000-acquisition,H91-1033,0,\N,Missing
harabagiu-maiorano-2000-acquisition,W97-1002,0,\N,Missing
J01-2009,J97-1003,0,0.0979459,"Missing"
J01-2009,J99-3003,0,\N,Missing
J01-2009,P00-1038,0,\N,Missing
J01-2009,P94-1002,0,\N,Missing
J01-2009,W99-0609,0,\N,Missing
J14-2004,W06-0901,0,0.687124,"ovide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at the topic"
J14-2004,W99-0201,0,0.874475,"was used in order to provide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at"
J14-2004,P98-1013,0,0.240104,"Missing"
J14-2004,bejan-harabagiu-2008-linguistic,1,0.952789,"igned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across groups. However, the HDP mixture model is a nonparametric generalization of LDA th"
J14-2004,P10-1143,1,0.850275,"ecently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference resolution that uses fully unsupervised methods and is based on Bayesian models. Over the past years, Bayesian models have been extensively used for the purpose of solving similar problems or subproblems of the generic problem presented in the previous section. In 2003, Blei, Ng, and Jordan proposed a parametric approach, called latent Dirichlet allocation (LDA), for automatically learning probability distributions of words corresponding to a specific number of latent classes (or topics) from a large 314 Bejan and Harabagiu Unsupervised Event"
J14-2004,S07-1102,1,0.865938,"Missing"
J14-2004,D07-1109,0,0.0885913,"Missing"
J14-2004,W99-0611,0,0.156262,"ionale is that events are expressed in many more varied linguistic constructs. For example, event mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related info"
J14-2004,C10-1022,0,0.0528643,"Missing"
J14-2004,D10-1085,0,0.0473847,"Missing"
J14-2004,W09-3208,0,0.686481,"er justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at the topic- or document-coll"
J14-2004,P07-1033,0,0.0372395,"Missing"
J14-2004,P08-1118,0,0.0457904,"Missing"
J14-2004,D08-1069,0,0.02005,"Missing"
J14-2004,P07-1035,0,0.0803528,"Missing"
J14-2004,P08-2012,0,0.0242065,"lly annotated coreference chains) against the set of chains predicted by a coreference resolution system S. Here, a coreference link represents a pair of coreferential mentions whereas a coreference chain represents all the event mentions from the same cluster with coreference links between consecutive mentions. The MUC recall computes the number of common coreference links in T and S divided by the number of links in T , and the MUC precision computes the number of common links in T and S divided by the number of links in S. As was previously noted (Luo et al. 2004; Denis and Baldridge 2008; Finkel and Manning 2008), this metric favors the systems that group mentions into smaller number of clusters (or, in other words, systems that predict large coreference chains) and does not take into account single mention clusters. For instance, a system that groups all entity mentions into the same cluster achieves a MUC score that surpasses any published results of known systems developed for the task of entity coreference resolution. The B3 metric was designed to overcome some of the MUC metric’s shortcomings. This metric computes the recall and precision for each mention and then estimates the 334 Bejan and Hara"
J14-2004,P06-1085,0,0.017905,"Missing"
J14-2004,P07-1107,0,0.410312,"itself distributed according to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of mult"
J14-2004,D09-1120,0,0.0156449,"expressed in many more varied linguistic constructs. For example, event mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at whi"
J14-2004,W97-1311,0,0.930174,"Missing"
J14-2004,D12-1045,0,0.796896,"f entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. More recently, Lee et al. (2012) proposed an approach to jointly model event and entity coreference by allowing information from event coreference to help entity coreference, and the other way around. Their supervised method uses a high-precision entity resolution method based on a collection of deterministic models (called sieves) to produce both entity and event clusters that are optimally merged using linear regression. A similar technique that treated entity and event coreference resolution jointly was reported in He (2007) using narrative clinical data. Research that aimed at resolving only event coreference was initiat"
J14-2004,D07-1072,0,0.00982342,"prove the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of multiple features. For example, in HDP, each data point is represented only by its corresponding word. For this reason, we built new Bayesian models on top of already-existing models with the main goal of providing a more flexible framework for representing da"
J14-2004,W97-0204,0,0.090539,"Missing"
J14-2004,H05-1004,0,0.273715,"the same approach as described in Bagga and Baldwin (1999) by merging all the documents from the same topic into a meta-document and then scoring this document as performed for within-document evaluation. To compute the final results of our experiments, we averaged the results over five runs of the generative models. 7.2 Coreference Resolution Metrics Because there is no agreement on the best coreference resolution metric, we used four metrics for our evaluation: the link-based MUC metric (Vilain et al. 1995), the mentionbased B3 metric (Bagga and Baldwin 1998), the entity-based CEAF metric (Luo 2005), and the pairwise (PW) metric. These metrics report results in terms of recall (R), precision (P), and F-score (F) by comparing the true set of coreference chains T (i.e., the manually annotated coreference chains) against the set of chains predicted by a coreference resolution system S. Here, a coreference link represents a pair of coreferential mentions whereas a coreference chain represents all the event mentions from the same cluster with coreference links between consecutive mentions. The MUC recall computes the number of common coreference links in T and S divided by the number of links"
J14-2004,P04-1018,0,0.166655,"et of coreference chains T (i.e., the manually annotated coreference chains) against the set of chains predicted by a coreference resolution system S. Here, a coreference link represents a pair of coreferential mentions whereas a coreference chain represents all the event mentions from the same cluster with coreference links between consecutive mentions. The MUC recall computes the number of common coreference links in T and S divided by the number of links in T , and the MUC precision computes the number of common links in T and S divided by the number of links in S. As was previously noted (Luo et al. 2004; Denis and Baldridge 2008; Finkel and Manning 2008), this metric favors the systems that group mentions into smaller number of clusters (or, in other words, systems that predict large coreference chains) and does not take into account single mention clusters. For instance, a system that groups all entity mentions into the same cluster achieves a MUC score that surpasses any published results of known systems developed for the task of entity coreference resolution. The B3 metric was designed to overcome some of the MUC metric’s shortcomings. This metric computes the recall and precision for ea"
J14-2004,C04-1100,1,0.281239,"olving event coreference is an essential requirement for many natural language processing (NLP) applications. For instance, in topic detection and tracking, event coreference resolution is required in order to identify new seminal events in broadcast news that have not been mentioned before (Allan et al. 1998). In information extraction, event coreference information was used for filling predefined template structures from text documents (Humphreys, Gaizauskas, and Azzam 1997). In question answering, a novel method of mapping event structures was used in order to provide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In"
J14-2004,D08-1067,0,0.114577,"ding to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of multiple featu"
J14-2004,J05-1004,0,0.129968,"Missing"
J14-2004,D08-1068,0,0.275817,"ween the observed random variables corresponding to object features. Thus, instead of considering as features only the words that express the event mentions (which is the way an observable object is represented in the original HDP model), we devised an HDP extension that is also able to represent features such as location, time, and agent for each event mention. This extension was inspired from the fully generative Bayesian model proposed by Haghighi and Klein (2007). However, Haghighi and Klein’s model was strictly customized for the task of entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos (2008), whenever new features need to be considered in Haghighi and Klein’s model, the extension becomes a challenging task. Also, Daum´e III and Marcu (2005) performed 315 Computational Linguistics Volume 40, Number 2 related work in this direction by proposing a generative model for solving supervised clustering problems. As an alternative to the HDP model, an important extension of latent class models that are able to represent feature-rich objects is the Indian buffet process (IBP) model presented in Griffiths and Ghahramani (2005). The IBP model defines a distributi"
J14-2004,D10-1048,0,0.0280146,"mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of thei"
J14-2004,P11-1082,0,0.00929636,"redications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. More re"
J14-2004,P09-1070,0,0.054935,"Missing"
J14-2004,P09-1074,0,0.102347,"aried linguistic constructs. For example, event mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the e"
J14-2004,M95-1005,0,0.923016,"ECB data sets. For evaluating the cross-document coreference annotations from EventCorefBank, we adopted the same approach as described in Bagga and Baldwin (1999) by merging all the documents from the same topic into a meta-document and then scoring this document as performed for within-document evaluation. To compute the final results of our experiments, we averaged the results over five runs of the generative models. 7.2 Coreference Resolution Metrics Because there is no agreement on the best coreference resolution metric, we used four metrics for our evaluation: the link-based MUC metric (Vilain et al. 1995), the mentionbased B3 metric (Bagga and Baldwin 1998), the entity-based CEAF metric (Luo 2005), and the pairwise (PW) metric. These metrics report results in terms of recall (R), precision (P), and F-score (F) by comparing the true set of coreference chains T (i.e., the manually annotated coreference chains) against the set of chains predicted by a coreference resolution system S. Here, a coreference link represents a pair of coreferential mentions whereas a coreference chain represents all the event mentions from the same cluster with coreference links between consecutive mentions. The MUC re"
J14-2004,N10-1061,0,\N,Missing
J14-2004,H05-1049,0,\N,Missing
J14-2004,C98-1013,0,\N,Missing
L16-1732,akbik-michael-2014-weltmodell,0,0.0241698,"Missing"
L16-1732,P15-1034,0,0.0221417,"Missing"
L16-1732,S13-1035,0,0.0218951,"s many major news events, such as the 2008 Olympics, both United States presidential nominating conventions, the United States financial crisis, etc. Gordon and Swanson identified nearly one million English-language narrative blog posts discussing personal stories from this dataset, using a supervised neural network based on lexical features (Gordon and Swanson, 2009). A total of 937,994 blog posts were classified as containing narrative stories. 3.1.2. Google Books Inspired by (Akbik and Michael, 2014), we considered a secondary source of narrative information: the Syntactic N-Grams dataset (Goldberg and Orwant, 2013) which was extracted from 3.5 million digitized English Books available through Google Books (Michel et al., 2011). The dataset contains over 10 billion syntactic n-grams, which are rooted syntactic dependency tree fragments (noun phrases and verb phrases) and is the largest publicly available corpus of its kind. Each tree fragment is annotated with the dependency information, its head word, and the frequency with which it occurred. An example tree fragment is (n0 ) “when/WRB the/DT small/JJ boy/NN ate/VBD cookies/NN”. We converted each tree fragment in this dataset into a sentence by removing"
L16-1732,P15-1009,0,0.167502,"y of all positive edges in the knowledge graph and minimize the plausibility of negative edges which are not consistent with the knowledge graph. We construct so-called negative edges by randomly sampling edges from the knowledge graph and replacing the lexical relation with another, random lexical relation such that the new edge is not in the network. This allows optimal latent variables to be learned by comparing the margin (gap) in the geometric space between edges in the knowledge graph, and the negative edges which do not occur in the knowledge graph. Formally, we define the margin loss (Guo et al., 2015): L= X X max(0, γ + f (t+ ) − f (t− )) (4) t+ ∈SN t− ∈SN − where t+ = |s1 , r, s2 |∈ SN refers to each triple in the knowledge graph, t− ∈ SN − refers to artificially created negative triples, and γ, which indicates how much of a margin (or distance) should exist between positive triples encoded in the knowledge graph and the negative triples we randomly generated. As described in (Bordes et al., 2013; ?), we applied stochastic gradient descent to solve this minimization problem. 4626 4.3.5. Smoothing the Knowledge Embeddings The embedded knowledge graph obtained by Equation 3 relies on distri"
L16-1732,P08-1052,0,0.0651282,"encoding the plausibility of a lexicalized relation between a pair of concepts, e.g. P (q) = hc1 , r, c2 i, the knowledge encoding framework that we describe is able to capture salient information as knowledge embeddings without discarding the long-tail aspects of this knowledge. Moreover these knowledge embeddings are able to generalize the knowledge discerned from individual sentences by incorporating two notions of semantic smoothness: (1) functional similarity (Turney, 2012), the idea that two phrases are similar if they engage in similar lexical relations, and (2) behavioral similarity (Nakov and Hearst, 2008), the idea that two lexical relationships are similar if they operate on similar concepts. In this paper, we present a novel knowledge embedding framework that generates promising results both on common-sense knowledge and domain-specific knowledge by learning an optimal embedding for each concept and lexical relation according to (1) the functional similarity between pairs of concepts, (2) the behavioral similarity between pairs of lexical relations, and (3) the role of the semantic composition for each word occurring in each concept and lexical relation. The remainder of this paper is organi"
lacatusu-etal-2004-multi,C02-1134,0,\N,Missing
lacatusu-etal-2004-multi,C00-1072,0,\N,Missing
lacatusu-etal-2004-multi,W02-1022,0,\N,Missing
lacatusu-etal-2004-multi,N03-1020,0,\N,Missing
lacatusu-etal-2006-impact,C04-1084,1,\N,Missing
lacatusu-etal-2006-impact,C00-1072,0,\N,Missing
lacatusu-etal-2006-impact,N04-1019,0,\N,Missing
lacatusu-etal-2006-impact,N03-1003,0,\N,Missing
lacatusu-etal-2006-impact,W04-1013,0,\N,Missing
lacatusu-etal-2006-impact,P05-1026,1,\N,Missing
morarescu-harabagiu-2004-namenet,C96-1079,0,\N,Missing
N01-1008,M95-1005,0,\N,Missing
N01-1008,C96-1021,0,\N,Missing
N01-1008,W97-1306,0,\N,Missing
N01-1008,P88-1014,0,\N,Missing
N01-1008,J95-2003,0,\N,Missing
N01-1008,J94-4002,0,\N,Missing
N01-1008,P87-1022,0,\N,Missing
N01-1008,P99-1032,0,\N,Missing
N01-1008,P98-2143,0,\N,Missing
N01-1008,C98-2138,0,\N,Missing
N01-1008,W99-0611,0,\N,Missing
N01-1008,W97-1307,0,\N,Missing
N03-1022,P02-1005,1,0.505389,"Missing"
N03-1022,P01-1052,1,0.244805,"h in terms of high failure rate due to insufficient input axioms, as well as long processing time. Our solution is to integrate the prover into the QA system and rely on reasoning methods only to augment other previously implemented answer extraction techniques. 2 Integration of Logic Prover into a QA System The QA system includes traditional modules such as question processing, document retrieval, answer extraction, built in ontologies, as well as many tools such as syntactic parser, name entity recognizer, word sense disambiguation (Moldovan and Noviscki 2002), logic representation of text (Moldovan and Rus 2001) and others. The Logic Prover is integrated in this rich NLP environment and augments the QA system operation. As shown in Figure 1, the inputs to COGEX consist of logic representations of questions, potential answer paragraphs, world knowledge and lexical information. The term Answer Logic Form (ALF) refers to the candidate answers in logic form. Candidate answers returned by the Answer Extraction module are classified as open text due to the unpredictable nature of their grammatical structure. The term Question Logic Form (QLF) refers to the questions posed to the Question Answering system r"
N03-1022,C02-1167,1,\N,Missing
P01-1037,A00-1023,0,\N,Missing
P01-1037,C00-1043,1,\N,Missing
P01-1037,H94-1052,0,\N,Missing
P01-1037,P00-1071,1,\N,Missing
P01-1037,A00-1025,0,\N,Missing
P01-1037,P96-1025,0,\N,Missing
P01-1037,A00-1021,0,\N,Missing
P01-1037,P95-1037,0,\N,Missing
P01-1037,A00-1041,0,\N,Missing
P02-1005,A00-1023,0,\N,Missing
P02-1005,C00-1043,1,\N,Missing
P02-1005,N01-1005,0,\N,Missing
P02-1005,A00-1025,0,\N,Missing
P02-1005,W01-1201,0,\N,Missing
P02-1005,H01-1069,0,\N,Missing
P02-1005,A00-1041,0,\N,Missing
P03-1002,J95-4004,0,0.248294,"Missing"
P03-1002,P97-1003,0,0.0654598,"led for. Additionally, the argument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal. 2.2 The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument. Each task can be cast a separate classifier. For example, the result of the first classifier on the sentence illustrated in Figure 2 is the identification of the two NPs as arguments. The second classifier assigns the specific roles ARG1 and ARG0 given the predicate “assailed”. − PHRASE TYPE (pt): This feature indicates the syntactic type of the phrase labeled as a predicate argument, e.g. NP for A"
P03-1002,J02-3001,0,0.822141,"ecific. For example, when retrieving the argument structure for the verb-predicate assail with the sense ”to tear attack” from www.cis.upenn.edu/ cotton/cgibin/pblex fmt.cgi, we find Arg0:agent, Arg1:entity assailed and Arg2:assailed for. Additionally, the argument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal. 2.2 The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument. Each task can be cast a separate classifier. For example, the result of the first classifier on the sentence illustrated in Figure 2 is the identification of the two NPs as arguments."
P03-1002,P02-1031,0,0.815742,"or direct object or theme whereas Arg2 represents indirect object, benefactive or instrument, but mnemonics tend to be verb specific. For example, when retrieving the argument structure for the verb-predicate assail with the sense ”to tear attack” from www.cis.upenn.edu/ cotton/cgibin/pblex fmt.cgi, we find Arg0:agent, Arg1:entity assailed and Arg2:assailed for. Additionally, the argument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal. 2.2 The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument. Each task can be cast a separate classifier. For exampl"
P03-1002,C00-2136,0,\N,Missing
P03-1002,N01-1006,0,\N,Missing
P03-1002,P98-1013,0,\N,Missing
P03-1002,C98-1013,0,\N,Missing
P05-1026,C04-1084,1,0.885908,"gether to define a topic structure that will govern future interactions with the Q/A system. In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. 207 The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature. Later work by (Harabagiu, 2004) demonstrated that topic signatures can be further enhanced by discovering the most relevant relations that exist between pairs of concepts. However, both of these types of topic representations are limited by the fact that they require the identification of topic-relevant documents prior to the discovery of the topic signatures. In our experiments, we were only presented with a set of documents relevant to a particular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopi"
P05-1026,P94-1002,0,0.0328955,"ular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopic, we considered four different approaches: Approach 1: All documents in the CNS collection were initially clustered using K-Nearest Neighbor (KNN) clustering (Dudani, 1976). Each cluster that contained at least one keyword that described the sub-topic was deemed relevant to the topic. Approach 2: Since individual documents may contain discourse segments pertaining to different sub-topics, we first used TextTiling (Hearst, 1994) to automatically segment all of the documents in the CNS collection into individual text tiles. These individual discourse segments then served as input to the KNN clustering algorithm described in Approach 1. Approach 3: In this approach, relevant documents were discovered simultaneously with the discovery of topic signatures. First, we associated a binary seed relation  for each each  sub-topic  . (Seed relations were created both by hand and using the method presented in (Harabagiu, 2004).) Since seed relations are by definition relevant to a particular subtopic, they can be used to de"
P05-1026,W97-1307,0,0.0336162,"n relations were identified by patterns used for processing definition questions. Extraction relations are discovered by processing documents in order to identify three types of relations, including: (1) syntactic attachment relations (including subject-verb, object-verb, and verb-PP relations), (2) predicate-argument relations, and (3) salience-based relations that can be used to encode long-distance dependencies between topic-relevant concepts. (Salience-based relations are discovered using a technique first reported in (Harabagiu, 2004) which approximates a Centering Theory-style approach (Kameyama, 1997) to the resolution of coreference.) Subtopic: Egypt’s production of toxins and BW agents Topic Signature: produce − phosphorous trichloride (TOXIN) house − ORGANIZATION cultivate − non−pathogenic Bacilus Subtilis (TOXIN) produce − mycotoxins (TOXIN) acquire − FACILITY Subtopic: Egypt’s allies and partners Topic Signature: cooperate − COUNTRY provide − COUNTRY train − PERSON cultivate − COUNTRY supply − know−how supply − precursors Figure 3: Example of two topic signatures acquired for the scenario illustrated in Figure 2. We made the extraction relations associated with each topic signature mo"
P05-1026,C00-1072,0,0.143199,"esearch) and audience (i.e. the organization receiving the information), as well as a series of evidence conditions which specify how much verification information must be subject to before it can be accepted as fact. We assume the set of sub-topics mentioned in the general background and the scenario can be used together to define a topic structure that will govern future interactions with the Q/A system. In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. 207 The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature. Later work by (Harabagiu, 2004) demonstrated that topic signatures can be further enhanced by discovering the most relevant relations that exist between pairs of concepts. However, both of these types of topic representations are limited by the fact that they require the identification of topic-relevant documents pr"
P05-1026,C04-1100,1,0.30758,"Missing"
P05-1026,P03-1044,0,0.0112707,"Missing"
P05-1026,C00-2136,0,\N,Missing
P05-1026,P03-1002,1,\N,Missing
P06-1114,P01-1037,1,0.487528,"Missing"
P06-1114,P05-1026,1,0.71852,"walked away with the whole island. Table 1: Re-ranking of answers by Method 1. Method 2. Since AP is often a resourceintensive process for most Q/A systems, we expect that TE information can be used to limit the number of passages considered during AP. As illustrated in Method 2 in Figure 1, lists of passages retrieved by a PR module can either be ranked (or filtered) using TE information. Once ranking is complete, answer extraction takes place only on the set of entailed passages that the system considers likely to contain a correct answer to the user’s question. Method 3. In previous work (Harabagiu et al., 2005b), we have described techniques that can be used to automatically generate well-formed natural language questions from the text of paragraphs retrieved by a PR module. In our current system, sets of automatically-generated questions (AGQ) are created using a stand-alone AutoQUAB generation module, which assembles question-answer pairs (known as QUABs) from the top-ranked passages returned in response to a question. Table 2 lists some of the questions that this module has produced for the question Q2 : “How hot does the inside of an active volcano get?”. A2 AGQ1 AGQ2 AGQ3 AGQ4 3 The Textual En"
P06-1114,P02-1054,0,0.0636825,"Missing"
P06-1114,P02-1005,1,0.318771,"Missing"
P06-1114,N03-1003,0,0.0102201,"ICAL C HANGES: This feature is a vector representing the number of Morphological-Derivation relations found in each antonymy chain. 6 N UMBER OF N ODES WITH D EPENDENCIES: This feature is a vector indexing the number of nodes in each antonymy chain that contain dependency relations. 7 T RUTH -VALUE M ISMATCH: This is a boolean feature which fired when two aligned predicates differed in any truth value. 8 P OLARITY M ISMATCH: This is a boolean feature which fired when predicates were assigned opposite polarity values. 3.2 Paraphrase Acquisition Much recent work on automatic paraphrasing (Barzilay and Lee, 2003) has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora. Since sentence-level paraphrases are generally assumed to contain information about the same event, these approaches have generally assumed that all of the available paraphrases for a given sentence will include at least one pair of entities which can be used to extract sets of paraphrases from text. The TE system uses a similar approach to gather phrase-level alternations for each entailment pair. In our system, the two highest-confidence entity alignments retu"
P06-1114,W05-1209,0,0.0203658,"Missing"
P06-1114,N03-1022,1,0.536472,"cument collection, as a response to a question asked in natural language. In the quest for producing accurate answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and 1 http://www.pascal-network.org/Challenges/RTE 905 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 905–912, c Sydney, July 2006. 2006 Association for Computational Linguistics Question Keywords Question Processing Module (QP) Documents Ranked List of Paragraphs AUTO−QUAB Generation Passage Retrie"
P06-1114,N04-3012,0,0.0128035,"700 degrees the inside of an active volcano an active volcano Figure 3: Alignment Graph Aligned constituents are then used to extract sets of phrase-level alternations (or “paraphrases”) from the WWW that could be used to capture correspondences between texts longer than individual constituents. The top 8 candidate paraphrases for two of the aligned elements from Figure 3 are presented in Table 3. Finally, the Classification Module employs a 908 Alignment Classifier: (1) a set of statistical features (e.g. cosine similarity), (2) a set of lexicosemantic features (including WordNet Similarity (Pedersen et al., 2004), named entity class equality, and part-of-speech equality), and (3) a set of string-based features (such as Levenshtein edit distance and morphological stem equality). As in (Hickl et al., 2006), we used a twostep approach to obtain sufficient training data for the Alignment Classifier. First, humans were tasked with annotating a total of 10,000 alignment pairs (extracted from the 2006 PASCAL Development Set) as either positive or negative instances of alignment. These annotations were then used to train a hillclimber that was used to annotate a larger set of 450,000 alignment pairs selected"
P06-1114,P03-1003,0,0.0640828,"e answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and 1 http://www.pascal-network.org/Challenges/RTE 905 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 905–912, c Sydney, July 2006. 2006 Association for Computational Linguistics Question Keywords Question Processing Module (QP) Documents Ranked List of Paragraphs AUTO−QUAB Generation Passage Retrieval Module (PR) List of Questions Entailed Paragraphs List of Entailed Paragraphs Entailed Questions TEXTUAL EN"
P06-1114,P04-1073,0,0.0608344,"Missing"
P06-1114,W05-1208,0,0.00937333,"TE system. Table 4 presents results from TE’s linearand Maximum Entropy-based Alignment Classifiers on a sample of 1000 alignment pairs selected at random from the 2006 PASCAL Test Set. ALIGNMENT FEATURES: These three features are derived from the results of the lexical alignment classification. 1 L ONGEST C OMMON S TRING: This feature represents the longest contiguous string common to both texts. 2 U NALIGNED C HUNK: This feature represents the number of chunks in one text that are not aligned with a chunk from the other 3 L EXICAL E NTAILMENT P ROBABILITY: This feature is defined in (Glickman and Dagan, 2005). DEPENDENCY FEATURES: These four features are computed from the PropBank-style annotations assigned by the semantic parser. 1 E NTITY-A RG M ATCH: This is a boolean feature which fires when aligned entities were assigned the same argument role label. 2 E NTITY-N EAR -A RG M ATCH: This feature is collapsing the arguments Arg1 and Arg2 (as well as the ArgM subtypes) into single categories for the purpose of counting matches. 3 P REDICATE -A RG M ATCH: This boolean feature is flagged when at least two aligned arguments have the same role. 4 P REDICATE -N EAR -A RG M ATCH: This feature is"
P06-1114,W07-1401,0,\N,Missing
P06-4007,C04-1084,1,0.883397,"tive question generation component, please see (Harabagiu et al., 2005b). 2 This test was run on a machine with a Pentium 4 3.0 GHz processor with 2 GB of RAM. 3 Predictive Question-Answering First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 question-answering assumes that Q/A systems can use the set of documents relevant to a user’s query in order to generate sets of questions – known as predictive questions – that anticipate a user’s information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a user’s domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage. Generated questions are then assembled into question-answer pairs – known as QUABs – with the original passage serving as the question’s “answer”, and are then returned to the user. For example, two of the predictive question-answer pairs generate"
P06-4007,C00-1072,0,0.120304,"ation on F ERRET’s predictive question generation component, please see (Harabagiu et al., 2005b). 2 This test was run on a machine with a Pentium 4 3.0 GHz processor with 2 GB of RAM. 3 Predictive Question-Answering First introduced in (Harabagiu et al., 2005b), a predictive questioning approach to automatic 26 question-answering assumes that Q/A systems can use the set of documents relevant to a user’s query in order to generate sets of questions – known as predictive questions – that anticipate a user’s information needs. Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a user’s domain of interest. Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage. Generated questions are then assembled into question-answer pairs – known as QUABs – with the original passage serving as the question’s “answer”, and are then returned to the user. For example, two of the predictive question"
P06-4007,P05-1026,1,\N,Missing
P10-1143,W99-0201,0,0.853011,"tity coreference resolution, solving event coreference has already proved its usefulness in various applications such as topic detection and tracking (Allan et al., 1998), information extraction (Humphreys et al., 1997), question answering (Narayanan and Harabagiu, 2004), textual entailment (Haghighi et al., 2005), and contradiction detection (de Marneffe et al., 2008). Previous approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys et al., 1997; Bagga and Baldwin, 1999; Ahn, 2006; Chen and Ji, 2009). In spite of being successful for a particular labeled corpus, these pairwise models are dependent on the domain or language that they are trained on. Moreover, since event coreference resolution is a complex task that involves exploring a rich set of linguistic features, annotating a large corpus with event coreference information for a new language or domain of interest requires a substantial amount of manual effort. Also, since these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at topic or document c"
P10-1143,P98-1013,0,0.170976,"Missing"
P10-1143,bejan-harabagiu-2008-linguistic,1,0.613547,"models for both within- and cross-document event coreference resolution. One important step in the creation process of this corpus consists in finding sets of related documents that describe the same seminal event such that the annotation of coreferential event mentions across documents is possible. For this purpose, we selected from the GoogleNews archive7 various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natural disaster, etc. The entire annotation process for creating the ECB resource is described in (Bejan and Harabagiu, 2008). Table 1 lists several basic statistics extracted from these two corpora. Evaluation For a more realistic approach, we not only trained the models on the manually annotated event mentions (i.e., true mentions), but also on all the possible mentions encoded in the two datasets. To extract all event mentions, we ran the event identifier described in (Bejan, 2007). The mentions extracted by this system (i.e., system men6 7 ECB is available at http://www.hlt.utdallas.edu/∼ady. http://news.google.com/ Number of topics Number of documents Number of within-topic events Number of cross-document event"
P10-1143,S07-1102,1,0.893951,"Missing"
P10-1143,W09-3208,0,0.499747,"event coreference has already proved its usefulness in various applications such as topic detection and tracking (Allan et al., 1998), information extraction (Humphreys et al., 1997), question answering (Narayanan and Harabagiu, 2004), textual entailment (Haghighi et al., 2005), and contradiction detection (de Marneffe et al., 2008). Previous approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys et al., 1997; Bagga and Baldwin, 1999; Ahn, 2006; Chen and Ji, 2009). In spite of being successful for a particular labeled corpus, these pairwise models are dependent on the domain or language that they are trained on. Moreover, since event coreference resolution is a complex task that involves exploring a rich set of linguistic features, annotating a large corpus with event coreference information for a new language or domain of interest requires a substantial amount of manual effort. Also, since these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at topic or document collection level. To address the"
P10-1143,P08-1118,0,0.0119036,"Missing"
P10-1143,P07-1107,0,0.0129535,"presented by a finite vocabulary of feature values, f v. Thus, we can represent the observable properties of an event mention as a vector of L feature type – feature value pairs h(FT1 : f v1i), . . . , (FTL : f vLi)i, where each feature value index i ranges in the feature value space associated with a feature type. 3.1 A Finite Feature Model We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L. Our HDP extension is also inspired from the Bayesian model proposed by Haghighi and Klein (2007). However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008). In the HDP model, a Dirichlet process (DP) (Ferguson, 1973) is associated with each document, and each mixture component (i.e., event) is shared across documents. To describe its extension, we consider Z the set of indicator random variables for indices of events, φz the set of parameters associated with an event z, φ a notation for all model parameters, and X a notation"
P10-1143,H05-1049,0,0.0115962,"ce shows significant improvements of the models when compared against two baselines for this task. 1 Introduction The event coreference task consists of finding clusters of event mentions that refer to the same event. Although it has not been extensively studied in comparison with the related problem of entity coreference resolution, solving event coreference has already proved its usefulness in various applications such as topic detection and tracking (Allan et al., 1998), information extraction (Humphreys et al., 1997), question answering (Narayanan and Harabagiu, 2004), textual entailment (Haghighi et al., 2005), and contradiction detection (de Marneffe et al., 2008). Previous approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys et al., 1997; Bagga and Baldwin, 1999; Ahn, 2006; Chen and Ji, 2009). In spite of being successful for a particular labeled corpus, these pairwise models are dependent on the domain or language that they are trained on. Moreover, since event coreference resolution is a complex task that involves exploring a rich set of linguist"
P10-1143,W97-1311,0,0.908574,"al outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task. 1 Introduction The event coreference task consists of finding clusters of event mentions that refer to the same event. Although it has not been extensively studied in comparison with the related problem of entity coreference resolution, solving event coreference has already proved its usefulness in various applications such as topic detection and tracking (Allan et al., 1998), information extraction (Humphreys et al., 1997), question answering (Narayanan and Harabagiu, 2004), textual entailment (Haghighi et al., 2005), and contradiction detection (de Marneffe et al., 2008). Previous approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys et al., 1997; Bagga and Baldwin, 1999; Ahn, 2006; Chen and Ji, 2009). In spite of being successful for a particular labeled corpus, these pairwise models are dependent on the domain or language that they are trained on. Moreover, sin"
P10-1143,W97-0204,0,0.0822732,"SA 0, WSA 1). Feature Combinations (FC) We also explore various combinations of the features presented above. Examples include HW+HWC, HL+FR, FR+ARG1, LHL+RHL, etc. It is worth noting that there exist event mentions for which not all the features can be extracted. For example, the LHE and RHE features are missing for the first and last event mentions in a document, respectively. Also, many semantic roles can be absent for an event mention in a given context. 1 The reason for extracting this feature is given by the fact that, in general, frames are able to capture properties of generic events (Lowe et al., 1997). 1414 3 Nonparametric Bayesian Models As input for our models, we consider a collection of I documents, where each document i has Ji event mentions. For features, we make the distinction between feature types and feature values (e.g., POS is a feature type and has values such as NN and VB). Each event mention is characterized by L feature types, FT, and each feature type is represented by a finite vocabulary of feature values, f v. Thus, we can represent the observable properties of an event mention as a vector of L feature type – feature value pairs h(FT1 : f v1i), . . . , (FTL : f vLi)i, wh"
P10-1143,H05-1004,0,0.469582,"t events Number of within-document events Number of true mentions Number of system mentions Number of distinct feature values ACE ECB – 745 – – 4946 6553 45289 391798 43 482 339 208 1302 1744 21175 237197 Table 1: Statistics of the ACE and ECB corpora. tions) were able to cover all the true mentions from both datasets. As shown in Table 1, we extracted from ACE and ECB corpora 45289 and 21175 system mentions, respectively. We report results in terms of recall (R), precision (P), and F-score (F) by employing the mention -based B3 metric (Bagga and Baldwin, 1998), the entity -based CEAF metric (Luo, 2005), and the pairwise F1 (PW) metric. All the results are averaged over 5 runs of the generative models. In the evaluation process, we considered only the true mentions of the ACE test dataset, and the event mentions of the test sets derived from a 5fold cross validation scheme on the ECB dataset. For evaluating the cross-document coreference annotations, we adopted the same approach as described in (Bagga and Baldwin, 1999) by merging all the documents from the same topic into a meta-document and then scoring this document as performed for within-document evaluation. For both corpora, we conside"
P10-1143,C04-1100,1,0.791238,"ving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task. 1 Introduction The event coreference task consists of finding clusters of event mentions that refer to the same event. Although it has not been extensively studied in comparison with the related problem of entity coreference resolution, solving event coreference has already proved its usefulness in various applications such as topic detection and tracking (Allan et al., 1998), information extraction (Humphreys et al., 1997), question answering (Narayanan and Harabagiu, 2004), textual entailment (Haghighi et al., 2005), and contradiction detection (de Marneffe et al., 2008). Previous approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys et al., 1997; Bagga and Baldwin, 1999; Ahn, 2006; Chen and Ji, 2009). In spite of being successful for a particular labeled corpus, these pairwise models are dependent on the domain or language that they are trained on. Moreover, since event coreference resolution is a complex task th"
P10-1143,D08-1067,0,0.0256617,"i, where each feature value index i ranges in the feature value space associated with a feature type. 3.1 A Finite Feature Model We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L. Our HDP extension is also inspired from the Bayesian model proposed by Haghighi and Klein (2007). However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008). In the HDP model, a Dirichlet process (DP) (Ferguson, 1973) is associated with each document, and each mixture component (i.e., event) is shared across documents. To describe its extension, we consider Z the set of indicator random variables for indices of events, φz the set of parameters associated with an event z, φ a notation for all model parameters, and X a notation for all random variables that represent observable features.2 Given a document collection annotated with event mentions, the goal is to find the best assignment of event indices Z∗ , which maximize"
P10-1143,J05-1004,0,0.103081,"Missing"
P10-1143,D08-1068,0,0.0207577,"ach feature value index i ranges in the feature value space associated with a feature type. 3.1 A Finite Feature Model We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L. Our HDP extension is also inspired from the Bayesian model proposed by Haghighi and Klein (2007). However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008). In the HDP model, a Dirichlet process (DP) (Ferguson, 1973) is associated with each document, and each mixture component (i.e., event) is shared across documents. To describe its extension, we consider Z the set of indicator random variables for indices of events, φz the set of parameters associated with an event z, φ a notation for all model parameters, and X a notation for all random variables that represent observable features.2 Given a document collection annotated with event mentions, the goal is to find the best assignment of event indices Z∗ , which maximize the posterior probability"
P10-1143,C98-1013,0,\N,Missing
P10-1143,W06-0901,0,\N,Missing
P96-1051,H91-1077,0,0.0191496,"with noun3, and noun4, respectively. • noun1, A particular case is when noun1 (noun2) and noun3 (noun4) are synonyms. Definition 2: Two prepositional structures <: verb1 prep noun1 > and < verb2 prep noun2 > belong to the same class if one of the following conditions holds: description • verb1, and noun1 are h y p e r n y m / h y p o n y m of verb2, and noun2, respectively or In this paper, we address the problem of disambiguation and understanding prepositional attachment. The arguments of prepositional relations are automatically categorized into semantically equivalent classes of WordNet (Miller and Teibel, 1991) concepts. Then by applying inferential heuristics on each class, we establish semantic connections between arguments that explain the validity of that prepositional structure. The method uses information provided by WordNet, such as semantic relations and textual glosses. We have collected prepositional relations from the Wall Street Journal tagged articles of the PENN TREEBANK. Here, we focus on preposition of, the most frequently used preposition in the corpus. 2 Classes of prepositional and noun1 have a common hypern y m / h y p o n y m with verb2, and n o u n 2 , respectively. • verb1, A"
P96-1051,C94-2195,0,\N,Missing
roberts-etal-2010-linguistic,W04-2705,0,\N,Missing
roberts-etal-2010-linguistic,C04-1100,1,\N,Missing
roberts-etal-2010-linguistic,W04-3212,0,\N,Missing
roberts-etal-2010-linguistic,H05-1047,0,\N,Missing
roberts-etal-2010-linguistic,W09-2507,1,\N,Missing
roberts-etal-2010-linguistic,P03-1054,0,\N,Missing
roberts-etal-2010-linguistic,P98-1013,0,\N,Missing
roberts-etal-2010-linguistic,C98-1013,0,\N,Missing
roberts-etal-2010-linguistic,P03-1002,1,\N,Missing
roberts-etal-2010-linguistic,J02-3001,0,\N,Missing
roberts-etal-2010-linguistic,J05-1004,0,\N,Missing
roberts-etal-2010-linguistic,fillmore-etal-2002-framenet,0,\N,Missing
roberts-etal-2012-annotating,C08-2024,0,\N,Missing
roberts-etal-2012-annotating,D11-1027,0,\N,Missing
roberts-etal-2012-annotating,P86-1004,0,\N,Missing
roberts-etal-2012-annotating,H86-1011,0,\N,Missing
roberts-etal-2012-annotating,H05-1088,0,\N,Missing
roberts-etal-2012-annotating,J11-4005,0,\N,Missing
roberts-etal-2012-annotating,P09-1068,0,\N,Missing
roberts-etal-2012-annotating,P10-1143,1,\N,Missing
roberts-etal-2012-annotating,W09-4303,0,\N,Missing
roberts-etal-2012-annotating,mani-etal-2008-spatialml,0,\N,Missing
roberts-etal-2012-empatweet,W11-1514,0,\N,Missing
roberts-etal-2012-empatweet,W11-1709,0,\N,Missing
roberts-etal-2012-empatweet,pak-paroubek-2010-twitter,0,\N,Missing
S07-1101,de-marneffe-etal-2006-generating,0,0.0129766,"Missing"
S07-1101,S07-1003,0,0.0936055,", and had items confiscated from his home. Cinnamon oil is distilled from bark chips and used to alleviate stomach upsets. The port scanner is a utility to scan a system to get the status of the TCP. The granite benches are former windowsills from the Hearst Memorial Mining Building. The kitchen holds patient drinks and snacks. Table 1: Examples of semantic relations. of a lexical item through a combination of semantic structure and semantic content. 2 Semantic Tasks The two semantic tasks addressed in this paper are: Classification of Semantic Relations between Nominals (Task 4), defined in (Girju et al., 2007) and Metonymy Resolution (Task 8), defined in (Markert and Nissim, 2007). Please refer to these task description papers for more details. Both are cast as classification tasks: given an unlabeled instance, a system must label it according to one class of a set specific to each task. The training and testing datasets for the metonymy resolution task are annotated in an XML format. There are 1090 training and 842 testing instances for companies, and 941 training and 908 testing instances for locations. Each training instance corresponds to a context in which a single name is annotated with its r"
S07-1101,W02-1027,0,0.382324,"Missing"
S07-1101,W05-0635,0,\N,Missing
S07-1101,S07-1007,0,\N,Missing
S07-1101,W00-0207,0,\N,Missing
S10-1056,P07-1054,0,0.0130191,"te paper .322 tall tale .319 commendation .310 field theory .309 Given a set of seed concepts, we mine WordNet for other concepts that may be in the same semantic class. Clearly, this approach has both practical limitations (WordNet does not contain every possible concept) and linguistic limitations (concepts may belong to different semantic classes based on their context). However, given the often vague nature of semantic classes (is a building an A RTI FACT or a L OCATION ?), access to a weighted list of semantic class members can prove useful for arguments not seen in the train set. Using (Esuli and Sebastiani, 2007) as inspiration, we have implemented our own naive version of WordNet PageRank. They use sensedisambiguated glosses provided by eXtended WordNet (Harabagiu et al., 1999) to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values. For our task, however, hypernymy relations are more appropriate for determining a given synset’s membership in a semantic class. Hypernymy defines an I S -A relationship between the parent class (the hypernym) and one of its child classes (the hyponym). Furthermore, while Page"
S10-1056,W99-0501,1,0.662003,"arly, this approach has both practical limitations (WordNet does not contain every possible concept) and linguistic limitations (concepts may belong to different semantic classes based on their context). However, given the often vague nature of semantic classes (is a building an A RTI FACT or a L OCATION ?), access to a weighted list of semantic class members can prove useful for arguments not seen in the train set. Using (Esuli and Sebastiani, 2007) as inspiration, we have implemented our own naive version of WordNet PageRank. They use sensedisambiguated glosses provided by eXtended WordNet (Harabagiu et al., 1999) to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values. For our task, however, hypernymy relations are more appropriate for determining a given synset’s membership in a semantic class. Hypernymy defines an I S -A relationship between the parent class (the hypernym) and one of its child classes (the hyponym). Furthermore, while PageRank assumes directed edges (e.g., hyperlinks in a web page), we use undirected edges. In this way, if H YPERNYM O F (A, B), then A’s membership in a semantic class stre"
S10-1056,P05-1026,1,0.826311,"ic parse information from a large corpus to identify similar arguments by the predicates that select them. We show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach. 1 Introduction Argument coercion (a type of metonymy) occurs when the expected semantic class (relative to the a predicate) is substituted for an object of a different semantic class. Metonymy is a pervasive phenomenon in language and the interpretation of metonymic expressions can impact tasks from semantic parsing (Scheffczyk et al., 2006) to question answering (Harabagiu et al., 2005). A seminal example in metonymy from (Lakoff and Johnson, 1980) is: (1) The ham sandwich is waiting for his check. The A RG 1 for the predicate wait is typically an animate, but the “ham sandwich” is clearly not an animate. Rather, the argument is coerced to fulfill the predicate’s typing requirement. This coercion is allowed because an object that would normally fulfill the typing requirement (the customer) can be uniquely identified by an attribute (the ham sandwich he ordered). 2 PageRanking WordNet Hypernyms Our first approach assumes that semantic class members can be defined and acquired"
S10-1056,J05-1004,0,0.0444884,"of the approaches outlined in Sections 2 and 3 along with additional features commonly found in information extraction literature. All experiments were conducted using the SVMmulticlass support vector machine library4 . 4.1 WordNet PageRank We experimented with the output of our WordNet PageRank implementation along three separate dimensions: (1) which sense to use (since we did not incorporate a word sense disambiguation system), (2) whether to use the highest scoring se3 The notable exception to this, however, is arrive, where the data uses the destination argument. In the PropBank scheme (Palmer et al., 2005), this would correspond to the A RG 4, which usually signifies an end state. 4 http://svmlight.joachims.org/svm multiclass.html 4.4 Ablation Test We conducted an ablation test using combinations of five feature sets: (1) our WordNet PageR254 +WNSH +WNPR +GWPA +EVNT 96.1 WORD 89.2 94.2 95.0 95.6 EVNT 31.1 89.7 89.9 90.8 GWPA 89.6 90.8 91.0 WNPR 75.6 89.4 WNSH 89.0 when the predicate may be polysemous or unseen predicates may be encountered. Acknowledgments The authors would like to thank Bryan Rink for several insights during the course of this work. Table 3: Ablation test of feature sets showi"
S10-1056,W09-2414,0,0.0405876,"Missing"
S10-1056,J03-2004,0,\N,Missing
S10-1056,S10-1005,0,\N,Missing
S10-1057,S07-1085,0,0.199522,"extract the words and parts of speech for E1 and E2 , the words, parts of speech, and prefixes of length 5 for tokens between the nominals, and the words before and single word after E1 and E2 respectively. The words between the nominals can be strong indicators for the type of relation. For example the words into, produced, and caused are likely to occur in Entity-Destination, Product-Producer, and Cause-Effect relations, respectively. Using the prefixes of length 5 for the words between the nominals provides a kind of stemming (produced → produ, caused → cause). Inspired by a feature from (Beamer et al., 2007), we extract a coarse-grained part of speech sequence for the words between the nominals. This is accomplished by building a string using the first letter of each token’s Treebank POS tag. This feature is motivated by the fact that relations such as Member-Collection usually invoke prepositional phrases such as: of, in the, and of various. The corresponding POS sequences we extract are: “I”, “I D”, and “I J”. Finally, we also use the number of words between the nominals as a feature because relations such as Product-Producer and Entity-Origin often have no intervening tokens (e.g., organ build"
S10-1057,S10-1006,0,0.0252199,"Missing"
S10-1057,P05-3014,0,0.0616282,"ich 256 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 256–259, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the Stanford dependency parser3 for the simpler syntactic structure it produces. Our dependency features are based on paths in the dependency tree of length 1 and length 2. The paths encode the dependencies and words those dependencies attach to. To generalize the paths, some of the features replace verbs in the path with their top-level Levin class, as determined by running a word sense disambiguation system (Mihalcea and Csomai, 2005) followed by a lookup in VerbNet4 . One of the features for length 2 paths generalizes further by replacing all words with their location relative to the nominals, either BEFORE, BETWEEN, or AFTER. Consider example 117 from Table 1. The length 2 dependency path (feature depPathLen2VerbNet) neatly captures the fact that E1 is the subject of a verb falling into Levin class 27, and E2 is the direct object. Levin class 27 is the class of engender verbs, such as cause, spawn, and generate. This path is indicative of a Cause-Effect relation. Semantic parses such as ASSERT’s PropBank parse5 and LTH’s"
S10-1057,N07-4013,0,0.010077,"en in Table 1 in the row for NGrams. The neighbors for motion in the table show the difficulty this feature has with ambiguity, incorrectly picking up words similar to the sense meaning a proposal for action. 5 Pre-existing Relation Features For some examples the context and the individual nominal affiliations provide little help in determining the semantic relation, such as example 5884 from before (i.e., corn flour). These examples require knowledge of the interaction between the nominals and we cannot rely solely on determining the role of one nominal or the other. We turned to TextRunner (Yates et al., 2007) as a large source of background knowledge about pre-existing relations between nominals. TextRunner is a queryable database of NOUN - VERB - NOUN triples extracted from a large corpus of webpages. For example, the phrases retrieved from TextRunner for “corn flour” include: “is ground into”, “to make”, “to obtain”, and “makes”. Querying in the reverse direction, for “flour corn” returns phrases such as: “contain”, “filled with”, “comprises”, and “is made from”. We use the top ten phrases for the <E2 >” query results, and also for “<E1 > the “<E2 > <E1 >” results, forming two features. In addit"
S12-1055,S12-1047,0,0.0462005,"rju et al., 2009). However, most previous work has considered membership assignment for a semantic relation as a binary property. In this paper we discuss an approach which assigns a degree of membership to a pair of concepts for a given relation. For example, for the semantic relation CLASS - INCLUSION (Taxonomic), the concept pairs weapon:spear and bird:robin are stronger members of the relationship than hair:brown, because brown may describe many things other than hair, and brown is also used much less frequently as a noun than the words in the first two word pairs. Task 2 of SemEval 2012 (Jurgens et al., 2012) was designed to evaluate the effectiveness of automatic approaches for determining the similarity of a pair of concepts to a specific semantic relation. The task focused on 79 semantic relations from Bejar et al. (1991) which broadly fall into the ten categories enumerated in Table 1. The data for the task was collected in two phases using Amazon Mechanical Turk 1 . During Phase 1, Turkers were asked to provide pairs of words which fit a relation template, such as “X possesses/owns/has Y”. Turkers provided word pairs such as expert:experience, mall:shops, letters:words, and doctor:degree. A t"
S12-1055,P04-1055,0,0.10885,"Missing"
S12-1055,C08-1114,0,0.143879,"ation. For the example in Figure 1, most Turkers chose either shrubs:roots or farmer:crops as the most illustrative of the Creature:Possession relation, and homeowner:door as the least illustrative. When Turkers select a pair of words they are performing a semantic inference that we wanted to also perform in a computational manner. In this paper we present a method for automatically ranking word pairs according to their relatedness to a given semantic relation. 2 Approach for Determining Relational Similarity In the vein of previous methods for determining relational similarity (Turney, 2011; Turney, 2008a; Turney, 2008b; Turney, 2005), we propose two approaches using patterns generated from the contexts in which the word pairs occur. Our corpus consists of 8.4 million documents from Gigaword (Parker and Consortium, 2009) and over 4 million articles from Wikipedia. For each word pair, <W1>, <W2> provided by Turkers in Phase 1, as well as the three relation examples, we collected all contexts which 414 matched the schema: “ [0 or more non-content words] <W1> [0 to 7 words] <W2> [0 or more non-content words]” We also include those contexts where W1 and W2 are swapped. The window size of seven wo"
S12-1056,P03-1054,0,0.0151307,"Missing"
S12-1056,S12-1048,0,0.0957195,"h feature set based on the complete relation instead of individual relation arguments. Our best official submission achieves an F1 measure of 0.573 on relation recognition, best in the task and outperforming the previous best result on the same data set (0.500). (5) there is a picture on the wall above the bed. 1 Introduction A significant amount of spatial information in natural language is encoded in spatial relationships between objects. In this paper, we present our approach for detecting the special case of spatial relations evaluated in SemEval-2012 Task 3, Spatial Role Labeling (SpRL) (Kordjamshidi et al., 2012). This task considers the most common type of spatial relationships between objects, namely those described with a spatial preposition (e.g., in, on, over) or a spatial phrase (e.g., in front of, on the left), referred to as the spatial I NDICATOR. A spatial I NDI CATOR connects an object of interest (the T RAJEC TOR ) with a grounding location (the L ANDMARK ). Examples of this type of spatial relationship include: (1) [cars]T parked [in front of]I the [house]L . (2) [bushes]T1 and small [trees]T2 [on]I the [hill]L . (3) a huge [column]L with a [football]T [on top]I . (4) [trees]T [on the rig"
S12-1056,J93-2004,0,0.0398029,"Missing"
S12-1056,de-marneffe-etal-2006-generating,0,\N,Missing
S12-1063,P08-2045,0,0.024668,"Missing"
S12-1063,bethard-etal-2008-building,0,0.0716212,"Missing"
S12-1063,S12-1052,0,0.0599051,"Missing"
S12-1063,P03-1054,0,0.0137591,"Missing"
S12-1063,N03-1033,0,0.0456742,"Missing"
S12-1063,P10-1040,0,0.0275871,"Missing"
S12-1063,P05-3021,0,0.0485696,"Missing"
W01-1206,J98-3005,0,\N,Missing
W01-1206,A00-1023,0,\N,Missing
W01-1206,C00-1043,1,\N,Missing
W01-1206,P00-1071,1,\N,Missing
W01-1206,P96-1025,0,\N,Missing
W01-1206,A00-1041,0,\N,Missing
W04-0819,P98-1013,0,\N,Missing
W04-0819,C98-1013,0,\N,Missing
W04-0819,P03-1002,1,\N,Missing
W04-0819,J02-3001,0,\N,Missing
W04-2501,J96-1002,0,0.0351833,"Missing"
W04-2501,P96-1025,0,0.136108,"Missing"
W04-2501,P03-1003,0,0.229283,"eve that Q/A systems capable of successfully processing complex questions should employ multiple strategies instead of the current pipeline approach, consisting of (1) question processing, (2) passage retrieval and (3) answer selection. The pipeline architecture was reported in (Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001). Recently, a novel approach based on combinations of multiple independent agents implementing different answer finding strategies (multistrategy) and multiple search spaces (multiple-source) was developed by the IBM QA group (Chu-Carroll et al., 2003). In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. In this project we propose a different form of finding optimal strategies of advanced QA which is based on (a) Question Decomposition, (b) Answer Fusion and feedback from (c) Interactive Q&A and (d) User Background Recognition. We argue that all this new architectures operate under the assump"
W04-2501,fillmore-etal-2002-framenet,0,0.0326084,"Missing"
W04-2501,P03-1001,0,0.0623473,"Missing"
W04-2501,C00-1043,1,0.896089,"Missing"
W04-2501,P00-1071,1,0.817666,"d on Q/A and discuss the pragmatics pf processing negation in Q/A. 1 Introduction Our fundamental premise is that progress in Q/A cannot be achieved only by enhancing the processing components, but it also requires generating the best strategies for processing each individual question. Thus we believe that Q/A systems capable of successfully processing complex questions should employ multiple strategies instead of the current pipeline approach, consisting of (1) question processing, (2) passage retrieval and (3) answer selection. The pipeline architecture was reported in (Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001). Recently, a novel approach based on combinations of multiple independent agents implementing different answer finding strategies (multistrategy) and multiple search spaces (multiple-source) was developed by the IBM QA group (Chu-Carroll et al., 2003). In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from"
W04-2501,J98-3005,0,0.172638,"d in Figure 3. Our method first converts the extracted answers into a series of open-domain templates, which are based on predicate-argument frames (Surdeanu et al, 2003). The T3: kill ARG1: the terrorists ARG2: a phone bomb “greedy” merge correct merge Figure 3: Examples of templates and template relations 2.1 2 T2: kill ARG0: Yehya Ayyash, nicknamed “The Engineer” ARG1: twelve Israelis Open-domain template representation A key issue to the proposed approach is the opendomain template representation. While template-based representations have been proposed for information merging in the past (Radev and McKeown, 1998), they considered only domain-specific scenarios. Based on our recent successes with the extraction of predicateargument frames (Surdeanu et al, 2003), we propose a template representation that is a direct mapping of predicate-argument frames. For example, the first template in Figure 3 is generated from the frame detected for the predicate “assassinate”: the first slot – ARG0 – typically stands for subject or agent; the second slot – ARG1 – stands for the predicate object, and the modifier arguments ARGM-LOC and ARGM-TMP indicate the location and date of the event. 2.2 Detection of template r"
W04-2501,P03-1002,1,0.761453,"ree-step process. First, an open-domain, template-based answer formalization is constructed based on predicate-argument frames. Second, a probabilistic model is trained to detect relations between the extracted templates. Finally, a set of template merging operators are introduced to construct the merged answer. The block architecture for answer fusion is illustrated in Figure 2. The system functionality is demonstrated with the example illustrated in Figure 3. Our method first converts the extracted answers into a series of open-domain templates, which are based on predicate-argument frames (Surdeanu et al, 2003). The T3: kill ARG1: the terrorists ARG2: a phone bomb “greedy” merge correct merge Figure 3: Examples of templates and template relations 2.1 2 T2: kill ARG0: Yehya Ayyash, nicknamed “The Engineer” ARG1: twelve Israelis Open-domain template representation A key issue to the proposed approach is the opendomain template representation. While template-based representations have been proposed for information merging in the past (Radev and McKeown, 1998), they considered only domain-specific scenarios. Based on our recent successes with the extraction of predicateargument frames (Surdeanu et al, 2"
W04-2502,J96-1002,0,0.0232277,"Missing"
W04-2502,P96-1025,0,0.0218324,"Missing"
W04-2502,P03-1003,0,0.0600034,"Missing"
W04-2502,fillmore-etal-2002-framenet,0,0.064657,"Missing"
W04-2502,P03-1001,0,0.0270337,"Missing"
W04-2502,C00-1043,1,0.871336,"Missing"
W04-2502,P00-1071,1,0.875055,"Missing"
W04-2502,J98-3005,0,0.0616703,"Missing"
W04-2505,C02-1042,0,0.0681666,"Missing"
W04-2505,W03-1008,0,0.0125115,"basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2 This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3 We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers u"
W04-2505,J02-3001,0,0.0834271,"tructures are used for indexing/retrieving candidate passages. The Answer Processing function involves the recognition of the answer structure and intentional structure. Often this requires reference resolution. The implied information coerced from both the question and the candidate answer is also validated before deciding on the answer correctness. 4 Predicate-Argument Structures To identify predicate-argument structures in questions and passages, we have: (1) used the Proposition Bank or PropBank as training data; and (2) a mode for predicting argument roles similar to the one employed by (Gildea and Jurafsky, 2002). PropBank is a one million word corpus annotated with predicate-argument structures on top of the Penn Treebank 2 Wall Street Journal texts. For any given predicate, the expected arguments are labeled sequentially from Arg 0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for direct object or theme or patient, Arg 2 for indirect object or benefactive or instrument or attribute or end state, Arg 3 for start point or benefactive or attribute and Arg4 for end point. In addition to these core arguments, adjunctative arguments are marked up. They include functional tags from Treebank, e.g. ArgM"
W04-2505,P02-1031,0,0.0154399,"adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2 This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3 We point out that we removed from the Penn TreeBank the special tags of noun phrases like Subj and TMP as parsers usually are not able to pr"
W04-2505,W03-1006,0,0.0127029,"common consensus on the basic features that should be adopted. These standard features, first proposed in (Gildea and Jurafsky, 2002), are derived from parse trees as illustrated by Table 2. 4.2 Parsing Sentence into Predicate Argument Structures For the experiments, we used PropBank (www.cis.upenn.edu/∼ace) along with PennTreeBank3 2 (www.cis.upenn.edu/∼treebank) (Echihabi and Marcu, 2003). This corpus contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Gildea and Palmer, 2002; Pradhan et al., 2003). In this split, Sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered all PropBank arguments from Arg0 to Arg9, ArgA and ArgM even if only Arg0 from Arg4 and ArgM contain enough training/testing 2 This is a basic method to pass from binary categorization into a multi-class categorization problem; several optimization have been proposed, e.g. (Goh et al., 2001). 3 We point out that we removed from the Penn TreeBank the special tags of noun phrases"
W04-2505,C00-1043,1,0.873305,"Missing"
W04-2505,P01-1037,1,0.841954,"ing and (3) Answer Extraction. In the case of factoid questions , question processing involves the classification of questions with the purpose of predicting what semantic class the answer should belong to. Thus we may have questions asking about P EOPLE, O RGANIZATIONS, T IME or L OCATIONS. Since opendomain Q/A systems process questions regardless of the domain of interest, question processing must be based on an extended ontology of answer types. The identification of the expected answer type is based either on binary semantic dependencies extracted from the syntactic parse of the question (Harabagiu et al., 2001) or on the predicateargument structure of the question. In both cases, the relation to the question stem (i.e. what, who, when) enables the classification. Figure 2 illustrates a factoid question generated as an intended question and the derivation of its expected answer type. However, many times the expected answer type needs to be identified from an ontology that has high lexicosemantic coverage. Many Q/A systems use the WordNet database for this purpose. In contrast, definition questions do not require the identification of the expected anDefinition Question: Question Parse: What is ETA in"
W04-2505,P03-1002,1,0.905027,"tion of the question context. In this paper, by considering the intentional information and the implied information that can be derived when processing questions, we introduce a novel model of Q/A, which has access to rich semantic structures and enables the retrieval of more accurate answers as well as inference processes that explain the validity and contextual coverage of answers. Figure 5 shows the structure of the novel model of Q/A we propose. Both Question Processing and Document Processing have the recognition of predicate-argument structures as a crux of their models. As reported in (Surdeanu et al., 2003), the recognition of predicate-argument structures depends on features made available by full syntactic parses and by Named Entity Recognizers. As we shall show in this paper, the predicate-argument structures enable the recognition of question pattern, the question focus and the intentional structure associated with Question Processing Syntactic Parse Document Processing Named Entity Recognition Answer Processing Named Entity Recognition Syntactic Parse Recognition of Answer Structure Identification of Predicate−Argument Structures Recognition of Identification of Question Pattern Predicate−A"
W04-2505,J03-2004,0,\N,Missing
W04-2505,P03-1003,0,\N,Missing
W04-2505,N04-1030,0,\N,Missing
W04-2506,A00-2018,0,0.0235811,"lues. However, most of the time the question stems are either ambiguous or they simply do not exist. For example, questions having what as their stem may ask about anything. In this case another word from the question needs to be used to determine the semantic class of the expected answer. In particular, the additional word is semantically classified against an ontology of semantic classes. To determine which word indicates the semantic class of the expected answer, the syntactic dependencies1 between the question words may be employed (Harabagiu 1 Syntactic parsers publicly available, e.g., (Charniak, 2000; et al., 2000; Pasca and Harabagiu, 2001; Harabagiu et al., 2001). Sometimes the semantic class of the expected answers cannot be identified or is erroneously identified causing the selection of erroneous answers. The use of text classification aims to filter out the incorrect set of answers that Q/A systems provide. 2.2 Paragraph Retrieval Once the question processing has chosen the relevant keywords of questions, some term expansion techniques are applied: all nouns and adjectives as well as morphological variations of nouns are inserted in a list. To find the morphological variations of th"
W04-2506,P97-1003,0,0.160529,"anking: We compute the sentence ranks as a by product of sorting the selected sentences. To sort the sentences, we may use any sorting algorithm, e.g., the quicksort, given that we provide a comparison function between each pair of sentences. To learn the comparison function we use a simple neural network, namely, the perceptron, to compute a relative comparison between any two sentences. This score is computed by considering four different features for each sentence as explained in (Pasca and Harabagiu, 2001). Step 3) Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997), can be used to capture the binary dependencies between the head of each phrase. answers. If we lead fewer than 5 sentences to select from, we return all of them. Once the answers are extracted we can apply an additional filter based on text categories. The idea is to match the categories of the answers against those of the questions. Next section addresses the problem of question and answer categorization. 3 Text and Question Categorization To exploit category information for Q/A we categorize both answers and questions. For the former, we define as categories of an answer a the categories o"
W04-2506,C00-1043,1,0.874019,"Missing"
W04-2506,P01-1037,1,0.850815,"ther ambiguous or they simply do not exist. For example, questions having what as their stem may ask about anything. In this case another word from the question needs to be used to determine the semantic class of the expected answer. In particular, the additional word is semantically classified against an ontology of semantic classes. To determine which word indicates the semantic class of the expected answer, the syntactic dependencies1 between the question words may be employed (Harabagiu 1 Syntactic parsers publicly available, e.g., (Charniak, 2000; et al., 2000; Pasca and Harabagiu, 2001; Harabagiu et al., 2001). Sometimes the semantic class of the expected answers cannot be identified or is erroneously identified causing the selection of erroneous answers. The use of text classification aims to filter out the incorrect set of answers that Q/A systems provide. 2.2 Paragraph Retrieval Once the question processing has chosen the relevant keywords of questions, some term expansion techniques are applied: all nouns and adjectives as well as morphological variations of nouns are inserted in a list. To find the morphological variations of the nouns, we used the CELEX (Baayen et al., 1995) database. The lis"
W06-0705,P05-1026,1,0.887492,"human users are generally able to identify their information needs independently, the information needs of organizations are often presented in the form of short prose descriptions – known as scenarios – which outline the range of knowledge sought by a customer in order to achieve a specific outcome or to accomplish a particular task. (An example of one scenario is presented in Figure 1.) Recent work in Q/A has sought to use information derived from these kinds of scenarios in order to retrieve sets of answers that are more relevant – and responsive – to a customer’s information needs. While (Harabagiu et al., 2005) used topic signatures (Lin and Hovy, 2000; 32 Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 32–39, c Sydney, July 2006. 2006 Association for Computational Linguistics textual entailment can be computed and how it can be used in the intrinsic and extrinsic evaluation of a Q/A system. The remainder of the paper is organized in the following way. Section 2 introduces our notion of contextual entailment and provides a framework for recognizing instances of CE between scenarios and both questions and answers. Section 3 describes the textual entailment syst"
W06-0705,P06-4007,1,0.819629,", the customer wants to know why American corporations have sought to outsource jobs to India, the types of economic advantages that American companies could gain from relocating to India, and the kinds of economic or political inducements that India has offered to American companies looking to outsource jobs there. The customer is not interested in demographic information on Indian employees of American firms. Abstract This paper describes a novel framework for using scenario knowledge in opendomain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description. An intrinsic and an extrinsic evaluation of this method is presented in the context of an automatic Q/A system and results from several user scenarios are discussed. Table 1: Example of a User Scenario. Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a sc"
W06-0705,C00-1072,0,0.119752,"Missing"
W06-0705,N06-1006,0,0.12908,"Missing"
W06-0705,N03-1022,1,0.824732,"tomatic Q/A system and results from several user scenarios are discussed. Table 1: Example of a User Scenario. Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, contextsensitive inferences that could be used to answer questions. Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al., 2003). Under this approach, information extracted from the scenario narrative is converted to logical axioms that can used in conjunction with a logic prover in order justify answers returned for questions. In this paper, we propose that scenario-relevant passages in natural language texts can be identified by recognizing a semantic relation, known as contextual entailment (CE), that exists between a text passage and one of a set of subquestions that are conventionally implied by a scenario. Under this model, we expect that a scenario S can be considered to contextually entail a passage t, when the"
W06-0705,H05-1049,0,0.0241988,"Missing"
W06-0705,C04-1100,1,0.822036,"ge in opendomain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description. An intrinsic and an extrinsic evaluation of this method is presented in the context of an automatic Q/A system and results from several user scenarios are discussed. Table 1: Example of a User Scenario. Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, contextsensitive inferences that could be used to answer questions. Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al., 2003). Under this approach, information extracted from the scenario narrative is converted to logical axioms that can used in conjunction with a logic prover in order justify answers returned for questions. In this paper, we propose that scenario-relevant passages in natural la"
W06-0705,C04-1084,1,\N,Missing
W06-0705,P06-1114,1,\N,Missing
W06-0705,W07-1401,0,\N,Missing
W06-3004,P05-1026,1,0.902947,"eve will return answers that will let them reach certain information goals. Users need more than answers, however: while they might be cognizant of many of the different types of information that they need, few – if any – users are capable of identifying all of the questions that must be asked and answered for a particular scenario. In order to take full advantage of the Q/A capabilities of current systems, users need access to sources of domain-specific knowledge that will expose them to new concepts and ideas and will allow them to ask better questions. In previous work (Hickl et al., 2004; Harabagiu et al., 2005a), we have argued that interactive questionanswering systems should be based on a predictive dialogue architecture which can be used to provide users with both precise answers to their questions as well as suggestions of relevant research topics that could be explored throughout the course of an interactive Q/A dialogue. Typically, the quality of interactive Q/A dialogues has been measured in three ways: (1) efficiency, defined as the number of questions that the user must pose to find particular information, (2) effectiveness, defined by the relevance of the answer returned, and (3) user sat"
W06-3004,C04-1084,1,0.840946,"ies the QUABs that best meet the user’s expected information requirements. At the core of the F ERRET’s predictive dialogue module is the Predictive Dialogue Network (PQN), a large database of QUABs that were either generated off-line by human annotators or created on-line by F ERRET (either during the current dialogue or during some previous dialogue)1 . In order to generate QUABs automatically, documents identified from F ERRET’s automatic Q/A system are first submitted to a Topic Representation module, which computes both topic signatures (Lin and Hovy, 2000) and enhanced topic signatures (Harabagiu, 2004) in order to identify a set of topic-relevant passages. Passages are then submitted to an Information Extraction module, which annotates texts with a wide 1 Techniques used by human annotators for creating QUABs were fi rst described in (Hickl et al., 2004); full details of F ER RET ’s automatic QUAB generation components are provided in (Harabagiu et al., 2005a). 27 range of lexical, semantic, and syntactic information, including (1) morphological information, (2) named entity information from LCC’s C ICERO L ITE named entity recognition system, (3) semantic dependencies extracted from LCC’s"
W06-3004,W04-2508,1,0.932093,"ions which they believe will return answers that will let them reach certain information goals. Users need more than answers, however: while they might be cognizant of many of the different types of information that they need, few – if any – users are capable of identifying all of the questions that must be asked and answered for a particular scenario. In order to take full advantage of the Q/A capabilities of current systems, users need access to sources of domain-specific knowledge that will expose them to new concepts and ideas and will allow them to ask better questions. In previous work (Hickl et al., 2004; Harabagiu et al., 2005a), we have argued that interactive questionanswering systems should be based on a predictive dialogue architecture which can be used to provide users with both precise answers to their questions as well as suggestions of relevant research topics that could be explored throughout the course of an interactive Q/A dialogue. Typically, the quality of interactive Q/A dialogues has been measured in three ways: (1) efficiency, defined as the number of questions that the user must pose to find particular information, (2) effectiveness, defined by the relevance of the answer re"
W06-3004,C00-1072,0,0.0103764,"sent to a predictive dialogue module, which identifies the QUABs that best meet the user’s expected information requirements. At the core of the F ERRET’s predictive dialogue module is the Predictive Dialogue Network (PQN), a large database of QUABs that were either generated off-line by human annotators or created on-line by F ERRET (either during the current dialogue or during some previous dialogue)1 . In order to generate QUABs automatically, documents identified from F ERRET’s automatic Q/A system are first submitted to a Topic Representation module, which computes both topic signatures (Lin and Hovy, 2000) and enhanced topic signatures (Harabagiu, 2004) in order to identify a set of topic-relevant passages. Passages are then submitted to an Information Extraction module, which annotates texts with a wide 1 Techniques used by human annotators for creating QUABs were fi rst described in (Hickl et al., 2004); full details of F ER RET ’s automatic QUAB generation components are provided in (Harabagiu et al., 2005a). 27 range of lexical, semantic, and syntactic information, including (1) morphological information, (2) named entity information from LCC’s C ICERO L ITE named entity recognition system,"
W06-3004,J05-1004,0,0.0091117,"Q/A dialogue, we introduced a large number of features to estimate relevance for each QUAB suggestion. The features we used are presented in Figure 6 (a) Rank of QUAB: the rank (1, ..., 10) of the QUAB in question. (b) Similarity: similarity of QUAB, Qn and QUAB, Qn−1 . (c) Relation likelihood: equal to the likelihood of each predicate-argument structure included in QUAB given all QUABs contained in F ERRET’s QUAB; calculated for Arg-0, Arg-1, and ArgM-TMP for each predicate found in QUAB suggestions. (Predicate-argument relations were identifi ed using a semantic parser trained on PropBank (Palmer et al., 2005) annotations.) (d) Conditional Expected Answer Type likelihood: equal to the joint probability p(EATQU AB |EATquestion ) calculated from a corpus of dialogues collected from human users of F ERRET. (e) Terms in common: real-valued feature equal to the number of terms in common between the QUAB and both Qn and Qn−1 . (f) Named Entities in common: same as terms in common, but calculated for named entities detected by LCC’s C IERO L ITE named entity recognition system. Figure 6: Relevance Features. In the next section, we describe how we utilized the user interaction model described in Subsection"
W06-3004,N03-1028,0,0.0223864,"and deserves further consideration. 4 User Interaction Models for Relevance Estimation Unlike systems that utilize mixed initiative dialogues in order to determine a user’s information needs (Small and Strzalkowski, 2004), systems (like F ERRET) which rely on interactions based on predictive questioning have traditionally not incorporated techniques that allow them to gather relevance feedback from users. In this section, we describe how we have used a new set of user interaction models (UIM) in conjunction with a relevance classifier based on conditional random fields (CRF) (McCallum, 2003; Sha and Pereira, 2003) in order to improve the relevance of the QUAB suggestions that F ERRET returns in response to a user’s query. We believe that systems based on predictive questioning can derive feedback from users in three ways. First, systems can learn which suggestions or answers are relevant to a user’s domain of interest by tracking which elements users select throughout the course of a dialogue. With F ERRET, each answer or suggestion presented to a user is associated with a hyperlink that links to the original text that the answer or QUAB was derived from. While users do not always follow links associat"
W06-3004,C04-1189,0,0.0300806,"ons between concepts) needed to formulate new, more specific hypotheses and questions. Returning again to Figure 3, we can see that questions like N Q1 and N Q3 are designed to discover new knowledge that the user does not currently possess, while questions like N Q6 try to establish whether or not the user’s hypothesis (i.e. namely, that EU countries view job outsourcing to India as an problem) is valid and deserves further consideration. 4 User Interaction Models for Relevance Estimation Unlike systems that utilize mixed initiative dialogues in order to determine a user’s information needs (Small and Strzalkowski, 2004), systems (like F ERRET) which rely on interactions based on predictive questioning have traditionally not incorporated techniques that allow them to gather relevance feedback from users. In this section, we describe how we have used a new set of user interaction models (UIM) in conjunction with a relevance classifier based on conditional random fields (CRF) (McCallum, 2003; Sha and Pereira, 2003) in order to improve the relevance of the QUAB suggestions that F ERRET returns in response to a user’s query. We believe that systems based on predictive questioning can derive feedback from users in"
W07-1420,de-marneffe-etal-2006-generating,0,0.0390588,"Missing"
W07-1420,W07-1401,0,\N,Missing
W13-0118,J10-4006,0,0.0120269,"ned all tuples matching a relation over a large corpus. There has been much previous research effort on inducing semantic classes as well. Most approaches use some form of context around words to induce the classes. Older approaches simply used a bag of words context (Roark and Charniak, 1998), but this leads to induced classes containing more paradigmatically similar words rather than syntagmatically similar words (Widdows and Dorow, 2002). More recent approaches have utilized a subset of semantically-rich syntactic relations such as verb-object, noun modifier, coordination, and preposition (Baroni and Lenci, 2010; Widdows and Dorow, 2002). Lin and Pantel (2001) induce semantic classes using dependency parse contexts. Their approach is based on a vector space rather than the probabilistic setting of an LDA. Rahman and Ng (2010) use a factor graph with various semantic, morphological, and grammatical features to induce a set of semantic classes with the goal of performing better named entity recognition. Pantel (2003) uses short contextual patterns to inform a clustering approach to category induction. 3 Inducing semantic word classes We consider a semantic class to be a set of words which share a seman"
W13-0118,P06-1038,0,0.052171,"Missing"
W13-0118,W08-1301,0,0.0423733,"Missing"
W13-0118,S12-1047,0,0.465489,"and cognition. One particular interesting usage of semantic relations is provided by analogical reasoning. As reported by Gentner (1983) and Holyoak and Thagard (1996), whenever a new situation arises, humans tend to search for an analogous situation from their past experience. Analogical reasoning relies on relational similarity, as reported by Turney (2006) and Turney (2008). In analogical reasoning, the degree of relational similarity is an estimation of the likelihood of applicability of the knowledge transfer (from past to present). Thus, as postulated in the recent SemEval 2012 Task 2 (Jurgens et al., 2012), the automatic analysis of relational similarity may have practical benefits of indicating the appropriateness of an analogy. Relational similarity, as reported in Turney (2006), is one of the forms of similarity, the other one being provided by attributional similarity. Relational similarity evaluates the correspondence between relations (Medin et al., 1990), while attributional similarity evaluates the correspondence between attributes. As stated by Turney: “When two words have a high degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relati"
W13-0118,P10-1045,0,0.0255922,"Missing"
W13-0118,N07-1071,0,0.0219948,"tes the correspondence between attributes. As stated by Turney: “When two words have a high degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relational similarity, we say they are analogous.” We claim that there is a special property that arguments of relations need to share. The arguments of relations are words which are predications of binary facts, properties, actions, etc. As such, we are aware from the work of Resnik (1996) that words which appear as arguments of a predicate define the selectional preferences of the predicate. Moreover, Pantel et al. (2007) have extended the notion of predicate selectional preferences to “relational selectional preferences” of binary relations. For a binary relation r(x, y), the semantic classes C(x) which can be instantiated for the argument x as well as C(y), the semantic classes which can be instantiated for the argument y constitute the relational selectional preferences of the binary relation. Thus we believe and show in this paper that semantic relations have selectional preferences and that word pairs x:y are more similar to a relation when those words are more admissible under the relational selectional"
W13-0118,C10-1105,0,0.0185831,"r approaches simply used a bag of words context (Roark and Charniak, 1998), but this leads to induced classes containing more paradigmatically similar words rather than syntagmatically similar words (Widdows and Dorow, 2002). More recent approaches have utilized a subset of semantically-rich syntactic relations such as verb-object, noun modifier, coordination, and preposition (Baroni and Lenci, 2010; Widdows and Dorow, 2002). Lin and Pantel (2001) induce semantic classes using dependency parse contexts. Their approach is based on a vector space rather than the probabilistic setting of an LDA. Rahman and Ng (2010) use a factor graph with various semantic, morphological, and grammatical features to induce a set of semantic classes with the goal of performing better named entity recognition. Pantel (2003) uses short contextual patterns to inform a clustering approach to category induction. 3 Inducing semantic word classes We consider a semantic class to be a set of words which share a semantic property. For example, the semantic property “male” forms a semantic class which includes the words “man, bull, boy, boyfriend, groom”. Under this definition, words can belong to many semantic classes. For example"
W13-0118,S12-1055,1,0.675381,"uce semantic classes. Section 4 describes the dataset we use for measuring relational similarity. Section 5 describes how the induced semantic classes are used to model the selectional preferences of semantic relations. Section 5 describes how we determine the extent to which a word pair matches a relation’s selectional preferences. Section 6 gives our experimental setup and the results of our evaluation. Section 7 analyzes the types of semantic classes that were automatically induced and Section 8 concludes the paper. 2 Previous work Prior work on relational similarity (Jurgens et al., 2012; Rink and Harabagiu, 2012; Turney, 2005, 2006) has understandably focused the actual relation between a pair of words under consideration. These approaches have all considered how the two words co-occur in a large corpus and what contexts can be found near the words when they co-occur. Contextual information is useful for determining the relationship between two words. Therefore we believe the selectional preference agreement method can complement these approaches. The best-performing relational similarity approach at the SemEval 2012 Task 2 utilized a graphical model to determine patterns likely to be found between t"
W13-0118,P10-1044,0,0.0236582,"asses derived for nouns which are modified by adjectives for instance. In our approach we induce semantic classes independently of the relations whose selectional preferences we are modeling. We take this approach because our relational data consists only of word pairs with no context. Further, some of the word pairs may never occur in the same sentence even in a large corpus (e.g., signature:acknowledgment) yet we can still check the admissibility of the words as arguments to the desired relation (e.g, X represents Y). An extension to Latent Dirichlet Allocation model has been used before by Ritter and Etzioni (2010) to model semantic relations and their selectional preferences. There are two distinct reasons their approach is not well-suited to the relational similarity task. First, they were additionally inducing the set of relations present in their data, while in the relational similarity task we aim to determine membership to an existing set of relations. The second difference in their approach is the large size of their dataset. While we were able to train our models using on average around 40 word pairs per relation, their data contained all tuples matching a relation over a large corpus. There has"
W13-0118,P98-2182,0,0.0103273,"he set of relations present in their data, while in the relational similarity task we aim to determine membership to an existing set of relations. The second difference in their approach is the large size of their dataset. While we were able to train our models using on average around 40 word pairs per relation, their data contained all tuples matching a relation over a large corpus. There has been much previous research effort on inducing semantic classes as well. Most approaches use some form of context around words to induce the classes. Older approaches simply used a bag of words context (Roark and Charniak, 1998), but this leads to induced classes containing more paradigmatically similar words rather than syntagmatically similar words (Widdows and Dorow, 2002). More recent approaches have utilized a subset of semantically-rich syntactic relations such as verb-object, noun modifier, coordination, and preposition (Baroni and Lenci, 2010; Widdows and Dorow, 2002). Lin and Pantel (2001) induce semantic classes using dependency parse contexts. Their approach is based on a vector space rather than the probabilistic setting of an LDA. Rahman and Ng (2010) use a factor graph with various semantic, morphologic"
W13-0118,J06-3003,0,0.0506886,"score of 0.229. 1 Introduction In natural language, words participate often in a variety of semantic relations. Both linguists and psychology researchers have been interested in categorizing semantic relations and to understand their usage in language and cognition. One particular interesting usage of semantic relations is provided by analogical reasoning. As reported by Gentner (1983) and Holyoak and Thagard (1996), whenever a new situation arises, humans tend to search for an analogous situation from their past experience. Analogical reasoning relies on relational similarity, as reported by Turney (2006) and Turney (2008). In analogical reasoning, the degree of relational similarity is an estimation of the likelihood of applicability of the knowledge transfer (from past to present). Thus, as postulated in the recent SemEval 2012 Task 2 (Jurgens et al., 2012), the automatic analysis of relational similarity may have practical benefits of indicating the appropriateness of an analogy. Relational similarity, as reported in Turney (2006), is one of the forms of similarity, the other one being provided by attributional similarity. Relational similarity evaluates the correspondence between relations"
W13-0118,C08-1114,0,0.0632856,"Introduction In natural language, words participate often in a variety of semantic relations. Both linguists and psychology researchers have been interested in categorizing semantic relations and to understand their usage in language and cognition. One particular interesting usage of semantic relations is provided by analogical reasoning. As reported by Gentner (1983) and Holyoak and Thagard (1996), whenever a new situation arises, humans tend to search for an analogous situation from their past experience. Analogical reasoning relies on relational similarity, as reported by Turney (2006) and Turney (2008). In analogical reasoning, the degree of relational similarity is an estimation of the likelihood of applicability of the knowledge transfer (from past to present). Thus, as postulated in the recent SemEval 2012 Task 2 (Jurgens et al., 2012), the automatic analysis of relational similarity may have practical benefits of indicating the appropriateness of an analogy. Relational similarity, as reported in Turney (2006), is one of the forms of similarity, the other one being provided by attributional similarity. Relational similarity evaluates the correspondence between relations (Medin et al., 19"
W13-0118,C02-1114,0,0.041733,"second difference in their approach is the large size of their dataset. While we were able to train our models using on average around 40 word pairs per relation, their data contained all tuples matching a relation over a large corpus. There has been much previous research effort on inducing semantic classes as well. Most approaches use some form of context around words to induce the classes. Older approaches simply used a bag of words context (Roark and Charniak, 1998), but this leads to induced classes containing more paradigmatically similar words rather than syntagmatically similar words (Widdows and Dorow, 2002). More recent approaches have utilized a subset of semantically-rich syntactic relations such as verb-object, noun modifier, coordination, and preposition (Baroni and Lenci, 2010; Widdows and Dorow, 2002). Lin and Pantel (2001) induce semantic classes using dependency parse contexts. Their approach is based on a vector space rather than the probabilistic setting of an LDA. Rahman and Ng (2010) use a factor graph with various semantic, morphological, and grammatical features to induce a set of semantic classes with the goal of performing better named entity recognition. Pantel (2003) uses short"
W13-0118,C98-2177,0,\N,Missing
W13-0119,P10-1143,1,0.815714,"orm of temporal event relations. The TimeML annotation standard (Pustejovsky et al., 2003) for temporal relations as well as the TimeBank corpus (Pustejovsky et al., 2003) have inspired a significant number of automatic systems for this task (Verhagen et al. 2009, Verhagen et al. 2010, UzZaman et al. 2012, Sun et al. 2013). Beyond temporal relations, work in other types of event relations has received less attention. Prominent among the other event relation types is causation (Bethard and Martin 2008, Beamer and Girju 2009, Rink et al. 2010, Do et al. 2011) and co-reference (Chen et al. 2009, Bejan and Harabagiu 2010). Beyond event relations, Chambers and Jurafsky (2008, 2009) and Bejan (2008) both create narrative schemas based on commonly co-occurring event structures, which is a useful tool for determining a prior likelihood of two or more events being related. Spatial relations between non-events has likewise received much attention. Several such works are spatial annotation schemas. SpatialML (Mani et al., 2008) focuses on recognizing geographic regions and expressions. For example, the following text: a town some 50 miles south of Salzburg in the central Austrian Alps SpatialML would recognize town,"
W13-0119,P08-2045,0,0.0318273,"2 Related Work Event relations in general have received significant attention in the literature, but largely in the form of temporal event relations. The TimeML annotation standard (Pustejovsky et al., 2003) for temporal relations as well as the TimeBank corpus (Pustejovsky et al., 2003) have inspired a significant number of automatic systems for this task (Verhagen et al. 2009, Verhagen et al. 2010, UzZaman et al. 2012, Sun et al. 2013). Beyond temporal relations, work in other types of event relations has received less attention. Prominent among the other event relation types is causation (Bethard and Martin 2008, Beamer and Girju 2009, Rink et al. 2010, Do et al. 2011) and co-reference (Chen et al. 2009, Bejan and Harabagiu 2010). Beyond event relations, Chambers and Jurafsky (2008, 2009) and Bejan (2008) both create narrative schemas based on commonly co-occurring event structures, which is a useful tool for determining a prior likelihood of two or more events being related. Spatial relations between non-events has likewise received much attention. Several such works are spatial annotation schemas. SpatialML (Mani et al., 2008) focuses on recognizing geographic regions and expressions. For example,"
W13-0119,P09-1068,0,0.0977436,"Missing"
W13-0119,W09-4303,0,0.0200293,"t largely in the form of temporal event relations. The TimeML annotation standard (Pustejovsky et al., 2003) for temporal relations as well as the TimeBank corpus (Pustejovsky et al., 2003) have inspired a significant number of automatic systems for this task (Verhagen et al. 2009, Verhagen et al. 2010, UzZaman et al. 2012, Sun et al. 2013). Beyond temporal relations, work in other types of event relations has received less attention. Prominent among the other event relation types is causation (Bethard and Martin 2008, Beamer and Girju 2009, Rink et al. 2010, Do et al. 2011) and co-reference (Chen et al. 2009, Bejan and Harabagiu 2010). Beyond event relations, Chambers and Jurafsky (2008, 2009) and Bejan (2008) both create narrative schemas based on commonly co-occurring event structures, which is a useful tool for determining a prior likelihood of two or more events being related. Spatial relations between non-events has likewise received much attention. Several such works are spatial annotation schemas. SpatialML (Mani et al., 2008) focuses on recognizing geographic regions and expressions. For example, the following text: a town some 50 miles south of Salzburg in the central Austrian Alps Spati"
W13-0119,de-marneffe-etal-2006-generating,0,0.0722303,"Missing"
W13-0119,D11-1027,0,0.0147882,"nt attention in the literature, but largely in the form of temporal event relations. The TimeML annotation standard (Pustejovsky et al., 2003) for temporal relations as well as the TimeBank corpus (Pustejovsky et al., 2003) have inspired a significant number of automatic systems for this task (Verhagen et al. 2009, Verhagen et al. 2010, UzZaman et al. 2012, Sun et al. 2013). Beyond temporal relations, work in other types of event relations has received less attention. Prominent among the other event relation types is causation (Bethard and Martin 2008, Beamer and Girju 2009, Rink et al. 2010, Do et al. 2011) and co-reference (Chen et al. 2009, Bejan and Harabagiu 2010). Beyond event relations, Chambers and Jurafsky (2008, 2009) and Bejan (2008) both create narrative schemas based on commonly co-occurring event structures, which is a useful tool for determining a prior likelihood of two or more events being related. Spatial relations between non-events has likewise received much attention. Several such works are spatial annotation schemas. SpatialML (Mani et al., 2008) focuses on recognizing geographic regions and expressions. For example, the following text: a town some 50 miles south of Salzburg"
W13-0119,P10-1160,0,0.0246222,"ms quite low compared to other natural language tasks such as named entity recognition (NER) and semantic role labeling (SRL). However, those tasks are limited to explicit context, such as contiguous tokens for NER and parse nodes within the syntactic scope for SRL. These tasks also utilize more predictable features, such as surface-level casing features for NER and predictable argument structures for SRL (e.g., the syntactic subject for an active verb is usually the A RG 0). Proper comparison requires evaluating our results alongside other implicit tasks. One such work involves implicit SRL. Gerber and Chai (2010) perform nominal SRL and achieve an overall F1 -measure of 42.3. While the tasks are not directly comparable in terms of difficulty, this does suggest that implicit tasks require far more advanced methods to achieve superior performance and that downstream systems will likely need to be highly tolerant to noise. To address this, we discuss future work below, analyzing the types of errors that our system makes to give context to these ideas. As might be guessed, rare event mentions with long dependency paths are highly likely to result in false negatives, such as the relation between elected an"
W13-0119,P03-1054,0,0.00358866,"ion (rather focusing on entity information). Instead, we focus on learning which individual events are likely to participate in a spatial relation (using the training data), which pairs of events are likely to participate in a spatial relation (also from the training data), and which pairs of events are likely to be related (from unlabeled data). We utilize the following classes of features: • Individual arguments (separate features for first and second arguments). Includes features based on event mention’s surface form, caseless form, lemmatized form, part-of-speech from the Stanford Parser (Klein and Manning, 2003), General Inquirer categories (Stone et al., 1966), TimeML event classes from TARSQI (Verhagen et al., 2005), WordNet (Fellbaum, 1998) synsets and hypernyms, and VerbNet (Kipper et al., 1998) classes. • Concatenation of the above individual argument features for both arguments (e.g., “draw::show” for lemmatized form, “25.2::29.5-2” for VerbNet classes). • Intersection of feature values for individual argument features. • Statistical association of events based on various resources: – Gigaword (Parker et al., 2009) sentence co-occurrence – TimeML relations on Gigaword – Wikipedia co-occurrence"
W13-0119,S12-1048,0,0.219624,"ian to their respective geo-political entities, recognize the direction and distance relation between town and Salzburg, and the containment relations between Salzburg and Austrian and Alps and Austrian. SpatialML has no handling, however, for spatial event relations. Likewise, SpRL (Kordjamshidi et al., 2010) represents spatial relations beyond geographic relations, but would have difficulty representing event relations because SpRL requires an indicator (trigger, e.g., in, on, at, to the left of) that is rarely present in spatial event mentions. SpRL does, however, have an annotated corpus (Kordjamshidi et al., 2012) and several automatic approaches have been proposed (Kordjamshidi et al. 2011, Roberts and Harabagiu 2012). STML (Pustejovsky and Moszkowicz, 2008) focuses on the annotation of spatial relations for events, specifically motion events. But their scheme connects a motion event with its motion-specific arguments, and does not include event-event spatial relations. Despite significant work in both event relations and spatial relations, work specific to spatial relations between events has been quite sparse. ISO-Space (Pustejovsky et al. 2011a, Pustejovsky et al. 2011b, Lee et al. 2011) is an on-g"
W13-0119,kordjamshidi-etal-2010-spatial,0,0.269662,"Missing"
W13-0119,mani-etal-2008-spatialml,0,0.623287,"ntion. Prominent among the other event relation types is causation (Bethard and Martin 2008, Beamer and Girju 2009, Rink et al. 2010, Do et al. 2011) and co-reference (Chen et al. 2009, Bejan and Harabagiu 2010). Beyond event relations, Chambers and Jurafsky (2008, 2009) and Bejan (2008) both create narrative schemas based on commonly co-occurring event structures, which is a useful tool for determining a prior likelihood of two or more events being related. Spatial relations between non-events has likewise received much attention. Several such works are spatial annotation schemas. SpatialML (Mani et al., 2008) focuses on recognizing geographic regions and expressions. For example, the following text: a town some 50 miles south of Salzburg in the central Austrian Alps SpatialML would recognize town, Salzburg, Austrian, and Alps as geographic locations, normalize Salzburg and Austrian to their respective geo-political entities, recognize the direction and distance relation between town and Salzburg, and the containment relations between Salzburg and Austrian and Alps and Austrian. SpatialML has no handling, however, for spatial event relations. Likewise, SpRL (Kordjamshidi et al., 2010) represents sp"
W13-0119,D09-1143,0,0.0161103,"t Leo Klinghoffer was located on the Achille Lauro when he was killed. Without such a relation, we would have to make the (un-principled) assumption that the closest location (in this case a vehicle) is the location of the killed event. 4 Method We utilize a mostly supervised, two-stage machine learning approach for detecting spatial event relations. A binary support vector machine (SVM) classifier is used for recognizing spatial relations and a multi-class SVM is used for determining the relation type. Previous SVM-based approaches to relation extraction have utilized advanced kernels (e.g., Nguyen et al. (2009)). In this work, however, 1 Gaizauskas et al. (2012) have annotated a small corpus of facility design reports with a version of ISO-Space, but it is neither publicly available nor large enough to utilize as training data in a machine learning approach. Furthermore, the majority of its spatial relations (perhaps all) are not between events. In October of 1985, four hijackers under his command took over the Italian cruise ship Achille Lauro and killed a wheelchair-bound American tourist, Leo Klinghoffer. Leo Klinghoffer four hijackers participant took participant contains killed location cruise"
W13-0119,C08-2024,0,0.0757668,"relations between Salzburg and Austrian and Alps and Austrian. SpatialML has no handling, however, for spatial event relations. Likewise, SpRL (Kordjamshidi et al., 2010) represents spatial relations beyond geographic relations, but would have difficulty representing event relations because SpRL requires an indicator (trigger, e.g., in, on, at, to the left of) that is rarely present in spatial event mentions. SpRL does, however, have an annotated corpus (Kordjamshidi et al., 2012) and several automatic approaches have been proposed (Kordjamshidi et al. 2011, Roberts and Harabagiu 2012). STML (Pustejovsky and Moszkowicz, 2008) focuses on the annotation of spatial relations for events, specifically motion events. But their scheme connects a motion event with its motion-specific arguments, and does not include event-event spatial relations. Despite significant work in both event relations and spatial relations, work specific to spatial relations between events has been quite sparse. ISO-Space (Pustejovsky et al. 2011a, Pustejovsky et al. 2011b, Lee et al. 2011) is an on-going effort to develop a detailed annotation system for spatial information (beyond just spatial language). However, no publicly available corpus is"
W13-0119,D10-1048,0,0.113289,"Missing"
W13-0119,roberts-etal-2012-annotating,1,0.903939,"Missing"
W13-0119,S12-1056,1,0.824706,"and Salzburg, and the containment relations between Salzburg and Austrian and Alps and Austrian. SpatialML has no handling, however, for spatial event relations. Likewise, SpRL (Kordjamshidi et al., 2010) represents spatial relations beyond geographic relations, but would have difficulty representing event relations because SpRL requires an indicator (trigger, e.g., in, on, at, to the left of) that is rarely present in spatial event mentions. SpRL does, however, have an annotated corpus (Kordjamshidi et al., 2012) and several automatic approaches have been proposed (Kordjamshidi et al. 2011, Roberts and Harabagiu 2012). STML (Pustejovsky and Moszkowicz, 2008) focuses on the annotation of spatial relations for events, specifically motion events. But their scheme connects a motion event with its motion-specific arguments, and does not include event-event spatial relations. Despite significant work in both event relations and spatial relations, work specific to spatial relations between events has been quite sparse. ISO-Space (Pustejovsky et al. 2011a, Pustejovsky et al. 2011b, Lee et al. 2011) is an on-going effort to develop a detailed annotation system for spatial information (beyond just spatial language)."
W13-0119,S13-2001,0,0.0740421,"Missing"
W13-0119,P05-3021,0,0.0861733,"Missing"
W13-0119,S10-1010,0,\N,Missing
W14-3410,P98-1013,0,0.101624,"incised, the incision was carried, made an incision). Like our approach, they observed sentences can be categorized into actions, perceptions/reports, and other (though we make this distinction at the event mention level). They adapted the Stanford Parser (Klein and Manning, 2003) with the Specialist Lexicon (Browne et al., 1993) similar to Huang et al. (2005). They do not, however, propose any automatic system for recognizing and categorizing actions. Instead, they concentrate on evaluating existing resources. They find that many resources, such as UMLS (Lindberg et al., 1993) and FrameNet (Baker et al., 1998) have poor coverage of surgical actions, while Specialist and WordNet (Fellbaum, 1998) have good coverage. A notable limitation of their work is that they only studied actions at the sentence level, looking at the main verb of the independent clause. We have found in our study that multiple actions can occur within a sentence, and we thus study actions at the event mention level. Wang et al. (2012) noted this shortcoming and provide the following illustrative examples: 2 Previous Work An early tool for processing operative notes was proposed by Lamiell et al. (1993). They develop an auditing t"
W14-3410,J05-1004,0,0.0969818,"ia, she was placed supine on the operating table. The second event mention in the first sentence (administered) and the first event mention in the second sentence (induction) are ignored in Wang et al. (2012)’s study. Despite the fact that they are stated in dependent clauses, these mentions may be more semantically important to the narrative than the mentions in the independent clauses. This is because a grammatical relation does not necessarily imply event prominence. In a further study, Wang et al. (2013) work toward the creation of an automatic extraction system by annotating 69 PropBank (Palmer et al., 2005) style predicateargument structures on thirty common surgical actions. minology for event mentions is fairly limited, resulting in reasonable similarity between surgeons (e.g., the verbal description used for the dividing of the mesoappendix is typically one of the following mentions: fire, staple, divide, separate, remove). 3 Event Structures in Operative Notes Since operations are considered to be one of the riskier forms of clinical treatment, surgeons follow strict procedures that are highly structured and require significant training and oversight. Thus, a surgeon’s description of a parti"
W14-3410,D07-1051,0,0.0341483,"ework for natural language annotation in the biomedical domain (Hahn et al., 2012; Figueroa et al., 2012; Chen et al., 2013a; Chen et al., 2013b). In an active learning setting, instead of performing manual annotation separate from automatic system development, an existing ML classifier is employed to help choose which examples to annotate. Thus, human annotators can focus on examples that would prove difficult for a classifier, which can dramatically reduce overall annotation time. However, active learning is not without pitfalls, notably sampling bias (Dasgupta and Hsu, 2008), re-usability (Tomanek et al., 2007), and class imbalance (Tomanek and Hahn, 2009). In our work, the purpose of utilizing an active learning framework is to produce a fully-annotated corpus of labeled event mentions in as small a period of time as possible. To some extent, the goal of full-annotation alleviates some of the active learning issues discussed above (re-usability and class imbalance), but sampling bias could still lead to significantly longer annotation time. Our goal is to (1) distinguish event mentions in one of the four classes introduced in Section 3.1 F1. F2. F3. F4. Event mention’s lexical form (e.g., identifie"
W14-3410,P03-1054,0,\N,Missing
W14-3410,C98-1013,0,\N,Missing
W98-0720,H91-1033,0,0.0255554,"ering database about commercial air flights, comprising questions of the form: I I I I I I I I i I I I I I I I I I I (QI) Vhich wide-body jets serve dinner? (Q2) Which airlines fly from Boston to Denver? The ATIS domain is characterized by a preestablished formal system of categories and relations onto which the utterances must be mapped. In this domain it is known that only flights fly or serve meals; thus, both (Q1) and (Q2) can only be understood metonymically. The interpretation of the ATT$ utterances is performed in the logical language imposed by the implementation in the DELPHI system (Bobrow et al., 1991). In this framework, a question is translated into a LISP-expression: (wh x S (and (PI x) (P2 x ) ) interpreted as a query for all members of S (the semantic class of the wh-NP) that satisfy both PI (defined as the modifiers of the wh-NP) and P2 (the predicate of the clause). For exemplification, (qLFi) and (0LF2) represent the logical form translations of (QI) and (Q2)t: (QLFI) (whx flights (and (exists y jets (and (aircraft-of x y) (.ide-bodyy)) (serve flight-of x meal-of dinner))) (QLF2) (wh x airlines (exists y flights (and (airline-of y x) (fly flight-of y orig-o~ Boston dest-of Denver)))"
W98-0720,J91-1003,0,0.370914,"Missing"
W98-0720,J95-2003,0,0.0600932,"Missing"
W98-0720,P96-1051,1,0.837249,"of (Q1) brings forward a reasoning path showing that variable k may be coerced to any subsumer of concept person. Such a subsumer is synset {steward, flight attendant}, having the gloss (an attendant on an airplane). This gloss translates the generic relationbetween jets (a hyponym of airplanes) and variable k to the predicate on, cued by the prepositional relation attendant-on-+airplane. The interpretation of this prepositional relation is produced when it is matched against WordNetbased classesof prepositionalattachments collected from large treebanks, followingthe methodology described in (Harabagiu, 1996). For this case, the onprepositional relationattaches the place of work to the worker, thus giving meaning to the coercion of jets into flight attendants. Although the coercions derived in TACITUS do not distinguish the predicative or referential cases, they present a different method of building metonymic paths. By incorporating this unification-based mechanism of producing coercions with a lexical path-finder working on WordNet, a novel way of deriving metonymies is made possible. It has the advantage that it relies only on approximations of sortal knowledge, as indirectly available from the"
W98-0720,H86-1003,0,0.122053,"Missing"
W98-0720,H92-1003,0,0.022472,"Missing"
W98-0720,A88-1032,0,\N,Missing
W98-0720,P93-1012,0,\N,Missing
W99-0104,W97-1306,0,0.126154,"Missing"
W99-0104,W97-0703,0,0.0776452,"Missing"
W99-0104,P87-1022,0,0.0822978,"supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re.arch on corefea~mcea n d discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon&apos;thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). • In the past two MUC competitions, the high scoring systems achieved a recall in the high 50&apos;s to low 60&apos;s and a precision in the low 70&apos;s (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora r"
W99-0104,C88-1021,0,0.103711,"Missing"
W99-0104,W99-0611,0,0.0677801,"Missing"
W99-0104,W98-0720,1,0.880771,"Missing"
W99-0104,J97-1003,0,0.0682566,"require that most frequent senses of Until now, lex/m/cohes/on, arising from semantic nouns be promoted. The same precedence of f~connections between words, was successfully used as quent senses is implemented in the assi~ment of categories, defined as the immediate WordN~ h~ t h e only form of textual cohesive structure, known as pernTpn. The category of proper names is dictated • lez/cd chdn& At present there are three methods by the proper name recognizer, ~qlo~ing such cateof generating lexical chains. The first one, implegories m Person, Organization or mented in the TextTning algorithm (Hearst, 1997), counts the f~lUencies of term repetitions and is an I n this way, coreference between &quot;IBM~ and ~he ideal, lightweight tool for segmenting texts. The secwo,mded computer 9lent ~ can be estab!|~bed, since ond method, adds knowledge from semantic dictiosense 3 of noun #/ant is Organim6on, the category naries (e.g. Roget&apos;s Thesaurus in the work of (Morof ~IBM~. Simi!m- ~tegory-based semaatic cheCkS ris and Hirst, 1991) or WordNet in the methods allow the recognition of the antecedent of proceeds presented in ( B ~ y and Elhadad, 1997), (Hirst from the second example listed in Table 6. The and S"
W99-0104,J91-1002,0,0.154062,"t example from Table 6 is resolved by HTNom, since d/scass/on the nominalization of d/scuss b~_q intentional structure of texts and in the detection of malapropism. The third method is based on a the category communication, a hypernym of subject, path-finding algorithm detailed in (Harabagiu and The antecedent is the object of the verb d/scuss. Moldovan, 1998). This method creates a richer The last heuristic, H9Nom identifies coreferring links with coerced entities of nominals. Coercions SDefiuition introduced in (Halliday and Ha.man, 1976) are obtained as paths of meronyms or hypernyms. and (Morris and Hirst, 1991) 34 0 0 0 @ 0 @ 0 O O 0 @ 0 0 @ 0 0 0 0 0 0 @ 0 @ 0 @ 0 0 0 0 @ 0 0 0 0 0 0 0 0 0 0 0 0 O @ O O O O O O O O O O O @ O O O 0 O O O O O O O @ O 0 O @ O O O O @ O O O O O @ O @ O O @ structure, useful for the al~duction of coherer~e relations from the knowledge encoded in WordNet. Here we describe a new cohesion structure that (a) incorporates both lexical and referential cohesion and (b) produces a unique chain that contains not only single words, but also textual entities encompassing head-adjunct lists. We use the finite-state parses of FaSTU$ (Appelt et al., 1993) for recognizing these entiti"
W99-0104,J98-2001,0,0.074241,"Missing"
W99-0104,A88-1003,0,0.101536,"Missing"
W99-0104,J94-4002,0,0.242819,"Missing"
