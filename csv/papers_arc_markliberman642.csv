2021.bppf-1.1,"Benchmarking: Past, Present and Future",2021,-1,-1,2,0,3453,kenneth church,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future",0,"Where have we been, and where are we going? It is easier to talk about the past than the future. These days, benchmarks evolve more bottom up (such as papers with code). There used to be more top-down leadership from government (and industry, in the case of systems, with benchmarks such as SPEC). Going forward, there may be more top-down leadership from organizations like MLPerf and/or influencers like David Ferrucci, who was responsible for IBM{'}s success with Jeopardy, and has recently written a paper suggesting how the community should think about benchmarking for machine comprehension. Tasks such as reading comprehension become even more interesting as we move beyond English. Multilinguality introduces many challenges, and even more opportunities."
2020.lrec-1.423,A Progress Report on Activities at the {L}inguistic {D}ata {C}onsortium Benefitting the {LREC} Community,2020,-1,-1,6,0,17560,christopher cieri,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This latest in a series of Linguistic Data Consortium (LDC) progress reports to the LREC community does not describe any single language resource, evaluation campaign or technology but sketches the activities, since the last report, of a data center devoted to supporting the work of LREC attendees among other research communities. Specifically, we describe 96 new corpora released in 2018-2020 to date, a new technology evaluation campaign, ongoing activities to support multiple common task human language technology programs, and innovations to advance the methodology of language data collection and annotation."
2020.cllrd-1.1,{L}anguage{ARC}: Developing Language Resources Through Citizen Linguistics,2020,-1,-1,4,1,17563,james fiumara,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',0,"This paper introduces the citizen science platform, LanguageARC, developed within the NIEUW (Novel Incentives and Workflows) project supported by the National Science Foundation under Grant No. 1730377. LanguageARC is a community-oriented online platform bringing together researchers and {``}citizen linguists{''} with the shared goal of contributing to linguistic research and language technology development. Like other Citizen Science platforms and projects, LanguageARC harnesses the power and efforts of volunteers who are motivated by the incentives of contributing to science, learning and discovery, and belonging to a community dedicated to social improvement. Citizen linguists contribute language data and judgments by participating in research tasks such as classifying regional accents from audio clips, recording audio of picture descriptions and answering personality questionnaires to create baseline data for NLP research into autism and neurodegenerative conditions. Researchers can create projects on Language ARC without any coding or HTML required using our Project Builder Toolkit."
W18-3801,"Corpus Phonetics: Past, Present, and Future",2018,-1,-1,1,1,12065,mark liberman,Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing,0,Invited talk
L18-1024,Introducing {NIEUW}: Novel Incentives and Workflows for Eliciting Linguistic Data,2018,0,0,3,0,17560,christopher cieri,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1516,From {`}Solved Problems{'} to New Challenges: A Report on {LDC} Activities,2018,0,0,2,0,17560,christopher cieri,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W16-0308,Exploring Autism Spectrum Disorders Using {HLT},2016,7,7,2,0,24556,julia parishmorris,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,None
L16-1333,Building Language Resources for Exploring Autism Spectrum Disorders,2016,19,4,3,0,24556,julia parishmorris,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Autism spectrum disorder (ASD) is a complex neurodevelopmental condition that would benefit from low-cost and reliable improvements to screening and diagnosis. Human language technologies (HLTs) provide one possible route to automating a series of subjective decisions that currently inform {``}Gold Standard{''} diagnosis based on clinical judgment. In this paper, we describe a new resource to support this goal, comprised of 100 20-minute semi-structured English language samples labeled with child age, sex, IQ, autism symptom severity, and diagnostic classification. We assess the feasibility of digitizing and processing sensitive clinical samples for data sharing, and identify areas of difficulty. Using the methods described here, we propose to join forces with researchers and clinicians throughout the world to establish an international repository of annotated language samples from individuals with ASD and related disorders. This project has the potential to improve the lives of individuals with ASD and their families by identifying linguistic features that could improve remote screening, inform personalized intervention, and promote advancements in clinically-oriented HLTs."
2016.jeptalnrecital-invite.2,From Human Language Technology to Human Language Science,2016,-1,-1,1,1,12065,mark liberman,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. Volume 4 : Conf{\\'e}rences invit{\\'e}es,0,"Thirty years ago, in order to get past roadblocks in Machine Translation and Automatic Speech Recognition, DARPA invented a new way to organize and manage technological R{\&}D : a {``}common task{''} is defined by a formal quantitative evaluation metric and a body of shared training data, and researchers join an open competition to compare approaches. Over the past three decades, this method has produced steadily improving technologies, with many practical applications now possible. And Moore{'}s law has created a sort of digital shadow universe, which increasingly mirrors the real world in flows and stores of bits, while the same improvements in digital hardware and software make it increasingly easy to pull content out of the these rivers and oceans of information. It{'}s natural to be excited about these technologies, where we can see an open road to rapid improvements beyond the current state of the art, and an explosion of near-term commercial applications. But there are some important opportunities in a less obvious direction. Several areas of scientific and humanistic research are being revolutionized by the application of Human Language Technology. At a minimum, orders of magnitude more data can be addressed with orders of magnitude less effort - but this change also transforms old theoretical questions, and poses new ones. And eventually, new modes of research organization and funding are likely to emerge.."
W15-3104,Sentence selection for automatic scoring of {M}andarin proficiency,2015,13,0,6,0,3452,jiahong yuan,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"A central problem in research on automatic proficiency scoring is to differentiate the variability between and within groups of standard and non-standard speakers. Along with the effort to improve the robustness of techniques and models, we can also select test sentences that are more reliable for measuring the between-group variability. This study demonstrated that the performance of an automatic scoring system could be significantly improved by excluding xe2x80x9cbadxe2x80x9d sentences from the scoring procedure. The experiments on a dataset of Putonghua Shuiping Ceshi (Mandarin proficiency test) showed that, compared to all available sentences, using only best-performed sentences improved the speaker-level correlation between human and automatic scores from r = .640 to r = .824."
P14-2109,Parser Evaluation Using Derivation Trees: A Complement to evalb,2014,10,1,6,0,23568,seth kulick,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees. We analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank. This provides some insight into the evalb scores, and the problem of domain adaptation with the web data. We also analyze a xe2x80x9ctest-ontrainxe2x80x9d dataset, showing a wide variance in how the parser is generalizing from different structures in the training material."
cieri-etal-2014-new,New Directions for Language Resource Development and Distribution,2014,12,2,3,0.611853,17560,christopher cieri,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Despite the growth in the number of linguistic data centers around the world, their accomplishments and expansions and the advances they have help enable, the language resources that exist are a small fraction of those required to meet the goals of Human Language Technologies (HLT) for the worldÂs languages and the promises they offer: broad access to knowledge, direct communication across language boundaries and engagement in a global community. Using the Linguistic Data Consortium as a focus case, this paper sketches the progress of data centers, summarizes recent activities and then turns to several issues that have received inadequate attention and proposes some new approaches to their resolution."
N13-1083,A Cross-language Study on Automatic Speech Disfluency Detection,2013,12,3,4,0,13014,wen wang,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,We investigate two systems for automatic disfluency detection on English and Mandarin conversational speech data. The first system combines various lexical and prosodic features in a Conditional Random Field model for detecting edit disfluencies. The second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition. We compare the contributions of different knowledge sources to detection performance between these two languages.
cieri-etal-2012-twenty,Twenty Years of Language Resource Development and Distribution: A Progress Report on {LDC} Activities,2012,11,1,4,0.692338,17560,christopher cieri,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"On the Linguistic Data Consortium's (LDC) 20th anniversary, this paper describes the changes to the language resource landscape over the past two decades, how LDC has adjusted its practice to adapt to them and how the business model continues to grow. Specifically, we will discuss LDC's evolving roles and changes in the sizes and types of LDC language resources (LR) as well as the data they include and the annotations of that data. We will also discuss adaptations of the LDC business model and the sponsored projects it supports."
cieri-liberman-2010-adapting,Adapting to Trends in Language Resource Development: A Progress Report on {LDC} Activities,2010,4,1,2,0.724502,17560,christopher cieri,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes changing needs among the communities that exploit language resources and recent LDC activities and publications that support those needs by providing greater volumes of data and associated resources in a growing inventory of languages with ever more sophisticated annotation. Specifically, it covers the evolving role of data centers with specific emphasis on the LDC, the publications released by the LDC in the two years since our last report and the sponsored research programs that provide LRs initially to participants in those programs but eventually to the larger HLT research communities and beyond."
J10-4001,{O}bituary: Fred Jelinek,2010,-1,-1,1,1,12065,mark liberman,Computational Linguistics,0,None
D10-1071,A New Approach to Lexical Disambiguation of {A}rabic Text,2010,19,7,3,0,2922,rushin shah,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We describe a model for the lexical analysis of Arabic text, using the lists of alternatives supplied by a broad-coverage morphological analyzer, SAMA, which include stable lemma IDs that correspond to combinations of broad word sense categories and POS tags. We break down each of the hundreds of thousands of possible lexical labels into its constituent elements, including lemma ID and part-of-speech. Features are computed for each lexical token based on its local and document-level context and used in a novel, simple, and highly efficient two-stage supervised machine learning algorithm that overcomes the extreme sparsity of label distribution in the training data. The resulting system achieves accuracy of 90.6% for its first choice, and 96.2% for its top two choices, in selecting among the alternatives provided by the SAMA lexical analyzer. We have successfully used this system in applications such as an online reading helper for intermediate learners of the Arabic language, and a tool for improving the productivity of Arabic Treebank annotators."
W09-0102,The Annotation Conundrum,2009,0,3,1,1,12065,mark liberman,"Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",0,"Without lengthy, iterative refinement of guidelines, and equally lengthy and iterative training of annotators, the level of inter-subjective agreement on simple tasks of phonetic, phonological, syntactic, semantic, and pragmatic annotation is shockingly low.n n This is a significant practical problem in speech and language technology, but it poses questions of interest to psychologists, philosophers of language, and theoretical linguists as well."
cieri-liberman-2008-15,15 Years of Language Resource Creation and Sharing: a Progress Report on {LDC} Activities,2008,1,5,2,0.807435,17560,christopher cieri,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper, the fifth in a series of biennial progress reports, reviews the activities of the Linguistic Data Consortium with particular emphasis on general trends in the language resource landscape and on changes that distinguish the two years since LDCÂs last report at LREC from the preceding 8 years. After providing a perspective on the current landscape of language resources, the paper goes on to describe our vision of the role of LDC within the research communities it serves before sketching briefly specific publications and resources creations projects that have been the focus our attention since the last report."
W06-2919,A Context Pattern Induction Method for Named Entity Extraction,2006,14,70,3,0,8056,partha talukdar,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"We present a novel context pattern induction method for information extraction, specifically named entity extraction. Using this method, we extended several classes of seed entity lists into much larger high-precision lists. Using token membership in these extended lists as additional features, we improved the accuracy of a conditional random field-based named entity tagger. In contrast, features derived from the seed lists decreased extractor accuracy."
cieri-etal-2006-mixer,"The Mixer and Transcript Reading Corpora: Resources for Multilingual, Crosschannel Speaker Recognition Research",2006,3,22,7,0.974026,17560,christopher cieri,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the planning and creation of the Mixer and Transcript Reading corpora, their properties and yields, and reports on the lessons learned during their development."
cieri-liberman-2006-data,More Data and Tools for More Languages and Research Areas: A Progress Report on {LDC} Activities,2006,5,5,2,0.974026,17560,christopher cieri,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This presentation reports on recent progress the Linguistic Data Consortium has made in addressing the needs of multiple research communities by collecting, annotating and distributing, simplifying access and developing standards and tools. Specifically, it describes new trends in publication, a sample of recent projects and significant improvements to LDC Online that improve access to LDC data especially for those with limited computing support."
strassel-etal-2006-integrated,Integrated Linguistic Resources for Language Exploitation Technologies,2006,3,9,5,0,14744,stephanie strassel,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Linguistic Data Consortium has recently embarked on an effort to create integrated linguistic resources and related infrastructure for language exploitation technologies within the DARPA GALE (Global Autonomous Language Exploitation) Program. GALE targets an end-to-end system consisting of three major engines: Transcription, Translation and Distillation. Multilingual speech or text from a variety of genres is taken as input and English text is given as output, with information of interest presented in an integrated and consolidated fashion to the end user. GALE's goals require a quantum leap in the performance of human language technology, while also demanding solutions that are more intelligent, more robust, more adaptable, more efficient and more integrated. LDC has responded to this challenge with a comprehensive approach to linguistic resource development designed to support GALE's research and evaluation needs and to provide lasting resources for the larger Human Language Technology community."
W04-3111,Integrated Annotation for Biomedical Information Extraction,2004,19,156,3,0,23568,seth kulick,"{HLT}-{NAACL} 2004 Workshop: Linking Biological Literature, Ontologies and Databases",0,"We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics. We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities. Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process."
cieri-liberman-2004-progress,A Progress Report from the {L}inguistic {D}ata {C}onsortium: Recent Activities in Resource Creation and Distribution and the Development of Tools and Standards,2004,3,2,2,0.974026,17560,christopher cieri,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper described recent activities of the Linguistic Data Consortium in the collection, annotation and distribution of language data the developments of tools and standards for using that data, the creation of metadata to facilitate the search for linguistic resources."
cieri-liberman-2002-language,Language Resource Creation and Distribution at the {L}inguistic {D}ata {C}onsortium: A Progress Report,2002,8,0,2,1,17560,christopher cieri,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
cieri-liberman-2002-tides,{TIDES} Language Resources: A Resource Map for Translingual Information Access,2002,7,7,2,1,17560,christopher cieri,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
bird-etal-2000-atlas,{ATLAS}: A Flexible and Extensible Architecture for Linguistic Annotation,2000,8,94,6,0.869565,8953,steven bird,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We describe a formal model for annotating linguistic artifacts, from which we derive an application programming interface (API) to a suite of tools for manipulating these annotations. The abstract logical model provides for a range of storage formats and promotes the reuse of tools that interact through this API. We focus first on xe2x80x9cAnnotation Graphs,xe2x80x9d a graph model for annotations on linear signals (such as text and speech) indexed by intervals, for which efficient database storage and querying techniques are applicable. We note how a wide range of existing annotated corpora can be mapped to this annotation graph model. This model is then generalized to encompass a wider variety of linguistic xe2x80x9csignals,xe2x80x9d including both naturally occuring phenomena (as recorded in images, video, multi-modal interactions, etc.), as well as the derived resources that are increasingly important to the engineering of natural language processing systems (such as word lists, dictionaries, aligned bilingual corpora, etc.). We conclude with a review of the current efforts towards implementing key pieces of this architecture."
cieri-liberman-2000-issues,Issues in Corpus Creation and Distribution: The Evolution of the {L}inguistic {D}ata {C}onsortium,2000,7,7,2,1,17560,christopher cieri,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
cieri-etal-2000-large,"Large, Multilingual, Broadcast News Corpora for Cooperative Research in Topic Detection and Tracking: The {TDT}-2 and {TDT}-3 Corpus Efforts",2000,4,18,3,1,17560,christopher cieri,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper describes the creation and content two corpora, TDT-2 and TDT-3, created for the DARPA sponsored Topic Detection and Tracking project. The research goal in the TDT program is to create the core technology of a news understanding system that can process multilingual news content categorizing individual stories according to the topic(s) they describe. The research tasks include segmentation of the news streams into individual stories, detection of new topics, identification of the first story to discuss any topic, tracking of all stories on selected topics and detection of links among stories discussing the same topics. The corpora contain English and Chinese broadcast television and radio, newswires, and text from web sites devoted to news. For each source there are texts or text intermediaries; for the broadcast stories the audio is also available. Each broadcast is also segment to show start and end times of all news stories. LDC staff have defined news topics in the corpora and annotated each story to indicate its relevance to each topic. The end products are massive, richly annotated corpora available to support research and development in information retrieval, topic detection and tracking, information extraction message understanding directly or after additional annotation. This paper will describe the corpora created for TDT including sources, collection processes, formats, topic selection and definition, annotation, distribution and project management for large corpora."
W99-0301,Annotation Graphs as a Framework for Multidimensional Linguistic Data Analysis,1999,4,48,2,0.869565,8953,steven bird,Towards Standards and Tools for Discourse Tagging,0,None
1999.mtsummit-1.79,{BITS}: a method for bilingual text search over the Web,1999,10,81,2,0,37104,xiaoyi ma,Proceedings of Machine Translation Summit VII,0,"Parallel corpus are valuable resource for machine translation, multi-lingual text retrieval, language education and other applications, but for various reasons, its availability is very limited at present. Noticed that the World Word Web is a potential source to mine parallel text, researchers are making their efforts to explore the Web in order to get a big collection of bitext. This paper presents BITS (Bilingual Internet Text Search), a system which harvests multilingual texts over the World Wide Web with virtually no human intervention. The technique is simple, easy to port to any language pairs, and with high accuracy. The results of the experiments on German-English pair proved that the method is very successful."
J94-3002,Commentary on {K}aplan and {K}ay,1994,2,2,1,1,12065,mark liberman,Computational Linguistics,0,None
H94-1004,Lexicons for Human Language Technology,1994,0,1,1,1,12065,mark liberman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"Information about words---their pronunciation, syntax and meaning---is a crucial and costly part of human language technology. Many questions remain about the best way to express and use such lexical information. Nevertheless, much of this information is common to all current approaches, and therefore the effort to collect it can usefully be shared. The Linguistic Data Consortium (LDC) has undertaken to provide such common lexical information for the community of HLT researchers. The purpose of this paper is to sketch the various LDC lexical projects now underway or planned, and to solicit feedback from the community of HLT researchers."
H92-1070,Session 1{O}b: Core {NL} Lexicon and Grammar,1992,-1,-1,1,1,12065,mark liberman,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
H91-1002,Session 1: Speech and Natural Language Efforts in the {U. S.} and Abroad,1991,0,0,1,1,12065,mark liberman,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"We see two purposes for this first session: increased communications among research communities in some danger of drifting apart, and a comparison of alternative goals and organizational structures for such communities. Obviously, a single hour-long session is no more than a symbolic gesture in this direction, even ff the time had not been truncated further by schedule overruns pressing against an inflexible dinner hour, but we feel that the symbol was nevertheless a worthwhile and important one."
C90-3049,A Finite-State Morphological Processor for {S}panish,1990,10,37,2,0,4939,evelyne tzoukermann,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"A finite transducer that processes Spanish inflectional and derivational morphology is presented. The system handles both generation and analysis of tens of millions inflected forms. Lexical and surface (orthographic) representations of the words are linked by a program that interprets a finite directed graph whose arcs are labelled by n-tuples of strings. Each of about 55,000 base forms requires at least one are in the graph. Representing the inflectional and derivational possibilities for these forms imposed an overhead of only about 3000 additional arcs, of which about 2500 represent (phonologically predictable) stem allomorphy, so that we pay a storage price of about 5% for compiling these forms offline. A simple interpreter for the resulting automaton processes several hundred words per second on a Sun4."
H89-2024,Text on Tap: the {ACL}/{DCI},1989,0,15,1,1,12065,mark liberman,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"There has been a recent upsurge of interest in computational studies of large bodies of text. The aim of such studies varies widely, from lexicography and studies of language change to automatic indexing methods and statistical models for improving the performance of speech recognition systems and optical character readers. In general, corpus-based studies are critical for the development of adequate models of linguistic structure and for insights into the nature of language use. However, research workers have been severely hampered by the lack of appropriate materials, and specifically by the lack of a large enough body of text on which published results can be replicated or extended by others."
P87-1020,Toward Treating {E}nglish Nominals Correctly,1987,12,8,2,0,6614,richard sproat,25th Annual Meeting of the Association for Computational Linguistics,1,We describe a program for assigning correct stress contours to nominals in English. It makes use of idiosyncratic knowledge about the stress behavior of various nominal types and general knowledge about English stress rules. We have also investigated the related issue of parsing complex nominals in English. The importance of this work and related research to the problem of text-to-speech is discussed.
P86-1026,Questions about Connectionist Models of Natural Language,1986,0,1,1,1,12065,mark liberman,24th Annual Meeting of the Association for Computational Linguistics,1,None
