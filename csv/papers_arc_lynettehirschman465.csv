W10-1111,Measuring Risk and Information Preservation: Toward New Metrics for De-identification of Clinical Texts,2010,15,5,1,1,45475,lynette hirschman,Proceedings of the {NAACL} {HLT} 2010 Second Louhi Workshop on Text and Data Mining of Health Documents,0,"Current metrics for de-identification are based on information extraction metrics, and do not address the real-world questions how good are current systems, and how good do they need to be. Metrics are needed that quantify both the risk of re-identification and information preservation. We review the challenges in de-identifying clinical texts and the current metrics for assessing clinical de-identification systems. We then introduce three areas to explore that can lead to metrics that quantify re-identification risk and information preservation."
N04-3004,{M}i{TAP} for {SARS} Detection,2004,5,8,5,1,51857,laurie damianos,Demonstration Papers at {HLT}-{NAACL} 2004,0,"The MiTAP prototype for SARS detection uses human language technology for detecting, monitoring, and analyzing potential indicators of infectious disease outbreaks and reasoning for issuing warnings and alerts. MiTAP focuses on providing timely, multilingual information access to analysts, domain experts, and decision-makers worldwide. Data sources are captured, filtered, translated, summarized, and categorized by content. Critical information is automatically extracted and tagged to facilitate browsing, searching, and scanning, and to provide key terms at a glance. The processed articles are made available through an easy-to-use news server and cross-language information retrieval system for access and analysis anywhere, any time. Specialized newsgroups and customizable filters or searches on incoming stories allow users to create their own view into the data while a variety of tools summarize, indicate trends, and provide alerts to potentially relevant spikes of activity."
W03-1301,Gene Name Extraction Using {F}ly{B}ase Resources,2003,13,39,2,0,52683,alex morgan,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"Machine-learning based entity extraction requires a large corpus of annotated training to achieve acceptable results. However, the cost of expert annotation of relevant data, coupled with issues of inter-annotator variability, makes it expensive and time-consuming to create the necessary corpora. We report here on a simple method for the automatic creation of large quantities of imperfect training data for a biological entity (gene or protein) extraction system. We used resources available in the FlyBase model organism database; these resources include a curated lists of genes and the articles from which the entries were drawn, together a synonym lexicon. We applied simple pattern matching to identify gene names in the associated abstracts and filtered these entities using the list of curated entries for the article. This process created a data set that could be used to train a simple Hidden Markov Model (HMM) entity tagger. The results from the HMM tagger were comparable to those reported by other groups (F-measure of 0.75). This method has the advantage of being rapidly transferable to new domains that have similar existing resources."
W01-1607,Comparing Several Aspects of Human-Computer and Human-Human Dialogues,2001,16,34,4,0,42791,christine doran,Proceedings of the Second {SIG}dial Workshop on Discourse and Dialogue,0,"While researchers have many intuitions about the differences between human-computer and human-human interactions, most of these have not previously been subject to empirical scrutiny. This work presents some initial experiments in this direction, with the ultimate goal being to use what we learn to improve computer dialogue systems. Working with data from the air travel domain, we identified a number of striking differences between the human-human and human-computer interactions."
H01-1028,Finding Errors Automatically in Semantically Tagged Dialogues,2001,7,6,5,1,24700,john aberdeen,Proceedings of the First International Conference on Human Language Technology Research,0,"We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags. We examined 27 HC dialogues from the DARPA Communicator air travel domain, comparing user inputs to system responses to look for slot value discrepancies, both automatically and manually. For the automatic method, we labeled the dialogues with semantic tags corresponding to slots that would be filled in frames in the course of the travel task. We then applied an automatic algorithm to detect errors in the dialogues. The same dialogues were also manually tagged (by a different annotator) to label errors directly. An analysis of the results of the two tagging methods indicates that it may be possible to detect errors automatically in this way, but our method needs further work to reduce the number of false errors detected. Finally, we present a discussion of the differing results from the two tagging methods."
H01-1038,"Integrated Feasibility Experiment for Bio-Security: {IFE}-Bio, A {TIDES} Demonstration",2001,3,3,1,1,45475,lynette hirschman,Proceedings of the First International Conference on Human Language Technology Research,0,"As part of MITRE's work under the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) program, we are preparing a series of demonstrations to showcase the TIDES Integrated Feasibility Experiment on Bio-Security (IFE-Bio). The current demonstration illustrates some of the resources that can be made available to analysts tasked with monitoring infectious disease outbreaks and other biological threats."
walker-etal-2000-evaluation,Evaluation for Darpa Communicator Spoken Dialogue Systems,2000,17,30,2,0,6000,marilyn walker,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The overall objective of the DARPA COMMUNICATOR project is to support rapid, cost-effective development of multi-modal speechenabled dialogue systems with advanced conversational capabilities, such as plan optimization, explanation and negotiation. In order to make this a reality, we need to find methods for evaluating the contribution of various techniques to the usersxe2x80x99 willingness and ability to use the system. This paper reports on the approach to spoken dialogue system evaluation that we are applying in the COMMUNICATOR program. We describe our overall approach, the experimental design, the logfile standard, and the metrics applied in the experimental evaluation planned for June of 2000."
breck-etal-2000-evaluate,How to Evaluate Your Question Answering System Every Day ... and Still Get Real Work Done,2000,12,16,4,0,47810,eric breck,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper, we report on Qaviar, an experimental automated evaluation system for question answering applications. The goal of our research was to find an automatically calculated measure that correlates well with human judges' assessment of answer correctness in the context of question answering tasks. Qaviar judges the response by computing recall against the stemmed content words in the human-generated answer key. It counts the answer correct if it exceeds agiven recall threshold. We determined that the answer correctness predicted by Qaviar agreed with the human 93% to 95% of the time. 41 question-answering systems were ranked by both Qaviar and human assessors, and these rankings correlated with a Kendall's Tau measure of 0.920, compared to a correlation of 0.956 between human assessors on the same data."
damianos-etal-2000-evaluating,Evaluating Multi-party Multi-modal Systems,2000,12,6,4,1,51857,laurie damianos,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The MITRE Corporationxe2x80x99s Evaluation Working Group has developed a methodology for evaluating multi-modal groupware systems and capturing data on human-human interactions. The methodology consists of a framework for describing collaborative systems, a scenario-based evaluation approach, and evaluation metrics for the various components of collaborative systems. We designed and ran two sets of experiments to validate the methodology by evaluating collaborative systems. In one experiment, we compared two configurations of a multi-modal collaborative application using a map navigation scenario requiring information sharing and decision making. In the second experiment, we applied the evaluation methodology to a loosely integrated set of collaborative tools, again using a scenario-based approach. In both experiments, multi-modal, multi-user data were collected, visualized, annotated, and"
P99-1042,Deep Read: A Reading Comprehension System,1999,9,163,1,1,45475,lynette hirschman,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes initial work on Deep Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it. We have acquired a corpus of 60 development and 60 test stories of 3rd to 6th grade material; each story is followed by short-answer questions (an answer key was also provided). We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution). This simple system retrieves the sentence containing the answer 30--40% of the time."
E99-1011,The {TIPSTER} {SUMMAC} Text Summarization Evaluation,1999,14,173,4,0,42626,inderjeet mani,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,The TIPSTER Text Summarization Evaluation (SUMMAC) has established definitively that automatic text summarization is very effective in relevance assessment tasks. Summaries as short as 17% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in F-score accuracy. SUMMAC has also introduced a new intrinsic method for automated evaluation of informative summaries.
P98-1031,Named Entity Scoring for Speech Input,1998,11,15,3,1,41044,john burger,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"This paper describes a new scoring algorithm that supports comparison of linguistically annotated data from noisy sources. The new algorithm generalizes the Message Understanding Conference (MUC) Named Entity scoring algorithm, using a comparison based on explicit alignment of the underlying texts, followed by a scoring phase. The scoring procedure maps corresponding tagged regions and compares these according to tag type and tag extent, allowing us to reproduce the MUC Named Entity scoring for identical underlying texts. In addition, the new algorithm scores for content (transcription correctness) of the tagged region, a useful distinction when dealing with noisy data that may differ from a reference transcription (e.g., speech recognizer output). To illustrate the algorithm, we have prepared a small test data set consisting of a careful transcription of speech data and manual insertion of SGML named entity annotation. We report results for this small test corpus on a variety of experiments involving automatic speech recognition and named entity tagging."
M98-1029,Appendix {F}: {MUC}-7 Coreference Task Definition (version 3.0),1998,0,83,1,1,45475,lynette hirschman,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,None
C98-1031,Named Entity Scoring for Speech Input,1998,11,15,3,1,41044,john burger,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"This paper describes a new scoring algorithm that supports comparison of linguistically annotated data from noisy sources. The new algorithm generalizes the Message Understanding Conference (MUC) Named Entity scoring algorithm, using a comparison based on explicit alignment of the underlying texts, followed by a scoring phase. The scoring procedure maps corresponding tagged regions and compares these according to tag type and tag extent, allowing us to reproduce the MUC Named Entity scoring for identical underlying texts. In addition, the new algorithm scores for content (transcription correctness) of the tagged region, a useful distinction when dealing with noisy data that may differ from a reference transcription (e.g., speech recognizer output). To illustrate the algorithm, we have prepared a small test data set consisting of a careful transcription of speech data and manual insertion of SGML named entity annotation. We report results for this small test corpus on a variety of experiments involving automatic speech recognition and named entity tagging."
A97-1051,Mixed-Initiative Development of Language Processing Systems,1997,6,146,3,1,47358,david day,Fifth Conference on Applied Natural Language Processing,0,"Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to bootstrapping the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate named entities demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domain specific annotation rules that can be used to annotate similar texts automatically through the Alembic-NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session."
X96-1053,{MITRE}: Description of the {A}lembic System as Used in {MET},1996,2,14,4,1,24700,john aberdeen,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"Alembic is a comprehensive information extraction system that has been applied to a range of tasks. These include the now-standard components of the formal MUC evaluations: name tagging (NE in MUC-6), name normalization (TE), and template generation (ST). The system has also been exploited to help segment and index broadcast video and was used for early experiments on variants of the co-reference identification task. (For details, see [1].)"
M95-1005,A Model-Theoretic Coreference Scoring Scheme,1995,0,452,5,0,47056,marc vilain,"Sixth Message Understanding Conference ({MUC}-6): Proceedings of a Conference Held in {C}olumbia, {M}aryland, November 6-8, 1995",0,This note describes a scoring scheme for the coreference task in MUC6. It improves on the original approach by: (1) grounding the scoring scheme in terms of a model; (2) producing more intuitive recall and precision scores; and (3) not requiring explicit computation of the transitive closure of coreference. The principal conceptual difference is that we have moved from a syntactic scoring model based on following coreference links to an approach defined by the model theory of those links.
M95-1012,{MITRE}: Description of the {A}lembic System Used for {MUC}-6,1995,10,121,4,1,24700,john aberdeen,"Sixth Message Understanding Conference ({MUC}-6): Proceedings of a Conference Held in {C}olumbia, {M}aryland, November 6-8, 1995",0,"As with several other veteran MUC participants, MITRE's Alembic system has undergone a major transformation in the past two years. The genesis of this transformation occurred during a dinner conversation at the last MUC conference, MUC-5. At that time, several of us reluctantly admitted that our major impediment towards improved performance was reliance on then-standard linguistic models of syntax. We knew we would need an alternative to traditional linguistic grammars, even to the somewhat non-traditional categorial pseudo-parser we had in place at the time. The problem was, which alternative?"
H94-1017,Session 3: Human Language Evaluation,1994,8,0,1,1,45475,lynette hirschman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"* Cross-system evaluation: This is a mainstay of the periodic ARPA evaluations on competing systems. Multiple sites agree to run their respective systems on a single application, so that results across systems are comparable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spoken language systems (ATIS)[8], and automated speech recognition (CSR)[8]."
J93-3001,Evaluating Message Understanding Systems: An Analysis of the Third {M}essage {U}nderstanding {C}onference ({MUC}-3),1993,18,129,2,0,49841,nancy chinchor,Computational Linguistics,0,"This paper describes and analyzes the results of the Third Message Understanding Conference (MUC-3). It reviews the purpose, history, and methodology of the conference, summarizes the participating systems, discusses issues of measuring system effectiveness, describes the linguistic phenomena tests, and provides a critical look at the evaluation in terms of the lessons learned. One of the common problems with evaluations is that the statistical significance of the results is unknown. In the discussion of system performance, the statistical significance of the evaluation results is reported and the use of approximate randomization to calculate the statistical significance of the results of MUC-3 is described."
H93-1095,Spoken Language Recognition and Understanding,1993,-1,-1,2,0.73116,45080,victor zue,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
M92-1005,An Adjunct Test for Discourse Processing in {MUC}-4,1992,0,9,1,1,45475,lynette hirschman,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,"The motivation for this adjunct test came from an exploratory study done by Beth Sundheim during MUC-3. This study showed a degradation in correctness of message processing as the information distribution in the message became more complex, that is, as slot fills were drawn from larger portions of the message and required more discourse processing to extract the information and reassemble it correctly in the required template(s). The study also suggested that systems did worse on messages requiring multiple templates than on single-template messages. These observations led us define the MUC-4 adjunct test to examine two hypotheses related to discourse complexity and expected system performance:xe2x80xa2 The Source Complexity HypothesisThe more complex the distribution of the source information for filling a given slot or template (the more sentences, and the more widely separated the sentences), the more difficult it will be to process the message correctly.xe2x80xa2The Output Complexity HypothesisThe more complex the output (in terms of number of templates), the harder it will be to process the message correctly."
H92-1003,Multi-Site Data Collection for a Spoken Language Corpus,1992,13,117,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"This paper describes a recently collected spoken language corpus for the ATIS (Air Travel Information System) domain. This data collection effort has been co-ordinated by MADCOW (Multi-site ATIS Data COllection Working group). We summarize the motivation for this effort, the goals, the implementation of a multi-site data collection paradigm, and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech, natural language and spoken language."
H92-1005,Experiments in Evaluating Interactive Spoken Language Systems,1992,4,37,2,0,45985,joseph polifroni,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"As the DARPA spoken language community moves towards developing useful systems for interactive problem solving, we must explore alternative evaluation procedures that measure whether these systems aid people in solving problems within the task domain. In this paper, we describe several experiments exploring new evaluation procedures. To look at end-to-end evaluation, we modified our data collection procedure slightly in order to experiment with several objective task completion measures. We found that the task completion time is well correlated with the number of queries used. We also explored log file evaluation, where evaluators were asked to judge the clarity of the query and the correctness of the response based on examination of the log file. Our results show that seven evaluators were unanimous on more than 80% of the queries, and that at least 6 out of 7 evaluators agreed over 90% of the time. Finally, we applied these new procedures to compare two systems, one system requiring a complete parse and the other using the more flexible robust parsing mechanism. We found that these metrics could distinguish between these systems: there were significant differences in ability to complete the task, number of queries required to complete the task, and score (as computed through a log file evaluation) between the robust and the non-robust modes."
H92-1006,Subject-Based Evaluation Measures for Interactive Spoken Language Systems,1992,13,25,2,0,54006,patti price,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The DARPA Spoken Language effort has profited greatly from its emphasis on tasks and common evaluation metrics. Common, standardized evaluation procedures have helped the community to focus research effort, to measure progress, and to encourage communication among participating sites. The task and the evaluation metrics, however, must be consistent with the goals of the Spoken Language program, namely interactive problem solving. Our evaluation methods have evolved with the technology, moving from evaluation of read speech from a fixed corpus through evaluation of isolated canned sentences to evaluation of spontaneous speech in context in a canned corpus. A key component missed in current evaluations is the role of subject interaction with the system. Because of the great variability across subjects, however, it is necessary to use either a large number of subjects or a within-subject design. This paper proposes a within-subject design comparing the results of a software-sharing exercise carried out jointly by MIT and SRI."
H92-1016,The {MIT} {ATIS} System: {F}ebruary 1992 Progress Report,1992,9,16,5,0.73116,45080,victor zue,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"This paper describes the status of the MIT ATIS system as of February 1992, focusing especially on the changes made to the SUMMIT recognizer. These include context-dependent phonetic modelling, the use of a bigram language model in conjunction with a probabilistic LR parser, and refinements made to the lexicon. Together with the use of a larger training set, these modifications combined to reduce the speech recognition word and sentence error rates by a factor of 2.5 and 1.6, respectively, on the October '91 test set. The weighted error for the entire spoken language system on the same test set is 49.3%. Similar results were also obtained on the February '92 benchmark evaluation."
H92-1107,Spoken Language Recognition and Understanding,1992,0,1,2,0.73116,45080,victor zue,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The goal of this research is to demonstrate spoken language systems in support of interactive problem solving. The system accepts continuous speech input and handles multiple speakers without explicit speaker enrollment. The MIT spoken language system combines SUMMIT, a segment-based speech recognition system, and TINA, a probabilistic natural language system, to achieve speech understanding. The system engages in interactive dialogue with the user, providing output in the form of tabular displays, as well as spoken and written output. The system has been demonstrated on several applications, including travel planning and direction assistance."
M91-1003,Comparing {MUCK}-{II} and {MUC}-3: Assessing the Difficulty of Different Tasks,1991,0,12,1,1,45475,lynette hirschman,"{T}hird {M}essage {U}understanding {C}onference ({MUC}-3): Proceedings of a Conference Held in {S}an {D}iego, {C}alifornia, {M}ay 21-23, 1991",0,"The natural language community has made impressive progress in evaluation over the last four years. However, as the evaluations become more sophisticated and more ambitious, a fundamental problem emerges: how to compare results across changing evaluation paradigms. When we change domain, task, and scoring procedures, as has been the case from MUCK-I to MUCK-II to MUC-3, we lose comparability of results. This makes it difficult to determine whether the field has made progress since the last evaluation. Part of the success of the MUC conferences has been due to the incremental approach taken to system evaluation. Over the four year period of the three conferences, the domain has become more realistic, the task has become more ambitious and specified in much greater detail, and the scoring procedures have evolved to provide a largely automated scoring mechanism. This process has been critical to demonstrating the utility of the overall evaluation process. However we still need some way to assess overall progress of the field, and thus we need to compare results and task difficulty of MUC-3 relative to MUCK-II."
H91-1014,Development and Preliminary Evaluation of the {MIT} {ATIS} System,1991,8,17,5,0,36748,stephanie seneff,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"This paper represents a status report on the MIT ATIS system. The most significant new achievement is that we now have a speech-input mode. It is based on the MIT SUMMIT system using context independent phone models, and includes a word-pair grammar with perplexity 92 (on the June-90 test set). In addition, we have completely redesigned the back-end component, in order to emphasize portability and extensibility. The parser now produces an intermediate semantic frame representation, which serves as the focal point for all back-end operations, such as history management, text generation, and SQL query generation. Most of those aspects of the system that are tied to a particular domain are now entered through a set of tables associated with a small artificial language for decoding them. We have also improved the display of the database table, making it considerably easier for a subject to comprehend the information given. We report here on the results of the official DARPA February-91 evaluation, as well as on results of an evaluation on data collected at MIT, for both speech input and text input."
H91-1070,Interactive Problem Solving and Dialogue in the {ATIS} Domain,1991,4,40,2,0,36748,stephanie seneff,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"This paper describes the present status of the discourse and dialogue models within the MIT ATIS system, extended to support the notion of booking a flight. The discourse model includes not only the resolution of explicit anaphoric references, but also indirect and direct references to information mentioned earlier in the conversation, such as a direct reference to an entry in a previously displayed table or an indirect reference to a date, as in the following Thursday. The system keeps a history table containing objects such as flights and dates, represented as semantic frames, as well as the active ticket, previously booked tickets, and previously displayed tables. During flight reservations scenarios, the system monitors the state of the ticket (which is displayed to the user), making sure that all information is complete (by querying the user) before allowing a booking. It may even initiate calls to the database to provide additional unsolicited information as appropriate. We have collected several dialogues of subjects using the system to make reservations, and from these, we are learning how to design better dialogue models."
H91-1072,Integrating Syntax and Semantics into Spoken Language Understanding,1991,11,2,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"This paper describes several experiments combining natural language and acoustic constraints to improve overall performance of the MIT VOYAGER spoken language system. This system couples the SUMMIT speech recognition system with the TINA language understanding system to answer spoken queries about navigational assistance in the Cambridge, MA, area. The overall goal of our research is to combine acoustic, syntactic and semantic knowledge sources. Our first experiment showed improvement by combining acoustic score and parse probability normalized for number of terminals. Results were further improved by the use of an explicit rejection criterion based on normalized parse probabilities. The use of the combined parse/acoustic score, together with the rejection criterion, gave an improvement in overall score of more than 33% on both training and test data, where score is defined as percent correct minus percent incorrect. Experiments on a fully integrated system which uses the parser to predict possible next words to the recognizer are now underway."
H91-1090,Spoken Language Recognition and Understanding,1991,-1,-1,2,0.673163,45080,victor zue,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,None
H90-1013,Session 3: Natural Language Evaluation,1990,0,0,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,None
H90-1023,Beyond Class A: A Proposal for Automatic Evaluation of Discourse,1990,6,28,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The DARPA Spoken Language community has just completed the first trial evaluation of spontaneous query/response pairs in the Air Travel (ATIS) domain.1 Our goal has been to find a methodology for evaluating correct responses to user queries. To this end, we agreed, for the first trial evaluation, to constrain the problem in several ways:Database Application: Constrain the application to a database query application, to ease the burden of a) constructing the back-end, and b) determining correct responses;"
H90-1030,Management and Evaluation of Interactive Dialog in the Air Travel Domain,1990,4,15,4,0.833333,55258,lewis norton,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"This paper presents the Unisys Spoken Language System, as applied to the Air Travel Planning (ATIS) domain. This domain provides a rich source of interactive dialog, and has been chosen as a common application task for the development and evaluation of spoken language understanding systems. The Unisys approach to developing a spoken language system combines SUMMIT (the MIT speech recognition system [6]), PUNDIT (the Unisys language understanding system [3]) and an Ingres database of air travel information for eleven cities and nine airports (the ATIS database). Access to the database is mediated via a general knowledge-base/database interface (the Intelligent Database Server [4]). To date, we have concentrated on the language understanding and database interface components."
H90-1044,Training and Evaluation of a Spoken Language Understanding System,1990,15,11,2,0.641026,55257,deborah dahl,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,This paper describes our results on a spoken language application for finding directions. The spoken language system consists of the MIT SUMMIT speech recognition system ([20]) loosely coupled to the UNISYS PUNDIT language understanding system ([9]) with SUMMIT providing the top N candidates (based on acoustic score) to the PUNDIT system. The direction finding capability is provided by an expert system which is also part of the MIT VOYAGER system [18]).
H90-1098,Project Summary: Linguistic Knowledge Sources for Spoken Language Understanding,1990,0,0,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The objective of the Unisys Spoken Language Systems effort is to develop and demonstrate technology for the understanding of goal-directed spontaneous speech input. The Unisys spoken language architecture couples a speech recognition system (the MIT Summit system) with the Unisys discourse understanding system Pundit. Pundit is a broad-coverage language understanding system used in a variety of message understanding applications and extended to handle spoken language input. Its power comes from the integration of syntax, semantics and pragmatics (context), the ability to port rapidly to new task domains, and from an open, modular architecture. Pundit is unique in its ability to handle connected discourse; it includes a reference resolution module that tracks discourse entities and distinguishes references to previously mentioned entities from the introduction of new entities. The Pundit front-end supports turn-taking dialogue and permits the system to include both questions and answers in building an integrated discourse context, required for the handling of interactive communication. Pundit has been interfaced to speech recognition systems (both Summit and the ITT continuous speech recogniser) to perform applications on direction-finding assistance (Voyager), air travel planning (ATIS), and air traffic control (flight strip updating)."
H89-2001,Report on Session {I}: Prosodic Aids to Speech Recognition,1989,0,0,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"Four papers were presented in the opening session of the conference. The papers were Prosody and Parsing by P. J. Price (SRI), M. Ostendorf (Boston University), and C. W. Wightman (Boston University), Timing Models for Prosody and Cross-word Coarticulation in Connected Speech by M. Beckman (Ohio State University), Intonational Meaning in the Interpretation of Discourse, by J. Hirschberg (AT&T Bell Laboratories), and Structure and Intonation in Spoken Language Systems, by M. Steedman (University of Pennsylvania). Price et al. reported on the use of prosodic information to resolve several types of syntactic ambiguities, the development of a prosodic information coding system suitable for a parser, and the development of automatic algorithms for extracting prosodic information. (Work jointly supported by NSF and DARPA.) Mary Beckman reported on work in articulatory dynamics which suggests a new approach to the use of durational information in continuous speech recognition. New models of articulatory gesture allow for useful distinctions among the timing effects found in global tempo increase, phrase-final lengthening, and sentence accent. (Work supported by NSF.) Julia Hirschberg reported on work in empirical observation of the pragmatic uses of selected pitch contours. In addition, her report addressed the need for better speech data (goal-directed speech in a specific task domain) on which to test hypotheses about the interaction of prosodic constructs with the other components of a spoken language understanding system, particularly semantics and pragmatics. (Work supported by AT&T Bell Laboratories.) Mark Steedman reported on work in the description of intonational and syntactic structures in a combinatory extension of categorial grammar. Combinatory categorial grammar predicts syntactic units which align with boundaries in the intonational structure, thus helping to clarify the structure of an utterance for spoken language understanding. (Work supported by DARPA and NSF.)"
H89-2009,Answers and Questions: Processing Messages and Queries,1989,8,10,4,0,48278,catherine ball,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"This paper describes issues in adapting the PUNDIT system, designed originally for message processing, to a query-answering system for the VOYAGER application. The resulting system, whose architecture and capabilities are described here, represents a first step towards our goal of demonstrating spoken language understanding in an interactive problem-solving context."
H89-1048,"Natural Language Understanding: Integrating Syntax, Semantics, and Discourse.",1989,0,0,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"Technical Summary: The focus of the UNISYS research is on integrating multiple knowledge sources, including syntax, semantics, pragmatics, a domain model, and both domain-specific and domain-independent knowledge sources, to produce a system capable of understanding messages in a restricted domain."
H89-1051,Porting {PUNDIT} to the Resource Management Domain,1989,2,4,1,1,45475,lynette hirschman,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"This paper describes our experiences porting the PUNDIT natural language processing system to the Resource Management domain. PUNDIT has previously been applied to a range of messages (see the paper Analyzing Explicitly Structured Discourse in a Limited Domain: Trouble and Failure Reports by C. Ball (appearing in this volume), and also [Hirschman1989]. However, it had not not been tested on any significant corpus of queries, such as that represented by the Resource Management corpus. Our goal was to assess PUNDIT's portability, and to determine its coverage of syntax over this domain. Time constraints precluded testing of the semantic component, but we plan to report on this at subsequent meetings. We performed this port with the intention of coupling PUNDIT to the MIT SUMMIT speech recognition system. This work is described in another paper in this volume, Reducing Search by Partitioning the Word Network, by J. Dowding."
P88-1002,Sentence Fragments Regular Structures,1988,20,19,3,0,55259,marcia linebarger,26th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes an analysis of telegraphic fragments as regular structures (not errors) handled by minimal extensions to a system designed for processing the standard language. The modular approach which has been implemented in the Unisys natural language processing system PUNDIT is based on a division of labor in which syntax regulates the occurrence and distribution of elided elements, and semantics and pragmatics use the system's standard mechanisms to interpret them."
A88-1007,Improved Portability and Parsing Through Interactive Acquisition of Semantic Information,1988,14,27,2,0,57798,francoismichel lang,Second Conference on Applied Natural Language Processing,0,"This paper presents SPQR (Selectional Pattern Queries and Responses), a module of the PUNDIT text-processing system designed to facilitate the acquisition of domain-specific semantic information, and to improve the accuracy and efficiency of the parser. SPQR operates by interactively and incrementally collecting information about the semantic acceptability of certain lexical co-occurrence patterns (e.g., subject-verb-object) found in partially constructed parses. The module has proved to be a valuable tool for porting PUNDIT to new domains and acquiring essential semantic information about the domains. Preliminary results also indicate that SPQR causes a threefold reduction in the number of parses found, and about a 40% reduction in total parsing time."
P86-1004,Recovering Implicit Information,1986,12,62,4,0,4859,martha palmer,24th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing natural language messages. PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components. Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and reference resolution can be directed to find specific referents for the entities. In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution. The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents."
J86-3002,Discovery Procedures for Sublanguage Selectional Patterns: Initial Experiments,1986,15,75,2,0.459923,10781,ralph grishman,Computational Linguistics,0,"Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure."
H86-1002,"{PROTEUS} and {PUNDIT}: {RESEARCH} {IN} {TEXT} {UNDERSTANDING} at the {D}epartment of {C}omputer {S}cience, {N}ew {Y}ork {U}niversity and {S}ystem {D}evelopment {C}orporation -- A {B}urroughs Company",1986,0,0,2,0.459923,10781,ralph grishman,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"Abstract : We are engaged in the development systems capable of analyzing short narrative messages dealing with a limited domain and extracting the information contained in the narrative. These systems are initially being applied to messages describing equipment failure. This work is a joint effort of New York University and the System Development Corp. for the DARPA Strategic Computing Program. Our aim is to create a system reliable enough for use in an operational environment. This is a formidable task, both because the texts are unedited (and so contain various errors) and because the complexity of any real domain precludes us from assembling a complete collection of the relationships and domain knowledge relevant to understanding texts in the domain. Our basic approach to increasing reliability will be to bring to bear on the analysis task as many different types of constraints as possible. These include constraints related to syntax, semantics, domain knowledge, and discourse structure. In order to be able to capture the detailed knowledge about the domain that is needed for correct message analyses, we are initially limiting ourselves to messages about one particular piece of equipment (the starting air compressor); if we are successful in this narrow domain, we intend to gradually broaden our system."
H86-1011,Recovering Implicit Information,1986,12,62,4,0,4859,martha palmer,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing natural language messages. PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components. Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and reference resolution can be directed to find specific referents for the entities. In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution. The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents."
P84-1023,Automated Determination of Sublanguage Syntactic Usage,1984,6,12,4,1,10781,ralph grishman,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"Sublanguages differ from each other, and from the standard language, in their syntactic, semantic, and discourse properties. Understanding these differnces is important if we are to improve our ability to process these sublanguages. We have developed a semiautomatic procedure for identifying sublanguage syntactic usage from a sample of text in the sublanguage. We describe the results of applying this procedure to three text samples: two sets of medical documents and a set of equipment failure messages."
C82-2029,Constraints on Noun Phrase Conjunction: A Domain-Independent Mechanism,1982,0,3,1,1,45475,lynette hirschman,{C}oling 1982 Abstracts: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics Abstracts,0,None
C82-1014,Natural Language Interfaces Using Limited Semantic Information,1982,8,10,2,0,10781,ralph grishman,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In order to analyze their input properly, natural language interfaces require access to domain-specific semantic information. However, design considerations for practical systems -- in particular, the desire to construct interfaces which are readily portable to new domains -- require us to limit and segregate this domain-specific information. We consider here the possibility of limiting ourselves to a characterization of the structure of information in a domain. This structure is captured in a domain information schema , which specifies the semantic classes of the domain, the words and phrases which belong to these classes, and the predicate-argument relationships among members of these classes which are meaningful in the domain. We describe how this schema is used by the various stages of two large natural language processing systems."
