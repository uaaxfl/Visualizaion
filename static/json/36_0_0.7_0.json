[
    [
        {
            "id": "2001",
            "citation_count": 172,
            "name": 172,
            "cx": 28.5975,
            "cy": -1731.93,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2002",
            "citation_count": 167,
            "name": 167,
            "cx": 28.5975,
            "cy": -1642.19,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-1812",
            "name": "Parsing and Hypergraphs",
            "publication_data": 2001,
            "citation": 91,
            "abstract": "None",
            "cx": 6149.6,
            "cy": -1731.93,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1016",
            "name": "{A}* Parsing: Fast Exact {V}iterbi Parse Selection",
            "publication_data": 2003,
            "citation": 192,
            "abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of pre-computation, reduces the work to less than 5%. Un-like best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.",
            "cx": 6016.6,
            "cy": -1552.45,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1108",
            "name": "K-Best {A}* Parsing",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "A* parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing k-best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types.",
            "cx": 6003.6,
            "cy": -1014.01,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2037",
            "name": "Top-Down K-Best {A}* Parsing",
            "publication_data": 2010,
            "citation": 9,
            "abstract": "We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA* is a variant of the k-best A* (KA*) algorithm of Pauls and Klein (2009). In contrast to KA*, which performs an inside and outside pass before performing k-best extraction bottom up, TKA* performs only the inside pass before extracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement.",
            "cx": 6084.6,
            "cy": -924.271,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-0714",
            "name": "Distributional phrase structure induction",
            "publication_data": 2001,
            "citation": 36,
            "abstract": "Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.",
            "cx": 489.597,
            "cy": -1731.93,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P02-1017",
            "name": "A Generative Constituent-Context Model for Improved Grammar Induction",
            "publication_data": 2002,
            "citation": 167,
            "abstract": "We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.",
            "cx": 489.597,
            "cy": -1642.19,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P01-1044",
            "name": "Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the {P}enn {T}reebank",
            "publication_data": 2001,
            "citation": 45,
            "abstract": "This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.",
            "cx": 5983.6,
            "cy": -1731.93,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P03-1054",
            "name": "Accurate Unlexicalized Parsing",
            "publication_data": 2003,
            "citation": 2541,
            "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.",
            "cx": 5065.6,
            "cy": -1552.45,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2003",
            "citation_count": 2733,
            "name": 2733,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P04-1061",
            "name": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
            "publication_data": 2004,
            "citation": 360,
            "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
            "cx": 630.597,
            "cy": -1462.71,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P06-1111",
            "name": "Prototype-Driven Grammar Induction",
            "publication_data": 2006,
            "citation": 39,
            "abstract": "We investigate prototype-driven learning for primarily unsupervised grammar induction. Prior knowledge is specified declaratively, by providing a few canonical examples of each target phrase type. This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model. We show that distributional features are effective at distinguishing bracket labels, but not determining bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction.",
            "cx": 489.597,
            "cy": -1283.23,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "Unsupervised Parsing via Constituency Tests",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.",
            "cx": 4290.6,
            "cy": -26.8701,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2004",
            "citation_count": 558,
            "name": 558,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W04-3201",
            "name": "Max-Margin Parsing",
            "publication_data": 2004,
            "citation": 198,
            "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the modelxe2x80x99s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.",
            "cx": 5526.6,
            "cy": -1462.71,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W05-0104",
            "name": "A Core-Tools Statistical {NLP} Course",
            "publication_data": 2005,
            "citation": 1,
            "abstract": "In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms. The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system. Using provided scaffolding, students built realistic tools with nearly state-of-the-art performance in most cases. This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way.",
            "cx": 5727.6,
            "cy": -1372.97,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W06-2903",
            "name": "Non-Local Modeling with a Mixture of {PCFG}s",
            "publication_data": 2006,
            "citation": 0,
            "abstract": "While most work on parsing with PCFGs has focused on local correlations between tree configurations, we attempt to model non-local correlations using a finite mixture of PCFGs. A mixture grammar fit with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood. We argue that this improvement comes from the learning of specialized grammars that capture non-local correlations.",
            "cx": 5380.6,
            "cy": -1283.23,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P06-1055",
            "name": "Learning Accurate, Compact, and Interpretable Tree Annotation",
            "publication_data": 2006,
            "citation": 727,
            "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
            "cx": 3467.6,
            "cy": -1283.23,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N07-1051",
            "name": "Improved Inference for Unlexicalized Parsing",
            "publication_data": 2007,
            "citation": 533,
            "abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammarxe2x80x99s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.",
            "cx": 4148.6,
            "cy": -1193.49,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D07-1072",
            "name": "The Infinite {PCFG} Using Hierarchical {D}irichlet Processes",
            "publication_data": 2007,
            "citation": 146,
            "abstract": "We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available. In addition to presenting a fully Bayesian model for the PCFG, we also develop an efficient variational inference procedure. On synthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars.",
            "cx": 4930.6,
            "cy": -1193.49,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W08-1005",
            "name": "Parsing {G}erman with Latent Variable Grammars",
            "publication_data": 2008,
            "citation": 23,
            "abstract": "We describe experiments on learning latent variable grammars for various German tree-banks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks.",
            "cx": 3333.6,
            "cy": -1103.75,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1091",
            "name": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing",
            "publication_data": 2008,
            "citation": 35,
            "abstract": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.",
            "cx": 5109.6,
            "cy": -1103.75,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1026",
            "name": "Efficient Parsing for Transducer Grammars",
            "publication_data": 2009,
            "citation": 24,
            "abstract": "The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized. We address the problem of parsing efficiently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU.",
            "cx": 1609.6,
            "cy": -1014.01,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1063",
            "name": "Hierarchical Search for Parsing",
            "publication_data": 2009,
            "citation": 17,
            "abstract": "Both coarse-to-fine and A* parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarse-to-fine case because of accumulated slack in A* heuristics.",
            "cx": 5452.6,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D09-1120",
            "name": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
            "publication_data": 2009,
            "citation": 158,
            "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).",
            "cx": 2770.6,
            "cy": -1014.01,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P10-1112",
            "name": "Simple, Accurate Parsing with an All-Fragments Grammar",
            "publication_data": 2010,
            "citation": 26,
            "abstract": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.",
            "cx": 4489.6,
            "cy": -924.271,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-2127",
            "name": "The Surprising Variance in Shortest-Derivation Parsing",
            "publication_data": 2011,
            "citation": 2,
            "abstract": "We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.",
            "cx": 4436.6,
            "cy": -834.531,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P12-2021",
            "name": "Robust Conversion of {CCG} Derivations to Phrase Structure Trees",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.",
            "cx": 5218.6,
            "cy": -744.791,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P12-1101",
            "name": "Large-Scale Syntactic Language Modeling with Treelets",
            "publication_data": 2012,
            "citation": 27,
            "abstract": "We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.",
            "cx": 4128.6,
            "cy": -744.791,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1091",
            "name": "An Empirical Investigation of Statistical Significance in {NLP}",
            "publication_data": 2012,
            "citation": 45,
            "abstract": "We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.",
            "cx": 2003.6,
            "cy": -744.791,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1096",
            "name": "Parser Showdown at the {W}all {S}treet Corral: An Empirical Investigation of Error Types in Parser Output",
            "publication_data": 2012,
            "citation": 47,
            "abstract": "Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.",
            "cx": 3655.6,
            "cy": -744.791,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1105",
            "name": "Training Factored {PCFG}s with Expectation Propagation",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguistically-motivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.",
            "cx": 4908.6,
            "cy": -744.791,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2018",
            "name": "An Empirical Examination of Challenges in {C}hinese Parsing",
            "publication_data": 2013,
            "citation": 11,
            "abstract": "Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.",
            "cx": 3990.6,
            "cy": -655.051,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1022",
            "name": "Less Grammar, More Features",
            "publication_data": 2014,
            "citation": 43,
            "abstract": "We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bjorkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.",
            "cx": 5059.6,
            "cy": -565.311,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P15-1030",
            "name": "Neural {CRF} Parsing",
            "publication_data": 2015,
            "citation": 33,
            "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.",
            "cx": 4681.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P17-1076",
            "name": "A Minimal Span-Based Neural Constituency Parser",
            "publication_data": 2017,
            "citation": 39,
            "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",
            "cx": 4335.6,
            "cy": -296.09,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N18-1091",
            "name": "What{'}s Going On in Neural Constituency Parsers? An Analysis",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.",
            "cx": 4922.6,
            "cy": -206.35,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D08-1012",
            "name": "Coarse-to-Fine Syntactic Machine Translation using Language Projections",
            "publication_data": 2008,
            "citation": 36,
            "abstract": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incrementally introduced. In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.",
            "cx": 4604.6,
            "cy": -1103.75,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2064",
            "name": "Hierarchical {A}* Parsing with Bridge Outside Scores",
            "publication_data": 2010,
            "citation": 6,
            "abstract": "Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.",
            "cx": 5452.6,
            "cy": -924.271,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1147",
            "name": "Discriminative Modeling of Extraction Sets for Machine Translation",
            "publication_data": 2010,
            "citation": 25,
            "abstract": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.",
            "cx": 1940.6,
            "cy": -924.271,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2005",
            "citation_count": 175,
            "name": 175,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1049",
            "name": "Jointly Learning to Extract and Compress",
            "publication_data": 2011,
            "citation": 165,
            "abstract": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.",
            "cx": 3911.6,
            "cy": -834.531,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D15-1032",
            "name": "An Empirical Analysis of Optimization for Max-Margin {NLP}",
            "publication_data": 2015,
            "citation": 14,
            "abstract": "Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.",
            "cx": 4848.6,
            "cy": -475.571,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1014",
            "name": "Alignment by Agreement",
            "publication_data": 2006,
            "citation": 392,
            "abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.",
            "cx": 1174.6,
            "cy": -1283.23,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P08-1100",
            "name": "Analyzing the Errors of Unsupervised Learning",
            "publication_data": 2008,
            "citation": 21,
            "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
            "cx": 620.597,
            "cy": -1103.75,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1131",
            "name": "Phylogenetic Grammar Induction",
            "publication_data": 2010,
            "citation": 50,
            "abstract": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.",
            "cx": 988.597,
            "cy": -924.271,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1083",
            "name": "Painless Unsupervised Learning with Features",
            "publication_data": 2010,
            "citation": 168,
            "abstract": "We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.",
            "cx": 782.597,
            "cy": -924.271,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D12-1001",
            "name": "Syntactic Transfer Using a Bilingual Lexicon",
            "publication_data": 2012,
            "citation": 43,
            "abstract": "We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different low-resource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).",
            "cx": 809.597,
            "cy": -744.791,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 1640,
            "name": 1640,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H05-1010",
            "name": "A Discriminative Matching Approach to Word Alignment",
            "publication_data": 2005,
            "citation": 174,
            "abstract": "We present a discriminative, large-margin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.",
            "cx": 1721.6,
            "cy": -1372.97,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P07-1003",
            "name": "Tailoring Word Alignments to Syntactic Machine Translation",
            "publication_data": 2007,
            "citation": 100,
            "abstract": "Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our modelxe2x80x99s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",
            "cx": 1459.6,
            "cy": -1193.49,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P09-1104",
            "name": "Better Word Alignments with Supervised {ITG} Models",
            "publication_data": 2009,
            "citation": 91,
            "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA alignments.",
            "cx": 1775.6,
            "cy": -1014.01,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1015",
            "name": "Joint Parsing and Alignment with Weakly Synchronized Grammars",
            "publication_data": 2010,
            "citation": 56,
            "abstract": "Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",
            "cx": 1332.6,
            "cy": -924.271,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N12-1004",
            "name": "Fast Inference in Phrase Extraction Models with Belief Propagation",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up.",
            "cx": 2155.6,
            "cy": -744.791,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2007",
            "citation_count": 925,
            "name": 925,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W06-3105",
            "name": "Why Generative Phrase Models Underperform Surface Heuristics",
            "publication_data": 2006,
            "citation": 71,
            "abstract": "We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased deter-minization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score.",
            "cx": 1990.6,
            "cy": -1283.23,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P08-2007",
            "name": "The Complexity of Phrase Alignment Problems",
            "publication_data": 2008,
            "citation": 63,
            "abstract": "Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.",
            "cx": 1938.6,
            "cy": -1103.75,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1033",
            "name": "Sampling Alignment Structure under a {B}ayesian Translation Model",
            "publication_data": 2008,
            "citation": 86,
            "abstract": "We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase-based translation systems.",
            "cx": 2217.6,
            "cy": -1103.75,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1014",
            "name": "Unsupervised Syntactic Alignment with Inversion Transduction Grammars",
            "publication_data": 2010,
            "citation": 18,
            "abstract": "Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.",
            "cx": 1527.6,
            "cy": -924.271,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D07-1094",
            "name": "Learning Structured Models for Phone Recognition",
            "publication_data": 2007,
            "citation": 13,
            "abstract": "We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",
            "cx": 3413.6,
            "cy": -1193.49,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W10-2906",
            "name": "Learning Better Monolingual Models with Unannotated Bilingual Text",
            "publication_data": 2010,
            "citation": 33,
            "abstract": "This work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text. We build a multiview learning objective that enforces agreement between monolingual and bilingual models. In our method the first, monolingual view consists of supervised predictors learned separately for each language. The second, bilingual view consists of log-linear predictors learned over both languages on bilingual text. Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model, and we show how to combine the two models to account for dependence between views. For the task of named entity recognition, using bilingual predictors increases F1 by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F1 by 14.6%. For syntactic parsing, our bilingual predictor increases F1 by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%.",
            "cx": 3217.6,
            "cy": -924.271,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2054",
            "name": "An Entity-Level Approach to Information Extraction",
            "publication_data": 2010,
            "citation": 8,
            "abstract": "We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.",
            "cx": 3378.6,
            "cy": -924.271,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N10-1061",
            "name": "Coreference Resolution in a Modular, Entity-Centered Model",
            "publication_data": 2010,
            "citation": 119,
            "abstract": "Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.",
            "cx": 2830.6,
            "cy": -924.271,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N10-1082",
            "name": "Type-Based {MCMC}",
            "publication_data": 2010,
            "citation": 32,
            "abstract": "Most existing algorithms for learning latent-variable models---such as EM and existing Gibbs samplers---are token-based, meaning that they update the variables associated with one sentence at a time. The incremental nature of these methods makes them susceptible to local optima/slow mixing. In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences. We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars.",
            "cx": 2238.6,
            "cy": -924.271,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-1916",
            "name": "Mention Detection: Heuristics for the {O}nto{N}otes annotations",
            "publication_data": 2011,
            "citation": 10,
            "abstract": "Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3, Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.",
            "cx": 3349.6,
            "cy": -834.531,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1060",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2011,
            "citation": 315,
            "abstract": "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (Geo and Jobs), our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
            "cx": 1546.6,
            "cy": -834.531,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P11-1070",
            "name": "Web-Scale Features for Full-Scale Parsing",
            "publication_data": 2011,
            "citation": 41,
            "abstract": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.",
            "cx": 2948.6,
            "cy": -834.531,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1012",
            "name": "Decentralized Entity-Level Modeling for Coreference Resolution",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.",
            "cx": 2731.6,
            "cy": -655.051,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "J13-2005",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2013,
            "citation": 54,
            "abstract": "This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.",
            "cx": 1584.6,
            "cy": -655.051,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "Q14-1037",
            "name": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking",
            "publication_data": 2014,
            "citation": 122,
            "abstract": "We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",
            "cx": 2900.6,
            "cy": -565.311,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P16-1188",
            "name": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
            "publication_data": 2016,
            "citation": 28,
            "abstract": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.",
            "cx": 3403.6,
            "cy": -385.831,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P06-1096",
            "name": "An End-to-End Discriminative Approach to Machine Translation",
            "publication_data": 2006,
            "citation": 244,
            "abstract": "We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic.",
            "cx": 2498.6,
            "cy": -1283.23,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D08-1092",
            "name": "Two Languages are Better than One (for Syntactic Parsing)",
            "publication_data": 2008,
            "citation": 90,
            "abstract": "We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",
            "cx": 1409.6,
            "cy": -1103.75,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P09-1011",
            "name": "Learning Semantic Correspondences with Less Supervision",
            "publication_data": 2009,
            "citation": 190,
            "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.",
            "cx": 1948.6,
            "cy": -1014.01,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D12-1079",
            "name": "Transforming Trees to Improve Syntactic Convergence",
            "publication_data": 2012,
            "citation": 13,
            "abstract": "We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.",
            "cx": 1271.6,
            "cy": -744.791,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1041",
            "name": "Prototype-Driven Learning for Sequence Models",
            "publication_data": 2006,
            "citation": 167,
            "abstract": "We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system's error trends.",
            "cx": 2188.6,
            "cy": -1283.23,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 613,
            "name": 613,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P07-1107",
            "name": "Unsupervised Coreference Resolution in a Nonparametric {B}ayesian Model",
            "publication_data": 2007,
            "citation": 127,
            "abstract": "We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",
            "cx": 2798.6,
            "cy": -1193.49,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-2036",
            "name": "Asynchronous Binarization for Synchronous Grammars",
            "publication_data": 2009,
            "citation": 12,
            "abstract": "Binarization of n-ary rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.",
            "cx": 4224.6,
            "cy": -1014.01,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W14-1607",
            "name": "Grounding Language with Points and Paths in Continuous Spaces",
            "publication_data": 2014,
            "citation": 10,
            "abstract": "We present a model for generating pathvalued interpretations of natural language text. Our model encodes a map from natural language descriptions to paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010).",
            "cx": 1699.6,
            "cy": -565.311,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-2133",
            "name": "How much do word embeddings encode about syntax?",
            "publication_data": 2014,
            "citation": 36,
            "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
            "cx": 4719.6,
            "cy": -565.311,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1020",
            "name": "Sparser, Better, Faster {GPU} Parsing",
            "publication_data": 2014,
            "citation": 16,
            "abstract": "Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per secondxe2x80x94more than a 2x speedupxe2x80x94on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruningxe2x80x94nearly a 6x speedup.",
            "cx": 3871.6,
            "cy": -565.311,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P19-1031",
            "name": "Cross-Domain Generalization of Neural Constituency Parsers",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing{---}but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",
            "cx": 4430.6,
            "cy": -116.61,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N07-1052",
            "name": "Approximate Factoring for {A}* Search",
            "publication_data": 2007,
            "citation": 6,
            "abstract": "We present a novel method for creating Axe2x88x97 estimates for structured search problems. In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains.",
            "cx": 5379.6,
            "cy": -1193.49,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2009",
            "citation_count": 728,
            "name": 728,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P08-1088",
            "name": "Learning Bilingual Lexicons from Monolingual Corpora",
            "publication_data": 2008,
            "citation": 259,
            "abstract": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
            "cx": 394.597,
            "cy": -1103.75,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D11-1029",
            "name": "Simple Effective Decipherment via Combinatorial Optimization",
            "publication_data": 2011,
            "citation": 11,
            "abstract": "We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.",
            "cx": 485.597,
            "cy": -834.531,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1069",
            "name": "Online {EM} for Unsupervised Models",
            "publication_data": 2009,
            "citation": 157,
            "abstract": "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.",
            "cx": 593.597,
            "cy": -1014.01,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D09-1147",
            "name": "Consensus Training for Consensus Decoding in Machine Translation",
            "publication_data": 2009,
            "citation": 25,
            "abstract": "We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forest-based consensus and minimum Bayes risk decoding methods. Our continuous objective can be optimized using simple gradient ascent. However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here. Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning. First, it specifically optimizes model weights for downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.",
            "cx": 4708.6,
            "cy": -1014.01,
            "rx": 95.4188,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1021",
            "name": "Unsupervised Transcription of Historical Documents",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Googlexe2x80x99s open source OCR system.",
            "cx": 2504.6,
            "cy": -655.051,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-2020",
            "name": "Improved Typesetting Models for Historical {OCR}",
            "publication_data": 2014,
            "citation": 13,
            "abstract": "We present richer typesetting models that extend the unsupervised historical document recognition system of BergKirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers.",
            "cx": 4153.6,
            "cy": -565.311,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 573,
            "name": 573,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N09-1008",
            "name": "Improved Reconstruction of Protolanguage Word Forms",
            "publication_data": 2009,
            "citation": 19,
            "abstract": "We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous work. First, previous work focused on faithfulness features, which model changes between successive languages. We add markedness features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of Proto-Oceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date.",
            "cx": 337.597,
            "cy": -1014.01,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1105",
            "name": "Finding Cognate Groups Using Phylogenies",
            "publication_data": 2010,
            "citation": 23,
            "abstract": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.",
            "cx": 337.597,
            "cy": -924.271,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1032",
            "name": "Large-Scale Cognate Recovery",
            "publication_data": 2011,
            "citation": 6,
            "abstract": "We present a system for the large scale induction of cognate groups. Our model explains the evolution of cognates as a sequence of mutations and innovations along a phylogeny. On the task of identifying cognates from over 21,000 words in 218 different languages from the Oceanic language family, our model achieves a cluster purity score over 91%, while maintaining pairwise recall over 62%.",
            "cx": 288.597,
            "cy": -834.531,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P12-1041",
            "name": "Coreference Semantics from Web Features",
            "publication_data": 2012,
            "citation": 36,
            "abstract": "To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.",
            "cx": 2915.6,
            "cy": -744.791,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1027",
            "name": "Error-Driven Analysis of Challenges in Coreference Resolution",
            "publication_data": 2013,
            "citation": 25,
            "abstract": "Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.",
            "cx": 3344.6,
            "cy": -655.051,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1203",
            "name": "Easy Victories and Uphill Battles in Coreference Resolution",
            "publication_data": 2013,
            "citation": 126,
            "abstract": "Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win xe2x80x9ceasy victoriesxe2x80x9d without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an xe2x80x9cuphill battle.xe2x80x9d Nonetheless, our final system 1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bjxc2xa8 orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.",
            "cx": 2917.6,
            "cy": -655.051,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2011",
            "citation_count": 653,
            "name": 653,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 230,
            "name": 230,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1027",
            "name": "Faster and Smaller N-Gram Language Models",
            "publication_data": 2011,
            "citation": 103,
            "abstract": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.",
            "cx": 4121.6,
            "cy": -834.531,
            "rx": 76.2353,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D15-1138",
            "name": "Alignment-Based Compositional Semantics for Instruction Following",
            "publication_data": 2015,
            "citation": 17,
            "abstract": "This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the modelxe2x80x99s flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",
            "cx": 1699.6,
            "cy": -475.571,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N16-1181",
            "name": "Learning to Compose Neural Networks for Question Answering",
            "publication_data": 2016,
            "citation": 205,
            "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.",
            "cx": 1469.6,
            "cy": -385.831,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P14-1098",
            "name": "Structured Learning for Taxonomy Induction with Belief Propagation",
            "publication_data": 2014,
            "citation": 29,
            "abstract": "We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts. For efficient inference over taxonomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor. To train the system, we extract sub-structures of WordNet and discriminatively learn to reproduce them, using adaptive subgradient stochastic optimization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1.",
            "cx": 3105.6,
            "cy": -565.311,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1087",
            "name": "Decipherment with a Million Random Restarts",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.",
            "cx": 485.597,
            "cy": -655.051,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 265,
            "name": 265,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1340",
            "name": "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
            "publication_data": 2019,
            "citation": 9,
            "abstract": "We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",
            "cx": 4165.6,
            "cy": -116.61,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 269,
            "name": 269,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 64,
            "name": 64,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1249",
            "name": "Constituency Parsing with a Self-Attentive Encoder",
            "publication_data": 2018,
            "citation": 22,
            "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
            "cx": 4554.6,
            "cy": -206.35,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 269,
            "name": 269,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.557",
            "name": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word{'}s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.",
            "cx": 4685.6,
            "cy": -26.8701,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "Q17-1031",
            "name": "Parsing with Traces: An {O}(n4) Algorithm and a Structural Representation",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3{\\%} of the Penn English Treebank. We also implement a proof-of-concept parser that recovers a range of null elements and trace types.",
            "cx": 4844.6,
            "cy": -296.09,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 64,
            "name": 64,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D16-1125",
            "name": "Reasoning about Pragmatics with Neural Listeners and Speakers",
            "publication_data": 2016,
            "citation": 36,
            "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural listener and speaker models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.",
            "cx": 6019.6,
            "cy": -385.831,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P17-1022",
            "name": "Translating Neuralese",
            "publication_data": 2017,
            "citation": "???",
            "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents{'} messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",
            "cx": 6019.6,
            "cy": -296.09,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018",
            "citation_count": 41,
            "name": 41,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2025",
            "name": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.",
            "cx": 4154.6,
            "cy": -296.09,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1178",
            "name": "Effective Inference for Generative Neural Parsing",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.",
            "cx": 4593.6,
            "cy": -296.09,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-2075",
            "name": "Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser{'}s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",
            "cx": 4165.6,
            "cy": -206.35,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1311",
            "name": "Analogs of Linguistic Structure in Deep Representations",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a {``}syntax{''} with functional analogues to qualitative properties of natural language.",
            "cx": 6218.6,
            "cy": -296.09,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1188",
            "name": "Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.",
            "cx": 6275.6,
            "cy": -116.61,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 10,
            "name": 10,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1197",
            "name": "Learning with Latent Language",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter{'}s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.",
            "cx": 6332.6,
            "cy": -206.35,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        }
    ],
    [
        {
            "source": "2001",
            "target": "2002",
            "d": "M28.5975,-1713.6C28.5975,-1698.24 28.5975,-1675.92 28.5975,-1660.55"
        },
        {
            "source": "W01-1812",
            "target": "N03-1016",
            "d": "M6130.82,-1705.88C6107.46,-1674.7 6067.47,-1621.34 6041.55,-1586.74"
        },
        {
            "source": "W01-1812",
            "target": "P09-1108",
            "d": "M6148.56,-1704.84C6144.28,-1614.9 6124.04,-1310.52 6039.6,-1076.88 6036.21,-1067.51 6031.42,-1057.95 6026.43,-1049.22"
        },
        {
            "source": "W01-1812",
            "target": "P10-2037",
            "d": "M6158.7,-1705.14C6169.94,-1670.93 6187.6,-1608.44 6187.6,-1553.45 6187.6,-1553.45 6187.6,-1553.45 6187.6,-1102.75 6187.6,-1048.66 6178.5,-1032.86 6149.6,-987.141 6142.63,-976.127 6133.28,-965.706 6123.85,-956.666"
        },
        {
            "source": "W01-0714",
            "target": "P02-1017",
            "d": "M489.597,-1704.61C489.597,-1696.64 489.597,-1687.75 489.597,-1679.26"
        },
        {
            "source": "P01-1044",
            "target": "W01-1812",
            "d": "M6058.17,-1731.93C6060.49,-1731.93 6062.81,-1731.93 6065.13,-1731.93"
        },
        {
            "source": "P01-1044",
            "target": "P03-1054",
            "d": "M5919.05,-1718.45C5754.92,-1686.72 5322.11,-1603.04 5143.94,-1568.6"
        },
        {
            "source": "P01-1044",
            "target": "N03-1016",
            "d": "M5988.47,-1704.74C5994.14,-1674.25 6003.54,-1623.7 6009.89,-1589.5"
        },
        {
            "source": "2002",
            "target": "2003",
            "d": "M28.5975,-1623.86C28.5975,-1608.5 28.5975,-1586.18 28.5975,-1570.81"
        },
        {
            "source": "P02-1017",
            "target": "P04-1061",
            "d": "M508.907,-1616.89C533.555,-1585.86 576.316,-1532.04 604.025,-1497.16"
        },
        {
            "source": "P02-1017",
            "target": "P06-1111",
            "d": "M489.597,-1615.23C489.597,-1552.67 489.597,-1393.38 489.597,-1321.01"
        },
        {
            "source": "P02-1017",
            "target": "2020.emnlp-main.389",
            "d": "M437.196,-1641.36C327.883,-1638.67 85.5975,-1615.59 85.5975,-1463.71 85.5975,-1463.71 85.5975,-1463.71 85.5975,-205.35 85.5975,-100.215 3486.24,-40.6423 4171.23,-29.7089"
        },
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "P03-1054",
            "target": "W04-3201",
            "d": "M5134.95,-1538.57C5201.55,-1526.16 5304.99,-1506.79 5394.6,-1489.58 5409.65,-1486.69 5425.66,-1483.58 5441.14,-1480.56"
        },
        {
            "source": "P03-1054",
            "target": "W05-0104",
            "d": "M5146.38,-1552C5289.09,-1551.53 5578.59,-1543.7 5661.6,-1489.58 5689.75,-1471.23 5707.22,-1436.42 5717.05,-1409.77"
        },
        {
            "source": "P03-1054",
            "target": "W06-2903",
            "d": "M5142.71,-1544.4C5186.7,-1536.9 5240.48,-1521.44 5278.6,-1489.58 5331.84,-1445.08 5359.87,-1365.97 5372.33,-1320.02"
        },
        {
            "source": "P03-1054",
            "target": "P06-1055",
            "d": "M4985.05,-1550.5C4715.08,-1546.94 3852.57,-1532.3 3735.6,-1489.58 3635.25,-1452.94 3541.95,-1364.39 3496.52,-1316.42"
        },
        {
            "source": "P03-1054",
            "target": "N07-1051",
            "d": "M4984.88,-1550.85C4834.26,-1548.57 4516.87,-1537.98 4419.6,-1489.58 4299.7,-1429.92 4206.86,-1293.09 4168.2,-1228.91"
        },
        {
            "source": "P03-1054",
            "target": "D07-1072",
            "d": "M5055.78,-1525.49C5031.96,-1462.52 4971.08,-1301.55 4943.97,-1229.86"
        },
        {
            "source": "P03-1054",
            "target": "W08-1005",
            "d": "M4984.98,-1551.57C4766.68,-1551.11 4146.06,-1544.64 3633.6,-1489.58 3333.15,-1457.3 2963.6,-1676.15 2963.6,-1373.97 2963.6,-1373.97 2963.6,-1373.97 2963.6,-1282.23 2963.6,-1156.49 3139.46,-1119.77 3248.69,-1109.09"
        },
        {
            "source": "P03-1054",
            "target": "D08-1091",
            "d": "M5065.6,-1525.29C5065.6,-1490.67 5065.6,-1427.74 5065.6,-1373.97 5065.6,-1373.97 5065.6,-1373.97 5065.6,-1282.23 5065.6,-1231.6 5082.68,-1175.16 5095.56,-1139.81"
        },
        {
            "source": "P03-1054",
            "target": "P09-1108",
            "d": "M5145.57,-1548.7C5334.47,-1541.72 5797.13,-1521.54 5859.6,-1489.58 6018.44,-1408.31 6014.48,-1147.64 6007.2,-1051.27"
        },
        {
            "source": "P03-1054",
            "target": "N09-1026",
            "d": "M4984.96,-1550.66C4596.4,-1546.71 2914.65,-1527.65 2391.6,-1489.58 2223.19,-1477.33 1640.6,-1542.83 1640.6,-1373.97 1640.6,-1373.97 1640.6,-1373.97 1640.6,-1282.23 1640.6,-1199 1625.57,-1102.14 1616.44,-1050.97"
        },
        {
            "source": "P03-1054",
            "target": "N09-1063",
            "d": "M5130.25,-1536.16C5156.34,-1526.84 5184.5,-1512.2 5202.6,-1489.58 5268.71,-1406.95 5173.91,-1329.29 5250.6,-1256.36 5326.85,-1183.85 5416.08,-1300.22 5484.6,-1220.36 5525.61,-1172.55 5495.8,-1094.45 5472.69,-1049.6"
        },
        {
            "source": "P03-1054",
            "target": "D09-1120",
            "d": "M4985.12,-1550.53C4624.66,-1546.19 3167.45,-1526.51 2966.6,-1489.58 2812.51,-1461.25 2633.6,-1530.64 2633.6,-1373.97 2633.6,-1373.97 2633.6,-1373.97 2633.6,-1282.23 2633.6,-1188.84 2700.68,-1095.31 2740.96,-1047.56"
        },
        {
            "source": "P03-1054",
            "target": "P10-1112",
            "d": "M4988.25,-1544.69C4827.38,-1528.48 4471.6,-1480.59 4471.6,-1373.97 4471.6,-1373.97 4471.6,-1373.97 4471.6,-1102.75 4471.6,-1053.56 4478.53,-997.096 4483.79,-961.314"
        },
        {
            "source": "P03-1054",
            "target": "P11-2127",
            "d": "M4985.19,-1550.16C4852.84,-1546.76 4596.04,-1534.41 4518.6,-1489.58 4463.4,-1457.63 4433.6,-1437.75 4433.6,-1373.97 4433.6,-1373.97 4433.6,-1373.97 4433.6,-1102.75 4433.6,-1010.09 4371.88,-986.42 4397.6,-897.401 4400.45,-887.52 4405.4,-877.698 4410.84,-868.887"
        },
        {
            "source": "P03-1054",
            "target": "P12-2021",
            "d": "M5146.09,-1550.21C5293.77,-1546.84 5601.69,-1534.5 5697.6,-1489.58 5749.04,-1465.49 5766.68,-1452.24 5788.6,-1399.84 5880.67,-1179.74 5777.97,-1055.85 5599.6,-897.401 5515.08,-822.326 5388.82,-782.054 5305.12,-762.295"
        },
        {
            "source": "P03-1054",
            "target": "P12-1101",
            "d": "M5005.55,-1534.32C4936.81,-1510.6 4833.6,-1460.56 4833.6,-1373.97 4833.6,-1373.97 4833.6,-1373.97 4833.6,-1282.23 4833.6,-1150.75 4881.31,-1099.24 4812.6,-987.141 4668.89,-752.685 4502.88,-822.35 4232.6,-771.661 4224.38,-770.12 4215.85,-768.324 4207.36,-766.409"
        },
        {
            "source": "P03-1054",
            "target": "D12-1091",
            "d": "M4984.99,-1550.72C4570.59,-1546.77 2694.74,-1526.89 2577.6,-1489.58 2489.83,-1461.63 2405.6,-1466.08 2405.6,-1373.97 2405.6,-1373.97 2405.6,-1373.97 2405.6,-1192.49 2405.6,-1153.74 2361.11,-910.112 2348.6,-897.401 2294.43,-842.353 2249.84,-891.209 2178.6,-861.401 2126.73,-839.7 2073.76,-802.116 2039.89,-775.705"
        },
        {
            "source": "P03-1054",
            "target": "D12-1096",
            "d": "M4985.26,-1549.12C4699.53,-1539.79 3749.6,-1499.33 3749.6,-1373.97 3749.6,-1373.97 3749.6,-1373.97 3749.6,-1192.49 3749.6,-1041.31 3699.32,-1010.02 3671.6,-861.401 3666.67,-834.973 3662.55,-804.885 3659.73,-782.029"
        },
        {
            "source": "P03-1054",
            "target": "D12-1105",
            "d": "M5137.34,-1539.98C5169.51,-1531.29 5205.35,-1516.1 5228.6,-1489.58 5298.8,-1409.5 5205.57,-1328.84 5283.6,-1256.36 5358.12,-1187.14 5425.2,-1277.13 5509.6,-1220.36 5559.44,-1186.83 5579.6,-1164.82 5579.6,-1104.75 5579.6,-1104.75 5579.6,-1104.75 5579.6,-1013.01 5579.6,-960.94 5596.19,-935.409 5560.6,-897.401 5540.86,-876.323 5158.63,-796.561 4988.77,-761.964"
        },
        {
            "source": "P03-1054",
            "target": "P13-2018",
            "d": "M5084.19,-1526.15C5106.69,-1492.98 5141.6,-1432.13 5141.6,-1373.97 5141.6,-1373.97 5141.6,-1373.97 5141.6,-1282.23 5141.6,-1188.19 5215.26,-1168.15 5192.6,-1076.88 5149.09,-901.694 5154.32,-815.712 5002.6,-717.921 4925.73,-668.38 4306.82,-658.5 4076.52,-656.536"
        },
        {
            "source": "P03-1054",
            "target": "P14-1022",
            "d": "M5145.14,-1547.65C5291.68,-1539.93 5592.68,-1520.49 5628.6,-1489.58 5678.6,-1446.55 5628.94,-1400.27 5666.6,-1346.1 5682.44,-1323.32 5701.27,-1332.53 5717.6,-1310.1 5742.19,-1276.32 5739.3,-1261.67 5745.6,-1220.36 5749.2,-1196.75 5746.1,-1190.5 5745.6,-1166.62 5742.49,-1019.37 5731.6,-982.814 5731.6,-835.531 5731.6,-835.531 5731.6,-835.531 5731.6,-743.791 5731.6,-709.241 5740.6,-688.671 5595.6,-628.181 5515.32,-594.691 5269.42,-577.23 5140.46,-570.167"
        },
        {
            "source": "P03-1054",
            "target": "P15-1030",
            "d": "M5098.91,-1527.83C5111.43,-1517.4 5124.76,-1504.2 5133.6,-1489.58 5176.24,-1419.04 5310.12,-853.424 5320.6,-771.661 5323.63,-747.97 5334.26,-737.515 5320.6,-717.921 5228.23,-585.408 5130.43,-649.999 4979.6,-592.181 4919.3,-569.065 4904.79,-561.842 4844.6,-538.441 4810.49,-525.178 4772.23,-510.642 4741.36,-498.993"
        },
        {
            "source": "P03-1054",
            "target": "P17-1076",
            "d": "M4985.13,-1550.67C4570.69,-1546.47 2694.39,-1525.5 2646.6,-1489.58 2559.67,-1424.24 2606.97,-1359.53 2572.6,-1256.36 2521.25,-1102.26 2331.6,-728.739 2331.6,-566.311 2331.6,-566.311 2331.6,-566.311 2331.6,-474.571 2331.6,-331.354 4256.93,-323.265 4258.6,-322.96 4265.12,-321.77 4271.85,-320.143 4278.48,-318.281"
        },
        {
            "source": "P03-1054",
            "target": "N18-1091",
            "d": "M5145.09,-1547.97C5313.97,-1540.04 5697.28,-1519.02 5748.6,-1489.58 5911.06,-1396.38 5895.6,-925.585 5895.6,-925.271 5895.6,-925.271 5895.6,-925.271 5895.6,-384.831 5895.6,-296.338 5264.69,-235.28 5018.94,-214.854"
        },
        {
            "source": "N03-1016",
            "target": "D08-1012",
            "d": "M5989.16,-1527.8C5975.93,-1516.34 5959.87,-1502.32 5945.6,-1489.58 5875.07,-1426.64 5871.06,-1392.3 5788.6,-1346.1 5583.42,-1231.15 5505.66,-1264.18 5274.6,-1220.36 5156.1,-1197.89 5127.35,-1187.7 5008.6,-1166.62 4905.65,-1148.35 4787.36,-1130.61 4705.5,-1118.85"
        },
        {
            "source": "N03-1016",
            "target": "P09-1108",
            "d": "M6021.15,-1525.38C6026.77,-1490.87 6035.6,-1428.05 6035.6,-1373.97 6035.6,-1373.97 6035.6,-1373.97 6035.6,-1192.49 6035.6,-1142.77 6023.28,-1086.43 6013.93,-1050.82"
        },
        {
            "source": "N03-1016",
            "target": "N09-1063",
            "d": "M6000.41,-1526.06C5984.56,-1501.92 5959.13,-1465.05 5933.6,-1435.84 5789.65,-1271.14 5582.16,-1110.54 5494.36,-1045.4"
        },
        {
            "source": "N03-1016",
            "target": "P10-2037",
            "d": "M6049.65,-1529.06C6063.09,-1518.48 6077.75,-1504.83 6087.6,-1489.58 6139.33,-1409.5 6149.6,-1379.57 6149.6,-1284.23 6149.6,-1284.23 6149.6,-1284.23 6149.6,-1102.75 6149.6,-1050.27 6145.79,-1035.58 6125.6,-987.141 6121.62,-977.597 6116.11,-967.956 6110.41,-959.218"
        },
        {
            "source": "N03-1016",
            "target": "P10-2064",
            "d": "M6028.98,-1525.98C6034,-1514.97 6039.55,-1501.82 6043.6,-1489.58 6085.8,-1361.78 6111.6,-1329.08 6111.6,-1194.49 6111.6,-1194.49 6111.6,-1194.49 6111.6,-1102.75 6111.6,-1050.68 6130.62,-1022.72 6092.6,-987.141 6054.63,-951.623 5733.02,-935.035 5560.68,-928.635"
        },
        {
            "source": "N03-1016",
            "target": "P10-1147",
            "d": "M6007.08,-1525.6C5992.5,-1486.43 5963.41,-1409.86 5935.6,-1346.1 5900.24,-1265.02 5908.65,-1232.48 5849.6,-1166.62 5748.66,-1054.05 5705.78,-1029.38 5560.6,-987.141 5377.51,-933.876 2318.76,-965.166 2128.6,-951.141 2098.62,-948.93 2066.13,-945.009 2036.72,-940.877"
        },
        {
            "source": "2004",
            "target": "2005",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "W04-3201",
            "target": "D08-1091",
            "d": "M5523.92,-1435.7C5518.27,-1386.1 5503.46,-1280.75 5477.6,-1256.36 5410.93,-1193.5 5359.23,-1255.48 5274.6,-1220.36 5224.86,-1199.72 5175.27,-1162.1 5143.64,-1135.38"
        },
        {
            "source": "W04-3201",
            "target": "P11-1049",
            "d": "M5533.49,-1435.51C5539.69,-1412.05 5549.1,-1376.74 5557.6,-1346.1 5587.12,-1239.59 5617.6,-1215.28 5617.6,-1104.75 5617.6,-1104.75 5617.6,-1104.75 5617.6,-1013.01 5617.6,-955.723 5608.88,-928.228 5560.6,-897.401 5489.21,-851.83 4120.74,-871.022 4036.6,-861.401 4019.48,-859.444 4001.23,-856.29 3984.12,-852.846"
        },
        {
            "source": "W04-3201",
            "target": "P14-1022",
            "d": "M5542.16,-1436.02C5583.62,-1366.63 5693.6,-1175.82 5693.6,-1104.75 5693.6,-1104.75 5693.6,-1104.75 5693.6,-743.791 5693.6,-630.028 5310.27,-585.875 5139.74,-571.862"
        },
        {
            "source": "W04-3201",
            "target": "D15-1032",
            "d": "M5557.34,-1437.1C5569.51,-1426.44 5583.08,-1413.32 5593.6,-1399.84 5621.55,-1364.01 5622.07,-1350.64 5642.6,-1310.1 5741.24,-1115.31 5769.6,-1053.87 5769.6,-835.531 5769.6,-835.531 5769.6,-835.531 5769.6,-654.051 5769.6,-569.154 5158.4,-504.796 4931.81,-483.854"
        },
        {
            "source": "P04-1061",
            "target": "P06-1111",
            "d": "M610.396,-1436.28C585.651,-1405.14 543.658,-1352.28 516.278,-1317.81"
        },
        {
            "source": "P04-1061",
            "target": "N06-1014",
            "d": "M714.416,-1447.15C769.696,-1436.52 843.31,-1420.39 906.597,-1399.84 982.754,-1375.12 1067,-1336.87 1120.34,-1311.21"
        },
        {
            "source": "P04-1061",
            "target": "P08-1100",
            "d": "M629.87,-1435.75C628.117,-1373.19 623.655,-1213.9 621.628,-1141.53"
        },
        {
            "source": "P04-1061",
            "target": "P10-1131",
            "d": "M706.731,-1444.64C784.087,-1422.26 892.597,-1374.75 892.597,-1284.23 892.597,-1284.23 892.597,-1284.23 892.597,-1102.75 892.597,-1046.97 929.739,-991.915 957.818,-958.265"
        },
        {
            "source": "P04-1061",
            "target": "N10-1083",
            "d": "M670.259,-1437.64C714.098,-1407.66 778.597,-1352.03 778.597,-1284.23 778.597,-1284.23 778.597,-1284.23 778.597,-1102.75 778.597,-1054.01 780.124,-997.763 781.291,-961.881"
        },
        {
            "source": "P04-1061",
            "target": "D12-1001",
            "d": "M541.705,-1449.26C396.299,-1426.37 123.597,-1371.78 123.597,-1284.23 123.597,-1284.23 123.597,-1284.23 123.597,-923.271 123.597,-870.187 113.184,-842.08 153.597,-807.661 195.491,-771.981 545.012,-754.984 716.05,-748.742"
        },
        {
            "source": "2005",
            "target": "2006",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "H05-1010",
            "target": "N06-1014",
            "d": "M1671.05,-1365.68C1591.25,-1355.52 1431.29,-1334.16 1296.6,-1310.1 1283.6,-1307.78 1269.88,-1305.1 1256.51,-1302.36"
        },
        {
            "source": "H05-1010",
            "target": "P07-1003",
            "d": "M1690.77,-1351.09C1644.26,-1319.58 1555.95,-1259.76 1502.97,-1223.87"
        },
        {
            "source": "H05-1010",
            "target": "P09-1104",
            "d": "M1725.52,-1346.01C1735.03,-1283.16 1759.3,-1122.72 1770.18,-1050.82"
        },
        {
            "source": "H05-1010",
            "target": "P10-1147",
            "d": "M1733.27,-1346.71C1765.62,-1276.91 1854.79,-1085.99 1866.6,-1076.88 1930.52,-1027.6 1993.49,-1103.36 2044.6,-1040.88 2059.72,-1022.39 2056.4,-1007.9 2044.6,-987.141 2036.13,-972.257 2022.52,-960.578 2007.89,-951.574"
        },
        {
            "source": "H05-1010",
            "target": "N10-1015",
            "d": "M1674.23,-1361.55C1601.26,-1343.49 1460.12,-1300.41 1367.6,-1220.36 1286.31,-1150.03 1280.23,-1092.06 1303.6,-987.141 1305.66,-977.873 1309.27,-968.351 1313.26,-959.641"
        },
        {
            "source": "H05-1010",
            "target": "N12-1004",
            "d": "M1773.94,-1370.17C1911.09,-1364.95 2272.96,-1347.71 2310.6,-1310.1 2375.75,-1245 2388.86,-980.233 2348.6,-897.401 2319.78,-838.123 2255.32,-795.127 2208.82,-770.38"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "W06-3105",
            "target": "P08-2007",
            "d": "M1983.04,-1256.42C1974.06,-1225.8 1959.05,-1174.57 1948.99,-1140.23"
        },
        {
            "source": "W06-3105",
            "target": "D08-1033",
            "d": "M2039.29,-1264.6C2064.69,-1254.04 2095.41,-1239.03 2119.6,-1220.36 2149.41,-1197.35 2176.52,-1163.93 2194.63,-1138.94"
        },
        {
            "source": "W06-3105",
            "target": "P10-1147",
            "d": "M2005.54,-1256.8C2041.54,-1193.97 2127.87,-1033.19 2096.6,-987.141 2082.22,-965.97 2059.34,-952.043 2035.49,-942.882"
        },
        {
            "source": "W06-3105",
            "target": "N10-1014",
            "d": "M1929.73,-1271.82C1824.29,-1250.15 1610.28,-1189.25 1519.6,-1040.88 1505.09,-1017.15 1508.59,-985.332 1514.87,-961.085"
        },
        {
            "source": "W06-3105",
            "target": "N12-1004",
            "d": "M2019.12,-1258.71C2030.91,-1247.93 2044.02,-1234.43 2053.6,-1220.36 2077.96,-1184.55 2077.74,-1171.66 2091.6,-1130.62 2104.77,-1091.61 2108.88,-1081.66 2114.6,-1040.88 2123.46,-977.706 2113.13,-960.96 2118.6,-897.401 2122.05,-857.249 2121.36,-846.639 2131.6,-807.661 2133.92,-798.836 2137.1,-789.551 2140.38,-780.934"
        },
        {
            "source": "P06-1055",
            "target": "N07-1051",
            "d": "M3545.22,-1276.27C3653.96,-1267.32 3857.64,-1248.44 4029.6,-1220.36 4043.37,-1218.11 4057.94,-1215.31 4072,-1212.38"
        },
        {
            "source": "P06-1055",
            "target": "D07-1072",
            "d": "M3545.74,-1277.55C3799.65,-1262.32 4599.63,-1214.34 4852.9,-1199.15"
        },
        {
            "source": "P06-1055",
            "target": "D07-1094",
            "d": "M3451.98,-1256.85C3446.6,-1248.11 3440.48,-1238.16 3434.73,-1228.83"
        },
        {
            "source": "P06-1055",
            "target": "W08-1005",
            "d": "M3390.83,-1275.35C3354.48,-1267.78 3314.16,-1252.21 3291.6,-1220.36 3273.93,-1195.42 3289.14,-1162.23 3306,-1137.81"
        },
        {
            "source": "P06-1055",
            "target": "D08-1012",
            "d": "M3398.55,-1269.56C3370.68,-1260.69 3341.23,-1245.68 3324.6,-1220.36 3311.48,-1200.4 3308.12,-1183.91 3324.6,-1166.62 3364.43,-1124.82 4181.74,-1110.12 4489.09,-1106.07"
        },
        {
            "source": "P06-1055",
            "target": "D08-1091",
            "d": "M3546.6,-1278.82C3716.48,-1271.02 4111.74,-1250.21 4242.6,-1220.36 4307.17,-1205.63 4316.87,-1180.66 4381.6,-1166.62 4650.92,-1108.23 4727.94,-1170.59 5000.6,-1130.62 5014.07,-1128.65 5028.32,-1125.82 5041.93,-1122.75"
        },
        {
            "source": "P06-1055",
            "target": "P09-1108",
            "d": "M3547.6,-1281.41C3898.54,-1277.57 5292.89,-1259.96 5484.6,-1220.36 5664.65,-1183.17 5862.9,-1088.31 5953.47,-1041.69"
        },
        {
            "source": "P06-1055",
            "target": "N09-1026",
            "d": "M3388.3,-1279.83C3178.73,-1273.12 2613.66,-1252.5 2429.6,-1220.36 2394.8,-1214.29 2157.62,-1143.18 2124.6,-1130.62 2072.24,-1110.71 2064.47,-1092.24 2010.6,-1076.88 1876.36,-1038.62 1835.43,-1068.46 1698.6,-1040.88 1691.36,-1039.42 1683.86,-1037.66 1676.41,-1035.74"
        },
        {
            "source": "P06-1055",
            "target": "N09-1063",
            "d": "M3547.67,-1282.17C3826.07,-1281.49 4750.66,-1275.05 5041.6,-1220.36 5161.74,-1197.78 5193.9,-1188.46 5301.6,-1130.62 5344.67,-1107.49 5388.43,-1072.44 5417.68,-1046.99"
        },
        {
            "source": "P06-1055",
            "target": "W10-2906",
            "d": "M3390.56,-1275.87C3316.65,-1267.97 3212.65,-1251.56 3187.6,-1220.36 3126.84,-1144.71 3171.77,-1020.57 3199.73,-960.291"
        },
        {
            "source": "P06-1055",
            "target": "P10-2037",
            "d": "M3547.46,-1281.1C3854.52,-1276.49 4945.13,-1257.56 5008.6,-1220.36 5032.78,-1206.19 5018.56,-1182.59 5041.6,-1166.62 5110.44,-1118.9 5154.64,-1173.5 5226.6,-1130.62 5297.53,-1088.36 5271.48,-1025.49 5344.6,-987.141 5399.45,-958.374 5789,-937.93 5979.8,-929.525"
        },
        {
            "source": "P06-1055",
            "target": "P10-2054",
            "d": "M3392.53,-1273.99C3326.4,-1264.99 3237.32,-1248.06 3215.6,-1220.36 3176.25,-1170.18 3190.13,-1135.35 3215.6,-1076.88 3239.7,-1021.54 3295.07,-977.294 3334.58,-951.203"
        },
        {
            "source": "P06-1055",
            "target": "P10-2064",
            "d": "M3545.83,-1277.42C3696.88,-1267.95 4039.13,-1245.6 4326.6,-1220.36 4329.61,-1220.1 5190.07,-1132.28 5192.6,-1130.62 5253.85,-1090.49 5217.35,-1034.03 5273.6,-987.141 5298.78,-966.151 5331.5,-952.126 5362.07,-942.824"
        },
        {
            "source": "P06-1055",
            "target": "P10-1112",
            "d": "M3547.16,-1280.3C3667.33,-1276.05 3888.08,-1262.56 3953.6,-1220.36 4056.02,-1154.4 3996.18,-1054.62 4097.6,-987.141 4145.64,-955.178 4298.63,-938.555 4398.66,-930.882"
        },
        {
            "source": "P06-1055",
            "target": "N10-1061",
            "d": "M3388.53,-1278.9C3278.05,-1272.96 3084.41,-1257.48 3025.6,-1220.36 2999.76,-1204.06 2896.27,-1034.81 2851.46,-960.232"
        },
        {
            "source": "P06-1055",
            "target": "N10-1082",
            "d": "M3387.8,-1280.66C3171.23,-1275.91 2583.43,-1259.52 2505.6,-1220.36 2412.22,-1173.38 2419.57,-1121.96 2353.6,-1040.88 2334.47,-1017.37 2332.6,-1009 2311.6,-987.141 2301.41,-976.539 2289.54,-965.872 2278.37,-956.452"
        },
        {
            "source": "P06-1055",
            "target": "W11-1916",
            "d": "M3392.98,-1273.29C3355.02,-1265.19 3310.68,-1249.76 3280.6,-1220.36 3250.31,-1190.77 3255.81,-1172.35 3248.6,-1130.62 3244.53,-1107.09 3237.14,-1097.84 3248.6,-1076.88 3298.23,-986.065 3394.96,-1041.96 3444.6,-951.141 3456.05,-930.183 3455.98,-918.399 3444.6,-897.401 3436.83,-883.072 3424.15,-871.539 3410.54,-862.493"
        },
        {
            "source": "P06-1055",
            "target": "P11-2127",
            "d": "M3541.44,-1272.75C3594.55,-1264 3666.85,-1248.04 3725.6,-1220.36 3941.49,-1118.64 3937.73,-997.029 4154.6,-897.401 4191.28,-880.549 4293.45,-860.409 4364.17,-847.791"
        },
        {
            "source": "P06-1055",
            "target": "P11-1060",
            "d": "M3387.9,-1280.84C3183.04,-1276.68 2649.37,-1261.94 2581.6,-1220.36 2549.91,-1200.92 2515.12,-1108.52 2495.6,-1076.88 2445.97,-996.463 2462.68,-945.941 2381.6,-897.401 2350.31,-878.668 1854.8,-851.278 1642.23,-840.321"
        },
        {
            "source": "P06-1055",
            "target": "P11-1070",
            "d": "M3388.14,-1279.91C3284.48,-1275.27 3110.77,-1261.41 3063.6,-1220.36 3010.02,-1173.73 2968.89,-957.618 2954.32,-871.292"
        },
        {
            "source": "P06-1055",
            "target": "P12-1101",
            "d": "M3541.38,-1272.84C3578.2,-1264.63 3620.88,-1249.22 3649.6,-1220.36 3697.03,-1172.7 3662.99,-1134.54 3697.6,-1076.88 3754.14,-982.675 3773.38,-953.923 3867.6,-897.401 3919.38,-866.339 3949.17,-896.078 3998.6,-861.401 4022.54,-844.601 4015.67,-828.102 4036.6,-807.661 4048.91,-795.632 4063.85,-784.456 4078.08,-775.006"
        },
        {
            "source": "P06-1055",
            "target": "D12-1001",
            "d": "M3387.79,-1281.17C3175.65,-1277.74 2608.99,-1264.51 2537.6,-1220.36 2533.18,-1217.63 2353.05,-900.076 2348.6,-897.401 2262.98,-846.001 1549.86,-872.289 1450.6,-861.401 1245.33,-838.885 1006.73,-789.663 886.813,-763.294"
        },
        {
            "source": "P06-1055",
            "target": "D12-1091",
            "d": "M3387.97,-1279.94C3196.27,-1273.9 2720.26,-1255.5 2657.6,-1220.36 2503.22,-1133.8 2565.95,-1003.91 2424.6,-897.401 2417.01,-891.687 2175.36,-806.239 2061.67,-766.203"
        },
        {
            "source": "P06-1055",
            "target": "D12-1096",
            "d": "M3521.84,-1263.46C3541.56,-1253.77 3561.88,-1239.79 3573.6,-1220.36 3586.99,-1198.15 3579.3,-1012.72 3583.6,-987.141 3596.18,-912.216 3624.85,-827.818 3641.97,-781.32"
        },
        {
            "source": "P06-1055",
            "target": "D12-1105",
            "d": "M3539.3,-1271.32C3581.93,-1262.44 3635.68,-1246.88 3677.6,-1220.36 3704.66,-1203.24 3702.44,-1188.74 3725.6,-1166.62 3862.75,-1035.67 3886.61,-981.448 4056.6,-897.401 4118.06,-867.014 4140.86,-880.854 4206.6,-861.401 4275.31,-841.07 4288.76,-823.712 4358.6,-807.661 4516.16,-771.446 4704.18,-756.109 4814.57,-749.85"
        },
        {
            "source": "P06-1055",
            "target": "P13-2018",
            "d": "M3523.2,-1263.74C3546.58,-1253.79 3572.68,-1239.55 3591.6,-1220.36 3678.62,-1132.12 3742.81,-802.275 3833.6,-717.921 3856.74,-696.422 3888.23,-682.058 3916.94,-672.632"
        },
        {
            "source": "P06-1055",
            "target": "P13-1012",
            "d": "M3388.38,-1279.66C3292.71,-1274.78 3139.82,-1260.71 3101.6,-1220.36 2946.27,-1056.41 3194.54,-867.81 3025.6,-717.921 2963.27,-662.618 2923.06,-699.462 2841.6,-681.921 2832.87,-680.041 2823.76,-678.011 2814.69,-675.944"
        },
        {
            "source": "P06-1055",
            "target": "J13-2005",
            "d": "M3388.1,-1279.74C3202.15,-1273.37 2750.02,-1254.54 2690.6,-1220.36 2685.87,-1217.64 2470.36,-901.356 2466.6,-897.401 2376.5,-802.691 2355.2,-765.885 2233.6,-717.921 2135.53,-679.238 1836.53,-664.092 1680.96,-658.711"
        },
        {
            "source": "P06-1055",
            "target": "Q14-1037",
            "d": "M3485.76,-1257.03C3492.45,-1246.28 3499.2,-1233.25 3502.6,-1220.36 3508.68,-1197.26 3511.26,-1188.88 3502.6,-1166.62 3494.73,-1146.39 3477.51,-1150.83 3469.6,-1130.62 3388.23,-922.665 3619.62,-791.75 3467.6,-628.181 3432.24,-590.142 3052.83,-600.656 3001.6,-592.181 2986.8,-589.733 2971,-586.159 2956.39,-582.444"
        },
        {
            "source": "P06-1055",
            "target": "P14-1022",
            "d": "M3494.16,-1257.87C3503.9,-1247.35 3513.91,-1234.26 3519.6,-1220.36 3561.99,-1116.71 3486.66,-803.748 3558.6,-717.921 3715.31,-530.949 4425.95,-618.27 4799.6,-592.181 4861.01,-587.893 4930.37,-580.882 4982,-575.239"
        },
        {
            "source": "P06-1055",
            "target": "P16-1188",
            "d": "M3389.92,-1276.82C3308.91,-1269.48 3189.46,-1253.29 3158.6,-1220.36 3076.04,-1132.29 3130.62,-1071.59 3122.6,-951.141 3112.63,-801.342 3149.72,-759.987 3221.6,-628.181 3265.56,-547.555 3334.47,-464.381 3373.67,-419.816"
        },
        {
            "source": "P06-1096",
            "target": "P08-2007",
            "d": "M2447.91,-1266.17C2344.29,-1233.33 2108.65,-1158.65 1998.03,-1123.59"
        },
        {
            "source": "P06-1096",
            "target": "P11-1049",
            "d": "M2562.84,-1278.77C2663.01,-1272.34 2850.73,-1255.85 2906.6,-1220.36 2943.04,-1197.21 3085.34,-919.217 3122.6,-897.401 3152.5,-879.891 3625.27,-851.556 3824.32,-840.331"
        },
        {
            "source": "P06-1111",
            "target": "N10-1083",
            "d": "M484.974,-1256.28C478.888,-1213.08 473.774,-1126.27 519.597,-1076.88 569.466,-1023.13 617.48,-1078.2 680.597,-1040.88 714.34,-1020.93 742.856,-985.975 761.084,-959.708"
        },
        {
            "source": "P06-1111",
            "target": "D12-1001",
            "d": "M389.851,-1270.52C293.351,-1252.98 161.597,-1209.8 161.597,-1104.75 161.597,-1104.75 161.597,-1104.75 161.597,-923.271 161.597,-871.2 142.603,-843.268 180.597,-807.661 218.828,-771.832 550.151,-755.026 715.934,-748.802"
        },
        {
            "source": "N06-1014",
            "target": "P07-1003",
            "d": "M1236.44,-1263.19C1281.98,-1249.17 1343.92,-1230.1 1390.99,-1215.61"
        },
        {
            "source": "N06-1014",
            "target": "D08-1092",
            "d": "M1221.13,-1259.86C1241.89,-1249.05 1266.22,-1235.21 1286.6,-1220.36 1320.94,-1195.34 1355.84,-1161.57 1379.65,-1137"
        },
        {
            "source": "N06-1014",
            "target": "P09-1011",
            "d": "M1267.41,-1282.45C1356.87,-1279.57 1494.39,-1266.89 1603.6,-1220.36 1699,-1179.71 1697.33,-1127.92 1787.6,-1076.88 1816.05,-1060.79 1849.51,-1047.18 1878.54,-1036.85"
        },
        {
            "source": "N06-1014",
            "target": "P09-1104",
            "d": "M1267.39,-1281.67C1346.57,-1277.85 1461.55,-1264.21 1551.6,-1220.36 1636.29,-1179.12 1638.09,-1140.46 1707.6,-1076.88 1718.59,-1066.83 1730.61,-1055.88 1741.44,-1046.02"
        },
        {
            "source": "N06-1014",
            "target": "P10-1147",
            "d": "M1192.59,-1256.57C1226.46,-1208.57 1299.05,-1107.29 1329.6,-1076.88 1376.79,-1029.89 1387.84,-1012.07 1449.6,-987.141 1484.47,-973.064 1701.26,-949.307 1834.85,-935.692"
        },
        {
            "source": "N06-1014",
            "target": "N10-1014",
            "d": "M1181.11,-1256.39C1192.24,-1215.55 1217.77,-1134.82 1258.6,-1076.88 1293,-1028.07 1307.41,-1017.9 1358.6,-987.141 1385.91,-970.729 1418.27,-957.575 1447.34,-947.687"
        },
        {
            "source": "N06-1014",
            "target": "N10-1015",
            "d": "M1166.66,-1256.1C1151.59,-1200.87 1125.17,-1070.49 1182.6,-987.141 1193.01,-972.034 1231.66,-956.283 1267.11,-944.454"
        },
        {
            "source": "N06-1014",
            "target": "N10-1083",
            "d": "M1088.55,-1273.19C1044.92,-1265.06 993.144,-1249.64 954.597,-1220.36 865.135,-1152.41 813.886,-1023.42 793.287,-960.95"
        },
        {
            "source": "N06-1014",
            "target": "N12-1004",
            "d": "M1160.24,-1256.54C1154.93,-1245.71 1149.49,-1232.76 1146.6,-1220.36 1123.03,-1119.36 1105.6,-1078.55 1154.6,-987.141 1166.21,-965.473 1178.39,-967.659 1196.6,-951.141 1222.15,-927.965 1221.13,-911.537 1252.6,-897.401 1344.31,-856.2 1604.81,-880.114 1703.6,-861.401 1784.37,-846.1 1800.7,-827.024 1880.6,-807.661 1967.1,-786.698 1991.37,-793.724 2077.6,-771.661 2083.58,-770.129 2089.78,-768.375 2095.94,-766.518"
        },
        {
            "source": "N06-1014",
            "target": "D12-1001",
            "d": "M1083.79,-1277.71C1026.34,-1271.23 952.626,-1255.96 896.597,-1220.36 768.971,-1139.28 737.438,-1094.24 688.597,-951.141 665.712,-884.089 727.566,-814.87 771.045,-776.377"
        },
        {
            "source": "N06-1014",
            "target": "D12-1079",
            "d": "M1086.81,-1274.26C1051.58,-1266.25 1014.27,-1250.61 992.597,-1220.36 955.465,-1168.52 979.911,-1139.38 992.597,-1076.88 1001.21,-1034.43 1001.13,-1018.89 1030.6,-987.141 1054.4,-961.5 1073.32,-973.049 1100.6,-951.141 1165.19,-899.26 1166.5,-871.257 1219.6,-807.661 1227.32,-798.414 1235.71,-788.411 1243.45,-779.199"
        },
        {
            "source": "N06-1014",
            "target": "D12-1091",
            "d": "M1137.89,-1258.46C1125.55,-1248.35 1113.25,-1235.4 1106.6,-1220.36 1064.68,-1125.56 1061.53,-1080.48 1106.6,-987.141 1113.57,-972.703 1204.7,-903.319 1219.6,-897.401 1394.95,-827.76 1462.46,-917.543 1642.6,-861.401 1688.54,-847.082 1691.79,-825.221 1736.6,-807.661 1800.93,-782.448 1878.19,-765.931 1933.01,-756.358"
        },
        {
            "source": "N06-1041",
            "target": "N10-1061",
            "d": "M2226.8,-1257.72C2309.59,-1205.05 2512.48,-1078.47 2690.6,-987.141 2715.86,-974.187 2744.43,-961.268 2769.13,-950.612"
        },
        {
            "source": "N06-1041",
            "target": "N10-1082",
            "d": "M2207.43,-1256.48C2225.04,-1232.72 2252.33,-1196.68 2277.6,-1166.62 2291.56,-1150.01 2302.73,-1150.85 2310.6,-1130.62 2333.87,-1070.8 2294.53,-999.923 2265.44,-959.022"
        },
        {
            "source": "N06-1041",
            "target": "N10-1083",
            "d": "M2110.49,-1263.76C2095.97,-1260.87 2080.87,-1258.23 2066.6,-1256.36 1623.32,-1198.32 1469.75,-1327.95 1068.6,-1130.62 970.457,-1082.35 869.79,-1001.24 818.071,-956.775"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "P07-1003",
            "target": "D08-1092",
            "d": "M1444.87,-1166.64C1439.94,-1158 1434.37,-1148.22 1429.14,-1139.04"
        },
        {
            "source": "P07-1003",
            "target": "P09-1104",
            "d": "M1499.16,-1169.7C1539.99,-1146.24 1605.59,-1108.72 1662.6,-1076.88 1683.85,-1065.01 1707.45,-1052.05 1727.57,-1041.08"
        },
        {
            "source": "P07-1003",
            "target": "P10-1147",
            "d": "M1484.9,-1167.59C1493.99,-1157.08 1503.3,-1144.15 1508.6,-1130.62 1531.91,-1071.07 1475.97,-1033.91 1519.6,-987.141 1540.76,-964.456 1715.41,-944.903 1833.02,-934.139"
        },
        {
            "source": "P07-1003",
            "target": "N10-1014",
            "d": "M1477.41,-1167.17C1492.66,-1142.06 1509.31,-1103.57 1489.6,-1076.88 1457.27,-1033.12 1404.92,-1084.64 1372.6,-1040.88 1358.41,-1021.67 1359.2,-1006.91 1372.6,-987.141 1386.6,-966.481 1408.72,-952.719 1431.92,-943.553"
        },
        {
            "source": "P07-1003",
            "target": "N10-1015",
            "d": "M1395.07,-1176.5C1370.7,-1167.16 1345.13,-1152.7 1329.6,-1130.62 1294.28,-1080.44 1307.8,-1004.93 1320.46,-960.818"
        },
        {
            "source": "P07-1003",
            "target": "N10-1083",
            "d": "M1377.27,-1190.46C1296.03,-1185.96 1169.67,-1172.15 1068.6,-1130.62 966.358,-1088.62 865.858,-1003.5 815.65,-957.127"
        },
        {
            "source": "P07-1003",
            "target": "D12-1079",
            "d": "M1392.91,-1177.48C1363.31,-1168.07 1329.77,-1153.28 1305.6,-1130.62 1241.06,-1070.14 1239.2,-1037.4 1219.6,-951.141 1205.91,-890.921 1232.56,-821.35 1252.51,-780.604"
        },
        {
            "source": "P07-1107",
            "target": "D08-1033",
            "d": "M2713.49,-1179.64C2603.46,-1163.02 2412.93,-1134.25 2302.83,-1117.62"
        },
        {
            "source": "P07-1107",
            "target": "D09-1120",
            "d": "M2794.47,-1166.3C2789.65,-1135.81 2781.68,-1085.25 2776.29,-1051.06"
        },
        {
            "source": "P07-1107",
            "target": "N10-1061",
            "d": "M2812.2,-1166.55C2830.84,-1127.91 2860.93,-1052.71 2850.6,-987.141 2849.22,-978.405 2846.81,-969.222 2844.14,-960.683"
        },
        {
            "source": "N07-1051",
            "target": "W08-1005",
            "d": "M4067,-1185.97C3945.06,-1175.91 3708.18,-1155.2 3507.6,-1130.62 3476.14,-1126.77 3441.6,-1121.79 3411.64,-1117.23"
        },
        {
            "source": "N07-1051",
            "target": "D08-1012",
            "d": "M4219.85,-1178.78C4298.64,-1163.62 4425.83,-1139.15 4512.22,-1122.53"
        },
        {
            "source": "N07-1051",
            "target": "D08-1091",
            "d": "M4217.46,-1177.67C4239.45,-1173.45 4263.96,-1169.28 4286.6,-1166.62 4611.41,-1128.4 4699.11,-1185.13 5021.6,-1130.62 5029.84,-1129.23 5038.39,-1127.32 5046.79,-1125.17"
        },
        {
            "source": "N07-1051",
            "target": "D08-1092",
            "d": "M4064.99,-1188.66C3941.87,-1183.13 3704.54,-1172.89 3502.6,-1166.62 2775.66,-1144.07 2593.32,-1159.18 1866.6,-1130.62 1734.38,-1125.43 1580.82,-1116.04 1489.62,-1110.12"
        },
        {
            "source": "N07-1051",
            "target": "P09-2036",
            "d": "M4163.19,-1166.99C4169.23,-1155.97 4176.1,-1142.83 4181.6,-1130.62 4193.39,-1104.43 4204.63,-1073.94 4212.66,-1050.85"
        },
        {
            "source": "N07-1051",
            "target": "N09-1026",
            "d": "M4068.21,-1184.65C3971.03,-1174.64 3802.7,-1155.57 3659.6,-1130.62 3551.49,-1111.77 3527.55,-1090.04 3418.6,-1076.88 3039.05,-1031.05 2076.5,-1098.73 1698.6,-1040.88 1690.88,-1039.7 1682.89,-1038.04 1675.01,-1036.12"
        },
        {
            "source": "N07-1051",
            "target": "N09-1063",
            "d": "M4217.47,-1177.75C4239.46,-1173.53 4263.97,-1169.35 4286.6,-1166.62 4432.21,-1149.07 4806.39,-1176.78 4945.6,-1130.62 4986.6,-1117.02 4986.24,-1092.3 5026.6,-1076.88 5129.78,-1037.45 5255.82,-1022.93 5343.93,-1017.7"
        },
        {
            "source": "N07-1051",
            "target": "W10-2906",
            "d": "M4070.1,-1183.23C4001.03,-1173.87 3898.24,-1156.98 3811.6,-1130.62 3754.13,-1113.14 3743.8,-1098.07 3687.6,-1076.88 3548.46,-1024.43 3382.13,-973.465 3289.32,-946.044"
        },
        {
            "source": "N07-1051",
            "target": "P10-1112",
            "d": "M4152.39,-1166.61C4154.82,-1142.43 4155.59,-1105.84 4143.6,-1076.88 4135.29,-1056.83 4118.47,-1061.11 4110.6,-1040.88 4101.94,-1018.62 4095.13,-1005.34 4110.6,-987.141 4113,-984.32 4294.68,-955.604 4405.6,-938.302"
        },
        {
            "source": "N07-1051",
            "target": "N10-1015",
            "d": "M4065.79,-1187.58C3972.58,-1180.63 3817.23,-1164.75 3687.6,-1130.62 3625.55,-1114.29 3616.33,-1090.35 3553.6,-1076.88 3365.93,-1036.58 2882.17,-1052.84 2690.6,-1040.88 2403.05,-1022.93 2332,-1007.28 2044.6,-987.141 1763.94,-967.478 1689.15,-1002.85 1412.6,-951.141 1405.95,-949.898 1399.08,-948.256 1392.29,-946.401"
        },
        {
            "source": "N07-1051",
            "target": "P11-2127",
            "d": "M4138.74,-1166.56C4125.08,-1130.12 4101.62,-1065.48 4097.6,-1040.88 4093.74,-1017.31 4084.61,-1007.18 4097.6,-987.141 4113.94,-961.93 4286.22,-893.052 4378.68,-857.465"
        },
        {
            "source": "N07-1051",
            "target": "P12-2021",
            "d": "M4206.74,-1173.71C4217.25,-1170.91 4228.18,-1168.38 4238.6,-1166.62 4312,-1154.26 4842.43,-1166.61 4907.6,-1130.62 4933.2,-1116.48 4924.81,-1097.46 4945.6,-1076.88 4965.57,-1057.1 4976.78,-1059.77 4997.6,-1040.88 5086.46,-960.254 5163.69,-839.465 5198.67,-780.575"
        },
        {
            "source": "N07-1051",
            "target": "D12-1079",
            "d": "M4066.02,-1187.16C4001.67,-1180.41 3911.48,-1165.15 3839.6,-1130.62 3738.94,-1082.27 3559.08,-847.083 3454.6,-807.661 3375.31,-777.744 2014.3,-774.326 1929.6,-771.661 1738.74,-765.657 1516.84,-756.475 1385.64,-750.818"
        },
        {
            "source": "N07-1051",
            "target": "D12-1096",
            "d": "M4067.07,-1185.94C4013.04,-1178.64 3942.31,-1163.21 3887.6,-1130.62 3860.08,-1114.23 3864.69,-1096.78 3839.6,-1076.88 3813.42,-1056.12 3795.78,-1065.87 3773.6,-1040.88 3737.9,-1000.67 3688.16,-850.624 3666.45,-781.309"
        },
        {
            "source": "N07-1051",
            "target": "D12-1105",
            "d": "M4206.75,-1173.77C4217.25,-1170.96 4228.18,-1168.41 4238.6,-1166.62 4368.71,-1144.25 4712.79,-1188.19 4831.6,-1130.62 4860.42,-1116.66 4856.72,-1099.29 4879.6,-1076.88 4897.16,-1059.68 4911.32,-1063.22 4921.6,-1040.88 4961.22,-954.739 4936.82,-839.094 4920.22,-781.501"
        },
        {
            "source": "N07-1051",
            "target": "P13-2018",
            "d": "M4195.07,-1170.87C4241.16,-1146.56 4309.04,-1102.29 4338.6,-1040.88 4348.96,-1019.36 4342.9,-1010.64 4338.6,-987.141 4337.16,-979.275 4242.57,-723.239 4236.6,-717.921 4192.97,-679.071 4128.5,-663.871 4076.5,-658.242"
        },
        {
            "source": "N07-1051",
            "target": "W14-1607",
            "d": "M4082.55,-1176.38C4046.19,-1166.07 4001,-1150.8 3963.6,-1130.62 3906.85,-1100 3893.02,-1088.55 3849.6,-1040.88 3817.62,-1005.77 3808.76,-995.422 3791.6,-951.141 3753.62,-853.15 3821.56,-797.226 3752.6,-717.921 3730.81,-692.868 3500.32,-633.798 3467.6,-628.181 3304.19,-600.129 2158.66,-575.383 1804,-568.322"
        },
        {
            "source": "N07-1051",
            "target": "P14-2133",
            "d": "M4206.77,-1173.89C4217.27,-1171.07 4228.2,-1168.49 4238.6,-1166.62 4343.89,-1147.75 4620.86,-1174.1 4718.6,-1130.62 4771.37,-1107.15 4789.88,-1093.98 4812.6,-1040.88 4829.98,-1000.26 4755.88,-705.519 4729.01,-602.077"
        },
        {
            "source": "N07-1051",
            "target": "P14-1020",
            "d": "M4113.18,-1168.99C4064.08,-1135.55 3979.54,-1074.57 3963.6,-1040.88 3953.38,-1019.29 3952.39,-1008.23 3963.6,-987.141 3997.07,-924.177 4032.54,-928.734 4096.6,-897.401 4142.81,-874.798 4170.11,-897.66 4206.6,-861.401 4252.81,-815.481 4269.6,-774.093 4236.6,-717.921 4181.04,-623.354 4052.57,-587.773 3963.89,-574.386"
        },
        {
            "source": "N07-1051",
            "target": "P14-1022",
            "d": "M4206.74,-1173.74C4217.25,-1170.93 4228.18,-1168.39 4238.6,-1166.62 4377.28,-1143.04 4747.99,-1199.58 4870.6,-1130.62 4895.87,-1116.41 4888.89,-1099.04 4907.6,-1076.88 4922.4,-1059.35 4930.62,-1059.08 4944.6,-1040.88 5008.3,-957.913 5059.6,-940.135 5059.6,-835.531 5059.6,-835.531 5059.6,-835.531 5059.6,-743.791 5059.6,-695.064 5059.6,-638.812 5059.6,-602.925"
        },
        {
            "source": "N07-1051",
            "target": "P15-1030",
            "d": "M4127.65,-1167.4C4119.65,-1156.66 4111.13,-1143.62 4105.6,-1130.62 4080.41,-1071.49 4067.34,-1048.13 4087.6,-987.141 4103.29,-939.904 4115.3,-927.954 4154.6,-897.401 4187.63,-871.715 4206.04,-883.469 4241.6,-861.401 4413.81,-754.515 4585.12,-580.176 4652.12,-508.69"
        },
        {
            "source": "N07-1051",
            "target": "P17-1076",
            "d": "M4082.11,-1176.63C4052.07,-1167.01 4017.58,-1152.3 3991.6,-1130.62 3929.57,-1078.85 3787.31,-879.339 3824.6,-807.661 3937,-591.588 4316.6,-809.87 4316.6,-566.311 4316.6,-566.311 4316.6,-566.311 4316.6,-474.571 4316.6,-425.354 4323.91,-368.894 4329.46,-333.121"
        },
        {
            "source": "N07-1051",
            "target": "N18-1091",
            "d": "M4206.76,-1173.83C4217.26,-1171.02 4228.19,-1168.45 4238.6,-1166.62 4355.31,-1146.14 4668.5,-1190.76 4770.6,-1130.62 4929.54,-1037.01 4768.55,-896.544 4814.6,-717.921 4841.91,-611.982 4902.48,-607.324 4933.6,-502.441 4963.13,-402.915 4965.7,-371.263 4946.6,-269.22 4944.93,-260.334 4942.04,-251.083 4938.82,-242.525"
        },
        {
            "source": "N07-1051",
            "target": "P19-1031",
            "d": "M4206.17,-1173.5C4242.96,-1160.56 4286.51,-1143.51 4301.6,-1130.62 4337,-1100.4 4336.9,-1083.51 4355.6,-1040.88 4382.28,-980.029 4350.94,-944.706 4397.6,-897.401 4435.8,-858.667 4482.27,-905.165 4514.6,-861.401 4528.79,-842.189 4517.52,-831.366 4514.6,-807.661 4496.01,-657.099 4431.6,-628.275 4431.6,-476.571 4431.6,-476.571 4431.6,-476.571 4431.6,-384.831 4431.6,-302.321 4431.11,-205.482 4430.82,-154.047"
        },
        {
            "source": "N07-1052",
            "target": "D08-1012",
            "d": "M5291.45,-1182.51C5147.72,-1166.24 4863.48,-1134.06 4709.56,-1116.63"
        },
        {
            "source": "N07-1052",
            "target": "P09-1108",
            "d": "M5445.2,-1173.83C5563.74,-1140.12 5812.59,-1069.34 5934,-1034.81"
        },
        {
            "source": "N07-1052",
            "target": "N09-1063",
            "d": "M5390.21,-1166.68C5402.81,-1136.06 5423.88,-1084.83 5438,-1050.49"
        },
        {
            "source": "D07-1072",
            "target": "D08-1091",
            "d": "M4972.57,-1171.92C4997.98,-1159.46 5030.61,-1143.47 5057.65,-1130.21"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "P08-2007",
            "target": "D08-1033",
            "d": "M2001.54,-1103.75C2042.08,-1103.75 2082.62,-1103.75 2123.17,-1103.75"
        },
        {
            "source": "P08-2007",
            "target": "P09-1104",
            "d": "M1900.38,-1082.18C1877.52,-1069.87 1848.26,-1054.12 1823.81,-1040.97"
        },
        {
            "source": "P08-2007",
            "target": "P10-1147",
            "d": "M1984.57,-1085.17C2022.88,-1070.07 2072.06,-1049.41 2077.6,-1040.88 2090.61,-1020.85 2090.55,-1007.21 2077.6,-987.141 2065.63,-968.603 2046.62,-955.562 2026.53,-946.408"
        },
        {
            "source": "P08-1088",
            "target": "D11-1029",
            "d": "M424.154,-1078.75C435.114,-1068.27 446.574,-1055.13 453.597,-1040.88 480.518,-986.288 485.855,-914.469 486.363,-871.837"
        },
        {
            "source": "P08-1100",
            "target": "N09-1069",
            "d": "M612.643,-1076.9C610.111,-1068.67 607.264,-1059.42 604.559,-1050.63"
        },
        {
            "source": "D08-1012",
            "target": "P09-1104",
            "d": "M4499.9,-1100.12C4308.48,-1095.33 3889.16,-1084.94 3535.6,-1076.88 3348.6,-1072.62 2036.86,-1073.03 1852.6,-1040.88 1846.06,-1039.74 1839.33,-1038.15 1832.69,-1036.3"
        },
        {
            "source": "D08-1012",
            "target": "N09-1026",
            "d": "M4499.9,-1100.06C4308.48,-1095.17 3889.17,-1084.62 3535.6,-1076.88 3331.5,-1072.41 1900.42,-1071.64 1698.6,-1040.88 1690.88,-1039.71 1682.89,-1038.05 1675.01,-1036.13"
        },
        {
            "source": "D08-1012",
            "target": "D09-1147",
            "d": "M4634.12,-1077.84C4645.75,-1068.04 4659.23,-1056.66 4671.5,-1046.31"
        },
        {
            "source": "D08-1012",
            "target": "P10-1112",
            "d": "M4569.88,-1078.27C4557.12,-1067.9 4543.42,-1054.96 4533.6,-1040.88 4516.72,-1016.68 4505.29,-985.161 4498.32,-961.148"
        },
        {
            "source": "D08-1012",
            "target": "P13-1021",
            "d": "M4587.28,-1077.11C4581.04,-1066.4 4574.76,-1053.52 4571.6,-1040.88 4565.8,-1017.71 4568.37,-1010.81 4571.6,-987.141 4573.84,-970.688 4579.35,-967.595 4581.6,-951.141 4584.82,-927.476 4589.13,-920.068 4581.6,-897.401 4565.91,-850.164 4555.08,-836.617 4514.6,-807.661 4408.99,-732.132 4364.62,-739.537 4236.6,-717.921 3882.63,-658.155 2978,-724.877 2621.6,-681.921 2610.84,-680.625 2599.63,-678.786 2588.58,-676.679"
        },
        {
            "source": "D08-1012",
            "target": "P14-2020",
            "d": "M4604.44,-1076.6C4604.04,-1025.17 4602.71,-914.342 4598.6,-897.401 4588.17,-854.421 4584.41,-841.218 4555.6,-807.661 4462.23,-698.902 4307.11,-625.573 4219.37,-590.39"
        },
        {
            "source": "D08-1033",
            "target": "P09-1011",
            "d": "M2160.22,-1084.04C2118.1,-1070.3 2060.74,-1051.59 2016.36,-1037.11"
        },
        {
            "source": "D08-1033",
            "target": "P10-1147",
            "d": "M2209.79,-1076.82C2200.55,-1050.47 2182.64,-1010.34 2153.6,-987.141 2122.25,-962.103 2081.14,-947.197 2043.4,-938.323"
        },
        {
            "source": "D08-1033",
            "target": "N12-1004",
            "d": "M2229.42,-1076.83C2238.98,-1051.61 2248.09,-1013.36 2229.6,-987.141 2202.13,-948.196 2156.06,-990.087 2128.6,-951.141 2114.83,-931.622 2126.5,-921.193 2128.6,-897.401 2132.14,-857.324 2140.85,-811.974 2147.42,-781.4"
        },
        {
            "source": "D08-1091",
            "target": "P10-1112",
            "d": "M5064.69,-1082.3C5007.28,-1056.85 4904.32,-1013.54 4812.6,-987.141 4733.14,-964.272 4640.22,-947.478 4574.49,-937.204"
        },
        {
            "source": "D08-1091",
            "target": "D12-1105",
            "d": "M5103.6,-1076.77C5097.6,-1052.85 5087.3,-1016.71 5073.6,-987.141 5033.87,-901.398 5015.09,-884.151 4959.6,-807.661 4952.58,-797.988 4944.53,-787.844 4936.94,-778.634"
        },
        {
            "source": "D08-1091",
            "target": "P14-1022",
            "d": "M5130.58,-1077.72C5191.36,-1003.84 5360.7,-787.59 5320.6,-717.921 5279.34,-646.25 5189.77,-605.329 5127.13,-584.482"
        },
        {
            "source": "D08-1091",
            "target": "P15-1030",
            "d": "M5108.27,-1076.87C5103.74,-1011.06 5084.24,-834.297 5002.6,-717.921 4993.95,-705.597 4811.99,-546.983 4799.6,-538.441 4779.61,-524.662 4756.17,-511.781 4735.48,-501.378"
        },
        {
            "source": "D08-1092",
            "target": "W10-2906",
            "d": "M1424.91,-1077.36C1443.38,-1049.39 1477.31,-1005.92 1519.6,-987.141 1591.37,-955.275 2854.2,-955.693 2932.6,-951.141 2997.62,-947.366 3070.78,-940.674 3126.7,-935.041"
        },
        {
            "source": "D08-1092",
            "target": "P10-1131",
            "d": "M1363.34,-1083.25C1287.26,-1051.18 1135.7,-987.286 1050.48,-951.359"
        },
        {
            "source": "D08-1092",
            "target": "N10-1015",
            "d": "M1370.92,-1080.81C1356.83,-1070.63 1342.44,-1057.15 1334.6,-1040.88 1322.75,-1016.31 1322.92,-985.247 1325.68,-961.51"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "P09-1011",
            "target": "P11-1060",
            "d": "M1900.11,-991.603C1825.76,-958.779 1683.88,-896.138 1604.33,-861.019"
        },
        {
            "source": "P09-1011",
            "target": "J13-2005",
            "d": "M1885.46,-995.495C1874.53,-992.606 1863.25,-989.716 1852.6,-987.141 1778.37,-969.201 1749.33,-991.655 1684.6,-951.141 1658.9,-935.058 1668.5,-913.152 1642.6,-897.401 1568.42,-852.289 1506.04,-928.21 1450.6,-861.401 1402.37,-803.291 1485.32,-727.111 1540.81,-685.808"
        },
        {
            "source": "P09-1104",
            "target": "P10-1147",
            "d": "M1815.12,-991.995C1836.75,-980.493 1863.85,-966.082 1887.32,-953.6"
        },
        {
            "source": "P09-1104",
            "target": "N10-1014",
            "d": "M1726.01,-995.467C1689.1,-982.411 1638.25,-964.418 1597.26,-949.918"
        },
        {
            "source": "P09-1104",
            "target": "N10-1015",
            "d": "M1727.37,-994.958C1718.24,-992.009 1708.7,-989.238 1699.6,-987.141 1574.32,-958.28 1538.09,-979.042 1412.6,-951.141 1406.33,-949.748 1399.86,-948.071 1393.43,-946.247"
        },
        {
            "source": "P09-1104",
            "target": "N10-1083",
            "d": "M1728.31,-994.612C1718.93,-991.618 1709.05,-988.919 1699.6,-987.141 1339.78,-919.468 1238.27,-1008.08 876.597,-951.141 868.365,-949.845 859.823,-948.091 851.394,-946.103"
        },
        {
            "source": "P09-1104",
            "target": "N12-1004",
            "d": "M1781.96,-987.096C1789.38,-961.52 1803.92,-922.679 1828.6,-897.401 1867.91,-857.132 2013.42,-798.601 2096.48,-767.353"
        },
        {
            "source": "P09-1104",
            "target": "D12-1091",
            "d": "M1763.77,-987.259C1746.2,-944.727 1719.56,-859.376 1761.6,-807.661 1782.95,-781.389 1868.29,-764.148 1931.61,-754.696"
        },
        {
            "source": "P09-1108",
            "target": "P10-2037",
            "d": "M6026.59,-988.103C6035.24,-978.737 6045.2,-967.942 6054.41,-957.972"
        },
        {
            "source": "N09-1008",
            "target": "P10-1105",
            "d": "M337.597,-986.686C337.597,-978.72 337.597,-969.829 337.597,-961.34"
        },
        {
            "source": "N09-1008",
            "target": "D11-1029",
            "d": "M381.954,-989.333C398.095,-979.154 415.558,-966.132 428.597,-951.141 448.992,-927.694 464.033,-895.666 473.527,-871.266"
        },
        {
            "source": "N09-1008",
            "target": "D11-1032",
            "d": "M286.404,-990.309C270.745,-980.633 255.356,-967.688 246.597,-951.141 235.423,-930.032 239.58,-920.232 246.597,-897.401 249.63,-887.536 254.829,-877.831 260.557,-869.138"
        },
        {
            "source": "N09-1026",
            "target": "P09-2036",
            "d": "M1650.19,-1037.34C1667.32,-1045.92 1687.85,-1054.58 1707.6,-1058.88 1772.44,-1073.02 4031.18,-1070.02 4096.6,-1058.88 4118.34,-1055.18 4141.31,-1048.18 4161.69,-1040.79"
        },
        {
            "source": "N09-1026",
            "target": "P09-1104",
            "d": "M1690.33,-1014.01C1692.76,-1014.01 1695.2,-1014.01 1697.63,-1014.01"
        },
        {
            "source": "N09-1026",
            "target": "N10-1014",
            "d": "M1586.32,-988.103C1577.57,-978.737 1567.48,-967.942 1558.16,-957.972"
        },
        {
            "source": "N09-1026",
            "target": "N10-1082",
            "d": "M1666.67,-994.764C1677.21,-991.871 1688.17,-989.169 1698.6,-987.141 1853.83,-956.946 1895.59,-970.038 2052.6,-951.141 2081.55,-947.657 2112.98,-943.448 2141.62,-939.448"
        },
        {
            "source": "N09-1063",
            "target": "P09-1108",
            "d": "M5551.61,-1014.01C5672.1,-1014.01 5792.6,-1014.01 5913.1,-1014.01"
        },
        {
            "source": "N09-1063",
            "target": "P10-2064",
            "d": "M5452.6,-986.686C5452.6,-978.72 5452.6,-969.829 5452.6,-961.34"
        },
        {
            "source": "N09-1069",
            "target": "N10-1083",
            "d": "M638.868,-991.995C665.261,-979.743 698.762,-964.19 726.78,-951.183"
        },
        {
            "source": "N09-1069",
            "target": "P11-1060",
            "d": "M606.504,-987.289C621.894,-959.688 650.412,-917.158 688.597,-897.401 754.829,-863.133 1239.98,-844.676 1450.06,-838.218"
        },
        {
            "source": "N09-1069",
            "target": "J13-2005",
            "d": "M591.029,-987.021C588.192,-946.339 587.452,-866.223 617.597,-807.661 644.777,-754.859 661.986,-741.253 716.597,-717.921 786.053,-688.247 1276.65,-667.18 1488.06,-659.38"
        },
        {
            "source": "D09-1120",
            "target": "N10-1061",
            "d": "M2787.95,-987.633C2793.99,-978.798 2800.87,-968.739 2807.32,-959.317"
        },
        {
            "source": "D09-1120",
            "target": "P12-1041",
            "d": "M2748.85,-988.323C2740.84,-977.649 2732.72,-964.544 2728.6,-951.141 2721.58,-928.311 2719.03,-919.286 2728.6,-897.401 2753.55,-840.317 2812.85,-798.308 2858.03,-773.172"
        },
        {
            "source": "D09-1120",
            "target": "D13-1027",
            "d": "M2823.06,-995.726C2855.29,-984.439 2897.06,-968.591 2932.6,-951.141 3084,-876.798 3239.69,-748.317 3308.34,-688.472"
        },
        {
            "source": "D09-1120",
            "target": "D13-1203",
            "d": "M2743.48,-988.867C2733.67,-978.37 2723.74,-965.247 2718.6,-951.141 2681.63,-849.698 2711.82,-795.804 2786.6,-717.921 2797.76,-706.297 2830.56,-690.81 2860.55,-678.238"
        },
        {
            "source": "D09-1120",
            "target": "Q14-1037",
            "d": "M2740.34,-989.669C2727.46,-978.821 2712.85,-965.219 2701.6,-951.141 2627.52,-858.434 2667.61,-787.667 2571.6,-717.921 2504.53,-669.202 2441.25,-745.942 2388.6,-681.921 2373.43,-663.474 2372.75,-646.054 2388.6,-628.181 2417.84,-595.197 2705.03,-576.348 2836.04,-569.405"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "P10-1105",
            "target": "D11-1029",
            "d": "M376.096,-900.448C395.196,-889.125 418.43,-875.35 438.575,-863.408"
        },
        {
            "source": "P10-1105",
            "target": "D11-1032",
            "d": "M323.161,-897.421C318.376,-888.853 312.971,-879.175 307.881,-870.061"
        },
        {
            "source": "P10-1112",
            "target": "P11-2127",
            "d": "M4473.98,-897.421C4468.71,-888.687 4462.73,-878.8 4457.14,-869.532"
        },
        {
            "source": "P10-1131",
            "target": "D12-1001",
            "d": "M962.951,-897.843C931.136,-866.298 876.859,-812.482 842.142,-778.059"
        },
        {
            "source": "P10-1147",
            "target": "N12-1004",
            "d": "M1970.95,-898.219C2009.98,-865.993 2077.71,-810.083 2119.4,-775.674"
        },
        {
            "source": "N10-1015",
            "target": "N10-1014",
            "d": "M1404.2,-924.271C1406.48,-924.271 1408.77,-924.271 1411.05,-924.271"
        },
        {
            "source": "N10-1015",
            "target": "D12-1079",
            "d": "M1323.73,-897.464C1313.2,-866.838 1295.59,-815.608 1283.79,-781.269"
        },
        {
            "source": "N10-1015",
            "target": "D12-1105",
            "d": "M1381.97,-904.841C1391.98,-901.809 1402.52,-899.103 1412.6,-897.401 1772.48,-836.61 2693.65,-915.503 3054.6,-861.401 3141.39,-848.393 3157.91,-821.292 3244.6,-807.661 3462.51,-773.397 4016.16,-779.904 4236.6,-771.661 4442.77,-763.951 4684.52,-754.558 4814.48,-749.48"
        },
        {
            "source": "N10-1061",
            "target": "P10-2054",
            "d": "M2893.07,-944.242C2984.78,-970.436 3160.48,-1008.68 3303.6,-969.141 3316.18,-965.666 3328.77,-959.461 3339.95,-952.753"
        },
        {
            "source": "N10-1061",
            "target": "W11-1916",
            "d": "M2914.36,-912.618C2997.27,-901.599 3127.66,-882.995 3239.6,-861.401 3249.28,-859.534 3259.4,-857.403 3269.41,-855.184"
        },
        {
            "source": "N10-1061",
            "target": "P12-1041",
            "d": "M2827.96,-897.345C2826.65,-872.814 2827.81,-835.68 2842.6,-807.661 2848.9,-795.719 2858.56,-785.123 2868.81,-776.222"
        },
        {
            "source": "N10-1061",
            "target": "P13-1012",
            "d": "M2810.62,-897.903C2802.88,-887.124 2794.49,-874.125 2788.6,-861.401 2762.45,-804.913 2746.05,-733.98 2737.88,-691.986"
        },
        {
            "source": "N10-1061",
            "target": "D13-1203",
            "d": "M2817.96,-897.518C2799.77,-856.461 2771.93,-774.99 2805.6,-717.921 2816.76,-699.01 2835.99,-685.478 2855.41,-675.992"
        },
        {
            "source": "N10-1061",
            "target": "Q14-1037",
            "d": "M2899.71,-906.329C2963.7,-890.314 3049.72,-867.812 3054.6,-861.401 3092.82,-811.195 3082.25,-720.182 3012.6,-628.181 2997.94,-608.818 2975.38,-594.71 2954.33,-584.9"
        },
        {
            "source": "N10-1083",
            "target": "P10-1131",
            "d": "M867.492,-924.271C869.953,-924.271 872.415,-924.271 874.876,-924.271"
        },
        {
            "source": "N10-1083",
            "target": "P13-1021",
            "d": "M814.394,-899.111C876.285,-853.676 1018.97,-756.604 1156.6,-717.921 1215.36,-701.406 2071.96,-670.772 2388.32,-659.956"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "W11-1916",
            "target": "D13-1027",
            "d": "M3348.86,-807.343C3348,-776.846 3346.58,-726.294 3345.61,-692.103"
        },
        {
            "source": "P11-1027",
            "target": "P12-1101",
            "d": "M4123.7,-807.206C4124.33,-799.239 4125.04,-790.348 4125.72,-781.86"
        },
        {
            "source": "P11-1049",
            "target": "D15-1032",
            "d": "M3927.7,-807.832C3946.4,-780.465 3979.92,-738.34 4020.6,-717.921 4144.56,-655.696 4197.11,-719.607 4330.6,-681.921 4476.32,-640.781 4497.76,-591.435 4639.6,-538.441 4693.75,-518.206 4709.19,-518.95 4764.6,-502.441 4770.64,-500.639 4776.93,-498.745 4783.22,-496.835"
        },
        {
            "source": "P11-1049",
            "target": "P16-1188",
            "d": "M3885.25,-808.953C3859.89,-785.398 3820.38,-748.944 3785.6,-717.921 3660.66,-606.499 3509.83,-477.305 3441.05,-418.681"
        },
        {
            "source": "P11-1060",
            "target": "J13-2005",
            "d": "M1552.2,-807.343C1558.73,-776.846 1569.56,-726.294 1576.88,-692.103"
        },
        {
            "source": "P11-1060",
            "target": "W14-1607",
            "d": "M1576.5,-809.026C1607.84,-781.546 1655.88,-733.962 1680.6,-681.921 1692.4,-657.064 1697.04,-626.194 1698.78,-602.607"
        },
        {
            "source": "P11-1060",
            "target": "D15-1138",
            "d": "M1591.89,-811.559C1611.57,-800.919 1634.26,-787.101 1652.6,-771.661 1732.34,-704.517 1767.88,-690.116 1803.6,-592.181 1811.78,-569.742 1815.4,-559.203 1803.6,-538.441 1795.51,-524.224 1782.73,-512.933 1768.85,-504.102"
        },
        {
            "source": "P11-1060",
            "target": "N16-1181",
            "d": "M1527.76,-808.271C1504.97,-775.131 1469.6,-714.325 1469.6,-656.051 1469.6,-656.051 1469.6,-656.051 1469.6,-564.311 1469.6,-515.584 1469.6,-459.331 1469.6,-423.445"
        },
        {
            "source": "P11-1070",
            "target": "P12-1041",
            "d": "M2938.87,-807.681C2935.75,-799.365 2932.23,-790.005 2928.89,-781.129"
        },
        {
            "source": "P11-1070",
            "target": "D12-1001",
            "d": "M2851.9,-831.549C2582.98,-825.745 1803.52,-806.93 1156.6,-771.661 1069.5,-766.912 970.253,-759.29 900.278,-753.547"
        },
        {
            "source": "P11-1070",
            "target": "P14-1098",
            "d": "M2986.25,-809.504C3000.08,-799.199 3014.95,-786.182 3025.6,-771.661 3064.14,-719.102 3086.81,-645.567 3097.67,-602.243"
        },
        {
            "source": "D11-1029",
            "target": "P13-1021",
            "d": "M520.125,-810.261C563.722,-782.334 642.064,-736.898 716.597,-717.921 736.905,-712.75 1994.66,-672.296 2388.21,-659.749"
        },
        {
            "source": "D11-1029",
            "target": "D13-1087",
            "d": "M485.597,-807.343C485.597,-776.846 485.597,-726.294 485.597,-692.103"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "P12-1041",
            "target": "P13-1012",
            "d": "M2867.73,-720.967C2843.46,-709.391 2813.8,-695.251 2788.38,-683.127"
        },
        {
            "source": "P12-1041",
            "target": "D13-1203",
            "d": "M2916.2,-717.466C2916.38,-709.499 2916.58,-700.608 2916.78,-692.12"
        },
        {
            "source": "P12-1041",
            "target": "Q14-1037",
            "d": "M2958.81,-720.205C2972.53,-710.329 2986.08,-697.484 2993.6,-681.921 3003.99,-660.415 3004.88,-649.234 2993.6,-628.181 2984.32,-610.874 2967.84,-597.579 2951.3,-587.83"
        },
        {
            "source": "N12-1004",
            "target": "D12-1105",
            "d": "M2223.35,-750.931C2342.7,-761.404 2598.27,-782.305 2814.6,-789.661 2924.87,-793.411 4692.4,-811.244 4800.6,-789.661 4818.99,-785.992 4838.18,-779.126 4855.19,-771.85"
        },
        {
            "source": "N12-1004",
            "target": "P13-1012",
            "d": "M2222.14,-736.985C2312.69,-727.258 2479.59,-707.671 2620.6,-681.921 2629.92,-680.219 2639.63,-678.249 2649.27,-676.171"
        },
        {
            "source": "N12-1004",
            "target": "Q14-1037",
            "d": "M2186.22,-720.646C2223.9,-693.511 2290.85,-649.536 2355.6,-628.181 2443.56,-599.17 2712.54,-578.523 2836.64,-570.281"
        },
        {
            "source": "D12-1091",
            "target": "P19-1340",
            "d": "M2028.29,-719.862C2058.48,-687.917 2105.6,-628.148 2105.6,-566.311 2105.6,-566.311 2105.6,-566.311 2105.6,-295.09 2105.6,-196.002 3638.48,-135.799 4059.65,-121.128"
        },
        {
            "source": "D12-1096",
            "target": "P13-2018",
            "d": "M3720.98,-726.666C3778.58,-711.58 3862.23,-689.67 3921.05,-674.267"
        },
        {
            "source": "D12-1096",
            "target": "D13-1027",
            "d": "M3592.29,-725.93C3544.15,-712.35 3477.57,-693.565 3425.56,-678.892"
        },
        {
            "source": "D12-1105",
            "target": "P14-1022",
            "d": "M4929.91,-718.739C4956.55,-687.424 5002.22,-633.746 5031.65,-599.163"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "P13-1012",
            "target": "D13-1203",
            "d": "M2832.95,-655.051C2835.28,-655.051 2837.61,-655.051 2839.94,-655.051"
        },
        {
            "source": "P13-1012",
            "target": "Q14-1037",
            "d": "M2776.44,-630.768C2800.72,-618.164 2830.65,-602.626 2855.04,-589.964"
        },
        {
            "source": "P13-1021",
            "target": "P14-2020",
            "d": "M2578.5,-635.477C2592.72,-632.538 2607.56,-629.905 2621.6,-628.181 3215.13,-555.297 3370.62,-641.23 3966.6,-592.181 3998.72,-589.537 4033.77,-585.106 4064.74,-580.651"
        },
        {
            "source": "J13-2005",
            "target": "W14-1607",
            "d": "M1616.33,-629.843C1629.66,-619.667 1645.34,-607.711 1659.46,-596.932"
        },
        {
            "source": "J13-2005",
            "target": "D15-1138",
            "d": "M1580.71,-628.028C1578.36,-603.085 1578.55,-565.354 1595.6,-538.441 1604.49,-524.403 1617.81,-513.134 1631.98,-504.263"
        },
        {
            "source": "D13-1203",
            "target": "D13-1027",
            "d": "M2985.15,-655.051C3063.58,-655.051 3142.02,-655.051 3220.45,-655.051"
        },
        {
            "source": "D13-1203",
            "target": "Q14-1037",
            "d": "M2912.59,-628.2C2911.01,-620.062 2909.24,-610.922 2907.55,-602.216"
        },
        {
            "source": "D13-1203",
            "target": "D15-1032",
            "d": "M2936.31,-629.165C2944.72,-617.915 2954.72,-604.425 2963.6,-592.181 2980.77,-568.499 2975.76,-552.153 3001.6,-538.441 3088.13,-492.52 4667.88,-518.008 4764.6,-502.441 4771.74,-501.29 4779.13,-499.678 4786.41,-497.815"
        },
        {
            "source": "D13-1203",
            "target": "P16-1188",
            "d": "M2877.1,-633.273C2861.78,-623.128 2846.03,-609.361 2837.6,-592.181 2827.07,-570.739 2824.2,-558.212 2837.6,-538.441 2888.75,-462.966 3145.78,-418.853 3295.29,-399.188"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "W14-1607",
            "target": "D15-1138",
            "d": "M1699.6,-537.986C1699.6,-530.019 1699.6,-521.128 1699.6,-512.64"
        },
        {
            "source": "Q14-1037",
            "target": "D15-1032",
            "d": "M2940.2,-546.641C2949.35,-543.294 2959.17,-540.271 2968.6,-538.441 3360.47,-462.349 4370.47,-565.803 4764.6,-502.441 4771.74,-501.292 4779.13,-499.68 4786.41,-497.817"
        },
        {
            "source": "Q14-1037",
            "target": "P16-1188",
            "d": "M2942.64,-548.212C2951.22,-544.956 2960.19,-541.572 2968.6,-538.441 3077.65,-497.806 3104.61,-486.752 3214.6,-448.701 3252.18,-435.7 3294.2,-421.874 3329.06,-410.597"
        },
        {
            "source": "P14-2133",
            "target": "P15-1030",
            "d": "M4708.4,-538.46C4704.77,-530.071 4700.67,-520.617 4696.8,-511.671"
        },
        {
            "source": "P14-1020",
            "target": "P16-1188",
            "d": "M3824.3,-542.69C3770.29,-518.372 3678.98,-478.401 3598.6,-448.701 3560.68,-434.693 3517.94,-420.913 3482.16,-409.926"
        },
        {
            "source": "P14-1022",
            "target": "P15-1030",
            "d": "M4998.29,-551.389C4978.59,-547.245 4956.67,-542.641 4936.6,-538.441 4859.73,-522.358 4839.59,-522.28 4763.6,-502.441 4757.44,-500.832 4751.05,-499.041 4744.69,-497.174"
        },
        {
            "source": "P14-1022",
            "target": "D15-1032",
            "d": "M5012.77,-544.838C4981.28,-531.743 4939.56,-514.394 4906.05,-500.462"
        },
        {
            "source": "P14-1022",
            "target": "P18-1249",
            "d": "M5057.98,-538.041C5052.87,-479.543 5031.85,-338.279 4946.6,-269.22 4924.5,-251.321 4767.87,-230.777 4659.42,-218.443"
        },
        {
            "source": "P14-1022",
            "target": "N18-1091",
            "d": "M5062.66,-538.212C5067.72,-483.591 5072.39,-355.185 5017.6,-269.22 5008.57,-255.063 4995.02,-243.471 4980.95,-234.307"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "P15-1030",
            "target": "D15-1032",
            "d": "M4755.31,-475.571C4757.77,-475.571 4760.23,-475.571 4762.69,-475.571"
        },
        {
            "source": "P15-1030",
            "target": "P18-1249",
            "d": "M4636.57,-454.253C4589.76,-430.353 4519.15,-385.807 4488.6,-322.96 4478.16,-301.479 4479.1,-291.136 4488.6,-269.22 4493.62,-257.622 4502.01,-247.2 4511.15,-238.363"
        },
        {
            "source": "P15-1030",
            "target": "2020.acl-main.557",
            "d": "M4691.66,-448.86C4704.08,-414.734 4723.6,-352.343 4723.6,-297.09 4723.6,-297.09 4723.6,-297.09 4723.6,-205.35 4723.6,-153.593 4722.17,-139.949 4709.6,-89.7401 4707.38,-80.889 4704.24,-71.5931 4700.96,-62.9747"
        },
        {
            "source": "D15-1032",
            "target": "P16-1188",
            "d": "M4795.77,-456.082C4785.54,-453.136 4774.82,-450.477 4764.6,-448.701 4527.86,-407.553 3809.61,-392.748 3523.29,-388.39"
        },
        {
            "source": "D15-1032",
            "target": "Q17-1031",
            "d": "M4848.01,-448.383C4847.32,-417.886 4846.18,-367.334 4845.41,-333.142"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "D16-1125",
            "target": "P17-1022",
            "d": "M6019.6,-358.506C6019.6,-350.539 6019.6,-341.648 6019.6,-333.159"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "P17-2025",
            "target": "D17-1178",
            "d": "M4201.78,-319.447C4221.6,-328.023 4245.21,-336.678 4267.6,-340.96 4354.33,-357.55 4455.17,-337.75 4521.46,-319.455"
        },
        {
            "source": "P17-2025",
            "target": "P18-2075",
            "d": "M4157.9,-268.765C4158.9,-260.799 4160.01,-251.908 4161.08,-243.419"
        },
        {
            "source": "P17-2025",
            "target": "P18-1249",
            "d": "M4223.6,-277.539C4235.26,-274.683 4247.26,-271.814 4258.6,-269.22 4327.42,-253.475 4405.61,-237.2 4464.34,-225.296"
        },
        {
            "source": "P17-2025",
            "target": "P19-1031",
            "d": "M4192.36,-271.406C4210.6,-259.93 4232.77,-245.916 4252.6,-233.22 4297.51,-204.459 4348.54,-171.265 4384.4,-147.845"
        },
        {
            "source": "P17-2025",
            "target": "P19-1340",
            "d": "M4113.27,-271.875C4099.63,-261.899 4086.05,-248.894 4078.6,-233.22 4068.34,-211.649 4067.65,-200.706 4078.6,-179.48 4085.5,-166.097 4096.79,-154.977 4108.98,-146.032"
        },
        {
            "source": "P17-1022",
            "target": "D17-1311",
            "d": "M6115.99,-296.09C6118.47,-296.09 6120.95,-296.09 6123.43,-296.09"
        },
        {
            "source": "P17-1076",
            "target": "D17-1178",
            "d": "M4403.62,-296.09C4431.38,-296.09 4459.14,-296.09 4486.9,-296.09"
        },
        {
            "source": "P17-1076",
            "target": "P18-2075",
            "d": "M4295.31,-274.297C4271.79,-262.156 4241.88,-246.719 4216.71,-233.729"
        },
        {
            "source": "P17-1076",
            "target": "P18-1249",
            "d": "M4382.04,-276.483C4413.6,-263.838 4455.77,-246.946 4490.51,-233.025"
        },
        {
            "source": "P17-1076",
            "target": "N18-1091",
            "d": "M4396.96,-284.342C4424.89,-279.546 4458.39,-273.927 4488.6,-269.22 4606.99,-250.775 4743.9,-231.618 4831.78,-219.6"
        },
        {
            "source": "P17-1076",
            "target": "P19-1031",
            "d": "M4349.21,-269.662C4365.67,-238.91 4393.46,-186.992 4411.92,-152.511"
        },
        {
            "source": "P17-1076",
            "target": "2020.emnlp-main.389",
            "d": "M4331.08,-269.077C4325.85,-238.886 4317,-187.594 4309.6,-143.48 4305.13,-116.865 4300.19,-86.7837 4296.47,-63.9781"
        },
        {
            "source": "P17-1076",
            "target": "2020.acl-main.557",
            "d": "M4397.19,-284.54C4491.67,-268.221 4662.05,-238.209 4666.6,-233.22 4708.44,-187.315 4702.3,-109.321 4693.99,-63.8115"
        },
        {
            "source": "D17-1178",
            "target": "P18-1249",
            "d": "M4582.11,-269.24C4578.41,-260.925 4574.25,-251.564 4570.3,-242.688"
        },
        {
            "source": "D17-1311",
            "target": "P19-1188",
            "d": "M6219.99,-269.099C6221.94,-245.172 6226.58,-209.022 6237.6,-179.48 6241.09,-170.1 6246.08,-160.586 6251.29,-151.923"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "P18-2075",
            "target": "P19-1340",
            "d": "M4165.6,-179.025C4165.6,-171.059 4165.6,-162.168 4165.6,-153.679"
        },
        {
            "source": "P18-1249",
            "target": "P19-1031",
            "d": "M4520.06,-180.91C4505.67,-170.727 4488.8,-158.795 4473.62,-148.051"
        },
        {
            "source": "P18-1249",
            "target": "P19-1340",
            "d": "M4478.21,-188.12C4412.2,-173.232 4316.98,-151.755 4249,-136.422"
        },
        {
            "source": "P18-1249",
            "target": "2020.emnlp-main.389",
            "d": "M4562.19,-179.292C4567.96,-153.605 4571.85,-114.679 4551.6,-89.7401 4531.99,-65.5903 4461.76,-49.7962 4399.09,-40.2138"
        },
        {
            "source": "N18-1091",
            "target": "P18-2075",
            "d": "M4858.13,-225.139C4820.68,-235.072 4772.39,-246.222 4728.6,-251.22 4606.28,-265.181 4574.34,-260.79 4451.6,-251.22 4380.69,-245.692 4300.79,-232.566 4243.5,-221.931"
        },
        {
            "source": "N18-1091",
            "target": "P18-1249",
            "d": "M4832.71,-206.35C4777.92,-206.35 4723.12,-206.35 4668.33,-206.35"
        },
        {
            "source": "N18-1197",
            "target": "P19-1188",
            "d": "M6316.11,-179.972C6310.37,-171.137 6303.84,-161.078 6297.71,-151.656"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        },
        {
            "source": "P19-1340",
            "target": "P19-1031",
            "d": "M4262.64,-116.61C4277.98,-116.61 4293.32,-116.61 4308.66,-116.61"
        },
        {
            "source": "P19-1340",
            "target": "2020.acl-main.557",
            "d": "M4245.16,-101.101C4266.11,-97.34 4288.7,-93.3372 4309.6,-89.7401 4403.04,-73.6603 4509.7,-56.1939 4585.49,-43.9329"
        }
    ],
    [
        {
            "id": "2001",
            "name": "2001",
            "x": "28.5975",
            "y": "-1728.23"
        },
        {
            "id": "2002",
            "name": "2002",
            "x": "28.5975",
            "y": "-1638.49"
        },
        {
            "id": "W01-1812",
            "name": "dan2001Parsing",
            "x": "6149.6",
            "y": "-1735.73"
        },
        {
            "id": "W01-1812",
            "name": "91",
            "x": "6149.6",
            "y": "-1720.73"
        },
        {
            "id": "N03-1016",
            "name": "dan2003{A}*",
            "x": "6016.6",
            "y": "-1556.25"
        },
        {
            "id": "N03-1016",
            "name": "192",
            "x": "6016.6",
            "y": "-1541.25"
        },
        {
            "id": "P09-1108",
            "name": "adam2009K-Best",
            "x": "6003.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1108",
            "name": "35",
            "x": "6003.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-2037",
            "name": "adam2010Top-Down",
            "x": "6084.6",
            "y": "-928.071"
        },
        {
            "id": "P10-2037",
            "name": "9",
            "x": "6084.6",
            "y": "-913.071"
        },
        {
            "id": "W01-0714",
            "name": "dan2001Distributional",
            "x": "489.597",
            "y": "-1735.73"
        },
        {
            "id": "W01-0714",
            "name": "36",
            "x": "489.597",
            "y": "-1720.73"
        },
        {
            "id": "P02-1017",
            "name": "dan2002A",
            "x": "489.597",
            "y": "-1645.99"
        },
        {
            "id": "P02-1017",
            "name": "167",
            "x": "489.597",
            "y": "-1630.99"
        },
        {
            "id": "P01-1044",
            "name": "dan2001Parsing",
            "x": "5983.6",
            "y": "-1735.73"
        },
        {
            "id": "P01-1044",
            "name": "45",
            "x": "5983.6",
            "y": "-1720.73"
        },
        {
            "id": "P03-1054",
            "name": "dan2003Accurate",
            "x": "5065.6",
            "y": "-1556.25"
        },
        {
            "id": "P03-1054",
            "name": "2541",
            "x": "5065.6",
            "y": "-1541.25"
        },
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "P04-1061",
            "name": "dan2004Corpus-Based",
            "x": "630.597",
            "y": "-1466.51"
        },
        {
            "id": "P04-1061",
            "name": "360",
            "x": "630.597",
            "y": "-1451.51"
        },
        {
            "id": "P06-1111",
            "name": "aria2006Prototype-Driven",
            "x": "489.597",
            "y": "-1287.03"
        },
        {
            "id": "P06-1111",
            "name": "39",
            "x": "489.597",
            "y": "-1272.03"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "steven2020Unsupervised",
            "x": "4290.6",
            "y": "-30.6701"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "???",
            "x": "4290.6",
            "y": "-15.6701"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "W04-3201",
            "name": "ben2004Max-Margin",
            "x": "5526.6",
            "y": "-1466.51"
        },
        {
            "id": "W04-3201",
            "name": "198",
            "x": "5526.6",
            "y": "-1451.51"
        },
        {
            "id": "W05-0104",
            "name": "dan2005A",
            "x": "5727.6",
            "y": "-1376.77"
        },
        {
            "id": "W05-0104",
            "name": "1",
            "x": "5727.6",
            "y": "-1361.77"
        },
        {
            "id": "W06-2903",
            "name": "slav2006Non-Local",
            "x": "5380.6",
            "y": "-1287.03"
        },
        {
            "id": "W06-2903",
            "name": "0",
            "x": "5380.6",
            "y": "-1272.03"
        },
        {
            "id": "P06-1055",
            "name": "slav2006Learning",
            "x": "3467.6",
            "y": "-1287.03"
        },
        {
            "id": "P06-1055",
            "name": "727",
            "x": "3467.6",
            "y": "-1272.03"
        },
        {
            "id": "N07-1051",
            "name": "slav2007Improved",
            "x": "4148.6",
            "y": "-1197.29"
        },
        {
            "id": "N07-1051",
            "name": "533",
            "x": "4148.6",
            "y": "-1182.29"
        },
        {
            "id": "D07-1072",
            "name": "percy2007The",
            "x": "4930.6",
            "y": "-1197.29"
        },
        {
            "id": "D07-1072",
            "name": "146",
            "x": "4930.6",
            "y": "-1182.29"
        },
        {
            "id": "W08-1005",
            "name": "slav2008Parsing",
            "x": "3333.6",
            "y": "-1107.55"
        },
        {
            "id": "W08-1005",
            "name": "23",
            "x": "3333.6",
            "y": "-1092.55"
        },
        {
            "id": "D08-1091",
            "name": "slav2008Sparse",
            "x": "5109.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1091",
            "name": "35",
            "x": "5109.6",
            "y": "-1092.55"
        },
        {
            "id": "N09-1026",
            "name": "john2009Efficient",
            "x": "1609.6",
            "y": "-1017.81"
        },
        {
            "id": "N09-1026",
            "name": "24",
            "x": "1609.6",
            "y": "-1002.81"
        },
        {
            "id": "N09-1063",
            "name": "adam2009Hierarchical",
            "x": "5452.6",
            "y": "-1017.81"
        },
        {
            "id": "N09-1063",
            "name": "17",
            "x": "5452.6",
            "y": "-1002.81"
        },
        {
            "id": "D09-1120",
            "name": "aria2009Simple",
            "x": "2770.6",
            "y": "-1017.81"
        },
        {
            "id": "D09-1120",
            "name": "158",
            "x": "2770.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-1112",
            "name": "mohit2010Simple,",
            "x": "4489.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1112",
            "name": "26",
            "x": "4489.6",
            "y": "-913.071"
        },
        {
            "id": "P11-2127",
            "name": "mohit2011The",
            "x": "4436.6",
            "y": "-838.331"
        },
        {
            "id": "P11-2127",
            "name": "2",
            "x": "4436.6",
            "y": "-823.331"
        },
        {
            "id": "P12-2021",
            "name": "jonathan2012Robust",
            "x": "5218.6",
            "y": "-748.591"
        },
        {
            "id": "P12-2021",
            "name": "2",
            "x": "5218.6",
            "y": "-733.591"
        },
        {
            "id": "P12-1101",
            "name": "adam2012Large-Scale",
            "x": "4128.6",
            "y": "-748.591"
        },
        {
            "id": "P12-1101",
            "name": "27",
            "x": "4128.6",
            "y": "-733.591"
        },
        {
            "id": "D12-1091",
            "name": "taylor2012An",
            "x": "2003.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1091",
            "name": "45",
            "x": "2003.6",
            "y": "-733.591"
        },
        {
            "id": "D12-1096",
            "name": "jonathan2012Parser",
            "x": "3655.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1096",
            "name": "47",
            "x": "3655.6",
            "y": "-733.591"
        },
        {
            "id": "D12-1105",
            "name": "david2012Training",
            "x": "4908.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1105",
            "name": "8",
            "x": "4908.6",
            "y": "-733.591"
        },
        {
            "id": "P13-2018",
            "name": "jonathan2013An",
            "x": "3990.6",
            "y": "-658.851"
        },
        {
            "id": "P13-2018",
            "name": "11",
            "x": "3990.6",
            "y": "-643.851"
        },
        {
            "id": "P14-1022",
            "name": "david2014Less",
            "x": "5059.6",
            "y": "-569.111"
        },
        {
            "id": "P14-1022",
            "name": "43",
            "x": "5059.6",
            "y": "-554.111"
        },
        {
            "id": "P15-1030",
            "name": "greg2015Neural",
            "x": "4681.6",
            "y": "-479.371"
        },
        {
            "id": "P15-1030",
            "name": "33",
            "x": "4681.6",
            "y": "-464.371"
        },
        {
            "id": "P17-1076",
            "name": "mitchell2017A",
            "x": "4335.6",
            "y": "-299.89"
        },
        {
            "id": "P17-1076",
            "name": "39",
            "x": "4335.6",
            "y": "-284.89"
        },
        {
            "id": "N18-1091",
            "name": "david2018What{'}s",
            "x": "4922.6",
            "y": "-210.15"
        },
        {
            "id": "N18-1091",
            "name": "3",
            "x": "4922.6",
            "y": "-195.15"
        },
        {
            "id": "D08-1012",
            "name": "slav2008Coarse-to-Fine",
            "x": "4604.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1012",
            "name": "36",
            "x": "4604.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-2064",
            "name": "adam2010Hierarchical",
            "x": "5452.6",
            "y": "-928.071"
        },
        {
            "id": "P10-2064",
            "name": "6",
            "x": "5452.6",
            "y": "-913.071"
        },
        {
            "id": "P10-1147",
            "name": "john2010Discriminative",
            "x": "1940.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1147",
            "name": "25",
            "x": "1940.6",
            "y": "-913.071"
        },
        {
            "id": "2005",
            "name": "2005",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "P11-1049",
            "name": "taylor2011Jointly",
            "x": "3911.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1049",
            "name": "165",
            "x": "3911.6",
            "y": "-823.331"
        },
        {
            "id": "D15-1032",
            "name": "jonathan2015An",
            "x": "4848.6",
            "y": "-479.371"
        },
        {
            "id": "D15-1032",
            "name": "14",
            "x": "4848.6",
            "y": "-464.371"
        },
        {
            "id": "N06-1014",
            "name": "percy2006Alignment",
            "x": "1174.6",
            "y": "-1287.03"
        },
        {
            "id": "N06-1014",
            "name": "392",
            "x": "1174.6",
            "y": "-1272.03"
        },
        {
            "id": "P08-1100",
            "name": "percy2008Analyzing",
            "x": "620.597",
            "y": "-1107.55"
        },
        {
            "id": "P08-1100",
            "name": "21",
            "x": "620.597",
            "y": "-1092.55"
        },
        {
            "id": "P10-1131",
            "name": "taylor2010Phylogenetic",
            "x": "988.597",
            "y": "-928.071"
        },
        {
            "id": "P10-1131",
            "name": "50",
            "x": "988.597",
            "y": "-913.071"
        },
        {
            "id": "N10-1083",
            "name": "taylor2010Painless",
            "x": "782.597",
            "y": "-928.071"
        },
        {
            "id": "N10-1083",
            "name": "168",
            "x": "782.597",
            "y": "-913.071"
        },
        {
            "id": "D12-1001",
            "name": "greg2012Syntactic",
            "x": "809.597",
            "y": "-748.591"
        },
        {
            "id": "D12-1001",
            "name": "43",
            "x": "809.597",
            "y": "-733.591"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "H05-1010",
            "name": "ben2005A",
            "x": "1721.6",
            "y": "-1376.77"
        },
        {
            "id": "H05-1010",
            "name": "174",
            "x": "1721.6",
            "y": "-1361.77"
        },
        {
            "id": "P07-1003",
            "name": "john2007Tailoring",
            "x": "1459.6",
            "y": "-1197.29"
        },
        {
            "id": "P07-1003",
            "name": "100",
            "x": "1459.6",
            "y": "-1182.29"
        },
        {
            "id": "P09-1104",
            "name": "aria2009Better",
            "x": "1775.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1104",
            "name": "91",
            "x": "1775.6",
            "y": "-1002.81"
        },
        {
            "id": "N10-1015",
            "name": "david2010Joint",
            "x": "1332.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1015",
            "name": "56",
            "x": "1332.6",
            "y": "-913.071"
        },
        {
            "id": "N12-1004",
            "name": "david2012Fast",
            "x": "2155.6",
            "y": "-748.591"
        },
        {
            "id": "N12-1004",
            "name": "9",
            "x": "2155.6",
            "y": "-733.591"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "W06-3105",
            "name": "john2006Why",
            "x": "1990.6",
            "y": "-1287.03"
        },
        {
            "id": "W06-3105",
            "name": "71",
            "x": "1990.6",
            "y": "-1272.03"
        },
        {
            "id": "P08-2007",
            "name": "john2008The",
            "x": "1938.6",
            "y": "-1107.55"
        },
        {
            "id": "P08-2007",
            "name": "63",
            "x": "1938.6",
            "y": "-1092.55"
        },
        {
            "id": "D08-1033",
            "name": "john2008Sampling",
            "x": "2217.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1033",
            "name": "86",
            "x": "2217.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1014",
            "name": "adam2010Unsupervised",
            "x": "1527.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1014",
            "name": "18",
            "x": "1527.6",
            "y": "-913.071"
        },
        {
            "id": "D07-1094",
            "name": "slav2007Learning",
            "x": "3413.6",
            "y": "-1197.29"
        },
        {
            "id": "D07-1094",
            "name": "13",
            "x": "3413.6",
            "y": "-1182.29"
        },
        {
            "id": "W10-2906",
            "name": "david2010Learning",
            "x": "3217.6",
            "y": "-928.071"
        },
        {
            "id": "W10-2906",
            "name": "33",
            "x": "3217.6",
            "y": "-913.071"
        },
        {
            "id": "P10-2054",
            "name": "aria2010An",
            "x": "3378.6",
            "y": "-928.071"
        },
        {
            "id": "P10-2054",
            "name": "8",
            "x": "3378.6",
            "y": "-913.071"
        },
        {
            "id": "N10-1061",
            "name": "aria2010Coreference",
            "x": "2830.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1061",
            "name": "119",
            "x": "2830.6",
            "y": "-913.071"
        },
        {
            "id": "N10-1082",
            "name": "percy2010Type-Based",
            "x": "2238.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1082",
            "name": "32",
            "x": "2238.6",
            "y": "-913.071"
        },
        {
            "id": "W11-1916",
            "name": "jonathan2011Mention",
            "x": "3349.6",
            "y": "-838.331"
        },
        {
            "id": "W11-1916",
            "name": "10",
            "x": "3349.6",
            "y": "-823.331"
        },
        {
            "id": "P11-1060",
            "name": "percy2011Learning",
            "x": "1546.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1060",
            "name": "315",
            "x": "1546.6",
            "y": "-823.331"
        },
        {
            "id": "P11-1070",
            "name": "mohit2011Web-Scale",
            "x": "2948.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1070",
            "name": "41",
            "x": "2948.6",
            "y": "-823.331"
        },
        {
            "id": "P13-1012",
            "name": "greg2013Decentralized",
            "x": "2731.6",
            "y": "-658.851"
        },
        {
            "id": "P13-1012",
            "name": "22",
            "x": "2731.6",
            "y": "-643.851"
        },
        {
            "id": "J13-2005",
            "name": "percy2013Learning",
            "x": "1584.6",
            "y": "-658.851"
        },
        {
            "id": "J13-2005",
            "name": "54",
            "x": "1584.6",
            "y": "-643.851"
        },
        {
            "id": "Q14-1037",
            "name": "greg2014A",
            "x": "2900.6",
            "y": "-569.111"
        },
        {
            "id": "Q14-1037",
            "name": "122",
            "x": "2900.6",
            "y": "-554.111"
        },
        {
            "id": "P16-1188",
            "name": "greg2016Learning-Based",
            "x": "3403.6",
            "y": "-389.631"
        },
        {
            "id": "P16-1188",
            "name": "28",
            "x": "3403.6",
            "y": "-374.631"
        },
        {
            "id": "P06-1096",
            "name": "percy2006An",
            "x": "2498.6",
            "y": "-1287.03"
        },
        {
            "id": "P06-1096",
            "name": "244",
            "x": "2498.6",
            "y": "-1272.03"
        },
        {
            "id": "D08-1092",
            "name": "david2008Two",
            "x": "1409.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1092",
            "name": "90",
            "x": "1409.6",
            "y": "-1092.55"
        },
        {
            "id": "P09-1011",
            "name": "percy2009Learning",
            "x": "1948.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1011",
            "name": "190",
            "x": "1948.6",
            "y": "-1002.81"
        },
        {
            "id": "D12-1079",
            "name": "david2012Transforming",
            "x": "1271.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1079",
            "name": "13",
            "x": "1271.6",
            "y": "-733.591"
        },
        {
            "id": "N06-1041",
            "name": "aria2006Prototype-Driven",
            "x": "2188.6",
            "y": "-1287.03"
        },
        {
            "id": "N06-1041",
            "name": "167",
            "x": "2188.6",
            "y": "-1272.03"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "P07-1107",
            "name": "aria2007Unsupervised",
            "x": "2798.6",
            "y": "-1197.29"
        },
        {
            "id": "P07-1107",
            "name": "127",
            "x": "2798.6",
            "y": "-1182.29"
        },
        {
            "id": "P09-2036",
            "name": "john2009Asynchronous",
            "x": "4224.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-2036",
            "name": "12",
            "x": "4224.6",
            "y": "-1002.81"
        },
        {
            "id": "W14-1607",
            "name": "jacob2014Grounding",
            "x": "1699.6",
            "y": "-569.111"
        },
        {
            "id": "W14-1607",
            "name": "10",
            "x": "1699.6",
            "y": "-554.111"
        },
        {
            "id": "P14-2133",
            "name": "jacob2014How",
            "x": "4719.6",
            "y": "-569.111"
        },
        {
            "id": "P14-2133",
            "name": "36",
            "x": "4719.6",
            "y": "-554.111"
        },
        {
            "id": "P14-1020",
            "name": "david2014Sparser,",
            "x": "3871.6",
            "y": "-569.111"
        },
        {
            "id": "P14-1020",
            "name": "16",
            "x": "3871.6",
            "y": "-554.111"
        },
        {
            "id": "P19-1031",
            "name": "daniel2019Cross-Domain",
            "x": "4430.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1031",
            "name": "0",
            "x": "4430.6",
            "y": "-105.41"
        },
        {
            "id": "N07-1052",
            "name": "aria2007Approximate",
            "x": "5379.6",
            "y": "-1197.29"
        },
        {
            "id": "N07-1052",
            "name": "6",
            "x": "5379.6",
            "y": "-1182.29"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "P08-1088",
            "name": "aria2008Learning",
            "x": "394.597",
            "y": "-1107.55"
        },
        {
            "id": "P08-1088",
            "name": "259",
            "x": "394.597",
            "y": "-1092.55"
        },
        {
            "id": "D11-1029",
            "name": "taylor2011Simple",
            "x": "485.597",
            "y": "-838.331"
        },
        {
            "id": "D11-1029",
            "name": "11",
            "x": "485.597",
            "y": "-823.331"
        },
        {
            "id": "N09-1069",
            "name": "percy2009Online",
            "x": "593.597",
            "y": "-1017.81"
        },
        {
            "id": "N09-1069",
            "name": "157",
            "x": "593.597",
            "y": "-1002.81"
        },
        {
            "id": "D09-1147",
            "name": "adam2009Consensus",
            "x": "4708.6",
            "y": "-1017.81"
        },
        {
            "id": "D09-1147",
            "name": "25",
            "x": "4708.6",
            "y": "-1002.81"
        },
        {
            "id": "P13-1021",
            "name": "taylor2013Unsupervised",
            "x": "2504.6",
            "y": "-658.851"
        },
        {
            "id": "P13-1021",
            "name": "22",
            "x": "2504.6",
            "y": "-643.851"
        },
        {
            "id": "P14-2020",
            "name": "taylor2014Improved",
            "x": "4153.6",
            "y": "-569.111"
        },
        {
            "id": "P14-2020",
            "name": "13",
            "x": "4153.6",
            "y": "-554.111"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "N09-1008",
            "name": "alexandre2009Improved",
            "x": "337.597",
            "y": "-1017.81"
        },
        {
            "id": "N09-1008",
            "name": "19",
            "x": "337.597",
            "y": "-1002.81"
        },
        {
            "id": "P10-1105",
            "name": "david2010Finding",
            "x": "337.597",
            "y": "-928.071"
        },
        {
            "id": "P10-1105",
            "name": "23",
            "x": "337.597",
            "y": "-913.071"
        },
        {
            "id": "D11-1032",
            "name": "david2011Large-Scale",
            "x": "288.597",
            "y": "-838.331"
        },
        {
            "id": "D11-1032",
            "name": "6",
            "x": "288.597",
            "y": "-823.331"
        },
        {
            "id": "P12-1041",
            "name": "mohit2012Coreference",
            "x": "2915.6",
            "y": "-748.591"
        },
        {
            "id": "P12-1041",
            "name": "36",
            "x": "2915.6",
            "y": "-733.591"
        },
        {
            "id": "D13-1027",
            "name": "jonathan2013Error-Driven",
            "x": "3344.6",
            "y": "-658.851"
        },
        {
            "id": "D13-1027",
            "name": "25",
            "x": "3344.6",
            "y": "-643.851"
        },
        {
            "id": "D13-1203",
            "name": "greg2013Easy",
            "x": "2917.6",
            "y": "-658.851"
        },
        {
            "id": "D13-1203",
            "name": "126",
            "x": "2917.6",
            "y": "-643.851"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "P11-1027",
            "name": "adam2011Faster",
            "x": "4121.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1027",
            "name": "103",
            "x": "4121.6",
            "y": "-823.331"
        },
        {
            "id": "D15-1138",
            "name": "jacob2015Alignment-Based",
            "x": "1699.6",
            "y": "-479.371"
        },
        {
            "id": "D15-1138",
            "name": "17",
            "x": "1699.6",
            "y": "-464.371"
        },
        {
            "id": "N16-1181",
            "name": "jacob2016Learning",
            "x": "1469.6",
            "y": "-389.631"
        },
        {
            "id": "N16-1181",
            "name": "205",
            "x": "1469.6",
            "y": "-374.631"
        },
        {
            "id": "P14-1098",
            "name": "mohit2014Structured",
            "x": "3105.6",
            "y": "-569.111"
        },
        {
            "id": "P14-1098",
            "name": "29",
            "x": "3105.6",
            "y": "-554.111"
        },
        {
            "id": "D13-1087",
            "name": "taylor2013Decipherment",
            "x": "485.597",
            "y": "-658.851"
        },
        {
            "id": "D13-1087",
            "name": "5",
            "x": "485.597",
            "y": "-643.851"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "P19-1340",
            "name": "nikita2019Multilingual",
            "x": "4165.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1340",
            "name": "9",
            "x": "4165.6",
            "y": "-105.41"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "P18-1249",
            "name": "nikita2018Constituency",
            "x": "4554.6",
            "y": "-210.15"
        },
        {
            "id": "P18-1249",
            "name": "22",
            "x": "4554.6",
            "y": "-195.15"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "2020.acl-main.557",
            "name": "nikita2020Tetra-Tagging:",
            "x": "4685.6",
            "y": "-30.6701"
        },
        {
            "id": "2020.acl-main.557",
            "name": "???",
            "x": "4685.6",
            "y": "-15.6701"
        },
        {
            "id": "Q17-1031",
            "name": "jonathan2017Parsing",
            "x": "4844.6",
            "y": "-299.89"
        },
        {
            "id": "Q17-1031",
            "name": "4",
            "x": "4844.6",
            "y": "-284.89"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "D16-1125",
            "name": "jacob2016Reasoning",
            "x": "6019.6",
            "y": "-389.631"
        },
        {
            "id": "D16-1125",
            "name": "36",
            "x": "6019.6",
            "y": "-374.631"
        },
        {
            "id": "P17-1022",
            "name": "jacob2017Translating",
            "x": "6019.6",
            "y": "-299.89"
        },
        {
            "id": "P17-1022",
            "name": "???",
            "x": "6019.6",
            "y": "-284.89"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "P17-2025",
            "name": "daniel2017Improving",
            "x": "4154.6",
            "y": "-299.89"
        },
        {
            "id": "P17-2025",
            "name": "9",
            "x": "4154.6",
            "y": "-284.89"
        },
        {
            "id": "D17-1178",
            "name": "mitchell2017Effective",
            "x": "4593.6",
            "y": "-299.89"
        },
        {
            "id": "D17-1178",
            "name": "9",
            "x": "4593.6",
            "y": "-284.89"
        },
        {
            "id": "P18-2075",
            "name": "daniel2018Policy",
            "x": "4165.6",
            "y": "-210.15"
        },
        {
            "id": "P18-2075",
            "name": "8",
            "x": "4165.6",
            "y": "-195.15"
        },
        {
            "id": "D17-1311",
            "name": "jacob2017Analogs",
            "x": "6218.6",
            "y": "-299.89"
        },
        {
            "id": "D17-1311",
            "name": "3",
            "x": "6218.6",
            "y": "-284.89"
        },
        {
            "id": "P19-1188",
            "name": "david2019Pre-Learning",
            "x": "6275.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1188",
            "name": "1",
            "x": "6275.6",
            "y": "-105.41"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "N18-1197",
            "name": "jacob2018Learning",
            "x": "6332.6",
            "y": "-210.15"
        },
        {
            "id": "N18-1197",
            "name": "8",
            "x": "6332.6",
            "y": "-195.15"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-23.1701"
        }
    ],
    [
        "28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19",
        "6044.07,-1584.28 6035.28,-1578.38 6038.47,-1588.48 6044.07,-1584.28",
        "6029.34,-1047.27 6021.21,-1040.48 6023.33,-1050.86 6029.34,-1047.27",
        "6126.03,-953.922 6116.3,-949.727 6121.29,-959.073 6126.03,-953.922",
        "493.098,-1679.2 489.597,-1669.2 486.098,-1679.2 493.098,-1679.2",
        "6065.31,-1735.43 6075.31,-1731.93 6065.31,-1728.43 6065.31,-1735.43",
        "5144.59,-1565.16 5134.11,-1566.7 5143.26,-1572.03 5144.59,-1565.16",
        "6013.37,-1589.93 6011.76,-1579.46 6006.49,-1588.65 6013.37,-1589.93",
        "28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45",
        "606.902,-1499.16 610.382,-1489.16 601.421,-1494.81 606.902,-1499.16",
        "493.098,-1320.58 489.597,-1310.58 486.098,-1320.58 493.098,-1320.58",
        "4171.42,-33.2065 4181.36,-29.5476 4171.3,-26.2074 4171.42,-33.2065",
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "5442.09,-1483.94 5451.23,-1478.59 5440.74,-1477.07 5442.09,-1483.94",
        "5720.47,-1410.61 5720.45,-1400.01 5713.86,-1408.3 5720.47,-1410.61",
        "5375.76,-1320.75 5374.9,-1310.19 5368.98,-1318.98 5375.76,-1320.75",
        "3499.04,-1313.98 3489.65,-1309.08 3493.93,-1318.77 3499.04,-1313.98",
        "4171.01,-1226.79 4162.89,-1219.99 4165,-1230.37 4171.01,-1226.79",
        "4947.19,-1228.47 4940.38,-1220.35 4940.64,-1230.95 4947.19,-1228.47",
        "3249.09,-1112.57 3258.72,-1108.16 3248.44,-1105.6 3249.09,-1112.57",
        "5098.84,-1141.01 5099.06,-1130.42 5092.28,-1138.56 5098.84,-1141.01",
        "6010.68,-1050.83 6006.38,-1041.14 6003.7,-1051.39 6010.68,-1050.83",
        "1619.88,-1050.28 1614.65,-1041.07 1612.99,-1051.53 1619.88,-1050.28",
        "5475.67,-1047.75 5467.89,-1040.56 5469.49,-1051.04 5475.67,-1047.75",
        "2743.81,-1049.61 2747.67,-1039.74 2738.5,-1045.05 2743.81,-1049.61",
        "4487.26,-961.759 4485.29,-951.35 4480.34,-960.717 4487.26,-961.759",
        "4413.82,-870.728 4416.38,-860.448 4407.97,-866.887 4413.82,-870.728",
        "5305.66,-758.828 5295.13,-759.986 5304.08,-765.648 5305.66,-758.828",
        "4207.97,-762.957 4197.44,-764.114 4206.39,-769.777 4207.97,-762.957",
        "2041.73,-772.693 2031.71,-769.248 2037.39,-778.189 2041.73,-772.693",
        "3663.2,-781.583 3658.52,-772.076 3656.25,-782.424 3663.2,-781.583",
        "4989.38,-758.515 4978.88,-759.951 4987.98,-765.374 4989.38,-758.515",
        "4076.21,-653.034 4066.18,-656.451 4076.15,-660.033 4076.21,-653.034",
        "5140.41,-566.66 5130.24,-569.616 5140.04,-573.65 5140.41,-566.66",
        "4742.21,-495.571 4731.61,-495.318 4739.74,-502.121 4742.21,-495.571",
        "4279.52,-321.624 4288.1,-315.408 4277.52,-314.917 4279.52,-321.624",
        "5019.2,-211.364 5008.94,-214.028 5018.62,-218.34 5019.2,-211.364",
        "4705.85,-1115.36 4695.46,-1117.41 4704.86,-1122.29 4705.85,-1115.36",
        "6017.24,-1049.66 6011.26,-1040.91 6010.48,-1051.48 6017.24,-1049.66",
        "5496.36,-1042.53 5486.24,-1039.4 5492.2,-1048.16 5496.36,-1042.53",
        "6113.22,-957.124 6104.71,-950.817 6107.43,-961.056 6113.22,-957.124",
        "5560.68,-925.133 5550.56,-928.264 5560.42,-932.128 5560.68,-925.133",
        "2036.85,-937.361 2026.46,-939.41 2035.86,-944.29 2036.85,-937.361",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "5145.87,-1132.68 5136,-1128.84 5141.32,-1138 5145.87,-1132.68",
        "3984.69,-849.39 3974.19,-850.792 3983.27,-856.245 3984.69,-849.39",
        "5139.81,-568.356 5129.56,-571.04 5139.25,-575.333 5139.81,-568.356",
        "4931.91,-480.349 4921.64,-482.919 4931.27,-487.32 4931.91,-480.349",
        "518.952,-1315.55 509.991,-1309.9 513.471,-1319.91 518.952,-1315.55",
        "1122.12,-1314.24 1129.6,-1306.73 1119.07,-1307.94 1122.12,-1314.24",
        "625.114,-1140.99 621.335,-1131.1 618.117,-1141.19 625.114,-1140.99",
        "960.537,-960.471 964.373,-950.595 955.215,-955.923 960.537,-960.471",
        "784.804,-961.541 781.639,-951.429 777.808,-961.307 784.804,-961.541",
        "716.188,-752.239 726.055,-748.382 715.936,-745.244 716.188,-752.239",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "1256.88,-1298.86 1246.38,-1300.26 1255.46,-1305.71 1256.88,-1298.86",
        "1504.7,-1220.82 1494.46,-1218.11 1500.78,-1226.61 1504.7,-1220.82",
        "1773.65,-1051.28 1771.69,-1040.87 1766.73,-1050.24 1773.65,-1051.28",
        "2009.6,-948.523 1999.19,-946.577 2006.12,-954.593 2009.6,-948.523",
        "1316.45,-961.083 1317.67,-950.559 1310.15,-958.021 1316.45,-961.083",
        "2210.26,-767.184 2199.77,-765.674 2207.03,-773.394 2210.26,-767.184",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "1952.34,-1139.2 1946.17,-1130.59 1945.62,-1141.17 1952.34,-1139.2",
        "2197.7,-1140.65 2200.65,-1130.48 2192,-1136.6 2197.7,-1140.65",
        "2036.55,-939.545 2025.96,-939.48 2034.2,-946.138 2036.55,-939.545",
        "1518.31,-961.779 1517.69,-951.202 1511.58,-959.858 1518.31,-961.779",
        "2143.67,-782.135 2144.1,-771.549 2137.17,-779.557 2143.67,-782.135",
        "4073.06,-1215.73 4082.12,-1210.23 4071.61,-1208.88 4073.06,-1215.73",
        "4853.34,-1202.63 4863.11,-1198.54 4852.92,-1195.64 4853.34,-1202.63",
        "3437.59,-1226.8 3429.37,-1220.12 3431.63,-1230.47 3437.59,-1226.8",
        "3308.94,-1139.72 3311.98,-1129.57 3303.27,-1135.61 3308.94,-1139.72",
        "4489.41,-1109.56 4499.37,-1105.93 4489.32,-1102.56 4489.41,-1109.56",
        "5042.75,-1126.15 5051.7,-1120.48 5041.16,-1119.33 5042.75,-1126.15",
        "5955.14,-1044.77 5962.41,-1037.06 5951.92,-1038.55 5955.14,-1044.77",
        "1677.26,-1032.34 1666.7,-1033.14 1675.46,-1039.1 1677.26,-1032.34",
        "5420.33,-1049.32 5425.53,-1040.09 5415.71,-1044.06 5420.33,-1049.32",
        "3202.92,-961.718 3204.05,-951.183 3196.6,-958.717 3202.92,-961.718",
        "5980.01,-933.02 5989.85,-929.086 5979.71,-926.026 5980.01,-933.02",
        "3336.75,-953.966 3343.25,-945.598 3332.95,-948.087 3336.75,-953.966",
        "5363.28,-946.118 5371.9,-939.968 5361.33,-939.396 5363.28,-946.118",
        "4399.07,-934.361 4408.78,-930.122 4398.54,-927.381 4399.07,-934.361",
        "2854.19,-957.977 2846.04,-951.202 2848.19,-961.578 2854.19,-957.977",
        "2280.32,-953.525 2270.39,-949.832 2275.85,-958.912 2280.32,-953.525",
        "3412.33,-859.482 3401.98,-857.179 3408.63,-865.428 3412.33,-859.482",
        "4365.13,-851.176 4374.36,-845.985 4363.9,-844.283 4365.13,-851.176",
        "1642.19,-836.814 1632.02,-839.796 1641.83,-843.804 1642.19,-836.814",
        "2957.78,-870.716 2952.68,-861.428 2950.87,-871.867 2957.78,-870.716",
        "4080.32,-777.724 4086.81,-769.352 4076.52,-771.847 4080.32,-777.724",
        "887.504,-759.862 876.984,-761.125 885.995,-766.697 887.504,-759.862",
        "2062.65,-762.841 2052.06,-762.822 2060.33,-769.444 2062.65,-762.841",
        "3645.34,-782.308 3645.55,-771.715 3638.78,-779.868 3645.34,-782.308",
        "4814.88,-753.338 4824.67,-749.29 4814.5,-746.349 4814.88,-753.338",
        "3918.06,-675.95 3926.55,-669.622 3915.97,-669.27 3918.06,-675.95",
        "2815.41,-672.519 2804.88,-673.692 2813.84,-679.341 2815.41,-672.519",
        "1681.03,-655.211 1670.92,-658.37 1680.79,-662.207 1681.03,-655.211",
        "2957.06,-579.001 2946.5,-579.862 2955.29,-585.774 2957.06,-579.001",
        "4982.52,-578.703 4992.08,-574.13 4981.76,-571.745 4982.52,-578.703",
        "3376.39,-422.023 3380.4,-412.216 3371.15,-417.383 3376.39,-422.023",
        "1998.95,-1120.21 1988.36,-1120.52 1996.84,-1126.88 1998.95,-1120.21",
        "3824.67,-843.817 3834.45,-839.761 3824.27,-836.828 3824.67,-843.817",
        "764.158,-961.411 766.864,-951.167 758.36,-957.487 764.158,-961.411",
        "716.262,-752.292 726.125,-748.425 716.003,-745.297 716.262,-752.292",
        "1392.27,-1218.88 1400.8,-1212.59 1390.21,-1212.19 1392.27,-1218.88",
        "1382.27,-1139.32 1386.67,-1129.68 1377.22,-1134.47 1382.27,-1139.32",
        "1879.91,-1040.08 1888.2,-1033.47 1877.6,-1033.47 1879.91,-1040.08",
        "1744.04,-1048.39 1749.09,-1039.07 1739.33,-1043.21 1744.04,-1048.39",
        "1835.52,-939.142 1845.12,-934.649 1834.82,-932.178 1835.52,-939.142",
        "1448.67,-950.933 1457.05,-944.459 1446.46,-944.29 1448.67,-950.933",
        "1268.58,-947.657 1277,-941.226 1266.41,-941.002 1268.58,-947.657",
        "796.53,-959.601 790.13,-951.157 789.867,-961.749 796.53,-959.601",
        "2097.04,-769.84 2105.55,-763.532 2094.96,-763.155 2097.04,-769.84",
        "773.39,-778.977 778.656,-769.783 768.806,-773.686 773.39,-778.977",
        "1246.3,-781.252 1250.06,-771.346 1240.94,-776.747 1246.3,-781.252",
        "1933.71,-759.789 1942.97,-754.65 1932.53,-752.889 1933.71,-759.789",
        "2770.59,-953.798 2778.4,-946.643 2767.83,-947.363 2770.59,-953.798",
        "2268.14,-956.779 2259.42,-950.764 2262.48,-960.905 2268.14,-956.779",
        "820.051,-953.861 810.198,-949.965 815.471,-959.154 820.051,-953.861",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "1432.04,-1137.07 1424.05,-1130.11 1425.96,-1140.54 1432.04,-1137.07",
        "1729.44,-1044.04 1736.55,-1036.18 1726.09,-1037.9 1729.44,-1044.04",
        "1833.51,-937.609 1843.15,-933.22 1832.87,-930.638 1833.51,-937.609",
        "1433.37,-946.75 1441.55,-940.02 1430.96,-940.178 1433.37,-946.75",
        "1323.85,-961.693 1323.38,-951.109 1317.15,-959.678 1323.85,-961.693",
        "817.735,-954.285 808.032,-950.031 812.963,-959.408 817.735,-954.285",
        "1255.66,-782.132 1257.03,-771.626 1249.4,-778.985 1255.66,-782.132",
        "2303.17,-1114.14 2292.76,-1116.1 2302.13,-1121.06 2303.17,-1114.14",
        "2779.72,-1050.35 2774.7,-1041.02 2772.8,-1051.44 2779.72,-1050.35",
        "2847.4,-959.399 2840.91,-951.02 2840.76,-961.614 2847.4,-959.399",
        "3412.08,-1113.75 3401.66,-1115.7 3411.02,-1120.67 3412.08,-1113.75",
        "4513.12,-1125.92 4522.28,-1120.59 4511.8,-1119.04 4513.12,-1125.92",
        "5047.81,-1128.52 5056.55,-1122.54 5045.99,-1121.76 5047.81,-1128.52",
        "1489.84,-1106.63 1479.63,-1109.47 1489.38,-1113.61 1489.84,-1106.63",
        "4216.06,-1051.72 4216.01,-1041.12 4209.44,-1049.44 4216.06,-1051.72",
        "1675.68,-1032.67 1665.12,-1033.57 1673.93,-1039.45 1675.68,-1032.67",
        "5344.16,-1021.19 5353.94,-1017.13 5343.76,-1014.2 5344.16,-1021.19",
        "3290.05,-942.611 3279.47,-943.141 3288.07,-949.326 3290.05,-942.611",
        "4406.43,-941.715 4415.77,-936.716 4405.35,-934.798 4406.43,-941.715",
        "1393.03,-942.969 1382.45,-943.553 1391.08,-949.694 1393.03,-942.969",
        "4380.1,-860.667 4388.18,-853.815 4377.59,-854.132 4380.1,-860.667",
        "5201.86,-782.063 5203.92,-771.67 5195.82,-778.509 5201.86,-782.063",
        "1385.71,-747.318 1375.57,-750.383 1385.41,-754.312 1385.71,-747.318",
        "3669.77,-780.198 3663.46,-771.69 3663.09,-782.279 3669.77,-780.198",
        "4923.53,-780.362 4917.33,-771.774 4916.82,-782.357 4923.53,-780.362",
        "4076.58,-654.733 4066.28,-657.24 4075.89,-661.699 4076.58,-654.733",
        "1804.06,-564.823 1793.99,-568.124 1803.92,-571.822 1804.06,-564.823",
        "4732.32,-600.908 4726.41,-592.114 4725.54,-602.674 4732.32,-600.908",
        "3964.22,-570.897 3953.82,-572.935 3963.22,-577.826 3964.22,-570.897",
        "5063.1,-602.473 5059.6,-592.473 5056.1,-602.473 5063.1,-602.473",
        "4654.76,-510.993 4659.03,-501.294 4649.64,-506.216 4654.76,-510.993",
        "4332.93,-333.585 4331.05,-323.159 4326.02,-332.486 4332.93,-333.585",
        "4942.05,-241.199 4935.09,-233.213 4935.56,-243.798 4942.05,-241.199",
        "4434.32,-153.761 4430.76,-143.782 4427.32,-153.802 4434.32,-153.761",
        "4709.94,-1113.16 4699.61,-1115.51 4709.15,-1120.11 4709.94,-1113.16",
        "5935.27,-1038.08 5943.93,-1031.98 5933.35,-1031.35 5935.27,-1038.08",
        "5441.4,-1051.43 5441.97,-1040.85 5434.93,-1048.77 5441.4,-1051.43",
        "5059.52,-1133.19 5066.96,-1125.65 5056.44,-1126.91 5059.52,-1133.19",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "2123.2,-1107.25 2133.2,-1103.75 2123.2,-1100.25 2123.2,-1107.25",
        "1825.17,-1037.72 1814.71,-1036.06 1821.85,-1043.88 1825.17,-1037.72",
        "2027.82,-943.152 2017.25,-942.47 2025.09,-949.597 2027.82,-943.152",
        "489.864,-871.668 486.399,-861.655 482.864,-871.643 489.864,-871.668",
        "607.875,-1049.51 601.589,-1040.98 601.185,-1051.57 607.875,-1049.51",
        "1833.66,-1032.94 1823.07,-1033.45 1831.66,-1039.65 1833.66,-1032.94",
        "1675.67,-1032.68 1665.12,-1033.58 1673.93,-1039.46 1675.67,-1032.68",
        "4673.97,-1048.8 4679.36,-1039.68 4669.46,-1043.45 4673.97,-1048.8",
        "4501.65,-960.057 4495.6,-951.359 4494.9,-961.931 4501.65,-960.057",
        "2588.99,-673.192 2578.49,-674.676 2587.62,-680.057 2588.99,-673.192",
        "4220.64,-587.128 4210.05,-586.696 4218.06,-593.635 4220.64,-587.128",
        "2017.41,-1033.77 2006.81,-1034 2015.24,-1040.43 2017.41,-1033.77",
        "2043.89,-934.848 2033.37,-936.086 2042.37,-941.68 2043.89,-934.848",
        "2150.84,-782.144 2149.56,-771.627 2144,-780.648 2150.84,-782.144",
        "4575.01,-933.742 4564.59,-935.673 4573.94,-940.66 4575.01,-933.742",
        "4939.51,-776.253 4930.41,-770.816 4934.13,-780.737 4939.51,-776.253",
        "5127.91,-581.056 5117.32,-581.307 5125.75,-587.716 5127.91,-581.056",
        "4737,-498.229 4726.49,-496.927 4733.9,-504.502 4737,-498.229",
        "3127.33,-938.495 3136.93,-934.003 3126.63,-931.53 3127.33,-938.495",
        "1051.76,-948.099 1041.18,-947.439 1049.04,-954.549 1051.76,-948.099",
        "1329.16,-961.88 1327.05,-951.498 1322.23,-960.933 1329.16,-961.88",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "1605.51,-857.714 1594.95,-856.877 1602.68,-864.118 1605.51,-857.714",
        "1543.12,-688.459 1549.12,-679.73 1538.98,-682.81 1543.12,-688.459",
        "1889.27,-956.527 1896.46,-948.742 1885.99,-950.347 1889.27,-956.527",
        "1598.2,-946.539 1587.61,-946.503 1595.87,-953.138 1598.2,-946.539",
        "1394,-942.764 1383.42,-943.282 1392.01,-949.476 1394,-942.764",
        "852.124,-942.678 841.576,-943.674 850.443,-949.473 852.124,-942.678",
        "2097.91,-770.557 2106.04,-763.772 2095.45,-764.001 2097.91,-770.557",
        "1932.18,-758.15 1941.57,-753.245 1931.17,-751.223 1932.18,-758.15",
        "6057.05,-960.268 6061.26,-950.546 6051.91,-955.52 6057.05,-960.268",
        "341.098,-961.275 337.597,-951.275 334.098,-961.275 341.098,-961.275",
        "476.896,-872.25 477.135,-861.658 470.343,-869.789 476.896,-872.25",
        "263.519,-871.01 266.401,-860.815 257.79,-866.988 263.519,-871.01",
        "4163.02,-1044.03 4171.17,-1037.26 4160.58,-1037.47 4163.02,-1044.03",
        "1697.65,-1017.51 1707.65,-1014.01 1697.65,-1010.51 1697.65,-1017.51",
        "1560.6,-955.462 1551.22,-950.546 1555.49,-960.242 1560.6,-955.462",
        "2142.21,-942.9 2151.63,-938.043 2141.24,-935.968 2142.21,-942.9",
        "5913.26,-1017.51 5923.26,-1014.01 5913.26,-1010.51 5913.26,-1017.51",
        "5456.1,-961.275 5452.6,-951.275 5449.1,-961.275 5456.1,-961.275",
        "728.457,-954.264 736.054,-946.878 725.51,-947.915 728.457,-954.264",
        "1450.27,-841.713 1460.15,-837.91 1450.05,-834.716 1450.27,-841.713",
        "1488.35,-662.872 1498.22,-659.007 1488.09,-655.876 1488.35,-662.872",
        "2810.32,-961.126 2813.07,-950.896 2804.54,-957.175 2810.32,-961.126",
        "2859.75,-776.221 2866.87,-768.37 2856.41,-770.071 2859.75,-776.221",
        "3311.06,-690.744 3316.27,-681.522 3306.45,-685.478 3311.06,-690.744",
        "2862.25,-681.322 2870.16,-674.267 2859.58,-674.853 2862.25,-681.322",
        "2836.53,-572.885 2846.33,-568.867 2836.17,-565.894 2836.53,-572.885",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "440.474,-866.351 447.291,-858.24 436.904,-860.329 440.474,-866.351",
        "310.863,-868.221 302.931,-861.197 304.751,-871.634 310.863,-868.221",
        "4460.08,-867.644 4451.92,-860.894 4454.09,-871.264 4460.08,-867.644",
        "844.185,-775.156 834.619,-770.6 839.256,-780.126 844.185,-775.156",
        "2121.63,-778.366 2127.12,-769.301 2117.18,-772.968 2121.63,-778.366",
        "1411.07,-927.771 1421.07,-924.271 1411.07,-920.771 1411.07,-927.771",
        "1287.04,-779.949 1280.48,-771.629 1280.42,-782.224 1287.04,-779.949",
        "4814.93,-752.965 4824.79,-749.077 4814.66,-745.97 4814.93,-752.965",
        "3341.99,-955.603 3348.58,-947.308 3338.25,-949.682 3341.99,-955.603",
        "3270.37,-858.553 3279.36,-852.942 3268.84,-851.724 3270.37,-858.553",
        "2871.19,-778.794 2876.72,-769.752 2866.76,-773.376 2871.19,-778.794",
        "2741.28,-691.133 2735.98,-681.96 2734.4,-692.437 2741.28,-691.133",
        "2856.98,-679.124 2864.62,-671.784 2854.07,-672.758 2856.98,-679.124",
        "2955.68,-581.669 2945.12,-580.824 2952.85,-588.071 2955.68,-581.669",
        "874.995,-927.771 884.996,-924.271 874.996,-920.771 874.995,-927.771",
        "2388.56,-663.45 2398.43,-659.611 2388.32,-656.454 2388.56,-663.45",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "3349.11,-691.954 3345.33,-682.057 3342.11,-692.152 3349.11,-691.954",
        "4129.22,-782.042 4126.52,-771.795 4122.24,-781.485 4129.22,-782.042",
        "4784.52,-500.096 4793.06,-493.831 4782.48,-493.4 4784.52,-500.096",
        "3442.96,-415.709 3433.08,-411.888 3438.42,-421.038 3442.96,-415.709",
        "1580.36,-692.568 1579.03,-682.057 1573.51,-691.103 1580.36,-692.568",
        "1702.29,-602.521 1699.4,-592.33 1695.31,-602.103 1702.29,-602.521",
        "1770.49,-501.004 1760.1,-498.918 1766.92,-507.027 1770.49,-501.004",
        "1473.1,-422.993 1469.6,-412.993 1466.1,-422.993 1473.1,-422.993",
        "2932.16,-779.888 2925.36,-771.76 2925.61,-782.352 2932.16,-779.888",
        "900.425,-750.047 890.171,-752.713 899.85,-757.023 900.425,-750.047",
        "3101.09,-602.976 3100.05,-592.432 3094.29,-601.322 3101.09,-602.976",
        "2388.59,-663.239 2398.48,-659.422 2388.37,-656.243 2388.59,-663.239",
        "489.098,-692.057 485.597,-682.057 482.098,-692.057 489.098,-692.057",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "2789.75,-679.905 2779.22,-678.76 2786.74,-686.224 2789.75,-679.905",
        "2920.28,-692.132 2917,-682.055 2913.28,-691.972 2920.28,-692.132",
        "2952.59,-584.546 2942.15,-582.775 2949.21,-590.674 2952.59,-584.546",
        "4856.66,-775.026 4864.39,-767.783 4853.83,-768.623 4856.66,-775.026",
        "2650.25,-679.539 2659.26,-673.973 2648.74,-672.703 2650.25,-679.539",
        "2837,-573.765 2846.75,-569.615 2836.54,-566.78 2837,-573.765",
        "4059.82,-124.625 4069.69,-120.78 4059.57,-117.629 4059.82,-124.625",
        "3922.21,-677.581 3931,-671.661 3920.44,-670.809 3922.21,-677.581",
        "3426.27,-675.456 3415.69,-676.109 3424.37,-682.193 3426.27,-675.456",
        "5034.58,-601.12 5038.39,-591.235 5029.25,-596.584 5034.58,-601.12",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "2840.15,-658.551 2850.15,-655.051 2840.15,-651.551 2840.15,-658.551",
        "2856.83,-592.979 2864.09,-585.265 2853.6,-586.766 2856.83,-592.979",
        "4065.26,-584.113 4074.65,-579.206 4064.25,-577.186 4065.26,-584.113",
        "1661.75,-599.592 1667.57,-590.744 1657.5,-594.027 1661.75,-599.592",
        "1634.02,-507.123 1640.88,-499.046 1630.48,-501.084 1634.02,-507.123",
        "3220.52,-658.551 3230.52,-655.051 3220.52,-651.551 3220.52,-658.551",
        "2910.97,-601.431 2905.63,-592.28 2904.1,-602.763 2910.97,-601.431",
        "4787.52,-501.139 4796.25,-495.137 4785.69,-494.385 4787.52,-501.139",
        "3295.96,-402.631 3305.42,-397.871 3295.05,-395.689 3295.96,-402.631",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "1703.1,-512.575 1699.6,-502.575 1696.1,-512.575 1703.1,-512.575",
        "4787.52,-501.142 4796.26,-495.14 4785.69,-494.387 4787.52,-501.142",
        "3330.25,-413.89 3338.69,-407.487 3328.1,-407.229 3330.25,-413.89",
        "4699.9,-510.022 4692.71,-502.236 4693.48,-512.804 4699.9,-510.022",
        "3482.85,-406.476 3472.26,-406.903 3480.8,-413.171 3482.85,-406.476",
        "4745.33,-493.711 4734.74,-494.191 4743.32,-500.416 4745.33,-493.711",
        "4906.97,-497.052 4896.39,-496.444 4904.28,-503.516 4906.97,-497.052",
        "4659.74,-214.957 4649.41,-217.312 4658.95,-221.913 4659.74,-214.957",
        "4982.51,-231.161 4972.16,-228.914 4978.85,-237.127 4982.51,-231.161",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "4762.8,-479.071 4772.8,-475.571 4762.8,-472.071 4762.8,-479.071",
        "4513.53,-240.93 4518.6,-231.623 4508.83,-235.742 4513.53,-240.93",
        "4704.18,-61.5952 4697.23,-53.592 4697.67,-64.1778 4704.18,-61.5952",
        "3522.92,-384.884 3512.87,-388.233 3522.82,-391.884 3522.92,-384.884",
        "4848.91,-333.015 4845.18,-323.097 4841.91,-333.173 4848.91,-333.015",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "6023.1,-333.094 6019.6,-323.094 6016.1,-333.094 6023.1,-333.094",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "4522.72,-322.738 4531.39,-316.655 4520.82,-316 4522.72,-322.738",
        "4164.57,-243.712 4162.34,-233.354 4157.62,-242.841 4164.57,-243.712",
        "4465.22,-228.688 4474.33,-223.275 4463.83,-221.827 4465.22,-228.688",
        "4386.73,-150.508 4393.18,-142.109 4382.9,-144.648 4386.73,-150.508",
        "4111.24,-148.722 4117.52,-140.185 4107.29,-142.946 4111.24,-148.722",
        "6123.62,-299.591 6133.62,-296.09 6123.62,-292.591 6123.62,-299.591",
        "4487.13,-299.591 4497.13,-296.09 4487.13,-292.591 4487.13,-299.591",
        "4218.16,-230.541 4207.67,-229.064 4214.95,-236.761 4218.16,-230.541",
        "4492.12,-236.151 4500.1,-229.183 4489.52,-229.653 4492.12,-236.151",
        "4832.28,-223.064 4841.72,-218.243 4831.33,-216.128 4832.28,-223.064",
        "4415.22,-153.748 4416.86,-143.28 4409.05,-150.444 4415.22,-153.748",
        "4299.91,-63.3569 4294.85,-54.0505 4293,-64.4834 4299.91,-63.3569",
        "4697.39,-62.9404 4692.03,-53.7986 4690.52,-64.2846 4697.39,-62.9404",
        "4573.4,-241.035 4566.14,-233.319 4567,-243.879 4573.4,-241.035",
        "6254.39,-153.573 6256.75,-143.246 6248.46,-149.845 6254.39,-153.573",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "4169.1,-153.614 4165.6,-143.614 4162.1,-153.614 4169.1,-153.614",
        "4475.57,-145.143 4465.38,-142.224 4471.52,-150.857 4475.57,-145.143",
        "4249.7,-132.992 4239.18,-134.206 4248.16,-139.82 4249.7,-132.992",
        "4399.51,-36.7381 4389.1,-38.7289 4398.48,-43.6619 4399.51,-36.7381",
        "4243.91,-218.446 4233.43,-220.044 4242.62,-225.326 4243.91,-218.446",
        "4668.1,-202.85 4658.1,-206.35 4668.1,-209.85 4668.1,-202.85",
        "6300.63,-149.714 6292.24,-143.235 6294.76,-153.528 6300.63,-149.714",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701",
        "4308.74,-120.11 4318.74,-116.61 4308.74,-113.11 4308.74,-120.11",
        "4586.19,-47.3644 4595.51,-42.3135 4585.08,-40.4541 4586.19,-47.3644"
    ]
]