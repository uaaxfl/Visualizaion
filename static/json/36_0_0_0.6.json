[
    [
        {
            "id": "2001",
            "citation_count": 172,
            "name": 172,
            "cx": 28.5975,
            "cy": -1821.67,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2002",
            "citation_count": 167,
            "name": 167,
            "cx": 28.5975,
            "cy": -1731.93,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-1812",
            "name": "Parsing and Hypergraphs",
            "publication_data": 2001,
            "citation": 91,
            "abstract": "None",
            "cx": 2149.6,
            "cy": -1821.67,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1016",
            "name": "{A}* Parsing: Fast Exact {V}iterbi Parse Selection",
            "publication_data": 2003,
            "citation": 192,
            "abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of pre-computation, reduces the work to less than 5%. Un-like best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.",
            "cx": 2075.6,
            "cy": -1642.19,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1108",
            "name": "K-Best {A}* Parsing",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "A* parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing k-best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types.",
            "cx": 2647.6,
            "cy": -1103.75,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2037",
            "name": "Top-Down K-Best {A}* Parsing",
            "publication_data": 2010,
            "citation": 9,
            "abstract": "We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA* is a variant of the k-best A* (KA*) algorithm of Pauls and Klein (2009). In contrast to KA*, which performs an inside and outside pass before performing k-best extraction bottom up, TKA* performs only the inside pass before extracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement.",
            "cx": 2123.6,
            "cy": -1014.01,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-0714",
            "name": "Distributional phrase structure induction",
            "publication_data": 2001,
            "citation": 36,
            "abstract": "Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.",
            "cx": 397.597,
            "cy": -1821.67,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P02-1017",
            "name": "A Generative Constituent-Context Model for Improved Grammar Induction",
            "publication_data": 2002,
            "citation": 167,
            "abstract": "We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.",
            "cx": 397.597,
            "cy": -1731.93,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P01-1044",
            "name": "Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the {P}enn {T}reebank",
            "publication_data": 2001,
            "citation": 45,
            "abstract": "This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.",
            "cx": 1983.6,
            "cy": -1821.67,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2003",
            "citation_count": 192,
            "name": 192,
            "cx": 28.5975,
            "cy": -1642.19,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P04-1061",
            "name": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
            "publication_data": 2004,
            "citation": 360,
            "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
            "cx": 331.597,
            "cy": -1552.45,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "Unsupervised Parsing via Constituency Tests",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.",
            "cx": 4496.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2004",
            "citation_count": 558,
            "name": 558,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D08-1012",
            "name": "Coarse-to-Fine Syntactic Machine Translation using Language Projections",
            "publication_data": 2008,
            "citation": 36,
            "abstract": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incrementally introduced. In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.",
            "cx": 2190.6,
            "cy": -1193.49,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2064",
            "name": "Hierarchical {A}* Parsing with Bridge Outside Scores",
            "publication_data": 2010,
            "citation": 6,
            "abstract": "Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.",
            "cx": 2412.6,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2005",
            "citation_count": 246,
            "name": 246,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W04-3201",
            "name": "Max-Margin Parsing",
            "publication_data": 2004,
            "citation": 198,
            "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the modelxe2x80x99s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.",
            "cx": 4705.6,
            "cy": -1552.45,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D08-1091",
            "name": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing",
            "publication_data": 2008,
            "citation": 35,
            "abstract": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.",
            "cx": 4705.6,
            "cy": -1193.49,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1022",
            "name": "Less Grammar, More Features",
            "publication_data": 2014,
            "citation": 43,
            "abstract": "We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bjorkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.",
            "cx": 4283.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D15-1032",
            "name": "An Empirical Analysis of Optimization for Max-Margin {NLP}",
            "publication_data": 2015,
            "citation": 14,
            "abstract": "Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.",
            "cx": 3674.6,
            "cy": -565.311,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P08-1100",
            "name": "Analyzing the Errors of Unsupervised Learning",
            "publication_data": 2008,
            "citation": 21,
            "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
            "cx": 233.597,
            "cy": -1193.49,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1083",
            "name": "Painless Unsupervised Learning with Features",
            "publication_data": 2010,
            "citation": 168,
            "abstract": "We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.",
            "cx": 349.597,
            "cy": -1014.01,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 1265,
            "name": 1265,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P05-1046",
            "name": "Unsupervised Learning of Field Segmentation Models for Information Extraction",
            "publication_data": 2005,
            "citation": 72,
            "abstract": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.",
            "cx": 181.597,
            "cy": -1462.71,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "H05-1010",
            "name": "A Discriminative Matching Approach to Word Alignment",
            "publication_data": 2005,
            "citation": 174,
            "abstract": "We present a discriminative, large-margin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.",
            "cx": 1227.6,
            "cy": -1462.71,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N06-1014",
            "name": "Alignment by Agreement",
            "publication_data": 2006,
            "citation": 392,
            "abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.",
            "cx": 900.597,
            "cy": -1372.97,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N06-1015",
            "name": "Word Alignment via Quadratic Assignment",
            "publication_data": 2006,
            "citation": 75,
            "abstract": "Recently, discriminative word alignment methods have achieved state-of-the-art accuracies by extending the range of information sources that can be easily incorporated into aligners. The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on. However, the proposed bipartite matching model of Taskar et al. (2005), despite being tractable and effective, has two important limitations. First, it is limited by the restriction that words have fertility of at most one. More importantly, first order correlations between consecutive words cannot be directly captured by the model. In this work, we address these limitations by enriching the model form. We give estimation and inference algorithms for these enhancements. Our best model achieves a relative AER reduction of 25% over the basic matching formulation, outperforming intersected IBM Model 4 without using any overly compute-intensive features. By including predictions of other models as features, we achieve AER of 3.8 on the standard Hansards dataset.",
            "cx": 1088.6,
            "cy": -1372.97,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1147",
            "name": "Discriminative Modeling of Extraction Sets for Machine Translation",
            "publication_data": 2010,
            "citation": 25,
            "abstract": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.",
            "cx": 937.597,
            "cy": -1014.01,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N12-1004",
            "name": "Fast Inference in Phrase Extraction Models with Belief Propagation",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up.",
            "cx": 1355.6,
            "cy": -834.531,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2007",
            "citation_count": 815,
            "name": 815,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W06-3105",
            "name": "Why Generative Phrase Models Underperform Surface Heuristics",
            "publication_data": 2006,
            "citation": 71,
            "abstract": "We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased deter-minization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score.",
            "cx": 1557.6,
            "cy": -1372.97,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P08-2007",
            "name": "The Complexity of Phrase Alignment Problems",
            "publication_data": 2008,
            "citation": 63,
            "abstract": "Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.",
            "cx": 1616.6,
            "cy": -1193.49,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1033",
            "name": "Sampling Alignment Structure under a {B}ayesian Translation Model",
            "publication_data": 2008,
            "citation": 86,
            "abstract": "We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase-based translation systems.",
            "cx": 1070.6,
            "cy": -1193.49,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1014",
            "name": "Unsupervised Syntactic Alignment with Inversion Transduction Grammars",
            "publication_data": 2010,
            "citation": 18,
            "abstract": "Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.",
            "cx": 1713.6,
            "cy": -1014.01,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P06-1055",
            "name": "Learning Accurate, Compact, and Interpretable Tree Annotation",
            "publication_data": 2006,
            "citation": 727,
            "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
            "cx": 3872.6,
            "cy": -1372.97,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N07-1051",
            "name": "Improved Inference for Unlexicalized Parsing",
            "publication_data": 2007,
            "citation": 533,
            "abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammarxe2x80x99s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.",
            "cx": 4244.6,
            "cy": -1283.23,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D07-1094",
            "name": "Learning Structured Models for Phone Recognition",
            "publication_data": 2007,
            "citation": 13,
            "abstract": "We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",
            "cx": 3980.6,
            "cy": -1283.23,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1026",
            "name": "Efficient Parsing for Transducer Grammars",
            "publication_data": 2009,
            "citation": 24,
            "abstract": "The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized. We address the problem of parsing efficiently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU.",
            "cx": 1143.6,
            "cy": -1103.75,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1112",
            "name": "Simple, Accurate Parsing with an All-Fragments Grammar",
            "publication_data": 2010,
            "citation": 26,
            "abstract": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.",
            "cx": 4571.6,
            "cy": -1014.01,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-1916",
            "name": "Mention Detection: Heuristics for the {O}nto{N}otes annotations",
            "publication_data": 2011,
            "citation": 10,
            "abstract": "Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3, Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.",
            "cx": 3460.6,
            "cy": -924.271,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-2127",
            "name": "The Surprising Variance in Shortest-Derivation Parsing",
            "publication_data": 2011,
            "citation": 2,
            "abstract": "We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.",
            "cx": 4595.6,
            "cy": -924.271,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D12-1096",
            "name": "Parser Showdown at the {W}all {S}treet Corral: An Empirical Investigation of Error Types in Parser Output",
            "publication_data": 2012,
            "citation": 47,
            "abstract": "Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.",
            "cx": 3724.6,
            "cy": -834.531,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1105",
            "name": "Training Factored {PCFG}s with Expectation Propagation",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguistically-motivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.",
            "cx": 4283.6,
            "cy": -834.531,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2018",
            "name": "An Empirical Examination of Challenges in {C}hinese Parsing",
            "publication_data": 2013,
            "citation": 11,
            "abstract": "Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.",
            "cx": 3718.6,
            "cy": -744.791,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1195",
            "name": "A Multi-Teraflop Constituency Parser using {GPU}s",
            "publication_data": 2013,
            "citation": 17,
            "abstract": "Constituency parsing with rich grammars remains a computational challenge. Graphics Processing Units (GPUs) have previously been used to accelerate CKY chart evaluation, but gains over CPU parsers were modest. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPUxe2x80x99s practical maximum speed (a Teraflop), or around a half-trillion rule evaluations per second. Net parser performance on a 4-GPU system is over 1 thousand length30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.",
            "cx": 3866.6,
            "cy": -744.791,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P07-1003",
            "name": "Tailoring Word Alignments to Syntactic Machine Translation",
            "publication_data": 2007,
            "citation": 100,
            "abstract": "Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our modelxe2x80x99s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",
            "cx": 844.597,
            "cy": -1283.23,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P09-1011",
            "name": "Learning Semantic Correspondences with Less Supervision",
            "publication_data": 2009,
            "citation": 190,
            "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.",
            "cx": 615.597,
            "cy": -1103.75,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1104",
            "name": "Better Word Alignments with Supervised {ITG} Models",
            "publication_data": 2009,
            "citation": 91,
            "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA alignments.",
            "cx": 1309.6,
            "cy": -1103.75,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1079",
            "name": "Transforming Trees to Improve Syntactic Convergence",
            "publication_data": 2012,
            "citation": 13,
            "abstract": "We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.",
            "cx": 1548.6,
            "cy": -834.531,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 613,
            "name": 613,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P07-1107",
            "name": "Unsupervised Coreference Resolution in a Nonparametric {B}ayesian Model",
            "publication_data": 2007,
            "citation": 127,
            "abstract": "We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",
            "cx": 3454.6,
            "cy": -1283.23,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D09-1120",
            "name": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
            "publication_data": 2009,
            "citation": 158,
            "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).",
            "cx": 3569.6,
            "cy": -1103.75,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N10-1061",
            "name": "Coreference Resolution in a Modular, Entity-Centered Model",
            "publication_data": 2010,
            "citation": 119,
            "abstract": "Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.",
            "cx": 3180.6,
            "cy": -1014.01,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W08-1005",
            "name": "Parsing {G}erman with Latent Variable Grammars",
            "publication_data": 2008,
            "citation": 23,
            "abstract": "We describe experiments on learning latent variable grammars for various German tree-banks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks.",
            "cx": 4105.6,
            "cy": -1193.49,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1063",
            "name": "Hierarchical Search for Parsing",
            "publication_data": 2009,
            "citation": 17,
            "abstract": "Both coarse-to-fine and A* parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarse-to-fine case because of accumulated slack in A* heuristics.",
            "cx": 2450.6,
            "cy": -1103.75,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P12-2021",
            "name": "Robust Conversion of {CCG} Derivations to Phrase Structure Trees",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.",
            "cx": 4478.6,
            "cy": -834.531,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-2133",
            "name": "How much do word embeddings encode about syntax?",
            "publication_data": 2014,
            "citation": 36,
            "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
            "cx": 4762.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1020",
            "name": "Sparser, Better, Faster {GPU} Parsing",
            "publication_data": 2014,
            "citation": 16,
            "abstract": "Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per secondxe2x80x94more than a 2x speedupxe2x80x94on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruningxe2x80x94nearly a 6x speedup.",
            "cx": 4010.6,
            "cy": -655.051,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P15-1030",
            "name": "Neural {CRF} Parsing",
            "publication_data": 2015,
            "citation": 33,
            "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.",
            "cx": 4894.6,
            "cy": -565.311,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N18-1091",
            "name": "What{'}s Going On in Neural Constituency Parsers? An Analysis",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.",
            "cx": 4128.6,
            "cy": -296.09,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N07-1052",
            "name": "Approximate Factoring for {A}* Search",
            "publication_data": 2007,
            "citation": 6,
            "abstract": "We present a novel method for creating Axe2x88x97 estimates for structured search problems. In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains.",
            "cx": 2450.6,
            "cy": -1283.23,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D07-1093",
            "name": "A Probabilistic Approach to Diachronic Phonology",
            "publication_data": 2007,
            "citation": 36,
            "abstract": "We present a probabilistic model of diachronic phonology in which individual word forms undergo stochastic edits along the branches of a phylogenetic tree. Our approach allows us to achieve three goals with a single unified model: (1) reconstruction of both ancient and modern word forms, (2) discovery of general phonological changes, and (3) selection among different phylogenies. We learn our model using a Monte Carlo EM algorithm and present quantitative results validating the model.",
            "cx": 2902.6,
            "cy": -1283.23,
            "rx": 74.9067,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1105",
            "name": "Finding Cognate Groups Using Phylogenies",
            "publication_data": 2010,
            "citation": 23,
            "abstract": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.",
            "cx": 2911.6,
            "cy": -1014.01,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1131",
            "name": "Phylogenetic Grammar Induction",
            "publication_data": 2010,
            "citation": 50,
            "abstract": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.",
            "cx": 2670.6,
            "cy": -1014.01,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1029",
            "name": "Simple Effective Decipherment via Combinatorial Optimization",
            "publication_data": 2011,
            "citation": 11,
            "abstract": "We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.",
            "cx": 2593.6,
            "cy": -924.271,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1032",
            "name": "Large-Scale Cognate Recovery",
            "publication_data": 2011,
            "citation": 6,
            "abstract": "We present a system for the large scale induction of cognate groups. Our model explains the evolution of cognates as a sequence of mutations and innovations along a phylogeny. On the task of identifying cognates from over 21,000 words in 218 different languages from the Oceanic language family, our model achieves a cluster purity score over 91%, while maintaining pairwise recall over 62%.",
            "cx": 3171.6,
            "cy": -924.271,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2009",
            "citation_count": 728,
            "name": 728,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P08-1088",
            "name": "Learning Bilingual Lexicons from Monolingual Corpora",
            "publication_data": 2008,
            "citation": 259,
            "abstract": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
            "cx": 3179.6,
            "cy": -1193.49,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N09-1069",
            "name": "Online {EM} for Unsupervised Models",
            "publication_data": 2009,
            "citation": 157,
            "abstract": "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.",
            "cx": 236.597,
            "cy": -1103.75,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D09-1147",
            "name": "Consensus Training for Consensus Decoding in Machine Translation",
            "publication_data": 2009,
            "citation": 25,
            "abstract": "We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forest-based consensus and minimum Bayes risk decoding methods. Our continuous objective can be optimized using simple gradient ascent. However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here. Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning. First, it specifically optimizes model weights for downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.",
            "cx": 2124.6,
            "cy": -1103.75,
            "rx": 95.4188,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1021",
            "name": "Unsupervised Transcription of Historical Documents",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Googlexe2x80x99s open source OCR system.",
            "cx": 2188.6,
            "cy": -744.791,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-2020",
            "name": "Improved Typesetting Models for Historical {OCR}",
            "publication_data": 2014,
            "citation": 13,
            "abstract": "We present richer typesetting models that extend the unsupervised historical document recognition system of BergKirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers.",
            "cx": 2319.6,
            "cy": -655.051,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1092",
            "name": "Two Languages are Better than One (for Syntactic Parsing)",
            "publication_data": 2008,
            "citation": 90,
            "abstract": "We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",
            "cx": 1882.6,
            "cy": -1193.49,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W10-2906",
            "name": "Learning Better Monolingual Models with Unannotated Bilingual Text",
            "publication_data": 2010,
            "citation": 33,
            "abstract": "This work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text. We build a multiview learning objective that enforces agreement between monolingual and bilingual models. In our method the first, monolingual view consists of supervised predictors learned separately for each language. The second, bilingual view consists of log-linear predictors learned over both languages on bilingual text. Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model, and we show how to combine the two models to account for dependence between views. For the task of named entity recognition, using bilingual predictors increases F1 by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F1 by 14.6%. For syntactic parsing, our bilingual predictor increases F1 by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%.",
            "cx": 1923.6,
            "cy": -1014.01,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1015",
            "name": "Joint Parsing and Alignment with Weakly Synchronized Grammars",
            "publication_data": 2010,
            "citation": 56,
            "abstract": "Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",
            "cx": 1518.6,
            "cy": -1014.01,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 667,
            "name": 667,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P09-2036",
            "name": "Asynchronous Binarization for Synchronous Grammars",
            "publication_data": 2009,
            "citation": 12,
            "abstract": "Binarization of n-ary rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.",
            "cx": 1538.6,
            "cy": -1103.75,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D10-1040",
            "name": "A Game-Theoretic Approach to Generating Spatial Descriptions",
            "publication_data": 2010,
            "citation": 94,
            "abstract": "Language is sensitive to both semantic and pragmatic effects. To capture both effects, we model language use as a cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action. Specifically, we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker. We show that a speaker model that acts optimally with respect to an explicit, embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions.",
            "cx": 615.597,
            "cy": -1014.01,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "J13-2005",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2013,
            "citation": 54,
            "abstract": "This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.",
            "cx": 274.597,
            "cy": -744.791,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1008",
            "name": "Improved Reconstruction of Protolanguage Word Forms",
            "publication_data": 2009,
            "citation": 19,
            "abstract": "We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous work. First, previous work focused on faithfulness features, which model changes between successive languages. We add markedness features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of Proto-Oceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date.",
            "cx": 3044.6,
            "cy": -1103.75,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1082",
            "name": "Type-Based {MCMC}",
            "publication_data": 2010,
            "citation": 32,
            "abstract": "Most existing algorithms for learning latent-variable models---such as EM and existing Gibbs samplers---are token-based, meaning that they update the variables associated with one sentence at a time. The incremental nature of these methods makes them susceptible to local optima/slow mixing. In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences. We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars.",
            "cx": 1197.6,
            "cy": -1014.01,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P12-1041",
            "name": "Coreference Semantics from Web Features",
            "publication_data": 2012,
            "citation": 36,
            "abstract": "To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.",
            "cx": 2923.6,
            "cy": -834.531,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1027",
            "name": "Error-Driven Analysis of Challenges in Coreference Resolution",
            "publication_data": 2013,
            "citation": 25,
            "abstract": "Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.",
            "cx": 3428.6,
            "cy": -744.791,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1203",
            "name": "Easy Victories and Uphill Battles in Coreference Resolution",
            "publication_data": 2013,
            "citation": 126,
            "abstract": "Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win xe2x80x9ceasy victoriesxe2x80x9d without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an xe2x80x9cuphill battle.xe2x80x9d Nonetheless, our final system 1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bjxc2xa8 orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.",
            "cx": 3052.6,
            "cy": -744.791,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2011",
            "citation_count": 653,
            "name": 653,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-2054",
            "name": "An Entity-Level Approach to Information Extraction",
            "publication_data": 2010,
            "citation": 8,
            "abstract": "We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.",
            "cx": 3347.6,
            "cy": -1014.01,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-1012",
            "name": "Decentralized Entity-Level Modeling for Coreference Resolution",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.",
            "cx": 2756.6,
            "cy": -744.791,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "Q14-1037",
            "name": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking",
            "publication_data": 2014,
            "citation": 122,
            "abstract": "We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",
            "cx": 2923.6,
            "cy": -655.051,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W14-1607",
            "name": "Grounding Language with Points and Paths in Continuous Spaces",
            "publication_data": 2014,
            "citation": 10,
            "abstract": "We present a model for generating pathvalued interpretations of natural language text. Our model encodes a map from natural language descriptions to paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010).",
            "cx": 318.597,
            "cy": -655.051,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D16-1125",
            "name": "Reasoning about Pragmatics with Neural Listeners and Speakers",
            "publication_data": 2016,
            "citation": 36,
            "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural listener and speaker models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.",
            "cx": 698.597,
            "cy": -475.571,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D17-1015",
            "name": "Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space",
            "publication_data": 2017,
            "citation": 7,
            "abstract": "We present a model for locating regions in space based on natural language descriptions. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as {`}on top of{'} or {`}next to,{'} and finally locate the region described in the sentence. All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation. To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions. We achieve a 32{\\%} relative error reduction compared to a strong neural baseline.",
            "cx": 892.597,
            "cy": -385.831,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1177",
            "name": "Unified Pragmatic Models for Generating and Following Instructions",
            "publication_data": 2018,
            "citation": 11,
            "abstract": "We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.",
            "cx": 576.597,
            "cy": -296.09,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 187,
            "name": 187,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1027",
            "name": "Faster and Smaller N-Gram Language Models",
            "publication_data": 2011,
            "citation": 103,
            "abstract": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.",
            "cx": 5026.6,
            "cy": -924.271,
            "rx": 76.2353,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P12-1101",
            "name": "Large-Scale Syntactic Language Modeling with Treelets",
            "publication_data": 2012,
            "citation": 27,
            "abstract": "We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.",
            "cx": 5026.6,
            "cy": -834.531,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-1049",
            "name": "Jointly Learning to Extract and Compress",
            "publication_data": 2011,
            "citation": 165,
            "abstract": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.",
            "cx": 3690.6,
            "cy": -924.271,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P16-1188",
            "name": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
            "publication_data": 2016,
            "citation": 28,
            "abstract": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.",
            "cx": 3387.6,
            "cy": -475.571,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-1060",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2011,
            "citation": 315,
            "abstract": "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (Geo and Jobs), our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
            "cx": 5207.6,
            "cy": -924.271,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N16-1181",
            "name": "Learning to Compose Neural Networks for Question Answering",
            "publication_data": 2016,
            "citation": 205,
            "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.",
            "cx": 5207.6,
            "cy": -475.571,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P11-1070",
            "name": "Web-Scale Features for Full-Scale Parsing",
            "publication_data": 2011,
            "citation": 41,
            "abstract": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.",
            "cx": 2919.6,
            "cy": -924.271,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1087",
            "name": "Decipherment with a Million Random Restarts",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.",
            "cx": 2529.6,
            "cy": -744.791,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 282,
            "name": 282,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D12-1091",
            "name": "An Empirical Investigation of Statistical Significance in {NLP}",
            "publication_data": 2012,
            "citation": 45,
            "abstract": "We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.",
            "cx": 4730.6,
            "cy": -834.531,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P19-1340",
            "name": "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
            "publication_data": 2019,
            "citation": 9,
            "abstract": "We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",
            "cx": 4371.6,
            "cy": -206.35,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 240,
            "name": 240,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1225",
            "name": "A Deep Factorization of Style and Structure in Fonts",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying generative model combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our model learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our model outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.",
            "cx": 2315.6,
            "cy": -206.35,
            "rx": 58.8803,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1105",
            "name": "Abstract Syntax Networks for Code Generation and Semantic Parsing",
            "publication_data": 2017,
            "citation": 58,
            "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7{\\%} exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1{\\%}. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.",
            "cx": 170.597,
            "cy": -385.831,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 71,
            "name": 71,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D15-1138",
            "name": "Alignment-Based Compositional Semantics for Instruction Following",
            "publication_data": 2015,
            "citation": 17,
            "abstract": "This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the modelxe2x80x99s flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",
            "cx": 318.597,
            "cy": -565.311,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N16-1150",
            "name": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mentionxe2x80x99s context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1",
            "cx": 2923.6,
            "cy": -475.571,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2052",
            "name": "Fine-Grained Entity Typing with High-Multiplicity Assignments",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.",
            "cx": 3335.6,
            "cy": -385.831,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N15-1109",
            "name": "Unsupervised Code-Switching for Multilingual Historical Document Transcription",
            "publication_data": 2015,
            "citation": 7,
            "abstract": "Transcribing documents from the printing press era, a challenge in its own right, is more complicated when documents interleave multiple languagesxe2x80x94a common feature of 16th century texts. Additionally, many of these documents precede consistent orthographic conventions, making the task even harder. We extend the state-of-the-art historical OCR model of Berg-Kirkpatrick et al. (2013) to handle word-level code-switching between multiple languages. Further, we enable our system to handle spelling variability, including now-obsolete shorthand systems used by printers. Our results show average relative character error reductions of 14% across a variety of historical texts.",
            "cx": 2315.6,
            "cy": -565.311,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1249",
            "name": "Constituency Parsing with a Self-Attentive Encoder",
            "publication_data": 2018,
            "citation": 22,
            "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
            "cx": 4703.6,
            "cy": -296.09,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 273,
            "name": 273,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.557",
            "name": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word{'}s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.",
            "cx": 4834.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "Q17-1031",
            "name": "Parsing with Traces: An {O}(n4) Algorithm and a Structural Representation",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3{\\%} of the Penn English Treebank. We also implement a proof-of-concept parser that recovers a range of null elements and trace types.",
            "cx": 3674.6,
            "cy": -385.831,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1655",
            "name": "Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "Vision-and-Language Navigation (VLN) requires grounding instructions, such as {``}turn right and stop at the door{''}, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. {``}stop at the door{''} might ground into visual objects, while {``}turn right{''} might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.",
            "cx": 318.597,
            "cy": -206.35,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 133,
            "name": 133,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1022",
            "name": "Translating Neuralese",
            "publication_data": 2017,
            "citation": "???",
            "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents{'} messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",
            "cx": 700.597,
            "cy": -385.831,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018",
            "citation_count": 52,
            "name": 52,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2025",
            "name": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.",
            "cx": 4369.6,
            "cy": -385.831,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1178",
            "name": "Effective Inference for Generative Neural Parsing",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.",
            "cx": 4770.6,
            "cy": -385.831,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-2075",
            "name": "Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser{'}s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",
            "cx": 4352.6,
            "cy": -296.09,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1031",
            "name": "Cross-Domain Generalization of Neural Constituency Parsers",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing{---}but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",
            "cx": 4636.6,
            "cy": -206.35,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1311",
            "name": "Analogs of Linguistic Structure in Deep Representations",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a {``}syntax{''} with functional analogues to qualitative properties of natural language.",
            "cx": 5103.6,
            "cy": -385.831,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1076",
            "name": "A Minimal Span-Based Neural Constituency Parser",
            "publication_data": 2017,
            "citation": 39,
            "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",
            "cx": 4550.6,
            "cy": -385.831,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.acl-main.208",
            "name": "Semantic Scaffolds for Pseudocode-to-Code Generation",
            "publication_data": 2020,
            "citation": 1,
            "abstract": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10{\\%} absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",
            "cx": 170.597,
            "cy": -116.61,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1188",
            "name": "Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.",
            "cx": 5111.6,
            "cy": -206.35,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 15,
            "name": 15,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1197",
            "name": "Learning with Latent Language",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter{'}s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.",
            "cx": 5046.6,
            "cy": -296.09,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020",
            "citation_count": 1,
            "name": 1,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.naacl-main.81",
            "name": "Modular Networks for Compositional Instruction Following",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.",
            "cx": 318.597,
            "cy": -26.8701,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        }
    ],
    [
        {
            "source": "2001",
            "target": "2002",
            "d": "M28.5975,-1803.34C28.5975,-1787.98 28.5975,-1765.66 28.5975,-1750.29"
        },
        {
            "source": "W01-1812",
            "target": "N03-1016",
            "d": "M2138.84,-1794.87C2126.03,-1764.15 2104.59,-1712.72 2090.27,-1678.38"
        },
        {
            "source": "W01-1812",
            "target": "P09-1108",
            "d": "M2215.99,-1809.37C2343.38,-1785.79 2609.6,-1726.54 2609.6,-1643.19 2609.6,-1643.19 2609.6,-1643.19 2609.6,-1282.23 2609.6,-1232.19 2624.22,-1175.93 2635.33,-1140.42"
        },
        {
            "source": "W01-1812",
            "target": "P10-2037",
            "d": "M2107.2,-1799.46C2057.44,-1771.2 1981.6,-1716.17 1981.6,-1643.19 1981.6,-1643.19 1981.6,-1643.19 1981.6,-1282.23 1981.6,-1190.69 1946.21,-1152.64 1997.6,-1076.88 2009.47,-1059.38 2027.78,-1046.63 2046.79,-1037.43"
        },
        {
            "source": "W01-0714",
            "target": "P02-1017",
            "d": "M397.597,-1794.35C397.597,-1786.38 397.597,-1777.49 397.597,-1769"
        },
        {
            "source": "P01-1044",
            "target": "W01-1812",
            "d": "M2058.17,-1821.67C2060.49,-1821.67 2062.81,-1821.67 2065.13,-1821.67"
        },
        {
            "source": "P01-1044",
            "target": "N03-1016",
            "d": "M1996.98,-1794.87C2012.96,-1764.02 2039.79,-1712.27 2057.59,-1677.93"
        },
        {
            "source": "2002",
            "target": "2003",
            "d": "M28.5975,-1713.6C28.5975,-1698.24 28.5975,-1675.92 28.5975,-1660.55"
        },
        {
            "source": "P02-1017",
            "target": "P04-1061",
            "d": "M388.141,-1705.5C376.787,-1674.97 357.673,-1623.57 344.851,-1589.09"
        },
        {
            "source": "P02-1017",
            "target": "2020.emnlp-main.389",
            "d": "M413.002,-1706.12C432.223,-1672.73 462.597,-1610.88 462.597,-1553.45 462.597,-1553.45 462.597,-1553.45 462.597,-923.271 462.597,-804.493 466.597,-774.829 466.597,-656.051 466.597,-656.051 466.597,-656.051 466.597,-295.09 466.597,-96.785 2049.44,-187.251 2247.6,-179.48 3061.48,-147.564 4041.68,-126.592 4377.23,-119.911"
        },
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1623.86C28.5975,-1608.5 28.5975,-1586.18 28.5975,-1570.81"
        },
        {
            "source": "N03-1016",
            "target": "D08-1012",
            "d": "M2071.29,-1615.11C2065.96,-1580.59 2057.6,-1517.76 2057.6,-1463.71 2057.6,-1463.71 2057.6,-1463.71 2057.6,-1371.97 2057.6,-1310.86 2108.72,-1257.31 2147.57,-1225.43"
        },
        {
            "source": "N03-1016",
            "target": "P10-2037",
            "d": "M2062.18,-1615.82C2045.62,-1582.07 2019.6,-1520.09 2019.6,-1463.71 2019.6,-1463.71 2019.6,-1463.71 2019.6,-1282.23 2019.6,-1236.54 1986.23,-1116.14 2009.6,-1076.88 2019.26,-1060.64 2034.85,-1048.39 2051.44,-1039.27"
        },
        {
            "source": "N03-1016",
            "target": "P10-2064",
            "d": "M2108.87,-1618.76C2149.13,-1588.64 2211.6,-1531.06 2211.6,-1463.71 2211.6,-1463.71 2211.6,-1463.71 2211.6,-1371.97 2211.6,-1292.92 2273.48,-1293.03 2304.6,-1220.36 2330.56,-1159.72 2307.95,-1133.02 2342.6,-1076.88 2349.66,-1065.44 2359.48,-1054.93 2369.54,-1045.93"
        },
        {
            "source": "2004",
            "target": "2005",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "W04-3201",
            "target": "D08-1091",
            "d": "M4705.6,-1525.49C4705.6,-1462.93 4705.6,-1303.64 4705.6,-1231.27"
        },
        {
            "source": "W04-3201",
            "target": "P14-1022",
            "d": "M4731.34,-1526.41C4761.55,-1494.2 4807.6,-1435.1 4807.6,-1373.97 4807.6,-1373.97 4807.6,-1373.97 4807.6,-1282.23 4807.6,-1104.59 4799.39,-1044.36 4699.6,-897.401 4685.6,-876.781 4673.71,-879.522 4656.6,-861.401 4636.15,-839.754 4637.63,-828.738 4616.6,-807.661 4569.56,-760.516 4556.2,-747.63 4496.6,-717.921 4452.25,-695.815 4398.66,-680.3 4356.34,-670.336"
        },
        {
            "source": "W04-3201",
            "target": "D15-1032",
            "d": "M4649.73,-1530.91C4547.91,-1492.67 4326.45,-1405.71 4150.6,-1310.1 4112.64,-1289.47 4107.16,-1277.7 4069.6,-1256.36 4037.45,-1238.1 4016.26,-1250.33 3994.6,-1220.36 3862.68,-1037.89 4053.28,-906.077 3929.6,-717.921 3885.45,-650.754 3800.36,-609.136 3740.65,-586.878"
        },
        {
            "source": "P04-1061",
            "target": "P08-1100",
            "d": "M324.471,-1525.49C307.216,-1462.64 263.169,-1302.2 243.428,-1230.3"
        },
        {
            "source": "P04-1061",
            "target": "N10-1083",
            "d": "M336.867,-1525.41C343.376,-1490.93 353.597,-1428.15 353.597,-1373.97 353.597,-1373.97 353.597,-1373.97 353.597,-1192.49 353.597,-1143.75 352.071,-1087.5 350.904,-1051.62"
        },
        {
            "source": "2005",
            "target": "2006",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "P05-1046",
            "target": "P08-1100",
            "d": "M186.678,-1435.6C196.017,-1387.61 215.928,-1285.29 226.561,-1230.65"
        },
        {
            "source": "H05-1010",
            "target": "N06-1014",
            "d": "M1181.79,-1449.42C1128.73,-1435.18 1040.5,-1411.51 977.008,-1394.47"
        },
        {
            "source": "H05-1010",
            "target": "N06-1015",
            "d": "M1195.36,-1441.36C1176.99,-1429.76 1153.68,-1415.05 1133.53,-1402.34"
        },
        {
            "source": "H05-1010",
            "target": "P10-1147",
            "d": "M1220.73,-1435.89C1213.02,-1410.74 1198.51,-1372.55 1175.6,-1346.1 1107.34,-1267.31 1036.35,-1306.48 977.597,-1220.36 959.118,-1193.27 946.807,-1102.24 941.157,-1050.94"
        },
        {
            "source": "H05-1010",
            "target": "N12-1004",
            "d": "M1266.22,-1444.4C1318.57,-1417.99 1405.6,-1362.35 1405.6,-1284.23 1405.6,-1284.23 1405.6,-1284.23 1405.6,-1192.49 1405.6,-1073.94 1378.21,-935.357 1363.98,-871.32"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "W06-3105",
            "target": "P08-2007",
            "d": "M1566.18,-1346.16C1576.36,-1315.54 1593.39,-1264.31 1604.8,-1229.97"
        },
        {
            "source": "W06-3105",
            "target": "D08-1033",
            "d": "M1525.86,-1348.99C1483.8,-1318.91 1411.45,-1268.59 1381.6,-1256.36 1310.39,-1227.19 1224.23,-1211.43 1160.67,-1203.13"
        },
        {
            "source": "W06-3105",
            "target": "N10-1014",
            "d": "M1588.13,-1348.65C1636.21,-1309.71 1727.61,-1226.26 1761.6,-1130.62 1769.6,-1108.12 1769.34,-1099.48 1761.6,-1076.88 1758.09,-1066.65 1752.13,-1056.8 1745.57,-1048.08"
        },
        {
            "source": "W06-3105",
            "target": "N12-1004",
            "d": "M1546.5,-1346.4C1530.21,-1304.5 1505.75,-1220.52 1544.6,-1166.62 1574.18,-1125.58 1623.01,-1171.67 1652.6,-1130.62 1666.56,-1111.25 1668.02,-1095.12 1652.6,-1076.88 1590.31,-1003.24 1510.91,-1104.7 1438.6,-1040.88 1388.69,-996.828 1423.5,-957.841 1395.6,-897.401 1391.17,-887.821 1385.45,-877.99 1379.72,-869.064"
        },
        {
            "source": "P06-1055",
            "target": "N07-1051",
            "d": "M3936.97,-1356.79C4001.19,-1341.64 4099.85,-1318.37 4168.05,-1302.29"
        },
        {
            "source": "P06-1055",
            "target": "D07-1094",
            "d": "M3902.11,-1348C3914.63,-1337.83 3929.37,-1325.85 3942.68,-1315.04"
        },
        {
            "source": "P06-1055",
            "target": "D08-1091",
            "d": "M3951.72,-1369.03C4131.63,-1361.82 4560.27,-1341.53 4617.6,-1310.1 4650.24,-1292.2 4674.5,-1256.51 4689.18,-1229.5"
        },
        {
            "source": "P06-1055",
            "target": "P09-1108",
            "d": "M3792.35,-1372.32C3548.53,-1372.71 2827.83,-1368.85 2741.6,-1310.1 2684.39,-1271.13 2661.38,-1188.25 2652.61,-1140.55"
        },
        {
            "source": "P06-1055",
            "target": "N09-1026",
            "d": "M3793.04,-1370.39C3508.65,-1364.52 2543,-1342.6 2235.6,-1310.1 2004.98,-1285.72 1913.45,-1350.63 1721.6,-1220.36 1698.41,-1204.62 1712.5,-1181.26 1688.6,-1166.62 1601.91,-1113.54 1332.59,-1148.9 1232.6,-1130.62 1225.22,-1129.27 1217.59,-1127.56 1210.03,-1125.66"
        },
        {
            "source": "P06-1055",
            "target": "P10-2037",
            "d": "M3792.58,-1371.69C3519.74,-1370.28 2627.29,-1361.72 2345.6,-1310.1 2221.63,-1287.38 2175.58,-1298.37 2076.6,-1220.36 2058.14,-1205.81 2027.07,-1153.22 2020.6,-1130.62 2014.02,-1107.66 2008.83,-1097.67 2020.6,-1076.88 2029.14,-1061.78 2043.01,-1049.96 2057.83,-1040.89"
        },
        {
            "source": "P06-1055",
            "target": "P10-2064",
            "d": "M3792.68,-1372.3C3560.19,-1372.58 2895.73,-1368.21 2818.6,-1310.1 2774.72,-1277.04 2778.28,-1112.67 2736.6,-1076.88 2671.06,-1020.61 2630.44,-1057.13 2545.6,-1040.88 2531.33,-1038.15 2516.21,-1035.23 2501.47,-1032.37"
        },
        {
            "source": "P06-1055",
            "target": "P10-1112",
            "d": "M3951.74,-1368.97C4109.01,-1362.3 4449.25,-1344.08 4489.6,-1310.1 4566.69,-1245.19 4574.58,-1114.64 4573.47,-1051.23"
        },
        {
            "source": "P06-1055",
            "target": "W11-1916",
            "d": "M3821.61,-1352.19C3763.24,-1325.8 3675.6,-1273.09 3675.6,-1194.49 3675.6,-1194.49 3675.6,-1194.49 3675.6,-1102.75 3675.6,-1025.87 3592.82,-976.064 3529.99,-949.335"
        },
        {
            "source": "P06-1055",
            "target": "P11-2127",
            "d": "M3951.71,-1368.71C4115.21,-1361.37 4479.03,-1341.67 4525.6,-1310.1 4581.24,-1272.38 4647.82,-1106.23 4663.6,-1040.88 4669.2,-1017.66 4673.26,-1008.99 4663.6,-987.141 4658.04,-974.578 4648.54,-963.47 4638.37,-954.257"
        },
        {
            "source": "P06-1055",
            "target": "D12-1096",
            "d": "M3846.92,-1347.22C3837.31,-1336.63 3827.15,-1323.61 3820.6,-1310.1 3797.69,-1262.89 3796.6,-1246.97 3796.6,-1194.49 3796.6,-1194.49 3796.6,-1194.49 3796.6,-1013.01 3796.6,-960.94 3799.57,-944.61 3777.6,-897.401 3772.7,-886.888 3765.51,-876.764 3757.99,-867.857"
        },
        {
            "source": "P06-1055",
            "target": "D12-1105",
            "d": "M3950.7,-1366.66C4064.68,-1358.05 4270.43,-1339.05 4338.6,-1310.1 4397.63,-1285.03 4423.71,-1278.12 4451.6,-1220.36 4513.86,-1091.41 4381.66,-933.777 4316.77,-867.399"
        },
        {
            "source": "P06-1055",
            "target": "P13-2018",
            "d": "M3863.5,-1346.18C3852.25,-1311.97 3834.6,-1249.48 3834.6,-1194.49 3834.6,-1194.49 3834.6,-1194.49 3834.6,-1013.01 3834.6,-921.562 3869.13,-885.788 3821.6,-807.661 3811.72,-791.421 3795.82,-778.904 3779.44,-769.516"
        },
        {
            "source": "P06-1055",
            "target": "D13-1195",
            "d": "M3872.35,-1345.86C3871.4,-1247.09 3868.04,-896.116 3866.94,-781.957"
        },
        {
            "source": "P06-1055",
            "target": "P14-1022",
            "d": "M3951.73,-1368.63C3993.96,-1362.32 4042.66,-1346.8 4069.6,-1310.1 4083.73,-1290.85 4080.38,-1277.67 4069.6,-1256.36 4057.39,-1232.25 4032.8,-1244.47 4020.6,-1220.36 4009.81,-1199.05 4016.09,-1190.08 4020.6,-1166.62 4061.1,-955.771 4068.34,-887.829 4199.6,-717.921 4209.78,-704.746 4223.48,-693.055 4236.85,-683.482"
        },
        {
            "source": "N06-1014",
            "target": "N06-1015",
            "d": "M993.496,-1372.97C995.81,-1372.97 998.124,-1372.97 1000.44,-1372.97"
        },
        {
            "source": "N06-1014",
            "target": "P07-1003",
            "d": "M884.099,-1346.12C878.524,-1337.39 872.214,-1327.5 866.299,-1318.23"
        },
        {
            "source": "N06-1014",
            "target": "P09-1011",
            "d": "M810.824,-1366.2C766.805,-1358.94 715.998,-1343.3 681.597,-1310.1 634.848,-1264.99 621.151,-1186.71 617.177,-1140.94"
        },
        {
            "source": "N06-1014",
            "target": "P09-1104",
            "d": "M961.164,-1352.55C1028.8,-1328.82 1139.18,-1283.86 1218.6,-1220.36 1247.33,-1197.39 1272.54,-1163.68 1289.09,-1138.58"
        },
        {
            "source": "N06-1014",
            "target": "P10-1147",
            "d": "M919.468,-1346.54C926.277,-1335.85 933.134,-1322.94 936.597,-1310.1 942.818,-1287.04 944.728,-1278.82 936.597,-1256.36 929.507,-1236.78 913.688,-1239.94 906.597,-1220.36 884.889,-1160.4 891.92,-1138.94 906.597,-1076.88 908.77,-1067.69 912.53,-1058.32 916.698,-1049.73"
        },
        {
            "source": "N06-1014",
            "target": "N10-1014",
            "d": "M967.945,-1354.51C1024.65,-1339.71 1101.24,-1319.43 1131.6,-1310.1 1317.35,-1253.04 1358.42,-1222.28 1544.6,-1166.62 1607.39,-1147.85 1641.85,-1177.55 1687.6,-1130.62 1707.92,-1109.78 1713.72,-1076.65 1714.84,-1051.19"
        },
        {
            "source": "N06-1014",
            "target": "N12-1004",
            "d": "M823.394,-1358.06C796.349,-1349.08 768.726,-1334.33 752.597,-1310.1 724.426,-1267.79 731.164,-1084.23 825.597,-987.141 889.25,-921.699 1152.38,-869.596 1282.32,-847.296"
        },
        {
            "source": "N06-1014",
            "target": "D12-1079",
            "d": "M817.51,-1360.85C786.6,-1352.25 754.481,-1337.03 735.597,-1310.1 680.758,-1231.91 772.8,-945.01 855.597,-897.401 967.163,-833.252 1306.19,-879.581 1433.6,-861.401 1443.81,-859.944 1454.45,-858.059 1464.97,-855.969"
        },
        {
            "source": "N06-1015",
            "target": "P10-1147",
            "d": "M1055.09,-1348.58C1020.23,-1322.05 967.609,-1275.27 944.597,-1220.36 921.336,-1164.86 925.693,-1093.33 931.37,-1051"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "P07-1003",
            "target": "P09-1104",
            "d": "M925.502,-1277.2C992.003,-1270.6 1087.34,-1255.39 1163.6,-1220.36 1208.18,-1199.88 1251.2,-1163.06 1278.85,-1136.47"
        },
        {
            "source": "P07-1003",
            "target": "P10-1147",
            "d": "M841.991,-1256.22C839.102,-1215.51 838.294,-1135.36 868.597,-1076.88 874.555,-1065.38 883.587,-1055 893.171,-1046.17"
        },
        {
            "source": "P07-1003",
            "target": "N10-1014",
            "d": "M918.199,-1270.95C993.733,-1259.15 1114.59,-1239.71 1218.6,-1220.36 1242.8,-1215.86 1632.92,-1145.42 1652.6,-1130.62 1678.82,-1110.91 1694.87,-1076.61 1703.89,-1050.47"
        },
        {
            "source": "P07-1003",
            "target": "N10-1083",
            "d": "M794.354,-1261.77C729.407,-1234.66 613.51,-1184 519.597,-1130.62 473.399,-1104.36 423.438,-1069.54 389.716,-1045"
        },
        {
            "source": "P07-1003",
            "target": "D12-1079",
            "d": "M832.597,-1256.64C808.351,-1200.73 761.087,-1066.36 825.597,-987.141 999.842,-773.174 1163.87,-919.626 1433.6,-861.401 1442.73,-859.43 1452.26,-857.34 1461.76,-855.235"
        },
        {
            "source": "P07-1107",
            "target": "D09-1120",
            "d": "M3471.32,-1256.42C3491.45,-1225.36 3525.31,-1173.09 3547.56,-1138.76"
        },
        {
            "source": "P07-1107",
            "target": "N10-1061",
            "d": "M3428.82,-1257.09C3378.82,-1208.33 3268.73,-1100.96 3213.52,-1047.12"
        },
        {
            "source": "N07-1051",
            "target": "W08-1005",
            "d": "M4207.71,-1258.95C4190.08,-1247.82 4168.82,-1234.4 4150.27,-1222.69"
        },
        {
            "source": "N07-1051",
            "target": "D08-1012",
            "d": "M4171.6,-1269.49C4140.29,-1264.53 4103.25,-1259.34 4069.6,-1256.36 3531.9,-1208.81 3395.07,-1238.85 2855.6,-1220.36 2661.89,-1213.72 2436.5,-1204.68 2304.12,-1199.23"
        },
        {
            "source": "N07-1051",
            "target": "D08-1091",
            "d": "M4316.12,-1268.62C4401.75,-1252.32 4544.46,-1225.16 4631.3,-1208.63"
        },
        {
            "source": "N07-1051",
            "target": "N09-1026",
            "d": "M4171.62,-1269.26C4140.31,-1264.27 4103.27,-1259.11 4069.6,-1256.36 3944.05,-1246.13 1922.9,-1257.69 1802.6,-1220.36 1761.33,-1207.56 1762.47,-1180.61 1721.6,-1166.62 1515.41,-1096.08 1447.07,-1169.25 1232.6,-1130.62 1225.22,-1129.29 1217.58,-1127.59 1210.02,-1125.7"
        },
        {
            "source": "N07-1051",
            "target": "N09-1063",
            "d": "M4171.6,-1269.51C4140.29,-1264.55 4103.25,-1259.35 4069.6,-1256.36 3548.74,-1210.05 3411.18,-1294.85 2893.6,-1220.36 2759.32,-1201.04 2607.18,-1156.09 2520.68,-1128.28"
        },
        {
            "source": "N07-1051",
            "target": "P10-1112",
            "d": "M4298.05,-1262.27C4322.82,-1251.74 4351.94,-1237.43 4375.6,-1220.36 4445.63,-1169.84 4511.61,-1092.29 4546.28,-1048.36"
        },
        {
            "source": "N07-1051",
            "target": "P11-2127",
            "d": "M4286.42,-1259.82C4303.57,-1249.33 4322.82,-1235.74 4337.6,-1220.36 4379.63,-1176.6 4435.49,-1028.81 4479.6,-987.141 4496.58,-971.103 4518.51,-958.001 4538.74,-948.051"
        },
        {
            "source": "N07-1051",
            "target": "P12-2021",
            "d": "M4263.92,-1257.05C4287.31,-1223.99 4323.6,-1163.27 4323.6,-1104.75 4323.6,-1104.75 4323.6,-1104.75 4323.6,-1013.01 4323.6,-947.869 4383.57,-895.098 4428.9,-864.511"
        },
        {
            "source": "N07-1051",
            "target": "D12-1079",
            "d": "M4171.16,-1269.54C4139.94,-1264.62 4103.09,-1259.44 4069.6,-1256.36 3853.25,-1236.45 3302.06,-1278.05 3092.6,-1220.36 2952.96,-1181.9 2918.95,-1147.2 2820.6,-1040.88 2800.73,-1019.41 2807.71,-1002.15 2782.6,-987.141 2776.52,-983.507 1949.96,-883.806 1653.18,-848.1"
        },
        {
            "source": "N07-1051",
            "target": "D12-1105",
            "d": "M4254.42,-1256.5C4266.55,-1222.35 4285.6,-1159.94 4285.6,-1104.75 4285.6,-1104.75 4285.6,-1104.75 4285.6,-1013.01 4285.6,-964.281 4284.83,-908.03 4284.25,-872.144"
        },
        {
            "source": "N07-1051",
            "target": "P14-2133",
            "d": "M4302.58,-1263.58C4326.03,-1253.72 4351.92,-1239.59 4370.6,-1220.36 4480.47,-1107.22 4390.56,-990.851 4517.6,-897.401 4621.15,-821.226 4719.54,-957.794 4804.6,-861.401 4846.83,-813.539 4811.47,-734.852 4785.02,-690.057"
        },
        {
            "source": "N07-1051",
            "target": "P14-1020",
            "d": "M4179.27,-1266.01C4115.13,-1249.81 4026.08,-1226.47 4020.6,-1220.36 3985.79,-1181.63 4001.6,-1156.82 4001.6,-1104.75 4001.6,-1104.75 4001.6,-1104.75 4001.6,-833.531 4001.6,-784.531 4005.06,-728.019 4007.69,-692.175"
        },
        {
            "source": "N07-1051",
            "target": "P14-1022",
            "d": "M4240.4,-1256.2C4225.58,-1163.27 4177.24,-851.035 4189.6,-807.661 4202.42,-762.66 4232.85,-717.969 4255.63,-688.896"
        },
        {
            "source": "N07-1051",
            "target": "P15-1030",
            "d": "M4302.32,-1263.44C4333.14,-1252.5 4371.28,-1237.5 4403.6,-1220.36 4622.03,-1104.52 4714.05,-1080.51 4828.6,-861.401 4873.47,-775.559 4887.95,-660.161 4892.53,-602.408"
        },
        {
            "source": "N07-1051",
            "target": "N18-1091",
            "d": "M4234.81,-1256.31C4230.92,-1245.33 4226.66,-1232.34 4223.6,-1220.36 4160.49,-973.67 4124.6,-910.686 4124.6,-656.051 4124.6,-656.051 4124.6,-656.051 4124.6,-474.571 4124.6,-425.831 4126.12,-369.582 4127.29,-333.7"
        },
        {
            "source": "N07-1052",
            "target": "D08-1012",
            "d": "M2390.95,-1262.1C2352.5,-1249.13 2302.31,-1232.19 2261.78,-1218.51"
        },
        {
            "source": "N07-1052",
            "target": "P09-1108",
            "d": "M2478.4,-1257.18C2513.66,-1225.42 2574.44,-1170.66 2612.75,-1136.15"
        },
        {
            "source": "N07-1052",
            "target": "N09-1063",
            "d": "M2450.6,-1256.04C2450.6,-1225.55 2450.6,-1175 2450.6,-1140.8"
        },
        {
            "source": "D07-1093",
            "target": "P10-1105",
            "d": "M2903.48,-1256.12C2905.09,-1208.23 2908.52,-1106.24 2910.37,-1051.51"
        },
        {
            "source": "D07-1093",
            "target": "P10-1131",
            "d": "M2896.81,-1256.1C2886.13,-1213.41 2859.5,-1128.31 2807.6,-1076.88 2790.88,-1060.32 2768.95,-1047.75 2747.55,-1038.41"
        },
        {
            "source": "D07-1093",
            "target": "D11-1029",
            "d": "M2886.41,-1256.66C2851.93,-1202.26 2774.01,-1079.79 2769.6,-1076.88 2690.15,-1024.55 2619.95,-1113.59 2558.6,-1040.88 2539.12,-1017.8 2552.63,-984.098 2568.19,-959.032"
        },
        {
            "source": "D07-1093",
            "target": "D11-1032",
            "d": "M2974.43,-1275.53C3065.7,-1266.08 3217.31,-1246.96 3266.6,-1220.36 3362.48,-1168.61 3393.72,-1142.28 3433.6,-1040.88 3442.34,-1018.65 3448.49,-1005.81 3433.6,-987.141 3388.68,-930.841 3345.66,-967.835 3275.6,-951.141 3268.09,-949.352 3260.28,-947.464 3252.47,-945.555"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "P08-1088",
            "target": "D11-1029",
            "d": "M3182.02,-1166.28C3182.95,-1140.84 3180.43,-1102.43 3160.6,-1076.88 3136.19,-1045.44 3110.43,-1064.77 3078.6,-1040.88 3053.68,-1022.19 3060.94,-1002.06 3033.6,-987.141 2921.13,-925.782 2874.35,-969.757 2747.6,-951.141 2723.42,-947.591 2697.18,-943.334 2673.33,-939.308"
        },
        {
            "source": "P08-1100",
            "target": "N09-1069",
            "d": "M234.498,-1166.17C234.77,-1158.2 235.074,-1149.31 235.364,-1140.82"
        },
        {
            "source": "D08-1012",
            "target": "P09-1104",
            "d": "M2097.24,-1180.91C2056.01,-1176.06 2006.94,-1170.62 1962.6,-1166.62 1723.92,-1145.11 1661.76,-1165.02 1424.6,-1130.62 1408.5,-1128.29 1391.31,-1124.91 1375.25,-1121.36"
        },
        {
            "source": "D08-1012",
            "target": "N09-1026",
            "d": "M2097.82,-1180.61C2056.49,-1175.67 2007.17,-1170.25 1962.6,-1166.62 1638.83,-1140.3 1552.97,-1184.32 1232.6,-1130.62 1225.01,-1129.35 1217.15,-1127.65 1209.39,-1125.73"
        },
        {
            "source": "D08-1012",
            "target": "D09-1147",
            "d": "M2171.15,-1166.64C2164.45,-1157.73 2156.84,-1147.61 2149.74,-1138.18"
        },
        {
            "source": "D08-1012",
            "target": "P13-1021",
            "d": "M2209.44,-1166.86C2216.47,-1156.14 2223.89,-1143.26 2228.6,-1130.62 2246.79,-1081.83 2247.6,-1067.08 2247.6,-1015.01 2247.6,-1015.01 2247.6,-1015.01 2247.6,-923.271 2247.6,-871.776 2224.96,-815.971 2207.72,-780.958"
        },
        {
            "source": "D08-1012",
            "target": "P14-2020",
            "d": "M2222.17,-1167.81C2233.22,-1157.5 2244.6,-1144.66 2251.6,-1130.62 2326.04,-981.297 2325.59,-774.835 2321.9,-692.282"
        },
        {
            "source": "D08-1033",
            "target": "P09-1011",
            "d": "M999.504,-1178.78C917.881,-1163.04 784.197,-1137.26 698.071,-1120.66"
        },
        {
            "source": "D08-1033",
            "target": "P10-1147",
            "d": "M1049.59,-1167.16C1040.52,-1156.07 1029.89,-1142.81 1020.6,-1130.62 1000.01,-1103.61 977.608,-1072.28 961.27,-1049.05"
        },
        {
            "source": "D08-1033",
            "target": "N12-1004",
            "d": "M1061.82,-1166.62C1058.54,-1155.66 1055.22,-1142.65 1053.6,-1130.62 1044.84,-1065.67 1044.93,-1036.88 1087.6,-987.141 1143.35,-922.152 1234.26,-879.135 1294.5,-856.037"
        },
        {
            "source": "D08-1091",
            "target": "P10-1112",
            "d": "M4686.68,-1167.44C4663.21,-1136.34 4623.08,-1083.2 4596.95,-1048.6"
        },
        {
            "source": "D08-1091",
            "target": "P14-1022",
            "d": "M4709.47,-1166.26C4716.14,-1111.94 4724.56,-984.901 4673.6,-897.401 4659.82,-873.739 4643.45,-880.266 4623.6,-861.401 4601.43,-840.328 4604.31,-826.99 4580.6,-807.661 4507.4,-747.988 4406.9,-702.804 4343.6,-677.874"
        },
        {
            "source": "D08-1091",
            "target": "P15-1030",
            "d": "M4754.55,-1173.07C4812.02,-1146.65 4899.6,-1093.53 4899.6,-1015.01 4899.6,-1015.01 4899.6,-1015.01 4899.6,-743.791 4899.6,-695.045 4897.69,-638.797 4896.23,-602.917"
        },
        {
            "source": "D08-1092",
            "target": "W10-2906",
            "d": "M1888.56,-1166.68C1895.6,-1136.19 1907.37,-1085.27 1915.3,-1050.93"
        },
        {
            "source": "D08-1092",
            "target": "N10-1015",
            "d": "M1855.79,-1168.42C1825.62,-1142.49 1774.19,-1101.61 1723.6,-1076.88 1671.65,-1051.5 1653.89,-1057.78 1598.6,-1040.88 1592.96,-1039.16 1587.12,-1037.34 1581.27,-1035.49"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "P09-1011",
            "target": "D10-1040",
            "d": "M615.597,-1076.43C615.597,-1068.46 615.597,-1059.57 615.597,-1051.08"
        },
        {
            "source": "P09-1011",
            "target": "J13-2005",
            "d": "M588.553,-1078.17C576.456,-1066.98 562.128,-1053.45 549.597,-1040.88 457.228,-948.24 353.228,-833.666 303.759,-778.503"
        },
        {
            "source": "P09-1104",
            "target": "P10-1147",
            "d": "M1260.4,-1085.03C1251.54,-1082.12 1242.34,-1079.27 1233.6,-1076.88 1166.7,-1058.58 1090.02,-1042.69 1031.47,-1031.6"
        },
        {
            "source": "P09-1104",
            "target": "N10-1014",
            "d": "M1367.47,-1089.53C1385.73,-1085.41 1405.99,-1080.9 1424.6,-1076.88 1501.79,-1060.21 1521.41,-1057.55 1598.6,-1040.88 1607.73,-1038.91 1617.26,-1036.82 1626.76,-1034.71"
        },
        {
            "source": "P09-1104",
            "target": "N10-1083",
            "d": "M1262.3,-1084.42C1252.91,-1081.42 1243.04,-1078.7 1233.6,-1076.88 934.674,-1019.29 852.769,-1068.43 549.597,-1040.88 512.15,-1037.48 470.922,-1032.38 435.677,-1027.6"
        },
        {
            "source": "P09-1104",
            "target": "N12-1004",
            "d": "M1314.09,-1076.64C1322.37,-1028.53 1340.06,-925.805 1349.45,-871.269"
        },
        {
            "source": "P09-1108",
            "target": "P10-2037",
            "d": "M2590.45,-1084.84C2579.92,-1081.92 2568.98,-1079.11 2558.6,-1076.88 2447.13,-1052.91 2417.48,-1056.95 2304.6,-1040.88 2276.47,-1036.88 2245.88,-1032.51 2218,-1028.53"
        },
        {
            "source": "N09-1008",
            "target": "P10-1105",
            "d": "M3007.55,-1078.31C2991.45,-1067.69 2972.46,-1055.16 2955.64,-1044.07"
        },
        {
            "source": "N09-1008",
            "target": "D11-1029",
            "d": "M3042.71,-1076.73C3039.29,-1049.91 3029.66,-1009.03 3002.6,-987.141 2957.24,-950.466 2800.23,-960.132 2742.6,-951.141 2719.89,-947.599 2695.28,-943.453 2672.74,-939.532"
        },
        {
            "source": "N09-1008",
            "target": "D11-1032",
            "d": "M3150.39,-1099.74C3247.24,-1094.48 3380.39,-1080.11 3413.6,-1040.88 3429.03,-1022.65 3428.25,-1006.01 3413.6,-987.141 3411.03,-983.831 3323.25,-961.943 3254.15,-945.13"
        },
        {
            "source": "N09-1026",
            "target": "P09-2036",
            "d": "M1184.19,-1127.08C1201.32,-1135.66 1221.85,-1144.32 1241.6,-1148.62 1314.98,-1164.62 1336.55,-1161.22 1410.6,-1148.62 1432.34,-1144.92 1455.31,-1137.92 1475.69,-1130.53"
        },
        {
            "source": "N09-1026",
            "target": "P09-1104",
            "d": "M1224.33,-1103.75C1226.76,-1103.75 1229.2,-1103.75 1231.63,-1103.75"
        },
        {
            "source": "N09-1026",
            "target": "N10-1014",
            "d": "M1200.19,-1084.61C1210.87,-1081.67 1222.01,-1078.92 1232.6,-1076.88 1393.09,-1045.94 1437.22,-1066.87 1598.6,-1040.88 1608.55,-1039.28 1618.93,-1037.34 1629.2,-1035.24"
        },
        {
            "source": "N09-1026",
            "target": "N10-1082",
            "d": "M1159.22,-1077.37C1164.6,-1068.63 1170.72,-1058.68 1176.47,-1049.35"
        },
        {
            "source": "N09-1063",
            "target": "P09-1108",
            "d": "M2549.87,-1103.75C2552.34,-1103.75 2554.81,-1103.75 2557.28,-1103.75"
        },
        {
            "source": "N09-1063",
            "target": "P10-2064",
            "d": "M2439.4,-1076.9C2435.8,-1068.59 2431.75,-1059.22 2427.9,-1050.35"
        },
        {
            "source": "N09-1069",
            "target": "N10-1083",
            "d": "M267.475,-1078.78C280.695,-1068.51 296.288,-1056.4 310.316,-1045.51"
        },
        {
            "source": "N09-1069",
            "target": "J13-2005",
            "d": "M239.361,-1076.79C246.051,-1013.94 263.131,-853.503 270.786,-781.597"
        },
        {
            "source": "D09-1120",
            "target": "P12-1041",
            "d": "M3553.95,-1077.36C3534.75,-1047.01 3502.54,-998.842 3484.6,-987.141 3422.81,-946.86 3385.19,-990.152 3322.6,-951.141 3296.64,-934.962 3305.93,-912.971 3279.6,-897.401 3238,-872.805 3117.54,-855.405 3028.2,-845.449"
        },
        {
            "source": "D09-1120",
            "target": "D13-1027",
            "d": "M3574.72,-1076.65C3581.39,-1036.5 3589.56,-957.903 3565.6,-897.401 3546.09,-848.144 3502.72,-804.761 3469.96,-777.162"
        },
        {
            "source": "D09-1120",
            "target": "D13-1203",
            "d": "M3565.87,-1076.56C3560.67,-1050 3548.63,-1009.7 3521.6,-987.141 3463.63,-938.773 3419.06,-992.022 3355.6,-951.141 3330.57,-935.018 3337.85,-917.189 3315.6,-897.401 3251.75,-840.617 3163.3,-794.951 3107,-769.094"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "P10-1105",
            "target": "D11-1029",
            "d": "M2850.28,-996.094C2796.91,-981.367 2719.55,-960.023 2663.73,-944.621"
        },
        {
            "source": "P10-1112",
            "target": "P11-2127",
            "d": "M4578.67,-987.161C4580.92,-978.934 4583.45,-969.684 4585.85,-960.892"
        },
        {
            "source": "N10-1015",
            "target": "N10-1014",
            "d": "M1590.2,-1014.01C1592.48,-1014.01 1594.77,-1014.01 1597.05,-1014.01"
        },
        {
            "source": "N10-1015",
            "target": "D12-1079",
            "d": "M1523.02,-986.824C1528.18,-956.326 1536.72,-905.775 1542.5,-871.583"
        },
        {
            "source": "N10-1015",
            "target": "D12-1105",
            "d": "M1567.96,-994.538C1577.97,-991.509 1588.52,-988.816 1598.6,-987.141 2076.34,-907.767 3296.83,-1009.39 3777.6,-951.141 3935.87,-931.965 4117.06,-884.076 4213.89,-856.32"
        },
        {
            "source": "N10-1061",
            "target": "P10-2054",
            "d": "M3273.23,-1014.01C3275.72,-1014.01 3278.21,-1014.01 3280.71,-1014.01"
        },
        {
            "source": "N10-1061",
            "target": "W11-1916",
            "d": "M3242.04,-993.756C3285.24,-980.221 3343.18,-962.065 3388.55,-947.847"
        },
        {
            "source": "N10-1061",
            "target": "P13-1012",
            "d": "M3113.21,-995.473C3100.13,-992.422 3086.48,-989.482 3073.6,-987.141 3009.05,-975.414 2826.85,-997.665 2780.6,-951.141 2736.58,-906.861 2741.3,-827.945 2748.85,-781.907"
        },
        {
            "source": "N10-1061",
            "target": "D13-1203",
            "d": "M3117.92,-994.076C3097.16,-984.583 3076.16,-970.79 3063.6,-951.141 3030.65,-899.627 3036.87,-825.128 3044.59,-781.457"
        },
        {
            "source": "N10-1061",
            "target": "Q14-1037",
            "d": "M3112.15,-995.888C3098.44,-992.748 3084.1,-989.668 3070.6,-987.141 3013.91,-976.529 2851.44,-994.656 2813.6,-951.141 2771.75,-903.024 2783.75,-864.014 2813.6,-807.661 2826.93,-782.497 2847.9,-793.142 2866.6,-771.661 2887.13,-748.07 2902.23,-715.784 2911.71,-691.326"
        },
        {
            "source": "D10-1040",
            "target": "W14-1607",
            "d": "M591.568,-989.388C581.529,-978.488 570.408,-964.923 562.597,-951.141 529.444,-892.64 553.538,-862.481 514.597,-807.661 476.724,-754.342 414.711,-710.883 370.586,-684.377"
        },
        {
            "source": "D10-1040",
            "target": "D16-1125",
            "d": "M622.544,-987.066C631.123,-952.683 644.597,-890.012 644.597,-835.531 644.597,-835.531 644.597,-835.531 644.597,-654.051 644.597,-602.826 665.441,-546.746 681.228,-511.616"
        },
        {
            "source": "D10-1040",
            "target": "D17-1015",
            "d": "M630.047,-988.013C636.309,-976.859 643.57,-963.475 649.597,-951.141 745.523,-754.847 843.504,-511.421 878.847,-421.938"
        },
        {
            "source": "D10-1040",
            "target": "N18-1177",
            "d": "M606.256,-987.239C594.718,-953.046 576.597,-890.582 576.597,-835.531 576.597,-835.531 576.597,-835.531 576.597,-474.571 576.597,-425.843 576.597,-369.591 576.597,-333.705"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "W11-1916",
            "target": "D13-1027",
            "d": "M3455.88,-897.083C3450.38,-866.586 3441.26,-816.035 3435.1,-781.843"
        },
        {
            "source": "P11-1027",
            "target": "P12-1101",
            "d": "M5026.6,-896.946C5026.6,-888.98 5026.6,-880.089 5026.6,-871.6"
        },
        {
            "source": "P11-1049",
            "target": "D15-1032",
            "d": "M3657.53,-899.873C3645.77,-889.569 3633.88,-876.386 3627.6,-861.401 3590.41,-772.767 3631.67,-657.857 3657.31,-601.134"
        },
        {
            "source": "P11-1049",
            "target": "P16-1188",
            "d": "M3652.39,-900.588C3637.63,-890.222 3621.66,-876.803 3610.6,-861.401 3578.88,-817.239 3570.6,-800.162 3570.6,-745.791 3570.6,-745.791 3570.6,-745.791 3570.6,-654.051 3570.6,-584.391 3501.42,-533.299 3448.13,-504.275"
        },
        {
            "source": "P11-1060",
            "target": "N16-1181",
            "d": "M5207.6,-897.109C5207.6,-862.489 5207.6,-799.559 5207.6,-745.791 5207.6,-745.791 5207.6,-745.791 5207.6,-654.051 5207.6,-605.324 5207.6,-549.071 5207.6,-513.185"
        },
        {
            "source": "P11-1070",
            "target": "P12-1041",
            "d": "M2920.8,-896.946C2921.16,-888.98 2921.57,-880.089 2921.95,-871.6"
        },
        {
            "source": "D11-1029",
            "target": "P13-1021",
            "d": "M2546.35,-902.565C2472.86,-870.361 2331.2,-808.283 2249.93,-772.667"
        },
        {
            "source": "D11-1029",
            "target": "D13-1087",
            "d": "M2584.29,-897.464C2573.25,-866.838 2554.77,-815.608 2542.39,-781.269"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "P12-1041",
            "target": "P13-1012",
            "d": "M2879.28,-810.249C2857.97,-799.049 2832.24,-785.534 2809.87,-773.778"
        },
        {
            "source": "P12-1041",
            "target": "D13-1203",
            "d": "M2959.19,-809.323C2975.19,-798.438 2994.19,-785.517 3010.86,-774.179"
        },
        {
            "source": "P12-1041",
            "target": "Q14-1037",
            "d": "M2923.6,-807.343C2923.6,-776.846 2923.6,-726.294 2923.6,-692.103"
        },
        {
            "source": "N12-1004",
            "target": "D12-1105",
            "d": "M1391.33,-857.841C1406.49,-866.41 1424.76,-875.072 1442.6,-879.401 1516.37,-897.305 4101.15,-894.251 4175.6,-879.401 4193.99,-875.732 4213.18,-868.867 4230.19,-861.59"
        },
        {
            "source": "N12-1004",
            "target": "P13-1012",
            "d": "M1403.72,-815.159C1413.47,-812.122 1423.76,-809.4 1433.6,-807.661 1698.94,-760.755 2378.4,-806.414 2645.6,-771.661 2655.78,-770.336 2666.4,-768.485 2676.86,-766.375"
        },
        {
            "source": "N12-1004",
            "target": "Q14-1037",
            "d": "M1405.15,-815.611C1414.51,-812.649 1424.29,-809.839 1433.6,-807.661 1712.84,-742.311 1787.42,-748.278 2072.6,-717.921 2365.4,-686.753 2716.57,-666.685 2858.93,-659.271"
        },
        {
            "source": "D12-1091",
            "target": "P19-1340",
            "d": "M4716.25,-808.025C4701.75,-783.464 4677.8,-746.009 4651.6,-717.921 4502.42,-557.983 4360.9,-609.553 4265.6,-412.701 4237.81,-355.304 4233.84,-324.521 4265.6,-269.22 4274.42,-253.851 4288.7,-241.937 4303.96,-232.856"
        },
        {
            "source": "D12-1096",
            "target": "P13-2018",
            "d": "M3722.8,-807.206C3722.25,-799.239 3721.64,-790.348 3721.06,-781.86"
        },
        {
            "source": "D12-1096",
            "target": "D13-1027",
            "d": "M3662.91,-815.245C3617.73,-801.854 3556.06,-783.574 3507.33,-769.13"
        },
        {
            "source": "D12-1105",
            "target": "P14-1022",
            "d": "M4283.6,-807.343C4283.6,-776.846 4283.6,-726.294 4283.6,-692.103"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "P13-1012",
            "target": "D13-1203",
            "d": "M2857.77,-744.791C2896.78,-744.791 2935.78,-744.791 2974.79,-744.791"
        },
        {
            "source": "P13-1012",
            "target": "Q14-1037",
            "d": "M2800.91,-720.509C2824.82,-707.949 2854.27,-692.477 2878.32,-679.84"
        },
        {
            "source": "P13-1021",
            "target": "P14-2020",
            "d": "M2225.09,-719.35C2240.69,-708.901 2259.04,-696.609 2275.41,-685.65"
        },
        {
            "source": "P13-1021",
            "target": "D19-1225",
            "d": "M2188.6,-717.629C2188.6,-683.009 2188.6,-620.079 2188.6,-566.311 2188.6,-566.311 2188.6,-566.311 2188.6,-384.831 2188.6,-322.928 2240.32,-267.751 2277.75,-235.927"
        },
        {
            "source": "J13-2005",
            "target": "W14-1607",
            "d": "M287.561,-717.941C291.813,-709.462 296.61,-699.896 301.139,-690.865"
        },
        {
            "source": "J13-2005",
            "target": "P17-1105",
            "d": "M245.378,-719.424C234.354,-708.895 222.565,-695.808 214.597,-681.921 166.915,-598.81 165.169,-481.728 167.869,-423.168"
        },
        {
            "source": "D13-1203",
            "target": "Q14-1037",
            "d": "M3019.38,-721.196C3002.31,-709.589 2981.39,-695.361 2963.46,-683.165"
        },
        {
            "source": "D13-1203",
            "target": "D15-1032",
            "d": "M3115.6,-735.153C3181.28,-725.355 3286.82,-707.363 3375.6,-681.921 3462.89,-656.903 3560.38,-616.811 3619.6,-591.024"
        },
        {
            "source": "D13-1203",
            "target": "P16-1188",
            "d": "M3091.09,-722.659C3138.72,-695.604 3221.18,-645.776 3283.6,-592.181 3312.67,-567.214 3341.48,-534.389 3361.38,-510.096"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "W14-1607",
            "target": "D15-1138",
            "d": "M318.597,-627.726C318.597,-619.759 318.597,-610.868 318.597,-602.38"
        },
        {
            "source": "Q14-1037",
            "target": "P16-1188",
            "d": "M2977.26,-649.746C3039.07,-643.285 3142.64,-627.786 3224.6,-592.181 3272.65,-571.306 3320.73,-534.832 3352.08,-508.425"
        },
        {
            "source": "Q14-1037",
            "target": "N16-1150",
            "d": "M2923.6,-627.863C2923.6,-597.366 2923.6,-546.814 2923.6,-512.622"
        },
        {
            "source": "Q14-1037",
            "target": "P17-2052",
            "d": "M2955.58,-633.305C3028.08,-586.286 3205.12,-471.46 3289.46,-416.756"
        },
        {
            "source": "P14-2020",
            "target": "N15-1109",
            "d": "M2318.4,-627.726C2318.03,-619.759 2317.63,-610.868 2317.24,-602.38"
        },
        {
            "source": "P14-2020",
            "target": "D19-1225",
            "d": "M2373.84,-633.183C2392.38,-623.435 2411.46,-609.972 2423.6,-592.181 2452.94,-549.161 2442.6,-528.642 2442.6,-476.571 2442.6,-476.571 2442.6,-476.571 2442.6,-384.831 2442.6,-322.928 2390.87,-267.751 2353.44,-235.927"
        },
        {
            "source": "P14-2133",
            "target": "P15-1030",
            "d": "M4796.93,-631.227C4813.61,-620.144 4833.82,-606.712 4851.51,-594.948"
        },
        {
            "source": "P14-1020",
            "target": "P16-1188",
            "d": "M3970.19,-631.324C3921.66,-605.017 3837.1,-562.264 3759.6,-538.441 3674.13,-512.168 3573.97,-496.319 3499.13,-487.249"
        },
        {
            "source": "P14-1022",
            "target": "P18-1249",
            "d": "M4323.57,-632.697C4391.71,-594.935 4532.22,-511.009 4627.6,-412.701 4634.25,-405.846 4661.97,-362.885 4682.01,-331.327"
        },
        {
            "source": "P14-1022",
            "target": "N18-1091",
            "d": "M4272.45,-628.379C4245.2,-565.629 4175.18,-404.369 4143.99,-332.532"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "P15-1030",
            "target": "P18-1249",
            "d": "M4900.26,-538.457C4907.92,-496.159 4916.37,-411.615 4875.6,-358.96 4857.78,-335.954 4830.67,-321.491 4803.19,-312.403"
        },
        {
            "source": "P15-1030",
            "target": "2020.acl-main.557",
            "d": "M4906.23,-538.589C4910.74,-527.648 4915.55,-514.623 4918.6,-502.441 4931.17,-452.232 4932.6,-438.588 4932.6,-386.831 4932.6,-386.831 4932.6,-386.831 4932.6,-295.09 4932.6,-239.043 4894.68,-184.054 4866.02,-150.49"
        },
        {
            "source": "D15-1032",
            "target": "Q17-1031",
            "d": "M3674.6,-538.123C3674.6,-507.626 3674.6,-457.074 3674.6,-422.882"
        },
        {
            "source": "D15-1032",
            "target": "P17-2052",
            "d": "M3645.11,-540.537C3612.66,-515.232 3558.24,-475.253 3506.6,-448.701 3477.08,-433.524 3442.97,-420.379 3413.02,-410.156"
        },
        {
            "source": "D15-1138",
            "target": "P19-1655",
            "d": "M318.597,-538.351C318.597,-475.785 318.597,-316.497 318.597,-244.128"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "D16-1125",
            "target": "P17-1022",
            "d": "M699.198,-448.246C699.379,-440.279 699.582,-431.388 699.775,-422.9"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "P17-2025",
            "target": "D17-1178",
            "d": "M4416.78,-409.187C4436.6,-417.763 4460.21,-426.418 4482.6,-430.701 4556.37,-444.812 4577.7,-444.169 4651.6,-430.701 4671.85,-427.009 4693.14,-420.053 4712.02,-412.702"
        },
        {
            "source": "P17-2025",
            "target": "P18-2075",
            "d": "M4364.59,-358.98C4363.01,-350.842 4361.24,-341.702 4359.55,-332.995"
        },
        {
            "source": "P17-2025",
            "target": "P18-1249",
            "d": "M4437.59,-366.969C4491.21,-352.884 4566.14,-333.199 4622.82,-318.311"
        },
        {
            "source": "P17-2025",
            "target": "P19-1031",
            "d": "M4405.61,-360.895C4453.74,-328.897 4539.06,-272.184 4591.45,-237.363"
        },
        {
            "source": "P17-2025",
            "target": "P19-1340",
            "d": "M4407.73,-360.973C4420.31,-350.928 4432.8,-338.043 4439.6,-322.96 4449.41,-301.186 4449.26,-291.065 4439.6,-269.22 4434.4,-257.471 4425.75,-246.995 4416.33,-238.151"
        },
        {
            "source": "P17-1022",
            "target": "D17-1311",
            "d": "M748.219,-409.188C768.213,-417.764 792.029,-426.419 814.597,-430.701 927.118,-452.047 4824.07,-445.774 4937.6,-430.701 4970.43,-426.341 5005.99,-417.331 5035.74,-408.521"
        },
        {
            "source": "P17-1076",
            "target": "D17-1178",
            "d": "M4618.49,-385.831C4633.76,-385.831 4649.03,-385.831 4664.3,-385.831"
        },
        {
            "source": "P17-1076",
            "target": "P18-1249",
            "d": "M4588.03,-363.367C4607.69,-352.091 4632.06,-338.117 4653.34,-325.911"
        },
        {
            "source": "P17-1076",
            "target": "N18-1091",
            "d": "M4501.64,-367.138C4492.4,-364.154 4482.76,-361.277 4473.6,-358.96 4382.64,-335.954 4357.92,-339.665 4265.6,-322.96 4248.57,-319.88 4230.38,-316.503 4212.99,-313.236"
        },
        {
            "source": "P17-1076",
            "target": "P19-1031",
            "d": "M4557.52,-358.682C4564.43,-334.647 4576.21,-298.425 4591.6,-269.22 4596.66,-259.603 4603.09,-249.811 4609.49,-240.939"
        },
        {
            "source": "P17-1076",
            "target": "2020.emnlp-main.389",
            "d": "M4544.08,-358.917C4536.64,-328.818 4524.36,-277.611 4515.6,-233.22 4510.37,-206.744 4505.46,-176.657 4501.94,-153.814"
        },
        {
            "source": "P17-1076",
            "target": "2020.acl-main.557",
            "d": "M4608.6,-371.725C4686.07,-354.161 4813.87,-324.944 4815.6,-322.96 4856.3,-276.259 4850.66,-198.882 4842.73,-153.628"
        },
        {
            "source": "P17-1105",
            "target": "2020.acl-main.208",
            "d": "M170.597,-358.722C170.597,-310.828 170.597,-208.836 170.597,-154.11"
        },
        {
            "source": "D17-1178",
            "target": "P18-1249",
            "d": "M4751.22,-359.452C4744.4,-350.527 4736.63,-340.353 4729.38,-330.848"
        },
        {
            "source": "D17-1311",
            "target": "P19-1188",
            "d": "M5123.5,-359.542C5130.69,-348.878 5137.93,-335.94 5141.6,-322.96 5148.09,-299.976 5146.94,-292.499 5141.6,-269.22 5139.5,-260.078 5135.86,-250.719 5131.83,-242.138"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "P18-2075",
            "target": "P19-1340",
            "d": "M4358.2,-269.24C4359.96,-261.102 4361.94,-251.962 4363.82,-243.255"
        },
        {
            "source": "P18-1249",
            "target": "P19-1031",
            "d": "M4683.86,-269.24C4677.11,-260.404 4669.46,-250.388 4662.31,-241.027"
        },
        {
            "source": "P18-1249",
            "target": "P19-1340",
            "d": "M4632.78,-276.376C4579.07,-262.18 4505.27,-242.676 4449.81,-228.021"
        },
        {
            "source": "P18-1249",
            "target": "2020.emnlp-main.389",
            "d": "M4732.6,-270.24C4742.5,-259.918 4752.39,-247.104 4757.6,-233.22 4765.98,-210.857 4772.41,-198.216 4757.6,-179.48 4738.3,-155.078 4668.06,-139.295 4605.3,-129.781"
        },
        {
            "source": "N18-1091",
            "target": "P18-2075",
            "d": "M4218.72,-296.09C4233.66,-296.09 4248.61,-296.09 4263.55,-296.09"
        },
        {
            "source": "N18-1091",
            "target": "P18-1249",
            "d": "M4183.85,-317.335C4210.78,-326.584 4243.92,-336.352 4274.6,-340.96 4407.77,-360.963 4444.9,-363.884 4577.6,-340.96 4598.93,-337.275 4621.44,-330.322 4641.44,-322.97"
        },
        {
            "source": "N18-1197",
            "target": "P19-1188",
            "d": "M5065.4,-269.712C5072.01,-260.787 5079.55,-250.612 5086.59,-241.107"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "P19-1340",
            "target": "P19-1031",
            "d": "M4468.64,-206.35C4483.98,-206.35 4499.32,-206.35 4514.66,-206.35"
        },
        {
            "source": "P19-1340",
            "target": "2020.acl-main.557",
            "d": "M4449.88,-190.516C4529.87,-175.357 4654.16,-151.803 4739.71,-135.592"
        },
        {
            "source": "P19-1655",
            "target": "2021.naacl-main.81",
            "d": "M318.597,-179.162C318.597,-148.665 318.597,-98.1136 318.597,-63.9219"
        },
        {
            "source": "2020",
            "target": "2021",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        }
    ],
    [
        {
            "id": "2001",
            "name": "2001",
            "x": "28.5975",
            "y": "-1817.97"
        },
        {
            "id": "2002",
            "name": "2002",
            "x": "28.5975",
            "y": "-1728.23"
        },
        {
            "id": "W01-1812",
            "name": "dan2001Parsing",
            "x": "2149.6",
            "y": "-1825.47"
        },
        {
            "id": "W01-1812",
            "name": "91",
            "x": "2149.6",
            "y": "-1810.47"
        },
        {
            "id": "N03-1016",
            "name": "dan2003{A}*",
            "x": "2075.6",
            "y": "-1645.99"
        },
        {
            "id": "N03-1016",
            "name": "192",
            "x": "2075.6",
            "y": "-1630.99"
        },
        {
            "id": "P09-1108",
            "name": "adam2009K-Best",
            "x": "2647.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1108",
            "name": "35",
            "x": "2647.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-2037",
            "name": "adam2010Top-Down",
            "x": "2123.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-2037",
            "name": "9",
            "x": "2123.6",
            "y": "-1002.81"
        },
        {
            "id": "W01-0714",
            "name": "dan2001Distributional",
            "x": "397.597",
            "y": "-1825.47"
        },
        {
            "id": "W01-0714",
            "name": "36",
            "x": "397.597",
            "y": "-1810.47"
        },
        {
            "id": "P02-1017",
            "name": "dan2002A",
            "x": "397.597",
            "y": "-1735.73"
        },
        {
            "id": "P02-1017",
            "name": "167",
            "x": "397.597",
            "y": "-1720.73"
        },
        {
            "id": "P01-1044",
            "name": "dan2001Parsing",
            "x": "1983.6",
            "y": "-1825.47"
        },
        {
            "id": "P01-1044",
            "name": "45",
            "x": "1983.6",
            "y": "-1810.47"
        },
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1638.49"
        },
        {
            "id": "P04-1061",
            "name": "dan2004Corpus-Based",
            "x": "331.597",
            "y": "-1556.25"
        },
        {
            "id": "P04-1061",
            "name": "360",
            "x": "331.597",
            "y": "-1541.25"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "steven2020Unsupervised",
            "x": "4496.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "???",
            "x": "4496.6",
            "y": "-105.41"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "D08-1012",
            "name": "slav2008Coarse-to-Fine",
            "x": "2190.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1012",
            "name": "36",
            "x": "2190.6",
            "y": "-1182.29"
        },
        {
            "id": "P10-2064",
            "name": "adam2010Hierarchical",
            "x": "2412.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-2064",
            "name": "6",
            "x": "2412.6",
            "y": "-1002.81"
        },
        {
            "id": "2005",
            "name": "2005",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "W04-3201",
            "name": "ben2004Max-Margin",
            "x": "4705.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-3201",
            "name": "198",
            "x": "4705.6",
            "y": "-1541.25"
        },
        {
            "id": "D08-1091",
            "name": "slav2008Sparse",
            "x": "4705.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1091",
            "name": "35",
            "x": "4705.6",
            "y": "-1182.29"
        },
        {
            "id": "P14-1022",
            "name": "david2014Less",
            "x": "4283.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1022",
            "name": "43",
            "x": "4283.6",
            "y": "-643.851"
        },
        {
            "id": "D15-1032",
            "name": "jonathan2015An",
            "x": "3674.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1032",
            "name": "14",
            "x": "3674.6",
            "y": "-554.111"
        },
        {
            "id": "P08-1100",
            "name": "percy2008Analyzing",
            "x": "233.597",
            "y": "-1197.29"
        },
        {
            "id": "P08-1100",
            "name": "21",
            "x": "233.597",
            "y": "-1182.29"
        },
        {
            "id": "N10-1083",
            "name": "taylor2010Painless",
            "x": "349.597",
            "y": "-1017.81"
        },
        {
            "id": "N10-1083",
            "name": "168",
            "x": "349.597",
            "y": "-1002.81"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "P05-1046",
            "name": "trond2005Unsupervised",
            "x": "181.597",
            "y": "-1466.51"
        },
        {
            "id": "P05-1046",
            "name": "72",
            "x": "181.597",
            "y": "-1451.51"
        },
        {
            "id": "H05-1010",
            "name": "ben2005A",
            "x": "1227.6",
            "y": "-1466.51"
        },
        {
            "id": "H05-1010",
            "name": "174",
            "x": "1227.6",
            "y": "-1451.51"
        },
        {
            "id": "N06-1014",
            "name": "percy2006Alignment",
            "x": "900.597",
            "y": "-1376.77"
        },
        {
            "id": "N06-1014",
            "name": "392",
            "x": "900.597",
            "y": "-1361.77"
        },
        {
            "id": "N06-1015",
            "name": "simon2006Word",
            "x": "1088.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1015",
            "name": "75",
            "x": "1088.6",
            "y": "-1361.77"
        },
        {
            "id": "P10-1147",
            "name": "john2010Discriminative",
            "x": "937.597",
            "y": "-1017.81"
        },
        {
            "id": "P10-1147",
            "name": "25",
            "x": "937.597",
            "y": "-1002.81"
        },
        {
            "id": "N12-1004",
            "name": "david2012Fast",
            "x": "1355.6",
            "y": "-838.331"
        },
        {
            "id": "N12-1004",
            "name": "9",
            "x": "1355.6",
            "y": "-823.331"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "W06-3105",
            "name": "john2006Why",
            "x": "1557.6",
            "y": "-1376.77"
        },
        {
            "id": "W06-3105",
            "name": "71",
            "x": "1557.6",
            "y": "-1361.77"
        },
        {
            "id": "P08-2007",
            "name": "john2008The",
            "x": "1616.6",
            "y": "-1197.29"
        },
        {
            "id": "P08-2007",
            "name": "63",
            "x": "1616.6",
            "y": "-1182.29"
        },
        {
            "id": "D08-1033",
            "name": "john2008Sampling",
            "x": "1070.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1033",
            "name": "86",
            "x": "1070.6",
            "y": "-1182.29"
        },
        {
            "id": "N10-1014",
            "name": "adam2010Unsupervised",
            "x": "1713.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1014",
            "name": "18",
            "x": "1713.6",
            "y": "-1002.81"
        },
        {
            "id": "P06-1055",
            "name": "slav2006Learning",
            "x": "3872.6",
            "y": "-1376.77"
        },
        {
            "id": "P06-1055",
            "name": "727",
            "x": "3872.6",
            "y": "-1361.77"
        },
        {
            "id": "N07-1051",
            "name": "slav2007Improved",
            "x": "4244.6",
            "y": "-1287.03"
        },
        {
            "id": "N07-1051",
            "name": "533",
            "x": "4244.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1094",
            "name": "slav2007Learning",
            "x": "3980.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1094",
            "name": "13",
            "x": "3980.6",
            "y": "-1272.03"
        },
        {
            "id": "N09-1026",
            "name": "john2009Efficient",
            "x": "1143.6",
            "y": "-1107.55"
        },
        {
            "id": "N09-1026",
            "name": "24",
            "x": "1143.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-1112",
            "name": "mohit2010Simple,",
            "x": "4571.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1112",
            "name": "26",
            "x": "4571.6",
            "y": "-1002.81"
        },
        {
            "id": "W11-1916",
            "name": "jonathan2011Mention",
            "x": "3460.6",
            "y": "-928.071"
        },
        {
            "id": "W11-1916",
            "name": "10",
            "x": "3460.6",
            "y": "-913.071"
        },
        {
            "id": "P11-2127",
            "name": "mohit2011The",
            "x": "4595.6",
            "y": "-928.071"
        },
        {
            "id": "P11-2127",
            "name": "2",
            "x": "4595.6",
            "y": "-913.071"
        },
        {
            "id": "D12-1096",
            "name": "jonathan2012Parser",
            "x": "3724.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1096",
            "name": "47",
            "x": "3724.6",
            "y": "-823.331"
        },
        {
            "id": "D12-1105",
            "name": "david2012Training",
            "x": "4283.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1105",
            "name": "8",
            "x": "4283.6",
            "y": "-823.331"
        },
        {
            "id": "P13-2018",
            "name": "jonathan2013An",
            "x": "3718.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2018",
            "name": "11",
            "x": "3718.6",
            "y": "-733.591"
        },
        {
            "id": "D13-1195",
            "name": "john2013A",
            "x": "3866.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1195",
            "name": "17",
            "x": "3866.6",
            "y": "-733.591"
        },
        {
            "id": "P07-1003",
            "name": "john2007Tailoring",
            "x": "844.597",
            "y": "-1287.03"
        },
        {
            "id": "P07-1003",
            "name": "100",
            "x": "844.597",
            "y": "-1272.03"
        },
        {
            "id": "P09-1011",
            "name": "percy2009Learning",
            "x": "615.597",
            "y": "-1107.55"
        },
        {
            "id": "P09-1011",
            "name": "190",
            "x": "615.597",
            "y": "-1092.55"
        },
        {
            "id": "P09-1104",
            "name": "aria2009Better",
            "x": "1309.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1104",
            "name": "91",
            "x": "1309.6",
            "y": "-1092.55"
        },
        {
            "id": "D12-1079",
            "name": "david2012Transforming",
            "x": "1548.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1079",
            "name": "13",
            "x": "1548.6",
            "y": "-823.331"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "P07-1107",
            "name": "aria2007Unsupervised",
            "x": "3454.6",
            "y": "-1287.03"
        },
        {
            "id": "P07-1107",
            "name": "127",
            "x": "3454.6",
            "y": "-1272.03"
        },
        {
            "id": "D09-1120",
            "name": "aria2009Simple",
            "x": "3569.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1120",
            "name": "158",
            "x": "3569.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1061",
            "name": "aria2010Coreference",
            "x": "3180.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1061",
            "name": "119",
            "x": "3180.6",
            "y": "-1002.81"
        },
        {
            "id": "W08-1005",
            "name": "slav2008Parsing",
            "x": "4105.6",
            "y": "-1197.29"
        },
        {
            "id": "W08-1005",
            "name": "23",
            "x": "4105.6",
            "y": "-1182.29"
        },
        {
            "id": "N09-1063",
            "name": "adam2009Hierarchical",
            "x": "2450.6",
            "y": "-1107.55"
        },
        {
            "id": "N09-1063",
            "name": "17",
            "x": "2450.6",
            "y": "-1092.55"
        },
        {
            "id": "P12-2021",
            "name": "jonathan2012Robust",
            "x": "4478.6",
            "y": "-838.331"
        },
        {
            "id": "P12-2021",
            "name": "2",
            "x": "4478.6",
            "y": "-823.331"
        },
        {
            "id": "P14-2133",
            "name": "jacob2014How",
            "x": "4762.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2133",
            "name": "36",
            "x": "4762.6",
            "y": "-643.851"
        },
        {
            "id": "P14-1020",
            "name": "david2014Sparser,",
            "x": "4010.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1020",
            "name": "16",
            "x": "4010.6",
            "y": "-643.851"
        },
        {
            "id": "P15-1030",
            "name": "greg2015Neural",
            "x": "4894.6",
            "y": "-569.111"
        },
        {
            "id": "P15-1030",
            "name": "33",
            "x": "4894.6",
            "y": "-554.111"
        },
        {
            "id": "N18-1091",
            "name": "david2018What{'}s",
            "x": "4128.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1091",
            "name": "3",
            "x": "4128.6",
            "y": "-284.89"
        },
        {
            "id": "N07-1052",
            "name": "aria2007Approximate",
            "x": "2450.6",
            "y": "-1287.03"
        },
        {
            "id": "N07-1052",
            "name": "6",
            "x": "2450.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1093",
            "name": "alexandre2007A",
            "x": "2902.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1093",
            "name": "36",
            "x": "2902.6",
            "y": "-1272.03"
        },
        {
            "id": "P10-1105",
            "name": "david2010Finding",
            "x": "2911.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1105",
            "name": "23",
            "x": "2911.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-1131",
            "name": "taylor2010Phylogenetic",
            "x": "2670.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1131",
            "name": "50",
            "x": "2670.6",
            "y": "-1002.81"
        },
        {
            "id": "D11-1029",
            "name": "taylor2011Simple",
            "x": "2593.6",
            "y": "-928.071"
        },
        {
            "id": "D11-1029",
            "name": "11",
            "x": "2593.6",
            "y": "-913.071"
        },
        {
            "id": "D11-1032",
            "name": "david2011Large-Scale",
            "x": "3171.6",
            "y": "-928.071"
        },
        {
            "id": "D11-1032",
            "name": "6",
            "x": "3171.6",
            "y": "-913.071"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "P08-1088",
            "name": "aria2008Learning",
            "x": "3179.6",
            "y": "-1197.29"
        },
        {
            "id": "P08-1088",
            "name": "259",
            "x": "3179.6",
            "y": "-1182.29"
        },
        {
            "id": "N09-1069",
            "name": "percy2009Online",
            "x": "236.597",
            "y": "-1107.55"
        },
        {
            "id": "N09-1069",
            "name": "157",
            "x": "236.597",
            "y": "-1092.55"
        },
        {
            "id": "D09-1147",
            "name": "adam2009Consensus",
            "x": "2124.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1147",
            "name": "25",
            "x": "2124.6",
            "y": "-1092.55"
        },
        {
            "id": "P13-1021",
            "name": "taylor2013Unsupervised",
            "x": "2188.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1021",
            "name": "22",
            "x": "2188.6",
            "y": "-733.591"
        },
        {
            "id": "P14-2020",
            "name": "taylor2014Improved",
            "x": "2319.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2020",
            "name": "13",
            "x": "2319.6",
            "y": "-643.851"
        },
        {
            "id": "D08-1092",
            "name": "david2008Two",
            "x": "1882.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1092",
            "name": "90",
            "x": "1882.6",
            "y": "-1182.29"
        },
        {
            "id": "W10-2906",
            "name": "david2010Learning",
            "x": "1923.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-2906",
            "name": "33",
            "x": "1923.6",
            "y": "-1002.81"
        },
        {
            "id": "N10-1015",
            "name": "david2010Joint",
            "x": "1518.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1015",
            "name": "56",
            "x": "1518.6",
            "y": "-1002.81"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "P09-2036",
            "name": "john2009Asynchronous",
            "x": "1538.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-2036",
            "name": "12",
            "x": "1538.6",
            "y": "-1092.55"
        },
        {
            "id": "D10-1040",
            "name": "dave2010A",
            "x": "615.597",
            "y": "-1017.81"
        },
        {
            "id": "D10-1040",
            "name": "94",
            "x": "615.597",
            "y": "-1002.81"
        },
        {
            "id": "J13-2005",
            "name": "percy2013Learning",
            "x": "274.597",
            "y": "-748.591"
        },
        {
            "id": "J13-2005",
            "name": "54",
            "x": "274.597",
            "y": "-733.591"
        },
        {
            "id": "N09-1008",
            "name": "alexandre2009Improved",
            "x": "3044.6",
            "y": "-1107.55"
        },
        {
            "id": "N09-1008",
            "name": "19",
            "x": "3044.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1082",
            "name": "percy2010Type-Based",
            "x": "1197.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1082",
            "name": "32",
            "x": "1197.6",
            "y": "-1002.81"
        },
        {
            "id": "P12-1041",
            "name": "mohit2012Coreference",
            "x": "2923.6",
            "y": "-838.331"
        },
        {
            "id": "P12-1041",
            "name": "36",
            "x": "2923.6",
            "y": "-823.331"
        },
        {
            "id": "D13-1027",
            "name": "jonathan2013Error-Driven",
            "x": "3428.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1027",
            "name": "25",
            "x": "3428.6",
            "y": "-733.591"
        },
        {
            "id": "D13-1203",
            "name": "greg2013Easy",
            "x": "3052.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1203",
            "name": "126",
            "x": "3052.6",
            "y": "-733.591"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "P10-2054",
            "name": "aria2010An",
            "x": "3347.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-2054",
            "name": "8",
            "x": "3347.6",
            "y": "-1002.81"
        },
        {
            "id": "P13-1012",
            "name": "greg2013Decentralized",
            "x": "2756.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1012",
            "name": "22",
            "x": "2756.6",
            "y": "-733.591"
        },
        {
            "id": "Q14-1037",
            "name": "greg2014A",
            "x": "2923.6",
            "y": "-658.851"
        },
        {
            "id": "Q14-1037",
            "name": "122",
            "x": "2923.6",
            "y": "-643.851"
        },
        {
            "id": "W14-1607",
            "name": "jacob2014Grounding",
            "x": "318.597",
            "y": "-658.851"
        },
        {
            "id": "W14-1607",
            "name": "10",
            "x": "318.597",
            "y": "-643.851"
        },
        {
            "id": "D16-1125",
            "name": "jacob2016Reasoning",
            "x": "698.597",
            "y": "-479.371"
        },
        {
            "id": "D16-1125",
            "name": "36",
            "x": "698.597",
            "y": "-464.371"
        },
        {
            "id": "D17-1015",
            "name": "nikita2017Where",
            "x": "892.597",
            "y": "-389.631"
        },
        {
            "id": "D17-1015",
            "name": "7",
            "x": "892.597",
            "y": "-374.631"
        },
        {
            "id": "N18-1177",
            "name": "daniel2018Unified",
            "x": "576.597",
            "y": "-299.89"
        },
        {
            "id": "N18-1177",
            "name": "11",
            "x": "576.597",
            "y": "-284.89"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "P11-1027",
            "name": "adam2011Faster",
            "x": "5026.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1027",
            "name": "103",
            "x": "5026.6",
            "y": "-913.071"
        },
        {
            "id": "P12-1101",
            "name": "adam2012Large-Scale",
            "x": "5026.6",
            "y": "-838.331"
        },
        {
            "id": "P12-1101",
            "name": "27",
            "x": "5026.6",
            "y": "-823.331"
        },
        {
            "id": "P11-1049",
            "name": "taylor2011Jointly",
            "x": "3690.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1049",
            "name": "165",
            "x": "3690.6",
            "y": "-913.071"
        },
        {
            "id": "P16-1188",
            "name": "greg2016Learning-Based",
            "x": "3387.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1188",
            "name": "28",
            "x": "3387.6",
            "y": "-464.371"
        },
        {
            "id": "P11-1060",
            "name": "percy2011Learning",
            "x": "5207.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1060",
            "name": "315",
            "x": "5207.6",
            "y": "-913.071"
        },
        {
            "id": "N16-1181",
            "name": "jacob2016Learning",
            "x": "5207.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1181",
            "name": "205",
            "x": "5207.6",
            "y": "-464.371"
        },
        {
            "id": "P11-1070",
            "name": "mohit2011Web-Scale",
            "x": "2919.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1070",
            "name": "41",
            "x": "2919.6",
            "y": "-913.071"
        },
        {
            "id": "D13-1087",
            "name": "taylor2013Decipherment",
            "x": "2529.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1087",
            "name": "5",
            "x": "2529.6",
            "y": "-733.591"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "D12-1091",
            "name": "taylor2012An",
            "x": "4730.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1091",
            "name": "45",
            "x": "4730.6",
            "y": "-823.331"
        },
        {
            "id": "P19-1340",
            "name": "nikita2019Multilingual",
            "x": "4371.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1340",
            "name": "9",
            "x": "4371.6",
            "y": "-195.15"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "D19-1225",
            "name": "nikita2019A",
            "x": "2315.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1225",
            "name": "1",
            "x": "2315.6",
            "y": "-195.15"
        },
        {
            "id": "P17-1105",
            "name": "maxim2017Abstract",
            "x": "170.597",
            "y": "-389.631"
        },
        {
            "id": "P17-1105",
            "name": "58",
            "x": "170.597",
            "y": "-374.631"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "D15-1138",
            "name": "jacob2015Alignment-Based",
            "x": "318.597",
            "y": "-569.111"
        },
        {
            "id": "D15-1138",
            "name": "17",
            "x": "318.597",
            "y": "-554.111"
        },
        {
            "id": "N16-1150",
            "name": "matthew2016Capturing",
            "x": "2923.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1150",
            "name": "4",
            "x": "2923.6",
            "y": "-464.371"
        },
        {
            "id": "P17-2052",
            "name": "maxim2017Fine-Grained",
            "x": "3335.6",
            "y": "-389.631"
        },
        {
            "id": "P17-2052",
            "name": "4",
            "x": "3335.6",
            "y": "-374.631"
        },
        {
            "id": "N15-1109",
            "name": "dan2015Unsupervised",
            "x": "2315.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1109",
            "name": "7",
            "x": "2315.6",
            "y": "-554.111"
        },
        {
            "id": "P18-1249",
            "name": "nikita2018Constituency",
            "x": "4703.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1249",
            "name": "22",
            "x": "4703.6",
            "y": "-284.89"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "2020.acl-main.557",
            "name": "nikita2020Tetra-Tagging:",
            "x": "4834.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.557",
            "name": "???",
            "x": "4834.6",
            "y": "-105.41"
        },
        {
            "id": "Q17-1031",
            "name": "jonathan2017Parsing",
            "x": "3674.6",
            "y": "-389.631"
        },
        {
            "id": "Q17-1031",
            "name": "4",
            "x": "3674.6",
            "y": "-374.631"
        },
        {
            "id": "P19-1655",
            "name": "ronghang2019Are",
            "x": "318.597",
            "y": "-210.15"
        },
        {
            "id": "P19-1655",
            "name": "4",
            "x": "318.597",
            "y": "-195.15"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "P17-1022",
            "name": "jacob2017Translating",
            "x": "700.597",
            "y": "-389.631"
        },
        {
            "id": "P17-1022",
            "name": "???",
            "x": "700.597",
            "y": "-374.631"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "P17-2025",
            "name": "daniel2017Improving",
            "x": "4369.6",
            "y": "-389.631"
        },
        {
            "id": "P17-2025",
            "name": "9",
            "x": "4369.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1178",
            "name": "mitchell2017Effective",
            "x": "4770.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1178",
            "name": "9",
            "x": "4770.6",
            "y": "-374.631"
        },
        {
            "id": "P18-2075",
            "name": "daniel2018Policy",
            "x": "4352.6",
            "y": "-299.89"
        },
        {
            "id": "P18-2075",
            "name": "8",
            "x": "4352.6",
            "y": "-284.89"
        },
        {
            "id": "P19-1031",
            "name": "daniel2019Cross-Domain",
            "x": "4636.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1031",
            "name": "0",
            "x": "4636.6",
            "y": "-195.15"
        },
        {
            "id": "D17-1311",
            "name": "jacob2017Analogs",
            "x": "5103.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1311",
            "name": "3",
            "x": "5103.6",
            "y": "-374.631"
        },
        {
            "id": "P17-1076",
            "name": "mitchell2017A",
            "x": "4550.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1076",
            "name": "39",
            "x": "4550.6",
            "y": "-374.631"
        },
        {
            "id": "2020.acl-main.208",
            "name": "ruiqi2020Semantic",
            "x": "170.597",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.208",
            "name": "1",
            "x": "170.597",
            "y": "-105.41"
        },
        {
            "id": "P19-1188",
            "name": "david2019Pre-Learning",
            "x": "5111.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1188",
            "name": "1",
            "x": "5111.6",
            "y": "-195.15"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "N18-1197",
            "name": "jacob2018Learning",
            "x": "5046.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1197",
            "name": "8",
            "x": "5046.6",
            "y": "-284.89"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "2021.naacl-main.81",
            "name": "rodolfo2021Modular",
            "x": "318.597",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.81",
            "name": "???",
            "x": "318.597",
            "y": "-15.6701"
        },
        {
            "id": "2021",
            "name": "2021",
            "x": "28.5975",
            "y": "-23.1701"
        }
    ],
    [
        "28.5975,-1749.93 28.5975,-1749.93 28.5975,-1749.93 28.5975,-1749.93",
        "2093.33,-1676.63 2086.25,-1668.74 2086.87,-1679.32 2093.33,-1676.63",
        "2638.78,-1141.13 2638.5,-1130.54 2632.11,-1139 2638.78,-1141.13",
        "2048.49,-1040.5 2056.16,-1033.19 2045.6,-1034.12 2048.49,-1040.5",
        "401.098,-1768.94 397.597,-1758.94 394.098,-1768.94 401.098,-1768.94",
        "2065.31,-1825.17 2075.31,-1821.67 2065.31,-1818.17 2065.31,-1825.17",
        "2060.86,-1679.23 2062.35,-1668.74 2054.64,-1676.01 2060.86,-1679.23",
        "28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19",
        "348.017,-1587.56 341.251,-1579.41 341.456,-1590 348.017,-1587.56",
        "4377.52,-123.406 4387.45,-119.708 4377.38,-116.408 4377.52,-123.406",
        "28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45",
        "2150.03,-1227.94 2155.65,-1218.96 2145.66,-1222.48 2150.03,-1227.94",
        "2053.41,-1042.19 2060.72,-1034.52 2050.22,-1035.96 2053.41,-1042.19",
        "2371.9,-1048.52 2377.24,-1039.36 2367.36,-1043.19 2371.9,-1048.52",
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "4709.1,-1230.84 4705.6,-1220.84 4702.1,-1230.84 4709.1,-1230.84",
        "4356.91,-666.877 4346.38,-668.045 4355.34,-673.699 4356.91,-666.877",
        "3741.52,-583.47 3730.93,-583.34 3739.13,-590.048 3741.52,-583.47",
        "246.72,-1229.07 240.698,-1220.35 239.97,-1230.92 246.72,-1229.07",
        "354.387,-1051.05 350.556,-1041.17 347.391,-1051.28 354.387,-1051.05",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "230.022,-1231.19 228.496,-1220.71 223.15,-1229.85 230.022,-1231.19",
        "977.606,-1391.01 967.04,-1391.8 975.791,-1397.77 977.606,-1391.01",
        "1135.15,-1399.22 1124.82,-1396.84 1131.41,-1405.14 1135.15,-1399.22",
        "944.63,-1050.51 940.084,-1040.94 937.67,-1051.26 944.63,-1050.51",
        "1367.34,-870.322 1361.73,-861.335 1360.51,-871.86 1367.34,-870.322",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "1608.17,-1230.92 1608.01,-1220.33 1601.53,-1228.72 1608.17,-1230.92",
        "1161.03,-1199.65 1150.67,-1201.87 1160.15,-1206.6 1161.03,-1199.65",
        "1748.16,-1045.71 1739.17,-1040.1 1742.7,-1050.09 1748.16,-1045.71",
        "1382.47,-866.888 1374.03,-860.484 1376.64,-870.754 1382.47,-866.888",
        "4168.95,-1305.67 4177.88,-1299.97 4167.34,-1298.86 4168.95,-1305.67",
        "3945.19,-1317.5 3950.75,-1308.48 3940.78,-1312.07 3945.19,-1317.5",
        "4692.44,-1230.83 4693.98,-1220.35 4686.24,-1227.58 4692.44,-1230.83",
        "2656.05,-1139.91 2650.9,-1130.65 2649.15,-1141.1 2656.05,-1139.91",
        "1210.72,-1122.22 1200.16,-1123.06 1208.94,-1128.99 1210.72,-1122.22",
        "2059.69,-1043.85 2066.64,-1035.85 2056.22,-1037.78 2059.69,-1043.85",
        "2501.83,-1028.88 2491.35,-1030.41 2500.5,-1035.75 2501.83,-1028.88",
        "4576.97,-1050.83 4573.21,-1040.92 4569.97,-1051.01 4576.97,-1050.83",
        "3531.07,-945.992 3520.49,-945.401 3528.39,-952.46 3531.07,-945.992",
        "4640.4,-951.393 4630.51,-947.59 4635.87,-956.73 4640.4,-951.393",
        "3760.6,-865.525 3751.35,-860.364 3755.36,-870.168 3760.6,-865.525",
        "4319.1,-864.782 4309.58,-860.138 4314.13,-869.707 4319.1,-864.782",
        "3780.79,-766.269 3770.33,-764.619 3777.48,-772.435 3780.79,-766.269",
        "3870.44,-781.631 3866.85,-771.665 3863.44,-781.698 3870.44,-781.631",
        "4238.86,-686.353 4245.11,-677.8 4234.89,-680.587 4238.86,-686.353",
        "1000.58,-1376.47 1010.58,-1372.97 1000.58,-1369.47 1000.58,-1376.47",
        "869.116,-1316.14 860.786,-1309.6 863.215,-1319.91 869.116,-1316.14",
        "620.66,-1140.57 616.415,-1130.87 613.68,-1141.1 620.66,-1140.57",
        "1292.08,-1140.39 1294.57,-1130.09 1286.21,-1136.59 1292.08,-1140.39",
        "919.844,-1051.27 921.324,-1040.78 913.625,-1048.06 919.844,-1051.27",
        "1718.34,-1051.23 1715.07,-1041.16 1711.34,-1051.08 1718.34,-1051.23",
        "1282.93,-850.743 1292.2,-845.614 1281.75,-843.842 1282.93,-850.743",
        "1465.9,-859.351 1474.99,-853.911 1464.49,-852.494 1465.9,-859.351",
        "934.864,-1051.29 932.825,-1040.9 927.935,-1050.29 934.864,-1051.29",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "1281.52,-1138.75 1286.23,-1129.26 1276.63,-1133.75 1281.52,-1138.75",
        "895.677,-1048.63 900.927,-1039.42 891.084,-1043.34 895.677,-1048.63",
        "1707.23,-1051.49 1707,-1040.9 1700.58,-1049.32 1707.23,-1051.49",
        "391.642,-1042.08 381.504,-1039 387.509,-1047.73 391.642,-1042.08",
        "1462.62,-858.628 1471.62,-853.041 1461.1,-851.795 1462.62,-858.628",
        "3550.73,-1140.31 3553.23,-1130.02 3544.85,-1136.5 3550.73,-1140.31",
        "3215.71,-1044.37 3206.11,-1039.89 3210.82,-1049.38 3215.71,-1044.37",
        "4152.02,-1219.65 4141.69,-1217.28 4148.28,-1225.57 4152.02,-1219.65",
        "2304.11,-1195.72 2293.97,-1198.81 2303.82,-1202.72 2304.11,-1195.72",
        "4632.18,-1212.03 4641.35,-1206.72 4630.87,-1205.15 4632.18,-1212.03",
        "1210.71,-1122.26 1200.15,-1123.11 1208.94,-1129.03 1210.71,-1122.26",
        "2521.57,-1124.89 2510.98,-1125.14 2519.42,-1131.55 2521.57,-1124.89",
        "4549.18,-1050.34 4552.59,-1040.31 4543.67,-1046.03 4549.18,-1050.34",
        "4540.35,-951.157 4547.88,-943.701 4537.35,-944.835 4540.35,-951.157",
        "4431.04,-867.297 4437.47,-858.877 4427.19,-861.45 4431.04,-867.297",
        "1653.19,-844.577 1642.85,-846.857 1652.36,-851.527 1653.19,-844.577",
        "4287.74,-871.633 4284.08,-861.692 4280.74,-871.749 4287.74,-871.633",
        "4787.89,-688.036 4779.71,-681.302 4781.9,-691.667 4787.89,-688.036",
        "4011.18,-692.426 4008.44,-682.192 4004.2,-691.902 4011.18,-692.426",
        "4258.63,-690.756 4262.13,-680.758 4253.16,-686.387 4258.63,-690.756",
        "4896.03,-602.546 4893.28,-592.315 4889.05,-602.03 4896.03,-602.546",
        "4130.8,-333.36 4127.64,-323.249 4123.81,-333.126 4130.8,-333.36",
        "2262.82,-1215.17 2252.23,-1215.29 2260.58,-1221.8 2262.82,-1215.17",
        "2615.48,-1138.4 2620.56,-1129.11 2610.79,-1133.2 2615.48,-1138.4",
        "2454.1,-1140.76 2450.6,-1130.76 2447.1,-1140.76 2454.1,-1140.76",
        "2913.88,-1051.34 2910.71,-1041.23 2906.88,-1051.1 2913.88,-1051.34",
        "2748.68,-1035.09 2738.1,-1034.48 2745.98,-1041.55 2748.68,-1035.09",
        "2571.18,-960.843 2573.72,-950.557 2565.32,-957.016 2571.18,-960.843",
        "3253.15,-942.116 3242.6,-943.132 3251.48,-948.915 3253.15,-942.116",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "2673.71,-935.823 2663.26,-937.6 2672.54,-942.724 2673.71,-935.823",
        "238.864,-1140.87 235.708,-1130.76 231.869,-1140.63 238.864,-1140.87",
        "1375.95,-1117.93 1365.43,-1119.13 1374.41,-1124.75 1375.95,-1117.93",
        "1210.19,-1122.32 1199.64,-1123.19 1208.43,-1129.1 1210.19,-1122.32",
        "2152.48,-1136 2143.68,-1130.11 2146.89,-1140.21 2152.48,-1136",
        "2210.66,-779.007 2203.03,-771.655 2204.41,-782.16 2210.66,-779.007",
        "2325.38,-691.971 2321.4,-682.156 2318.39,-692.316 2325.38,-691.971",
        "698.553,-1117.18 688.071,-1118.73 697.227,-1124.06 698.553,-1117.18",
        "963.93,-1046.75 955.322,-1040.57 958.199,-1050.77 963.93,-1046.75",
        "1295.77,-859.299 1303.89,-852.501 1293.3,-852.748 1295.77,-859.299",
        "4599.45,-1046.09 4590.63,-1040.22 4593.86,-1050.31 4599.45,-1046.09",
        "4344.75,-674.563 4334.16,-674.195 4342.21,-681.086 4344.75,-674.563",
        "4899.71,-602.312 4895.8,-592.467 4892.71,-602.604 4899.71,-602.312",
        "1918.79,-1051.38 1917.63,-1040.85 1911.97,-1049.81 1918.79,-1051.38",
        "1582.04,-1032.06 1571.45,-1032.37 1579.92,-1038.73 1582.04,-1032.06",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "619.098,-1051.02 615.597,-1041.02 612.098,-1051.02 619.098,-1051.02",
        "306.143,-775.918 296.864,-770.804 300.929,-780.589 306.143,-775.918",
        "1031.97,-1028.13 1021.5,-1029.72 1030.68,-1035.01 1031.97,-1028.13",
        "1627.62,-1038.11 1636.62,-1032.52 1626.1,-1031.27 1627.62,-1038.11",
        "435.807,-1024.09 425.424,-1026.2 434.858,-1031.02 435.807,-1024.09",
        "1352.91,-871.801 1351.15,-861.352 1346.01,-870.614 1352.91,-871.801",
        "2218.25,-1025.03 2207.85,-1027.07 2217.26,-1031.96 2218.25,-1025.03",
        "2957.35,-1041 2947.07,-1038.41 2953.49,-1046.84 2957.35,-1041",
        "2673.33,-936.083 2662.88,-937.809 2672.13,-942.978 2673.33,-936.083",
        "3254.65,-941.649 3244.11,-942.689 3253,-948.451 3254.65,-941.649",
        "1477.02,-1133.77 1485.17,-1127 1474.58,-1127.21 1477.02,-1133.77",
        "1231.65,-1107.25 1241.65,-1103.75 1231.65,-1100.25 1231.65,-1107.25",
        "1629.93,-1038.66 1639.01,-1033.19 1628.5,-1031.81 1629.93,-1038.66",
        "1179.57,-1050.99 1181.83,-1040.64 1173.6,-1047.32 1179.57,-1050.99",
        "2557.43,-1107.25 2567.43,-1103.75 2557.43,-1100.25 2557.43,-1107.25",
        "2431.03,-1048.77 2423.84,-1040.98 2424.61,-1051.55 2431.03,-1048.77",
        "312.612,-1048.16 318.364,-1039.26 308.319,-1042.63 312.612,-1048.16",
        "274.266,-781.967 271.844,-771.653 267.305,-781.226 274.266,-781.967",
        "3028.34,-841.943 3018.01,-844.331 3027.57,-848.901 3028.34,-841.943",
        "3471.82,-774.16 3461.89,-770.491 3467.37,-779.557 3471.82,-774.16",
        "3108.12,-765.757 3097.57,-764.807 3105.23,-772.13 3108.12,-765.757",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "2664.48,-941.199 2653.91,-941.913 2662.62,-947.947 2664.48,-941.199",
        "4589.23,-961.809 4588.49,-951.24 4582.48,-959.962 4589.23,-961.809",
        "1597.07,-1017.51 1607.07,-1014.01 1597.07,-1010.51 1597.07,-1017.51",
        "1545.99,-871.981 1544.2,-861.537 1539.08,-870.814 1545.99,-871.981",
        "4214.92,-859.665 4223.56,-853.533 4212.98,-852.938 4214.92,-859.665",
        "3280.95,-1017.51 3290.95,-1014.01 3280.95,-1010.51 3280.95,-1017.51",
        "3389.83,-951.115 3398.33,-944.784 3387.74,-944.435 3389.83,-951.115",
        "2752.34,-782.236 2750.64,-771.779 2745.45,-781.015 2752.34,-782.236",
        "3048.04,-782.051 3046.46,-771.575 3041.16,-780.752 3048.04,-782.051",
        "2915.08,-692.299 2915.31,-681.706 2908.53,-689.846 2915.08,-692.299",
        "372.124,-681.221 361.735,-679.143 368.561,-687.246 372.124,-681.221",
        "684.522,-512.832 685.526,-502.284 678.164,-509.903 684.522,-512.832",
        "882.142,-423.12 882.554,-412.534 875.63,-420.553 882.142,-423.12",
        "580.098,-333.253 576.597,-323.253 573.098,-333.253 580.098,-333.253",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "3438.51,-781.017 3433.29,-771.797 3431.62,-782.26 3438.51,-781.017",
        "5030.1,-871.535 5026.6,-861.535 5023.1,-871.535 5030.1,-871.535",
        "3660.56,-602.436 3661.58,-591.89 3654.21,-599.499 3660.56,-602.436",
        "3449.53,-501.057 3439.06,-499.46 3446.25,-507.24 3449.53,-501.057",
        "5211.1,-512.733 5207.6,-502.733 5204.1,-512.733 5211.1,-512.733",
        "2925.45,-871.684 2922.41,-861.535 2918.46,-871.365 2925.45,-871.684",
        "2250.89,-769.266 2240.32,-768.458 2248.08,-775.678 2250.89,-769.266",
        "2545.6,-779.849 2538.91,-771.629 2539.01,-782.224 2545.6,-779.849",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "2811.31,-770.582 2800.83,-769.03 2808.05,-776.779 2811.31,-770.582",
        "3013.04,-776.931 3019.34,-768.413 3009.1,-771.143 3013.04,-776.931",
        "2927.1,-692.057 2923.6,-682.057 2920.1,-692.057 2927.1,-692.057",
        "4231.66,-864.766 4239.39,-857.523 4228.83,-858.363 4231.66,-864.766",
        "2677.75,-769.765 2686.82,-764.282 2676.31,-762.914 2677.75,-769.765",
        "2859.42,-662.751 2869.22,-658.738 2859.06,-655.76 2859.42,-662.751",
        "4305.98,-235.742 4313.03,-227.832 4302.58,-229.619 4305.98,-235.742",
        "3724.55,-781.533 3720.38,-771.795 3717.57,-782.01 3724.55,-781.533",
        "3508.04,-765.688 3497.45,-766.202 3506.05,-772.399 3508.04,-765.688",
        "4287.1,-692.057 4283.6,-682.057 4280.1,-692.057 4287.1,-692.057",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "2975.24,-748.291 2985.24,-744.791 2975.24,-741.291 2975.24,-748.291",
        "2880.02,-682.897 2887.25,-675.148 2876.77,-676.701 2880.02,-682.897",
        "2277.39,-688.53 2283.76,-680.057 2273.5,-682.714 2277.39,-688.53",
        "2280.04,-238.577 2285.5,-229.497 2275.57,-233.191 2280.04,-238.577",
        "304.373,-692.224 305.727,-681.716 298.115,-689.086 304.373,-692.224",
        "171.377,-423.108 168.415,-412.936 164.387,-422.735 171.377,-423.108",
        "2965.17,-680.092 2954.93,-677.362 2961.23,-685.88 2965.17,-680.092",
        "3621.36,-594.07 3629.12,-586.855 3618.56,-587.658 3621.36,-594.07",
        "3364.19,-512.181 3367.77,-502.209 3358.76,-507.773 3364.19,-512.181",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "322.098,-602.315 318.597,-592.315 315.098,-602.315 322.098,-602.315",
        "3354.74,-510.759 3360.07,-501.606 3350.2,-505.434 3354.74,-510.759",
        "2927.1,-512.577 2923.6,-502.577 2920.1,-512.577 2927.1,-512.577",
        "3291.57,-419.558 3298.06,-411.18 3287.76,-413.685 3291.57,-419.558",
        "2320.73,-602.145 2316.78,-592.315 2313.74,-602.464 2320.73,-602.145",
        "2355.62,-233.191 2345.69,-229.497 2351.15,-238.577 2355.62,-233.191",
        "4853.58,-597.775 4859.97,-589.325 4849.71,-591.946 4853.58,-597.775",
        "3499.09,-483.719 3488.74,-486.014 3498.26,-490.67 3499.09,-483.719",
        "4685.04,-333.088 4687.44,-322.768 4679.13,-329.34 4685.04,-333.088",
        "4147.05,-330.788 4139.85,-323.009 4140.62,-333.576 4147.05,-330.788",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "4804.06,-309.009 4793.48,-309.407 4802,-315.698 4804.06,-309.009",
        "4868.55,-148.064 4859.33,-142.842 4863.28,-152.673 4868.55,-148.064",
        "3678.1,-422.837 3674.6,-412.837 3671.1,-422.837 3678.1,-422.837",
        "3414.04,-406.808 3403.45,-406.94 3411.82,-413.444 3414.04,-406.808",
        "322.098,-243.695 318.597,-233.695 315.098,-243.695 322.098,-243.695",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "703.276,-422.912 700.005,-412.835 696.278,-422.752 703.276,-422.912",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "4713.39,-415.925 4721.37,-408.955 4710.78,-409.427 4713.39,-415.925",
        "4362.97,-332.211 4357.63,-323.059 4356.1,-333.542 4362.97,-332.211",
        "4624.04,-321.609 4632.82,-315.683 4622.26,-314.839 4624.04,-321.609",
        "4593.49,-240.21 4599.88,-231.759 4589.61,-234.38 4593.49,-240.21",
        "4418.49,-235.387 4408.67,-231.416 4413.87,-240.646 4418.49,-235.387",
        "5037.03,-411.788 5045.6,-405.548 5035.01,-405.086 5037.03,-411.788",
        "4664.34,-389.331 4674.34,-385.831 4664.34,-382.331 4664.34,-389.331",
        "4655.32,-328.812 4662.25,-320.801 4651.84,-322.739 4655.32,-328.812",
        "4213.34,-309.739 4202.86,-311.327 4212.04,-316.618 4213.34,-309.739",
        "4612.42,-242.866 4615.57,-232.751 4606.8,-238.692 4612.42,-242.866",
        "4505.39,-153.227 4500.42,-143.868 4498.47,-154.281 4505.39,-153.227",
        "4846.15,-152.847 4840.86,-143.667 4839.27,-154.142 4846.15,-152.847",
        "174.098,-153.825 170.597,-143.825 167.098,-153.825 174.098,-153.825",
        "4732.02,-328.539 4723.17,-322.715 4726.45,-332.787 4732.02,-328.539",
        "5134.95,-240.559 5127.35,-233.178 5128.69,-243.688 5134.95,-240.559",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "4367.28,-243.833 4365.97,-233.319 4360.44,-242.352 4367.28,-243.833",
        "4665.05,-238.839 4656.2,-233.016 4659.48,-243.088 4665.05,-238.839",
        "4450.59,-224.607 4440.03,-225.436 4448.81,-231.375 4450.59,-224.607",
        "4605.71,-126.303 4595.3,-128.307 4604.69,-133.228 4605.71,-126.303",
        "4263.89,-299.591 4273.89,-296.09 4263.89,-292.591 4263.89,-299.591",
        "4642.92,-326.151 4651.04,-319.341 4640.45,-319.603 4642.92,-326.151",
        "5089.47,-243.094 5092.61,-232.975 5083.85,-238.927 5089.47,-243.094",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "4514.74,-209.85 4524.74,-206.35 4514.74,-202.85 4514.74,-209.85",
        "4740.51,-139.003 4749.68,-133.702 4739.2,-132.126 4740.51,-139.003",
        "322.098,-63.8762 318.597,-53.8762 315.098,-63.8762 322.098,-63.8762",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701"
    ]
]