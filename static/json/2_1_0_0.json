[
    [
        {
            "id": "1988",
            "citation_count": 194,
            "name": 194,
            "cx": 28.5975,
            "cy": -2988.29,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1989",
            "citation_count": 2,
            "name": 2,
            "cx": 28.5975,
            "cy": -2898.55,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P88-1020",
            "name": "Planning Coherent Multisentential Text",
            "publication_data": 1988,
            "citation": 172,
            "abstract": "Though most text generators are capable of simply stringing together more than one sentence, they cannot determine which order will ensure a coherent paragraph. A paragraph is coherent when the information in successive sentences follows some pattern of inference or of knowledge with which the hearer is familiar. To signal such inferences, speakers usually use relations that link successive sentences in fixed ways. A set of 20 relations that span most of what people usually say in English is proposed in the Rhetorical Structure Theory of Mann and Thompson. This paper describes the formalization of these relations and their use in a prototype text planner that structures input elements into coherent paragraphs.",
            "cx": 598.597,
            "cy": -2988.29,
            "rx": 91.4341,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W90-0117",
            "name": "Parsimonious and Profligate Approaches to the Question of Discourse Structure Relations",
            "publication_data": 1990,
            "citation": 58,
            "abstract": "None",
            "cx": 452.597,
            "cy": -2808.81,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "1997.tmi-1.6",
            "name": "{MT} at the paragraph level: improving {E}nglish synthesis in {SYSTRAN}",
            "publication_data": 1997,
            "citation": "???",
            "abstract": "None",
            "cx": 798.597,
            "cy": -2180.63,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D14-1218",
            "name": "A Model of Coherence Based on Distributed Sentence Representation",
            "publication_data": 2014,
            "citation": 65,
            "abstract": "Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argumentation semantics and crosssentence syntax (such as coreference and tense rules) are very hard to formalize. In this paper, we introduce a neural network model for the coherence task based on distributed sentence representation. The proposed approach learns a syntacticosemantic representation for sentences automatically, using either recurrent or recursive neural networks. The architecture obviated the need for feature engineering, and learns sentence representations, which are to some extent able to capture the xe2x80x98rulesxe2x80x99 governing coherent sentence structure. The proposed approach outperforms existing baselines and generates the stateof-art performance in standard coherence evaluation tasks 1 .",
            "cx": 596.597,
            "cy": -655.051,
            "rx": 55.3091,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P88-1022",
            "name": "Two Types of Planning in Language Generation",
            "publication_data": 1988,
            "citation": 22,
            "abstract": "As our understanding of natural language generation has increased, a number of tasks have been separated from realization and put together under the heading text planning. So far, however, no-one has enumerated the kinds of tasks a text planner should be able to do. This paper describes the principal lesson learned in combining a number of planning tasks in a planner-realizer: planning and realization should be interleaved, in a limited-commitment planning paradigm, to perform two types of planning: prescriptive and restrictive. Limited-commitment planning consists of both prescriptive (hierarchical expansion) planning and of restrictive planning (selecting from options with reference to the status of active goals). At present, existing text planners use prescriptive plans exclusively. However, a large class of planner tasks, especially those concerned with the pragmatic (non-literal) content of text such as style and slant, is most easily performed under restrictive planning. The kinds of tasks suited to each planning style are listed, and a program that uses both styles is described.",
            "cx": 412.597,
            "cy": -2988.29,
            "rx": 76.2353,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "1990",
            "citation_count": 66,
            "name": 66,
            "cx": 28.5975,
            "cy": -2808.81,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "J89-2012",
            "name": "Book Reviews: Systemic Text Generation as Problem Solving",
            "publication_data": 1989,
            "citation": "???",
            "abstract": "None",
            "cx": 946.597,
            "cy": -2898.55,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "H89-2015",
            "name": "New Possibilities in Machine Translation",
            "publication_data": 1989,
            "citation": 2,
            "abstract": "There is a growing need for language translation of documents in commerce, government, science, and international organizations. At the same time, translation by computer (MT) is reaching the stage where it can deliver significant cost savings (systems are being sold in Japan that reputedly reduce the time required for translation by up to 50%). Although fully automated high-quality translation is technically not feasible today or in the near future, a number of recent theoretical developments make possible MT systens that are more powerful and effective than existing ones. These developments include: better representation techniques, a clearer understanding of semantics for translation, more complete grammars, and better generation and parsing technology. By making optimal use of existing technology, new MT projects can reach a sophisticated level of performance within a short time. This paper provides reasons for starting a new MT program and recommends the establishment of three small MT projects that address the same domain but use different theoretical frameworks.",
            "cx": 1120.6,
            "cy": -2898.55,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H89-2065",
            "name": "The Current Status of the Penman Language Generation System",
            "publication_data": 1989,
            "citation": 0,
            "abstract": "Penman is one of the largest English language generation programs in the world. Developed mainly at ISI/USC, it is the result of over 15 person-years' work, and forms the core of an investigation of the computational aspects of the theories of Systemic Functional Linguistics.",
            "cx": 1287.6,
            "cy": -2898.55,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H89-1021",
            "name": "The {P}enman Language Generation Project",
            "publication_data": 1989,
            "citation": 0,
            "abstract": "None",
            "cx": 1452.6,
            "cy": -2898.55,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1991",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -2719.07,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H90-1011",
            "name": "Performing Integrated Syntactic and Semantic Parsing Using Classification",
            "publication_data": 1990,
            "citation": 6,
            "abstract": "This paper describes a particular approach to parsing that utilizes recent advances in unification-based parsing and in classification-based knowledge representation. As unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information, they begin to share many of the properties that have been developed in KL-ONE-like knowledge representation systems. This commonality suggests that some of the classification-based representation techniques can be applied to unification-based linguistic descriptions. This merging supports the integration of semantic and syntactic information into the same system, simultaneously subject to the same types of processes, in an efficient manner. The result is expected to be more efficient parsing due to the increased organization of knowledge.",
            "cx": 965.597,
            "cy": -2808.81,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H90-1072",
            "name": "Machine Translation Again?",
            "publication_data": 1990,
            "citation": 2,
            "abstract": "Machine translation (MT) remains the paradigm task for natural language processing (NLP) since its inception in the 1950s. Unless NLP can succeed with the central task of machine translation, it cannot be considered successful as a field. We maintain that the most profitable approach to MT at the present time is an interlingual and modular one. MT is one the precious few computational tasks falling broadly within artificial intelligence (AI) that combine a fundamental intellectual research challenge with enormous proven need. To establish the latter, one only has to note that in Japan alone the current MT requirement is for 20 billion pages a year (a market of some $66 billion a year).",
            "cx": 1171.6,
            "cy": -2808.81,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1992",
            "citation_count": 27,
            "name": 27,
            "cx": 28.5975,
            "cy": -2629.33,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H91-1106",
            "name": "The {P}enman Natural Language Project Systemics-Based Machine Translation",
            "publication_data": 1991,
            "citation": 0,
            "abstract": "The development of an integrated knowledge-based machine-aided translation system based on Systemic Linguistics. Parts of the system are to function as modules to be incorporated in the MAT system being codeveloped with CMU and CRL. Our work involves the enhancement of Penman's existing parsing technology to match the level of the language generation system; the development of ancillary knowledge sources and software (such as bilingual lexicons and interlingua/transfer structures); the maintenance and continued distribution of the sentence generator; and the embedding of all these parts into the joint system.",
            "cx": 940.597,
            "cy": -2719.07,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1993",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -2539.59,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H92-1052",
            "name": "Approximating an Interlingua in a Principled Way",
            "publication_data": 1992,
            "citation": 27,
            "abstract": "We address the problem of constructing in a principled way an ontology of terms to be used in an interlingua for machine translation. Given our belief that the a true language-neutral ontology of terms can only be approached asymptotically, the construction method outlined involves a stepwise folding in of one language at a time. This is effected in three steps: first building for each language a taxonomy of the linguistic generalizations required to analyze and generate that language, then organizing the domain entities in terms of that taxonomy, and finally merging the result with the existing interlingua ontology in a well-defined way. This methodology is based not on intuitive grounds about what is and is not 'true' about the world, which is a question of language-independence, but instead on practical concerns, namely what information the analysis and generation programs require in order to perform their tasks, a question of language-neutrality. After each merging is complete, the resulting taxonomy contains, declaratively and explicitly represented, those distinctions required to control the analysis and generation of the linguistic phenomena. The paper is based on current work of the PANGLOSS MT project.",
            "cx": 393.597,
            "cy": -2629.33,
            "rx": 116.845,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "H94-1025",
            "name": "Building {J}apanese-{E}nglish Dictionary based on Ontology for Machine Translation",
            "publication_data": 1994,
            "citation": 31,
            "abstract": "This paper describes a semi-automatic method for associating a Japanese lexicon with a semantic concept taxonomy called an ontology, using a Japanese-English bilingual dictionary as a bridge. The ontology supports semantic processing in a knowledge-based machine translation system by providing a set of language-neutral symbols and semantic information. To put the ontology to practical use, lexical items of each language of interest must be linked to appropriate ontology items. The association of ontology items with lexical items of various languages is a process fraught with difficulty: since much of this work depends on the subjective decisions of human workers, large MT dictionaries tend to be subject to some dispersion and inconsistency. The problem we focus on here is how to associate concepts in the ontology with Japanese lexical entities by automatic methods, since it is too difficult to define adequately many concepts manually. We have designed three algorithms to associate a Japanese lexicon with the concepts of the ontology automatically: the equivalent-word match, the argument match, and the example match. We simulated these algorithms for 980 nouns, 860 verbs and 520 adjectives as preliminary experiments. The algorithms are found to be effective for more than 80% of the words.",
            "cx": 475.597,
            "cy": -2449.85,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "1994.amta-1.23",
            "name": "Lexicon-to-Ontology Concept Association Using a Bilingual Dictionary",
            "publication_data": 1994,
            "citation": "???",
            "abstract": "None",
            "cx": 219.597,
            "cy": -2449.85,
            "rx": 144.5,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "H92-1124",
            "name": "In-Depth Knowledge-Based Machine Translation",
            "publication_data": 1992,
            "citation": 0,
            "abstract": "The development of an integrated knowledge-based machine-aided translation system called PANGLOSS in collaboration with the Center for Machine Translation (CMT) at CMU and the Computing Research Laboratory (CRL) at New Mexico State University. The ISI part of the collaboration is focused initially on providing the system's output capabilities, primarily in English and then in other languages, including (some of) German, Chinese, and Japanese. Additional tasks are the maintenance and continued distribution of the Penman sentence generator and text planner and the development of ancillary knowledge sources and software.",
            "cx": 959.597,
            "cy": -2629.33,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1994",
            "citation_count": 82,
            "name": 82,
            "cx": 28.5975,
            "cy": -2449.85,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W93-0210",
            "name": "In Defense of Syntax: Informational, Intentional, and Rhetorical Structures in Discourse",
            "publication_data": 1993,
            "citation": "???",
            "abstract": "None",
            "cx": 932.597,
            "cy": -2539.59,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "H93-1115",
            "name": "The {P}enman Project on Knowledge-Based Machine Translation",
            "publication_data": 1993,
            "citation": 0,
            "abstract": "The joint development, together with the ULTRA project at New Mexico State University and the Center for Machine Translation at Carnegie Mellon University, of an integrated knowledge-based machine-aided translation system called PANGLOSS. The ISI-specific work includes the development of English sentence generation and sentence planning capabilities and the construction of an Ontology of concepts to act as the semantic lexicon for all modules of the system as a whole. In addition, we continue to enhance Penman's existing generation technology, to collect and develop ancillary knowledge sources and software (such as grammars or bilingual dictionaries and lexicons for German, Japanese, Spanish, and Chinese), and to maintain and distribute Penman.",
            "cx": 1089.6,
            "cy": -2539.59,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1995",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -2360.11,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W94-0328",
            "name": "Toward a Multidimensional Framework to Guide the Automated Generation of Text Types",
            "publication_data": 1994,
            "citation": 0,
            "abstract": "A central concern limiting the sophistication of text generation systems today is the ability to make appropriate choices given the bewildering number of options present during the planning and realisation processes. As illustrated in several systems [Hovy 88, Bateman & Paris 89, Paris 93], the same core communication may be realised in numerous different ways, depending (among other factors) on the nature and relation of the interlocutors, the context of the communication, the media employed, etc. The combinatoric number of possibilities of all such factors is extremely large. Since most of them are not well understood at this time, automated text generation may appear to be a hopeless endeavour.",
            "cx": 944.597,
            "cy": -2449.85,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H94-1023",
            "name": "Session 4: Machine Translation",
            "publication_data": 1994,
            "citation": 0,
            "abstract": "None",
            "cx": 1128.6,
            "cy": -2449.85,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H94-1121",
            "name": "{PANGLOSS}: Knowledge-Based Machine Translation",
            "publication_data": 1994,
            "citation": 3,
            "abstract": "The goals of the PANGLOSS project are to investigate and develop a new-generation knowledge-based interlingual machine translation system, combining symbolic and statistical techniques. The system is to translate newspaper texts in arbitrary domains (though a specific financial domain is given preference) to as high quality as possible using as little human intervention as possible.",
            "cx": 1356.6,
            "cy": -2449.85,
            "rx": 121.745,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1994.amta-1.10",
            "name": "Integrating Translations from Multiple Sources within the {PANGLOSS} Mark {III} Machine Translation System",
            "publication_data": 1994,
            "citation": "???",
            "abstract": "None",
            "cx": 1592.6,
            "cy": -2449.85,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1994.amta-1.18",
            "name": "Integrating Knowledge Bases and Statistics in {MT}",
            "publication_data": 1994,
            "citation": 48,
            "abstract": "We summarize recent machine translation (MT) research at the Information Sciences Institute of USC, and we describe its application to the development of a Japanese-English newspaper MT system. Our work aims at scaling up grammar-based, knowledge-based MT techniques. This scale-up involves the use of statistical methods, both in acquiring effective knowledge resources and in making reasonable linguistic choices in the face of knowledge gaps.",
            "cx": 717.597,
            "cy": -2449.85,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "1997.mtsummit-workshop.7",
            "name": "Improving the precision of lexicon-to-ontology alignment algorithms",
            "publication_data": 1997,
            "citation": 8,
            "abstract": "This paper describes an automatic technique for improving the association of two combined machine-readable Arabic-English lexicons with a semantic word thesaurus called WordNet (version 1.5). Our main goal is to attach individual Arabic word meanings to appropriate WN concepts. This paper describes the automatic methods used to achieve this goal for nouns. When searching WordNet for a concept, we performed name matching and definition matching simultaneously. Unfortunately, these matches provided too many suggested alignments. Our main contribution is the reduction in number of matches by applying a new filtering heuristic.",
            "cx": 219.597,
            "cy": -2180.63,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1994.amta-1.27",
            "name": "Is {MT} Research Doing Any Good?",
            "publication_data": 1994,
            "citation": "???",
            "abstract": "None",
            "cx": 1775.6,
            "cy": -2449.85,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1994.amta-1.41",
            "name": "{PANGLOSS}",
            "publication_data": 1994,
            "citation": 0,
            "abstract": "None",
            "cx": 1974.6,
            "cy": -2449.85,
            "rx": 112.36,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1996",
            "citation_count": 43,
            "name": 43,
            "cx": 28.5975,
            "cy": -2270.37,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "J95-1009",
            "name": "Book Reviews: Challenges in Natural Language Processing",
            "publication_data": 1995,
            "citation": "???",
            "abstract": "None",
            "cx": 946.597,
            "cy": -2360.11,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1997",
            "citation_count": 617,
            "name": 617,
            "cx": 28.5975,
            "cy": -2180.63,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W96-0508",
            "name": "On Lexical Aggregation and Ordering",
            "publication_data": 1996,
            "citation": 6,
            "abstract": "Aggregation is the process of removing redundant information during language generation while preserving the information to be conveyed. Aggregation is an important component of text or sentence planning. Without aggregation, automated language generation systems would not be able to p roduce f luent text f rom real -wor ld databases and knowledge bases, since information is rarely stored in computers in forms directly supporting fluent expression. Various types of aggregation (syntactic, lexical, referential) have been identified in [Hovy88, Cook84, Reinhart91, Horacek92, Dalianis&Hovy93,Wilkinson95,Dalianis95a, 95b,96a]. This paper investigates lexical aggregation, the process by which a set of items is replaced with a single new lexeme that encompasses the same meaning. We call the elements that will be aggregated the aggregands and the element (the lexeme) which is the result of the aggregation the aggregator. Lexical aggregation can be divided into two major types, bounded and unbounded. With Bounded Lexical (BL) aggregation the aggregator lexeme covers a closed set of concepts and the redundancy is obvious, the aggregated information is recoverable, and the aggregation process must be carried out. In contrast, Unbounded Lexical (UL) aggregation is carried out over an open set of aggregands and consequently the aggregated information is not recoverable and has to be licensed by other factors, such as the hearer's goals. The example in Figure 1 contains both types of aggregation, where f ight and week are the unbounded and bounded aggregators respectively.",
            "cx": 942.597,
            "cy": -2270.37,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W96-0401",
            "name": "The {H}ealth{D}oc Sentence Planner",
            "publication_data": 1996,
            "citation": 37,
            "abstract": "None",
            "cx": 1094.6,
            "cy": -2270.37,
            "rx": 57.9655,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "1996.amta-1.24",
            "name": "Panel: The limits of automation: optimists vs skeptics.",
            "publication_data": 1996,
            "citation": "???",
            "abstract": "None",
            "cx": 1251.6,
            "cy": -2270.37,
            "rx": 81.135,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1996.amta-1.31",
            "name": "{JAPANGLOSS}: using statistics to fill knowledge gaps",
            "publication_data": 1996,
            "citation": "???",
            "abstract": "None",
            "cx": 1476.6,
            "cy": -2270.37,
            "rx": 125.73,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1998",
            "citation_count": 172,
            "name": 172,
            "cx": 28.5975,
            "cy": -2090.89,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W97-0704",
            "name": "Automated Text Summarization in {SUMMARIST}",
            "publication_data": 1997,
            "citation": 367,
            "abstract": "SUMMARIST is an attempt to create a robust automated text summarization system, based on the xe2x80x98equationxe2x80x99: summarization = topic identification  interpretation  generation. Each of these stages contains several independent modules, many of them trained on large corpora of text. We describe the systemxe2x80x99s architecture and provide details of some of its modules.",
            "cx": 5208.6,
            "cy": -2180.63,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "gerber-hovy-1998-improving",
            "name": "Improving translation quality by manipulating sentence length",
            "publication_data": 1998,
            "citation": 12,
            "abstract": "Translation systems tend to have more trouble with long sentences than with short ones for a variety of reasons. When the source and target languages differ rather markedly, as do Japanese and English, this problem is reflected in lower quality output. To improve readability, we experimented with automatically splitting long sentences into shorter ones. This paper outlines the problem, describes the sentence splitting procedure and rules, and provides an evaluation of the results.",
            "cx": 5367.6,
            "cy": -2090.89,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C00-1072",
            "name": "The Automated Acquisition of Topic Signatures for Text Summarization",
            "publication_data": 2000,
            "citation": 401,
            "abstract": "In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures-sets of related words, with associated weights, organized around head topics and illustrate with signatures we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with outologies and its evaluaton on an automated text summarization system.",
            "cx": 5081.6,
            "cy": -1911.41,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "J02-4001",
            "name": "Introduction to the Special Issue on Summarization",
            "publication_data": 2002,
            "citation": 319,
            "abstract": "generation based on rhetorical structure extraction. In Proceedings of the International Conference on Computational Linguistics, Kyoto, Japan, pages 344xe2x80x93348. Otterbacher, Jahna, Dragomir R. Radev, and Airong Luo. 2002. Revisions that improve cohesion in multi-document summaries: A preliminary study. In ACL Workshop on Text Summarization, Philadelphia. Papineni, K., S. Roukos, T. Ward, and W-J. Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. Research Report RC22176, IBM. Radev, Dragomir, Simone Teufel, Horacio Saggion, Wai Lam, John Blitzer, Arda Celebi, Hong Qi, Elliott Drabek, and Danyu Liu. 2002. Evaluation of text summarization in a cross-lingual information retrieval framework. Technical Report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, June. Radev, Dragomir R., Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies. In ANLP/NAACL Workshop on Summarization, Seattle, April. Radev, Dragomir R. and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469xe2x80x93500. Rau, Lisa and Paul Jacobs. 1991. Creating segmented databases from free text for text retrieval. In Proceedings of the 14th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, New York, pages 337xe2x80x93346. Saggion, Horacio and Guy Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational Linguistics, 28(4), 497xe2x80x93526. Salton, G., A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing & Management, 33(2):193xe2x80x93207. Silber, H. Gregory and Kathleen McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4), 487xe2x80x93496. Sparck Jones, Karen. 1999. Automatic summarizing: Factors and directions. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 1xe2x80x9313. Strzalkowski, Tomek, Gees Stein, J. Wang, and Bowden Wise. 1999. A robust practical text summarizer. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 137xe2x80x93154. Teufel, Simone and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4), 409xe2x80x93445. White, Michael and Claire Cardie. 2002. Selecting sentences for multidocument summaries using randomized local search. In Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July. Association for Computational Linguistics, New Brunswick, NJ, pages 9xe2x80x9318. Witbrock, Michael and Vibhu Mittal. 1999. Ultra-summarization: A statistical approach to generating highly condensed non-extractive summaries. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, pages 315xe2x80x93316. Zechner, Klaus. 2002. Automatic summarization of open-domain multiparty dialogues in diverse genres. Computational Linguistics, 28(4), 447xe2x80x93485.",
            "cx": 5463.6,
            "cy": -1731.93,
            "rx": 115.017,
            "ry": 26.7407,
            "stroke": "black",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "A97-1042",
            "name": "Identifying Topics by Position",
            "publication_data": 1997,
            "citation": 242,
            "abstract": "This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization.",
            "cx": 4948.6,
            "cy": -2180.63,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "X98-1026",
            "name": "Automated Text Summarization and the {S}ummarist System",
            "publication_data": 1998,
            "citation": 160,
            "abstract": "This paper consists of three parts: a preliminary typology of summaries in general; a description of the current and planned modules and performance of the SUMMARIST automated multilingual text summarization system being built sat ISI, and a discussion of three methods to evaluate summaries.",
            "cx": 5041.6,
            "cy": -2090.89,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P06-2063",
            "name": "Automatic Identification of Pro and Con Reasons in Online Reviews",
            "publication_data": 2006,
            "citation": 154,
            "abstract": "In this paper, we present a system that automatically extracts the pros and cons from online reviews. Although many approaches have been developed for extracting opinions from text, our focus here is on extracting the reasons of the opinions, which may themselves be in the form of either fact or opinion. Leveraging online review sites with author-generated pros and cons, we propose a system for aligning the pros and cons to their sentences in review texts. A maximum entropy model is then trained on the resulting labeled set to subsequently extract pros and cons from online review sites that do not explicitly provide them. Our experimental results show that our resulting system identifies pros and cons with 66% precision and 76% recall.",
            "cx": 4719.6,
            "cy": -1372.97,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "zhou-etal-2006-summarizing",
            "name": "Summarizing Answers for Complicated Questions",
            "publication_data": 2006,
            "citation": 5,
            "abstract": "Recent work in several computational linguistics (CL) applications (especially question answering) has shown the value of semantics (in fact, many people argue that the current performance ceiling experienced by so many CL applications derives from their inability to perform any kind of semantic processing). But the absence of a large semantic information repository that provides representations for sentences prevents the training of statistical CL engines and thus hampers the development of such semantics-enabled applications. This talk refers to recent work in several projects that seek to annotate large volumes of text with shallower or deeper representations of some semantic phenomena. It describes one of the essential problems\u00c2\u0097creating, managing, and annotating (at large scale) the meanings of words, and outlines the Omega ontology, being built at ISI, that acts as term repository. The talk illustrates how one can proceed from words via senses to concepts, and how the annotation process can help verify good concept decisions and expose bad ones. Much of this work is performed in the context of the OntoNotes project, joint with BBN, the Universities of Colorado and Pennsylvania, and ISI, that is working to build a corpus of about 1M words (English, Chinese, and Arabic), annotated for shallow semantics, over the next few years.",
            "cx": 2058.6,
            "cy": -1372.97,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1327",
            "name": "Earlier Isn{'}t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization",
            "publication_data": 2019,
            "citation": 2,
            "abstract": "Despite the recent developments on neural summarization systems, the underlying logic behind the improvements from the systems and its corpus-dependency remains largely unexplored. Position of sentences in the original text, for example, is a well known bias for news summarization. Following in the spirit of the claim that summarization is a combination of sub-functions, we define three sub-aspects of summarization: position, importance, and diversity and conduct an extensive analysis of the biases of each sub-aspect with respect to the domain of nine different summarization corpora (e.g., news, academic papers, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system.",
            "cx": 1704.6,
            "cy": -206.35,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1997.mtsummit-tutorials.1",
            "name": "A gentle introduction to {MT}: theory and current practice",
            "publication_data": 1997,
            "citation": "???",
            "abstract": "This tutorial provides a nontechnical introduction to machine translation. It reviews the whole scope of MT, outlining briefly its history and the major application areas today, and describing the various kinds of MT techniques that have been invented{---}from direct replacement through transfer to the holy grail of interlinguas. It briefly outlines the newest statistics-based techniques and provides an introduction to the difficult questions of MT evaluation. Topics include: History and development of MT; Theoretical foundations of MT; Traditional and modern MT techniques; Newest MT research; Thorny questions of evaluating MT systems",
            "cx": 5392.6,
            "cy": -2180.63,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1999",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -2001.15,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1998.amta-tutorials.7",
            "name": "Multilingual text summarization",
            "publication_data": 1998,
            "citation": "???",
            "abstract": "None",
            "cx": 5581.6,
            "cy": -2090.89,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "1998.amta-panels.1",
            "name": "A seal of approval for {MT} systems",
            "publication_data": 1998,
            "citation": "???",
            "abstract": "None",
            "cx": 5767.6,
            "cy": -2090.89,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2000",
            "citation_count": 401,
            "name": 401,
            "cx": 28.5975,
            "cy": -1911.41,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "1999.mtsummit-1.31",
            "name": "{MT} evaluation",
            "publication_data": 1999,
            "citation": "???",
            "abstract": "This panel deals with the general topic of evaluation of machine translation systems. The first contribution sets out some recent work on creating standards for the design of evaluations. The second, by Eduard Hovy. takes up the particular issue of how metrics can be differentiated and systematized. Benjamin K. T'sou suggests that whilst men may evaluate machines, machines may also evaluate men. John S. White focuses on the question of the role of the user in evaluation design, and Yusoff Zaharin points out that circumstances and settings may have a major influence on evaluation design.",
            "cx": 5363.6,
            "cy": -2001.15,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2001",
            "citation_count": 240,
            "name": 240,
            "cx": 28.5975,
            "cy": -1821.67,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P02-1058",
            "name": "From Single to Multi-document Summarization",
            "publication_data": 2002,
            "citation": 200,
            "abstract": "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. NeATS is among the best performers in the large scale summarization evaluation DUC 2001.",
            "cx": 5245.6,
            "cy": -1731.93,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "C02-1130",
            "name": "Fine Grained Classification of Named Entities",
            "publication_data": 2002,
            "citation": 158,
            "abstract": "While Named Entity extraction is useful in many natural language applications, the coarse categories that most NE extractors work with prove insufficient for complex applications such as Question Answering and Ontology generation. We examine one coarse category of named entities, persons, and describe a method for automatically classifying person instances into eight finer-grained subcategories. We present a supervised learning method that considers the local context surrounding the entity as well as more global semantic information derived from topic signatures and WordNet. We reinforce this method with an algorithm that takes advantage of the presence of entities in multiple contexts.",
            "cx": 5026.6,
            "cy": -1731.93,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N03-1037",
            "name": "A Web-Trained Extraction Summarization System",
            "publication_data": 2003,
            "citation": 26,
            "abstract": "A serious bottleneck in the development of trainable text summarization systems is the shortage of training data. Constructing such data is a very tedious task, especially because there are in general many different correct ways to summarize a text. Fortunately we can utilize the Internet as a source of suitable training data. In this paper, we present a summarization system that uses the web as the source of training data. The procedure involves structuring the articles downloaded from various websites, building adequate corpora of (summary, text) and (extract, text) pairs, training on positive and negative data, and automatically learning to perform the task of extraction-based summarization at a level comparable to the best DUC systems.",
            "cx": 2312.6,
            "cy": -1642.19,
            "rx": 55.7232,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-2002",
            "name": "Summarizing Textual Information about Locations In a Geo-Spatial Information Display System",
            "publication_data": 2010,
            "citation": 1,
            "abstract": "This demo describes the summarization of textual material about locations in the context of a geo-spatial information display system. When the amount of associated textual data is large, it is organized and summarized before display. A hierarchical summarization framework, conditioned on the small space available for display, has been fully implemented. Snapshots of the system, with narrative descriptions, demonstrate our results.",
            "cx": 5370.6,
            "cy": -1014.01,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2000.amta-tutorials.2",
            "name": "A gentle introduction to {MT}: theory and current practice",
            "publication_data": 2000,
            "citation": "???",
            "abstract": "None",
            "cx": 5339.6,
            "cy": -1911.41,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2002",
            "citation_count": 1534,
            "name": 1534,
            "cx": 28.5975,
            "cy": -1731.93,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-1313",
            "name": "Assigning Time-Stamps To Event-Clauses",
            "publication_data": 2001,
            "citation": 116,
            "abstract": "We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans.",
            "cx": 5363.6,
            "cy": -1821.67,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "black",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "H01-1069",
            "name": "Toward Semantics-Based Answer Pinpointing",
            "publication_data": 2001,
            "citation": 124,
            "abstract": "We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.",
            "cx": 5559.6,
            "cy": -1821.67,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2003",
            "citation_count": 1349,
            "name": 1349,
            "cx": 28.5975,
            "cy": -1642.19,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W02-2108",
            "name": "Towards Emotional Variation in Speech-Based Natural Language Processing",
            "publication_data": 2002,
            "citation": 21,
            "abstract": "None",
            "cx": 5692.6,
            "cy": -1731.93,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W02-1105",
            "name": "Building Semantic/Ontological Knowledge by Text Mining",
            "publication_data": 2002,
            "citation": 2,
            "abstract": "People have long talked about having NLP systems employ semantic knowledge on a large scale. However, no-one has yet built a large ontology that was indeed practically useful for tasks such as question answering, machine translation, and information retrieval. Work on WordNet, a major contender, shows that it requires more content to realize its full potential, while efforts to use CYC show how hard it is to build general-purpose ontologies that can support NLP applications. In this talk I outline some recent efforts to automatically acquire knowledge that may be placed into terminological ontologies and used by NLP systems, and mention some problems in evaluating the quality of the results.",
            "cx": 5897.6,
            "cy": -1731.93,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W02-0406",
            "name": "Manual and automatic evaluation of summaries",
            "publication_data": 2002,
            "citation": 126,
            "abstract": "In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001). We first show the instability of the manual evaluation. Specifically, the low inter-human agreement indicates that more reference summaries are needed. To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries. The initial results provide encouraging correlations with human judgments, based on the Spearman rank-order correlation coefficient. However, relative ranking of systems needs to take into account the instability.",
            "cx": 1879.6,
            "cy": -1731.93,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W03-0510",
            "name": "The Potential and Limitations of Automatic Sentence Extraction for Summarization",
            "publication_data": 2003,
            "citation": 53,
            "abstract": "In this paper we present an empirical study of the potential and limitation of sentence extraction in text summarization. Our results show that the single document generic summarization task as defined in DUC 2001 needs to be carefully refocused as reflected in the low inter-human agreement at 100-word1 (0.40 score) and high upper bound at full text2 (0.88) summaries. For 100-word summaries, the performance upper bound, 0.65, achieved oracle extracts3. Such oracle extracts show the promise of sentence extraction algorithms; however, we first need to raise inter-human agreement to be able to achieve this performance level. We show that compression is a promising direction and that the compression ratio of summaries affects average human and system performance.",
            "cx": 1800.6,
            "cy": -1642.19,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1020",
            "name": "Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics",
            "publication_data": 2003,
            "citation": 1005,
            "abstract": "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.",
            "cx": 1600.6,
            "cy": -1642.19,
            "rx": 103.889,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "hovy-etal-2006-automated",
            "name": "Automated Summarization Evaluation with Basic Elements.",
            "publication_data": 2006,
            "citation": 143,
            "abstract": "As part of evaluating a summary automati-cally, it is usual to determine how much of the contents of one or more human-produced \u00c2\u0093ideal\u00c2\u0094 summaries it contains. Past automated methods such as ROUGE compare using fixed word ngrams, which are not ideal for a variety of reasons. In this paper we describe a framework in which summary evaluation measures can be instantiated and compared, and we implement a specific evaluation method using very small units of content, called Basic Elements that address some of the shortcomings of ngrams. This method is tested on DUC 2003, 2004, and 2005 systems and produces very good correlations with human judgments.",
            "cx": 1839.6,
            "cy": -1372.97,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P02-1006",
            "name": "Learning surface text patterns for a Question Answering System",
            "publication_data": 2002,
            "citation": 688,
            "abstract": "In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.",
            "cx": 2524.6,
            "cy": -1731.93,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "C02-1042",
            "name": "Using Knowledge to Facilitate Factoid Answer Pinpointing",
            "publication_data": 2002,
            "citation": 13,
            "abstract": "None",
            "cx": 2715.6,
            "cy": -1731.93,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P03-1001",
            "name": "Offline Strategies for Online Question Answering: Answering Questions Before They Are Asked",
            "publication_data": 2003,
            "citation": 120,
            "abstract": "Recent work in Question Answering has focused on web-based systems that extract answers using simple lexico-syntactic patterns. We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions. We evaluate our strategy on a challenging subset of questions, i.e. Who is ... questions, against a state of the art web-based Question Answering system. Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.",
            "cx": 3375.6,
            "cy": -1642.19,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "H05-1075",
            "name": "Handling Biographical Questions with Implicature",
            "publication_data": 2005,
            "citation": 3,
            "abstract": "Traditional question answering systems adopt the following framework: parsing questions, searching for relevant documents, and identifying/generating answers. However, this framework does not work well for questions with hidden assumptions and implicatures. In this paper, we describe a novel idea, a cascading guidance strategy, which can not only identify potential traps in questions but further guide the answer extraction procedure by recognizing whether there are multiple answers for a question. This is the first attempt to solve implicature problem for complex QA in a cascading fashion using N-gram language models as features. We here investigate questions with implicatures related to biography facts in a web-based QA system, Power-Bio. We compare the performances of Decision Tree, Naive Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods. The integration of the cascading guidance strategy can help extract answers for questions with implicatures and produce satisfactory results in our experiments.",
            "cx": 2643.6,
            "cy": -1462.71,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W08-0628",
            "name": "Adaptive Information Extraction for Complex Biomedical Tasks",
            "publication_data": 2008,
            "citation": 1,
            "abstract": "Biomedical information extraction tasks are often more complex and contain uncertainty at each step during problem solving processes. We present an adaptive information extraction framework and demonstrate how to explore uncertainty using feedback integration.",
            "cx": 2217.6,
            "cy": -1193.49,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I08-2124",
            "name": "Towards Automated Semantic Analysis on Biomedical Research Articles",
            "publication_data": 2008,
            "citation": 3,
            "abstract": "In this paper, we present an empirical study on adapting Conditional Random Fields (CRF) models to conduct semantic analysis on biomedical articles using active learning. We explore uncertaintybased active learning with the CRF model to dynamically select the most informative training examples. This abridges the power of the supervised methods and expensive human annotation cost.",
            "cx": 2002.6,
            "cy": -1193.49,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1162",
            "name": "Insights from Network Structure for Text Mining",
            "publication_data": 2011,
            "citation": 5,
            "abstract": "Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.",
            "cx": 4859.6,
            "cy": -924.271,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P03-2021",
            "name": "i{N}e{ATS}: Interactive Multi-Document Summarization",
            "publication_data": 2003,
            "citation": 34,
            "abstract": "We describe iNeATS -- an interactive multi-document summarization system that integrates a state-of-the-art summarization engine with an advanced user interface. Three main goals of the system are: (1) provide a user with control over the summarization process, (2) support exploration of the document set with the summary as the staring point, and (3) combine text summaries with alternative presentations such as a map-based visualization of documents.",
            "cx": 5379.6,
            "cy": -1642.19,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "hovy-etal-2002-computer",
            "name": "Computer-Aided Specification of Quality Models for Machine Translation Evaluation",
            "publication_data": 2002,
            "citation": 7,
            "abstract": "This article describes the principles and mechanism of an integrative effort in machine translation (MT) evaluation. Building upon previous standardization initiatives, above all ISO/IEC 9126, 14598 and EAGLES, we attempt to classify into a coherent taxonomy most of the characteristics, attributes and metrics that have been proposed for MT evaluation. The main articulation of this flexible framework is the link between a taxonomy that helps evaluators define a context of use for the evaluated software, and a taxonomy of the quality characteristics and associated metrics. The article explains the theoretical grounds of this articulation, along with an overview of the taxonomies in their present state, and a perspective on ongoing work in MT evaluation standardization.",
            "cx": 6130.6,
            "cy": -1731.93,
            "rx": 124.402,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P08-1119",
            "name": "Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs",
            "publication_data": 2008,
            "citation": 191,
            "abstract": "We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based extractions: popularity and productivity. Intuitively, a candidate is popular if it was discovered many times by other instances in the hyponym pattern. A candidate is productive if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies.",
            "cx": 4897.6,
            "cy": -1193.49,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D09-1099",
            "name": "Toward Completeness in Concept Extraction and Classification",
            "publication_data": 2009,
            "citation": 33,
            "abstract": "Many algorithms extract terms from text together with some kind of taxonomic classification (is-a) link. However, the general approaches used today, and specifically the methods of evaluating results, exhibit serious shortcomings. Harvesting without focusing on a specific conceptual area may deliver large numbers of terms, but they are scattered over an immense concept space, making Recall judgments impossible. Regarding Precision, simply judging the correctness of terms and their individual classification links may provide high scores, but this doesn't help with the eventual assembly of terms into a single coherent taxonomy. Furthermore, since there is no correct and complete gold standard to measure against, most work invents some ad hoc evaluation measure. We present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts 'below' a given seed term. Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet.",
            "cx": 5014.6,
            "cy": -1103.75,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1150",
            "name": "Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns",
            "publication_data": 2010,
            "citation": 48,
            "abstract": "A challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations. We propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the Web. We evaluate the performance of our algorithm on multiple semantic relations expressed using verb, noun, and verb prep lexico-syntactic patterns. Human-based evaluation shows that the accuracy of the harvested information is about 90%. We also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge.",
            "cx": 5138.6,
            "cy": -1014.01,
            "rx": 95.4188,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2004",
            "citation_count": 1232,
            "name": 1232,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W03-1209",
            "name": "Statistical {QA} - Classifier vs. Re-ranker: What{'}s the difference?",
            "publication_data": 2003,
            "citation": 39,
            "abstract": "In this paper, we show that we can obtain a good baseline performance for Question Answering (QA) by using only 4 simple features. Using these features, we contrast two approaches used for a Maximum Entropy based QA system. We view the QA problem as a classification problem and as a re-ranking problem. Our results indicate that the QA system viewed as a re-ranker clearly outperforms the QA system used as a classifier. Both systems are trained using the same data.",
            "cx": 3578.6,
            "cy": -1642.19,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1026",
            "name": "Identifying and Analyzing Judgment Opinions",
            "publication_data": 2006,
            "citation": 173,
            "abstract": "In this paper, we introduce a methodology for analyzing judgment opinions. We define a judgment opinion as consisting of a valence, a holder, and a topic. We decompose the task of opinion analysis into four parts: 1) recognizing the opinion; 2) identifying the valence; 3) identifying the holder; and 4) identifying the topic. In this paper, we address the first three parts and evaluate our methodology using both intrinsic and extrinsic measures.",
            "cx": 3793.6,
            "cy": -1372.97,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W03-1007",
            "name": "Maximum Entropy Models for {F}rame{N}et Classification",
            "publication_data": 2003,
            "citation": 61,
            "abstract": "The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence. Further we examine the use of sentence level syntactic pattern features to increase performance. We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p<0.01) of over 6%.",
            "cx": 3793.6,
            "cy": -1642.19,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W04-0832",
            "name": "Senseval automatic labeling of semantic roles using Maximum Entropy models",
            "publication_data": 2004,
            "citation": 6,
            "abstract": "As a task in SensEval-3, Automatic Labeling of Semantic Roles is to identify frame elements within a sentence and tag them with appropriate semantic roles given a sentence, a target word and its frame. We apply Maximum Entropy classification with feature sets of syntactic patterns from parse trees and officially attain 80.2% precision and 65.4% recall. When the frame element boundaries are given, the system performs 86.7% precision and 85.8% recall.",
            "cx": 3739.6,
            "cy": -1552.45,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C04-1179",
            "name": "{F}rame{N}et-based Semantic Parsing using Maximum Entropy Models",
            "publication_data": 2004,
            "citation": 14,
            "abstract": "As part of its description of lexico-semantic predicate frames or conceptual structures, the FrameNet project defines a set of semantic roles specific to the core predicate of a sentence. Recently, researchers have tried to automatically produce semantic interpretations of sentences using this information. Building on prior work, we describe a new method to perform such interpretations. We define sentence segmentation first and show how Maximum Entropy re-ranking helps achieve a level of 76.2% F-score (answer among topfive candidates) or 61.5% (correct answer).",
            "cx": 3997.6,
            "cy": -1552.45,
            "rx": 144.914,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I05-7009",
            "name": "The Omega Ontology",
            "publication_data": 2005,
            "citation": 60,
            "abstract": "We present the Omega ontology, a large terminological ontology obtained by remerging WordNet and Mikrokosmos, adding information from various other sources, and subordinating the result to a newly designed feature-oriented upper model. We explain the organizing principles of the representation used for Omega and discuss the methodology used to merge the constituent conceptual hierarchies. We survey a range of auxiliary knowledge sources (including instances, verb frame annotations, and domainspecific sub-ontologies) incorporated into the basic conceptual structure and applications that have benefited from Omega. Omega is available for browsing at http://omega.isi.edu/.",
            "cx": 3387.6,
            "cy": -1462.71,
            "rx": 74.9067,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W06-0301",
            "name": "Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text",
            "publication_data": 2006,
            "citation": 358,
            "abstract": "This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.",
            "cx": 4270.6,
            "cy": -1372.97,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P11-1147",
            "name": "Unsupervised Discovery of Domain-Specific Knowledge from Text",
            "publication_data": 2011,
            "citation": 13,
            "abstract": "Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicate-argument structures like quarterbacks throw passes to receivers. Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.",
            "cx": 4223.6,
            "cy": -924.271,
            "rx": 100.318,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W04-2709",
            "name": "Interlingual Annotation of Multilingual Text Corpora",
            "publication_data": 2004,
            "citation": 20,
            "abstract": "This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content. After presenting the background and objectives of the effort, we will go on to describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself. We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen.",
            "cx": 3027.6,
            "cy": -1552.45,
            "rx": 103.889,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W04-0701",
            "name": "Multi-Document Person Name Resolution",
            "publication_data": 2004,
            "citation": 60,
            "abstract": "Multi-document person name resolution focuses on the problem of determining if two instances with the same name and from different documents refer to the same individual. We present a two-step approach in which a Maximum Entropy model is trained to give the probability that two names refer to the same individual. We then apply a modified agglomerative clustering technique to partition the instances according to their referents.",
            "cx": 4760.6,
            "cy": -1552.45,
            "rx": 124.402,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C04-1111",
            "name": "Towards Terascale Semantic Acquisition",
            "publication_data": 2004,
            "citation": 10,
            "abstract": "None",
            "cx": 4524.6,
            "cy": -1552.45,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N03-2008",
            "name": "A Maximum Entropy Approach to {F}rame{N}et Tagging",
            "publication_data": 2003,
            "citation": 11,
            "abstract": "The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using Viterbi search to find the highest probability tag sequence for a given sentence. Further we examine the use of syntactic pattern based re-ranking to further increase performance. We analyze our strategy using both extracted and human generated syntactic features. Experiments indicate 85.7% accuracy using human annotations on a held out test set.",
            "cx": 5570.6,
            "cy": -1642.19,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W04-3256",
            "name": "Multi-Document Biography Summarization",
            "publication_data": 2004,
            "citation": "???",
            "abstract": "None",
            "cx": 2284.6,
            "cy": -1552.45,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W04-1010",
            "name": "Template-Filtered Headline Summarization",
            "publication_data": 2004,
            "citation": 12,
            "abstract": "Headline summarization is a difficult task because it requires maximizing text content in short summary length while maintaining grammaticality. This paper describes our first attempt toward solving this problem with a system that generates key headline clusters and fine-tunes them using templates.",
            "cx": 1540.6,
            "cy": -1552.45,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W06-1610",
            "name": "Re-evaluating Machine Translation Results with Paraphrase Support",
            "publication_data": 2006,
            "citation": 86,
            "abstract": "In this paper, we present ParaEval, an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations. Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching. ParaEval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments. We show that ParaEval correlates significantly better than BLEU with human assessment in measurements for both fluency and adequacy.",
            "cx": 1618.6,
            "cy": -1372.97,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1057",
            "name": "{P}ara{E}val: Using Paraphrases to Evaluate Summaries Automatically",
            "publication_data": 2006,
            "citation": 87,
            "abstract": "ParaEval is an automated evaluation method for comparing reference and peer summaries. It facilitates a tiered-comparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers. We utilize a domain-independent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation (MT). We show that the quality of ParaEval's evaluations, measured by correlating with human judgments, closely resembles that of ROUGE's.",
            "cx": 1394.6,
            "cy": -1372.97,
            "rx": 104.804,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N07-2055",
            "name": "A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation",
            "publication_data": 2007,
            "citation": 7,
            "abstract": "In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.",
            "cx": 1307.6,
            "cy": -1283.23,
            "rx": 55.7232,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1082",
            "name": "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
            "publication_data": 2017,
            "citation": 143,
            "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students{'} ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43{\\%}) and the ceiling human performance (95{\\%}). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at \\url{http://www.cs.cmu.edu/~glai1/data/race/}and the code is available at \\url{https://github.com/qizhex/RACE_AR_baselines}.",
            "cx": 1100.6,
            "cy": -385.831,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "black",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D19-1179",
            "name": "(Male, Bachelor) and (Female, {P}h.{D}) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas",
            "publication_data": 2019,
            "citation": 3,
            "abstract": "Stylistic variation in text needs to be studied with different aspects including the writer{'}s personal traits, interpersonal relations, rhetoric, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains {\\textasciitilde}41K parallel sentences (8.3K parallel stories) annotated across different personas. Each persona has different styles in conjunction: gender, age, country, political view, education, ethnic, and time-of-writing. The dataset is collected from human annotators with solid control of input denotation: not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the dataset on two interesting applications of style language, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple supervised model with our parallel text outperforms the unsupervised models using nonparallel text in style transfer. Our dataset is publicly available.",
            "cx": 3548.6,
            "cy": -206.35,
            "rx": 95.4188,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2003.mtsummit-plenaries.8",
            "name": "Holy and unholy grails",
            "publication_data": 2003,
            "citation": "???",
            "abstract": "None",
            "cx": 5732.6,
            "cy": -1642.19,
            "rx": 77.1494,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2003.mtsummit-papers.12",
            "name": "{BTL}: a hybrid model for {E}nglish-{V}ietnamese machine translation",
            "publication_data": 2003,
            "citation": "???",
            "abstract": "Machine Translation (MT) is the most interesting and difficult task which has been posed since the beginning of computer history. The highest difficulty which computers had to face with, is the built-in ambiguity of Natural Languages. Formerly, a lot of human-devised rules have been used to disambiguate those ambiguities. Building such a complete rule-set is time-consuming and labor-intensive task whilst it doesn{'}t cover all the cases. Besides, when the scale of system increases, it is very difficult to control that rule-set. In this paper, we present a new model of learning-based MT (entitled BTL: Bitext-Transfer Learning) that learns from bilingual corpus to extract disambiguating rules. This model has been experimented in English-to-Vietnamese MT system (EVT) and it gave encouraging results.",
            "cx": 5905.6,
            "cy": -1642.19,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2003.mtsummit-papers.30",
            "name": "{FEMTI}: creating and using a framework for {MT} evaluation",
            "publication_data": 2003,
            "citation": "???",
            "abstract": "This paper presents FEMTI, a web-based Framework for the Evaluation of Machine Translation in ISLE. FEMTI offers structured descriptions of potential user needs, linked to an overview of technical characteristics of MT systems. The description of possible systems is mainly articulated around the quality characteristics for software product set out in ISO/IEC standard 9126. Following the philosophy set out there and in the related 14598 series of standards, each quality characteristic bottoms out in metrics which may be applied to a particular instance of a system in order to judge how satisfactory the system is with respect to that characteristic. An evaluator can use the description of user needs to help identify the specific needs of his evaluation and the relations between them. He can then follow the pointers to system description to determine what metrics should be applied and how. In the current state of the framework, emphasis is on being exhaustive, including as much as possible of the information available in the literature on machine translation evaluation. Future work will aim at being more analytic, looking at characteristics and metrics to see how they relate to one another, validating metrics and investigating the correlation between particular metrics and human judgement.",
            "cx": 6125.6,
            "cy": -1642.19,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2005",
            "citation_count": 462,
            "name": 462,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "reeder-etal-2004-interlingual",
            "name": "Interlingual annotation for {MT} development",
            "publication_data": 2004,
            "citation": 7,
            "abstract": "MT systems that use only superficial representations, including the current generation of statistical MT systems, have been successful and useful. However, they will experience a plateau in quality, much like other {``}silver bullet{''} approaches to MT. We pursue work on the development of interlingual representations for use in symbolic or hybrid MT systems. In this paper, we describe the creation of an interlingua and the development of a corpus of semantically annotated text, to be validated in six languages and evaluated in several ways. We have established a distributed, well-functioning research methodology, designed a preliminary interlingua notation, created annotation manuals and tools, developed a test collection in six languages with associated English translations, annotated some 150 translations, and designed and applied various annotation metrics. We describe the data sets being annotated and the interlingual (IL) representation language which uses two ontologies and a systematic theta-role list. We present the annotation tools built and outline the annotation process. Following this, we describe our evaluation methodology and conclude with a summary of issues that have arisen.",
            "cx": 3254.6,
            "cy": -1552.45,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "rambow-etal-2006-parallel",
            "name": "Parallel Syntactic Annotation of Multiple Languages",
            "publication_data": 2006,
            "citation": 10,
            "abstract": "This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components.",
            "cx": 2841.6,
            "cy": -1372.97,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C04-1200",
            "name": "Determining the Sentiment of Opinions",
            "publication_data": 2004,
            "citation": 1103,
            "abstract": "Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.",
            "cx": 4305.6,
            "cy": -1552.45,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D07-1113",
            "name": "{C}rystal: Analyzing Predictive Opinions on the Web",
            "publication_data": 2007,
            "citation": 84,
            "abstract": "In this paper, we present an election prediction system (Crystal) based on web usersxe2x80x99 opinions posted on an election prediction website. Given a prediction message, Crystal first identifies which party the message predicts to win and then aggregates prediction analysis results of a large amount of opinions to project the election results. We collect past election prediction messages from the Web and automatically build a gold standard. We focus on capturing lexical patterns that people frequently use when they express their predictive opinions about a coming election. To predict election results, we apply SVM-based supervised learning. To improve performance, we propose a novel technique which generalizes n-gram feature patterns. Experimental results show that Crystal significantly outperforms several baselines as well as a non-generalized n-gram approach. Crystal predicts future elections with 81.68% accuracy.",
            "cx": 3189.6,
            "cy": -1283.23,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D14-1053",
            "name": "Sentiment Analysis on the People{'}s Daily",
            "publication_data": 2014,
            "citation": 11,
            "abstract": "We propose a semi-supervised bootstrapping algorithm for analyzing Chinaxe2x80x99s foreign relations from the Peoplexe2x80x99s Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.",
            "cx": 4386.6,
            "cy": -655.051,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2021.acl-long.185",
            "name": "Style is {NOT} a single variable: Case Studies for Cross-Stylistic Language Understanding",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Every natural text is written in some style. Style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. One cannot form a complete understanding of a text without considering these factors. The factors combine and co-vary in complex ways to form styles. Studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding. This paper provides the benchmark corpus (XSLUE) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation. The benchmark contains text in 15 different styles under the proposed four theoretical groupings: figurative, personal, affective, and interpersonal groups. For valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text. Using XSLUE, we propose three interesting cross-style applications in classification, correlation, and generation. First, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers. Second, our study shows that some styles are highly dependent on each other in human-written text. Finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for cross-style research. The preprocessed datasets and code are publicly available.",
            "cx": 3548.6,
            "cy": -26.8701,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N06-2015",
            "name": "{O}nto{N}otes: The 90{\\\\%} Solution",
            "publication_data": 2006,
            "citation": 537,
            "abstract": "We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.",
            "cx": 3061.6,
            "cy": -1372.97,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 1603,
            "name": 1603,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W05-1520",
            "name": "Statistical Shallow Semantic Parsing despite Little Training Data",
            "publication_data": 2005,
            "citation": 15,
            "abstract": "Natural language understanding is an essential module in any dialogue system. To obtain satisfactory performance levels, a dialogue system needs a semantic parser/natural language understanding system (NLU) that produces accurate and detailed dialogue oriented semantic output. Recently, a number of semantic parsers trained using either the FrameNet (Baker et al., 1998) or the Prop-Bank (Kingsbury et al., 2002) have been reported. Despite their reasonable performances on general tasks, these parsers do not work so well in specific domains. Also, where these general purpose parsers tend to provide case-frame structures, that include the standard core case roles (Agent, Patient, Instrument, etc.), dialogue oriented domains tend to require additional information about addressees, modality, speech acts, etc. Where general-purpose resources such as PropBank and Framenet provide invaluable training data for general case, it tends to be a problem to obtain enough training data in a specific dialogue oriented domain.",
            "cx": 3656.6,
            "cy": -1462.71,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "hartholt-etal-2008-common",
            "name": "A Common Ground for Virtual Humans: Using an Ontology in a Natural Language Oriented Virtual Human Architecture",
            "publication_data": 2008,
            "citation": 20,
            "abstract": "When dealing with large, distributed systems that use state-of-the-art components, individual components are usually developed in parallel. As development continues, the decoupling invariably leads to a mismatch between how these components internally represent concepts and how they communicate these representations to other components: representations can get out of synch, contain localized errors, or become manageable only by a small group of experts for each module. In this paper, we describe the use of an ontology as part of a complex distributed virtual human architecture in order to enable better communication between modules while improving the overall flexibility needed to change or extend the system. We focus on the natural language understanding capabilities of this architecture and the relationship between language and concepts within the entire system in general and the ontology in particular.",
            "cx": 3661.6,
            "cy": -1193.49,
            "rx": 55.3091,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P05-1037",
            "name": "Digesting Virtual {``}Geek{''} Culture: The Summarization of Technical {I}nternet Relay Chats",
            "publication_data": 2005,
            "citation": 57,
            "abstract": "This paper describes a summarization system for technical chats and emails on the Linux kernel. To reflect the complexity and sophistication of the discussions, they are clustered according to subtopic structure on the sub-message level, and immediate responding pairs are identified through machine learning methods. A resulting summary consists of one or more mini-summaries, each on a subtopic from the discussion.",
            "cx": 5370.6,
            "cy": -1462.71,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "H05-2003",
            "name": "{C}lassummary: Introducing Discussion Summarization to Online Classrooms",
            "publication_data": 2005,
            "citation": 1,
            "abstract": "This paper describes a novel summarization system, Classummary, for interactive online classroom discussions. This system is originally designed for Open Source Software (OSS) development forums. However, this new application provides valuable feedback on designing summarization systems and applying them to everyday use, in addition to the traditional natural language processing evaluation methods. In our demonstration at HLT, new users will be able to direct this summarizer themselves.",
            "cx": 5587.6,
            "cy": -1462.71,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N06-1027",
            "name": "Learning to Detect Conversation Focus of Threaded Discussions",
            "publication_data": 2006,
            "citation": 50,
            "abstract": "In this paper we present a novel feature-enriched approach that learns to detect the conversation focus of threaded discussions by combining NLP analysis and IR techniques. Using the graph-based algorithm HITS, we integrate different features such as lexical similarity, poster trustworthiness, and speech act analysis of human conversations with feature-oriented link generation functions. It is the first quantitative study to analyze human conversation focus in the context of online discussions that takes into account heterogeneous sources of evidence. Experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system.",
            "cx": 5370.6,
            "cy": -1372.97,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P05-1077",
            "name": "Randomized Algorithms and {NLP}: Using Locality Sensitive Hash Functions for High Speed Noun Clustering",
            "publication_data": 2005,
            "citation": 187,
            "abstract": "In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed.",
            "cx": 5828.6,
            "cy": -1462.71,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "I08-1048",
            "name": "Learning a Stopping Criterion for Active Learning for Word Sense Disambiguation and Text Classification",
            "publication_data": 2008,
            "citation": 32,
            "abstract": "In this paper, we address the problem of knowing when to stop the process of active learning. We propose a new statistical learning approach, called minimum expected error strategy, to defining a stopping criterion through estimation of the classifierxe2x80x99s expected error on future unlabeled examples in the active learning process. In experiments on active learning for word sense disambiguation and text classification tasks, experimental results show that the new proposed stopping criterion can reduce approximately 50% human labeling costs in word sense disambiguation with degradation of 0.5% average accuracy, and approximately 90% costs in text classification with degradation of 2% average accuracy.",
            "cx": 3310.6,
            "cy": -1193.49,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I05-2011",
            "name": "Automatic Detection of Opinion Bearing Words and Sentences",
            "publication_data": 2005,
            "citation": 135,
            "abstract": "We describe a sentence-level opinion detection system. We first define what an opinion means in our research and introduce an effective method for obtaining opinion-bearing and nonopinion-bearing words. Then we describe recognizing opinion-bearing sentences using these words We test the system on 3 different test sets: MPQA data, an internal corpus, and the TREC2003 Novelty track data. We show that our automatic method for obtaining opinion-bearing words can be used effectively to identify opinion-bearing sentences.",
            "cx": 4783.6,
            "cy": -1462.71,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2005.sigdial-1.25",
            "name": "Dealing with Doctors: A Virtual Human for Non-team Interaction",
            "publication_data": 2005,
            "citation": 4,
            "abstract": "None",
            "cx": 6037.6,
            "cy": -1462.71,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2007",
            "citation_count": 379,
            "name": 379,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D07-1082",
            "name": "Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem",
            "publication_data": 2007,
            "citation": 129,
            "abstract": "In this paper, we analyze the effect of resampling techniques, including undersampling and over-sampling used in active learning for word sense disambiguation (WSD). Experimental results show that under-sampling causes negative effects on active learning, but over-sampling is a relatively good choice. To alleviate the withinclass imbalance problem of over-sampling, we propose a bootstrap-based oversampling (BootOS) method that works better than ordinary over-sampling in active learning for WSD. Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning. According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions.",
            "cx": 2952.6,
            "cy": -1283.23,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "O08-6002",
            "name": "Corpus Cleanup of Mistaken Agreement Using Word Sense Disambiguation",
            "publication_data": 2008,
            "citation": 0,
            "abstract": "Word sense annotated corpora are useful resources for many text mining applications. Such corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, nobody has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective in identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% of the remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora.",
            "cx": 2820.6,
            "cy": -1193.49,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C08-1133",
            "name": "{O}nto{N}otes: Corpus Cleanup of Mistaken Agreement Using Word Sense Disambiguation",
            "publication_data": 2008,
            "citation": 5,
            "abstract": "Annotated corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, no-one has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective on identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora.",
            "cx": 2577.6,
            "cy": -1193.49,
            "rx": 130.215,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C08-1142",
            "name": "Multi-Criteria-Based Strategy to Stop Active Learning for Data Annotation",
            "publication_data": 2008,
            "citation": 33,
            "abstract": "In this paper, we address the issue of deciding when to stop active learning for building a labeled training corpus. Firstly, this paper presents a new stopping criterion, classification-change, which considers the potential ability of each unlabeled example on changing decision boundaries. Secondly, a multi-criteria-based combination strategy is proposed to solve the problem of predefining an appropriate threshold for each confidence-based stopping criterion, such as max-confidence, min-error, and overall-uncertainty. Finally, we examine the effectiveness of these stopping criteria on uncertainty sampling and heterogeneous uncertainty sampling for active learning. Experimental results show that these stopping criteria work well on evaluation data sets, and the combination strategies outperform individual criteria.",
            "cx": 3068.6,
            "cy": -1193.49,
            "rx": 135.115,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1144",
            "name": "Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information",
            "publication_data": 2010,
            "citation": 19,
            "abstract": "This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.",
            "cx": 3018.6,
            "cy": -1014.01,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-2005",
            "name": "An Extension of {BLANC} to System Mentions",
            "publication_data": 2014,
            "citation": 14,
            "abstract": "BLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions. This paper extends the original BLANC (xe2x80x9cBLANC-goldxe2x80x9d henceforth) to system mentions, removing the gold mention assumption. The proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data.",
            "cx": 2232.6,
            "cy": -655.051,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C14-1123",
            "name": "Unsupervised Word Sense Induction using Distributional Statistics",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "Word sense induction is an unsupervised task to find and characterize different senses of polysemous words. This work investigates two unsupervised approaches that focus on using distributional word statistics to cluster the contextual information of the target words using two different algorithms involving latent dirichlet allocation and spectral clustering. Using a large corpus for achieving this task, we quantitatively analyze our clusters on the Semeval-2010 dataset and also perform a qualitative analysis of our induced senses. Our results indicate that our methods successfully characterized the senses of the target words and were also able to find unconventional senses for those words.",
            "cx": 1989.6,
            "cy": -655.051,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N16-1116",
            "name": "Unsupervised Ranking Model for Entity Coreference Resolution",
            "publication_data": 2016,
            "citation": 5,
            "abstract": "Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%.",
            "cx": 4373.6,
            "cy": -475.571,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S17-1025",
            "name": "Embedded Semantic Lexicon Induction with Joint Global and Local Optimization",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "Creating annotated frame lexicons such as PropBank and FrameNet is expensive and labor intensive. We present a method to induce an embedded frame lexicon in an minimally supervised fashion using nothing more than unlabeled predicate-argument word pairs. We hypothesize that aggregating such pair selectional preferences across training leads us to a global understanding that captures predicate-argument frame structure. Our approach revolves around a novel integration between a predictive embedding model and an Indian Buffet Process posterior regularizer. We show, through our experimental evaluation, that we outperform baselines on two tasks and can learn an embedded frame lexicon that is able to capture some interesting generalities in relation to hand-crafted semantic frames.",
            "cx": 1581.6,
            "cy": -385.831,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2006.amta-tutorials.1",
            "name": "A Gentle Introduction to Ontologies",
            "publication_data": 2006,
            "citation": "???",
            "abstract": "None",
            "cx": 5550.6,
            "cy": -1372.97,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2008",
            "citation_count": 285,
            "name": 285,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P07-1129",
            "name": "Topic Analysis for Psychiatric Document Retrieval",
            "publication_data": 2007,
            "citation": 3,
            "abstract": "Psychiatric document retrieval attempts to help people to efficiently and effectively locate the consultation documents relevant to their depressive problems. Individuals can understand how to alleviate their symptoms according to recommendations in the relevant documents. This work proposes the use of high-level topic information extracted from consultation documents to improve the precision of retrieval results. The topic information adopted herein includes negative life events, depressive symptoms and semantic relations between symptoms, which are beneficial for better understanding of users' queries. Experimental results show that the proposed approach achieves higher precision than the word-based retrieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone.",
            "cx": 5362.6,
            "cy": -1283.23,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N07-1071",
            "name": "{ISP}: Learning Inferential Selectional Preferences",
            "publication_data": 2007,
            "citation": 87,
            "abstract": "Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness.",
            "cx": 5553.6,
            "cy": -1283.23,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D07-1017",
            "name": "{LEDIR}: An Unsupervised Algorithm for Learning Directionality of Inference Rules",
            "publication_data": 2007,
            "citation": 49,
            "abstract": "Semantic inference is a core component of many natural language applications. In response, several researchers have developed algorithms for automatically learning inference rules from textual corpora. However, these rules are often either imprecise or underspecified in directionality. In this paper we propose an algorithm called LEDIR that filters incorrect inference rules and identifies the directionality of correct ones. Based on an extension to Harrisxe2x80x99s distributional hypothesis, we use selectional preferences to gather evidence of inference directionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines.",
            "cx": 5746.6,
            "cy": -1283.23,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D07-1088",
            "name": "Extracting Data Records from Unstructured Biomedical Full Text",
            "publication_data": 2007,
            "citation": 20,
            "abstract": "In this paper, we address the problem of extracting data records and their attributes from unstructured biomedical full text. There has been little effort reported on this in the research community. We argue that semantics is important for record extraction or finer-grained language processing tasks. We derive a data record template including semantic language models from unstructured text and represent them with a discourse level Conditional Random Fields (CRF) model. We evaluate the approach from the perspective of Information Extraction and achieve significant improvements on system performance compared with other baseline systems.",
            "cx": 2174.6,
            "cy": -1283.23,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2007.mtsummit-aptme.3",
            "name": "Investigating why {BLEU} penalizes non-statistical systems",
            "publication_data": 2007,
            "citation": "???",
            "abstract": "None",
            "cx": 5962.6,
            "cy": -1283.23,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2009",
            "citation_count": 33,
            "name": 33,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N10-1087",
            "name": "Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds",
            "publication_data": 2010,
            "citation": 35,
            "abstract": "Open-class semantic lexicon induction is of great interest for current knowledge harvesting algorithms. We propose a general framework that uses patterns in bootstrapping fashion to learn open-class semantic lexicons for different kinds of relations. These patterns require seeds. To estimate the goodness (the potential yield) of new seeds, we introduce a regression model that considers the connectivity behavior of the seed during bootstrapping. The generalized regression model is evaluated on six different kinds of relations with over 10000 different seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefficient with 15% error rate for any of the patterns when predicting the goodness of seeds.",
            "cx": 4755.6,
            "cy": -1014.01,
            "rx": 76.2353,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D10-1108",
            "name": "A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web",
            "publication_data": 2010,
            "citation": 120,
            "abstract": "Although many algorithms have been developed to harvest lexical resources, few organize the mined terms into taxonomies. We propose (1) a semi-supervised algorithm that uses a root concept, a basic level concept, and recursive surface patterns to learn automatically from the Web hyponym-hypernym pairs subordinated to the root; (2) a Web based concept positioning procedure to validate the learned pairs' is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested.",
            "cx": 4956.6,
            "cy": -1014.01,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D14-1214",
            "name": "Major Life Event Extraction from {T}witter based on Congratulations/Condolences Speech Acts",
            "publication_data": 2014,
            "citation": 53,
            "abstract": "Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime. Currently, the majority of personal news and life events are published in a textual format, motivating information extraction systems that can provide a structured representations of major life events (weddings, graduation, etc. . . ). This paper demonstrates the feasibility of accurately extracting major life events. Our system extracts a fine-grained description of usersxe2x80x99 life events based on their published tweets. We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications, for example realtime friend recommendation.",
            "cx": 4738.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W12-1905",
            "name": "Exploiting Partial Annotations with {EM} Training",
            "publication_data": 2012,
            "citation": 4,
            "abstract": "For many NLP tasks, EM-trained HMMs are the common models. However, in order to escape local maxima and find the best model, we need to start with a good initial model. Researchers suggested repeated random restarts or constraints that guide the model evolution. Neither approach is ideal. Restarts are time-intensive, and most constraint-based approaches require serious re-engineering or external solvers. In this paper we measure the effectiveness of very limited initial constraints: specifically, annotations of a small number of words in the training data. We vary the amount and distribution of initial partial annotations, and compare the results to unsupervised and supervised approaches. We find that partial annotations improve accuracy and can reduce the need for random restarts, which speeds up training time considerably.",
            "cx": 2533.6,
            "cy": -834.531,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 531,
            "name": 531,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2011",
            "citation_count": 116,
            "name": 116,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-3401",
            "name": "Distributional Semantics and the Lexicon",
            "publication_data": 2010,
            "citation": 0,
            "abstract": "The lexicons used in computational linguistics systems contain morphological, syntactic, and occasionally also some semantic information (such as definitions, pointers to an ontology, verb frame filler preferences, etc.). But the human cognitive lexicon contains a great deal more, crucially, expectations about how a word tends to combine with others: not just general information-extraction-like patterns, but specific instantial expectations. Such information is very useful when it comes to listening in bad aural conditions and reading texts in which background information is taken for granted; without such specific expectation, one would be hard-pressed (and computers are completely unable) to form coherent and richly connected multi-sentence interpretations. Over the past few years, NLP work has increasingly treated topic signature word distributions (also called xe2x80x98context vectorsxe2x80x99, xe2x80x98topic modelsxe2x80x99, etc.) as a de facto replacement for semantics. Whether the task is wordsense disambiguation, certain forms of textual entailment, information extraction, paraphrase learning, and so on, it turns out to be very useful to consider a word(sense) as being defined by the distribution of word(senses) that regularly accompany it (in the classic words of Firth, xe2x80x9cyou shall know a word by the company it keepsxe2x80x9d). And this is true not only for individual wordsenses, but also for larger units such as topics: the product of LDA and similar topic characterization engines is similar. In this talk I argue for a new kind of semantics, which is being called Distributional Semantics. It combines traditional symbolic logicbased semantics with (computation-based) statistical word distribution information. The core resource is a single lexico-semantic lexicon that can be used for a variety of tasks, provided that it is reformulated accordingly. I show how to define such a semantics, how to build the appropriate lexicon, how to format it, and how to use it for various tasks. The talk pulls together a wide range of related topics, including Pantel-style resources like DIRT, inferences / expectations such as those used in Schank-style expectation-based parsing and expectation-driven NLU, PropBank-style word valence lexical items, and the treatment of negation and modalities. I conclude by arguing that the human cognitive lexicon has to have the same kinds of properties as the Distributional Semantics lexicon, given the ways people do things with words.",
            "cx": 5616.6,
            "cy": -1014.01,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-2111",
            "name": "Injecting Linguistics into {NLP} through Annotation",
            "publication_data": 2010,
            "citation": 0,
            "abstract": "Over the past 20 years, the size of the L in Computatioonal Linguistics has been shrinking relative to the size of the C. The result is that we are incresingly becoming a community of uniformed but sophisticated engineers, applying to problems very complex machine learning techninques that use very simple (simplistic?) analyses/theories. (Try finding a theoretical account of subjectivity, opinion, entailment, or inference in publications surrounding the associated competitions of the past few years.)",
            "cx": 5836.6,
            "cy": -1014.01,
            "rx": 91.4341,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-0903",
            "name": "Semantic Enrichment of Text with Background Knowledge",
            "publication_data": 2010,
            "citation": 14,
            "abstract": "Texts are replete with gaps, information omitted since authors assume a certain amount of background knowledge. We describe the kind of information (the formalism and methods to derive the content) useful for automated filling of such gaps. We describe a stepwise procedure with a detailed example.",
            "cx": 4195.6,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "S10-1049",
            "name": "{ISI}: Automatic Classification of Relations Between Nominals Using a Maximum Entropy Classifier",
            "publication_data": 2010,
            "citation": 7,
            "abstract": "The automatic interpretation of semantic relations between nominals is an important subproblem within natural language understanding applications and is an area of increasing interest. In this paper, we present the system we used to participate in the SemEval 2010 Task 8 Multi-Way Classification of Semantic Relations between Pairs of Nominals. Our system, based upon a Maximum Entropy classifier trained using a large number of boolean features, received the third highest score.",
            "cx": 6030.6,
            "cy": -1014.01,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-5004",
            "name": "Annotation",
            "publication_data": 2010,
            "citation": 172,
            "abstract": "None",
            "cx": 3531.6,
            "cy": -1014.01,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N13-1132",
            "name": "Learning Whom to Trust with {MACE}",
            "publication_data": 2013,
            "citation": 96,
            "abstract": "Non-expert annotation services like Amazonxe2x80x99s Mechanical Turk (AMT) are cheap and fast ways to evaluate systems and provide categorical annotations for training data. Unfortunately, some annotators choose bad labels in order to maximize their pay. Manual identification is tedious, so we experiment with an item-response model. It learns in an unsupervised fashion to a) identify which annotators are trustworthy and b) predict the correct underlying labels. We match performance of more complex state-of-the-art systems and perform well even under adversarial conditions. We show considerable improvements over standard baselines, both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download 1 .",
            "cx": 3550.6,
            "cy": -744.791,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1070",
            "name": "A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation",
            "publication_data": 2010,
            "citation": 49,
            "abstract": "The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.",
            "cx": 3344.6,
            "cy": -1014.01,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1116",
            "name": "A Fast, Accurate, Non-Projective, Semantically-Enriched Parser",
            "publication_data": 2011,
            "citation": 55,
            "abstract": "Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.",
            "cx": 3189.6,
            "cy": -924.271,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1037",
            "name": "Automatic Interpretation of the {E}nglish Possessive",
            "publication_data": 2013,
            "citation": 4,
            "abstract": "The English xe2x80x99s possessive construction occurs frequently in text and can encode several different semantic relations; however, it has received limited attention from the computational linguistics community. This paper describes the creation of a semantic relation inventory covering the use of xe2x80x99s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of possessives annotated according to the relations, and an accurate automatic annotation system for labeling new examples. Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic classification system, which achieves 87.4% accuracy in our classification experiment, and our annotation data are publicly available.",
            "cx": 3350.6,
            "cy": -744.791,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W13-1203",
            "name": "Events are Not Simple: Identity, Non-Identity, and Quasi-Identity",
            "publication_data": 2013,
            "citation": 25,
            "abstract": "Despite considerable theoretical and computational work on coreference, deciding when two entities or events are identical is very difficult. In a project to build corpora containing coreference links between events, we have identified three levels of event identity (full, partial, and none). Event coreference annotation on two corpora was performed to validate the findings.",
            "cx": 2759.6,
            "cy": -744.791,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "recasens-etal-2010-typology",
            "name": "A Typology of Near-Identity Relations for Coreference ({NIDENT})",
            "publication_data": 2010,
            "citation": 21,
            "abstract": "The task of coreference resolution requires people or systems to decide when two referring expressions refer to the 'same' entity or event. In real text, this is often a difficult decision because identity is never adequately defined, leading to contradictory treatment of cases in previous work. This paper introduces the concept of 'near-identity', a middle ground category between identity and non-identity, to handle such cases systematically. We present a typology of Near-Identity Relations (NIDENT) that includes fifteen types\u00e2\u0080\u0095grouped under four main families\u00e2\u0080\u0095that capture a wide range of ways in which (near-)coreference relations hold between discourse entities. We validate the theoretical model by annotating a small sample of real data and showing that inter-annotator agreement is high enough for stability (K=0.58, and up to K=0.65 and K=0.84 when leaving out one and two outliers, respectively). This work enables subsequent creation of the first internally consistent language resource of this type through larger annotation efforts.",
            "cx": 2148.6,
            "cy": -1014.01,
            "rx": 59.2941,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D18-1154",
            "name": "Automatic Event Salience Identification",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "Identifying the salience (i.e. importance) of discourse units is an important task in language understanding. While events play important roles in text documents, little research exists on analyzing their saliency status. This paper empirically studies Event Salience and proposes two salience detection models based on discourse relations. The first is a feature based salience model that incorporates cohesion among discourse units. The second is a neural model that captures more complex interactions between discourse units. In our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).",
            "cx": 2440.6,
            "cy": -296.09,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C10-2052",
            "name": "What{'}s in a Preposition? Dimensions of Sense Disambiguation for an Interesting Word Class",
            "publication_data": 2010,
            "citation": 26,
            "abstract": "Choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments. We explore this idea for prepositions, an often overlooked word class. We examine the parameters that must be considered in preposition disambiguation, namely context, features, and granularity. Doing so delivers an increased performance that significantly improves over two state-of-the-art systems, and shows potential for improving other word sense disambiguation tasks. We report accuracies of 91.8% and 84.8% for coarse and fine-grained preposition sense disambiguation, respectively.",
            "cx": 2675.6,
            "cy": -1014.01,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-2056",
            "name": "Models and Training for Unsupervised Preposition Sense Disambiguation",
            "publication_data": 2011,
            "citation": 8,
            "abstract": "We present a preliminary study on unsu-pervised preposition sense disambiguation (PSD), comparing different models and training techniques (EM, MAP-EM with L0 norm, Bayesian inference using Gibbs sampling). To our knowledge, this is the first attempt at un-supervised preposition sense disambiguation. Our best accuracy reaches 56%, a significant improvement (at p <.001) of 16% over the most-frequent-sense baseline.",
            "cx": 2675.6,
            "cy": -924.271,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C10-2113",
            "name": "Filling Knowledge Gaps in Text for Machine Reading",
            "publication_data": 2010,
            "citation": 19,
            "abstract": "Texts are replete with gaps, information omitted since authors assume a certain amount of background knowledge. We define the process of enrichment that fills these gaps. We describe how enrichment can be performed using a Background Knowledge Base built from a large corpus. We evaluate the effectiveness of various openly available background knowledge bases and we identify the kind of information necessary for enrichment.",
            "cx": 6220.6,
            "cy": -1014.01,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 84,
            "name": 84,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W11-3701",
            "name": "Invited Keynote: What are Subjectivity, Sentiment, and Affect?",
            "publication_data": 2011,
            "citation": "???",
            "abstract": "None",
            "cx": 5172.6,
            "cy": -924.271,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W11-0704",
            "name": "Contextual Bearing on Linguistic Variation in Social Media",
            "publication_data": 2011,
            "citation": "???",
            "abstract": "None",
            "cx": 5378.6,
            "cy": -924.271,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W11-0206",
            "name": "The Role of Information Extraction in the Design of a Document Triage Application for Biocuration",
            "publication_data": 2011,
            "citation": 4,
            "abstract": "Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features. This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag-of-words models. Our classifier includes lexico-syntactic patterns and more-complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. Our experimental results show that the IE-based features can improve performance over unigram and bigram features alone. We present intrinsic evaluation results of full-text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories. We also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge curation at MGI.",
            "cx": 5578.6,
            "cy": -924.271,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W11-0102",
            "name": "A New Semantics: Merging Propositional and Distributional Information",
            "publication_data": 2011,
            "citation": 2,
            "abstract": "Despite hundreds of years of study on semantics, theories and representations of semantic content---the actual meaning of the symbols used in semantic propositions---remain impoverished. The traditional extensional and intensional models of semantics are difficult to actually flesh out in practice, and no large-scale models of this kind exist. Recently, researchers in Natural Language Processing (NLP) have increasingly treated topic signature word distributions (also called 'context vectors', 'topic models', 'language models', etc.) as a de facto placeholder for semantics at various levels of granularity. This talk argues for a new kind of semantics that combines traditional symbolic logic-based proposition-style semantics (of the kind used in older NLP) with (computation-based) statistical word distribution information (what is being called Distributional Semantics in modern NLP). The core resource is a single lexico-semantic 'lexicon' that can be used for a variety of tasks. I show how to define such a lexicon, how to build and format it, and how to use it for various tasks. Combining the two views of semantics opens many fascinating questions that beg study, including the operation of logical operators such as negation and modalities over word(sense) distributions, the nature of ontological facets required to define concepts, and the action of compositionality over statistical concepts.",
            "cx": 5740.6,
            "cy": -924.271,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W11-0143",
            "name": "Granularity in Natural Language Discourse",
            "publication_data": 2011,
            "citation": 16,
            "abstract": "This paper discusses the phenomenon of granularity in natural language1. By 'granularity' we mean the level of detail of description of an event or object. Humans can seamlessly shift their granularity perspective while reading or understanding a text. To emulate this mechanism, we describe a set of features that identify the levels of granularity in text, and empirically verify this feature set using a human annotation study for granularity identification. This theory is the foundation for any system that can learn the (global) behavior of event descriptions from (local) behavior descriptions. This is the first research initiative, to our knowledge, for identifying granularity shifts in natural language descriptions.",
            "cx": 5913.6,
            "cy": -924.271,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-2096",
            "name": "An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques",
            "publication_data": 2011,
            "citation": 13,
            "abstract": "Paraphrase generation is an important task that has received a great deal of interest recently. Proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of NLP tools to more complex approaches that rely on numerous language-dependent resources. Despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches. This paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses. Our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy.",
            "cx": 6090.6,
            "cy": -924.271,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W13-3203",
            "name": "A Structured Distributional Semantic Model : Integrating Structure with Semantics",
            "publication_data": 2013,
            "citation": 7,
            "abstract": "In this paper we present a novel approach (SDSM) that incorporates structure in distributional semantics. SDSM represents meaning as relation specific distributions over syntactic neighborhoods. We empirically show that the model can effectively represent the semantics of single words and provides significant advantages when dealing with phrasal units that involve word composition. In particular, we demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches for composing distributional semantic representations on an artificial task of verb sense disambiguation and a real-world application of judging event coreference.",
            "cx": 2996.6,
            "cy": -744.791,
            "rx": 59.2941,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W13-0907",
            "name": "Identifying Metaphorical Word Use with Tree Kernels",
            "publication_data": 2013,
            "citation": 34,
            "abstract": "A metaphor is a figure of speech that refers to one concept in terms of another, as in xe2x80x9cHe is such a sweet personxe2x80x9d. Metaphors are ubiquitous and they present NLP with a range of challenges for WSD, IE, etc. Identifying metaphors is thus an important step in language understanding. However, since almost any word can serve as a metaphor, they are impossible to list. To identify metaphorical use, we assume that it results in unusual semantic patterns between the metaphor and its dependencies. To identify these cases, we use SVMs with tree-kernels on a balanced corpus of 3872 instances, created by bootstrapping from available metaphor lists. 1 We outperform two baselines, a sequential and a vectorbased approach, and achieve an F1-score of 0.75.",
            "cx": 3776.6,
            "cy": -744.791,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-2083",
            "name": "A Structured Distributional Semantic Model for Event Co-reference",
            "publication_data": 2013,
            "citation": 13,
            "abstract": "In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.",
            "cx": 3133.6,
            "cy": -744.791,
            "rx": 59.2941,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1144",
            "name": "A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations",
            "publication_data": 2013,
            "citation": 27,
            "abstract": "In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.",
            "cx": 3956.6,
            "cy": -744.791,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "liu-etal-2014-supervised",
            "name": "Supervised Within-Document Event Coreference using Information Propagation",
            "publication_data": 2014,
            "citation": 22,
            "abstract": "Event coreference is an important task for full text analysis. However, previous work uses a variety of approaches, sources and evaluation, making the literature confusing and the results incommensurate. We provide a description of the differences to facilitate future research. Second, we present a supervised method for event coreference resolution that uses a rich feature set and propagates information alternatively between events and their arguments, adapting appropriately for each type of argument.",
            "cx": 2648.6,
            "cy": -655.051,
            "rx": 121.745,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "araki-etal-2014-detecting",
            "name": "Detecting Subevent Structure for Event Coreference Resolution",
            "publication_data": 2014,
            "citation": 16,
            "abstract": "In the task of event coreference resolution, recent work has shown the need to perform not only full coreference but also partial coreference of events. We show that subevents can form a particular hierarchical event structure. This paper examines a novel two-stage approach to finding and improving subevent structures. First, we introduce a multiclass logistic regression model that can detect subevent relations in addition to full coreference. Second, we propose a method to improve subevent structure based on subevent clusters detected by the model. Using a corpus in the Intelligence Community domain, we show that the method achieves over 3.2 BLANC F1 gain in detecting subevent relations against the logistic regression model.",
            "cx": 2944.6,
            "cy": -655.051,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C14-1066",
            "name": "Inducing Latent Semantic Relations for Structured Distributional Semantics",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "Structured distributional semantic models aim to improve upon simple vector space models of semantics by hypothesizing that the meaning of a word is captured more effectively through its relational xe2x80x94 rather than its raw distributional xe2x80x94 signature. In accordance, they extend the vector space paradigm by structuring elements with relational information that decompose distributional signatures over discrete relation dimensions. However, the number and nature of these relations remains an open research question, with most previous work in the literature employing syntactic dependencies as surrogates for truly semantic relations. In this paper we propose a novel structured distributional semantic model with latent relation dimensions, and instantiate it using latent relational analysis. Evaluation of our model yields results that significantly outperform several other distributional approaches on two semantic tasks and performs competitively on a third relation classification task.",
            "cx": 3127.6,
            "cy": -655.051,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-0802",
            "name": "Word Sense Disambiguation via {P}rop{S}tore and {O}nto{N}otes for Event Mention Detection",
            "publication_data": 2015,
            "citation": 5,
            "abstract": "In this paper, we propose a novel approach for Word Sense Disambiguation (WSD) of verbs that can be applied directly in the event mention detection task to classify event types. By using the PropStore, a database of relations between words, our approach disambiguates senses of verbs by utilizing the information of verbs that appear in similar syntactic contexts. Importantly, the resource our approach requires is only a word sense dictionary, without any annotated sentences or structures and relations between different senses (as in WordNet). Our approach can be extended to disambiguate senses of words for parts of speech besides verbs.",
            "cx": 4123.6,
            "cy": -565.311,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C18-1309",
            "name": "Graph Based Decoding for Event Sequencing and Coreference Resolution",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "Events in text documents are interrelated in complex ways. In this paper, we study two types of relation: Event Coreference and Event Sequencing. We show that the popular tree-like decoding structure for automated Event Coreference is not suitable for Event Sequencing. To this end, we propose a graph-based decoding algorithm that is applicable to both tasks. The new decoding algorithm supports flexible feature sets for both tasks. Empirically, our event coreference system has achieved state-of-the-art performance on the TAC-KBP 2015 event coreference task and our event sequencing system beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these event relations.",
            "cx": 2805.6,
            "cy": -296.09,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 272,
            "name": 272,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W12-6102",
            "name": "Optimization for Efficient Determination of Chunk in Automatic Evaluation for Machine Translation",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "None",
            "cx": 5194.6,
            "cy": -834.531,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "R13-1030",
            "name": "Automatic Evaluation Metric for Machine Translation that is Independent of Sentence Length",
            "publication_data": 2013,
            "citation": 1,
            "abstract": "We propose new automatic evaluation metric to evaluate machine translation. Different from most similar metrics, our proposed metric does not depend heavily on sentence length. In most metrics based on f-measure comparisons of reference and candidate translations, the relative weight of each mismatched word in short sentences is larger than it in long sentences. Therefore, the evaluation score becomes disproportionally low in short sentences even when only one non-matching word exists. In our metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics.",
            "cx": 5194.6,
            "cy": -744.791,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N12-1083",
            "name": "Structured Event Retrieval over Microblog Archives",
            "publication_data": 2012,
            "citation": 73,
            "abstract": "Microblog streams often contain a considerable amount of information about local, regional, national, and global events. Most existing microblog search capabilities are focused on recent happenings and do not provide the ability to search and explore past events. This paper proposes the problem of structured retrieval of historical event information over microblog archives. Rather than retrieving individual microblog messages in response to an event query, we propose retrieving a ranked list of historical event summaries by distilling high quality event representations using a novel temporal query expansion technique. The results of an exploratory study carried out over a large archive of Twitter messages demonstrates both the value of the microblog event retrieval task and the effectiveness of our proposed search methodologies.",
            "cx": 5418.6,
            "cy": -834.531,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "penas-etal-2012-evaluating",
            "name": "Evaluating Machine Reading Systems through Comprehension Tests",
            "publication_data": 2012,
            "citation": 5,
            "abstract": "This paper describes a methodology for testing and evaluating the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. The methodology is being used in QA4MRE (QA for Machine Reading Evaluation), one of the labs of CLEF. The task was to answer a series of multiple choice tests, each based on a single document. This allows complex questions to be asked but makes evaluation simple and completely automatic. The evaluation architecture is completely multilingual: test documents, questions, and their answers are identical in all the supported languages. Background text collections are comparable collections harvested from the web for a set of predefined topics. Each test received an evaluation score between 0 and 1 using c@1. This measure encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. 12 groups participated in the task, submitting 62 runs in 3 different languages (German, English, and Romanian). All runs were monolingual; no team attempted a cross-language task. We report here the conclusions and lessons learned after the first campaign in 2011.",
            "cx": 5638.6,
            "cy": -834.531,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 554,
            "name": 554,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-1060",
            "name": "Vector space semantics with frequency-driven motifs",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items. In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs. The framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design. We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated. Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.",
            "cx": 3841.6,
            "cy": -655.051,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-2910",
            "name": "Evaluation for Partial Event Coreference",
            "publication_data": 2014,
            "citation": "???",
            "abstract": "None",
            "cx": 3608.6,
            "cy": -655.051,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W14-2303",
            "name": "Metaphor Detection through Term Relevance",
            "publication_data": 2014,
            "citation": 17,
            "abstract": "Most computational approaches to metaphor detection try to leverage either conceptual metaphor mappings or selectional preferences. Both require extensive knowledge of the mappings/preferences in question, as well as sufficient data for all involved conceptual domains. Creating these resources is expensive and often limits the scope of these systems. We propose a statistical approach to metaphor detection that utilizes the rarity of novel metaphors, marking words that do not match a textxe2x80x99s typical vocabulary as metaphor candidates. No knowledge of semantic concepts or the metaphorxe2x80x99s source domain is required. We analyze the performance of this approach as a stand-alone classifier and as a feature in a machine learning model, reporting improvements in F1 measure over a random baseline of 58% and 68%, respectively. We also observe that, as a feature, it appears to be particularly useful when data is sparse, while its effect diminishes as the amount of training data increases.",
            "cx": 4079.6,
            "cy": -655.051,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W17-5538",
            "name": "Finding Structure in Figurative Language: Metaphor Detection with Topic-based Frames",
            "publication_data": 2017,
            "citation": 2,
            "abstract": "In this paper, we present a novel and highly effective method for induction and application of metaphor frame templates as a step toward detecting metaphor in extended discourse. We infer implicit facets of a given metaphor frame using a semi-supervised bootstrapping approach on an unlabeled corpus. Our model applies this frame facet information to metaphor detection, and achieves the state-of-the-art performance on a social media dataset when building upon other proven features in a nonlinear machine learning model. In addition, we illustrate the mechanism through which the frame and topic information enable the more accurate metaphor detection.",
            "cx": 4048.6,
            "cy": -385.831,
            "rx": 81.135,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C14-1134",
            "name": "Modeling Newswire Events using Neural Networks for Anomaly Detection",
            "publication_data": 2014,
            "citation": 8,
            "abstract": "Automatically identifying anomalous newswire events is a hard problem. We discuss the complexity of the problem and introduce a novel technique to model events based on recursive neural networks to represent events as composition of their semantic arguments. Our model learns to differentiate between normal and anomalous events. We model anomaly detection as a binary classification problem and show that the model learns useful features to classify anomaly. We use headlines from the weird news category publicly available on newswire websites to extract anomalous training examples and those from Gigaword as normal examples. We evaluate the classifier on human annotated data and obtain an accuracy of 65.44%. We also show that our model is at least as competent as the least competent human annotator in anomaly detection.",
            "cx": 3407.6,
            "cy": -655.051,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1461",
            "name": "Toward Comprehensive Understanding of a Sentiment Based on Human Motives",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In sentiment detection, the natural language processing community has focused on determining holders, facets, and valences, but has paid little attention to the reasons for sentiment decisions. Our work considers human motives as the driver for human sentiments and addresses the problem of motive detection as the first step. Following a study in psychology, we define six basic motives that cover a wide range of topics appearing in review texts, annotate 1,600 texts in restaurant and laptop domains with the motives, and report the performance of baseline methods on this new dataset. We also show that cross-domain transfer learning boosts detection performance, which indicates that these universal motives exist across different domains.",
            "cx": 3241.6,
            "cy": -206.35,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "J13-3001",
            "name": "{S}quibs: What Is a Paraphrase?",
            "publication_data": 2013,
            "citation": 65,
            "abstract": "Paraphrases are sentences or phrases that convey the same meaning using different wording. Although the logical definition of paraphrases requires strict semantic equivalence, linguistics accepts a broader, approximate, equivalencexe2x80x94thereby allowing far more examples of xe2x80x9cquasi-paraphrase.xe2x80x9d But approximate equivalence is hard to define. Thus, the phenomenon of paraphrases, as understood in linguistics, is difficult to characterize. In this article, we list a set of 25 operations that generate quasi-paraphrases. We then empirically validate the scope and accuracy of this list by manually analyzing random samples of two publicly available paraphrase corpora. We provide the distribution of naturally occurring quasi-paraphrases in English text.",
            "cx": 5399.6,
            "cy": -744.791,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 397,
            "name": 397,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-3349",
            "name": "Application of Prize based on Sentence Length in Chunk-based Automatic Evaluation of Machine Translation",
            "publication_data": 2014,
            "citation": 2,
            "abstract": "As described in this paper, we propose a new automatic evaluation metric for machine translation. Our metric is based on chunking between the reference and candidate translation. Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST. We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment.",
            "cx": 4931.6,
            "cy": -655.051,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-2006",
            "name": "Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation",
            "publication_data": 2014,
            "citation": 67,
            "abstract": ": The definitions of two coreference scoring metrics- B3 and CEAF-are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms.",
            "cx": 2419.6,
            "cy": -655.051,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1016",
            "name": "Weakly Supervised User Profile Extraction from {T}witter",
            "publication_data": 2014,
            "citation": 56,
            "abstract": "While user attribute extraction on social media has received considerable attention, existing approaches, mostly supervised, encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates (e.g., gender). In this paper, we present a weaklysupervised approach to user profile extraction from Twitter. Usersxe2x80x99 profiles from social media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their attributes from user-generated text. In addition to traditional linguistic features used in distant supervision for information extraction, our approach also takes into account network information, a unique opportunity offered by social media. We test our algorithm on three attribute domains: spouse, education and job; experimental results demonstrate our approach is able to make accurate predictions for usersxe2x80x99 attributes based on their tweets. 1",
            "cx": 4571.6,
            "cy": -655.051,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1147",
            "name": "Towards a General Rule for Identifying Deceptive Opinion Spam",
            "publication_data": 2014,
            "citation": 133,
            "abstract": "Consumersxe2x80x99 purchase decisions are increasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the potential for posting deceptive opinion spamxe2x80x94 fictitious reviews that have been deliberately written to sound authentic, to deceive the reader. In this paper, we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset, which is comprised of data from three different domains (i.e. Hotel, Restaurant, Doctor), each of which contains three types of reviews, i.e. customer generated truthful reviews, Turker generated deceptive reviews and employee (domain-expert) generated deceptive reviews. Our approach tries to capture the general difference of language usage between deceptive and truthful reviews, which we hope will help customers when making purchase decisions and review portal operators, such as TripAdvisor or Yelp, investigate possible fraudulent activity on their sites. 1",
            "cx": 5136.6,
            "cy": -655.051,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "jain-etal-2014-corpus",
            "name": "A Corpus of Participant Roles in Contentious Discussions",
            "publication_data": 2014,
            "citation": 3,
            "abstract": "The expansion of social roles is, nowadays, a fact due to the ability of users to interact, discuss, exchange ideas and opinions, and form social networks though social media. Users in online social environment play a variety of social roles. The concept of {``}social role{''} has long been used in social science describe the intersection of behavioural, meaningful, and structural attributes that emerge regularly in particular settings. In this paper, we present a new corpus for social roles in online contentious discussions. We explore various behavioural attributes such as stubbornness, sensibility, influence, and ignorance to create a model of social roles to distinguish among various social roles participants assume in such setup. We annotate discussions drawn from two different sets of corpora in order to ensure that our model of social roles and their signals hold up in general. We discuss the various criteria for deciding values for each behavioural attributes which define the roles.",
            "cx": 5312.6,
            "cy": -655.051,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6005",
            "name": "Unsupervised Event Coreference for Abstract Words",
            "publication_data": 2016,
            "citation": 2,
            "abstract": "None",
            "cx": 2581.6,
            "cy": -475.571,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.blackboxnlp-1.1",
            "name": "{BERT}ering {RAMS}: What and How Much does {BERT} Already Know About Event Arguments? - A Study on the {RAMS} Dataset",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT{'}s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, varying from a low of 17.77{\\%} for Place to a high of 51.61{\\%} for Artifact. Next, we find that linear combinations of these heads, estimated with approx. 11{\\%} of available total event argument detection supervision, can push performance well higher for some roles {---} highest two being Victim (68.29{\\%} Accuracy) and Artifact (58.82{\\%} Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate {``}best heads{''} for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in another sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated {``}nonce{''} words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.",
            "cx": 2944.6,
            "cy": -116.61,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D15-1278",
            "name": "When Are Tree Structures Necessary for Deep Learning of Representations?",
            "publication_data": 2015,
            "citation": 41,
            "abstract": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction. Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.",
            "cx": 676.597,
            "cy": -565.311,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D14-1220",
            "name": "Recursive Deep Models for Discourse Parsing",
            "publication_data": 2014,
            "citation": 78,
            "abstract": "Text-level discourse parsing remains a challenge: most approaches employ features that fail to capture the intentional, semantic, and syntactic aspects that govern discourse coherence. In this paper, we propose a recursive model for discourse parsing that jointly models distributed representations for clauses, sentences, and entire discourses. The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically,. The proposed framework obtains comparable performance regarding standard discoursing parsing evaluations when compared against current state-of-art systems.",
            "cx": 757.597,
            "cy": -655.051,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 1832,
            "name": 1832,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D15-1154",
            "name": "Efficient Inner-to-outer Greedy Algorithm for Higher-order Labeled Dependency Parsing",
            "publication_data": 2015,
            "citation": 5,
            "abstract": "Many NLP systems use dependency parsers as critical components. Jonit learning parsers usually achieve better parsing accuracies than two-stage methods. However, classical joint parsing algorithms significantly increase computational complexity, which makes joint learning impractical. In this paper, we proposed an efficient dependency parsing algorithm that is capable of capturing multiple edge-label features, while maintaining low computational complexity. We evaluate our parser on 14 different languages. Our parser consistently obtains more accurate results than three baseline systems and three popular, off-the-shelf parsers.",
            "cx": 4532.6,
            "cy": -565.311,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I17-1007",
            "name": "Neural Probabilistic Model for Non-projective {MST} Parsing",
            "publication_data": 2017,
            "citation": 11,
            "abstract": "In this paper, we propose a probabilistic parsing model that defines a proper conditional probability distribution over non-projective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTMCNNs, which automatically benefits from both word- and character-level representations, by using a combination of bidirectional LSTMs and CNNs. On top of the neural network, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. By exploiting Kirchhoff{'}s Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straightforward end-to-end model training procedure via back-propagation. We evaluate our model on 17 different datasets, across 14 different languages. Our parser achieves state-of-the-art parsing performance on nine datasets.",
            "cx": 4551.6,
            "cy": -385.831,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-1130",
            "name": "Stack-Pointer Networks for Dependency Parsing",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with $O(n^2)$ time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them",
            "cx": 4440.6,
            "cy": -296.09,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-0807",
            "name": "Evaluation Algorithms for Event Nugget Detection : A Pilot Study",
            "publication_data": 2015,
            "citation": "???",
            "abstract": "None",
            "cx": 4759.6,
            "cy": -565.311,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N15-1070",
            "name": "Ontologically Grounded Multi-sense Representation Learning for Semantic Vector Space Models",
            "publication_data": 2015,
            "citation": 58,
            "abstract": "Words are polysemous. However, most approaches to representation learning for lexical semantics assign a single vector to every surface word type. Meanwhile, lexical ontologies such as WordNet provide a source of complementary knowledge to distributional information, including a word sense inventory. In this paper we propose two novel and general approaches for generating sense-specific word embeddings that are grounded in an ontology. The first applies graph smoothing as a postprocessing step to tease the vectors of different senses apart, and is applicable to any vector space model. The second adapts predictive maximum likelihood models that learn word embeddings with latent variables representing senses grounded in an specified ontology. Empirical results on lexical semantic tasks show that our approaches effectively captures information from both the ontology and distributional statistics. Moreover, in most cases our sense-specific models outperform other models we compare against.",
            "cx": 1092.6,
            "cy": -565.311,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D15-1284",
            "name": "Humor Recognition and Humor Anchor Extraction",
            "publication_data": 2015,
            "citation": 41,
            "abstract": "Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.",
            "cx": 1362.6,
            "cy": -565.311,
            "rx": 72.6644,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P17-1191",
            "name": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4{\\%} absolute points, which amounts to a 34.4{\\%} relative reduction in errors.",
            "cx": 817.597,
            "cy": -385.831,
            "rx": 128.887,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N15-1184",
            "name": "Retrofitting Word Vectors to Semantic Lexicons",
            "publication_data": 2015,
            "citation": 247,
            "abstract": "Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms.",
            "cx": 868.597,
            "cy": -565.311,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P16-1045",
            "name": "Tables as Semi-structured Knowledge for Question Answering",
            "publication_data": 2016,
            "citation": 17,
            "abstract": "None",
            "cx": 868.597,
            "cy": -475.571,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-1225",
            "name": "{A}dv{E}ntu{R}e: Adversarial Training for Textual Entailment with Knowledge-Guided Examples",
            "publication_data": 2018,
            "citation": 11,
            "abstract": "We consider the problem of learning textual entailment models with limited supervision (5K-10K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates. Second, to make the entailment model{---}a discriminator{---}more robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator{'}s weaknesses. We demonstrate effectiveness using two entailment datasets, where the proposed methods increase accuracy by 4.7{\\%} on SciTail and by 2.8{\\%} on a 1{\\%} sub-sample of SNLI. Notably, even a single hand-written rule, negate, improves the accuracy of negation examples in SNLI by 6.1{\\%}.",
            "cx": 1278.6,
            "cy": -296.09,
            "rx": 147.571,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P16-1101",
            "name": "End-to-end Sequence Labeling via Bi-directional {LSTM}-{CNN}s-{CRF}",
            "publication_data": 2016,
            "citation": 528,
            "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks xe2x80x94 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets xe2x80x94 97.55% accuracy for POS tagging and 91.21% F1 for NER.",
            "cx": 4682.6,
            "cy": -475.571,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 213,
            "name": 213,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P16-1228",
            "name": "Harnessing Deep Neural Networks with Logic Rules",
            "publication_data": 2016,
            "citation": 31,
            "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.",
            "cx": 4902.6,
            "cy": -475.571,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P19-1562",
            "name": "An Empirical Investigation of Structured Output Modeling for Graph-based Neural Dependency Parsing",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "In this paper, we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest.",
            "cx": 4609.6,
            "cy": -206.35,
            "rx": 72.6644,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.tacl-1.39",
            "name": "Nested Named Entity Recognition via Second-best Sequence Learning and Decoding",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. In addition, we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outside-to-inside way. Our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. Experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities, achieving F1-scores of 85.82{\\%}, 84.34{\\%}, and 77.36{\\%} on ACE-2004, ACE-2005, and GENIA datasets, respectively.",
            "cx": 4738.6,
            "cy": -116.61,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P16-1126",
            "name": "The Creation and Analysis of a Website Privacy Policy Corpus",
            "publication_data": 2016,
            "citation": 33,
            "abstract": "Website privacy policies are often ignored by Internet users, because these documents tend to be long and difficult to understand. However, the significance of privacy policies greatly exceeds the attention paid to them: these documents are binding legal agreements between website operators and their users, and their opaqueness is a challenge not only to Internet users but also to policy regulators. One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text, using a combination of crowdsourcing, natural language processing, and machine learning. However, there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies. To remedy this problem, we introduce a corpus of 115 privacy policies (267K words) with manual annotations for 23K fine-grained data practices. We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data. We provide findings based on a census of the annotations and show results toward automating the annotation procedure. Finally, we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies.",
            "cx": 5093.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N16-1082",
            "name": "Visualizing and Understanding Neural Models in {NLP}",
            "publication_data": 2016,
            "citation": 174,
            "abstract": "While neural networks have been successfully applied to many NLP tasks the resulting vectorbased models are very difficult to interpret. For example itxe2x80x99s not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allowing us to see wellknown markedness asymmetries in negation. We then introduce methods for visualizing a unitxe2x80x99s salience, the amount that it contributes to the final composed meaning from first-order derivatives. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks.",
            "cx": 5275.6,
            "cy": -475.571,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2020.findings-emnlp.344",
            "name": "Event-Related Bias Removal for Real-time Disaster Events",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.",
            "cx": 5201.6,
            "cy": -116.61,
            "rx": 119.502,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.emnlp-main.64",
            "name": "{SELFEXPLAIN}: A Self-Explaining Architecture for Neural Text Classifiers",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "We introduce SelfExplain, a novel self-explaining model that explains a text classifier{'}s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance. Most importantly, explanations from SelfExplain show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.",
            "cx": 5349.6,
            "cy": -26.8701,
            "rx": 135.115,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N16-1174",
            "name": "Hierarchical Attention Networks for Document Classification",
            "publication_data": 2016,
            "citation": 1037,
            "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.",
            "cx": 1723.6,
            "cy": -475.571,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N19-1364",
            "name": "Let{'}s Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "Modeling what makes a request persuasive - eliciting the desired response from a reader - is critical to the study of propaganda, behavioral economics, and advertising. Yet current models can{'}t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies - offering increased interpretability of persuasive speech - and has applications for other situations with document-level supervision but only partial sentence supervision.",
            "cx": 1875.6,
            "cy": -206.35,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1589",
            "name": "Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Generating a long, coherent text such as a paragraph requires a high-level control of different levels of relations between sentences (e.g., tense, coreference). We call such a logical connection between sentences as a (paragraph) flow. In order to produce a coherent flow of text, we explore two forms of intersentential relations in a paragraph: one is a human-created linguistical relation that forms a structure (e.g., discourse tree) and the other is a relation from latent representation learned from the sentences themselves. Our two proposed models incorporate each form of relations into document-level language models: the former is a supervised model that jointly learns a language model as well as discourse relation prediction, and the latter is an unsupervised model that is hierarchically conditioned by a recurrent neural network (RNN) over the latent information. Our proposed models with both forms of relations outperform the baselines in partially conditioned paragraph generation task. Our codes and data are publicly available.",
            "cx": 1473.6,
            "cy": -206.35,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1206",
            "name": "Edit Categories and Editor Role Identification in {W}ikipedia",
            "publication_data": 2016,
            "citation": 5,
            "abstract": "In this work, we introduced a corpus for categorizing edit types in Wikipedia. This fine-grained taxonomy of edit types enables us to differentiate editing actions and find editor roles in Wikipedia based on their low-level edit types. To do this, we first created an annotated corpus based on 1,996 edits obtained from 953 article revisions and built machine-learning models to automatically identify the edit categories associated with edits. Building on this automated measurement of edit types, we then applied a graphical model analogous to Latent Dirichlet Allocation to uncover the latent roles in editors{'} edit histories. Applying this technique revealed eight different roles editors play, such as Social Networker, Substantive Expert, etc.",
            "cx": 5444.6,
            "cy": -475.571,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018",
            "citation_count": 51,
            "name": 51,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-4902",
            "name": "Shakespearizing Modern Language Using Copy-Enriched Sequence to Sequence Models",
            "publication_data": 2017,
            "citation": 13,
            "abstract": "Variations in writing styles are commonly used to adapt the content to a specific context, audience, or purpose. However, applying stylistic variations is still by and large a manual process, and there have been little efforts towards automating it. In this paper we explore automated methods to transform text from modern English to Shakespearean English using an end to end trainable neural model with pointers to enable copy action. To tackle limited amount of parallel data, we pre-train embeddings of words by leveraging external dictionaries mapping Shakespearean words to modern English words as well as additional text. Our methods are able to get a BLEU score of 31+, an improvement of {\\mbox{$\\approx$}} 6 points above the strongest baseline. We publicly release our code to foster further research in this area.",
            "cx": 3833.6,
            "cy": -385.831,
            "rx": 115.017,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W17-4415",
            "name": "Huntsville, hospitals, and hockey teams: Names can reveal your location",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "Geolocation is the task of identifying a social media user{'}s primary location, and in natural language processing, there is a growing literature on to what extent automated analysis of social media posts can help. However, not all content features are equally revealing of a user{'}s location. In this paper, we evaluate nine name entity (NE) types. Using various metrics, we find that GEO-LOC, FACILITY and SPORT-TEAM are more informative for geolocation than other NE types. Using these types, we improve geolocation accuracy and reduce distance error over various famous text-based methods.",
            "cx": 5471.6,
            "cy": -385.831,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-2703",
            "name": "Event Detection Using Frame-Semantic Parser",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "Recent methods for Event Detection focus on Deep Learning for automatic feature generation and feature ranking. However, most of those approaches fail to exploit rich semantic information, which results in relatively poor recall. This paper is a small {\\&} focused contribution, where we introduce an Event Detection and classification system, based on deep semantic information retrieved from a frame-semantic parser. Our experiments show that our system achieves higher recall than state-of-the-art systems. Further, we claim that enhancing our system with deep learning techniques like feature ranking can achieve even better results, as it can benefit from both approaches.",
            "cx": 5671.6,
            "cy": -385.831,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1088",
            "name": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion",
            "publication_data": 2017,
            "citation": 14,
            "abstract": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets{---}WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.",
            "cx": 5840.6,
            "cy": -385.831,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D18-1257",
            "name": "Large-scale Cloze Test Dataset Created by Teachers",
            "publication_data": 2018,
            "citation": 12,
            "abstract": "Cloze tests are widely adopted in language exams to evaluate students{'} language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.",
            "cx": 824.597,
            "cy": -296.09,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I17-3016",
            "name": "{STCP}: Simplified-Traditional {C}hinese Conversion and Proofreading",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "This paper aims to provide an effective tool for conversion between Simplified Chinese and Traditional Chinese. We present STCP, a customizable system comprising statistical conversion model, and proofreading web interface. Experiments show that our system achieves comparable character-level conversion performance with the state-of-art systems. In addition, our proofreading interface can effectively support diagnostics and data annotation. STCP is available at \\url{http://lagos.lti.cs.cmu.edu:8002/}",
            "cx": 6007.6,
            "cy": -385.831,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.502",
            "name": "{SCDE}: Sentence Cloze Dataset with High Quality Distractors From Examinations",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. The blanks require joint solving and significantly impair each other{'}s context. Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging. Our experiments show that there is a significant performance gap between advanced models (72{\\%}) and humans (87{\\%}), encouraging future models to bridge this gap.",
            "cx": 882.597,
            "cy": -116.61,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1213",
            "name": "Identifying Semantic Edit Intentions from Revisions in {W}ikipedia",
            "publication_data": 2017,
            "citation": 11,
            "abstract": "Most studies on human editing focus merely on syntactic revision operations, failing to capture the intentions behind revision changes, which are essential for facilitating the single and collaborative writing process. In this work, we develop in collaboration with Wikipedia editors a 13-category taxonomy of the semantic intention behind edits in Wikipedia articles. Using labeled article edits, we build a computational classifier of intentions that achieved a micro-averaged F1 score of 0.621. We use this model to investigate edit intention effectiveness: how different types of edits predict the retention of newcomers and changes in the quality of articles, two key concerns for Wikipedia today. Our analysis shows that the types of edits that users make in their first session predict their subsequent survival as Wikipedia editors, and articles in different stages need different types of edits.",
            "cx": 6198.6,
            "cy": -385.831,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D17-1292",
            "name": "Detecting and Explaining Causes From Text For a Time Series Event",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.",
            "cx": 1324.6,
            "cy": -385.831,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1315",
            "name": "{C}harmanteau: Character Embedding Models For Portmanteau Creation",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "Portmanteaus are a word formation phenomenon where two words combine into a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.",
            "cx": 3584.6,
            "cy": -385.831,
            "rx": 115.931,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 50,
            "name": 50,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1437",
            "name": "{F}low{S}eq: Non-Autoregressive Conditional Sequence Generation with Generative Flow",
            "publication_data": 2019,
            "citation": 3,
            "abstract": "Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.",
            "cx": 4404.6,
            "cy": -206.35,
            "rx": 114.603,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1154",
            "name": "Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data",
            "publication_data": 2018,
            "citation": 4,
            "abstract": "This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.",
            "cx": 5463.6,
            "cy": -296.09,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1155",
            "name": "From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.",
            "cx": 5645.6,
            "cy": -296.09,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.deelio-1.4",
            "name": "{G}en{A}ug: Data Augmentation for Finetuning Text Generators",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we investigate data augmentation for text generation, which we call GenAug. Text generation and language modeling are important tasks within natural language processing, and are especially challenging for low-data regimes. We propose and evaluate various augmentation methods, including some that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp Reviews. We also examine the relationship between the amount of augmentation and the quality of the generated text. We utilize several metrics that evaluate important aspects of the generated text including its diversity and fluency. Our experiments demonstrate that insertion of character-level synthetic noise and keyword replacement with hypernyms are effective augmentation methods, and that the quality of generations improves to a peak at approximately three times the amount of original data.",
            "cx": 1150.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.findings-acl.84",
            "name": "A Survey of Data Augmentation Approaches for {NLP}",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "None",
            "cx": 1219.6,
            "cy": -26.8701,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N18-1149",
            "name": "A Dataset of Peer Reviews ({P}eer{R}ead): Collection, Insights and {NLP} Applications",
            "publication_data": 2018,
            "citation": 16,
            "abstract": "Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1),1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21{\\%} error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as {`}originality{'} and {`}impact{'}.",
            "cx": 1598.6,
            "cy": -296.09,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C18-1007",
            "name": "Low-resource Cross-lingual Event Type Detection via Distant Supervision with Minimal Effort",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "The use of machine learning for NLP generally requires resources for training. Tasks performed in a low-resource language usually rely on labeled data in another, typically resource-rich, language. However, there might not be enough labeled data even in a resource-rich language such as English. In such cases, one approach is to use a hand-crafted approach that utilizes only a small bilingual dictionary with minimal manual verification to create distantly supervised data. Another is to explore typical machine learning techniques, for example adversarial training of bilingual word representations. We find that in event-type detection task{---}the task to classify [parts of] documents into a fixed set of labels{---}they give about the same performance. We explore ways in which the two methods can be complementary and also see how to best utilize a limited budget for manual annotation to maximize performance gain.",
            "cx": 5853.6,
            "cy": -296.09,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-5009",
            "name": "Domain Adaptation of {SRL} Systems for Biological Processes",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "Domain adaptation remains one of the most challenging aspects in the wide-spread use of Semantic Role Labeling (SRL) systems. Current state-of-the-art methods are typically trained on large-scale datasets, but their performances do not directly transfer to low-resource domain-specific settings. In this paper, we propose two approaches for domain adaptation in the biological domain that involves pre-training LSTM-CRF based on existing large-scale datasets and adapting it for a low-resource corpus of biological processes. Our first approach defines a mapping between the source labels and the target labels, and the other approach modifies the final CRF layer in sequence-labeling neural network architecture. We perform our experiments on ProcessBank dataset which contains less than 200 paragraphs on biological processes. We improve over the previous state-of-the-art system on this dataset by 21 F1 points. We also show that, by incorporating event-event relationship in ProcessBank, we are able to achieve an additional 2.6 F1 gain, giving us possible insights into how to improve SRL systems for biological process using richer annotations.",
            "cx": 5467.6,
            "cy": -206.35,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-4502",
            "name": "A Cascade Model for Proposition Extraction in Argumentation",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We present a model to tackle a fundamental but understudied problem in computational argumentation: proposition extraction. Propositions are the basic units of an argument and the primary building blocks of most argument mining systems. However, they are usually substituted by argumentative discourse units obtained via surface-level text segmentation, which may yield text segments that lack semantic information necessary for subsequent argument mining processes. In contrast, our cascade model aims to extract complete propositions by handling anaphora resolution, text segmentation, reported speech, questions, imperatives, missing subjects, and revision. We formulate each task as a computational problem and test various models using a corpus of the 2016 U.S. presidential debates. We show promising performance for some tasks and discuss main challenges in proposition extraction.",
            "cx": 5636.6,
            "cy": -206.35,
            "rx": 61.5366,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.127",
            "name": "Machine-Aided Annotation for Fine-Grained Proposition Types in Argumentation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We introduce a corpus of the 2016 U.S. presidential debates and commentary, containing 4,648 argumentative propositions annotated with fine-grained proposition types. Modern machine learning pipelines for analyzing argument have difficulty distinguishing between types of propositions based on their factuality, rhetorical positioning, and speaker commitment. Inability to properly account for these facets leaves such systems inaccurate in understanding of fine-grained proposition types. In this paper, we demonstrate an approach to annotating for four complex proposition types, namely normative claims, desires, future possibility, and reported speech. We develop a hybrid machine learning and human workflow for annotation that allows for efficient and reliable annotation of complex linguistic phenomena, and demonstrate with preliminary analysis of rhetorical strategies and structure in presidential debates. This new dataset and method can support technical researchers seeking more nuanced representations of argument, as well as argumentation theorists developing new quantitative analyses.",
            "cx": 5564.6,
            "cy": -116.61,
            "rx": 115.017,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.emnlp-main.2",
            "name": "Extracting Implicitly Asserted Propositions in Argumentation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.",
            "cx": 5792.6,
            "cy": -116.61,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P19-1329",
            "name": "Exploring Numeracy in Word Embeddings",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "Word embeddings are now pervasive across NLP subfields as the de-facto method of forming text representataions. In this work, we show that existing embedding models are inadequate at constructing representations that capture salient aspects of mathematical meaning for numbers, which is important for language understanding. Numbers are ubiquitous and frequently appear in text. Inspired by cognitive studies on how humans perceive numbers, we develop an analysis framework to test how well word embeddings capture two essential properties of numbers: magnitude (e.g. 3{\\textless}4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems.",
            "cx": 5825.6,
            "cy": -206.35,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N19-1186",
            "name": "Word Embedding-Based Automatic {MT} Evaluation Metric using Word Position Information",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "We propose a new automatic evaluation metric for machine translation. Our proposed metric is obtained by adjusting the Earth Mover{'}s Distance (EMD) to the evaluation task. The EMD measure is used to obtain the distance between two probability distributions consisting of some signatures having a feature and a weight. We use word embeddings, sentence-level tf-idf, and cosine similarity between two word embeddings, respectively, as the features, weight, and the distance between two features. Results show that our proposed metric can evaluate machine translation based on word meaning. Moreover, for distance, cosine similarity and word position information are used to address word-order differences. We designate this metric as Word Embedding-Based automatic MT evaluation using Word Position Information (WE{\\_}WPI). A meta-evaluation using WMT16 metrics shared task set indicates that our WE{\\_}WPI achieves the highest correlation with human judgment among several representative metrics.",
            "cx": 6033.6,
            "cy": -206.35,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N19-1253",
            "name": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing",
            "publication_data": 2019,
            "citation": 18,
            "abstract": "Different languages might have different word orders. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.",
            "cx": 6192.6,
            "cy": -206.35,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N19-1273",
            "name": "Iterative Search for Weakly Supervised Semantic Parsing",
            "publication_data": 2019,
            "citation": 7,
            "abstract": "Training semantic parsers from question-answer pairs typically involves searching over an exponentially large space of logical forms, and an unguided search can easily be misled by spurious logical forms that coincidentally evaluate to the correct answer. We propose a novel iterative training algorithm that alternates between searching for consistent logical forms and maximizing the marginal likelihood of the retrieved ones. This training scheme lets us iteratively train models that provide guidance to subsequent ones to search for logical forms of increasing complexity, thus dealing with the problem of spuriousness. We evaluate these techniques on two hard datasets: WikiTableQuestions (WTQ) and Cornell Natural Language Visual Reasoning (NLVR), and show that our training algorithm outperforms the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision.",
            "cx": 6365.6,
            "cy": -206.35,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.473",
            "name": "Measuring Forecasting Skill from Text",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "People vary in their ability to make accurate predictions about the future. Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy. This leads to a natural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecasting skill. Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts. We present a number of linguistic metrics which are computed over text associated with people{'}s predictions about the future including: uncertainty, readability, and emotion. By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters. Furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language. This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier.",
            "cx": 1875.6,
            "cy": -116.61,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "K19-1033",
            "name": "{EQUATE}: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference",
            "publication_data": 2019,
            "citation": 5,
            "abstract": "Quantitative reasoning is a higher-order reasoning skill that any intelligent natural language understanding system can reasonably be expected to handle. We present EQUATE (Evaluating Quantitative Understanding Aptitude in Textual Entailment), a new framework for quantitative reasoning in textual entailment. We benchmark the performance of 9 published NLI models on EQUATE, and find that on average, state-of-the-art methods do not achieve an absolute improvement over a majority-class baseline, suggesting that they do not implicitly learn to reason with quantities. We establish a new baseline Q-REAS that manipulates quantities symbolically. In comparison to the best performing NLI model, it achieves success on numerical reasoning tests (+24.2 {\\%}), but has limited verbal reasoning capabilities (-8.1 {\\%}). We hope our evaluation framework will support the development of models of quantitative reasoning in language understanding.",
            "cx": 6595.6,
            "cy": -206.35,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.eacl-main.295",
            "name": "Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of {`}probing{'} tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.",
            "cx": 6595.6,
            "cy": -26.8701,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "J19-4002",
            "name": "Discourse in Multimedia: A Case Study in Extracting Geometry Knowledge from Textbooks",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "To ensure readability, text is often written and presented with due formatting. These text formatting devices help the writer to effectively convey the narrative. At the same time, these help the readers pick up the structure of the discourse and comprehend the conveyed information. There have been a number of linguistic theories on discourse structure of text. However, these theories only consider unformatted text. Multimedia text contains rich formatting features that can be leveraged for various NLP tasks. In this article, we study some of these discourse features in multimedia text and what communicative function they fulfill in the context. As a case study, we use these features to harvest structured subject knowledge of geometry from textbooks. We conclude that the discourse and text layout features provide information that is complementary to lexical semantic information. Finally, we show that the harvested structured knowledge can be used to improve an existing solver for geometry problems, making it more accurate as well as more explainable.",
            "cx": 6840.6,
            "cy": -206.35,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D19-5316",
            "name": "Do Sentence Interactions Matter? Leveraging Sentence Level Representations for Fake News Classification",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "The rising growth of fake news and misleading information through online media outlets demands an automatic method for detecting such news articles. Of the few limited works which differentiate between trusted vs other types of news article (satire, propaganda, hoax), none of them model sentence interactions within a document. We observe an interesting pattern in the way sentences interact with each other across different kind of news articles. To capture this kind of information for long news articles, we propose a graph neural network-based model which does away with the need of feature engineering for fine grained fake news classification. Through experiments, we show that our proposed method beats strong neural baselines and achieves state-of-the-art accuracy on existing datasets. Moreover, we establish the generalizability of our model by evaluating its performance in out-of-domain scenarios. Code is available at https://github.com/MysteryVaibhav/fake{\\_}news{\\_}semantics.",
            "cx": 7040.6,
            "cy": -206.35,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.emnlp-main.529",
            "name": "Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process. Where can the model learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph. However, the task suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of content keywords are provided, overall generation quality also increases.",
            "cx": 1473.6,
            "cy": -116.61,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.starsem-1.10",
            "name": "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: \\textit{do probing studies shed light on systematic knowledge in BERT representations?} As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT {`}understands{'} a concept, and it cannot be expected to systematically generalize across applicable contexts.",
            "cx": 6702.6,
            "cy": -116.61,
            "rx": 79.3924,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.sdp-1.1",
            "name": "Overview of the First Workshop on Scholarly Document Processing ({SDP})",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Next to keeping up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. To address these challenges, computational work on enhancing search, summarization, and analysis of scholarly documents has flourished. However, the various strands of research on scholarly document processing remain fragmented. To reach to the broader NLP and AI/ML community, pool distributed efforts and enable shared access to published research, we held the 1st Workshop on Scholarly Document Processing at EMNLP 2020 as a virtual event. The SDP workshop consisted of a research track (including a poster session), two invited talks and three Shared Tasks (CL-SciSumm, Lay-Summ and LongSumm), geared towards easier access to scientific methods and results. \\textbf{Website}: \\url{https://ornlcda.github.io/SDProc}",
            "cx": 6892.6,
            "cy": -116.61,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.sdp-1.24",
            "name": "Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: {CL}-{S}ci{S}umm, {L}ay{S}umm and {L}ong{S}umm",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing two or three of the tasks. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.",
            "cx": 7095.6,
            "cy": -116.61,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.findings-emnlp.160",
            "name": "An Empirical Exploration of Local Ordering Pre-training for Structured Prediction",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Recently, pre-training contextualized encoders with language model (LM) objectives has been shown an effective semi-supervised method for structured prediction. In this work, we empirically explore an alternative pre-training method for contextualized encoders. Instead of predicting words in LMs, we {``}mask out{''} and predict word order information, with a local ordering strategy and word-selecting objectives. With evaluations on three typical structured prediction tasks (dependency parsing, POS tagging, and NER) over four languages (English, Finnish, Czech, and Italian), we show that our method is consistently beneficial. We further conduct detailed error analysis, including one that examines a specific type of parsing error where the head is misidentified. The results show that pre-trained contextual encoders can bring improvements in a structured way, suggesting that they may be able to capture higher-order patterns and feature combinations from unlabeled data.",
            "cx": 7278.6,
            "cy": -116.61,
            "rx": 72.6644,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.findings-emnlp.300",
            "name": "What-if {I} ask you to explain: Explaining the effects of perturbations in procedural text",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "Our goal is to explain the effects of perturbations in procedural text, e.g., given a passage describing a rabbit{'}s life cycle, explain why illness (the perturbation) may reduce the rabbit population (the effect). Although modern systems are able to solve the original prediction task well (e.g., illness results in less rabbits), the explanation task - identifying the causal chain of events from perturbation to effect - remains largely unaddressed, and is the goal of this research. We present QUARTET, a system that constructs such explanations from paragraphs, by modeling the explanation task as a multitask learning problem. QUARTET constructs explanations from the sentences in the procedural text, achieving {\\textasciitilde}18 points better on explanation accuracy compared to several strong baselines on a recent process comprehension benchmark. On an end task on this benchmark, we show a surprising finding that good explanations do not have to come at the expense of end task performance, in fact leading to a 7{\\%} F1 improvement over SOTA.",
            "cx": 7457.6,
            "cy": -116.61,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.emnlp-main.1",
            "name": "Detecting Attackable Sentences in Arguments",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence{'}s attackability is associated with many of these characteristics regarding the sentence{'}s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.",
            "cx": 7655.6,
            "cy": -116.61,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.emnlp-main.79",
            "name": "Incorporating a Local Translation Mechanism into Non-autoregressive Translation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. Our code will be released to the public.",
            "cx": 7870.6,
            "cy": -116.61,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.emnlp-main.520",
            "name": "A Dataset for Tracking Entities in Open Domain Procedural Text",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this task provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create OPENPI, a high-quality (91.5{\\%} coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com. A current state-of-the-art generation model on this task achieves 16.1{\\%} F1 based on BLEU metric, leaving enough room for novel model architectures.",
            "cx": 8050.6,
            "cy": -116.61,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.coling-main.273",
            "name": "Definition Frames: Using Definitions for Hybrid Concept Representations",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Advances in word representations have shown tremendous improvements in downstream NLP tasks, but lack semantic interpretability. In this paper, we introduce Definition Frames (DF), a matrix distributed representation extracted from definitions, where each dimension is semantically interpretable. DF dimensions correspond to the Qualia structure relations: a set of relations that uniquely define a term. Our results show that DFs have competitive performance with other distributional semantic approaches on word similarity tasks.",
            "cx": 8229.6,
            "cy": -116.61,
            "rx": 104.804,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.blackboxnlp-1.20",
            "name": "Exploring Neural Entity Representations for Semantic Information",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task structure, while probing task evaluations often look at only a few attributes and models. We address both of these issues by evaluating a diverse set of eight neural entity embedding methods on a set of simple probing tasks, demonstrating which methods are able to remember words used to describe entities, learn type, relationship and factual information, and identify how frequently an entity is mentioned. We also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets.",
            "cx": 8449.6,
            "cy": -116.61,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.acl-main.667",
            "name": "A Two-Step Approach for Implicit Event Argument Detection",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "cx": 8632.6,
            "cy": -116.61,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.spnlp-1.8",
            "name": "Comparing Span Extraction Methods for Semantic Role Labeling",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "In this work, we empirically compare span extraction methods for the task of semantic role labeling (SRL). While recent progress incorporating pre-trained contextualized representations into neural encoders has greatly improved SRL F1 performance on popular benchmarks, the potential costs and benefits of structured decoding in these models have become less clear. With extensive experiments on PropBank SRL datasets, we find that more structured decoding methods outperform BIO-tagging when using static (word type) embeddings across all experimental settings. However, when used in conjunction with pre-trained contextualized word representations, the benefits are diminished. We also experiment in cross-genre and cross-lingual settings and find similar trends. We further perform speed comparisons and provide analysis on the accuracy-efficiency trade-offs among different decoding methods.",
            "cx": 6817.6,
            "cy": -26.8701,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.sigmorphon-1.22",
            "name": "Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Traditionally, character-level transduction problems have been solved with finite-state models designed to encode structural and linguistic knowledge of the underlying process, whereas recent approaches rely on the power and flexibility of sequence-to-sequence models with attention. Focusing on the less explored unsupervised learning scenario, we compare the two model classes side by side and find that they tend to make different types of errors even when achieving comparable performance. We analyze the distributions of different error classes using two unsupervised tasks as testbeds: converting informally romanized text into the native script of its language (for Russian, Arabic, and Kannada) and translating between a pair of closely related languages (Serbian and Bosnian). Finally, we investigate how combining finite-state and sequence-to-sequence models at decoding time affects the output quantitatively and qualitatively.",
            "cx": 7043.6,
            "cy": -26.8701,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.naacl-main.171",
            "name": "{S}tyle{PTB}: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.",
            "cx": 7274.6,
            "cy": -26.8701,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.inlg-1.21",
            "name": "{SAPPHIRE}: Approaches for Enhanced Concept-to-Text Generation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "We motivate and propose a suite of simple but effective improvements for concept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc PHrase Infilling and REcombination. We demonstrate their effectiveness on generative commonsense reasoning, a.k.a. the CommonGen task, through experiments using both BART and T5 models. Through extensive automatic and human evaluation, we show that SAPPHIRE noticeably improves model performance. An in-depth qualitative analysis illustrates that SAPPHIRE effectively addresses many issues of the baseline model generations, including lack of commonsense, insufficient specificity, and poor fluency.",
            "cx": 7517.6,
            "cy": -26.8701,
            "rx": 114.603,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.findings-emnlp.264",
            "name": "Knowledge-Enhanced Evidence Retrieval for Counterargument Generation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Finding counterevidence to statements is key to many tasks, including counterargument generation. We build a system that, given a statement, retrieves counterevidence from diverse sources on the Web. At the core of this system is a natural language inference (NLI) model that determines whether a candidate sentence is valid counterevidence or not. Most NLI models to date, however, lack proper reasoning abilities necessary to find counterevidence that involves complex inference. Thus, we present a knowledge-enhanced NLI model that aims to handle causality- and example-based inference by incorporating knowledge graphs. Our NLI model outperforms baselines for NLI tasks, especially for instances that require the targeted inference. In addition, this NLI model further improves the counterevidence retrieval system, notably finding complex counterevidence better.",
            "cx": 7791.6,
            "cy": -26.8701,
            "rx": 141.343,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.findings-acl.357",
            "name": "Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "None",
            "cx": 8044.6,
            "cy": -26.8701,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.emnlp-main.592",
            "name": "Investigating Robustness of Dialog Models to Popular Figurative Language Constructs",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open domain dialog datasets. When faced with dialog contexts consisting of figurative language, some models show very large drops in performance compared to contexts without figurative language. We encourage future research in dialog modeling to separately analyze and report results on figurative language in order to better test model capabilities relevant to real-world use. Finally, we propose lightweight solutions to help existing models become more robust to figurative language by simply using an external resource to translate figurative language to literal (non-figurative) forms while preserving the meaning to the best extent possible.",
            "cx": 8256.6,
            "cy": -26.8701,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.findings-acl.456",
            "name": "Could you give me a hint ? Generating inference graphs for defeasible reasoning",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "None",
            "cx": 8451.6,
            "cy": -26.8701,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.emnlp-main.508",
            "name": "Think about it! Improving defeasible reasoning by first modeling the question scenario.",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a {``}mental model{''} of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a system to {``}think about{''} a question and explicitly model the scenario, rather than answering reflexively.",
            "cx": 8619.6,
            "cy": -26.8701,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.emnlp-main.503",
            "name": "On the Benefit of Syntactic Supervision for Cross-lingual Transfer in Semantic Role Labeling",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Although recent developments in neural architectures and pre-trained representations have greatly increased state-of-the-art model performance on fully-supervised semantic role labeling (SRL), the task remains challenging for languages where supervised SRL training data are not abundant. Cross-lingual learning can improve performance in this setting by transferring knowledge from high-resource languages to low-resource ones. Moreover, we hypothesize that annotations of syntactic dependencies can be leveraged to further facilitate cross-lingual transfer. In this work, we perform an empirical exploration of the helpfulness of syntactic supervision for crosslingual SRL within a simple multitask learning scheme. With comprehensive evaluations across ten languages (in addition to English) and three SRL benchmark datasets, including both dependency- and span-based SRL, we show the effectiveness of syntactic supervision in low-resource scenarios.",
            "cx": 8785.6,
            "cy": -26.8701,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.eacl-main.259",
            "name": "{N}oise{QA}: Challenge Set Evaluation for User-Centric Question Answering",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "When Question-Answering (QA) systems are deployed in the real world, users query them through a variety of interfaces, such as speaking to voice assistants, typing questions into a search engine, or even translating questions to languages supported by the QA system. While there has been significant community attention devoted to identifying correct answers in passages assuming a perfectly formed question, we show that components in the pipeline that precede an answering engine can introduce varied and considerable sources of error, and performance can degrade substantially based on these upstream noise sources even for powerful pre-trained QA models. We conclude that there is substantial room for progress before QA systems can be effectively deployed, highlight the need for QA evaluation to expand to consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans.",
            "cx": 9002.6,
            "cy": -26.8701,
            "rx": 125.316,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.acl-long.94",
            "name": "More Identifiable yet Equally Performant Transformers for Text Classification",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Interpretability is an important aspect of the trustworthiness of a model{'}s predictions. Transformer{'}s predictions are widely explained by the attention weights, i.e., a probability distribution generated at its self-attention unit (head). Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique. A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights. For a given input to a head and its output, if the attention weights generated in it are unique, we call the weights identifiable. In this work, we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights. Ignored in the previous works, we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector. However, the weights are still prone to be non-unique attentions that make them unfit for interpretation. To tackle this issue, we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input. We prove the applicability of such variations by providing empirical justifications on varied text classification tasks. The implementations are available at https://github.com/declare-lab/identifiable-transformers.",
            "cx": 9226.6,
            "cy": -26.8701,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.acl-long.494",
            "name": "Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Aspect-based sentiment analysis is a fine-grained sentiment classification task. Recently, graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words. However, the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews. To overcome these challenges, in this paper, we propose a dual graph convolutional networks (DualGCN) model that considers the complementarity of syntax structures and semantic correlations simultaneously. Particularly, to alleviate dependency parsing errors, we design a SynGCN module with rich syntactic knowledge. To capture semantic correlations, we design a SemGCN module with self-attention mechanism. Furthermore, we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module. The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word. The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture. Experimental results on three public datasets show that our DualGCN model outperforms state-of-the-art methods and verify the effectiveness of our model.",
            "cx": 9396.6,
            "cy": -26.8701,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        }
    ],
    [
        {
            "source": "1988",
            "target": "1989",
            "d": "M28.5975,-2969.96C28.5975,-2954.6 28.5975,-2932.28 28.5975,-2916.92"
        },
        {
            "source": "P88-1020",
            "target": "W90-0117",
            "d": "M577.679,-2961.87C552.058,-2930.72 508.575,-2877.86 480.224,-2843.4"
        },
        {
            "source": "P88-1020",
            "target": "1997.tmi-1.6",
            "d": "M664.496,-2969.55C735.545,-2945.96 838.597,-2896.75 838.597,-2809.81 838.597,-2809.81 838.597,-2809.81 838.597,-2359.11 838.597,-2308.95 823.202,-2252.72 811.513,-2217.25"
        },
        {
            "source": "P88-1020",
            "target": "D14-1218",
            "d": "M598.118,-2961.13C597.527,-2926.51 596.597,-2863.58 596.597,-2809.81 596.597,-2809.81 596.597,-2809.81 596.597,-833.531 596.597,-784.804 596.597,-728.552 596.597,-692.666"
        },
        {
            "source": "P88-1022",
            "target": "W90-0117",
            "d": "M418.414,-2961.49C425.287,-2930.99 436.763,-2880.07 444.502,-2845.73"
        },
        {
            "source": "1989",
            "target": "1990",
            "d": "M28.5975,-2880.22C28.5975,-2864.86 28.5975,-2842.54 28.5975,-2827.18"
        },
        {
            "source": "1990",
            "target": "1991",
            "d": "M28.5975,-2790.48C28.5975,-2775.12 28.5975,-2752.8 28.5975,-2737.44"
        },
        {
            "source": "1991",
            "target": "1992",
            "d": "M28.5975,-2700.74C28.5975,-2685.38 28.5975,-2663.06 28.5975,-2647.7"
        },
        {
            "source": "1992",
            "target": "1993",
            "d": "M28.5975,-2611C28.5975,-2595.64 28.5975,-2573.32 28.5975,-2557.96"
        },
        {
            "source": "H92-1052",
            "target": "H94-1025",
            "d": "M405.521,-2602.53C419.732,-2571.77 443.544,-2520.23 459.41,-2485.89"
        },
        {
            "source": "H92-1052",
            "target": "1994.amta-1.23",
            "d": "M368.668,-2602.9C338.003,-2571.63 285.873,-2518.45 252.095,-2484"
        },
        {
            "source": "1993",
            "target": "1994",
            "d": "M28.5975,-2521.26C28.5975,-2505.9 28.5975,-2483.58 28.5975,-2468.21"
        },
        {
            "source": "1994",
            "target": "1995",
            "d": "M28.5975,-2431.52C28.5975,-2416.16 28.5975,-2393.84 28.5975,-2378.47"
        },
        {
            "source": "1994.amta-1.18",
            "target": "1997.tmi-1.6",
            "d": "M725.512,-2422.74C740.127,-2374.53 771.363,-2271.48 787.868,-2217.03"
        },
        {
            "source": "1994.amta-1.23",
            "target": "1994.amta-1.18",
            "d": "M289.344,-2473.49C317.893,-2481.99 351.474,-2490.52 382.597,-2494.72 479.054,-2507.76 505.917,-2512.58 601.597,-2494.72 621.3,-2491.05 641.965,-2484.14 660.304,-2476.82"
        },
        {
            "source": "1994.amta-1.23",
            "target": "1997.mtsummit-workshop.7",
            "d": "M219.597,-2422.74C219.597,-2374.85 219.597,-2272.86 219.597,-2218.13"
        },
        {
            "source": "1995",
            "target": "1996",
            "d": "M28.5975,-2341.78C28.5975,-2326.42 28.5975,-2304.1 28.5975,-2288.73"
        },
        {
            "source": "1996",
            "target": "1997",
            "d": "M28.5975,-2252.04C28.5975,-2236.68 28.5975,-2214.36 28.5975,-2198.99"
        },
        {
            "source": "W96-0508",
            "target": "W96-0401",
            "d": "M1018.3,-2270.37C1020.95,-2270.37 1023.6,-2270.37 1026.25,-2270.37"
        },
        {
            "source": "1997",
            "target": "1998",
            "d": "M28.5975,-2162.3C28.5975,-2146.94 28.5975,-2124.62 28.5975,-2109.25"
        },
        {
            "source": "W97-0704",
            "target": "gerber-hovy-1998-improving",
            "d": "M5251.21,-2156.12C5271.41,-2144.97 5295.7,-2131.57 5316.85,-2119.9"
        },
        {
            "source": "W97-0704",
            "target": "C00-1072",
            "d": "M5196.19,-2153.52C5173.23,-2105.21 5124.09,-2001.83 5098.26,-1947.47"
        },
        {
            "source": "W97-0704",
            "target": "J02-4001",
            "d": "M5217.7,-2153.84C5228.94,-2119.63 5246.6,-2057.14 5246.6,-2002.15 5246.6,-2002.15 5246.6,-2002.15 5246.6,-1910.41 5246.6,-1858.34 5231.44,-1834.11 5265.6,-1794.8 5279.93,-1778.31 5325.15,-1763.56 5368.76,-1752.59"
        },
        {
            "source": "A97-1042",
            "target": "W97-0704",
            "d": "M5053.97,-2180.63C5068.45,-2180.63 5082.92,-2180.63 5097.4,-2180.63"
        },
        {
            "source": "A97-1042",
            "target": "X98-1026",
            "d": "M4975.5,-2154.25C4985.51,-2144.81 4997.01,-2133.96 5007.58,-2123.98"
        },
        {
            "source": "A97-1042",
            "target": "J02-4001",
            "d": "M5041.23,-2167.82C5078.89,-2159.12 5120.71,-2144 5151.6,-2117.76 5195.26,-2080.67 5208.6,-2059.44 5208.6,-2002.15 5208.6,-2002.15 5208.6,-2002.15 5208.6,-1910.41 5208.6,-1857.22 5202.12,-1832.56 5239.6,-1794.8 5249.5,-1784.82 5315.07,-1767.36 5372.73,-1753.52"
        },
        {
            "source": "A97-1042",
            "target": "P06-2063",
            "d": "M4939.97,-2153.8C4929.32,-2119.55 4912.6,-2057.02 4912.6,-2002.15 4912.6,-2002.15 4912.6,-2002.15 4912.6,-1641.19 4912.6,-1589.12 4930.43,-1562.39 4893.6,-1525.58 4823.51,-1455.55 4737.75,-1565.09 4673.6,-1489.58 4653.49,-1465.92 4670.47,-1432.38 4689.22,-1407.51"
        },
        {
            "source": "A97-1042",
            "target": "zhou-etal-2006-summarizing",
            "d": "M4843.18,-2178.78C4323.42,-2173.92 2058.6,-2144.55 2058.6,-2002.15 2058.6,-2002.15 2058.6,-2002.15 2058.6,-1551.45 2058.6,-1502.72 2058.6,-1446.47 2058.6,-1410.59"
        },
        {
            "source": "A97-1042",
            "target": "D19-1327",
            "d": "M4843.5,-2178.41C4411.49,-2173.22 2770.95,-2151.64 2253.6,-2117.76 2015.39,-2102.16 1185.6,-2240.87 1185.6,-2002.15 1185.6,-2002.15 1185.6,-2002.15 1185.6,-743.791 1185.6,-643.231 1218.23,-617.323 1280.6,-538.441 1336.69,-467.495 1378.28,-478.25 1440.6,-412.701 1460.75,-391.499 1453.87,-374.592 1478.6,-358.96 1557.53,-309.061 1619.87,-389.297 1685.6,-322.96 1705.98,-302.387 1709.57,-268.936 1708.74,-243.296"
        },
        {
            "source": "1998",
            "target": "1999",
            "d": "M28.5975,-2072.56C28.5975,-2057.2 28.5975,-2034.88 28.5975,-2019.51"
        },
        {
            "source": "X98-1026",
            "target": "gerber-hovy-1998-improving",
            "d": "M5142.84,-2090.89C5183.39,-2090.89 5223.95,-2090.89 5264.5,-2090.89"
        },
        {
            "source": "1999",
            "target": "2000",
            "d": "M28.5975,-1982.82C28.5975,-1967.46 28.5975,-1945.14 28.5975,-1929.77"
        },
        {
            "source": "2000",
            "target": "2001",
            "d": "M28.5975,-1893.08C28.5975,-1877.72 28.5975,-1855.4 28.5975,-1840.03"
        },
        {
            "source": "C00-1072",
            "target": "P02-1058",
            "d": "M5108.44,-1886.02C5120.4,-1874.87 5134.48,-1861.31 5146.6,-1848.54 5160.69,-1833.69 5193.98,-1794.47 5218.08,-1765.82"
        },
        {
            "source": "C00-1072",
            "target": "C02-1130",
            "d": "M5073.6,-1884.61C5064.11,-1853.98 5048.23,-1802.75 5037.59,-1768.41"
        },
        {
            "source": "C00-1072",
            "target": "N03-1037",
            "d": "M5003.08,-1909.36C4588.29,-1903.36 2662.03,-1869.66 2422.6,-1758.8 2384.71,-1741.26 2353.31,-1704.31 2333.89,-1676.88"
        },
        {
            "source": "C00-1072",
            "target": "N10-2002",
            "d": "M5090.23,-1884.41C5103.24,-1845.36 5128.79,-1769.31 5151.6,-1705.06 5175.63,-1637.37 5207.6,-1625.28 5207.6,-1553.45 5207.6,-1553.45 5207.6,-1553.45 5207.6,-1192.49 5207.6,-1135.97 5222.31,-1118.46 5260.6,-1076.88 5273.18,-1063.21 5289.5,-1051.81 5305.69,-1042.67"
        },
        {
            "source": "C00-1072",
            "target": "D19-1327",
            "d": "M5003.19,-1909.54C4606.63,-1904.98 2831.2,-1882.81 2587.6,-1848.54 2385.67,-1820.14 2143.6,-1936.85 2143.6,-1732.93 2143.6,-1732.93 2143.6,-1732.93 2143.6,-1641.19 2143.6,-1608.3 2187.92,-1371.97 2167.6,-1346.1 2125.77,-1292.88 2081.06,-1338.48 2019.6,-1310.1 1957.83,-1281.59 1928.42,-1279.95 1895.6,-1220.36 1755.07,-965.243 1854.6,-857.571 1854.6,-566.311 1854.6,-566.311 1854.6,-566.311 1854.6,-384.831 1854.6,-319.831 1795.18,-266.349 1751.14,-235.633"
        },
        {
            "source": "2001",
            "target": "2002",
            "d": "M28.5975,-1803.34C28.5975,-1787.98 28.5975,-1765.66 28.5975,-1750.29"
        },
        {
            "source": "2002",
            "target": "2003",
            "d": "M28.5975,-1713.6C28.5975,-1698.24 28.5975,-1675.92 28.5975,-1660.55"
        },
        {
            "source": "W02-0406",
            "target": "J02-4001",
            "d": "M1965.99,-1741.14C2074.54,-1752.2 2266.6,-1770.11 2431.6,-1776.8 2592.52,-1783.33 5171.67,-1802.89 5330.6,-1776.8 5353.01,-1773.12 5376.74,-1766.17 5397.85,-1758.82"
        },
        {
            "source": "W02-0406",
            "target": "W03-0510",
            "d": "M1856.75,-1705.55C1848.41,-1696.29 1838.85,-1685.68 1830.02,-1675.87"
        },
        {
            "source": "W02-0406",
            "target": "N03-1020",
            "d": "M1818.71,-1711.79C1776.29,-1698.44 1719.47,-1680.57 1674.45,-1666.42"
        },
        {
            "source": "W02-0406",
            "target": "hovy-etal-2006-automated",
            "d": "M1883.73,-1704.81C1885.28,-1693.81 1886.84,-1680.84 1887.6,-1669.06 1893.69,-1574.46 1866.66,-1464.65 1850.67,-1409.48"
        },
        {
            "source": "P02-1006",
            "target": "C02-1042",
            "d": "M2592.12,-1713.51C2609.46,-1711.37 2626.79,-1711.21 2644.13,-1713.06"
        },
        {
            "source": "P02-1006",
            "target": "P03-1001",
            "d": "M2589.45,-1712.61C2601.4,-1709.72 2613.82,-1707.04 2625.6,-1705.06 2857.74,-1666.04 3134.63,-1651.3 3278.28,-1645.99"
        },
        {
            "source": "P02-1006",
            "target": "H05-1075",
            "d": "M2536.22,-1704.82C2557.74,-1656.51 2603.78,-1553.13 2627.98,-1498.77"
        },
        {
            "source": "P02-1006",
            "target": "W08-0628",
            "d": "M2516.07,-1705.07C2490.4,-1629.87 2408.05,-1407.07 2286.6,-1256.36 2277.91,-1245.58 2267.26,-1235.05 2256.96,-1225.83"
        },
        {
            "source": "P02-1006",
            "target": "I08-2124",
            "d": "M2504.83,-1705.67C2496.61,-1694.69 2487.23,-1681.52 2479.6,-1669.06 2442.22,-1608.06 2450.6,-1582 2406.6,-1525.58 2331.92,-1429.83 2307.97,-1402.82 2200.6,-1346.1 2144.55,-1316.5 2112.14,-1349.64 2062.6,-1310.1 2037.32,-1289.93 2021.56,-1256.21 2012.57,-1230.38"
        },
        {
            "source": "P02-1006",
            "target": "P11-1162",
            "d": "M2588.28,-1712.34C2600.57,-1709.4 2613.41,-1706.77 2625.6,-1705.06 2875.53,-1669.98 4704,-1745.92 4893.6,-1579.32 5071.09,-1423.36 4732.41,-1259.31 4670.6,-1040.88 4664.09,-1017.9 4656.58,-1006.48 4670.6,-987.141 4691.53,-958.264 4726.34,-942.617 4760.43,-934.226"
        },
        {
            "source": "P02-1058",
            "target": "J02-4001",
            "d": "M5330.75,-1731.93C5333.2,-1731.93 5335.64,-1731.93 5338.09,-1731.93"
        },
        {
            "source": "P02-1058",
            "target": "P03-2021",
            "d": "M5281.51,-1707.42C5297.62,-1696.87 5316.8,-1684.31 5333.92,-1673.1"
        },
        {
            "source": "P02-1058",
            "target": "N10-2002",
            "d": "M5245.6,-1704.77C5245.6,-1670.15 5245.6,-1607.22 5245.6,-1553.45 5245.6,-1553.45 5245.6,-1553.45 5245.6,-1192.49 5245.6,-1132.68 5293.64,-1078.81 5330.16,-1046.51"
        },
        {
            "source": "C02-1042",
            "target": "P02-1006",
            "d": "M2654.34,-1749.49C2637,-1752.12 2619.66,-1752.75 2602.33,-1751.39"
        },
        {
            "source": "C02-1042",
            "target": "H05-1075",
            "d": "M2708.56,-1704.82C2695.6,-1656.71 2667.92,-1553.99 2653.23,-1499.45"
        },
        {
            "source": "C02-1130",
            "target": "P03-1001",
            "d": "M4948.61,-1730.1C4675.33,-1726.84 3761.92,-1712.85 3472.6,-1669.06 3464.02,-1667.76 3455.11,-1665.99 3446.33,-1663.98"
        },
        {
            "source": "C02-1130",
            "target": "P08-1119",
            "d": "M5017.5,-1705.14C5006.25,-1670.93 4988.6,-1608.44 4988.6,-1553.45 4988.6,-1553.45 4988.6,-1553.45 4988.6,-1371.97 4988.6,-1316.91 4953.48,-1261.77 4926.88,-1227.9"
        },
        {
            "source": "C02-1130",
            "target": "D09-1099",
            "d": "M5026.6,-1704.77C5026.6,-1670.15 5026.6,-1607.22 5026.6,-1553.45 5026.6,-1553.45 5026.6,-1553.45 5026.6,-1282.23 5026.6,-1233.18 5021.98,-1176.68 5018.47,-1140.85"
        },
        {
            "source": "C02-1130",
            "target": "P10-1150",
            "d": "M5063.05,-1707.87C5105.42,-1677.9 5169.6,-1621.31 5169.6,-1553.45 5169.6,-1553.45 5169.6,-1553.45 5169.6,-1192.49 5169.6,-1142.81 5157.67,-1086.46 5148.61,-1050.84"
        },
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1623.86C28.5975,-1608.5 28.5975,-1586.18 28.5975,-1570.81"
        },
        {
            "source": "W03-1209",
            "target": "N06-1026",
            "d": "M3576.44,-1614.91C3575.83,-1588.67 3579.32,-1548.98 3602.6,-1525.58 3656.76,-1471.14 3717.89,-1544.47 3771.6,-1489.58 3791.82,-1468.92 3796.41,-1435.77 3796.47,-1410.26"
        },
        {
            "source": "W03-1007",
            "target": "W04-0832",
            "d": "M3777.69,-1615.34C3772.36,-1606.68 3766.33,-1596.89 3760.67,-1587.7"
        },
        {
            "source": "W03-1007",
            "target": "C04-1179",
            "d": "M3845.08,-1619.05C3871.59,-1607.65 3904.19,-1593.63 3932.44,-1581.48"
        },
        {
            "source": "W03-1007",
            "target": "I05-7009",
            "d": "M3725.78,-1622.47C3688.78,-1611.45 3642.51,-1596.35 3602.6,-1579.32 3542.19,-1553.54 3476.13,-1516.61 3433.39,-1491.47"
        },
        {
            "source": "W03-1007",
            "target": "W06-0301",
            "d": "M3888.55,-1633.86C3979.89,-1625.22 4110.06,-1608.25 4151.6,-1579.32 4167.34,-1568.36 4223.15,-1464.68 4252.4,-1408.98"
        },
        {
            "source": "W03-1007",
            "target": "P11-1147",
            "d": "M3724.72,-1622.61C3687.28,-1610.97 3646.43,-1595.2 3635.6,-1579.32 3622.14,-1559.59 3620.65,-1544.21 3635.6,-1525.58 3681.75,-1468.05 3731.45,-1522.21 3797.6,-1489.58 3853.26,-1462.12 3858.37,-1441.26 3904.6,-1399.84 3973.31,-1338.28 3986.36,-1318.45 4054.6,-1256.36 4162.84,-1157.87 4242.76,-1173.99 4303.6,-1040.88 4313.53,-1019.16 4314.12,-1008.58 4303.6,-987.141 4297.37,-974.458 4287.16,-963.635 4276.07,-954.756"
        },
        {
            "source": "P03-1001",
            "target": "W04-2709",
            "d": "M3309.32,-1624.48C3252.72,-1610.21 3171.04,-1589.62 3110.19,-1574.28"
        },
        {
            "source": "P03-1001",
            "target": "W04-0701",
            "d": "M3436.21,-1622.75C3448.17,-1619.76 3460.7,-1617.07 3472.6,-1615.32 3980.28,-1540.72 4116.61,-1636.14 4626.6,-1579.32 4639.05,-1577.94 4652.07,-1576.02 4664.9,-1573.85"
        },
        {
            "source": "P03-1001",
            "target": "C04-1111",
            "d": "M3436.48,-1622.75C3448.36,-1619.78 3460.79,-1617.1 3472.6,-1615.32 3889.98,-1552.55 4003.76,-1639.05 4421.6,-1579.32 4430.88,-1578 4440.53,-1576.18 4450.05,-1574.11"
        },
        {
            "source": "P03-1001",
            "target": "I05-7009",
            "d": "M3377.37,-1615C3379.43,-1584.51 3382.85,-1533.96 3385.16,-1499.76"
        },
        {
            "source": "P03-1001",
            "target": "H05-1075",
            "d": "M3289.68,-1636.6C3196.04,-1630 3042.49,-1614.49 2914.6,-1579.32 2835.8,-1557.66 2749.81,-1518.01 2696.3,-1491.26"
        },
        {
            "source": "N03-1020",
            "target": "W03-0510",
            "d": "M1704.89,-1642.19C1707.22,-1642.19 1709.55,-1642.19 1711.88,-1642.19"
        },
        {
            "source": "N03-1020",
            "target": "W04-3256",
            "d": "M1673.9,-1623.1C1687.06,-1620.22 1700.7,-1617.49 1713.6,-1615.32 1869.54,-1589.12 2051.41,-1571.63 2167.23,-1562.13"
        },
        {
            "source": "N03-1020",
            "target": "W04-1010",
            "d": "M1582.92,-1615.34C1576.94,-1606.6 1570.17,-1596.69 1563.82,-1587.41"
        },
        {
            "source": "N03-1020",
            "target": "W06-1610",
            "d": "M1637.13,-1617.02C1649.17,-1606.94 1661.12,-1594.12 1667.6,-1579.32 1692.6,-1522.2 1662.73,-1450.3 1639.88,-1408.5"
        },
        {
            "source": "N03-1020",
            "target": "N06-1057",
            "d": "M1505.49,-1631.34C1471.42,-1622.88 1435.94,-1607.43 1413.6,-1579.32 1375.23,-1531.07 1379.88,-1454.82 1387.04,-1410.07"
        },
        {
            "source": "N03-1020",
            "target": "hovy-etal-2006-automated",
            "d": "M1639.39,-1617.03C1654.92,-1606.44 1672.43,-1593.25 1686.6,-1579.32 1741.2,-1525.63 1791.59,-1451.22 1818.72,-1408.29"
        },
        {
            "source": "N03-1020",
            "target": "N07-2055",
            "d": "M1507.26,-1630.17C1466.61,-1621.61 1420.4,-1606.38 1385.6,-1579.32 1312.64,-1522.6 1304.77,-1489.04 1280.6,-1399.84 1274.35,-1376.79 1275.72,-1369.48 1280.6,-1346.1 1282.53,-1336.83 1285.93,-1327.25 1289.68,-1318.48"
        },
        {
            "source": "N03-1020",
            "target": "D17-1082",
            "d": "M1499.07,-1635.94C1387.41,-1624.04 1223.6,-1585.79 1223.6,-1463.71 1223.6,-1463.71 1223.6,-1463.71 1223.6,-564.311 1223.6,-504.712 1176.16,-450.691 1140.21,-418.304"
        },
        {
            "source": "N03-1020",
            "target": "D19-1179",
            "d": "M1671.94,-1622.55C1685.68,-1619.62 1700.02,-1617.01 1713.6,-1615.32 1896.13,-1592.68 3193.25,-1634.87 3368.6,-1579.32 3456.41,-1551.51 3540.6,-1555.82 3540.6,-1463.71 3540.6,-1463.71 3540.6,-1463.71 3540.6,-1282.23 3540.6,-1165.95 3609.51,-1152.65 3641.6,-1040.88 3655.97,-990.832 3660.6,-977.343 3660.6,-925.271 3660.6,-925.271 3660.6,-925.271 3660.6,-833.531 3660.6,-781.46 3661.96,-766.916 3679.6,-717.921 3685.87,-700.492 3695.76,-699.802 3700.6,-681.921 3706.83,-658.864 3710.35,-649.983 3700.6,-628.181 3641.93,-497.023 3518.27,-543.858 3459.6,-412.701 3432.45,-352.016 3481.34,-280.723 3516.76,-240.107"
        },
        {
            "source": "N03-1037",
            "target": "W04-3256",
            "d": "M2304.35,-1615.34C2301.72,-1607.11 2298.77,-1597.86 2295.97,-1589.07"
        },
        {
            "source": "2003.mtsummit-papers.12",
            "target": "2003.mtsummit-papers.12",
            "d": "M5967.32,-1658.68C5986.4,-1658.34 6001.38,-1652.85 6001.38,-1642.19 6001.38,-1633.53 5991.49,-1628.28 5977.55,-1626.44"
        },
        {
            "source": "2004",
            "target": "2005",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "W04-3256",
            "target": "H05-1075",
            "d": "M2361.61,-1532.63C2420.75,-1518.18 2502.2,-1498.27 2562.36,-1483.57"
        },
        {
            "source": "W04-2709",
            "target": "reeder-etal-2004-interlingual",
            "d": "M3131.79,-1552.45C3134.13,-1552.45 3136.46,-1552.45 3138.8,-1552.45"
        },
        {
            "source": "W04-2709",
            "target": "I05-7009",
            "d": "M3101.32,-1533.48C3164.48,-1518.09 3254.86,-1496.06 3317.08,-1480.9"
        },
        {
            "source": "W04-2709",
            "target": "rambow-etal-2006-parallel",
            "d": "M3001.34,-1526.4C2968.15,-1494.73 2911,-1440.19 2874.81,-1405.66"
        },
        {
            "source": "C04-1200",
            "target": "W06-0301",
            "d": "M4300.43,-1525.26C4294.42,-1494.77 4284.45,-1444.22 4277.71,-1410.02"
        },
        {
            "source": "C04-1200",
            "target": "P06-2063",
            "d": "M4380.83,-1533.18C4394.66,-1530.29 4409.02,-1527.59 4422.6,-1525.58 4474.52,-1517.9 4857.93,-1528.08 4893.6,-1489.58 4909.83,-1472.06 4907.37,-1455.36 4893.6,-1435.84 4876.41,-1411.48 4848.75,-1396.66 4820.49,-1387.65"
        },
        {
            "source": "C04-1200",
            "target": "D07-1113",
            "d": "M4221.5,-1535.67C4198.87,-1531.88 4174.36,-1528.18 4151.6,-1525.58 4020.65,-1510.64 3685.33,-1529.1 3559.6,-1489.58 3515.88,-1475.84 3511.35,-1458.64 3471.6,-1435.84 3393.27,-1390.91 3300.79,-1341.95 3243.66,-1312.17"
        },
        {
            "source": "C04-1200",
            "target": "D14-1053",
            "d": "M4329.07,-1525.99C4356.62,-1493.35 4398.6,-1433.8 4398.6,-1373.97 4398.6,-1373.97 4398.6,-1373.97 4398.6,-833.531 4398.6,-784.482 4393.98,-727.982 4390.47,-692.154"
        },
        {
            "source": "C04-1200",
            "target": "2021.acl-long.185",
            "d": "M4236.09,-1531.94C4166.16,-1507.55 4068.6,-1458.4 4068.6,-1373.97 4068.6,-1373.97 4068.6,-1373.97 4068.6,-1102.75 4068.6,-1009.22 4071.5,-980.41 4114.6,-897.401 4154.07,-821.376 4233.6,-831.451 4233.6,-745.791 4233.6,-745.791 4233.6,-745.791 4233.6,-205.35 4233.6,-85.7153 3837.11,-45.3884 3647.43,-32.9411"
        },
        {
            "source": "reeder-etal-2004-interlingual",
            "target": "I05-7009",
            "d": "M3291.29,-1527.24C3307.66,-1516.45 3327.07,-1503.64 3344.16,-1492.37"
        },
        {
            "source": "reeder-etal-2004-interlingual",
            "target": "N06-2015",
            "d": "M3227.35,-1526.4C3193.11,-1494.91 3134.28,-1440.81 3096.69,-1406.25"
        },
        {
            "source": "2005",
            "target": "2006",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "W05-1520",
            "target": "W05-1520",
            "d": "M3725.97,-1479.17C3746.41,-1478.64 3762.28,-1473.15 3762.28,-1462.71 3762.28,-1454.07 3751.4,-1448.82 3736.04,-1446.97"
        },
        {
            "source": "W05-1520",
            "target": "hartholt-etal-2008-common",
            "d": "M3657.09,-1435.6C3657.98,-1387.71 3659.89,-1285.72 3660.91,-1230.99"
        },
        {
            "source": "P05-1037",
            "target": "H05-2003",
            "d": "M5456.21,-1462.71C5458.66,-1462.71 5461.12,-1462.71 5463.57,-1462.71"
        },
        {
            "source": "P05-1037",
            "target": "N06-1027",
            "d": "M5370.6,-1435.39C5370.6,-1427.42 5370.6,-1418.53 5370.6,-1410.04"
        },
        {
            "source": "I05-7009",
            "target": "N06-2015",
            "d": "M3328.94,-1445.92C3278.08,-1432.24 3203.84,-1412.25 3146.49,-1396.82"
        },
        {
            "source": "I05-7009",
            "target": "hartholt-etal-2008-common",
            "d": "M3412.88,-1437.05C3463.12,-1388.06 3575.35,-1278.6 3630.2,-1225.11"
        },
        {
            "source": "I05-7009",
            "target": "I08-1048",
            "d": "M3380.07,-1435.6C3366.21,-1387.49 3336.61,-1284.77 3320.9,-1230.23"
        },
        {
            "source": "I05-2011",
            "target": "P06-2063",
            "d": "M4764.74,-1435.86C4758.36,-1427.11 4751.14,-1417.21 4744.37,-1407.93"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "W06-0301",
            "target": "D14-1053",
            "d": "M4293.08,-1346.66C4319.73,-1313.95 4360.6,-1254.09 4360.6,-1194.49 4360.6,-1194.49 4360.6,-1194.49 4360.6,-833.531 4360.6,-784.072 4370.6,-727.67 4378.2,-691.977"
        },
        {
            "source": "N06-2015",
            "target": "D07-1082",
            "d": "M3030.36,-1346.83C3017.96,-1336.85 3003.59,-1325.28 2990.59,-1314.82"
        },
        {
            "source": "N06-2015",
            "target": "O08-6002",
            "d": "M2976.8,-1353.56C2930.66,-1342.04 2879.61,-1326.34 2862.6,-1310.1 2840.73,-1289.24 2830.21,-1256.12 2825.17,-1230.67"
        },
        {
            "source": "N06-2015",
            "target": "I08-1048",
            "d": "M3161.68,-1357.47C3217.88,-1347.18 3280.31,-1331.4 3298.6,-1310.1 3317,-1288.67 3318.86,-1256.1 3316.77,-1230.99"
        },
        {
            "source": "N06-2015",
            "target": "C08-1133",
            "d": "M2977.5,-1353.35C2932.71,-1342.5 2877.03,-1327.54 2828.6,-1310.1 2759.33,-1285.16 2682.92,-1248.58 2632.82,-1223.28"
        },
        {
            "source": "N06-2015",
            "target": "C08-1142",
            "d": "M3062.63,-1345.78C3063.83,-1315.29 3065.83,-1264.74 3067.18,-1230.54"
        },
        {
            "source": "N06-2015",
            "target": "P10-1144",
            "d": "M3169.93,-1360.49C3218.97,-1351.82 3276.22,-1336.64 3322.6,-1310.1 3387.13,-1273.18 3450.58,-1227.98 3408.6,-1166.62 3345.37,-1074.21 3216.9,-1038.12 3124.68,-1024.03"
        },
        {
            "source": "N06-2015",
            "target": "P14-2005",
            "d": "M2973.96,-1354.16C2959.49,-1351.36 2944.64,-1348.58 2930.6,-1346.1 2697.31,-1304.91 2419.6,-1431.39 2419.6,-1194.49 2419.6,-1194.49 2419.6,-1194.49 2419.6,-833.531 2419.6,-761.241 2345.17,-709.523 2290.22,-681.194"
        },
        {
            "source": "N06-2015",
            "target": "C14-1123",
            "d": "M2976.59,-1353.6C2961.33,-1350.76 2945.53,-1348.1 2930.6,-1346.1 2875.06,-1338.66 2474.03,-1341.47 2427.6,-1310.1 2368.8,-1270.38 2408.46,-1216.09 2357.6,-1166.62 2260.68,-1072.36 2162.83,-1148.2 2080.6,-1040.88 2048.93,-999.548 2061.6,-977.343 2061.6,-925.271 2061.6,-925.271 2061.6,-925.271 2061.6,-833.531 2061.6,-780.725 2033.89,-725.135 2012.85,-690.537"
        },
        {
            "source": "N06-2015",
            "target": "N16-1116",
            "d": "M3173.42,-1361.85C3234.77,-1353.38 3311.06,-1337.97 3374.6,-1310.1 3467.39,-1269.4 4034.76,-838.855 4110.6,-771.661 4151.49,-735.433 4157.98,-722.46 4194.6,-681.921 4248.69,-622.04 4310.19,-550.759 4345.2,-509.874"
        },
        {
            "source": "N06-2015",
            "target": "S17-1025",
            "d": "M2976.59,-1353.54C2961.34,-1350.7 2945.54,-1348.06 2930.6,-1346.1 2870.76,-1338.24 2438.11,-1344.62 2388.6,-1310.1 2331.32,-1270.17 2379.75,-1209.44 2324.6,-1166.62 2249.45,-1108.29 2195.54,-1178.94 2113.6,-1130.62 1805.74,-949.091 1771.56,-822.034 1611.6,-502.441 1598.99,-477.242 1591.31,-446.22 1586.88,-422.657"
        },
        {
            "source": "N06-1026",
            "target": "P06-2063",
            "d": "M3873.21,-1389.75C3926.75,-1400.15 3999.07,-1412.52 4063.6,-1417.84 4199.58,-1429.06 4234.43,-1426.56 4370.6,-1417.84 4456.45,-1412.35 4553.46,-1399.32 4623.37,-1388.71"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "N07-1071",
            "target": "D07-1017",
            "d": "M5638.03,-1283.23C5640.63,-1283.23 5643.23,-1283.23 5645.83,-1283.23"
        },
        {
            "source": "D07-1082",
            "target": "O08-6002",
            "d": "M2917.57,-1258.95C2901.57,-1248.31 2882.43,-1235.59 2865.4,-1224.27"
        },
        {
            "source": "D07-1082",
            "target": "I08-1048",
            "d": "M3017.62,-1267.23C3070.27,-1254.96 3146.33,-1236.99 3212.6,-1220.36 3220.1,-1218.48 3227.91,-1216.48 3235.71,-1214.46"
        },
        {
            "source": "D07-1082",
            "target": "C08-1133",
            "d": "M2887.71,-1267.05C2827.96,-1253.07 2738.65,-1232.17 2671.2,-1216.39"
        },
        {
            "source": "D07-1082",
            "target": "C08-1142",
            "d": "M2984.29,-1258.26C2997.53,-1248.25 3013.08,-1236.48 3027.2,-1225.8"
        },
        {
            "source": "D07-1088",
            "target": "W08-0628",
            "d": "M2187.27,-1256.38C2191.42,-1247.9 2196.11,-1238.34 2200.54,-1229.31"
        },
        {
            "source": "D07-1088",
            "target": "I08-2124",
            "d": "M2128.96,-1258.95C2106.73,-1247.61 2079.85,-1233.9 2056.6,-1222.04"
        },
        {
            "source": "D07-1113",
            "target": "O08-6002",
            "d": "M3115.28,-1265.18C3062.12,-1252.91 2988.9,-1235.86 2924.6,-1220.36 2916.54,-1218.42 2908.13,-1216.37 2899.75,-1214.31"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "P08-1119",
            "target": "D09-1099",
            "d": "M4930.19,-1168.05C4943.75,-1157.88 4959.65,-1145.96 4973.96,-1135.22"
        },
        {
            "source": "P08-1119",
            "target": "P10-1150",
            "d": "M4992.17,-1186.68C5034.45,-1179.25 5081.62,-1163.48 5112.6,-1130.62 5132.48,-1109.53 5138.35,-1076.64 5139.61,-1051.33"
        },
        {
            "source": "P08-1119",
            "target": "N10-1087",
            "d": "M4875.88,-1167.08C4866.47,-1155.97 4855.39,-1142.73 4845.6,-1130.62 4823.51,-1103.32 4799.08,-1071.85 4781.24,-1048.64"
        },
        {
            "source": "P08-1119",
            "target": "D10-1108",
            "d": "M4898.83,-1166.44C4900.67,-1142.47 4905.24,-1106.29 4916.6,-1076.88 4920.36,-1067.13 4925.8,-1057.29 4931.46,-1048.42"
        },
        {
            "source": "P08-1119",
            "target": "P11-1162",
            "d": "M4893.88,-1166.38C4887.06,-1118.39 4872.51,-1016.07 4864.74,-961.429"
        },
        {
            "source": "P08-1119",
            "target": "D14-1214",
            "d": "M4828.09,-1174.63C4755.35,-1151.25 4651.6,-1102.56 4651.6,-1015.01 4651.6,-1015.01 4651.6,-1015.01 4651.6,-833.531 4651.6,-778.645 4685.56,-723.062 4711.04,-689.079"
        },
        {
            "source": "I08-2124",
            "target": "W08-0628",
            "d": "M2101.28,-1193.49C2103.77,-1193.49 2106.26,-1193.49 2108.75,-1193.49"
        },
        {
            "source": "I08-1048",
            "target": "C08-1142",
            "d": "M3221.3,-1193.49C3218.84,-1193.49 3216.37,-1193.49 3213.91,-1193.49"
        },
        {
            "source": "C08-1142",
            "target": "W12-1905",
            "d": "M2975.52,-1173.93C2851.04,-1147.39 2639.5,-1095.7 2583.6,-1040.88 2538.02,-996.194 2531.3,-917.424 2531.67,-871.521"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "D09-1099",
            "target": "P10-1150",
            "d": "M5048.48,-1078.78C5063.12,-1068.42 5080.42,-1056.18 5095.92,-1045.21"
        },
        {
            "source": "D09-1099",
            "target": "D10-1108",
            "d": "M4997.51,-1076.9C4991.68,-1068.08 4985.06,-1058.08 4978.88,-1048.73"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "W10-0903",
            "target": "P11-1147",
            "d": "M4203.85,-987.161C4206.47,-978.934 4209.42,-969.684 4212.23,-960.892"
        },
        {
            "source": "P10-5004",
            "target": "N13-1132",
            "d": "M3533.45,-986.903C3536.86,-939.009 3544.11,-837.016 3548,-782.291"
        },
        {
            "source": "P10-1070",
            "target": "D11-1116",
            "d": "M3306.68,-991.548C3285.4,-979.5 3258.67,-964.37 3236.13,-951.609"
        },
        {
            "source": "P10-1070",
            "target": "P13-1037",
            "d": "M3345.18,-986.903C3346.26,-939.009 3348.55,-837.016 3349.78,-782.291"
        },
        {
            "source": "P10-1070",
            "target": "N13-1132",
            "d": "M3363.98,-987.872C3401.25,-939.528 3482.91,-833.594 3524.76,-779.31"
        },
        {
            "source": "P10-1144",
            "target": "W13-1203",
            "d": "M2990.34,-988.168C2957.46,-958.859 2902,-908.186 2857.6,-861.401 2832.32,-834.769 2805.48,-802.819 2786.29,-779.262"
        },
        {
            "source": "P10-1150",
            "target": "P11-1162",
            "d": "M5076.68,-993.539C5033.45,-979.946 4975.66,-961.771 4930.57,-947.591"
        },
        {
            "source": "P10-1150",
            "target": "D14-1214",
            "d": "M5110.79,-988.194C5039.62,-924.682 4851.02,-756.375 4773.06,-686.809"
        },
        {
            "source": "N10-1087",
            "target": "P11-1162",
            "d": "M4784.02,-989.036C4795.97,-978.952 4810.03,-967.088 4822.76,-956.346"
        },
        {
            "source": "N10-1087",
            "target": "D14-1214",
            "d": "M4754.36,-987.052C4751.38,-924.486 4743.8,-765.198 4740.35,-692.828"
        },
        {
            "source": "recasens-etal-2010-typology",
            "target": "D18-1154",
            "d": "M2142.85,-986.998C2135.75,-952.541 2124.6,-889.788 2124.6,-835.531 2124.6,-835.531 2124.6,-835.531 2124.6,-474.571 2124.6,-377.283 2239.87,-332.772 2331.17,-312.775"
        },
        {
            "source": "C10-2052",
            "target": "P11-2056",
            "d": "M2675.6,-986.686C2675.6,-978.72 2675.6,-969.829 2675.6,-961.34"
        },
        {
            "source": "C10-2052",
            "target": "D11-1116",
            "d": "M2748.15,-1000.63C2846.24,-983.883 3020.09,-954.206 3117.79,-937.529"
        },
        {
            "source": "C10-2052",
            "target": "W12-1905",
            "d": "M2636.38,-990.293C2620.72,-979.835 2603.41,-966.36 2590.6,-951.141 2570.58,-927.37 2555.51,-895.369 2545.91,-871.063"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "P11-2056",
            "target": "W12-1905",
            "d": "M2639.03,-900.676C2621.07,-889.58 2599.24,-876.088 2580.1,-864.267"
        },
        {
            "source": "D11-1116",
            "target": "W13-3203",
            "d": "M3163.57,-899.335C3128.99,-867.535 3067.86,-811.321 3029.94,-776.449"
        },
        {
            "source": "D11-1116",
            "target": "W13-0907",
            "d": "M3242.47,-907.286C3348.27,-875.296 3586.12,-803.383 3705.46,-767.3"
        },
        {
            "source": "D11-1116",
            "target": "P13-2083",
            "d": "M3181.45,-897.464C3171.79,-866.838 3155.63,-815.608 3144.79,-781.269"
        },
        {
            "source": "D11-1116",
            "target": "P13-1037",
            "d": "M3211.98,-898.593C3240.34,-867.335 3289.29,-813.37 3320.79,-778.649"
        },
        {
            "source": "D11-1116",
            "target": "D13-1144",
            "d": "M3250.02,-911.993C3369.87,-889.159 3645.77,-834.275 3873.6,-771.661 3879.95,-769.914 3886.56,-768.01 3893.14,-766.054"
        },
        {
            "source": "D11-1116",
            "target": "liu-etal-2014-supervised",
            "d": "M3124.75,-916.155C3056.94,-907.571 2948,-890.534 2857.6,-861.401 2768.33,-832.634 2722.84,-846.722 2666.6,-771.661 2649.65,-749.044 2645.81,-716.865 2645.83,-692.197"
        },
        {
            "source": "D11-1116",
            "target": "araki-etal-2014-detecting",
            "d": "M3132.88,-909.425C3069.32,-890.446 2969.79,-849.468 2928.6,-771.661 2915.52,-746.95 2921.19,-715.289 2929.19,-691.35"
        },
        {
            "source": "D11-1116",
            "target": "C14-1066",
            "d": "M3198.62,-897.475C3211.15,-857.055 3229.34,-777.288 3201.6,-717.921 3195.66,-705.225 3185.7,-694.218 3174.98,-685.152"
        },
        {
            "source": "D11-1116",
            "target": "W15-0802",
            "d": "M3257.18,-921.204C3404.85,-914.585 3765.07,-886.902 4039.6,-771.661 4106.71,-743.487 4141.99,-745.958 4176.6,-681.921 4187.95,-660.908 4184.88,-650.583 4176.6,-628.181 4172.58,-617.305 4165.69,-607.05 4158.2,-598.133"
        },
        {
            "source": "D11-1116",
            "target": "C18-1309",
            "d": "M3122.39,-920.148C3070.85,-914.611 3000.49,-899.895 2951.6,-861.401 2880.52,-805.439 2884.66,-767.59 2855.6,-681.921 2838.87,-632.609 2836.6,-618.382 2836.6,-566.311 2836.6,-566.311 2836.6,-566.311 2836.6,-474.571 2836.6,-424.893 2824.67,-368.544 2815.61,-332.922"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "W12-6102",
            "target": "R13-1030",
            "d": "M5194.6,-807.206C5194.6,-799.239 5194.6,-790.348 5194.6,-781.86"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "W13-3203",
            "target": "P13-2083",
            "d": "M3056,-744.791C3058.65,-744.791 3061.3,-744.791 3063.96,-744.791"
        },
        {
            "source": "W13-3203",
            "target": "P14-1060",
            "d": "M3038.69,-725.721C3047.44,-722.618 3056.71,-719.794 3065.6,-717.921 3342.2,-659.639 3419.92,-715.493 3700.6,-681.921 3718.93,-679.729 3738.47,-676.635 3756.99,-673.347"
        },
        {
            "source": "W13-1203",
            "target": "P13-2083",
            "d": "M2812.72,-765.785C2872.53,-786.45 2972.24,-811.926 3055.6,-789.661 3068.71,-786.159 3081.9,-779.868 3093.61,-773.084"
        },
        {
            "source": "W13-1203",
            "target": "W14-2910",
            "d": "M2832.2,-731.2C2861.99,-726.419 2896.87,-721.313 2928.6,-717.921 3188.94,-690.083 3258.44,-725.606 3516.6,-681.921 3524.3,-680.617 3532.29,-678.917 3540.19,-677.01"
        },
        {
            "source": "W13-1203",
            "target": "liu-etal-2014-supervised",
            "d": "M2728.97,-719.583C2716.43,-709.67 2701.75,-698.068 2688.4,-687.51"
        },
        {
            "source": "W13-1203",
            "target": "araki-etal-2014-detecting",
            "d": "M2805.33,-722.102C2831.08,-709.891 2863.38,-694.569 2890.4,-681.756"
        },
        {
            "source": "W13-1203",
            "target": "C18-1309",
            "d": "M2768.94,-718.019C2780.48,-683.826 2798.6,-621.361 2798.6,-566.311 2798.6,-566.311 2798.6,-566.311 2798.6,-474.571 2798.6,-425.595 2801.29,-369.077 2803.34,-333.226"
        },
        {
            "source": "W13-0907",
            "target": "D13-1144",
            "d": "M3865.19,-744.791C3867.79,-744.791 3870.4,-744.791 3873,-744.791"
        },
        {
            "source": "W13-0907",
            "target": "W14-2303",
            "d": "M3839.01,-725.718C3887.92,-711.556 3956.04,-691.831 4007.31,-676.983"
        },
        {
            "source": "W13-0907",
            "target": "P14-1060",
            "d": "M3795.4,-718.413C3802.01,-709.488 3809.55,-699.313 3816.59,-689.808"
        },
        {
            "source": "W13-0907",
            "target": "W17-5538",
            "d": "M3841.18,-726.207C3881.9,-713.977 3929.54,-697.192 3944.6,-681.921 3981.77,-644.226 4022.63,-492.867 4040.01,-422.779"
        },
        {
            "source": "P13-2083",
            "target": "liu-etal-2014-supervised",
            "d": "M3090.61,-726.085C3082.1,-723.031 3073.14,-720.132 3064.6,-717.921 3009.27,-703.606 2866.06,-683.664 2763.13,-670.329"
        },
        {
            "source": "P13-2083",
            "target": "araki-etal-2014-detecting",
            "d": "M3093.05,-724.969C3065.84,-712.337 3029.66,-695.54 2999.83,-681.691"
        },
        {
            "source": "P13-2083",
            "target": "C14-1066",
            "d": "M3131.8,-717.466C3131.25,-709.499 3130.64,-700.608 3130.06,-692.12"
        },
        {
            "source": "P13-2083",
            "target": "W15-0802",
            "d": "M3165.36,-721.804C3182.36,-710.01 3203.65,-695.209 3222.6,-681.921 3256.47,-658.172 3259.67,-642.178 3298.6,-628.181 3431.23,-580.49 3845.4,-569.559 4031.46,-567.055"
        },
        {
            "source": "N13-1132",
            "target": "W13-0907",
            "d": "M3630.93,-744.791C3646.57,-744.791 3662.21,-744.791 3677.84,-744.791"
        },
        {
            "source": "N13-1132",
            "target": "C14-1134",
            "d": "M3513.4,-720.967C3495.53,-710.003 3473.91,-696.741 3454.89,-685.069"
        },
        {
            "source": "N13-1132",
            "target": "P19-1461",
            "d": "M3492.34,-726.265C3482.13,-723.361 3471.58,-720.47 3461.6,-717.921 3389.72,-699.557 3356.98,-727.704 3298.6,-681.921 3253.52,-646.568 3241.6,-623.599 3241.6,-566.311 3241.6,-566.311 3241.6,-566.311 3241.6,-384.831 3241.6,-336.103 3241.6,-279.851 3241.6,-243.965"
        },
        {
            "source": "D13-1144",
            "target": "W14-2910",
            "d": "M3902.74,-726.38C3893.07,-723.434 3883.06,-720.498 3873.6,-717.921 3811.29,-700.96 3739.97,-684.473 3687.02,-672.788"
        },
        {
            "source": "D13-1144",
            "target": "P14-1060",
            "d": "M3925.78,-720.278C3912.32,-710.009 3896.36,-697.829 3881.97,-686.85"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "P14-2005",
            "target": "P14-2006",
            "d": "M2312.58,-655.051C2315.05,-655.051 2317.52,-655.051 2319.99,-655.051"
        },
        {
            "source": "P14-1016",
            "target": "D14-1214",
            "d": "M4649.55,-655.051C4652,-655.051 4654.44,-655.051 4656.89,-655.051"
        },
        {
            "source": "liu-etal-2014-supervised",
            "target": "W16-6005",
            "d": "M2638.86,-628.244C2627.29,-597.618 2607.95,-546.388 2594.99,-512.048"
        },
        {
            "source": "liu-etal-2014-supervised",
            "target": "D18-1154",
            "d": "M2600.55,-630.141C2555.7,-605.006 2491.08,-560.973 2459.6,-502.441 2430.85,-448.988 2431.84,-376.281 2435.79,-333.259"
        },
        {
            "source": "liu-etal-2014-supervised",
            "target": "C18-1309",
            "d": "M2660.01,-628.092C2687.71,-565.115 2758.51,-404.145 2790.04,-332.456"
        },
        {
            "source": "araki-etal-2014-detecting",
            "target": "D18-1154",
            "d": "M2918.79,-629.28C2876.03,-588.893 2786.92,-507.475 2703.6,-448.701 2635.61,-400.744 2551.46,-354.279 2496.94,-325.704"
        },
        {
            "source": "araki-etal-2014-detecting",
            "target": "C18-1309",
            "d": "M2937.48,-628.068C2920.13,-565.448 2875.31,-407.771 2850.6,-358.96 2845.69,-349.262 2839.32,-339.441 2832.92,-330.567"
        },
        {
            "source": "araki-etal-2014-detecting",
            "target": "2020.blackboxnlp-1.1",
            "d": "M2944.6,-627.888C2944.6,-593.269 2944.6,-530.339 2944.6,-476.571 2944.6,-476.571 2944.6,-476.571 2944.6,-295.09 2944.6,-246.363 2944.6,-190.111 2944.6,-154.225"
        },
        {
            "source": "D14-1218",
            "target": "D15-1278",
            "d": "M618.458,-630.075C627.392,-620.277 637.859,-608.798 647.43,-598.3"
        },
        {
            "source": "D14-1220",
            "target": "D15-1278",
            "d": "M734.169,-628.673C725.456,-619.235 715.454,-608.4 706.256,-598.438"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "W15-0802",
            "target": "D15-1154",
            "d": "M4205.88,-565.311C4281.2,-565.311 4356.52,-565.311 4431.84,-565.311"
        },
        {
            "source": "W15-0802",
            "target": "I17-1007",
            "d": "M4144.54,-538.859C4168.01,-512.119 4208.57,-470.922 4252.6,-448.701 4317.6,-415.896 4398.92,-400.483 4460.34,-393.242"
        },
        {
            "source": "W15-0802",
            "target": "P18-1130",
            "d": "M4130.72,-538.273C4138.72,-512.966 4153.78,-474.654 4177.6,-448.701 4231.52,-389.935 4313.59,-347.669 4371.69,-322.93"
        },
        {
            "source": "N15-1070",
            "target": "D15-1284",
            "d": "M1195.96,-565.311C1223.78,-565.311 1251.61,-565.311 1279.44,-565.311"
        },
        {
            "source": "N15-1070",
            "target": "S17-1025",
            "d": "M1151.1,-543.079C1242.64,-509.854 1419.19,-445.775 1515.31,-410.89"
        },
        {
            "source": "N15-1070",
            "target": "P17-1191",
            "d": "M1067.03,-538.911C1040.73,-513.736 997.755,-475.201 955.597,-448.701 934.494,-435.435 910.064,-423.499 887.677,-413.732"
        },
        {
            "source": "N15-1184",
            "target": "N15-1070",
            "d": "M971.41,-565.311C973.873,-565.311 976.336,-565.311 978.799,-565.311"
        },
        {
            "source": "N15-1184",
            "target": "P16-1045",
            "d": "M868.597,-537.986C868.597,-530.019 868.597,-521.128 868.597,-512.64"
        },
        {
            "source": "N15-1184",
            "target": "P17-1191",
            "d": "M820.06,-541.442C804.928,-531.694 790.01,-518.747 781.597,-502.441 768.265,-476.598 779.851,-444.929 793.32,-421.274"
        },
        {
            "source": "N15-1184",
            "target": "P18-1225",
            "d": "M913.483,-540.952C928.835,-530.96 944.833,-517.982 955.597,-502.441 993.159,-448.211 945.911,-404.542 993.597,-358.96 1016.01,-337.541 1082.18,-322.457 1145.13,-312.548"
        },
        {
            "source": "D15-1154",
            "target": "P16-1101",
            "d": "M4572.4,-541.028C4591.07,-530.11 4613.49,-516.991 4633.23,-505.447"
        },
        {
            "source": "D15-1154",
            "target": "I17-1007",
            "d": "M4535.4,-538.123C4538.67,-507.626 4544.08,-457.074 4547.74,-422.882"
        },
        {
            "source": "D15-1154",
            "target": "P18-1130",
            "d": "M4527.55,-538.077C4522.04,-513.657 4511.67,-476.894 4494.6,-448.701 4483.04,-429.612 4469.65,-432.625 4459.6,-412.701 4447.15,-388.021 4442.56,-356.966 4441,-333.259"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "P16-1101",
            "target": "P16-1228",
            "d": "M4758.22,-493.755C4778.24,-496.048 4798.27,-496.337 4818.29,-494.624"
        },
        {
            "source": "P16-1101",
            "target": "I17-1007",
            "d": "M4646.45,-450.363C4630.59,-439.737 4611.83,-427.171 4595.19,-416.031"
        },
        {
            "source": "P16-1101",
            "target": "P18-1130",
            "d": "M4680.58,-448.439C4677.18,-422.319 4668.07,-382.717 4643.6,-358.96 4618.4,-334.492 4583.81,-319.667 4550.33,-310.697"
        },
        {
            "source": "P16-1101",
            "target": "P19-1562",
            "d": "M4682.61,-448.488C4682.09,-424.811 4679.97,-389.096 4672.6,-358.96 4662.39,-317.24 4642.37,-272.104 4627.62,-242.029"
        },
        {
            "source": "P16-1101",
            "target": "2020.tacl-1.39",
            "d": "M4686.67,-448.611C4696.53,-385.763 4721.7,-225.322 4732.98,-153.417"
        },
        {
            "source": "P16-1228",
            "target": "P16-1101",
            "d": "M4828.44,-457.557C4808.41,-455.167 4788.39,-454.78 4768.37,-456.396"
        },
        {
            "source": "N16-1082",
            "target": "2020.findings-emnlp.344",
            "d": "M5270.22,-448.611C5257.19,-385.763 5223.93,-225.322 5209.02,-153.417"
        },
        {
            "source": "N16-1082",
            "target": "2021.emnlp-main.64",
            "d": "M5293.7,-449.2C5315.61,-415.943 5349.6,-355.011 5349.6,-297.09 5349.6,-297.09 5349.6,-297.09 5349.6,-205.35 5349.6,-156.623 5349.6,-100.371 5349.6,-64.4848"
        },
        {
            "source": "N16-1116",
            "target": "P16-1101",
            "d": "M4485.55,-475.571C4513.54,-475.571 4541.53,-475.571 4569.53,-475.571"
        },
        {
            "source": "N16-1116",
            "target": "I17-1007",
            "d": "M4421.3,-451.058C4444.95,-439.4 4473.6,-425.278 4498,-413.248"
        },
        {
            "source": "N16-1116",
            "target": "P18-1130",
            "d": "M4383.34,-448.763C4394.9,-418.138 4414.24,-366.908 4427.2,-332.568"
        },
        {
            "source": "N16-1174",
            "target": "N19-1364",
            "d": "M1738.17,-448.948C1765.64,-400.665 1825.07,-296.185 1856,-241.808"
        },
        {
            "source": "N16-1174",
            "target": "D19-1589",
            "d": "M1722.07,-448.463C1719.07,-421.974 1710.28,-381.724 1684.6,-358.96 1625.83,-306.863 1570.16,-375.287 1511.6,-322.96 1489.16,-302.918 1479.93,-269.407 1476.15,-243.61"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "W17-4902",
            "target": "D19-1179",
            "d": "M3794.57,-360.525C3742.62,-328.173 3650.86,-271.033 3595.3,-236.432"
        },
        {
            "source": "P17-1191",
            "target": "D18-1257",
            "d": "M819.698,-358.506C820.333,-350.539 821.043,-341.648 821.72,-333.159"
        },
        {
            "source": "I17-1007",
            "target": "P18-1130",
            "d": "M4520.97,-360.623C4508.43,-350.71 4493.75,-339.107 4480.4,-328.55"
        },
        {
            "source": "I17-1007",
            "target": "P19-1562",
            "d": "M4560.03,-359.023C4570.04,-328.397 4586.78,-277.168 4598,-242.828"
        },
        {
            "source": "D17-1082",
            "target": "2020.acl-main.502",
            "d": "M1079.69,-359.208C1039.96,-310.501 953.57,-204.607 909.551,-150.649"
        },
        {
            "source": "D17-1292",
            "target": "P18-1225",
            "d": "M1311.04,-358.98C1306.64,-350.576 1301.67,-341.105 1296.98,-332.145"
        },
        {
            "source": "D17-1292",
            "target": "D19-1589",
            "d": "M1382.09,-363.009C1401.35,-353.271 1421.48,-340.062 1435.6,-322.96 1454.31,-300.289 1463.9,-268.115 1468.77,-243.462"
        },
        {
            "source": "D17-1315",
            "target": "W17-4902",
            "d": "M3700.83,-385.831C3703.27,-385.831 3705.72,-385.831 3708.16,-385.831"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "P18-1130",
            "target": "P19-1562",
            "d": "M4486.33,-271.347C4508.94,-259.608 4536.28,-245.415 4559.46,-233.381"
        },
        {
            "source": "P18-1130",
            "target": "D19-1437",
            "d": "M4429.99,-269.24C4426.58,-260.925 4422.74,-251.564 4419.1,-242.688"
        },
        {
            "source": "P18-1225",
            "target": "D19-1589",
            "d": "M1332.14,-270.999C1357.34,-259.658 1387.51,-246.087 1413.49,-234.395"
        },
        {
            "source": "P18-1225",
            "target": "2020.deelio-1.4",
            "d": "M1259.99,-269.283C1237.55,-238.174 1199.78,-185.807 1175.02,-151.476"
        },
        {
            "source": "P18-1225",
            "target": "2021.findings-acl.84",
            "d": "M1282.5,-269.091C1287.41,-229.078 1292.65,-150.651 1269.6,-89.7401 1265.48,-78.8508 1258.65,-68.4238 1251.34,-59.3355"
        },
        {
            "source": "N18-1149",
            "target": "D19-1327",
            "d": "M1627.56,-271.115C1639.85,-260.947 1654.32,-248.969 1667.38,-238.157"
        },
        {
            "source": "N18-1149",
            "target": "D19-1589",
            "d": "M1565.1,-271.578C1550.33,-261.213 1532.79,-248.9 1517.04,-237.842"
        },
        {
            "source": "D18-1257",
            "target": "2020.acl-main.502",
            "d": "M833.031,-269.283C843.04,-238.657 859.781,-187.428 871.003,-153.088"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "W19-4502",
            "target": "2020.lrec-1.127",
            "d": "M5616.54,-180.91C5608.92,-171.627 5600.11,-160.891 5591.94,-150.926"
        },
        {
            "source": "W19-4502",
            "target": "2020.emnlp-main.2",
            "d": "M5673.57,-184.557C5694.08,-173.021 5719.88,-158.51 5742.21,-145.949"
        },
        {
            "source": "N19-1364",
            "target": "2020.acl-main.473",
            "d": "M1875.6,-179.025C1875.6,-171.059 1875.6,-162.168 1875.6,-153.679"
        },
        {
            "source": "K19-1033",
            "target": "2021.eacl-main.295",
            "d": "M6595.6,-179.162C6595.6,-148.665 6595.6,-98.1136 6595.6,-63.9219"
        },
        {
            "source": "D19-1179",
            "target": "2021.acl-long.185",
            "d": "M3548.6,-179.162C3548.6,-148.665 3548.6,-98.1136 3548.6,-63.9219"
        },
        {
            "source": "D19-1327",
            "target": "2021.acl-long.185",
            "d": "M1714.02,-179.495C1725.66,-151.776 1748.35,-109.141 1783.6,-89.7401 1856.76,-49.4669 3084.17,-32.9276 3447.84,-28.897"
        },
        {
            "source": "D19-1589",
            "target": "2020.emnlp-main.529",
            "d": "M1473.6,-179.025C1473.6,-171.059 1473.6,-162.168 1473.6,-153.679"
        },
        {
            "source": "2020",
            "target": "2021",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        },
        {
            "source": "2020.sdp-1.1",
            "target": "2020.sdp-1.24",
            "d": "M6961.59,-134.695C6979.92,-137.027 6998.25,-137.386 7016.58,-135.774"
        },
        {
            "source": "2020.sdp-1.24",
            "target": "2020.sdp-1.1",
            "d": "M7026.76,-98.5444C7008.43,-96.2022 6990.1,-95.8317 6971.77,-97.4329"
        },
        {
            "source": "2020.lrec-1.127",
            "target": "2020.emnlp-main.2",
            "d": "M5679.93,-116.61C5682.41,-116.61 5684.88,-116.61 5687.36,-116.61"
        },
        {
            "source": "2020.deelio-1.4",
            "target": "2021.findings-acl.84",
            "d": "M1170.93,-89.7598C1178.13,-80.5978 1186.33,-70.1666 1193.93,-60.513"
        },
        {
            "source": "2021.findings-acl.357",
            "target": "2021.emnlp-main.592",
            "d": "M8138.18,-26.8701C8140.49,-26.8701 8142.81,-26.8701 8145.13,-26.8701"
        },
        {
            "source": "2021.findings-acl.456",
            "target": "2021.emnlp-main.508",
            "d": "M8527.39,-26.8701C8529.88,-26.8701 8532.36,-26.8701 8534.84,-26.8701"
        }
    ],
    [
        {
            "id": "1988",
            "name": "1988",
            "x": "28.5975",
            "y": "-2984.59"
        },
        {
            "id": "1989",
            "name": "1989",
            "x": "28.5975",
            "y": "-2894.85"
        },
        {
            "id": "P88-1020",
            "name": "eduard1988Planning",
            "x": "598.597",
            "y": "-2992.09"
        },
        {
            "id": "P88-1020",
            "name": "172",
            "x": "598.597",
            "y": "-2977.09"
        },
        {
            "id": "W90-0117",
            "name": "eduard1990Parsimonious",
            "x": "452.597",
            "y": "-2812.61"
        },
        {
            "id": "W90-0117",
            "name": "58",
            "x": "452.597",
            "y": "-2797.61"
        },
        {
            "id": "1997.tmi-1.6",
            "name": "eduard1997{MT}",
            "x": "798.597",
            "y": "-2184.43"
        },
        {
            "id": "1997.tmi-1.6",
            "name": "???",
            "x": "798.597",
            "y": "-2169.43"
        },
        {
            "id": "D14-1218",
            "name": "jiwei2014A",
            "x": "596.597",
            "y": "-658.851"
        },
        {
            "id": "D14-1218",
            "name": "65",
            "x": "596.597",
            "y": "-643.851"
        },
        {
            "id": "P88-1022",
            "name": "eduard1988Two",
            "x": "412.597",
            "y": "-2992.09"
        },
        {
            "id": "P88-1022",
            "name": "22",
            "x": "412.597",
            "y": "-2977.09"
        },
        {
            "id": "1990",
            "name": "1990",
            "x": "28.5975",
            "y": "-2805.11"
        },
        {
            "id": "J89-2012",
            "name": "eduard1989Book",
            "x": "946.597",
            "y": "-2902.35"
        },
        {
            "id": "J89-2012",
            "name": "???",
            "x": "946.597",
            "y": "-2887.35"
        },
        {
            "id": "H89-2015",
            "name": "eduard1989New",
            "x": "1120.6",
            "y": "-2902.35"
        },
        {
            "id": "H89-2015",
            "name": "2",
            "x": "1120.6",
            "y": "-2887.35"
        },
        {
            "id": "H89-2065",
            "name": "eduard1989The",
            "x": "1287.6",
            "y": "-2902.35"
        },
        {
            "id": "H89-2065",
            "name": "0",
            "x": "1287.6",
            "y": "-2887.35"
        },
        {
            "id": "H89-1021",
            "name": "william1989The",
            "x": "1452.6",
            "y": "-2902.35"
        },
        {
            "id": "H89-1021",
            "name": "0",
            "x": "1452.6",
            "y": "-2887.35"
        },
        {
            "id": "1991",
            "name": "1991",
            "x": "28.5975",
            "y": "-2715.37"
        },
        {
            "id": "H90-1011",
            "name": "robert1990Performing",
            "x": "965.597",
            "y": "-2812.61"
        },
        {
            "id": "H90-1011",
            "name": "6",
            "x": "965.597",
            "y": "-2797.61"
        },
        {
            "id": "H90-1072",
            "name": "yorick1990Machine",
            "x": "1171.6",
            "y": "-2812.61"
        },
        {
            "id": "H90-1072",
            "name": "2",
            "x": "1171.6",
            "y": "-2797.61"
        },
        {
            "id": "1992",
            "name": "1992",
            "x": "28.5975",
            "y": "-2625.63"
        },
        {
            "id": "H91-1106",
            "name": "eduard1991The",
            "x": "940.597",
            "y": "-2722.87"
        },
        {
            "id": "H91-1106",
            "name": "0",
            "x": "940.597",
            "y": "-2707.87"
        },
        {
            "id": "1993",
            "name": "1993",
            "x": "28.5975",
            "y": "-2535.89"
        },
        {
            "id": "H92-1052",
            "name": "eduard1992Approximating",
            "x": "393.597",
            "y": "-2633.13"
        },
        {
            "id": "H92-1052",
            "name": "27",
            "x": "393.597",
            "y": "-2618.13"
        },
        {
            "id": "H94-1025",
            "name": "akitoshi1994Building",
            "x": "475.597",
            "y": "-2453.65"
        },
        {
            "id": "H94-1025",
            "name": "31",
            "x": "475.597",
            "y": "-2438.65"
        },
        {
            "id": "1994.amta-1.23",
            "name": "akitoshi1994Lexicon-to-Ontology",
            "x": "219.597",
            "y": "-2453.65"
        },
        {
            "id": "1994.amta-1.23",
            "name": "???",
            "x": "219.597",
            "y": "-2438.65"
        },
        {
            "id": "H92-1124",
            "name": "eduard1992In-Depth",
            "x": "959.597",
            "y": "-2633.13"
        },
        {
            "id": "H92-1124",
            "name": "0",
            "x": "959.597",
            "y": "-2618.13"
        },
        {
            "id": "1994",
            "name": "1994",
            "x": "28.5975",
            "y": "-2446.15"
        },
        {
            "id": "W93-0210",
            "name": "eduard1993In",
            "x": "932.597",
            "y": "-2543.39"
        },
        {
            "id": "W93-0210",
            "name": "???",
            "x": "932.597",
            "y": "-2528.39"
        },
        {
            "id": "H93-1115",
            "name": "eduard1993The",
            "x": "1089.6",
            "y": "-2543.39"
        },
        {
            "id": "H93-1115",
            "name": "0",
            "x": "1089.6",
            "y": "-2528.39"
        },
        {
            "id": "1995",
            "name": "1995",
            "x": "28.5975",
            "y": "-2356.41"
        },
        {
            "id": "W94-0328",
            "name": "julia1994Toward",
            "x": "944.597",
            "y": "-2453.65"
        },
        {
            "id": "W94-0328",
            "name": "0",
            "x": "944.597",
            "y": "-2438.65"
        },
        {
            "id": "H94-1023",
            "name": "eduard1994Session",
            "x": "1128.6",
            "y": "-2453.65"
        },
        {
            "id": "H94-1023",
            "name": "0",
            "x": "1128.6",
            "y": "-2438.65"
        },
        {
            "id": "H94-1121",
            "name": "eduard1994{PANGLOSS}:",
            "x": "1356.6",
            "y": "-2453.65"
        },
        {
            "id": "H94-1121",
            "name": "3",
            "x": "1356.6",
            "y": "-2438.65"
        },
        {
            "id": "1994.amta-1.10",
            "name": "robert1994Integrating",
            "x": "1592.6",
            "y": "-2453.65"
        },
        {
            "id": "1994.amta-1.10",
            "name": "???",
            "x": "1592.6",
            "y": "-2438.65"
        },
        {
            "id": "1994.amta-1.18",
            "name": "kevin1994Integrating",
            "x": "717.597",
            "y": "-2453.65"
        },
        {
            "id": "1994.amta-1.18",
            "name": "48",
            "x": "717.597",
            "y": "-2438.65"
        },
        {
            "id": "1997.mtsummit-workshop.7",
            "name": "latifur1997Improving",
            "x": "219.597",
            "y": "-2184.43"
        },
        {
            "id": "1997.mtsummit-workshop.7",
            "name": "8",
            "x": "219.597",
            "y": "-2169.43"
        },
        {
            "id": "1994.amta-1.27",
            "name": "kenneth1994Is",
            "x": "1775.6",
            "y": "-2453.65"
        },
        {
            "id": "1994.amta-1.27",
            "name": "???",
            "x": "1775.6",
            "y": "-2438.65"
        },
        {
            "id": "1994.amta-1.41",
            "name": "jaime1994{PANGLOSS}",
            "x": "1974.6",
            "y": "-2453.65"
        },
        {
            "id": "1994.amta-1.41",
            "name": "0",
            "x": "1974.6",
            "y": "-2438.65"
        },
        {
            "id": "1996",
            "name": "1996",
            "x": "28.5975",
            "y": "-2266.67"
        },
        {
            "id": "J95-1009",
            "name": "eduard1995Book",
            "x": "946.597",
            "y": "-2363.91"
        },
        {
            "id": "J95-1009",
            "name": "???",
            "x": "946.597",
            "y": "-2348.91"
        },
        {
            "id": "1997",
            "name": "1997",
            "x": "28.5975",
            "y": "-2176.93"
        },
        {
            "id": "W96-0508",
            "name": "hercules1996On",
            "x": "942.597",
            "y": "-2274.17"
        },
        {
            "id": "W96-0508",
            "name": "6",
            "x": "942.597",
            "y": "-2259.17"
        },
        {
            "id": "W96-0401",
            "name": "leo1996The",
            "x": "1094.6",
            "y": "-2274.17"
        },
        {
            "id": "W96-0401",
            "name": "37",
            "x": "1094.6",
            "y": "-2259.17"
        },
        {
            "id": "1996.amta-1.24",
            "name": "eduard1996Panel:",
            "x": "1251.6",
            "y": "-2274.17"
        },
        {
            "id": "1996.amta-1.24",
            "name": "???",
            "x": "1251.6",
            "y": "-2259.17"
        },
        {
            "id": "1996.amta-1.31",
            "name": "kevin1996{JAPANGLOSS}:",
            "x": "1476.6",
            "y": "-2274.17"
        },
        {
            "id": "1996.amta-1.31",
            "name": "???",
            "x": "1476.6",
            "y": "-2259.17"
        },
        {
            "id": "1998",
            "name": "1998",
            "x": "28.5975",
            "y": "-2087.19"
        },
        {
            "id": "W97-0704",
            "name": "eduard1997Automated",
            "x": "5208.6",
            "y": "-2184.43"
        },
        {
            "id": "W97-0704",
            "name": "367",
            "x": "5208.6",
            "y": "-2169.43"
        },
        {
            "id": "gerber-hovy-1998-improving",
            "name": "laurie1998Improving",
            "x": "5367.6",
            "y": "-2094.69"
        },
        {
            "id": "gerber-hovy-1998-improving",
            "name": "12",
            "x": "5367.6",
            "y": "-2079.69"
        },
        {
            "id": "C00-1072",
            "name": "chinyew2000The",
            "x": "5081.6",
            "y": "-1915.21"
        },
        {
            "id": "C00-1072",
            "name": "401",
            "x": "5081.6",
            "y": "-1900.21"
        },
        {
            "id": "J02-4001",
            "name": "dragomir2002Introduction",
            "x": "5463.6",
            "y": "-1735.73"
        },
        {
            "id": "J02-4001",
            "name": "319",
            "x": "5463.6",
            "y": "-1720.73"
        },
        {
            "id": "A97-1042",
            "name": "chinyew1997Identifying",
            "x": "4948.6",
            "y": "-2184.43"
        },
        {
            "id": "A97-1042",
            "name": "242",
            "x": "4948.6",
            "y": "-2169.43"
        },
        {
            "id": "X98-1026",
            "name": "eduard1998Automated",
            "x": "5041.6",
            "y": "-2094.69"
        },
        {
            "id": "X98-1026",
            "name": "160",
            "x": "5041.6",
            "y": "-2079.69"
        },
        {
            "id": "P06-2063",
            "name": "soomin2006Automatic",
            "x": "4719.6",
            "y": "-1376.77"
        },
        {
            "id": "P06-2063",
            "name": "154",
            "x": "4719.6",
            "y": "-1361.77"
        },
        {
            "id": "zhou-etal-2006-summarizing",
            "name": "liang2006Summarizing",
            "x": "2058.6",
            "y": "-1376.77"
        },
        {
            "id": "zhou-etal-2006-summarizing",
            "name": "5",
            "x": "2058.6",
            "y": "-1361.77"
        },
        {
            "id": "D19-1327",
            "name": "taehee2019Earlier",
            "x": "1704.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1327",
            "name": "2",
            "x": "1704.6",
            "y": "-195.15"
        },
        {
            "id": "1997.mtsummit-tutorials.1",
            "name": "eduard1997A",
            "x": "5392.6",
            "y": "-2184.43"
        },
        {
            "id": "1997.mtsummit-tutorials.1",
            "name": "???",
            "x": "5392.6",
            "y": "-2169.43"
        },
        {
            "id": "1999",
            "name": "1999",
            "x": "28.5975",
            "y": "-1997.45"
        },
        {
            "id": "1998.amta-tutorials.7",
            "name": "eduard1998Multilingual",
            "x": "5581.6",
            "y": "-2094.69"
        },
        {
            "id": "1998.amta-tutorials.7",
            "name": "???",
            "x": "5581.6",
            "y": "-2079.69"
        },
        {
            "id": "1998.amta-panels.1",
            "name": "eduard1998A",
            "x": "5767.6",
            "y": "-2094.69"
        },
        {
            "id": "1998.amta-panels.1",
            "name": "???",
            "x": "5767.6",
            "y": "-2079.69"
        },
        {
            "id": "2000",
            "name": "2000",
            "x": "28.5975",
            "y": "-1907.71"
        },
        {
            "id": "1999.mtsummit-1.31",
            "name": "margaret1999{MT}",
            "x": "5363.6",
            "y": "-2004.95"
        },
        {
            "id": "1999.mtsummit-1.31",
            "name": "???",
            "x": "5363.6",
            "y": "-1989.95"
        },
        {
            "id": "2001",
            "name": "2001",
            "x": "28.5975",
            "y": "-1817.97"
        },
        {
            "id": "P02-1058",
            "name": "chinyew2002From",
            "x": "5245.6",
            "y": "-1735.73"
        },
        {
            "id": "P02-1058",
            "name": "200",
            "x": "5245.6",
            "y": "-1720.73"
        },
        {
            "id": "C02-1130",
            "name": "michael2002Fine",
            "x": "5026.6",
            "y": "-1735.73"
        },
        {
            "id": "C02-1130",
            "name": "158",
            "x": "5026.6",
            "y": "-1720.73"
        },
        {
            "id": "N03-1037",
            "name": "liang2003A",
            "x": "2312.6",
            "y": "-1645.99"
        },
        {
            "id": "N03-1037",
            "name": "26",
            "x": "2312.6",
            "y": "-1630.99"
        },
        {
            "id": "N10-2002",
            "name": "congxing2010Summarizing",
            "x": "5370.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-2002",
            "name": "1",
            "x": "5370.6",
            "y": "-1002.81"
        },
        {
            "id": "2000.amta-tutorials.2",
            "name": "eduard2000A",
            "x": "5339.6",
            "y": "-1915.21"
        },
        {
            "id": "2000.amta-tutorials.2",
            "name": "???",
            "x": "5339.6",
            "y": "-1900.21"
        },
        {
            "id": "2002",
            "name": "2002",
            "x": "28.5975",
            "y": "-1728.23"
        },
        {
            "id": "W01-1313",
            "name": "elena2001Assigning",
            "x": "5363.6",
            "y": "-1825.47"
        },
        {
            "id": "W01-1313",
            "name": "116",
            "x": "5363.6",
            "y": "-1810.47"
        },
        {
            "id": "H01-1069",
            "name": "eduard2001Toward",
            "x": "5559.6",
            "y": "-1825.47"
        },
        {
            "id": "H01-1069",
            "name": "124",
            "x": "5559.6",
            "y": "-1810.47"
        },
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1638.49"
        },
        {
            "id": "W02-2108",
            "name": "michael2002Towards",
            "x": "5692.6",
            "y": "-1735.73"
        },
        {
            "id": "W02-2108",
            "name": "21",
            "x": "5692.6",
            "y": "-1720.73"
        },
        {
            "id": "W02-1105",
            "name": "eduard2002Building",
            "x": "5897.6",
            "y": "-1735.73"
        },
        {
            "id": "W02-1105",
            "name": "2",
            "x": "5897.6",
            "y": "-1720.73"
        },
        {
            "id": "W02-0406",
            "name": "chinyew2002Manual",
            "x": "1879.6",
            "y": "-1735.73"
        },
        {
            "id": "W02-0406",
            "name": "126",
            "x": "1879.6",
            "y": "-1720.73"
        },
        {
            "id": "W03-0510",
            "name": "chinyew2003The",
            "x": "1800.6",
            "y": "-1645.99"
        },
        {
            "id": "W03-0510",
            "name": "53",
            "x": "1800.6",
            "y": "-1630.99"
        },
        {
            "id": "N03-1020",
            "name": "chinyew2003Automatic",
            "x": "1600.6",
            "y": "-1645.99"
        },
        {
            "id": "N03-1020",
            "name": "1005",
            "x": "1600.6",
            "y": "-1630.99"
        },
        {
            "id": "hovy-etal-2006-automated",
            "name": "eduard2006Automated",
            "x": "1839.6",
            "y": "-1376.77"
        },
        {
            "id": "hovy-etal-2006-automated",
            "name": "143",
            "x": "1839.6",
            "y": "-1361.77"
        },
        {
            "id": "P02-1006",
            "name": "deepak2002Learning",
            "x": "2524.6",
            "y": "-1735.73"
        },
        {
            "id": "P02-1006",
            "name": "688",
            "x": "2524.6",
            "y": "-1720.73"
        },
        {
            "id": "C02-1042",
            "name": "eduard2002Using",
            "x": "2715.6",
            "y": "-1735.73"
        },
        {
            "id": "C02-1042",
            "name": "13",
            "x": "2715.6",
            "y": "-1720.73"
        },
        {
            "id": "P03-1001",
            "name": "michael2003Offline",
            "x": "3375.6",
            "y": "-1645.99"
        },
        {
            "id": "P03-1001",
            "name": "120",
            "x": "3375.6",
            "y": "-1630.99"
        },
        {
            "id": "H05-1075",
            "name": "donghui2005Handling",
            "x": "2643.6",
            "y": "-1466.51"
        },
        {
            "id": "H05-1075",
            "name": "3",
            "x": "2643.6",
            "y": "-1451.51"
        },
        {
            "id": "W08-0628",
            "name": "donghui2008Adaptive",
            "x": "2217.6",
            "y": "-1197.29"
        },
        {
            "id": "W08-0628",
            "name": "1",
            "x": "2217.6",
            "y": "-1182.29"
        },
        {
            "id": "I08-2124",
            "name": "donghui2008Towards",
            "x": "2002.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-2124",
            "name": "3",
            "x": "2002.6",
            "y": "-1182.29"
        },
        {
            "id": "P11-1162",
            "name": "zornitsa2011Insights",
            "x": "4859.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1162",
            "name": "5",
            "x": "4859.6",
            "y": "-913.071"
        },
        {
            "id": "P03-2021",
            "name": "anton2003i{N}e{ATS}:",
            "x": "5379.6",
            "y": "-1645.99"
        },
        {
            "id": "P03-2021",
            "name": "34",
            "x": "5379.6",
            "y": "-1630.99"
        },
        {
            "id": "hovy-etal-2002-computer",
            "name": "eduard2002Computer-Aided",
            "x": "6130.6",
            "y": "-1735.73"
        },
        {
            "id": "hovy-etal-2002-computer",
            "name": "7",
            "x": "6130.6",
            "y": "-1720.73"
        },
        {
            "id": "P08-1119",
            "name": "zornitsa2008Semantic",
            "x": "4897.6",
            "y": "-1197.29"
        },
        {
            "id": "P08-1119",
            "name": "191",
            "x": "4897.6",
            "y": "-1182.29"
        },
        {
            "id": "D09-1099",
            "name": "eduard2009Toward",
            "x": "5014.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1099",
            "name": "33",
            "x": "5014.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-1150",
            "name": "zornitsa2010Learning",
            "x": "5138.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1150",
            "name": "48",
            "x": "5138.6",
            "y": "-1002.81"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "W03-1209",
            "name": "deepak2003Statistical",
            "x": "3578.6",
            "y": "-1645.99"
        },
        {
            "id": "W03-1209",
            "name": "39",
            "x": "3578.6",
            "y": "-1630.99"
        },
        {
            "id": "N06-1026",
            "name": "soomin2006Identifying",
            "x": "3793.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1026",
            "name": "173",
            "x": "3793.6",
            "y": "-1361.77"
        },
        {
            "id": "W03-1007",
            "name": "michael2003Maximum",
            "x": "3793.6",
            "y": "-1645.99"
        },
        {
            "id": "W03-1007",
            "name": "61",
            "x": "3793.6",
            "y": "-1630.99"
        },
        {
            "id": "W04-0832",
            "name": "namhee2004Senseval",
            "x": "3739.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-0832",
            "name": "6",
            "x": "3739.6",
            "y": "-1541.25"
        },
        {
            "id": "C04-1179",
            "name": "namhee2004{F}rame{N}et-based",
            "x": "3997.6",
            "y": "-1556.25"
        },
        {
            "id": "C04-1179",
            "name": "14",
            "x": "3997.6",
            "y": "-1541.25"
        },
        {
            "id": "I05-7009",
            "name": "andrew2005The",
            "x": "3387.6",
            "y": "-1466.51"
        },
        {
            "id": "I05-7009",
            "name": "60",
            "x": "3387.6",
            "y": "-1451.51"
        },
        {
            "id": "W06-0301",
            "name": "soomin2006Extracting",
            "x": "4270.6",
            "y": "-1376.77"
        },
        {
            "id": "W06-0301",
            "name": "358",
            "x": "4270.6",
            "y": "-1361.77"
        },
        {
            "id": "P11-1147",
            "name": "dirk2011Unsupervised",
            "x": "4223.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1147",
            "name": "13",
            "x": "4223.6",
            "y": "-913.071"
        },
        {
            "id": "W04-2709",
            "name": "stephen2004Interlingual",
            "x": "3027.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-2709",
            "name": "20",
            "x": "3027.6",
            "y": "-1541.25"
        },
        {
            "id": "W04-0701",
            "name": "michael2004Multi-Document",
            "x": "4760.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-0701",
            "name": "60",
            "x": "4760.6",
            "y": "-1541.25"
        },
        {
            "id": "C04-1111",
            "name": "patrick2004Towards",
            "x": "4524.6",
            "y": "-1556.25"
        },
        {
            "id": "C04-1111",
            "name": "10",
            "x": "4524.6",
            "y": "-1541.25"
        },
        {
            "id": "N03-2008",
            "name": "michael2003A",
            "x": "5570.6",
            "y": "-1645.99"
        },
        {
            "id": "N03-2008",
            "name": "11",
            "x": "5570.6",
            "y": "-1630.99"
        },
        {
            "id": "W04-3256",
            "name": "liang2004Multi-Document",
            "x": "2284.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-3256",
            "name": "???",
            "x": "2284.6",
            "y": "-1541.25"
        },
        {
            "id": "W04-1010",
            "name": "liang2004Template-Filtered",
            "x": "1540.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-1010",
            "name": "12",
            "x": "1540.6",
            "y": "-1541.25"
        },
        {
            "id": "W06-1610",
            "name": "liang2006Re-evaluating",
            "x": "1618.6",
            "y": "-1376.77"
        },
        {
            "id": "W06-1610",
            "name": "86",
            "x": "1618.6",
            "y": "-1361.77"
        },
        {
            "id": "N06-1057",
            "name": "liang2006{P}ara{E}val:",
            "x": "1394.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1057",
            "name": "87",
            "x": "1394.6",
            "y": "-1361.77"
        },
        {
            "id": "N07-2055",
            "name": "liang2007A",
            "x": "1307.6",
            "y": "-1287.03"
        },
        {
            "id": "N07-2055",
            "name": "7",
            "x": "1307.6",
            "y": "-1272.03"
        },
        {
            "id": "D17-1082",
            "name": "guokun2017{RACE}:",
            "x": "1100.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1082",
            "name": "143",
            "x": "1100.6",
            "y": "-374.631"
        },
        {
            "id": "D19-1179",
            "name": "dongyeop2019(Male,",
            "x": "3548.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1179",
            "name": "3",
            "x": "3548.6",
            "y": "-195.15"
        },
        {
            "id": "2003.mtsummit-plenaries.8",
            "name": "eduard2003Holy",
            "x": "5732.6",
            "y": "-1645.99"
        },
        {
            "id": "2003.mtsummit-plenaries.8",
            "name": "???",
            "x": "5732.6",
            "y": "-1630.99"
        },
        {
            "id": "2003.mtsummit-papers.12",
            "name": "dinh2003{BTL}:",
            "x": "5905.6",
            "y": "-1645.99"
        },
        {
            "id": "2003.mtsummit-papers.12",
            "name": "???",
            "x": "5905.6",
            "y": "-1630.99"
        },
        {
            "id": "2003.mtsummit-papers.30",
            "name": "margaret2003{FEMTI}:",
            "x": "6125.6",
            "y": "-1645.99"
        },
        {
            "id": "2003.mtsummit-papers.30",
            "name": "???",
            "x": "6125.6",
            "y": "-1630.99"
        },
        {
            "id": "2005",
            "name": "2005",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "reeder-etal-2004-interlingual",
            "name": "florence2004Interlingual",
            "x": "3254.6",
            "y": "-1556.25"
        },
        {
            "id": "reeder-etal-2004-interlingual",
            "name": "7",
            "x": "3254.6",
            "y": "-1541.25"
        },
        {
            "id": "rambow-etal-2006-parallel",
            "name": "owen2006Parallel",
            "x": "2841.6",
            "y": "-1376.77"
        },
        {
            "id": "rambow-etal-2006-parallel",
            "name": "10",
            "x": "2841.6",
            "y": "-1361.77"
        },
        {
            "id": "C04-1200",
            "name": "soomin2004Determining",
            "x": "4305.6",
            "y": "-1556.25"
        },
        {
            "id": "C04-1200",
            "name": "1103",
            "x": "4305.6",
            "y": "-1541.25"
        },
        {
            "id": "D07-1113",
            "name": "soomin2007{C}rystal:",
            "x": "3189.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1113",
            "name": "84",
            "x": "3189.6",
            "y": "-1272.03"
        },
        {
            "id": "D14-1053",
            "name": "jiwei2014Sentiment",
            "x": "4386.6",
            "y": "-658.851"
        },
        {
            "id": "D14-1053",
            "name": "11",
            "x": "4386.6",
            "y": "-643.851"
        },
        {
            "id": "2021.acl-long.185",
            "name": "dongyeop2021Style",
            "x": "3548.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.acl-long.185",
            "name": "???",
            "x": "3548.6",
            "y": "-15.6701"
        },
        {
            "id": "N06-2015",
            "name": "eduard2006{O}nto{N}otes:",
            "x": "3061.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-2015",
            "name": "537",
            "x": "3061.6",
            "y": "-1361.77"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "W05-1520",
            "name": "rahul2005Statistical",
            "x": "3656.6",
            "y": "-1466.51"
        },
        {
            "id": "W05-1520",
            "name": "15",
            "x": "3656.6",
            "y": "-1451.51"
        },
        {
            "id": "hartholt-etal-2008-common",
            "name": "arno2008A",
            "x": "3661.6",
            "y": "-1197.29"
        },
        {
            "id": "hartholt-etal-2008-common",
            "name": "20",
            "x": "3661.6",
            "y": "-1182.29"
        },
        {
            "id": "P05-1037",
            "name": "liang2005Digesting",
            "x": "5370.6",
            "y": "-1466.51"
        },
        {
            "id": "P05-1037",
            "name": "57",
            "x": "5370.6",
            "y": "-1451.51"
        },
        {
            "id": "H05-2003",
            "name": "liang2005{C}lassummary:",
            "x": "5587.6",
            "y": "-1466.51"
        },
        {
            "id": "H05-2003",
            "name": "1",
            "x": "5587.6",
            "y": "-1451.51"
        },
        {
            "id": "N06-1027",
            "name": "donghui2006Learning",
            "x": "5370.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1027",
            "name": "50",
            "x": "5370.6",
            "y": "-1361.77"
        },
        {
            "id": "P05-1077",
            "name": "deepak2005Randomized",
            "x": "5828.6",
            "y": "-1466.51"
        },
        {
            "id": "P05-1077",
            "name": "187",
            "x": "5828.6",
            "y": "-1451.51"
        },
        {
            "id": "I08-1048",
            "name": "jingbo2008Learning",
            "x": "3310.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-1048",
            "name": "32",
            "x": "3310.6",
            "y": "-1182.29"
        },
        {
            "id": "I05-2011",
            "name": "soomin2005Automatic",
            "x": "4783.6",
            "y": "-1466.51"
        },
        {
            "id": "I05-2011",
            "name": "135",
            "x": "4783.6",
            "y": "-1451.51"
        },
        {
            "id": "2005.sigdial-1.25",
            "name": "david2005Dealing",
            "x": "6037.6",
            "y": "-1466.51"
        },
        {
            "id": "2005.sigdial-1.25",
            "name": "4",
            "x": "6037.6",
            "y": "-1451.51"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "D07-1082",
            "name": "jingbo2007Active",
            "x": "2952.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1082",
            "name": "129",
            "x": "2952.6",
            "y": "-1272.03"
        },
        {
            "id": "O08-6002",
            "name": "liangchih2008Corpus",
            "x": "2820.6",
            "y": "-1197.29"
        },
        {
            "id": "O08-6002",
            "name": "0",
            "x": "2820.6",
            "y": "-1182.29"
        },
        {
            "id": "C08-1133",
            "name": "liangchih2008{O}nto{N}otes:",
            "x": "2577.6",
            "y": "-1197.29"
        },
        {
            "id": "C08-1133",
            "name": "5",
            "x": "2577.6",
            "y": "-1182.29"
        },
        {
            "id": "C08-1142",
            "name": "jingbo2008Multi-Criteria-Based",
            "x": "3068.6",
            "y": "-1197.29"
        },
        {
            "id": "C08-1142",
            "name": "33",
            "x": "3068.6",
            "y": "-1182.29"
        },
        {
            "id": "P10-1144",
            "name": "marta2010Coreference",
            "x": "3018.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1144",
            "name": "19",
            "x": "3018.6",
            "y": "-1002.81"
        },
        {
            "id": "P14-2005",
            "name": "xiaoqiang2014An",
            "x": "2232.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2005",
            "name": "14",
            "x": "2232.6",
            "y": "-643.851"
        },
        {
            "id": "C14-1123",
            "name": "kartik2014Unsupervised",
            "x": "1989.6",
            "y": "-658.851"
        },
        {
            "id": "C14-1123",
            "name": "4",
            "x": "1989.6",
            "y": "-643.851"
        },
        {
            "id": "N16-1116",
            "name": "xuezhe2016Unsupervised",
            "x": "4373.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1116",
            "name": "5",
            "x": "4373.6",
            "y": "-464.371"
        },
        {
            "id": "S17-1025",
            "name": "sujay2017Embedded",
            "x": "1581.6",
            "y": "-389.631"
        },
        {
            "id": "S17-1025",
            "name": "3",
            "x": "1581.6",
            "y": "-374.631"
        },
        {
            "id": "2006.amta-tutorials.1",
            "name": "eduard2006A",
            "x": "5550.6",
            "y": "-1376.77"
        },
        {
            "id": "2006.amta-tutorials.1",
            "name": "???",
            "x": "5550.6",
            "y": "-1361.77"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "P07-1129",
            "name": "liangchih2007Topic",
            "x": "5362.6",
            "y": "-1287.03"
        },
        {
            "id": "P07-1129",
            "name": "3",
            "x": "5362.6",
            "y": "-1272.03"
        },
        {
            "id": "N07-1071",
            "name": "patrick2007{ISP}:",
            "x": "5553.6",
            "y": "-1287.03"
        },
        {
            "id": "N07-1071",
            "name": "87",
            "x": "5553.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1017",
            "name": "rahul2007{LEDIR}:",
            "x": "5746.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1017",
            "name": "49",
            "x": "5746.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1088",
            "name": "donghui2007Extracting",
            "x": "2174.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1088",
            "name": "20",
            "x": "2174.6",
            "y": "-1272.03"
        },
        {
            "id": "2007.mtsummit-aptme.3",
            "name": "eduard2007Investigating",
            "x": "5962.6",
            "y": "-1287.03"
        },
        {
            "id": "2007.mtsummit-aptme.3",
            "name": "???",
            "x": "5962.6",
            "y": "-1272.03"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "N10-1087",
            "name": "zornitsa2010Not",
            "x": "4755.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1087",
            "name": "35",
            "x": "4755.6",
            "y": "-1002.81"
        },
        {
            "id": "D10-1108",
            "name": "zornitsa2010A",
            "x": "4956.6",
            "y": "-1017.81"
        },
        {
            "id": "D10-1108",
            "name": "120",
            "x": "4956.6",
            "y": "-1002.81"
        },
        {
            "id": "D14-1214",
            "name": "jiwei2014Major",
            "x": "4738.6",
            "y": "-658.851"
        },
        {
            "id": "D14-1214",
            "name": "53",
            "x": "4738.6",
            "y": "-643.851"
        },
        {
            "id": "W12-1905",
            "name": "dirk2012Exploiting",
            "x": "2533.6",
            "y": "-838.331"
        },
        {
            "id": "W12-1905",
            "name": "4",
            "x": "2533.6",
            "y": "-823.331"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "W10-3401",
            "name": "eduard2010Distributional",
            "x": "5616.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-3401",
            "name": "0",
            "x": "5616.6",
            "y": "-1002.81"
        },
        {
            "id": "W10-2111",
            "name": "eduard2010Injecting",
            "x": "5836.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-2111",
            "name": "0",
            "x": "5836.6",
            "y": "-1002.81"
        },
        {
            "id": "W10-0903",
            "name": "anselmo2010Semantic",
            "x": "4195.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-0903",
            "name": "14",
            "x": "4195.6",
            "y": "-1002.81"
        },
        {
            "id": "S10-1049",
            "name": "stephen2010{ISI}:",
            "x": "6030.6",
            "y": "-1017.81"
        },
        {
            "id": "S10-1049",
            "name": "7",
            "x": "6030.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-5004",
            "name": "eduard2010Annotation",
            "x": "3531.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-5004",
            "name": "172",
            "x": "3531.6",
            "y": "-1002.81"
        },
        {
            "id": "N13-1132",
            "name": "dirk2013Learning",
            "x": "3550.6",
            "y": "-748.591"
        },
        {
            "id": "N13-1132",
            "name": "96",
            "x": "3550.6",
            "y": "-733.591"
        },
        {
            "id": "P10-1070",
            "name": "stephen2010A",
            "x": "3344.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1070",
            "name": "49",
            "x": "3344.6",
            "y": "-1002.81"
        },
        {
            "id": "D11-1116",
            "name": "stephen2011A",
            "x": "3189.6",
            "y": "-928.071"
        },
        {
            "id": "D11-1116",
            "name": "55",
            "x": "3189.6",
            "y": "-913.071"
        },
        {
            "id": "P13-1037",
            "name": "stephen2013Automatic",
            "x": "3350.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1037",
            "name": "4",
            "x": "3350.6",
            "y": "-733.591"
        },
        {
            "id": "W13-1203",
            "name": "eduard2013Events",
            "x": "2759.6",
            "y": "-748.591"
        },
        {
            "id": "W13-1203",
            "name": "25",
            "x": "2759.6",
            "y": "-733.591"
        },
        {
            "id": "recasens-etal-2010-typology",
            "name": "marta2010A",
            "x": "2148.6",
            "y": "-1017.81"
        },
        {
            "id": "recasens-etal-2010-typology",
            "name": "21",
            "x": "2148.6",
            "y": "-1002.81"
        },
        {
            "id": "D18-1154",
            "name": "zhengzhong2018Automatic",
            "x": "2440.6",
            "y": "-299.89"
        },
        {
            "id": "D18-1154",
            "name": "2",
            "x": "2440.6",
            "y": "-284.89"
        },
        {
            "id": "C10-2052",
            "name": "dirk2010What{'}s",
            "x": "2675.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-2052",
            "name": "26",
            "x": "2675.6",
            "y": "-1002.81"
        },
        {
            "id": "P11-2056",
            "name": "dirk2011Models",
            "x": "2675.6",
            "y": "-928.071"
        },
        {
            "id": "P11-2056",
            "name": "8",
            "x": "2675.6",
            "y": "-913.071"
        },
        {
            "id": "C10-2113",
            "name": "anselmo2010Filling",
            "x": "6220.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-2113",
            "name": "19",
            "x": "6220.6",
            "y": "-1002.81"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "W11-3701",
            "name": "eduard2011Invited",
            "x": "5172.6",
            "y": "-928.071"
        },
        {
            "id": "W11-3701",
            "name": "???",
            "x": "5172.6",
            "y": "-913.071"
        },
        {
            "id": "W11-0704",
            "name": "stephan2011Contextual",
            "x": "5378.6",
            "y": "-928.071"
        },
        {
            "id": "W11-0704",
            "name": "???",
            "x": "5378.6",
            "y": "-913.071"
        },
        {
            "id": "W11-0206",
            "name": "sandeep2011The",
            "x": "5578.6",
            "y": "-928.071"
        },
        {
            "id": "W11-0206",
            "name": "4",
            "x": "5578.6",
            "y": "-913.071"
        },
        {
            "id": "W11-0102",
            "name": "eduard2011A",
            "x": "5740.6",
            "y": "-928.071"
        },
        {
            "id": "W11-0102",
            "name": "2",
            "x": "5740.6",
            "y": "-913.071"
        },
        {
            "id": "W11-0143",
            "name": "rutu2011Granularity",
            "x": "5913.6",
            "y": "-928.071"
        },
        {
            "id": "W11-0143",
            "name": "16",
            "x": "5913.6",
            "y": "-913.071"
        },
        {
            "id": "P11-2096",
            "name": "donald2011An",
            "x": "6090.6",
            "y": "-928.071"
        },
        {
            "id": "P11-2096",
            "name": "13",
            "x": "6090.6",
            "y": "-913.071"
        },
        {
            "id": "W13-3203",
            "name": "kartik2013A",
            "x": "2996.6",
            "y": "-748.591"
        },
        {
            "id": "W13-3203",
            "name": "7",
            "x": "2996.6",
            "y": "-733.591"
        },
        {
            "id": "W13-0907",
            "name": "dirk2013Identifying",
            "x": "3776.6",
            "y": "-748.591"
        },
        {
            "id": "W13-0907",
            "name": "34",
            "x": "3776.6",
            "y": "-733.591"
        },
        {
            "id": "P13-2083",
            "name": "kartik2013A",
            "x": "3133.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2083",
            "name": "13",
            "x": "3133.6",
            "y": "-733.591"
        },
        {
            "id": "D13-1144",
            "name": "shashank2013A",
            "x": "3956.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1144",
            "name": "27",
            "x": "3956.6",
            "y": "-733.591"
        },
        {
            "id": "liu-etal-2014-supervised",
            "name": "zhengzhong2014Supervised",
            "x": "2648.6",
            "y": "-658.851"
        },
        {
            "id": "liu-etal-2014-supervised",
            "name": "22",
            "x": "2648.6",
            "y": "-643.851"
        },
        {
            "id": "araki-etal-2014-detecting",
            "name": "jun2014Detecting",
            "x": "2944.6",
            "y": "-658.851"
        },
        {
            "id": "araki-etal-2014-detecting",
            "name": "16",
            "x": "2944.6",
            "y": "-643.851"
        },
        {
            "id": "C14-1066",
            "name": "sujay2014Inducing",
            "x": "3127.6",
            "y": "-658.851"
        },
        {
            "id": "C14-1066",
            "name": "1",
            "x": "3127.6",
            "y": "-643.851"
        },
        {
            "id": "W15-0802",
            "name": "nicolas2015Word",
            "x": "4123.6",
            "y": "-569.111"
        },
        {
            "id": "W15-0802",
            "name": "5",
            "x": "4123.6",
            "y": "-554.111"
        },
        {
            "id": "C18-1309",
            "name": "zhengzhong2018Graph",
            "x": "2805.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1309",
            "name": "2",
            "x": "2805.6",
            "y": "-284.89"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "W12-6102",
            "name": "hiroshi2012Optimization",
            "x": "5194.6",
            "y": "-838.331"
        },
        {
            "id": "W12-6102",
            "name": "2",
            "x": "5194.6",
            "y": "-823.331"
        },
        {
            "id": "R13-1030",
            "name": "hiroshi2013Automatic",
            "x": "5194.6",
            "y": "-748.591"
        },
        {
            "id": "R13-1030",
            "name": "1",
            "x": "5194.6",
            "y": "-733.591"
        },
        {
            "id": "N12-1083",
            "name": "donald2012Structured",
            "x": "5418.6",
            "y": "-838.331"
        },
        {
            "id": "N12-1083",
            "name": "73",
            "x": "5418.6",
            "y": "-823.331"
        },
        {
            "id": "penas-etal-2012-evaluating",
            "name": "anselmo2012Evaluating",
            "x": "5638.6",
            "y": "-838.331"
        },
        {
            "id": "penas-etal-2012-evaluating",
            "name": "5",
            "x": "5638.6",
            "y": "-823.331"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "P14-1060",
            "name": "shashank2014Vector",
            "x": "3841.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1060",
            "name": "4",
            "x": "3841.6",
            "y": "-643.851"
        },
        {
            "id": "W14-2910",
            "name": "jun2014Evaluation",
            "x": "3608.6",
            "y": "-658.851"
        },
        {
            "id": "W14-2910",
            "name": "???",
            "x": "3608.6",
            "y": "-643.851"
        },
        {
            "id": "W14-2303",
            "name": "marc2014Metaphor",
            "x": "4079.6",
            "y": "-658.851"
        },
        {
            "id": "W14-2303",
            "name": "17",
            "x": "4079.6",
            "y": "-643.851"
        },
        {
            "id": "W17-5538",
            "name": "hyeju2017Finding",
            "x": "4048.6",
            "y": "-389.631"
        },
        {
            "id": "W17-5538",
            "name": "2",
            "x": "4048.6",
            "y": "-374.631"
        },
        {
            "id": "C14-1134",
            "name": "pradeep2014Modeling",
            "x": "3407.6",
            "y": "-658.851"
        },
        {
            "id": "C14-1134",
            "name": "8",
            "x": "3407.6",
            "y": "-643.851"
        },
        {
            "id": "P19-1461",
            "name": "naoki2019Toward",
            "x": "3241.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1461",
            "name": "0",
            "x": "3241.6",
            "y": "-195.15"
        },
        {
            "id": "J13-3001",
            "name": "rahul2013{S}quibs:",
            "x": "5399.6",
            "y": "-748.591"
        },
        {
            "id": "J13-3001",
            "name": "65",
            "x": "5399.6",
            "y": "-733.591"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "W14-3349",
            "name": "hiroshi2014Application",
            "x": "4931.6",
            "y": "-658.851"
        },
        {
            "id": "W14-3349",
            "name": "2",
            "x": "4931.6",
            "y": "-643.851"
        },
        {
            "id": "P14-2006",
            "name": "sameer2014Scoring",
            "x": "2419.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2006",
            "name": "67",
            "x": "2419.6",
            "y": "-643.851"
        },
        {
            "id": "P14-1016",
            "name": "jiwei2014Weakly",
            "x": "4571.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1016",
            "name": "56",
            "x": "4571.6",
            "y": "-643.851"
        },
        {
            "id": "P14-1147",
            "name": "jiwei2014Towards",
            "x": "5136.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1147",
            "name": "133",
            "x": "5136.6",
            "y": "-643.851"
        },
        {
            "id": "jain-etal-2014-corpus",
            "name": "siddharth2014A",
            "x": "5312.6",
            "y": "-658.851"
        },
        {
            "id": "jain-etal-2014-corpus",
            "name": "3",
            "x": "5312.6",
            "y": "-643.851"
        },
        {
            "id": "W16-6005",
            "name": "dheeraj2016Unsupervised",
            "x": "2581.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6005",
            "name": "2",
            "x": "2581.6",
            "y": "-464.371"
        },
        {
            "id": "2020.blackboxnlp-1.1",
            "name": "varun2020{BERT}ering",
            "x": "2944.6",
            "y": "-120.41"
        },
        {
            "id": "2020.blackboxnlp-1.1",
            "name": "???",
            "x": "2944.6",
            "y": "-105.41"
        },
        {
            "id": "D15-1278",
            "name": "jiwei2015When",
            "x": "676.597",
            "y": "-569.111"
        },
        {
            "id": "D15-1278",
            "name": "41",
            "x": "676.597",
            "y": "-554.111"
        },
        {
            "id": "D14-1220",
            "name": "jiwei2014Recursive",
            "x": "757.597",
            "y": "-658.851"
        },
        {
            "id": "D14-1220",
            "name": "78",
            "x": "757.597",
            "y": "-643.851"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "D15-1154",
            "name": "xuezhe2015Efficient",
            "x": "4532.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1154",
            "name": "5",
            "x": "4532.6",
            "y": "-554.111"
        },
        {
            "id": "I17-1007",
            "name": "xuezhe2017Neural",
            "x": "4551.6",
            "y": "-389.631"
        },
        {
            "id": "I17-1007",
            "name": "11",
            "x": "4551.6",
            "y": "-374.631"
        },
        {
            "id": "P18-1130",
            "name": "xuezhe2018Stack-Pointer",
            "x": "4440.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1130",
            "name": "1",
            "x": "4440.6",
            "y": "-284.89"
        },
        {
            "id": "W15-0807",
            "name": "zhengzhong2015Evaluation",
            "x": "4759.6",
            "y": "-569.111"
        },
        {
            "id": "W15-0807",
            "name": "???",
            "x": "4759.6",
            "y": "-554.111"
        },
        {
            "id": "N15-1070",
            "name": "sujay2015Ontologically",
            "x": "1092.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1070",
            "name": "58",
            "x": "1092.6",
            "y": "-554.111"
        },
        {
            "id": "D15-1284",
            "name": "diyi2015Humor",
            "x": "1362.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1284",
            "name": "41",
            "x": "1362.6",
            "y": "-554.111"
        },
        {
            "id": "P17-1191",
            "name": "pradeep2017Ontology-Aware",
            "x": "817.597",
            "y": "-389.631"
        },
        {
            "id": "P17-1191",
            "name": "3",
            "x": "817.597",
            "y": "-374.631"
        },
        {
            "id": "N15-1184",
            "name": "manaal2015Retrofitting",
            "x": "868.597",
            "y": "-569.111"
        },
        {
            "id": "N15-1184",
            "name": "247",
            "x": "868.597",
            "y": "-554.111"
        },
        {
            "id": "P16-1045",
            "name": "sujay2016Tables",
            "x": "868.597",
            "y": "-479.371"
        },
        {
            "id": "P16-1045",
            "name": "17",
            "x": "868.597",
            "y": "-464.371"
        },
        {
            "id": "P18-1225",
            "name": "dongyeop2018{A}dv{E}ntu{R}e:",
            "x": "1278.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1225",
            "name": "11",
            "x": "1278.6",
            "y": "-284.89"
        },
        {
            "id": "P16-1101",
            "name": "xuezhe2016End-to-end",
            "x": "4682.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1101",
            "name": "528",
            "x": "4682.6",
            "y": "-464.371"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "P16-1228",
            "name": "zhiting2016Harnessing",
            "x": "4902.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1228",
            "name": "31",
            "x": "4902.6",
            "y": "-464.371"
        },
        {
            "id": "P19-1562",
            "name": "zhisong2019An",
            "x": "4609.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1562",
            "name": "1",
            "x": "4609.6",
            "y": "-195.15"
        },
        {
            "id": "2020.tacl-1.39",
            "name": "takashi2020Nested",
            "x": "4738.6",
            "y": "-120.41"
        },
        {
            "id": "2020.tacl-1.39",
            "name": "???",
            "x": "4738.6",
            "y": "-105.41"
        },
        {
            "id": "P16-1126",
            "name": "shomir2016The",
            "x": "5093.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1126",
            "name": "33",
            "x": "5093.6",
            "y": "-464.371"
        },
        {
            "id": "N16-1082",
            "name": "jiwei2016Visualizing",
            "x": "5275.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1082",
            "name": "174",
            "x": "5275.6",
            "y": "-464.371"
        },
        {
            "id": "2020.findings-emnlp.344",
            "name": "salvador2020Event-Related",
            "x": "5201.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.344",
            "name": "???",
            "x": "5201.6",
            "y": "-105.41"
        },
        {
            "id": "2021.emnlp-main.64",
            "name": "dheeraj2021{SELFEXPLAIN}:",
            "x": "5349.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.64",
            "name": "???",
            "x": "5349.6",
            "y": "-15.6701"
        },
        {
            "id": "N16-1174",
            "name": "zichao2016Hierarchical",
            "x": "1723.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1174",
            "name": "1037",
            "x": "1723.6",
            "y": "-464.371"
        },
        {
            "id": "N19-1364",
            "name": "diyi2019Let{'}s",
            "x": "1875.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1364",
            "name": "4",
            "x": "1875.6",
            "y": "-195.15"
        },
        {
            "id": "D19-1589",
            "name": "dongyeop2019Linguistic",
            "x": "1473.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1589",
            "name": "0",
            "x": "1473.6",
            "y": "-195.15"
        },
        {
            "id": "L16-1206",
            "name": "diyi2016Edit",
            "x": "5444.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1206",
            "name": "5",
            "x": "5444.6",
            "y": "-464.371"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "W17-4902",
            "name": "harsh2017Shakespearizing",
            "x": "3833.6",
            "y": "-389.631"
        },
        {
            "id": "W17-4902",
            "name": "13",
            "x": "3833.6",
            "y": "-374.631"
        },
        {
            "id": "W17-4415",
            "name": "bahar2017Huntsville,",
            "x": "5471.6",
            "y": "-389.631"
        },
        {
            "id": "W17-4415",
            "name": "3",
            "x": "5471.6",
            "y": "-374.631"
        },
        {
            "id": "W17-2703",
            "name": "evangelia2017Event",
            "x": "5671.6",
            "y": "-389.631"
        },
        {
            "id": "W17-2703",
            "name": "3",
            "x": "5671.6",
            "y": "-374.631"
        },
        {
            "id": "P17-1088",
            "name": "qizhe2017An",
            "x": "5840.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1088",
            "name": "14",
            "x": "5840.6",
            "y": "-374.631"
        },
        {
            "id": "D18-1257",
            "name": "qizhe2018Large-scale",
            "x": "824.597",
            "y": "-299.89"
        },
        {
            "id": "D18-1257",
            "name": "12",
            "x": "824.597",
            "y": "-284.89"
        },
        {
            "id": "I17-3016",
            "name": "jiarui2017{STCP}:",
            "x": "6007.6",
            "y": "-389.631"
        },
        {
            "id": "I17-3016",
            "name": "1",
            "x": "6007.6",
            "y": "-374.631"
        },
        {
            "id": "2020.acl-main.502",
            "name": "xiang2020{SCDE}:",
            "x": "882.597",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.502",
            "name": "0",
            "x": "882.597",
            "y": "-105.41"
        },
        {
            "id": "D17-1213",
            "name": "diyi2017Identifying",
            "x": "6198.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1213",
            "name": "11",
            "x": "6198.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1292",
            "name": "dongyeop2017Detecting",
            "x": "1324.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1292",
            "name": "3",
            "x": "1324.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1315",
            "name": "varun2017{C}harmanteau:",
            "x": "3584.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1315",
            "name": "3",
            "x": "3584.6",
            "y": "-374.631"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "D19-1437",
            "name": "xuezhe2019{F}low{S}eq:",
            "x": "4404.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1437",
            "name": "3",
            "x": "4404.6",
            "y": "-195.15"
        },
        {
            "id": "P18-1154",
            "name": "harsh2018Learning",
            "x": "5463.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1154",
            "name": "4",
            "x": "5463.6",
            "y": "-284.89"
        },
        {
            "id": "P18-1155",
            "name": "zihang2018From",
            "x": "5645.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1155",
            "name": "1",
            "x": "5645.6",
            "y": "-284.89"
        },
        {
            "id": "2020.deelio-1.4",
            "name": "steven2020{G}en{A}ug:",
            "x": "1150.6",
            "y": "-120.41"
        },
        {
            "id": "2020.deelio-1.4",
            "name": "???",
            "x": "1150.6",
            "y": "-105.41"
        },
        {
            "id": "2021.findings-acl.84",
            "name": "steven2021A",
            "x": "1219.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.findings-acl.84",
            "name": "???",
            "x": "1219.6",
            "y": "-15.6701"
        },
        {
            "id": "N18-1149",
            "name": "dongyeop2018A",
            "x": "1598.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1149",
            "name": "16",
            "x": "1598.6",
            "y": "-284.89"
        },
        {
            "id": "C18-1007",
            "name": "aldrian2018Low-resource",
            "x": "5853.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1007",
            "name": "2",
            "x": "5853.6",
            "y": "-284.89"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "W19-5009",
            "name": "dheeraj2019Domain",
            "x": "5467.6",
            "y": "-210.15"
        },
        {
            "id": "W19-5009",
            "name": "1",
            "x": "5467.6",
            "y": "-195.15"
        },
        {
            "id": "W19-4502",
            "name": "yohan2019A",
            "x": "5636.6",
            "y": "-210.15"
        },
        {
            "id": "W19-4502",
            "name": "1",
            "x": "5636.6",
            "y": "-195.15"
        },
        {
            "id": "2020.lrec-1.127",
            "name": "yohan2020Machine-Aided",
            "x": "5564.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.127",
            "name": "???",
            "x": "5564.6",
            "y": "-105.41"
        },
        {
            "id": "2020.emnlp-main.2",
            "name": "yohan2020Extracting",
            "x": "5792.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.2",
            "name": "???",
            "x": "5792.6",
            "y": "-105.41"
        },
        {
            "id": "P19-1329",
            "name": "aakanksha2019Exploring",
            "x": "5825.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1329",
            "name": "4",
            "x": "5825.6",
            "y": "-195.15"
        },
        {
            "id": "N19-1186",
            "name": "hiroshi2019Word",
            "x": "6033.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1186",
            "name": "0",
            "x": "6033.6",
            "y": "-195.15"
        },
        {
            "id": "N19-1253",
            "name": "wasi2019On",
            "x": "6192.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1253",
            "name": "18",
            "x": "6192.6",
            "y": "-195.15"
        },
        {
            "id": "N19-1273",
            "name": "pradeep2019Iterative",
            "x": "6365.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1273",
            "name": "7",
            "x": "6365.6",
            "y": "-195.15"
        },
        {
            "id": "2020.acl-main.473",
            "name": "shi2020Measuring",
            "x": "1875.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.473",
            "name": "???",
            "x": "1875.6",
            "y": "-105.41"
        },
        {
            "id": "K19-1033",
            "name": "abhilasha2019{EQUATE}:",
            "x": "6595.6",
            "y": "-210.15"
        },
        {
            "id": "K19-1033",
            "name": "5",
            "x": "6595.6",
            "y": "-195.15"
        },
        {
            "id": "2021.eacl-main.295",
            "name": "abhilasha2021Probing",
            "x": "6595.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.eacl-main.295",
            "name": "???",
            "x": "6595.6",
            "y": "-15.6701"
        },
        {
            "id": "J19-4002",
            "name": "mrinmaya2019Discourse",
            "x": "6840.6",
            "y": "-210.15"
        },
        {
            "id": "J19-4002",
            "name": "???",
            "x": "6840.6",
            "y": "-195.15"
        },
        {
            "id": "D19-5316",
            "name": "vaibhav2019Do",
            "x": "7040.6",
            "y": "-210.15"
        },
        {
            "id": "D19-5316",
            "name": "1",
            "x": "7040.6",
            "y": "-195.15"
        },
        {
            "id": "2020.emnlp-main.529",
            "name": "dongyeop2020Plan",
            "x": "1473.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.529",
            "name": "???",
            "x": "1473.6",
            "y": "-105.41"
        },
        {
            "id": "2021",
            "name": "2021",
            "x": "28.5975",
            "y": "-23.1701"
        },
        {
            "id": "2020.starsem-1.10",
            "name": "abhilasha2020On",
            "x": "6702.6",
            "y": "-120.41"
        },
        {
            "id": "2020.starsem-1.10",
            "name": "???",
            "x": "6702.6",
            "y": "-105.41"
        },
        {
            "id": "2020.sdp-1.1",
            "name": "muthu2020Overview",
            "x": "6892.6",
            "y": "-120.41"
        },
        {
            "id": "2020.sdp-1.1",
            "name": "???",
            "x": "6892.6",
            "y": "-105.41"
        },
        {
            "id": "2020.sdp-1.24",
            "name": "muthu2020Overview",
            "x": "7095.6",
            "y": "-120.41"
        },
        {
            "id": "2020.sdp-1.24",
            "name": "???",
            "x": "7095.6",
            "y": "-105.41"
        },
        {
            "id": "2020.findings-emnlp.160",
            "name": "zhisong2020An",
            "x": "7278.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.160",
            "name": "???",
            "x": "7278.6",
            "y": "-105.41"
        },
        {
            "id": "2020.findings-emnlp.300",
            "name": "dheeraj2020What-if",
            "x": "7457.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.300",
            "name": "0",
            "x": "7457.6",
            "y": "-105.41"
        },
        {
            "id": "2020.emnlp-main.1",
            "name": "yohan2020Detecting",
            "x": "7655.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.1",
            "name": "???",
            "x": "7655.6",
            "y": "-105.41"
        },
        {
            "id": "2020.emnlp-main.79",
            "name": "xiang2020Incorporating",
            "x": "7870.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.79",
            "name": "???",
            "x": "7870.6",
            "y": "-105.41"
        },
        {
            "id": "2020.emnlp-main.520",
            "name": "niket2020A",
            "x": "8050.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.520",
            "name": "???",
            "x": "8050.6",
            "y": "-105.41"
        },
        {
            "id": "2020.coling-main.273",
            "name": "evangelia2020Definition",
            "x": "8229.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.273",
            "name": "???",
            "x": "8229.6",
            "y": "-105.41"
        },
        {
            "id": "2020.blackboxnlp-1.20",
            "name": "andrew2020Exploring",
            "x": "8449.6",
            "y": "-120.41"
        },
        {
            "id": "2020.blackboxnlp-1.20",
            "name": "???",
            "x": "8449.6",
            "y": "-105.41"
        },
        {
            "id": "2020.acl-main.667",
            "name": "zhisong2020A",
            "x": "8632.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.667",
            "name": "???",
            "x": "8632.6",
            "y": "-105.41"
        },
        {
            "id": "2021.spnlp-1.8",
            "name": "zhisong2021Comparing",
            "x": "6817.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.spnlp-1.8",
            "name": "???",
            "x": "6817.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.sigmorphon-1.22",
            "name": "maria2021Comparative",
            "x": "7043.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.sigmorphon-1.22",
            "name": "???",
            "x": "7043.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.naacl-main.171",
            "name": "yiwei2021{S}tyle{PTB}:",
            "x": "7274.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.171",
            "name": "???",
            "x": "7274.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.inlg-1.21",
            "name": "steven2021{SAPPHIRE}:",
            "x": "7517.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.inlg-1.21",
            "name": "???",
            "x": "7517.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.findings-emnlp.264",
            "name": "yohan2021Knowledge-Enhanced",
            "x": "7791.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.findings-emnlp.264",
            "name": "???",
            "x": "7791.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.findings-acl.357",
            "name": "varun2021Improving",
            "x": "8044.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.findings-acl.357",
            "name": "???",
            "x": "8044.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.emnlp-main.592",
            "name": "harsh2021Investigating",
            "x": "8256.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.592",
            "name": "???",
            "x": "8256.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.findings-acl.456",
            "name": "aman2021Could",
            "x": "8451.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.findings-acl.456",
            "name": "???",
            "x": "8451.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.emnlp-main.508",
            "name": "aman2021Think",
            "x": "8619.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.508",
            "name": "???",
            "x": "8619.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.emnlp-main.503",
            "name": "zhisong2021On",
            "x": "8785.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.503",
            "name": "???",
            "x": "8785.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.eacl-main.259",
            "name": "abhilasha2021{N}oise{QA}:",
            "x": "9002.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.eacl-main.259",
            "name": "???",
            "x": "9002.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.acl-long.94",
            "name": "rishabh2021More",
            "x": "9226.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.acl-long.94",
            "name": "???",
            "x": "9226.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.acl-long.494",
            "name": "ruifan2021Dual",
            "x": "9396.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.acl-long.494",
            "name": "???",
            "x": "9396.6",
            "y": "-15.6701"
        }
    ],
    [
        "28.5975,-2916.55 28.5975,-2916.55 28.5975,-2916.55 28.5975,-2916.55",
        "482.77,-2840.98 473.714,-2835.48 477.364,-2845.43 482.77,-2840.98",
        "814.696,-2215.74 808.178,-2207.39 808.064,-2217.98 814.696,-2215.74",
        "600.098,-692.213 596.597,-682.213 593.098,-692.213 600.098,-692.213",
        "447.99,-2846.18 446.774,-2835.65 441.161,-2844.64 447.99,-2846.18",
        "28.5975,-2826.81 28.5975,-2826.81 28.5975,-2826.81 28.5975,-2826.81",
        "28.5975,-2737.07 28.5975,-2737.07 28.5975,-2737.07 28.5975,-2737.07",
        "28.5975,-2647.33 28.5975,-2647.33 28.5975,-2647.33 28.5975,-2647.33",
        "28.5975,-2557.59 28.5975,-2557.59 28.5975,-2557.59 28.5975,-2557.59",
        "462.643,-2487.24 463.66,-2476.69 456.288,-2484.3 462.643,-2487.24",
        "254.263,-2481.21 244.763,-2476.52 249.265,-2486.11 254.263,-2481.21",
        "28.5975,-2467.85 28.5975,-2467.85 28.5975,-2467.85 28.5975,-2467.85",
        "28.5975,-2378.11 28.5975,-2378.11 28.5975,-2378.11 28.5975,-2378.11",
        "791.219,-2218.04 790.77,-2207.45 784.52,-2216.01 791.219,-2218.04",
        "661.746,-2480.01 669.665,-2472.97 659.085,-2473.54 661.746,-2480.01",
        "223.098,-2217.85 219.597,-2207.85 216.098,-2217.85 223.098,-2217.85",
        "28.5975,-2288.37 28.5975,-2288.37 28.5975,-2288.37 28.5975,-2288.37",
        "28.5975,-2198.63 28.5975,-2198.63 28.5975,-2198.63 28.5975,-2198.63",
        "1026.48,-2273.87 1036.48,-2270.37 1026.48,-2266.87 1026.48,-2273.87",
        "28.5975,-2108.89 28.5975,-2108.89 28.5975,-2108.89 28.5975,-2108.89",
        "5318.63,-2122.91 5325.7,-2115.01 5315.25,-2116.78 5318.63,-2122.91",
        "5101.32,-1945.76 5093.87,-1938.23 5095,-1948.77 5101.32,-1945.76",
        "5369.73,-1755.96 5378.61,-1750.17 5368.06,-1749.16 5369.73,-1755.96",
        "5097.42,-2184.13 5107.42,-2180.63 5097.42,-2177.13 5097.42,-2184.13",
        "5010.21,-2126.32 5015.08,-2116.91 5005.4,-2121.23 5010.21,-2126.32",
        "5373.72,-1756.89 5382.64,-1751.16 5372.1,-1750.08 5373.72,-1756.89",
        "4692.15,-1409.46 4695.6,-1399.44 4686.66,-1405.12 4692.15,-1409.46",
        "2062.1,-1410.13 2058.6,-1400.13 2055.1,-1410.13 2062.1,-1410.13",
        "1712.23,-242.987 1708.17,-233.197 1705.24,-243.376 1712.23,-242.987",
        "28.5975,-2019.15 28.5975,-2019.15 28.5975,-2019.15 28.5975,-2019.15",
        "5264.53,-2094.39 5274.53,-2090.89 5264.53,-2087.39 5264.53,-2094.39",
        "28.5975,-1929.41 28.5975,-1929.41 28.5975,-1929.41 28.5975,-1929.41",
        "28.5975,-1839.67 28.5975,-1839.67 28.5975,-1839.67 28.5975,-1839.67",
        "5220.86,-1767.95 5224.61,-1758.05 5215.49,-1763.45 5220.86,-1767.95",
        "5040.91,-1767.29 5034.6,-1758.77 5034.22,-1769.36 5040.91,-1767.29",
        "2336.57,-1674.6 2328.01,-1668.35 2330.8,-1678.57 2336.57,-1674.6",
        "5307.52,-1045.67 5314.66,-1037.83 5304.19,-1039.51 5307.52,-1045.67",
        "1753.07,-232.714 1742.84,-229.981 1749.13,-238.501 1753.07,-232.714",
        "28.5975,-1749.93 28.5975,-1749.93 28.5975,-1749.93 28.5975,-1749.93",
        "28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19",
        "5399.11,-1762.09 5407.35,-1755.43 5396.76,-1755.5 5399.11,-1762.09",
        "1832.42,-1673.3 1823.12,-1668.21 1827.21,-1677.98 1832.42,-1673.3",
        "1675.33,-1663.03 1664.75,-1663.37 1673.23,-1669.7 1675.33,-1663.03",
        "1854.01,-1408.43 1847.82,-1399.83 1847.29,-1410.41 1854.01,-1408.43",
        "2643.98,-1716.56 2654.34,-1714.37 2644.87,-1709.62 2643.98,-1716.56",
        "3278.53,-1649.48 3288.39,-1645.62 3278.27,-1642.49 3278.53,-1649.48",
        "2631.23,-1500.09 2632.1,-1489.53 2624.83,-1497.24 2631.23,-1500.09",
        "2259.02,-1222.99 2249.18,-1219.05 2254.42,-1228.26 2259.02,-1222.99",
        "2015.79,-1228.97 2009.34,-1220.56 2009.14,-1231.16 2015.79,-1228.97",
        "4761.51,-937.57 4770.49,-931.947 4759.96,-930.743 4761.51,-937.57",
        "5338.14,-1735.43 5348.14,-1731.93 5338.14,-1728.43 5338.14,-1735.43",
        "5336.21,-1675.79 5342.65,-1667.38 5332.37,-1669.93 5336.21,-1675.79",
        "5332.47,-1049.14 5337.76,-1039.95 5327.9,-1043.84 5332.47,-1049.14",
        "2602.42,-1747.88 2592.12,-1750.36 2601.72,-1754.84 2602.42,-1747.88",
        "2656.54,-1498.28 2650.55,-1489.53 2649.78,-1500.1 2656.54,-1498.28",
        "3447.02,-1660.55 3436.48,-1661.62 3445.39,-1667.36 3447.02,-1660.55",
        "4929.32,-1225.35 4920.33,-1219.76 4923.87,-1229.74 4929.32,-1225.35",
        "5021.95,-1140.48 5017.47,-1130.88 5014.99,-1141.18 5021.95,-1140.48",
        "5151.93,-1049.72 5146.02,-1040.93 5145.16,-1051.49 5151.93,-1049.72",
        "28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45",
        "3799.96,-1410.13 3796.26,-1400.2 3792.96,-1410.27 3799.96,-1410.13",
        "3763.62,-1585.8 3755.39,-1579.12 3757.65,-1589.47 3763.62,-1585.8",
        "3934.03,-1584.6 3941.84,-1577.43 3931.27,-1578.17 3934.03,-1584.6",
        "3435.03,-1488.37 3424.64,-1486.29 3431.47,-1494.4 3435.03,-1488.37",
        "4255.66,-1410.3 4257.2,-1399.82 4249.46,-1407.05 4255.66,-1410.3",
        "4278,-951.825 4267.9,-948.636 4273.8,-957.43 4278,-951.825",
        "3110.79,-1570.82 3100.23,-1571.77 3109.07,-1577.6 3110.79,-1570.82",
        "4665.88,-1577.23 4675.13,-1572.06 4664.67,-1570.33 4665.88,-1577.23",
        "4450.92,-1577.5 4459.9,-1571.88 4449.37,-1570.68 4450.92,-1577.5",
        "3388.66,-1499.93 3385.84,-1489.72 3381.67,-1499.46 3388.66,-1499.93",
        "2697.85,-1488.12 2687.34,-1486.75 2694.7,-1494.37 2697.85,-1488.12",
        "1712.09,-1645.69 1722.09,-1642.19 1712.09,-1638.69 1712.09,-1645.69",
        "2167.85,-1565.59 2177.53,-1561.29 2167.28,-1558.61 2167.85,-1565.59",
        "1566.68,-1585.4 1558.15,-1579.12 1560.9,-1589.35 1566.68,-1585.4",
        "1642.79,-1406.55 1634.84,-1399.55 1636.69,-1409.98 1642.79,-1406.55",
        "1390.53,-1410.39 1388.79,-1399.94 1383.63,-1409.2 1390.53,-1410.39",
        "1821.8,-1409.95 1824.15,-1399.62 1815.87,-1406.24 1821.8,-1409.95",
        "1292.89,-1319.89 1293.84,-1309.34 1286.52,-1316.99 1292.89,-1319.89",
        "1142.1,-415.304 1132.27,-411.334 1137.48,-420.563 1142.1,-415.304",
        "3519.59,-242.195 3523.64,-232.405 3514.37,-237.533 3519.59,-242.195",
        "2299.26,-1587.88 2292.88,-1579.42 2292.59,-1590.01 2299.26,-1587.88",
        "5977.54,-1622.93 5967.32,-1625.71 5977.04,-1629.91 5977.54,-1622.93",
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "2563.32,-1486.94 2572.2,-1481.16 2561.66,-1480.14 2563.32,-1486.94",
        "3139.06,-1555.95 3149.06,-1552.45 3139.06,-1548.95 3139.06,-1555.95",
        "3317.93,-1484.29 3326.82,-1478.53 3316.27,-1477.49 3317.93,-1484.29",
        "2877.07,-1402.98 2867.42,-1398.61 2872.24,-1408.05 2877.07,-1402.98",
        "4281.09,-1409.11 4275.73,-1399.98 4274.23,-1410.47 4281.09,-1409.11",
        "4821.48,-1384.3 4810.9,-1384.82 4819.5,-1391.01 4821.48,-1384.3",
        "3244.95,-1308.89 3234.46,-1307.38 3241.71,-1315.1 3244.95,-1308.89",
        "4393.95,-691.777 4389.47,-682.176 4386.99,-692.475 4393.95,-691.777",
        "3647.65,-29.4484 3637.45,-32.2994 3647.2,-36.434 3647.65,-29.4484",
        "3346.43,-1495.06 3352.85,-1486.63 3342.58,-1489.22 3346.43,-1495.06",
        "3098.74,-1403.38 3089.01,-1399.18 3094.01,-1408.53 3098.74,-1403.38",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "3736.19,-1443.47 3725.97,-1446.25 3735.69,-1450.45 3736.19,-1443.47",
        "3664.42,-1230.77 3661.11,-1220.71 3657.42,-1230.64 3664.42,-1230.77",
        "5463.66,-1466.21 5473.66,-1462.71 5463.66,-1459.21 5463.66,-1466.21",
        "5374.1,-1409.98 5370.6,-1399.98 5367.1,-1409.98 5374.1,-1409.98",
        "3147.28,-1393.41 3136.71,-1394.19 3145.46,-1400.17 3147.28,-1393.41",
        "3632.84,-1227.43 3637.55,-1217.94 3627.95,-1222.42 3632.84,-1227.43",
        "3324.17,-1228.95 3318.04,-1220.31 3317.44,-1230.89 3324.17,-1228.95",
        "4747.04,-1405.65 4738.32,-1399.64 4741.38,-1409.78 4747.04,-1405.65",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "4381.66,-692.555 4380.37,-682.039 4374.82,-691.063 4381.66,-692.555",
        "2992.69,-1312.02 2982.71,-1308.47 2988.31,-1317.47 2992.69,-1312.02",
        "2828.58,-1229.86 2823.38,-1220.63 2821.69,-1231.09 2828.58,-1229.86",
        "3320.22,-1230.31 3315.67,-1220.74 3313.26,-1231.05 3320.22,-1230.31",
        "2634.28,-1220.09 2623.77,-1218.69 2631.11,-1226.34 2634.28,-1220.09",
        "3070.67,-1230.63 3067.57,-1220.5 3063.68,-1230.35 3070.67,-1230.63",
        "3125.14,-1020.56 3114.74,-1022.57 3124.13,-1027.48 3125.14,-1020.56",
        "2291.75,-678.045 2281.24,-676.681 2288.6,-684.299 2291.75,-678.045",
        "2015.65,-688.418 2007.4,-681.782 2009.71,-692.12 2015.65,-688.418",
        "4348,-511.989 4351.84,-502.115 4342.68,-507.439 4348,-511.989",
        "1590.32,-421.966 1585.13,-412.729 1583.42,-423.186 1590.32,-421.966",
        "4624.13,-1392.13 4633.48,-1387.16 4623.07,-1385.21 4624.13,-1392.13",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "5645.85,-1286.73 5655.85,-1283.23 5645.85,-1279.73 5645.85,-1286.73",
        "2866.97,-1221.12 2856.71,-1218.5 2863.1,-1226.95 2866.97,-1221.12",
        "3236.76,-1217.81 3245.55,-1211.9 3234.99,-1211.03 3236.76,-1217.81",
        "2671.94,-1212.97 2661.41,-1214.1 2670.35,-1219.79 2671.94,-1212.97",
        "3029.45,-1228.49 3035.32,-1219.66 3025.23,-1222.9 3029.45,-1228.49",
        "2203.76,-1230.68 2205.02,-1220.16 2197.48,-1227.6 2203.76,-1230.68",
        "2058.06,-1218.85 2047.56,-1217.43 2054.88,-1225.09 2058.06,-1218.85",
        "2900.48,-1210.88 2889.93,-1211.89 2898.8,-1217.68 2900.48,-1210.88",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "4976.28,-1137.86 4982.18,-1129.06 4972.08,-1132.26 4976.28,-1137.86",
        "5143.12,-1051.09 5139.9,-1041 5136.12,-1050.9 5143.12,-1051.09",
        "4783.85,-1046.29 4774.99,-1040.48 4778.3,-1050.55 4783.85,-1046.29",
        "4934.5,-1050.18 4937.14,-1039.92 4928.68,-1046.29 4934.5,-1050.18",
        "4868.2,-960.894 4863.33,-951.486 4861.27,-961.879 4868.2,-960.894",
        "4714,-690.98 4717.31,-680.918 4708.45,-686.715 4714,-690.98",
        "2109,-1196.99 2119,-1193.49 2109,-1189.99 2109,-1196.99",
        "3213.78,-1189.99 3203.78,-1193.49 3213.78,-1196.99 3213.78,-1189.99",
        "2535.17,-871.492 2531.87,-861.425 2528.17,-871.353 2535.17,-871.492",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "5098.18,-1047.9 5104.32,-1039.26 5094.14,-1042.18 5098.18,-1047.9",
        "4981.8,-1046.79 4973.36,-1040.37 4975.96,-1050.65 4981.8,-1046.79",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "4215.6,-961.831 4215.31,-951.24 4208.94,-959.703 4215.6,-961.831",
        "3551.52,-782.229 3548.73,-772.006 3544.53,-781.732 3551.52,-782.229",
        "3237.82,-948.545 3227.39,-946.665 3234.37,-954.637 3237.82,-948.545",
        "3353.28,-782.082 3350.01,-772.006 3346.29,-781.925 3353.28,-782.082",
        "3527.79,-781.115 3531.12,-771.058 3522.24,-776.841 3527.79,-781.115",
        "2788.86,-776.867 2779.84,-771.303 2783.42,-781.275 2788.86,-776.867",
        "4931.45,-944.199 4920.86,-944.538 4929.35,-950.877 4931.45,-944.199",
        "4775.22,-684.046 4765.43,-679.999 4770.56,-689.269 4775.22,-684.046",
        "4825.1,-958.952 4830.49,-949.828 4820.59,-953.602 4825.1,-958.952",
        "4743.82,-692.217 4739.85,-682.395 4736.83,-692.55 4743.82,-692.217",
        "2331.99,-316.179 2341.05,-310.685 2330.54,-309.331 2331.99,-316.179",
        "2679.1,-961.275 2675.6,-951.275 2672.1,-961.275 2679.1,-961.275",
        "3118.6,-940.941 3127.87,-935.808 3117.42,-934.041 3118.6,-940.941",
        "2549.09,-869.587 2542.25,-861.495 2542.55,-872.086 2549.09,-869.587",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "2581.88,-861.247 2571.53,-858.968 2578.2,-867.202 2581.88,-861.247",
        "3031.94,-773.54 3022.21,-769.348 3027.21,-778.693 3031.94,-773.54",
        "3706.68,-770.587 3715.24,-764.343 3704.65,-763.887 3706.68,-770.587",
        "3148.1,-780.113 3141.75,-771.629 3141.42,-782.219 3148.1,-780.113",
        "3323.49,-780.878 3327.62,-771.119 3318.31,-776.174 3323.49,-780.878",
        "3894.17,-769.4 3902.73,-763.163 3892.15,-762.697 3894.17,-769.4",
        "2649.33,-692.208 2646.05,-682.134 2642.33,-692.055 2649.33,-692.208",
        "2932.49,-692.527 2932.59,-681.933 2925.9,-690.15 2932.49,-692.527",
        "3177.09,-682.364 3167.08,-678.898 3172.75,-687.851 3177.09,-682.364",
        "4160.78,-595.776 4151.51,-590.66 4155.57,-600.445 4160.78,-595.776",
        "2818.93,-331.799 2813.02,-323.005 2812.16,-333.565 2818.93,-331.799",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "5198.1,-781.795 5194.6,-771.795 5191.1,-781.795 5198.1,-781.795",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "3064.19,-748.291 3074.19,-744.791 3064.19,-741.291 3064.19,-748.291",
        "3757.64,-676.785 3766.86,-671.559 3756.39,-669.897 3757.64,-676.785",
        "3095.52,-776.02 3102.25,-767.838 3091.89,-770.036 3095.52,-776.02",
        "3541.29,-680.343 3550.13,-674.5 3539.57,-673.556 3541.29,-680.343",
        "2690.3,-684.552 2680.28,-681.096 2685.96,-690.043 2690.3,-684.552",
        "2892.18,-684.786 2899.71,-677.338 2889.18,-678.461 2892.18,-684.786",
        "2806.83,-333.427 2803.92,-323.24 2799.84,-333.019 2806.83,-333.427",
        "3873.04,-748.291 3883.04,-744.791 3873.04,-741.291 3873.04,-748.291",
        "4008.37,-680.32 4017,-674.176 4006.43,-673.596 4008.37,-680.32",
        "3819.47,-691.795 3822.61,-681.676 3813.85,-687.628 3819.47,-691.795",
        "4043.41,-423.597 4042.4,-413.051 4036.61,-421.927 4043.41,-423.597",
        "2763.36,-666.829 2752.99,-669.02 2762.46,-673.772 2763.36,-666.829",
        "3000.93,-678.343 2990.38,-677.307 2997.98,-684.692 3000.93,-678.343",
        "3133.55,-691.793 3129.38,-682.055 3126.57,-692.27 3133.55,-691.793",
        "4031.55,-570.554 4041.5,-566.925 4031.46,-563.555 4031.55,-570.554",
        "3678.12,-748.291 3688.12,-744.791 3678.12,-741.291 3678.12,-748.291",
        "3456.71,-682.079 3446.36,-679.832 3453.05,-688.045 3456.71,-682.079",
        "3245.1,-243.513 3241.6,-233.513 3238.1,-243.513 3245.1,-243.513",
        "3687.54,-669.319 3677.02,-670.59 3686.04,-676.156 3687.54,-669.319",
        "3883.78,-683.831 3873.71,-680.548 3879.53,-689.396 3883.78,-683.831",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "2320.14,-658.551 2330.14,-655.051 2320.14,-651.551 2320.14,-658.551",
        "4656.94,-658.551 4666.94,-655.051 4656.94,-651.551 4656.94,-658.551",
        "2598.16,-510.528 2591.35,-502.409 2591.61,-513.001 2598.16,-510.528",
        "2439.28,-333.554 2436.82,-323.25 2432.32,-332.842 2439.28,-333.554",
        "2793.4,-333.515 2794.22,-322.952 2786.99,-330.697 2793.4,-333.515",
        "2498.26,-322.449 2487.78,-320.927 2495.03,-328.656 2498.26,-322.449",
        "2835.61,-328.316 2826.83,-322.384 2830,-332.496 2835.61,-328.316",
        "2948.1,-153.773 2944.6,-143.773 2941.1,-153.773 2948.1,-153.773",
        "650.055,-600.616 654.207,-590.868 644.883,-595.899 650.055,-600.616",
        "708.77,-596 699.415,-591.027 703.626,-600.749 708.77,-596",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "4431.98,-568.811 4441.98,-565.311 4431.98,-561.811 4431.98,-568.811",
        "4460.86,-396.705 4470.41,-392.107 4460.08,-389.749 4460.86,-396.705",
        "4373.29,-326.054 4381.16,-318.961 4370.58,-319.597 4373.29,-326.054",
        "1279.69,-568.811 1289.69,-565.311 1279.69,-561.811 1279.69,-568.811",
        "1516.72,-414.103 1524.92,-407.401 1514.33,-407.523 1516.72,-414.103",
        "888.793,-410.402 878.223,-409.681 886.036,-416.836 888.793,-410.402",
        "978.925,-568.811 988.925,-565.311 978.925,-561.811 978.925,-568.811",
        "872.098,-512.575 868.597,-502.575 865.098,-512.575 872.098,-512.575",
        "796.365,-423.002 798.526,-412.63 790.369,-419.391 796.365,-423.002",
        "1145.84,-315.98 1155.2,-311.002 1144.78,-309.061 1145.84,-315.98",
        "4635.22,-508.338 4642.09,-500.267 4631.69,-502.295 4635.22,-508.338",
        "4551.23,-423.152 4548.81,-412.837 4544.27,-422.407 4551.23,-423.152",
        "4444.5,-333.07 4440.49,-323.261 4437.5,-333.427 4444.5,-333.07",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "4818.85,-498.085 4828.44,-493.584 4818.13,-491.122 4818.85,-498.085",
        "4596.97,-413.01 4586.72,-410.353 4593.08,-418.826 4596.97,-413.01",
        "4551,-307.259 4540.45,-308.208 4549.29,-314.046 4551,-307.259",
        "4630.63,-240.212 4623.04,-232.817 4624.36,-243.33 4630.63,-240.212",
        "4736.45,-153.894 4734.54,-143.472 4729.53,-152.809 4736.45,-153.894",
        "4767.84,-452.931 4758.22,-457.386 4768.52,-459.898 4767.84,-452.931",
        "5212.42,-152.553 5206.96,-143.472 5205.56,-153.974 5212.42,-152.553",
        "5353.1,-64.0324 5349.6,-54.0324 5346.1,-64.0325 5353.1,-64.0324",
        "4569.84,-479.071 4579.84,-475.571 4569.84,-472.071 4569.84,-479.071",
        "4499.71,-416.31 4507.13,-408.749 4496.61,-410.031 4499.71,-416.31",
        "4430.59,-333.52 4430.84,-322.929 4424.04,-331.048 4430.59,-333.52",
        "1859.17,-243.318 1861.07,-232.895 1853.08,-239.857 1859.17,-243.318",
        "1479.6,-242.93 1474.88,-233.441 1472.65,-243.798 1479.6,-242.93",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "3597.04,-233.395 3586.7,-231.08 3593.34,-239.337 3597.04,-233.395",
        "825.216,-333.341 822.523,-323.094 818.239,-332.784 825.216,-333.341",
        "4482.3,-325.591 4472.28,-322.136 4477.96,-331.083 4482.3,-325.591",
        "4601.37,-243.781 4601.15,-233.189 4594.72,-241.607 4601.37,-243.781",
        "912.152,-148.301 903.119,-142.765 906.728,-152.726 912.152,-148.301",
        "1299.96,-330.291 1292.21,-323.059 1293.76,-333.541 1299.96,-330.291",
        "1472.25,-243.868 1470.57,-233.407 1465.36,-242.63 1472.25,-243.868",
        "3708.2,-389.331 3718.2,-385.831 3708.2,-382.331 3708.2,-389.331",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "4561.19,-236.427 4568.45,-228.713 4557.96,-230.215 4561.19,-236.427",
        "4422.29,-241.242 4415.25,-233.319 4415.81,-243.899 4422.29,-241.242",
        "1415.18,-237.475 1422.86,-230.18 1412.3,-231.091 1415.18,-237.475",
        "1177.71,-149.224 1169.02,-143.161 1172.04,-153.319 1177.71,-149.224",
        "1254,-57.0605 1244.85,-51.7156 1248.67,-61.5985 1254,-57.0605",
        "1669.83,-240.674 1675.3,-231.602 1665.36,-235.282 1669.83,-240.674",
        "1518.69,-234.728 1508.5,-231.847 1514.67,-240.457 1518.69,-234.728",
        "874.374,-154.041 874.154,-143.448 867.72,-151.867 874.374,-154.041",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "5594.6,-148.653 5585.55,-143.143 5589.19,-153.094 5594.6,-148.653",
        "5744.22,-148.834 5751.22,-140.881 5740.79,-142.733 5744.22,-148.834",
        "1879.1,-153.614 1875.6,-143.614 1872.1,-153.614 1879.1,-153.614",
        "6599.1,-63.8762 6595.6,-53.8762 6592.1,-63.8762 6599.1,-63.8762",
        "3552.1,-63.8762 3548.6,-53.8762 3545.1,-63.8762 3552.1,-63.8762",
        "3448.12,-32.3942 3458.09,-28.7842 3448.05,-25.3946 3448.12,-32.3942",
        "1477.1,-153.614 1473.6,-143.614 1470.1,-153.614 1477.1,-153.614",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701",
        "7017.19,-139.228 7026.76,-134.676 7016.44,-132.269 7017.19,-139.228",
        "6971.16,-93.9785 6961.59,-98.5253 6971.9,-100.939 6971.16,-93.9785",
        "5687.54,-120.11 5697.54,-116.61 5687.54,-113.11 5687.54,-120.11",
        "1196.69,-62.6559 1200.12,-52.6318 1191.19,-58.3287 1196.69,-62.6559",
        "8145.29,-30.3702 8155.29,-26.8701 8145.29,-23.3702 8145.29,-30.3702",
        "8535.04,-30.3702 8545.04,-26.8701 8535.04,-23.3702 8535.04,-30.3702"
    ]
]