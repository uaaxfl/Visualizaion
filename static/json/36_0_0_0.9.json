[
    [
        {
            "id": "2001",
            "citation_count": 136,
            "name": 136,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2003",
            "citation_count": 192,
            "name": 192,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-1812",
            "name": "Parsing and Hypergraphs",
            "publication_data": 2001,
            "citation": 91,
            "abstract": "None",
            "cx": 315.597,
            "cy": -1552.45,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1016",
            "name": "{A}* Parsing: Fast Exact {V}iterbi Parse Selection",
            "publication_data": 2003,
            "citation": 192,
            "abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of pre-computation, reduces the work to less than 5%. Un-like best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.",
            "cx": 315.597,
            "cy": -1462.71,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P01-1044",
            "name": "Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the {P}enn {T}reebank",
            "publication_data": 2001,
            "citation": 45,
            "abstract": "This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.",
            "cx": 149.597,
            "cy": -1552.45,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2004",
            "citation_count": 558,
            "name": 558,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-2037",
            "name": "Top-Down K-Best {A}* Parsing",
            "publication_data": 2010,
            "citation": 9,
            "abstract": "We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA* is a variant of the k-best A* (KA*) algorithm of Pauls and Klein (2009). In contrast to KA*, which performs an inside and outside pass before performing k-best extraction bottom up, TKA* performs only the inside pass before extracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement.",
            "cx": 469.597,
            "cy": -924.271,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 1194,
            "name": 1194,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W04-3201",
            "name": "Max-Margin Parsing",
            "publication_data": 2004,
            "citation": 198,
            "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the modelxe2x80x99s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.",
            "cx": 955.597,
            "cy": -1372.97,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D08-1091",
            "name": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing",
            "publication_data": 2008,
            "citation": 35,
            "abstract": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.",
            "cx": 955.597,
            "cy": -1103.75,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P04-1061",
            "name": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
            "publication_data": 2004,
            "citation": 360,
            "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
            "cx": 1221.6,
            "cy": -1372.97,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P08-1100",
            "name": "Analyzing the Errors of Unsupervised Learning",
            "publication_data": 2008,
            "citation": 21,
            "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
            "cx": 1153.6,
            "cy": -1103.75,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1083",
            "name": "Painless Unsupervised Learning with Features",
            "publication_data": 2010,
            "citation": 168,
            "abstract": "We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.",
            "cx": 1273.6,
            "cy": -924.271,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2007",
            "citation_count": 773,
            "name": 773,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P06-1055",
            "name": "Learning Accurate, Compact, and Interpretable Tree Annotation",
            "publication_data": 2006,
            "citation": 727,
            "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
            "cx": 1412.6,
            "cy": -1283.23,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D07-1094",
            "name": "Learning Structured Models for Phone Recognition",
            "publication_data": 2007,
            "citation": 13,
            "abstract": "We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",
            "cx": 1381.6,
            "cy": -1193.49,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1105",
            "name": "Training Factored {PCFG}s with Expectation Propagation",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguistically-motivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.",
            "cx": 1489.6,
            "cy": -744.791,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N06-1014",
            "name": "Alignment by Agreement",
            "publication_data": 2006,
            "citation": 392,
            "abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.",
            "cx": 1652.6,
            "cy": -1283.23,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P07-1003",
            "name": "Tailoring Word Alignments to Syntactic Machine Translation",
            "publication_data": 2007,
            "citation": 100,
            "abstract": "Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our modelxe2x80x99s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",
            "cx": 1652.6,
            "cy": -1193.49,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1015",
            "name": "Word Alignment via Quadratic Assignment",
            "publication_data": 2006,
            "citation": 75,
            "abstract": "Recently, discriminative word alignment methods have achieved state-of-the-art accuracies by extending the range of information sources that can be easily incorporated into aligners. The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on. However, the proposed bipartite matching model of Taskar et al. (2005), despite being tractable and effective, has two important limitations. First, it is limited by the restriction that words have fertility of at most one. More importantly, first order correlations between consecutive words cannot be directly captured by the model. In this work, we address these limitations by enriching the model form. We give estimation and inference algorithms for these enhancements. Our best model achieves a relative AER reduction of 25% over the basic matching formulation, outperforming intersected IBM Model 4 without using any overly compute-intensive features. By including predictions of other models as features, we achieve AER of 3.8 on the standard Hansards dataset.",
            "cx": 1859.6,
            "cy": -1283.23,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1147",
            "name": "Discriminative Modeling of Extraction Sets for Machine Translation",
            "publication_data": 2010,
            "citation": 25,
            "abstract": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.",
            "cx": 1859.6,
            "cy": -924.271,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 374,
            "name": 374,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N10-1014",
            "name": "Unsupervised Syntactic Alignment with Inversion Transduction Grammars",
            "publication_data": 2010,
            "citation": 18,
            "abstract": "Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.",
            "cx": 1632.6,
            "cy": -924.271,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P07-1107",
            "name": "Unsupervised Coreference Resolution in a Nonparametric {B}ayesian Model",
            "publication_data": 2007,
            "citation": 127,
            "abstract": "We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",
            "cx": 2073.6,
            "cy": -1193.49,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N10-1061",
            "name": "Coreference Resolution in a Modular, Entity-Centered Model",
            "publication_data": 2010,
            "citation": 119,
            "abstract": "Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.",
            "cx": 2073.6,
            "cy": -924.271,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N07-1051",
            "name": "Improved Inference for Unlexicalized Parsing",
            "publication_data": 2007,
            "citation": 533,
            "abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammarxe2x80x99s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.",
            "cx": 728.597,
            "cy": -1193.49,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W08-1005",
            "name": "Parsing {G}erman with Latent Variable Grammars",
            "publication_data": 2008,
            "citation": 23,
            "abstract": "We describe experiments on learning latent variable grammars for various German tree-banks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks.",
            "cx": 605.597,
            "cy": -1103.75,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1063",
            "name": "Hierarchical Search for Parsing",
            "publication_data": 2009,
            "citation": 17,
            "abstract": "Both coarse-to-fine and A* parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarse-to-fine case because of accumulated slack in A* heuristics.",
            "cx": 442.597,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1020",
            "name": "Sparser, Better, Faster {GPU} Parsing",
            "publication_data": 2014,
            "citation": 16,
            "abstract": "Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per secondxe2x80x94more than a 2x speedupxe2x80x94on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruningxe2x80x94nearly a 6x speedup.",
            "cx": 747.597,
            "cy": -565.311,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1022",
            "name": "Less Grammar, More Features",
            "publication_data": 2014,
            "citation": 43,
            "abstract": "We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bjorkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.",
            "cx": 922.597,
            "cy": -565.311,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2009",
            "citation_count": 325,
            "name": 325,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P08-1088",
            "name": "Learning Bilingual Lexicons from Monolingual Corpora",
            "publication_data": 2008,
            "citation": 259,
            "abstract": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
            "cx": 2222.6,
            "cy": -1103.75,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D11-1029",
            "name": "Simple Effective Decipherment via Combinatorial Optimization",
            "publication_data": 2011,
            "citation": 11,
            "abstract": "We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.",
            "cx": 2277.6,
            "cy": -834.531,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1012",
            "name": "Coarse-to-Fine Syntactic Machine Translation using Language Projections",
            "publication_data": 2008,
            "citation": 36,
            "abstract": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incrementally introduced. In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.",
            "cx": 2424.6,
            "cy": -1103.75,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1026",
            "name": "Efficient Parsing for Transducer Grammars",
            "publication_data": 2009,
            "citation": 24,
            "abstract": "The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized. We address the problem of parsing efficiently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU.",
            "cx": 2424.6,
            "cy": -1014.01,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 456,
            "name": 456,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P09-1104",
            "name": "Better Word Alignments with Supervised {ITG} Models",
            "publication_data": 2009,
            "citation": 91,
            "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA alignments.",
            "cx": 2590.6,
            "cy": -1014.01,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N12-1004",
            "name": "Fast Inference in Phrase Extraction Models with Belief Propagation",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up.",
            "cx": 2590.6,
            "cy": -744.791,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P09-1108",
            "name": "K-Best {A}* Parsing",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "A* parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing k-best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types.",
            "cx": 639.597,
            "cy": -1014.01,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D09-1120",
            "name": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
            "publication_data": 2009,
            "citation": 158,
            "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).",
            "cx": 2778.6,
            "cy": -1014.01,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P12-1041",
            "name": "Coreference Semantics from Web Features",
            "publication_data": 2012,
            "citation": 36,
            "abstract": "To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.",
            "cx": 2778.6,
            "cy": -744.791,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2011",
            "citation_count": 186,
            "name": 186,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1105",
            "name": "Finding Cognate Groups Using Phylogenies",
            "publication_data": 2010,
            "citation": 23,
            "abstract": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.",
            "cx": 2332.6,
            "cy": -924.271,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-1916",
            "name": "Mention Detection: Heuristics for the {O}nto{N}otes annotations",
            "publication_data": 2011,
            "citation": 10,
            "abstract": "Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3, Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.",
            "cx": 1997.6,
            "cy": -834.531,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D13-1203",
            "name": "Easy Victories and Uphill Battles in Coreference Resolution",
            "publication_data": 2013,
            "citation": 126,
            "abstract": "Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win xe2x80x9ceasy victoriesxe2x80x9d without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an xe2x80x9cuphill battle.xe2x80x9d Nonetheless, our final system 1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bjxc2xa8 orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.",
            "cx": 2121.6,
            "cy": -655.051,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D10-1040",
            "name": "A Game-Theoretic Approach to Generating Spatial Descriptions",
            "publication_data": 2010,
            "citation": 94,
            "abstract": "Language is sensitive to both semantic and pragmatic effects. To capture both effects, we model language use as a cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action. Specifically, we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker. We show that a speaker model that acts optimally with respect to an explicit, embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions.",
            "cx": 2907.6,
            "cy": -924.271,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W14-1607",
            "name": "Grounding Language with Points and Paths in Continuous Spaces",
            "publication_data": 2014,
            "citation": 10,
            "abstract": "We present a model for generating pathvalued interpretations of natural language text. Our model encodes a map from natural language descriptions to paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010).",
            "cx": 2907.6,
            "cy": -565.311,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 53,
            "name": 53,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1049",
            "name": "Jointly Learning to Extract and Compress",
            "publication_data": 2011,
            "citation": 165,
            "abstract": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.",
            "cx": 3055.6,
            "cy": -834.531,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P16-1188",
            "name": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
            "publication_data": 2016,
            "citation": 28,
            "abstract": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.",
            "cx": 3078.6,
            "cy": -385.831,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 126,
            "name": 126,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 191,
            "name": 191,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 64,
            "name": 64,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D15-1138",
            "name": "Alignment-Based Compositional Semantics for Instruction Following",
            "publication_data": 2015,
            "citation": 17,
            "abstract": "This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the modelxe2x80x99s flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",
            "cx": 2907.6,
            "cy": -475.571,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "Q14-1037",
            "name": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking",
            "publication_data": 2014,
            "citation": 122,
            "abstract": "We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",
            "cx": 3137.6,
            "cy": -565.311,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N18-1091",
            "name": "What{'}s Going On in Neural Constituency Parsers? An Analysis",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.",
            "cx": 922.597,
            "cy": -206.35,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 28,
            "name": 28,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P15-1030",
            "name": "Neural {CRF} Parsing",
            "publication_data": 2015,
            "citation": 33,
            "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.",
            "cx": 3239.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-1249",
            "name": "Constituency Parsing with a Self-Attentive Encoder",
            "publication_data": 2018,
            "citation": 22,
            "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
            "cx": 3239.6,
            "cy": -206.35,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D15-1032",
            "name": "An Empirical Analysis of Optimization for Max-Margin {NLP}",
            "publication_data": 2015,
            "citation": 14,
            "abstract": "Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.",
            "cx": 3406.6,
            "cy": -475.571,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "Q17-1031",
            "name": "Parsing with Traces: An {O}(n4) Algorithm and a Structural Representation",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3{\\%} of the Penn English Treebank. We also implement a proof-of-concept parser that recovers a range of null elements and trace types.",
            "cx": 3406.6,
            "cy": -296.09,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 110,
            "name": 110,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018",
            "citation_count": 25,
            "name": 25,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2025",
            "name": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.",
            "cx": 3611.6,
            "cy": -296.09,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1340",
            "name": "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
            "publication_data": 2019,
            "citation": 9,
            "abstract": "We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",
            "cx": 3611.6,
            "cy": -116.61,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1076",
            "name": "A Minimal Span-Based Neural Constituency Parser",
            "publication_data": 2017,
            "citation": 39,
            "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",
            "cx": 3908.6,
            "cy": -296.09,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P19-1031",
            "name": "Cross-Domain Generalization of Neural Constituency Parsers",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing{---}but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",
            "cx": 3838.6,
            "cy": -116.61,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.557",
            "name": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word{'}s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.",
            "cx": 3978.6,
            "cy": -26.8701,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P17-1105",
            "name": "Abstract Syntax Networks for Code Generation and Semantic Parsing",
            "publication_data": 2017,
            "citation": 58,
            "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7{\\%} exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1{\\%}. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.",
            "cx": 4190.6,
            "cy": -296.09,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.acl-main.208",
            "name": "Semantic Scaffolds for Pseudocode-to-Code Generation",
            "publication_data": 2020,
            "citation": 1,
            "abstract": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10{\\%} absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",
            "cx": 4190.6,
            "cy": -26.8701,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 9,
            "name": 9,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020",
            "citation_count": 1,
            "name": 1,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        }
    ],
    [
        {
            "source": "2001",
            "target": "2003",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "W01-1812",
            "target": "N03-1016",
            "d": "M315.597,-1525.13C315.597,-1517.16 315.597,-1508.27 315.597,-1499.78"
        },
        {
            "source": "P01-1044",
            "target": "W01-1812",
            "d": "M224.168,-1552.45C226.489,-1552.45 228.811,-1552.45 231.132,-1552.45"
        },
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "N03-1016",
            "target": "P10-2037",
            "d": "M315.597,-1435.55C315.597,-1400.93 315.597,-1338 315.597,-1284.23 315.597,-1284.23 315.597,-1284.23 315.597,-1102.75 315.597,-1050.68 303.003,-1028.53 334.597,-987.141 348.565,-968.843 369.211,-955.783 390.179,-946.532"
        },
        {
            "source": "2004",
            "target": "2006",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "W04-3201",
            "target": "D08-1091",
            "d": "M955.597,-1345.86C955.597,-1297.97 955.597,-1195.98 955.597,-1141.25"
        },
        {
            "source": "P04-1061",
            "target": "P08-1100",
            "d": "M1214.95,-1345.86C1202.71,-1297.75 1176.57,-1195.03 1162.69,-1140.49"
        },
        {
            "source": "P04-1061",
            "target": "N10-1083",
            "d": "M1234.19,-1346.2C1249.57,-1312.27 1273.6,-1250.36 1273.6,-1194.49 1273.6,-1194.49 1273.6,-1194.49 1273.6,-1102.75 1273.6,-1054.02 1273.6,-997.772 1273.6,-961.886"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "P06-1055",
            "target": "D07-1094",
            "d": "M1403.46,-1256.38C1400.56,-1248.15 1397.29,-1238.9 1394.18,-1230.11"
        },
        {
            "source": "P06-1055",
            "target": "D12-1105",
            "d": "M1441.59,-1258.11C1452.36,-1247.62 1463.63,-1234.49 1470.6,-1220.36 1493.62,-1173.66 1489.6,-1156.82 1489.6,-1104.75 1489.6,-1104.75 1489.6,-1104.75 1489.6,-923.271 1489.6,-874.544 1489.6,-818.292 1489.6,-782.406"
        },
        {
            "source": "N06-1014",
            "target": "P07-1003",
            "d": "M1652.6,-1255.91C1652.6,-1247.94 1652.6,-1239.05 1652.6,-1230.56"
        },
        {
            "source": "N06-1015",
            "target": "P10-1147",
            "d": "M1859.6,-1256.27C1859.6,-1193.71 1859.6,-1034.42 1859.6,-962.049"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "P07-1003",
            "target": "P10-1147",
            "d": "M1672.07,-1167.35C1709.5,-1119.03 1791.5,-1013.18 1833.57,-958.872"
        },
        {
            "source": "P07-1003",
            "target": "N10-1014",
            "d": "M1650.64,-1166.38C1647.06,-1118.49 1639.43,-1016.5 1635.33,-961.771"
        },
        {
            "source": "P07-1107",
            "target": "N10-1061",
            "d": "M2073.6,-1166.38C2073.6,-1118.49 2073.6,-1016.5 2073.6,-961.771"
        },
        {
            "source": "N07-1051",
            "target": "W08-1005",
            "d": "M695.312,-1168.75C680.425,-1158.13 662.721,-1145.5 646.99,-1134.28"
        },
        {
            "source": "N07-1051",
            "target": "N09-1063",
            "d": "M727.13,-1166.6C724.225,-1140.29 715.667,-1100.18 690.597,-1076.88 689.004,-1075.4 596.24,-1052.5 524.638,-1034.99"
        },
        {
            "source": "N07-1051",
            "target": "P14-1020",
            "d": "M733.148,-1166.42C738.769,-1131.91 747.597,-1069.09 747.597,-1015.01 747.597,-1015.01 747.597,-1015.01 747.597,-743.791 747.597,-695.064 747.597,-638.812 747.597,-602.925"
        },
        {
            "source": "N07-1051",
            "target": "P14-1022",
            "d": "M760.464,-1168.37C797.498,-1137.33 853.597,-1079.65 853.597,-1015.01 853.597,-1015.01 853.597,-1015.01 853.597,-743.791 853.597,-690.987 880.459,-634.983 900.662,-600.329"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "P08-1088",
            "target": "D11-1029",
            "d": "M2221.47,-1076.75C2220.52,-1037.07 2221.74,-959.465 2241.6,-897.401 2244.63,-887.914 2249.3,-878.302 2254.27,-869.573"
        },
        {
            "source": "D08-1012",
            "target": "N09-1026",
            "d": "M2424.6,-1076.43C2424.6,-1068.46 2424.6,-1059.57 2424.6,-1051.08"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "P09-1104",
            "target": "N12-1004",
            "d": "M2590.6,-986.903C2590.6,-939.009 2590.6,-837.016 2590.6,-782.291"
        },
        {
            "source": "P09-1108",
            "target": "P10-2037",
            "d": "M597.138,-991.097C574.639,-979.485 546.783,-965.108 522.848,-952.755"
        },
        {
            "source": "N09-1063",
            "target": "P09-1108",
            "d": "M541.867,-1014.01C544.337,-1014.01 546.806,-1014.01 549.276,-1014.01"
        },
        {
            "source": "D09-1120",
            "target": "P12-1041",
            "d": "M2778.6,-986.903C2778.6,-939.009 2778.6,-837.016 2778.6,-782.291"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "P10-1105",
            "target": "D11-1029",
            "d": "M2316.69,-897.893C2311.21,-889.148 2304.97,-879.204 2299.12,-869.865"
        },
        {
            "source": "N10-1061",
            "target": "W11-1916",
            "d": "M2051.62,-897.893C2043.66,-888.708 2034.56,-878.199 2026.12,-868.459"
        },
        {
            "source": "N10-1061",
            "target": "D13-1203",
            "d": "M2088.1,-897.542C2093.55,-886.708 2099.24,-873.765 2102.6,-861.401 2118.27,-803.713 2121.58,-733.923 2121.99,-692.356"
        },
        {
            "source": "D10-1040",
            "target": "W14-1607",
            "d": "M2907.6,-897.312C2907.6,-834.745 2907.6,-675.458 2907.6,-603.088"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "P11-1049",
            "target": "P16-1188",
            "d": "M3055.6,-807.369C3055.6,-772.749 3055.6,-709.819 3055.6,-656.051 3055.6,-656.051 3055.6,-656.051 3055.6,-564.311 3055.6,-514.964 3064.45,-458.536 3071.17,-422.805"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "W14-1607",
            "target": "D15-1138",
            "d": "M2907.6,-537.986C2907.6,-530.019 2907.6,-521.128 2907.6,-512.64"
        },
        {
            "source": "Q14-1037",
            "target": "P16-1188",
            "d": "M3129.02,-538.504C3118.84,-507.878 3101.81,-456.648 3090.39,-422.308"
        },
        {
            "source": "P14-1022",
            "target": "N18-1091",
            "d": "M922.597,-538.351C922.597,-475.785 922.597,-316.497 922.597,-244.128"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "P15-1030",
            "target": "P18-1249",
            "d": "M3239.6,-448.462C3239.6,-400.568 3239.6,-298.576 3239.6,-243.85"
        },
        {
            "source": "D15-1032",
            "target": "Q17-1031",
            "d": "M3406.6,-448.383C3406.6,-417.886 3406.6,-367.334 3406.6,-333.142"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "P17-2025",
            "target": "P19-1340",
            "d": "M3611.6,-268.903C3611.6,-238.405 3611.6,-187.854 3611.6,-153.662"
        },
        {
            "source": "P17-1076",
            "target": "P19-1031",
            "d": "M3898.42,-269.283C3886.34,-238.657 3866.13,-187.428 3852.59,-153.088"
        },
        {
            "source": "P17-1076",
            "target": "2020.acl-main.557",
            "d": "M3918.64,-269.446C3930,-239.596 3948.37,-188.621 3959.6,-143.48 3966.15,-117.119 3971.04,-86.7994 3974.21,-63.8254"
        },
        {
            "source": "P17-1105",
            "target": "2020.acl-main.208",
            "d": "M4190.6,-268.982C4190.6,-221.088 4190.6,-119.096 4190.6,-64.3697"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        }
    ],
    [
        {
            "id": "2001",
            "name": "2001",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "W01-1812",
            "name": "dan2001Parsing",
            "x": "315.597",
            "y": "-1556.25"
        },
        {
            "id": "W01-1812",
            "name": "91",
            "x": "315.597",
            "y": "-1541.25"
        },
        {
            "id": "N03-1016",
            "name": "dan2003{A}*",
            "x": "315.597",
            "y": "-1466.51"
        },
        {
            "id": "N03-1016",
            "name": "192",
            "x": "315.597",
            "y": "-1451.51"
        },
        {
            "id": "P01-1044",
            "name": "dan2001Parsing",
            "x": "149.597",
            "y": "-1556.25"
        },
        {
            "id": "P01-1044",
            "name": "45",
            "x": "149.597",
            "y": "-1541.25"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "P10-2037",
            "name": "adam2010Top-Down",
            "x": "469.597",
            "y": "-928.071"
        },
        {
            "id": "P10-2037",
            "name": "9",
            "x": "469.597",
            "y": "-913.071"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "W04-3201",
            "name": "ben2004Max-Margin",
            "x": "955.597",
            "y": "-1376.77"
        },
        {
            "id": "W04-3201",
            "name": "198",
            "x": "955.597",
            "y": "-1361.77"
        },
        {
            "id": "D08-1091",
            "name": "slav2008Sparse",
            "x": "955.597",
            "y": "-1107.55"
        },
        {
            "id": "D08-1091",
            "name": "35",
            "x": "955.597",
            "y": "-1092.55"
        },
        {
            "id": "P04-1061",
            "name": "dan2004Corpus-Based",
            "x": "1221.6",
            "y": "-1376.77"
        },
        {
            "id": "P04-1061",
            "name": "360",
            "x": "1221.6",
            "y": "-1361.77"
        },
        {
            "id": "P08-1100",
            "name": "percy2008Analyzing",
            "x": "1153.6",
            "y": "-1107.55"
        },
        {
            "id": "P08-1100",
            "name": "21",
            "x": "1153.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1083",
            "name": "taylor2010Painless",
            "x": "1273.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1083",
            "name": "168",
            "x": "1273.6",
            "y": "-913.071"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "P06-1055",
            "name": "slav2006Learning",
            "x": "1412.6",
            "y": "-1287.03"
        },
        {
            "id": "P06-1055",
            "name": "727",
            "x": "1412.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1094",
            "name": "slav2007Learning",
            "x": "1381.6",
            "y": "-1197.29"
        },
        {
            "id": "D07-1094",
            "name": "13",
            "x": "1381.6",
            "y": "-1182.29"
        },
        {
            "id": "D12-1105",
            "name": "david2012Training",
            "x": "1489.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1105",
            "name": "8",
            "x": "1489.6",
            "y": "-733.591"
        },
        {
            "id": "N06-1014",
            "name": "percy2006Alignment",
            "x": "1652.6",
            "y": "-1287.03"
        },
        {
            "id": "N06-1014",
            "name": "392",
            "x": "1652.6",
            "y": "-1272.03"
        },
        {
            "id": "P07-1003",
            "name": "john2007Tailoring",
            "x": "1652.6",
            "y": "-1197.29"
        },
        {
            "id": "P07-1003",
            "name": "100",
            "x": "1652.6",
            "y": "-1182.29"
        },
        {
            "id": "N06-1015",
            "name": "simon2006Word",
            "x": "1859.6",
            "y": "-1287.03"
        },
        {
            "id": "N06-1015",
            "name": "75",
            "x": "1859.6",
            "y": "-1272.03"
        },
        {
            "id": "P10-1147",
            "name": "john2010Discriminative",
            "x": "1859.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1147",
            "name": "25",
            "x": "1859.6",
            "y": "-913.071"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "N10-1014",
            "name": "adam2010Unsupervised",
            "x": "1632.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1014",
            "name": "18",
            "x": "1632.6",
            "y": "-913.071"
        },
        {
            "id": "P07-1107",
            "name": "aria2007Unsupervised",
            "x": "2073.6",
            "y": "-1197.29"
        },
        {
            "id": "P07-1107",
            "name": "127",
            "x": "2073.6",
            "y": "-1182.29"
        },
        {
            "id": "N10-1061",
            "name": "aria2010Coreference",
            "x": "2073.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1061",
            "name": "119",
            "x": "2073.6",
            "y": "-913.071"
        },
        {
            "id": "N07-1051",
            "name": "slav2007Improved",
            "x": "728.597",
            "y": "-1197.29"
        },
        {
            "id": "N07-1051",
            "name": "533",
            "x": "728.597",
            "y": "-1182.29"
        },
        {
            "id": "W08-1005",
            "name": "slav2008Parsing",
            "x": "605.597",
            "y": "-1107.55"
        },
        {
            "id": "W08-1005",
            "name": "23",
            "x": "605.597",
            "y": "-1092.55"
        },
        {
            "id": "N09-1063",
            "name": "adam2009Hierarchical",
            "x": "442.597",
            "y": "-1017.81"
        },
        {
            "id": "N09-1063",
            "name": "17",
            "x": "442.597",
            "y": "-1002.81"
        },
        {
            "id": "P14-1020",
            "name": "david2014Sparser,",
            "x": "747.597",
            "y": "-569.111"
        },
        {
            "id": "P14-1020",
            "name": "16",
            "x": "747.597",
            "y": "-554.111"
        },
        {
            "id": "P14-1022",
            "name": "david2014Less",
            "x": "922.597",
            "y": "-569.111"
        },
        {
            "id": "P14-1022",
            "name": "43",
            "x": "922.597",
            "y": "-554.111"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "P08-1088",
            "name": "aria2008Learning",
            "x": "2222.6",
            "y": "-1107.55"
        },
        {
            "id": "P08-1088",
            "name": "259",
            "x": "2222.6",
            "y": "-1092.55"
        },
        {
            "id": "D11-1029",
            "name": "taylor2011Simple",
            "x": "2277.6",
            "y": "-838.331"
        },
        {
            "id": "D11-1029",
            "name": "11",
            "x": "2277.6",
            "y": "-823.331"
        },
        {
            "id": "D08-1012",
            "name": "slav2008Coarse-to-Fine",
            "x": "2424.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1012",
            "name": "36",
            "x": "2424.6",
            "y": "-1092.55"
        },
        {
            "id": "N09-1026",
            "name": "john2009Efficient",
            "x": "2424.6",
            "y": "-1017.81"
        },
        {
            "id": "N09-1026",
            "name": "24",
            "x": "2424.6",
            "y": "-1002.81"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "P09-1104",
            "name": "aria2009Better",
            "x": "2590.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1104",
            "name": "91",
            "x": "2590.6",
            "y": "-1002.81"
        },
        {
            "id": "N12-1004",
            "name": "david2012Fast",
            "x": "2590.6",
            "y": "-748.591"
        },
        {
            "id": "N12-1004",
            "name": "9",
            "x": "2590.6",
            "y": "-733.591"
        },
        {
            "id": "P09-1108",
            "name": "adam2009K-Best",
            "x": "639.597",
            "y": "-1017.81"
        },
        {
            "id": "P09-1108",
            "name": "35",
            "x": "639.597",
            "y": "-1002.81"
        },
        {
            "id": "D09-1120",
            "name": "aria2009Simple",
            "x": "2778.6",
            "y": "-1017.81"
        },
        {
            "id": "D09-1120",
            "name": "158",
            "x": "2778.6",
            "y": "-1002.81"
        },
        {
            "id": "P12-1041",
            "name": "mohit2012Coreference",
            "x": "2778.6",
            "y": "-748.591"
        },
        {
            "id": "P12-1041",
            "name": "36",
            "x": "2778.6",
            "y": "-733.591"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "P10-1105",
            "name": "david2010Finding",
            "x": "2332.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1105",
            "name": "23",
            "x": "2332.6",
            "y": "-913.071"
        },
        {
            "id": "W11-1916",
            "name": "jonathan2011Mention",
            "x": "1997.6",
            "y": "-838.331"
        },
        {
            "id": "W11-1916",
            "name": "10",
            "x": "1997.6",
            "y": "-823.331"
        },
        {
            "id": "D13-1203",
            "name": "greg2013Easy",
            "x": "2121.6",
            "y": "-658.851"
        },
        {
            "id": "D13-1203",
            "name": "126",
            "x": "2121.6",
            "y": "-643.851"
        },
        {
            "id": "D10-1040",
            "name": "dave2010A",
            "x": "2907.6",
            "y": "-928.071"
        },
        {
            "id": "D10-1040",
            "name": "94",
            "x": "2907.6",
            "y": "-913.071"
        },
        {
            "id": "W14-1607",
            "name": "jacob2014Grounding",
            "x": "2907.6",
            "y": "-569.111"
        },
        {
            "id": "W14-1607",
            "name": "10",
            "x": "2907.6",
            "y": "-554.111"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "P11-1049",
            "name": "taylor2011Jointly",
            "x": "3055.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1049",
            "name": "165",
            "x": "3055.6",
            "y": "-823.331"
        },
        {
            "id": "P16-1188",
            "name": "greg2016Learning-Based",
            "x": "3078.6",
            "y": "-389.631"
        },
        {
            "id": "P16-1188",
            "name": "28",
            "x": "3078.6",
            "y": "-374.631"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "D15-1138",
            "name": "jacob2015Alignment-Based",
            "x": "2907.6",
            "y": "-479.371"
        },
        {
            "id": "D15-1138",
            "name": "17",
            "x": "2907.6",
            "y": "-464.371"
        },
        {
            "id": "Q14-1037",
            "name": "greg2014A",
            "x": "3137.6",
            "y": "-569.111"
        },
        {
            "id": "Q14-1037",
            "name": "122",
            "x": "3137.6",
            "y": "-554.111"
        },
        {
            "id": "N18-1091",
            "name": "david2018What{'}s",
            "x": "922.597",
            "y": "-210.15"
        },
        {
            "id": "N18-1091",
            "name": "3",
            "x": "922.597",
            "y": "-195.15"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "P15-1030",
            "name": "greg2015Neural",
            "x": "3239.6",
            "y": "-479.371"
        },
        {
            "id": "P15-1030",
            "name": "33",
            "x": "3239.6",
            "y": "-464.371"
        },
        {
            "id": "P18-1249",
            "name": "nikita2018Constituency",
            "x": "3239.6",
            "y": "-210.15"
        },
        {
            "id": "P18-1249",
            "name": "22",
            "x": "3239.6",
            "y": "-195.15"
        },
        {
            "id": "D15-1032",
            "name": "jonathan2015An",
            "x": "3406.6",
            "y": "-479.371"
        },
        {
            "id": "D15-1032",
            "name": "14",
            "x": "3406.6",
            "y": "-464.371"
        },
        {
            "id": "Q17-1031",
            "name": "jonathan2017Parsing",
            "x": "3406.6",
            "y": "-299.89"
        },
        {
            "id": "Q17-1031",
            "name": "4",
            "x": "3406.6",
            "y": "-284.89"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "P17-2025",
            "name": "daniel2017Improving",
            "x": "3611.6",
            "y": "-299.89"
        },
        {
            "id": "P17-2025",
            "name": "9",
            "x": "3611.6",
            "y": "-284.89"
        },
        {
            "id": "P19-1340",
            "name": "nikita2019Multilingual",
            "x": "3611.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1340",
            "name": "9",
            "x": "3611.6",
            "y": "-105.41"
        },
        {
            "id": "P17-1076",
            "name": "mitchell2017A",
            "x": "3908.6",
            "y": "-299.89"
        },
        {
            "id": "P17-1076",
            "name": "39",
            "x": "3908.6",
            "y": "-284.89"
        },
        {
            "id": "P19-1031",
            "name": "daniel2019Cross-Domain",
            "x": "3838.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1031",
            "name": "0",
            "x": "3838.6",
            "y": "-105.41"
        },
        {
            "id": "2020.acl-main.557",
            "name": "nikita2020Tetra-Tagging:",
            "x": "3978.6",
            "y": "-30.6701"
        },
        {
            "id": "2020.acl-main.557",
            "name": "???",
            "x": "3978.6",
            "y": "-15.6701"
        },
        {
            "id": "P17-1105",
            "name": "maxim2017Abstract",
            "x": "4190.6",
            "y": "-299.89"
        },
        {
            "id": "P17-1105",
            "name": "58",
            "x": "4190.6",
            "y": "-284.89"
        },
        {
            "id": "2020.acl-main.208",
            "name": "ruiqi2020Semantic",
            "x": "4190.6",
            "y": "-30.6701"
        },
        {
            "id": "2020.acl-main.208",
            "name": "1",
            "x": "4190.6",
            "y": "-15.6701"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-23.1701"
        }
    ],
    [
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "319.098,-1499.72 315.597,-1489.72 312.098,-1499.72 319.098,-1499.72",
        "231.311,-1555.95 241.311,-1552.45 231.311,-1548.95 231.311,-1555.95",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "391.592,-949.736 399.488,-942.671 388.91,-943.27 391.592,-949.736",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "959.098,-1140.97 955.597,-1130.97 952.098,-1140.97 959.098,-1140.97",
        "1166.03,-1139.4 1160.17,-1130.57 1159.24,-1141.13 1166.03,-1139.4",
        "1277.1,-961.434 1273.6,-951.434 1270.1,-961.434 1277.1,-961.434",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "1397.4,-1228.72 1390.77,-1220.46 1390.8,-1231.06 1397.4,-1228.72",
        "1493.1,-781.953 1489.6,-771.953 1486.1,-781.953 1493.1,-781.953",
        "1656.1,-1230.5 1652.6,-1220.5 1649.1,-1230.5 1656.1,-1230.5",
        "1863.1,-961.616 1859.6,-951.616 1856.1,-961.616 1863.1,-961.616",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "1836.36,-960.979 1839.72,-950.93 1830.83,-956.692 1836.36,-960.979",
        "1638.8,-961.197 1634.56,-951.486 1631.82,-961.719 1638.8,-961.197",
        "2077.1,-961.486 2073.6,-951.486 2070.1,-961.486 2077.1,-961.486",
        "648.657,-1131.17 638.483,-1128.21 644.592,-1136.87 648.657,-1131.17",
        "525.232,-1031.53 514.687,-1032.56 523.57,-1038.33 525.232,-1031.53",
        "751.098,-602.473 747.597,-592.473 744.098,-602.473 751.098,-602.473",
        "903.772,-601.948 905.894,-591.568 897.762,-598.359 903.772,-601.948",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "2257.37,-871.215 2259.51,-860.84 2251.37,-867.613 2257.37,-871.215",
        "2428.1,-1051.02 2424.6,-1041.02 2421.1,-1051.02 2428.1,-1051.02",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "2594.1,-782.006 2590.6,-772.006 2587.1,-782.006 2594.1,-782.006",
        "524.378,-949.606 513.887,-948.13 521.168,-955.826 524.378,-949.606",
        "549.428,-1017.51 559.428,-1014.01 549.428,-1010.51 549.428,-1017.51",
        "2782.1,-782.006 2778.6,-772.006 2775.1,-782.006 2782.1,-782.006",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "2301.94,-867.77 2293.66,-861.156 2296.01,-871.488 2301.94,-867.77",
        "2028.72,-866.119 2019.53,-860.852 2023.43,-870.702 2028.72,-866.119",
        "2125.49,-692.185 2122.04,-682.169 2118.49,-692.153 2125.49,-692.185",
        "2911.1,-602.655 2907.6,-592.655 2904.1,-602.655 2911.1,-602.655",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "3074.63,-423.338 3073.09,-412.856 3067.76,-422.013 3074.63,-423.338",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "2911.1,-512.575 2907.6,-502.575 2904.1,-512.575 2911.1,-512.575",
        "3093.66,-421.054 3087.19,-412.669 3087.02,-423.262 3093.66,-421.054",
        "926.098,-243.695 922.597,-233.695 919.098,-243.695 926.098,-243.695",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "3243.1,-243.565 3239.6,-233.565 3236.1,-243.565 3243.1,-243.565",
        "3410.1,-333.097 3406.6,-323.097 3403.1,-333.097 3410.1,-333.097",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "3615.1,-153.616 3611.6,-143.616 3608.1,-153.616 3615.1,-153.616",
        "3855.71,-151.467 3848.79,-143.448 3849.2,-154.035 3855.71,-151.467",
        "3977.69,-64.2032 3975.55,-53.8272 3970.76,-63.2745 3977.69,-64.2032",
        "4194.1,-64.0848 4190.6,-54.0848 4187.1,-64.0848 4194.1,-64.0848",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701"
    ]
]