[
    [
        {
            "id": "2001",
            "citation_count": 172,
            "name": 172,
            "cx": 28.5975,
            "cy": -1821.67,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2002",
            "citation_count": 257,
            "name": 257,
            "cx": 28.5975,
            "cy": -1731.93,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-1812",
            "name": "Parsing and Hypergraphs",
            "publication_data": 2001,
            "citation": 91,
            "abstract": "None",
            "cx": 6967.6,
            "cy": -1821.67,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1016",
            "name": "{A}* Parsing: Fast Exact {V}iterbi Parse Selection",
            "publication_data": 2003,
            "citation": 192,
            "abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of pre-computation, reduces the work to less than 5%. Un-like best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.",
            "cx": 6846.6,
            "cy": -1642.19,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1108",
            "name": "K-Best {A}* Parsing",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "A* parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing k-best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types.",
            "cx": 6821.6,
            "cy": -1103.75,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2037",
            "name": "Top-Down K-Best {A}* Parsing",
            "publication_data": 2010,
            "citation": 9,
            "abstract": "We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA* is a variant of the k-best A* (KA*) algorithm of Pauls and Klein (2009). In contrast to KA*, which performs an inside and outside pass before performing k-best extraction bottom up, TKA* performs only the inside pass before extracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement.",
            "cx": 6894.6,
            "cy": -1014.01,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-0714",
            "name": "Distributional phrase structure induction",
            "publication_data": 2001,
            "citation": 36,
            "abstract": "Unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data. Linguistic justifications of constituency, on the other hand, rely on notions such as substitutability and varying external contexts. We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features. The advantages and disadvantages of these systems are examined, including precision/recall trade-offs, error analysis, and extensibility.",
            "cx": 443.597,
            "cy": -1821.67,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P02-1017",
            "name": "A Generative Constituent-Context Model for Improved Grammar Induction",
            "publication_data": 2002,
            "citation": 167,
            "abstract": "We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.",
            "cx": 443.597,
            "cy": -1731.93,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P01-1044",
            "name": "Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the {P}enn {T}reebank",
            "publication_data": 2001,
            "citation": 45,
            "abstract": "This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.",
            "cx": 6801.6,
            "cy": -1821.67,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P03-1054",
            "name": "Accurate Unlexicalized Parsing",
            "publication_data": 2003,
            "citation": 2541,
            "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.",
            "cx": 4888.6,
            "cy": -1642.19,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2003",
            "citation_count": 4947,
            "name": 4947,
            "cx": 28.5975,
            "cy": -1642.19,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W02-1002",
            "name": "Conditional Structure versus Conditional Estimation in {NLP} Models",
            "publication_data": 2002,
            "citation": 90,
            "abstract": "This paper separates conditional parameter estimation, which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the independence assumptions of the conditional model structure are unsuited to linguistic sequences. The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work.",
            "cx": 1547.6,
            "cy": -1731.93,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1033",
            "name": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network",
            "publication_data": 2003,
            "citation": 2214,
            "abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.",
            "cx": 1547.6,
            "cy": -1642.19,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "black",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P04-1061",
            "name": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
            "publication_data": 2004,
            "citation": 360,
            "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
            "cx": 443.597,
            "cy": -1552.45,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P06-1111",
            "name": "Prototype-Driven Grammar Induction",
            "publication_data": 2006,
            "citation": 39,
            "abstract": "We investigate prototype-driven learning for primarily unsupervised grammar induction. Prior knowledge is specified declaratively, by providing a few canonical examples of each target phrase type. This sparse prototype information is then propagated across a corpus using distributional similarity features, which augment an otherwise standard PCFG model. We show that distributional features are effective at distinguishing bracket labels, but not determining bracket locations. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Using only a handful of prototypes, we show substantial improvements over naive PCFG induction for English and Chinese grammar induction.",
            "cx": 264.597,
            "cy": -1372.97,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "Unsupervised Parsing via Constituency Tests",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.",
            "cx": 2643.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2004",
            "citation_count": 558,
            "name": 558,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W04-3201",
            "name": "Max-Margin Parsing",
            "publication_data": 2004,
            "citation": 198,
            "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the modelxe2x80x99s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.",
            "cx": 3307.6,
            "cy": -1552.45,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W05-0104",
            "name": "A Core-Tools Statistical {NLP} Course",
            "publication_data": 2005,
            "citation": 1,
            "abstract": "In the fall term of 2004, I taught a new statistical NLP course focusing on core tools and machine-learning algorithms. The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system. Using provided scaffolding, students built realistic tools with nearly state-of-the-art performance in most cases. This paper briefly outlines the coverage of the course, the scope of the assignments, and some of the lessons learned in teaching the course in this way.",
            "cx": 5329.6,
            "cy": -1462.71,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W06-2903",
            "name": "Non-Local Modeling with a Mixture of {PCFG}s",
            "publication_data": 2006,
            "citation": 0,
            "abstract": "While most work on parsing with PCFGs has focused on local correlations between tree configurations, we attempt to model non-local correlations using a finite mixture of PCFGs. A mixture grammar fit with the EM algorithm shows improvement over a single PCFG, both in parsing accuracy and in test data likelihood. We argue that this improvement comes from the learning of specialized grammars that capture non-local correlations.",
            "cx": 5409.6,
            "cy": -1372.97,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P06-1055",
            "name": "Learning Accurate, Compact, and Interpretable Tree Annotation",
            "publication_data": 2006,
            "citation": 727,
            "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
            "cx": 5185.6,
            "cy": -1372.97,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N07-1051",
            "name": "Improved Inference for Unlexicalized Parsing",
            "publication_data": 2007,
            "citation": 533,
            "abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammarxe2x80x99s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.",
            "cx": 4241.6,
            "cy": -1283.23,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D07-1072",
            "name": "The Infinite {PCFG} Using Hierarchical {D}irichlet Processes",
            "publication_data": 2007,
            "citation": 146,
            "abstract": "We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available. In addition to presenting a fully Bayesian model for the PCFG, we also develop an efficient variational inference procedure. On synthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars.",
            "cx": 4839.6,
            "cy": -1283.23,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W08-1005",
            "name": "Parsing {G}erman with Latent Variable Grammars",
            "publication_data": 2008,
            "citation": 23,
            "abstract": "We describe experiments on learning latent variable grammars for various German tree-banks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks.",
            "cx": 5005.6,
            "cy": -1193.49,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1091",
            "name": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing",
            "publication_data": 2008,
            "citation": 35,
            "abstract": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.",
            "cx": 4533.6,
            "cy": -1193.49,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1026",
            "name": "Efficient Parsing for Transducer Grammars",
            "publication_data": 2009,
            "citation": 24,
            "abstract": "The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized. We address the problem of parsing efficiently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU.",
            "cx": 2264.6,
            "cy": -1103.75,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1063",
            "name": "Hierarchical Search for Parsing",
            "publication_data": 2009,
            "citation": 17,
            "abstract": "Both coarse-to-fine and A* parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarse-to-fine case because of accumulated slack in A* heuristics.",
            "cx": 6060.6,
            "cy": -1103.75,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D09-1120",
            "name": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
            "publication_data": 2009,
            "citation": 158,
            "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).",
            "cx": 6248.6,
            "cy": -1103.75,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P10-1112",
            "name": "Simple, Accurate Parsing with an All-Fragments Grammar",
            "publication_data": 2010,
            "citation": 26,
            "abstract": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.",
            "cx": 4707.6,
            "cy": -1014.01,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-2127",
            "name": "The Surprising Variance in Shortest-Derivation Parsing",
            "publication_data": 2011,
            "citation": 2,
            "abstract": "We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.",
            "cx": 4775.6,
            "cy": -924.271,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P12-2021",
            "name": "Robust Conversion of {CCG} Derivations to Phrase Structure Trees",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.",
            "cx": 3351.6,
            "cy": -834.531,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P12-1101",
            "name": "Large-Scale Syntactic Language Modeling with Treelets",
            "publication_data": 2012,
            "citation": 27,
            "abstract": "We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.",
            "cx": 5426.6,
            "cy": -834.531,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1091",
            "name": "An Empirical Investigation of Statistical Significance in {NLP}",
            "publication_data": 2012,
            "citation": 45,
            "abstract": "We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.",
            "cx": 2065.6,
            "cy": -834.531,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1096",
            "name": "Parser Showdown at the {W}all {S}treet Corral: An Empirical Investigation of Error Types in Parser Output",
            "publication_data": 2012,
            "citation": 47,
            "abstract": "Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.",
            "cx": 5631.6,
            "cy": -834.531,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1105",
            "name": "Training Factored {PCFG}s with Expectation Propagation",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguistically-motivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.",
            "cx": 3854.6,
            "cy": -834.531,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2018",
            "name": "An Empirical Examination of Challenges in {C}hinese Parsing",
            "publication_data": 2013,
            "citation": 11,
            "abstract": "Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.",
            "cx": 5005.6,
            "cy": -744.791,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1195",
            "name": "A Multi-Teraflop Constituency Parser using {GPU}s",
            "publication_data": 2013,
            "citation": 17,
            "abstract": "Constituency parsing with rich grammars remains a computational challenge. Graphics Processing Units (GPUs) have previously been used to accelerate CKY chart evaluation, but gains over CPU parsers were modest. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPUxe2x80x99s practical maximum speed (a Teraflop), or around a half-trillion rule evaluations per second. Net parser performance on a 4-GPU system is over 1 thousand length30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.",
            "cx": 6550.6,
            "cy": -744.791,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1022",
            "name": "Less Grammar, More Features",
            "publication_data": 2014,
            "citation": 43,
            "abstract": "We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bjorkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.",
            "cx": 3854.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P15-1030",
            "name": "Neural {CRF} Parsing",
            "publication_data": 2015,
            "citation": 33,
            "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.",
            "cx": 4182.6,
            "cy": -565.311,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P17-1076",
            "name": "A Minimal Span-Based Neural Constituency Parser",
            "publication_data": 2017,
            "citation": 39,
            "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",
            "cx": 2728.6,
            "cy": -385.831,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N18-1091",
            "name": "What{'}s Going On in Neural Constituency Parsers? An Analysis",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.",
            "cx": 2553.6,
            "cy": -296.09,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D08-1012",
            "name": "Coarse-to-Fine Syntactic Machine Translation using Language Projections",
            "publication_data": 2008,
            "citation": 36,
            "abstract": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incrementally introduced. In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.",
            "cx": 3842.6,
            "cy": -1193.49,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2064",
            "name": "Hierarchical {A}* Parsing with Bridge Outside Scores",
            "publication_data": 2010,
            "citation": 6,
            "abstract": "Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.",
            "cx": 6065.6,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1147",
            "name": "Discriminative Modeling of Extraction Sets for Machine Translation",
            "publication_data": 2010,
            "citation": 25,
            "abstract": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.",
            "cx": 3003.6,
            "cy": -1014.01,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-2075",
            "name": "Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser{'}s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",
            "cx": 2853.6,
            "cy": -296.09,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2005",
            "citation_count": 247,
            "name": 247,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1049",
            "name": "Jointly Learning to Extract and Compress",
            "publication_data": 2011,
            "citation": 165,
            "abstract": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.",
            "cx": 5009.6,
            "cy": -924.271,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D15-1032",
            "name": "An Empirical Analysis of Optimization for Max-Margin {NLP}",
            "publication_data": 2015,
            "citation": 14,
            "abstract": "Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.",
            "cx": 5043.6,
            "cy": -565.311,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1014",
            "name": "Alignment by Agreement",
            "publication_data": 2006,
            "citation": 392,
            "abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.",
            "cx": 1806.6,
            "cy": -1372.97,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P08-1100",
            "name": "Analyzing the Errors of Unsupervised Learning",
            "publication_data": 2008,
            "citation": 21,
            "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
            "cx": 361.597,
            "cy": -1193.49,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1131",
            "name": "Phylogenetic Grammar Induction",
            "publication_data": 2010,
            "citation": 50,
            "abstract": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%.",
            "cx": 626.597,
            "cy": -1014.01,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1083",
            "name": "Painless Unsupervised Learning with Features",
            "publication_data": 2010,
            "citation": 168,
            "abstract": "We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.",
            "cx": 420.597,
            "cy": -1014.01,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D12-1001",
            "name": "Syntactic Transfer Using a Bilingual Lexicon",
            "publication_data": 2012,
            "citation": 43,
            "abstract": "We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different low-resource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).",
            "cx": 383.597,
            "cy": -834.531,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 1715,
            "name": 1715,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P05-1046",
            "name": "Unsupervised Learning of Field Segmentation Models for Information Extraction",
            "publication_data": 2005,
            "citation": 72,
            "abstract": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.",
            "cx": 1299.6,
            "cy": -1462.71,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N06-1041",
            "name": "Prototype-Driven Learning for Sequence Models",
            "publication_data": 2006,
            "citation": 167,
            "abstract": "We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system's error trends.",
            "cx": 2030.6,
            "cy": -1372.97,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1011",
            "name": "Learning Semantic Correspondences with Less Supervision",
            "publication_data": 2009,
            "citation": 190,
            "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.",
            "cx": 1356.6,
            "cy": -1103.75,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "H05-1010",
            "name": "A Discriminative Matching Approach to Word Alignment",
            "publication_data": 2005,
            "citation": 174,
            "abstract": "We present a discriminative, large-margin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.",
            "cx": 2391.6,
            "cy": -1462.71,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N06-1015",
            "name": "Word Alignment via Quadratic Assignment",
            "publication_data": 2006,
            "citation": 75,
            "abstract": "Recently, discriminative word alignment methods have achieved state-of-the-art accuracies by extending the range of information sources that can be easily incorporated into aligners. The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on. However, the proposed bipartite matching model of Taskar et al. (2005), despite being tractable and effective, has two important limitations. First, it is limited by the restriction that words have fertility of at most one. More importantly, first order correlations between consecutive words cannot be directly captured by the model. In this work, we address these limitations by enriching the model form. We give estimation and inference algorithms for these enhancements. Our best model achieves a relative AER reduction of 25% over the basic matching formulation, outperforming intersected IBM Model 4 without using any overly compute-intensive features. By including predictions of other models as features, we achieve AER of 3.8 on the standard Hansards dataset.",
            "cx": 2497.6,
            "cy": -1372.97,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P07-1003",
            "name": "Tailoring Word Alignments to Syntactic Machine Translation",
            "publication_data": 2007,
            "citation": 100,
            "abstract": "Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our modelxe2x80x99s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",
            "cx": 1962.6,
            "cy": -1283.23,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P09-1104",
            "name": "Better Word Alignments with Supervised {ITG} Models",
            "publication_data": 2009,
            "citation": 91,
            "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA alignments.",
            "cx": 2430.6,
            "cy": -1103.75,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1015",
            "name": "Joint Parsing and Alignment with Weakly Synchronized Grammars",
            "publication_data": 2010,
            "citation": 56,
            "abstract": "Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",
            "cx": 1993.6,
            "cy": -1014.01,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N12-1004",
            "name": "Fast Inference in Phrase Extraction Models with Belief Propagation",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up.",
            "cx": 3171.6,
            "cy": -834.531,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2007",
            "citation_count": 961,
            "name": 961,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W06-3105",
            "name": "Why Generative Phrase Models Underperform Surface Heuristics",
            "publication_data": 2006,
            "citation": 71,
            "abstract": "We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased deter-minization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score.",
            "cx": 3312.6,
            "cy": -1372.97,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P08-2007",
            "name": "The Complexity of Phrase Alignment Problems",
            "publication_data": 2008,
            "citation": 63,
            "abstract": "Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.",
            "cx": 3090.6,
            "cy": -1193.49,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1033",
            "name": "Sampling Alignment Structure under a {B}ayesian Translation Model",
            "publication_data": 2008,
            "citation": 86,
            "abstract": "We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase-based translation systems.",
            "cx": 3331.6,
            "cy": -1193.49,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1014",
            "name": "Unsupervised Syntactic Alignment with Inversion Transduction Grammars",
            "publication_data": 2010,
            "citation": 18,
            "abstract": "Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.",
            "cx": 2226.6,
            "cy": -1014.01,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D07-1094",
            "name": "Learning Structured Models for Phone Recognition",
            "publication_data": 2007,
            "citation": 13,
            "abstract": "We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",
            "cx": 6041.6,
            "cy": -1283.23,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W10-2906",
            "name": "Learning Better Monolingual Models with Unannotated Bilingual Text",
            "publication_data": 2010,
            "citation": 33,
            "abstract": "This work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text. We build a multiview learning objective that enforces agreement between monolingual and bilingual models. In our method the first, monolingual view consists of supervised predictors learned separately for each language. The second, bilingual view consists of log-linear predictors learned over both languages on bilingual text. Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model, and we show how to combine the two models to account for dependence between views. For the task of named entity recognition, using bilingual predictors increases F1 by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F1 by 14.6%. For syntactic parsing, our bilingual predictor increases F1 by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%.",
            "cx": 4178.6,
            "cy": -1014.01,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2054",
            "name": "An Entity-Level Approach to Information Extraction",
            "publication_data": 2010,
            "citation": 8,
            "abstract": "We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.",
            "cx": 5777.6,
            "cy": -1014.01,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N10-1061",
            "name": "Coreference Resolution in a Modular, Entity-Centered Model",
            "publication_data": 2010,
            "citation": 119,
            "abstract": "Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.",
            "cx": 5610.6,
            "cy": -1014.01,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N10-1082",
            "name": "Type-Based {MCMC}",
            "publication_data": 2010,
            "citation": 32,
            "abstract": "Most existing algorithms for learning latent-variable models---such as EM and existing Gibbs samplers---are token-based, meaning that they update the variables associated with one sentence at a time. The incremental nature of these methods makes them susceptible to local optima/slow mixing. In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences. We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars.",
            "cx": 2451.6,
            "cy": -1014.01,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-1916",
            "name": "Mention Detection: Heuristics for the {O}nto{N}otes annotations",
            "publication_data": 2011,
            "citation": 10,
            "abstract": "Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3, Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.",
            "cx": 6347.6,
            "cy": -924.271,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1060",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2011,
            "citation": 315,
            "abstract": "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (Geo and Jobs), our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
            "cx": 1128.6,
            "cy": -924.271,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P11-1070",
            "name": "Web-Scale Features for Full-Scale Parsing",
            "publication_data": 2011,
            "citation": 41,
            "abstract": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.",
            "cx": 6022.6,
            "cy": -924.271,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1012",
            "name": "Decentralized Entity-Level Modeling for Coreference Resolution",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.",
            "cx": 5656.6,
            "cy": -744.791,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "J13-2005",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2013,
            "citation": 54,
            "abstract": "This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.",
            "cx": 851.597,
            "cy": -744.791,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "Q14-1037",
            "name": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking",
            "publication_data": 2014,
            "citation": 122,
            "abstract": "We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",
            "cx": 5664.6,
            "cy": -655.051,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N15-1029",
            "name": "Disfluency Detection with a Semi-{M}arkov Model and Prosodic Features",
            "publication_data": 2015,
            "citation": 16,
            "abstract": "We present a discriminative model for detecting disfluencies in spoken language transcripts. Structurally, our model is a semiMarkov conditional random field with features targeting characteristics unique to speech repairs. This gives a significant performance improvement over standard chain-structured CRFs that have been employed in past work. We then incorporate prosodic features over silences and relative word duration into our semi-CRF model, resulting in further performance gains; moreover, these features are not easily replaced by discrete prosodic indicators such as ToBI breaks. Our final system, the semi-CRF with prosodic information, achieves an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset.",
            "cx": 6755.6,
            "cy": -565.311,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P16-1188",
            "name": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
            "publication_data": 2016,
            "citation": 28,
            "abstract": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.",
            "cx": 5898.6,
            "cy": -475.571,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P06-1096",
            "name": "An End-to-End Discriminative Approach to Machine Translation",
            "publication_data": 2006,
            "citation": 244,
            "abstract": "We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic.",
            "cx": 4419.6,
            "cy": -1372.97,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D08-1092",
            "name": "Two Languages are Better than One (for Syntactic Parsing)",
            "publication_data": 2008,
            "citation": 90,
            "abstract": "We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",
            "cx": 1950.6,
            "cy": -1193.49,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D12-1079",
            "name": "Transforming Trees to Improve Syntactic Convergence",
            "publication_data": 2012,
            "citation": 13,
            "abstract": "We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.",
            "cx": 1876.6,
            "cy": -834.531,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 613,
            "name": 613,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P07-1107",
            "name": "Unsupervised Coreference Resolution in a Nonparametric {B}ayesian Model",
            "publication_data": 2007,
            "citation": 127,
            "abstract": "We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",
            "cx": 5654.6,
            "cy": -1283.23,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-2036",
            "name": "Asynchronous Binarization for Synchronous Grammars",
            "publication_data": 2009,
            "citation": 12,
            "abstract": "Binarization of n-ary rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.",
            "cx": 4045.6,
            "cy": -1103.75,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W14-1607",
            "name": "Grounding Language with Points and Paths in Continuous Spaces",
            "publication_data": 2014,
            "citation": 10,
            "abstract": "We present a model for generating pathvalued interpretations of natural language text. Our model encodes a map from natural language descriptions to paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010).",
            "cx": 923.597,
            "cy": -655.051,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-2133",
            "name": "How much do word embeddings encode about syntax?",
            "publication_data": 2014,
            "citation": 36,
            "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
            "cx": 4231.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1020",
            "name": "Sparser, Better, Faster {GPU} Parsing",
            "publication_data": 2014,
            "citation": 16,
            "abstract": "Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per secondxe2x80x94more than a 2x speedupxe2x80x94on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruningxe2x80x94nearly a 6x speedup.",
            "cx": 6382.6,
            "cy": -655.051,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P19-1031",
            "name": "Cross-Domain Generalization of Neural Constituency Parsers",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing{---}but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",
            "cx": 3080.6,
            "cy": -206.35,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N07-1052",
            "name": "Approximate Factoring for {A}* Search",
            "publication_data": 2007,
            "citation": 6,
            "abstract": "We present a novel method for creating Axe2x88x97 estimates for structured search problems. In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains.",
            "cx": 6273.6,
            "cy": -1283.23,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D07-1093",
            "name": "A Probabilistic Approach to Diachronic Phonology",
            "publication_data": 2007,
            "citation": 36,
            "abstract": "We present a probabilistic model of diachronic phonology in which individual word forms undergo stochastic edits along the branches of a phylogenetic tree. Our approach allows us to achieve three goals with a single unified model: (1) reconstruction of both ancient and modern word forms, (2) discovery of general phonological changes, and (3) selection among different phylogenies. We learn our model using a Monte Carlo EM algorithm and present quantitative results validating the model.",
            "cx": 741.597,
            "cy": -1283.23,
            "rx": 74.9067,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1105",
            "name": "Finding Cognate Groups Using Phylogenies",
            "publication_data": 2010,
            "citation": 23,
            "abstract": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.",
            "cx": 905.597,
            "cy": -1014.01,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1029",
            "name": "Simple Effective Decipherment via Combinatorial Optimization",
            "publication_data": 2011,
            "citation": 11,
            "abstract": "We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.",
            "cx": 867.597,
            "cy": -924.271,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1032",
            "name": "Large-Scale Cognate Recovery",
            "publication_data": 2011,
            "citation": 6,
            "abstract": "We present a system for the large scale induction of cognate groups. Our model explains the evolution of cognates as a sequence of mutations and innovations along a phylogeny. On the task of identifying cognates from over 21,000 words in 218 different languages from the Oceanic language family, our model achieves a cluster purity score over 91%, while maintaining pairwise recall over 62%.",
            "cx": 670.597,
            "cy": -924.271,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2009",
            "citation_count": 728,
            "name": 728,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P08-1088",
            "name": "Learning Bilingual Lexicons from Monolingual Corpora",
            "publication_data": 2008,
            "citation": 259,
            "abstract": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
            "cx": 1135.6,
            "cy": -1193.49,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N09-1069",
            "name": "Online {EM} for Unsupervised Models",
            "publication_data": 2009,
            "citation": 157,
            "abstract": "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.",
            "cx": 337.597,
            "cy": -1103.75,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D09-1147",
            "name": "Consensus Training for Consensus Decoding in Machine Translation",
            "publication_data": 2009,
            "citation": 25,
            "abstract": "We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forest-based consensus and minimum Bayes risk decoding methods. Our continuous objective can be optimized using simple gradient ascent. However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here. Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning. First, it specifically optimizes model weights for downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.",
            "cx": 3713.6,
            "cy": -1103.75,
            "rx": 95.4188,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1021",
            "name": "Unsupervised Transcription of Historical Documents",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Googlexe2x80x99s open source OCR system.",
            "cx": 1227.6,
            "cy": -744.791,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-2020",
            "name": "Improved Typesetting Models for Historical {OCR}",
            "publication_data": 2014,
            "citation": 13,
            "abstract": "We present richer typesetting models that extend the unsupervised historical document recognition system of BergKirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers.",
            "cx": 1242.6,
            "cy": -655.051,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 788,
            "name": 788,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D10-1040",
            "name": "A Game-Theoretic Approach to Generating Spatial Descriptions",
            "publication_data": 2010,
            "citation": 94,
            "abstract": "Language is sensitive to both semantic and pragmatic effects. To capture both effects, we model language use as a cooperative game between two players: a speaker, who generates an utterance, and a listener, who responds with an action. Specifically, we consider the task of generating spatial references to objects, wherein the listener must accurately identify an object described by the speaker. We show that a speaker model that acts optimally with respect to an explicit, embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions.",
            "cx": 1366.6,
            "cy": -1014.01,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D10-1049",
            "name": "A Simple Domain-Independent Probabilistic Approach to Generation",
            "publication_data": 2010,
            "citation": 121,
            "abstract": "We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains---Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.",
            "cx": 1501.6,
            "cy": -1014.01,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "black",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N09-1008",
            "name": "Improved Reconstruction of Protolanguage Word Forms",
            "publication_data": 2009,
            "citation": 19,
            "abstract": "We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous work. First, previous work focused on faithfulness features, which model changes between successive languages. We add markedness features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of Proto-Oceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date.",
            "cx": 958.597,
            "cy": -1103.75,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P12-1041",
            "name": "Coreference Semantics from Web Features",
            "publication_data": 2012,
            "citation": 36,
            "abstract": "To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.",
            "cx": 5952.6,
            "cy": -834.531,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1027",
            "name": "Error-Driven Analysis of Challenges in Coreference Resolution",
            "publication_data": 2013,
            "citation": 25,
            "abstract": "Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.",
            "cx": 6364.6,
            "cy": -744.791,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1203",
            "name": "Easy Victories and Uphill Battles in Coreference Resolution",
            "publication_data": 2013,
            "citation": 126,
            "abstract": "Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win xe2x80x9ceasy victoriesxe2x80x9d without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an xe2x80x9cuphill battle.xe2x80x9d Nonetheless, our final system 1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bjxc2xa8 orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.",
            "cx": 5956.6,
            "cy": -744.791,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2011",
            "citation_count": 653,
            "name": 653,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D16-1125",
            "name": "Reasoning about Pragmatics with Neural Listeners and Speakers",
            "publication_data": 2016,
            "citation": 36,
            "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural listener and speaker models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.",
            "cx": 1598.6,
            "cy": -475.571,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D17-1015",
            "name": "Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space",
            "publication_data": 2017,
            "citation": 7,
            "abstract": "We present a model for locating regions in space based on natural language descriptions. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as {`}on top of{'} or {`}next to,{'} and finally locate the region described in the sentence. All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation. To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions. We achieve a 32{\\%} relative error reduction compared to a strong neural baseline.",
            "cx": 1792.6,
            "cy": -385.831,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1177",
            "name": "Unified Pragmatic Models for Generating and Following Instructions",
            "publication_data": 2018,
            "citation": 11,
            "abstract": "We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.",
            "cx": 910.597,
            "cy": -296.09,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N19-1410",
            "name": "Pragmatically Informative Text Generation",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "We improve the informativeness of models for conditional text generation using techniques from computational pragmatics. These techniques formulate language production as a game between speakers and listeners, in which a speaker should generate output text that a listener can use to correctly identify the original input that the text describes. While such approaches are widely used in cognitive science and grounded language learning, they have received less attention for more standard language generation tasks. We consider two pragmatic modeling methods for text generation: one where pragmatics is imposed by information preservation, and another where pragmatics is imposed by explicit modeling of distractors. We find that these methods improve the performance of strong existing systems for abstractive summarization and generation from structured meaning representations.",
            "cx": 1572.6,
            "cy": -206.35,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 230,
            "name": 230,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1027",
            "name": "Faster and Smaller N-Gram Language Models",
            "publication_data": 2011,
            "citation": 103,
            "abstract": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.",
            "cx": 5517.6,
            "cy": -924.271,
            "rx": 76.2353,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D15-1138",
            "name": "Alignment-Based Compositional Semantics for Instruction Following",
            "publication_data": 2015,
            "citation": 17,
            "abstract": "This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the modelxe2x80x99s flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",
            "cx": 766.597,
            "cy": -565.311,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N16-1181",
            "name": "Learning to Compose Neural Networks for Question Answering",
            "publication_data": 2016,
            "citation": 205,
            "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.",
            "cx": 1324.6,
            "cy": -475.571,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P17-1105",
            "name": "Abstract Syntax Networks for Code Generation and Semantic Parsing",
            "publication_data": 2017,
            "citation": 58,
            "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7{\\%} exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1{\\%}. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.",
            "cx": 1291.6,
            "cy": -385.831,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1098",
            "name": "Structured Learning for Taxonomy Induction with Belief Propagation",
            "publication_data": 2014,
            "citation": 29,
            "abstract": "We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts. For efficient inference over taxonomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor. To train the system, we extract sub-structures of WordNet and discriminatively learn to reproduce them, using adaptive subgradient stochastic optimization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1.",
            "cx": 6184.6,
            "cy": -655.051,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1087",
            "name": "Decipherment with a Million Random Restarts",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.",
            "cx": 601.597,
            "cy": -744.791,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 282,
            "name": 282,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1340",
            "name": "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
            "publication_data": 2019,
            "citation": 9,
            "abstract": "We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",
            "cx": 2853.6,
            "cy": -206.35,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 269,
            "name": 269,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N15-1109",
            "name": "Unsupervised Code-Switching for Multilingual Historical Document Transcription",
            "publication_data": 2015,
            "citation": 7,
            "abstract": "Transcribing documents from the printing press era, a challenge in its own right, is more complicated when documents interleave multiple languagesxe2x80x94a common feature of 16th century texts. Additionally, many of these documents precede consistent orthographic conventions, making the task even harder. We extend the state-of-the-art historical OCR model of Berg-Kirkpatrick et al. (2013) to handle word-level code-switching between multiple languages. Further, we enable our system to handle spelling variability, including now-obsolete shorthand systems used by printers. Our results show average relative character error reductions of 14% across a variety of historical texts.",
            "cx": 1159.6,
            "cy": -565.311,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1225",
            "name": "A Deep Factorization of Style and Structure in Fonts",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying generative model combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our model learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our model outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.",
            "cx": 1102.6,
            "cy": -206.35,
            "rx": 58.8803,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 87,
            "name": 87,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N16-1150",
            "name": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mentionxe2x80x99s context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1",
            "cx": 5664.6,
            "cy": -475.571,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2052",
            "name": "Fine-Grained Entity Typing with High-Multiplicity Assignments",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.",
            "cx": 5121.6,
            "cy": -385.831,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1249",
            "name": "Constituency Parsing with a Self-Attentive Encoder",
            "publication_data": 2018,
            "citation": 22,
            "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
            "cx": 3128.6,
            "cy": -296.09,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 273,
            "name": 273,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.557",
            "name": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word{'}s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.",
            "cx": 2880.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "Q17-1031",
            "name": "Parsing with Traces: An {O}(n4) Algorithm and a Structural Representation",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3{\\%} of the Penn English Treebank. We also implement a proof-of-concept parser that recovers a range of null elements and trace types.",
            "cx": 4902.6,
            "cy": -385.831,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1655",
            "name": "Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "Vision-and-Language Navigation (VLN) requires grounding instructions, such as {``}turn right and stop at the door{''}, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. {``}stop at the door{''} might ground into visual objects, while {``}turn right{''} might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.",
            "cx": 843.597,
            "cy": -206.35,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 133,
            "name": 133,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1022",
            "name": "Translating Neuralese",
            "publication_data": 2017,
            "citation": "???",
            "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents{'} messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",
            "cx": 1600.6,
            "cy": -385.831,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018",
            "citation_count": 52,
            "name": 52,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2025",
            "name": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.",
            "cx": 2982.6,
            "cy": -385.831,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1178",
            "name": "Effective Inference for Generative Neural Parsing",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.",
            "cx": 3229.6,
            "cy": -385.831,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1311",
            "name": "Analogs of Linguistic Structure in Deep Representations",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a {``}syntax{''} with functional analogues to qualitative properties of natural language.",
            "cx": 5332.6,
            "cy": -385.831,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.208",
            "name": "Semantic Scaffolds for Pseudocode-to-Code Generation",
            "publication_data": 2020,
            "citation": 1,
            "abstract": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10{\\%} absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",
            "cx": 1291.6,
            "cy": -116.61,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1188",
            "name": "Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.",
            "cx": 5275.6,
            "cy": -206.35,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 19,
            "name": 19,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1197",
            "name": "Learning with Latent Language",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter{'}s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.",
            "cx": 5218.6,
            "cy": -296.09,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020",
            "citation_count": 1,
            "name": 1,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.naacl-main.81",
            "name": "Modular Networks for Compositional Instruction Following",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.",
            "cx": 843.597,
            "cy": -26.8701,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        }
    ],
    [
        {
            "source": "2001",
            "target": "2002",
            "d": "M28.5975,-1803.34C28.5975,-1787.98 28.5975,-1765.66 28.5975,-1750.29"
        },
        {
            "source": "W01-1812",
            "target": "N03-1016",
            "d": "M6950.26,-1795.24C6929,-1764.05 6892.89,-1711.09 6869.39,-1676.62"
        },
        {
            "source": "W01-1812",
            "target": "P09-1108",
            "d": "M6966.56,-1794.58C6962.28,-1704.64 6942.04,-1400.26 6857.6,-1166.62 6854.21,-1157.25 6849.42,-1147.69 6844.43,-1138.96"
        },
        {
            "source": "W01-1812",
            "target": "P10-2037",
            "d": "M6976.7,-1794.88C6987.94,-1760.67 7005.6,-1698.18 7005.6,-1643.19 7005.6,-1643.19 7005.6,-1643.19 7005.6,-1192.49 7005.6,-1138.4 6997.81,-1121.74 6967.6,-1076.88 6959.76,-1065.24 6949.14,-1054.54 6938.43,-1045.43"
        },
        {
            "source": "W01-0714",
            "target": "P02-1017",
            "d": "M443.597,-1794.35C443.597,-1786.38 443.597,-1777.49 443.597,-1769"
        },
        {
            "source": "P01-1044",
            "target": "W01-1812",
            "d": "M6876.17,-1821.67C6878.49,-1821.67 6880.81,-1821.67 6883.13,-1821.67"
        },
        {
            "source": "P01-1044",
            "target": "P03-1054",
            "d": "M6730.38,-1814.07C6434.26,-1786.59 5300.65,-1681.42 4975.97,-1651.3"
        },
        {
            "source": "P01-1044",
            "target": "N03-1016",
            "d": "M6808.14,-1794.87C6815.87,-1764.37 6828.78,-1713.45 6837.49,-1679.11"
        },
        {
            "source": "2002",
            "target": "2003",
            "d": "M28.5975,-1713.6C28.5975,-1698.24 28.5975,-1675.92 28.5975,-1660.55"
        },
        {
            "source": "W02-1002",
            "target": "N03-1033",
            "d": "M1547.6,-1704.61C1547.6,-1696.64 1547.6,-1687.75 1547.6,-1679.26"
        },
        {
            "source": "P02-1017",
            "target": "P04-1061",
            "d": "M443.597,-1704.74C443.597,-1674.25 443.597,-1623.7 443.597,-1589.5"
        },
        {
            "source": "P02-1017",
            "target": "P06-1111",
            "d": "M422.361,-1707.34C397.626,-1678.81 356.93,-1628.48 331.597,-1579.32 302.636,-1523.12 282.761,-1451.88 272.566,-1409.78"
        },
        {
            "source": "P02-1017",
            "target": "2020.emnlp-main.389",
            "d": "M486.021,-1715.86C617.54,-1666.88 1021.61,-1499.07 1222.6,-1220.36 1261.18,-1166.86 1238.65,-1139.09 1260.6,-1076.88 1275.12,-1035.7 1280.85,-1026.09 1300.6,-987.141 1356.49,-876.94 1438.6,-869.354 1438.6,-745.791 1438.6,-745.791 1438.6,-745.791 1438.6,-295.09 1438.6,-243.019 1418.8,-214.206 1457.6,-179.48 1496.9,-144.309 2231.19,-125.833 2524.27,-119.835"
        },
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1623.86C28.5975,-1608.5 28.5975,-1586.18 28.5975,-1570.81"
        },
        {
            "source": "P03-1054",
            "target": "W04-3201",
            "d": "M4808.15,-1639.99C4595.23,-1636.28 3999.42,-1622.55 3505.6,-1579.32 3470.86,-1576.28 3432.85,-1571.64 3399.5,-1567.12"
        },
        {
            "source": "P03-1054",
            "target": "W05-0104",
            "d": "M4967.35,-1636.11C5062.85,-1628.61 5216.88,-1611.82 5263.6,-1579.32 5291.19,-1560.13 5308.73,-1525.44 5318.74,-1499.05"
        },
        {
            "source": "P03-1054",
            "target": "W06-2903",
            "d": "M4966.85,-1635.2C5069.35,-1626.38 5242.73,-1607.78 5299.6,-1579.32 5350.4,-1553.91 5364.49,-1540.03 5390.6,-1489.58 5403.3,-1465.03 5407.87,-1433.97 5409.37,-1410.23"
        },
        {
            "source": "P03-1054",
            "target": "P06-1055",
            "d": "M4945.46,-1622.95C4971.82,-1612.75 5002.56,-1598.24 5026.6,-1579.32 5088.42,-1530.67 5139.83,-1452.95 5166.3,-1408.38"
        },
        {
            "source": "P03-1054",
            "target": "N07-1051",
            "d": "M4813.38,-1632.38C4755.82,-1623.79 4675.42,-1607.74 4609.6,-1579.32 4479.34,-1523.07 4452.48,-1493.14 4345.6,-1399.84 4316.73,-1374.64 4287.9,-1341.84 4267.94,-1317.62"
        },
        {
            "source": "P03-1054",
            "target": "D07-1072",
            "d": "M4885.03,-1615.23C4876.41,-1552.38 4854.38,-1391.94 4844.51,-1320.04"
        },
        {
            "source": "P03-1054",
            "target": "W08-1005",
            "d": "M4919.89,-1617.29C4956.62,-1586.26 5012.6,-1528.36 5012.6,-1463.71 5012.6,-1463.71 5012.6,-1463.71 5012.6,-1371.97 5012.6,-1323 5009.9,-1266.48 5007.86,-1230.63"
        },
        {
            "source": "P03-1054",
            "target": "D08-1091",
            "d": "M4821.73,-1627.17C4743.22,-1606.33 4623.6,-1558.95 4623.6,-1463.71 4623.6,-1463.71 4623.6,-1463.71 4623.6,-1371.97 4623.6,-1316.71 4588.47,-1261.22 4562.1,-1227.36"
        },
        {
            "source": "P03-1054",
            "target": "P09-1108",
            "d": "M4969.2,-1641.03C5313.33,-1640.03 6645.15,-1632.72 6715.6,-1579.32 6853.77,-1474.6 6838.47,-1232.48 6827.01,-1140.81"
        },
        {
            "source": "P03-1054",
            "target": "N09-1026",
            "d": "M4807.99,-1640.21C4431.27,-1635.48 2853.18,-1613.72 2635.6,-1579.32 2448.72,-1549.78 2317.51,-1576.43 2249.6,-1399.84 2215.71,-1311.72 2238.3,-1197.79 2253.68,-1140.7"
        },
        {
            "source": "P03-1054",
            "target": "N09-1063",
            "d": "M4969.11,-1640.52C5211.82,-1637.98 5933.77,-1626.56 6163.6,-1579.32 6307.63,-1549.72 6473.6,-1610.76 6473.6,-1463.71 6473.6,-1463.71 6473.6,-1463.71 6473.6,-1371.97 6473.6,-1305.47 6430.98,-1297.34 6378.6,-1256.36 6302.73,-1197 6199.84,-1153.29 6131.9,-1128.45"
        },
        {
            "source": "P03-1054",
            "target": "D09-1120",
            "d": "M4969.29,-1639.85C5252.82,-1634.85 6193.29,-1615.65 6323.6,-1579.32 6403.63,-1557.01 6445.23,-1561.06 6487.6,-1489.58 6560.93,-1365.87 6560.6,-1276.32 6467.6,-1166.62 6448.8,-1144.45 6379.24,-1127.32 6323.5,-1116.77"
        },
        {
            "source": "P03-1054",
            "target": "P10-1112",
            "d": "M4900.22,-1615.32C4914.42,-1581.28 4936.6,-1519.25 4936.6,-1463.71 4936.6,-1463.71 4936.6,-1463.71 4936.6,-1371.97 4936.6,-1319.9 4939.75,-1303.49 4917.6,-1256.36 4876.3,-1168.51 4793.51,-1088.13 4745.13,-1045.96"
        },
        {
            "source": "P03-1054",
            "target": "P11-2127",
            "d": "M4968.44,-1638.4C5245.63,-1627.68 6149.6,-1583.51 6149.6,-1463.71 6149.6,-1463.71 6149.6,-1463.71 6149.6,-1371.97 6149.6,-1319.9 6158.23,-1300.5 6130.6,-1256.36 6079.2,-1174.26 6022.26,-1197.91 5952.6,-1130.62 5895,-1074.98 5914.63,-1024.13 5843.6,-987.141 5752.76,-939.841 5024.43,-962.041 4922.6,-951.141 4897.23,-948.426 4869.64,-943.974 4845.28,-939.522"
        },
        {
            "source": "P03-1054",
            "target": "P12-2021",
            "d": "M4807.93,-1639.8C4530.7,-1634.69 3631.34,-1615.38 3581.6,-1579.32 3422.71,-1464.13 3537.48,-1327.15 3424.6,-1166.62 3410.46,-1146.51 3394.03,-1152.39 3382.6,-1130.62 3338.3,-1046.25 3341.84,-929.729 3347.21,-871.577"
        },
        {
            "source": "P03-1054",
            "target": "P12-1101",
            "d": "M4952.39,-1625.77C4988.03,-1615.59 5032.4,-1600.23 5068.6,-1579.32 5173.74,-1518.58 5193.19,-1489.94 5274.6,-1399.84 5294.21,-1378.14 5289.44,-1363.97 5312.6,-1346.1 5348.68,-1318.27 5376.42,-1342.37 5408.6,-1310.1 5447.31,-1271.29 5451.6,-1249.31 5451.6,-1194.49 5451.6,-1194.49 5451.6,-1194.49 5451.6,-1102.75 5451.6,-1034.84 5438.58,-1018.79 5432.6,-951.141 5430.26,-924.698 5428.73,-894.766 5427.81,-871.985"
        },
        {
            "source": "P03-1054",
            "target": "D12-1091",
            "d": "M4808.02,-1641.09C4309.04,-1639.97 1661.6,-1626.34 1661.6,-1463.71 1661.6,-1463.71 1661.6,-1463.71 1661.6,-1282.23 1661.6,-1072.01 1773.3,-1029.79 1936.6,-897.401 1957.54,-880.422 1983.91,-866.718 2007.35,-856.565"
        },
        {
            "source": "P03-1054",
            "target": "D12-1096",
            "d": "M4967.82,-1637.2C5081.53,-1630.36 5284.39,-1613.68 5348.6,-1579.32 5442.3,-1529.18 5469.07,-1499.27 5506.6,-1399.84 5515.03,-1377.5 5507.93,-1369.95 5506.6,-1346.1 5502.81,-1278.4 5489.6,-1262.3 5489.6,-1194.49 5489.6,-1194.49 5489.6,-1194.49 5489.6,-1102.75 5489.6,-1050.68 5473.32,-1025.44 5508.6,-987.141 5553.29,-938.619 5612,-1003.14 5652.6,-951.141 5670.41,-928.329 5662.03,-895.294 5651.02,-870.373"
        },
        {
            "source": "P03-1054",
            "target": "D12-1105",
            "d": "M4810.57,-1635.26C4730.79,-1627.7 4603.98,-1611.55 4498.6,-1579.32 4137.52,-1468.9 4025.47,-1453.68 3728.6,-1220.36 3704.55,-1201.46 3710.2,-1184.8 3685.6,-1166.62 3650.62,-1140.77 3619.05,-1166.59 3594.6,-1130.62 3521.15,-1022.58 3702.72,-911.207 3799.14,-861.829"
        },
        {
            "source": "P03-1054",
            "target": "P13-2018",
            "d": "M4808.2,-1638.67C4525.72,-1628.74 3595.6,-1586.7 3595.6,-1463.71 3595.6,-1463.71 3595.6,-1463.71 3595.6,-1371.97 3595.6,-1200.84 3517.31,-1145.33 3582.6,-987.141 3625.46,-883.292 3656.14,-849.025 3760.6,-807.661 3839.92,-776.249 4442.39,-775.966 4527.6,-771.661 4665.34,-764.701 4825.37,-755.893 4920.91,-750.558"
        },
        {
            "source": "P03-1054",
            "target": "D13-1195",
            "d": "M4969.12,-1640.9C5263.63,-1639.43 6272.15,-1630.66 6406.6,-1579.32 6709.75,-1463.56 6596.85,-926.617 6560.28,-781.838"
        },
        {
            "source": "P03-1054",
            "target": "P14-1022",
            "d": "M4808.04,-1640.21C4424.2,-1635.36 2797.62,-1612.87 2757.6,-1579.32 2717.38,-1545.61 2733.6,-1516.19 2733.6,-1463.71 2733.6,-1463.71 2733.6,-1463.71 2733.6,-1282.23 2733.6,-972.219 2845.49,-803.004 3143.6,-717.921 3259.97,-684.708 3612.71,-666.158 3773.35,-659.232"
        },
        {
            "source": "P03-1054",
            "target": "P15-1030",
            "d": "M4807.94,-1640.61C4516.75,-1638.16 3520.34,-1626.42 3205.6,-1579.32 3008.18,-1549.78 2771.6,-1663.33 2771.6,-1463.71 2771.6,-1463.71 2771.6,-1463.71 2771.6,-1282.23 2771.6,-981.835 2857.25,-808.72 3143.6,-717.921 3487.49,-608.873 3920.63,-577.82 4099.5,-569.343"
        },
        {
            "source": "P03-1054",
            "target": "P17-1076",
            "d": "M4807.79,-1640.95C4426.18,-1639.55 2819.18,-1630.62 2726.6,-1579.32 2672.6,-1549.4 2649.6,-1525.45 2649.6,-1463.71 2649.6,-1463.71 2649.6,-1463.71 2649.6,-1371.97 2649.6,-1173.41 2656.6,-1123.83 2656.6,-925.271 2656.6,-925.271 2656.6,-925.271 2656.6,-743.791 2656.6,-624.071 2696,-486.145 2716.5,-422.457"
        },
        {
            "source": "P03-1054",
            "target": "N18-1091",
            "d": "M4808.03,-1640.6C4339.27,-1637 1978.86,-1617.08 1830.6,-1579.32 1746.66,-1557.94 1706.62,-1560.99 1657.6,-1489.58 1614.71,-1427.12 1615.99,-871.631 1656.6,-807.661 1849.67,-503.502 2292.73,-362.178 2474.04,-315.522"
        },
        {
            "source": "N03-1016",
            "target": "D08-1012",
            "d": "M6786.53,-1630.39C6588.13,-1595.22 5935.45,-1483.51 5390.6,-1435.84 5159.12,-1415.59 4573.53,-1444.97 4345.6,-1399.84 4171.78,-1365.43 3982.21,-1270.95 3893.6,-1223.08"
        },
        {
            "source": "N03-1016",
            "target": "P09-1108",
            "d": "M6848.27,-1615.04C6850.35,-1580.44 6853.6,-1517.52 6853.6,-1463.71 6853.6,-1463.71 6853.6,-1463.71 6853.6,-1282.23 6853.6,-1232.51 6841.28,-1176.17 6831.93,-1140.56"
        },
        {
            "source": "N03-1016",
            "target": "N09-1063",
            "d": "M6845.23,-1615.16C6841.89,-1573.72 6830.5,-1491.73 6791.6,-1435.84 6776.82,-1414.61 6435.61,-1176.26 6411.6,-1166.62 6310.28,-1125.95 6275.79,-1151.36 6168.6,-1130.62 6159.7,-1128.9 6150.43,-1126.94 6141.22,-1124.88"
        },
        {
            "source": "N03-1016",
            "target": "P10-2037",
            "d": "M6874.26,-1617.54C6885.33,-1606.84 6897.38,-1593.43 6905.6,-1579.32 6953.58,-1496.94 6967.6,-1469.31 6967.6,-1373.97 6967.6,-1373.97 6967.6,-1373.97 6967.6,-1192.49 6967.6,-1140.01 6965.82,-1124.42 6943.6,-1076.88 6938.89,-1066.81 6932.24,-1056.9 6925.36,-1048.07"
        },
        {
            "source": "N03-1016",
            "target": "P10-2064",
            "d": "M6856.94,-1615.25C6879.19,-1557.04 6929.6,-1411.16 6929.6,-1284.23 6929.6,-1284.23 6929.6,-1284.23 6929.6,-1192.49 6929.6,-1140.42 6949.03,-1112.01 6910.6,-1076.88 6857.43,-1028.28 6390.86,-1017.83 6174.69,-1015.6"
        },
        {
            "source": "N03-1016",
            "target": "P10-1147",
            "d": "M6791.83,-1626.87C6696.8,-1602.53 6494.89,-1553.07 6321.6,-1525.58 6120.03,-1493.61 4697.15,-1360.84 4493.6,-1346.1 4151.9,-1321.36 4060.28,-1373.47 3723.6,-1310.1 3600.98,-1287.02 3555.18,-1298.11 3457.6,-1220.36 3435.68,-1202.9 3447.74,-1182.43 3424.6,-1166.62 3350.01,-1115.69 3309.26,-1162.08 3224.6,-1130.62 3180.57,-1114.26 3175.07,-1098.94 3133.6,-1076.88 3110.91,-1064.81 3085.48,-1052.46 3063.16,-1042"
        },
        {
            "source": "N03-1033",
            "target": "W04-3201",
            "d": "M1654.39,-1635.87C1970.12,-1620.13 2896.41,-1573.95 3205.72,-1558.53"
        },
        {
            "source": "N03-1033",
            "target": "P14-1022",
            "d": "M1553.6,-1615.34C1573.58,-1529.52 1637.83,-1254.56 1661.6,-1166.62 1672.47,-1126.41 1668.87,-1113.56 1688.6,-1076.88 1698.46,-1058.54 1709.75,-1059.74 1718.6,-1040.88 1763.35,-945.454 1681.89,-876.623 1761.6,-807.661 1801.41,-773.214 2656.26,-777.425 2708.6,-771.661 2840.2,-757.167 2870.27,-734.677 3001.6,-717.921 3283.79,-681.916 3621.39,-665.149 3773.38,-658.992"
        },
        {
            "source": "N03-1033",
            "target": "P17-1076",
            "d": "M1541.19,-1615.1C1538.62,-1604.1 1535.76,-1591.13 1533.6,-1579.32 1511.8,-1460.46 1504.06,-1430.64 1495.6,-1310.1 1487.96,-1201.35 1497.2,-1156.57 1571.6,-1076.88 1595.74,-1051.02 1621.12,-1069 1642.6,-1040.88 1706.83,-956.78 1614.34,-881.034 1690.6,-807.661 1761.05,-739.879 1814.24,-800.679 1907.6,-771.661 2218.13,-675.137 2562.11,-483.802 2683.34,-413.526"
        },
        {
            "source": "N03-1033",
            "target": "P18-2075",
            "d": "M1550.19,-1614.94C1552.65,-1591.45 1556.69,-1556.12 1561.6,-1525.58 1568.05,-1485.42 1571.05,-1475.61 1579.6,-1435.84 1580.54,-1431.47 1720.29,-810.672 1723.6,-807.661 1769.2,-766.203 1938.09,-783.388 1998.6,-771.661 2311.79,-710.958 2436.35,-780.855 2693.6,-592.181 2782.85,-526.721 2827.74,-395.968 2844.96,-332.834"
        },
        {
            "source": "N03-1033",
            "target": "N18-1091",
            "d": "M1518.24,-1616.2C1484.46,-1584.49 1433.6,-1526.33 1433.6,-1463.71 1433.6,-1463.71 1433.6,-1463.71 1433.6,-1282.23 1433.6,-1186.9 1433.39,-1149.13 1495.6,-1076.88 1519.98,-1048.56 1548.82,-1070.51 1571.6,-1040.88 1635.83,-957.318 1571.66,-903.918 1614.6,-807.661 1665.89,-692.67 1850.64,-420.312 1960.6,-358.96 2042.75,-313.124 2307.18,-301.187 2453.64,-298.118"
        },
        {
            "source": "2004",
            "target": "2005",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "W04-3201",
            "target": "D08-1091",
            "d": "M3380.65,-1535.62C3600.34,-1487.82 4245.4,-1346.08 4335.6,-1310.1 4393,-1287.2 4453.6,-1249.56 4492.41,-1223.47"
        },
        {
            "source": "W04-3201",
            "target": "P11-1049",
            "d": "M3398.88,-1546.6C3641.49,-1532.62 4297.2,-1487.58 4493.6,-1399.84 4724.24,-1296.81 4920.08,-1048.89 4985.93,-958.755"
        },
        {
            "source": "W04-3201",
            "target": "P14-1022",
            "d": "M3261.22,-1528.96C3242.21,-1518.5 3220.71,-1504.96 3203.6,-1489.58 3008.05,-1313.92 2797.63,-1232.63 2891.6,-987.141 2929.91,-887.063 2959.24,-860.276 3052.6,-807.661 3176.64,-737.748 3600.2,-684.167 3776.56,-664.363"
        },
        {
            "source": "W04-3201",
            "target": "D15-1032",
            "d": "M3293.83,-1525.76C3278.35,-1496.08 3253.19,-1445.46 3236.6,-1399.84 3194.29,-1283.49 3109.62,-984.161 3093.6,-861.401 3090.51,-837.718 3077.6,-825.396 3093.6,-807.661 3116.38,-782.405 4277.87,-686.322 4311.6,-681.921 4551.41,-650.627 4833.72,-602.956 4966.81,-579.832"
        },
        {
            "source": "P04-1061",
            "target": "P06-1111",
            "d": "M418.331,-1526.4C386.571,-1494.91 332.004,-1440.81 297.148,-1406.25"
        },
        {
            "source": "P04-1061",
            "target": "N06-1014",
            "d": "M546.187,-1552.39C724.305,-1552.37 1101.69,-1544.98 1414.6,-1489.58 1532.19,-1468.77 1664.59,-1425.35 1741.51,-1398.05"
        },
        {
            "source": "P04-1061",
            "target": "P08-1100",
            "d": "M437.634,-1525.49C423.197,-1462.64 386.341,-1302.2 369.823,-1230.3"
        },
        {
            "source": "P04-1061",
            "target": "P10-1131",
            "d": "M462.431,-1525.9C469.772,-1515.09 477.797,-1502.12 483.597,-1489.58 555.777,-1333.56 602.388,-1132.03 619.38,-1051.05"
        },
        {
            "source": "P04-1061",
            "target": "N10-1083",
            "d": "M452.699,-1525.66C463.941,-1491.45 481.597,-1428.96 481.597,-1373.97 481.597,-1373.97 481.597,-1373.97 481.597,-1192.49 481.597,-1140.67 458.052,-1084.74 440.218,-1049.8"
        },
        {
            "source": "P04-1061",
            "target": "D12-1001",
            "d": "M352.327,-1540.09C257.743,-1522.62 123.597,-1479.27 123.597,-1373.97 123.597,-1373.97 123.597,-1373.97 123.597,-1013.01 123.597,-923.781 229.35,-876.178 305.965,-853.399"
        },
        {
            "source": "2005",
            "target": "2006",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "P05-1046",
            "target": "N06-1041",
            "d": "M1401.05,-1454.77C1522.27,-1445.69 1731.02,-1427.61 1908.6,-1399.84 1919.16,-1398.19 1930.19,-1396.21 1941.11,-1394.1"
        },
        {
            "source": "P05-1046",
            "target": "P08-1100",
            "d": "M1199.93,-1453.46C1052.08,-1438.64 767.967,-1400.22 543.597,-1310.1 490.376,-1288.73 435.667,-1251.52 400.316,-1225.1"
        },
        {
            "source": "P05-1046",
            "target": "P09-1011",
            "d": "M1303.74,-1435.75C1313.78,-1372.9 1339.4,-1212.46 1350.88,-1140.56"
        },
        {
            "source": "H05-1010",
            "target": "N06-1014",
            "d": "M2340.16,-1457.19C2251.53,-1448.93 2064.53,-1429.49 1908.6,-1399.84 1900.06,-1398.22 1891.17,-1396.3 1882.35,-1394.26"
        },
        {
            "source": "H05-1010",
            "target": "N06-1015",
            "d": "M2418.35,-1439.57C2431.16,-1428.96 2446.72,-1416.08 2460.65,-1404.56"
        },
        {
            "source": "H05-1010",
            "target": "P07-1003",
            "d": "M2356.25,-1442.73C2310.19,-1418.45 2226.78,-1375.96 2152.6,-1346.1 2113.77,-1330.47 2069.33,-1315.83 2033.2,-1304.72"
        },
        {
            "source": "H05-1010",
            "target": "P09-1104",
            "d": "M2394.43,-1435.75C2401.3,-1372.9 2418.83,-1212.46 2426.69,-1140.56"
        },
        {
            "source": "H05-1010",
            "target": "P10-1147",
            "d": "M2442.48,-1456.13C2491.97,-1448.94 2567.93,-1433.17 2625.6,-1399.84 2787.6,-1306.22 2926.98,-1124.32 2980.37,-1048.97"
        },
        {
            "source": "H05-1010",
            "target": "N10-1015",
            "d": "M2363.57,-1439.76C2301.42,-1390.04 2149.89,-1262.8 2052.6,-1130.62 2034.16,-1105.58 2018.46,-1073.94 2007.9,-1050.1"
        },
        {
            "source": "H05-1010",
            "target": "N12-1004",
            "d": "M2441.8,-1454.38C2483.17,-1446.4 2541.81,-1430.44 2584.6,-1399.84 2590.41,-1395.69 2885.79,-991.299 2891.6,-987.141 2934.12,-956.714 2956.67,-974.207 3003.6,-951.141 3051.93,-927.384 3102.14,-891.043 3134.93,-865.46"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "W06-3105",
            "target": "P08-2007",
            "d": "M3309.42,-1345.98C3304.91,-1319.97 3294.26,-1280.46 3269.6,-1256.36 3268.44,-1255.23 3200.39,-1231.88 3148.54,-1214.2"
        },
        {
            "source": "W06-3105",
            "target": "D08-1033",
            "d": "M3315.4,-1345.78C3318.67,-1315.29 3324.08,-1264.74 3327.74,-1230.54"
        },
        {
            "source": "W06-3105",
            "target": "P10-1147",
            "d": "M3253.88,-1359.8C3184.73,-1342.07 3072.49,-1301.94 3018.6,-1220.36 3000.81,-1193.43 3000.65,-1102.34 3002.1,-1050.98"
        },
        {
            "source": "W06-3105",
            "target": "N10-1014",
            "d": "M3252.77,-1360.64C3198.43,-1349.74 3116.33,-1331.75 3046.6,-1310.1 2837.8,-1245.28 2782.56,-1231.51 2588.6,-1130.62 2550.27,-1110.69 2547.4,-1093.69 2507.6,-1076.88 2438.05,-1047.51 2415.31,-1057.17 2341.6,-1040.88 2332.55,-1038.88 2323.11,-1036.78 2313.69,-1034.68"
        },
        {
            "source": "W06-3105",
            "target": "N12-1004",
            "d": "M3340.56,-1348.37C3380.6,-1311.53 3447.1,-1237.23 3424.6,-1166.62 3383.39,-1037.33 3266.74,-919.751 3207.33,-866.135"
        },
        {
            "source": "P06-1055",
            "target": "N07-1051",
            "d": "M5109.15,-1364.87C4936.64,-1348.83 4517.78,-1309.9 4332.43,-1292.67"
        },
        {
            "source": "P06-1055",
            "target": "D07-1072",
            "d": "M5122.94,-1356.08C5061.57,-1340.52 4968.4,-1316.89 4905.83,-1301.03"
        },
        {
            "source": "P06-1055",
            "target": "D07-1094",
            "d": "M5249.9,-1357C5269.89,-1352.85 5292.07,-1348.76 5312.6,-1346.1 5592.93,-1309.8 5668.65,-1355.86 5947.6,-1310.1 5956.43,-1308.65 5965.62,-1306.7 5974.65,-1304.51"
        },
        {
            "source": "P06-1055",
            "target": "W08-1005",
            "d": "M5147.32,-1349.38C5130.55,-1338.57 5111.19,-1324.8 5095.6,-1310.1 5069.38,-1285.4 5044.63,-1252.65 5027.75,-1228.31"
        },
        {
            "source": "P06-1055",
            "target": "D08-1012",
            "d": "M5106.25,-1369C4870.83,-1359.96 4189.98,-1332.05 4147.6,-1310.1 4120.43,-1296.04 4130.45,-1272.71 4104.6,-1256.36 4097.01,-1251.56 4004.01,-1230.2 3930.94,-1213.91"
        },
        {
            "source": "P06-1055",
            "target": "D08-1091",
            "d": "M5138.38,-1351.25C5064.53,-1318.87 4928.88,-1259.65 4917.6,-1256.36 4815.53,-1226.62 4693.78,-1210.06 4615.15,-1201.72"
        },
        {
            "source": "P06-1055",
            "target": "P09-1108",
            "d": "M5249.53,-1356.69C5269.6,-1352.5 5291.92,-1348.47 5312.6,-1346.1 5459.14,-1329.32 6505.92,-1372.44 6639.6,-1310.1 6717.07,-1273.97 6775.35,-1187.61 6803.28,-1139.17"
        },
        {
            "source": "P06-1055",
            "target": "N09-1026",
            "d": "M5106.48,-1368.94C4803.48,-1357.29 3730.28,-1315.84 3657.6,-1310.1 3317.65,-1283.24 3233.24,-1268.17 2895.6,-1220.36 2653.84,-1186.13 2591.69,-1184.74 2353.6,-1130.62 2346.59,-1129.03 2339.31,-1127.2 2332.08,-1125.27"
        },
        {
            "source": "P06-1055",
            "target": "N09-1063",
            "d": "M5250.24,-1356.98C5270.14,-1352.85 5292.18,-1348.78 5312.6,-1346.1 5444.37,-1328.79 5792.86,-1373.64 5909.6,-1310.1 5918.77,-1305.11 5996.55,-1195.78 6036.75,-1138.73"
        },
        {
            "source": "P06-1055",
            "target": "W10-2906",
            "d": "M5170.78,-1346.39C5164.59,-1335.36 5157.49,-1322.23 5151.6,-1310.1 5136.46,-1278.94 5117.82,-1188.05 5090.6,-1166.62 5026.35,-1116.05 4486.58,-1049.93 4268.98,-1025.06"
        },
        {
            "source": "P06-1055",
            "target": "P10-2037",
            "d": "M5249.53,-1356.7C5269.6,-1352.52 5291.92,-1348.48 5312.6,-1346.1 5383.77,-1337.92 6538.44,-1343.92 6601.6,-1310.1 6627.39,-1296.29 6620.44,-1278.47 6639.6,-1256.36 6654.83,-1238.78 6664.67,-1239.7 6677.6,-1220.36 6715.55,-1163.58 6684.28,-1125.14 6732.6,-1076.88 6752.99,-1056.51 6780.79,-1042.72 6807.6,-1033.45"
        },
        {
            "source": "P06-1055",
            "target": "P10-2054",
            "d": "M5250.6,-1357.16C5270.41,-1353.06 5292.32,-1348.97 5312.6,-1346.1 5362.26,-1339.08 5727.65,-1346.08 5762.6,-1310.1 5797.44,-1274.23 5787.91,-1121.8 5781.39,-1051.19"
        },
        {
            "source": "P06-1055",
            "target": "P10-2064",
            "d": "M5250.24,-1357.03C5270.15,-1352.91 5292.19,-1348.82 5312.6,-1346.1 5374.29,-1337.87 5822.07,-1347.79 5871.6,-1310.1 5915.26,-1276.88 5917.17,-1118.77 5952.6,-1076.88 5965.27,-1061.9 5982.52,-1049.93 5999.73,-1040.65"
        },
        {
            "source": "P06-1055",
            "target": "P10-1112",
            "d": "M5179.14,-1346C5165.97,-1293.79 5136.16,-1179.92 5123.6,-1166.62 5036.86,-1074.86 4890.16,-1038.44 4796.15,-1024.11"
        },
        {
            "source": "P06-1055",
            "target": "N10-1061",
            "d": "M5251.65,-1357.75C5318.27,-1342.98 5413.28,-1320.6 5427.6,-1310.1 5519.87,-1242.4 5575.93,-1112.83 5598.74,-1050.38"
        },
        {
            "source": "P06-1055",
            "target": "N10-1082",
            "d": "M5106.42,-1368.9C4813.16,-1357.48 3802.15,-1317.74 3733.6,-1310.1 3719.71,-1308.55 3251.48,-1225.77 3238.6,-1220.36 3200.45,-1204.35 3200.49,-1183.22 3162.6,-1166.62 2959.23,-1077.56 2699.65,-1039.31 2557.12,-1024.09"
        },
        {
            "source": "P06-1055",
            "target": "W11-1916",
            "d": "M5249.54,-1356.76C5269.61,-1352.58 5291.93,-1348.53 5312.6,-1346.1 5373.21,-1338.99 6363.2,-1348.55 6410.6,-1310.1 6515.93,-1224.65 6417.8,-1037.23 6370.16,-959.8"
        },
        {
            "source": "P06-1055",
            "target": "P11-2127",
            "d": "M5186.6,-1345.94C5186.99,-1304.86 5182.87,-1223.79 5148.6,-1166.62 5077.56,-1048.15 4923.36,-977.316 4837.12,-945.451"
        },
        {
            "source": "P06-1055",
            "target": "P11-1060",
            "d": "M5106.62,-1368.83C4816.32,-1357.23 3824.35,-1317.11 3809.6,-1310.1 3780.67,-1296.36 3788.98,-1272.96 3761.6,-1256.36 3705.92,-1222.61 3679.48,-1245.92 3619.6,-1220.36 3578.14,-1202.67 3575.53,-1183.14 3533.6,-1166.62 3455.66,-1135.91 3423.71,-1167.7 3348.6,-1130.62 3262.35,-1088.05 3276.01,-1022.59 3186.6,-987.141 3094.61,-950.67 1621.47,-930.997 1225.78,-926.354"
        },
        {
            "source": "P06-1055",
            "target": "P11-1070",
            "d": "M5249.54,-1356.78C5269.61,-1352.59 5291.93,-1348.54 5312.6,-1346.1 5371.45,-1339.16 6337.41,-1352.7 6378.6,-1310.1 6487.46,-1197.5 6372.79,-1066.23 6237.6,-987.141 6199.97,-965.129 6154.01,-950.525 6114.25,-941.066"
        },
        {
            "source": "P06-1055",
            "target": "P12-1101",
            "d": "M5247.45,-1355.69C5288.55,-1343.65 5337.47,-1326.55 5351.6,-1310.1 5385.78,-1270.28 5375.6,-1246.97 5375.6,-1194.49 5375.6,-1194.49 5375.6,-1194.49 5375.6,-1013.01 5375.6,-960.534 5383.68,-947.408 5399.6,-897.401 5402.41,-888.557 5406.07,-879.263 5409.78,-870.645"
        },
        {
            "source": "P06-1055",
            "target": "D12-1001",
            "d": "M5106.45,-1368.76C4808.24,-1356.65 3769.04,-1314.16 3761.6,-1310.1 3735.92,-1296.09 3748.2,-1272.18 3723.6,-1256.36 3651.3,-1209.89 3612.71,-1253.94 3533.6,-1220.36 3495.52,-1204.2 3495.58,-1183.02 3457.6,-1166.62 3383.49,-1134.63 3350.94,-1168.37 3279.6,-1130.62 3249.39,-1114.64 3250.03,-1099.71 3224.6,-1076.88 3201.24,-1055.93 3145.04,-997.989 3115.6,-987.141 2898.39,-907.108 1252.59,-1023.16 1032.6,-951.141 993.282,-938.27 995.293,-912.033 956.597,-897.401 871.113,-865.076 615.854,-847.38 476.648,-839.895"
        },
        {
            "source": "P06-1055",
            "target": "D12-1091",
            "d": "M5106.33,-1369.3C4890.37,-1361.74 4294.82,-1338.83 4100.6,-1310.1 3932.35,-1285.22 3890.87,-1271.28 3728.6,-1220.36 3665.8,-1200.66 3653.02,-1187.5 3590.6,-1166.62 3534.55,-1147.88 3516.56,-1154.71 3462.6,-1130.62 3353.96,-1082.12 3348.47,-1027.63 3236.6,-987.141 3032.28,-913.204 2363.35,-857.756 2139.4,-840.877"
        },
        {
            "source": "P06-1055",
            "target": "D12-1096",
            "d": "M5246.53,-1355.5C5299.41,-1340.72 5368.97,-1319.88 5378.6,-1310.1 5416.25,-1271.83 5413.6,-1248.18 5413.6,-1194.49 5413.6,-1194.49 5413.6,-1194.49 5413.6,-1102.75 5413.6,-1044.45 5429.3,-1022.58 5475.6,-987.141 5531.75,-944.151 5586.37,-1006.33 5630.6,-951.141 5648.25,-929.116 5646.81,-896.403 5641.93,-871.391"
        },
        {
            "source": "P06-1055",
            "target": "D12-1105",
            "d": "M5250.26,-1357.17C5270.17,-1353.06 5292.21,-1348.94 5312.6,-1346.1 5364.91,-1338.82 5746.85,-1344.89 5786.6,-1310.1 5832.61,-1269.83 5837.23,-1101.7 5843.6,-1040.88 5846.08,-1017.13 5859.89,-1004.6 5843.6,-987.141 5804.08,-944.798 5380.86,-959.841 5323.6,-951.141 5221.1,-935.567 5199.24,-911.983 5096.6,-897.401 4875.36,-865.972 4199.82,-844.917 3949.35,-838.009"
        },
        {
            "source": "P06-1055",
            "target": "P13-2018",
            "d": "M5212.99,-1347.53C5222.71,-1337.09 5232.5,-1324.08 5237.6,-1310.1 5276.26,-1204 5237.25,-891.493 5161.6,-807.661 5140.73,-784.536 5110.08,-769.935 5081.46,-760.772"
        },
        {
            "source": "P06-1055",
            "target": "P13-1012",
            "d": "M5249.49,-1356.67C5273.84,-1347.41 5299.19,-1332.82 5313.6,-1310.1 5373.41,-1215.81 5246.68,-893.073 5318.6,-807.661 5347.64,-773.169 5460.25,-757.838 5547.6,-751.067"
        },
        {
            "source": "P06-1055",
            "target": "J13-2005",
            "d": "M5250.25,-1357.12C5270.16,-1353 5292.2,-1348.9 5312.6,-1346.1 5368.11,-1338.49 5771.85,-1345.12 5815.6,-1310.1 5872.19,-1264.8 5910.67,-1040.5 5861.6,-987.141 5829.22,-951.932 5479.76,-959.153 5432.6,-951.141 5338.47,-935.15 5318.83,-912.757 5224.6,-897.401 4663.49,-805.971 4516.62,-831.22 3948.6,-807.661 2688.69,-755.406 2370.62,-842.153 1111.6,-771.661 1054.93,-768.488 991.537,-762.236 941.538,-756.673"
        },
        {
            "source": "P06-1055",
            "target": "D13-1195",
            "d": "M5249.54,-1356.75C5269.61,-1352.57 5291.92,-1348.52 5312.6,-1346.1 5375.04,-1338.8 6390.44,-1343.66 6443.6,-1310.1 6530.45,-1255.28 6549.6,-1207.46 6549.6,-1104.75 6549.6,-1104.75 6549.6,-1104.75 6549.6,-923.271 6549.6,-874.543 6549.98,-818.291 6550.27,-782.405"
        },
        {
            "source": "P06-1055",
            "target": "Q14-1037",
            "d": "M5222.47,-1349.1C5235.79,-1338.86 5249.52,-1325.59 5257.6,-1310.1 5361.08,-1111.54 5163.62,-989.268 5294.6,-807.661 5366.49,-707.973 5515.92,-673.657 5601.25,-661.972"
        },
        {
            "source": "P06-1055",
            "target": "P14-1022",
            "d": "M5191.28,-1345.92C5193.4,-1334.92 5195.55,-1321.95 5196.6,-1310.1 5198.7,-1286.31 5199.46,-1280.07 5196.6,-1256.36 5184.35,-1155 5122.99,-921.52 5096.6,-897.401 5009.24,-817.575 4188.88,-701.121 3931.47,-666.269"
        },
        {
            "source": "P06-1055",
            "target": "N15-1029",
            "d": "M5249.53,-1356.71C5269.61,-1352.53 5291.92,-1348.49 5312.6,-1346.1 5381.67,-1338.13 6502.32,-1342.95 6563.6,-1310.1 6589.38,-1296.28 6582.44,-1278.47 6601.6,-1256.36 6616.83,-1238.78 6628.59,-1240.86 6639.6,-1220.36 6683.45,-1138.74 6675.6,-1107.67 6675.6,-1015.01 6675.6,-1015.01 6675.6,-1015.01 6675.6,-743.791 6675.6,-689.888 6706.65,-634.265 6730.06,-599.985"
        },
        {
            "source": "P06-1055",
            "target": "P16-1188",
            "d": "M5249.54,-1356.74C5269.61,-1352.55 5291.92,-1348.51 5312.6,-1346.1 5442.67,-1330.98 6375.34,-1372.26 6490.6,-1310.1 6610.28,-1245.55 6632.6,-947.859 6632.6,-925.271 6632.6,-925.271 6632.6,-925.271 6632.6,-654.051 6632.6,-528.85 6224.02,-491.677 6016.54,-480.871"
        },
        {
            "source": "P06-1096",
            "target": "P08-2007",
            "d": "M4354.64,-1370.13C4167.42,-1363.55 3611.74,-1335.01 3167.6,-1220.36 3160.99,-1218.66 3154.13,-1216.67 3147.35,-1214.57"
        },
        {
            "source": "P06-1096",
            "target": "P11-1049",
            "d": "M4450.08,-1348.9C4548.74,-1274.2 4859.38,-1039 4970,-955.249"
        },
        {
            "source": "P06-1111",
            "target": "N10-1083",
            "d": "M254.54,-1345.9C239.977,-1303.29 219.03,-1218.3 260.597,-1166.62 307.367,-1108.47 377.828,-1188.77 424.597,-1130.62 442.298,-1108.61 439.345,-1075.9 433.088,-1050.88"
        },
        {
            "source": "P06-1111",
            "target": "D12-1001",
            "d": "M247.047,-1346.21C226.022,-1312.77 193.597,-1251.9 193.597,-1194.49 193.597,-1194.49 193.597,-1194.49 193.597,-1013.01 193.597,-940.322 268.78,-888.854 324.558,-860.672"
        },
        {
            "source": "N06-1014",
            "target": "N06-1015",
            "d": "M1852.9,-1396.33C1872.36,-1404.9 1895.56,-1413.56 1917.6,-1417.84 2097.37,-1452.8 2313.48,-1415.72 2424.25,-1391.12"
        },
        {
            "source": "N06-1014",
            "target": "P07-1003",
            "d": "M1847.99,-1348.69C1868.13,-1337.36 1892.48,-1323.67 1913.55,-1311.82"
        },
        {
            "source": "N06-1014",
            "target": "D08-1092",
            "d": "M1817.18,-1346.21C1828.15,-1321.46 1847,-1283.87 1870.6,-1256.36 1881.1,-1244.12 1894.37,-1232.69 1907.06,-1223.08"
        },
        {
            "source": "N06-1014",
            "target": "P09-1011",
            "d": "M1760.43,-1349.52C1737.66,-1338.16 1709.93,-1323.88 1685.6,-1310.1 1581.52,-1251.16 1463.46,-1175.14 1400.94,-1134.12"
        },
        {
            "source": "N06-1014",
            "target": "P09-1104",
            "d": "M1873.66,-1354.31C1885.28,-1351.44 1897.27,-1348.59 1908.6,-1346.1 1993.82,-1327.37 2021.34,-1344.35 2101.6,-1310.1 2202.22,-1267.16 2207.47,-1222.37 2301.6,-1166.62 2325.46,-1152.49 2353.06,-1138.91 2376.58,-1128.09"
        },
        {
            "source": "N06-1014",
            "target": "P10-1147",
            "d": "M1871.85,-1353.83C1883.99,-1350.91 1896.63,-1348.17 1908.6,-1346.1 2077.07,-1317 2125.89,-1352.19 2291.6,-1310.1 2547.36,-1245.15 2592.57,-1174.81 2837.6,-1076.88 2869.42,-1064.16 2905,-1050.79 2935.03,-1039.75"
        },
        {
            "source": "N06-1014",
            "target": "N10-1014",
            "d": "M1874.62,-1354.65C1885.96,-1351.78 1897.61,-1348.85 1908.6,-1346.1 1973.43,-1329.89 2008.17,-1358.17 2054.6,-1310.1 2128.18,-1233.92 2039,-1160.82 2103.6,-1076.88 2115.89,-1060.91 2133.42,-1048.75 2151.41,-1039.62"
        },
        {
            "source": "N06-1014",
            "target": "N10-1015",
            "d": "M1806.78,-1346.08C1808.48,-1290.72 1819.59,-1159.23 1884.6,-1076.88 1897.63,-1060.37 1916.36,-1047.51 1934.63,-1037.89"
        },
        {
            "source": "N06-1014",
            "target": "N10-1083",
            "d": "M1713.78,-1372.33C1450.3,-1372.64 710.921,-1368.26 619.597,-1310.1 570.089,-1278.57 532.727,-1122.34 495.597,-1076.88 486.141,-1065.3 474.252,-1054.33 462.744,-1044.93"
        },
        {
            "source": "N06-1014",
            "target": "N12-1004",
            "d": "M1872.76,-1354.11C1884.64,-1351.21 1896.95,-1348.41 1908.6,-1346.1 2019.15,-1324.22 2051.58,-1342.26 2159.6,-1310.1 2210.72,-1294.88 2920.63,-955.693 3119.91,-860.293"
        },
        {
            "source": "N06-1014",
            "target": "D12-1001",
            "d": "M1713.72,-1371.7C1444.38,-1370.32 675.699,-1361.83 575.597,-1310.1 503.485,-1272.84 532.387,-1208.07 462.597,-1166.62 380.427,-1117.82 312.261,-1203.64 250.597,-1130.62 181.406,-1048.69 289.283,-924.927 348.684,-867.23"
        },
        {
            "source": "N06-1014",
            "target": "D12-1079",
            "d": "M1761.12,-1349.24C1746.38,-1339.38 1731.74,-1326.33 1723.6,-1310.1 1695,-1253.11 1710.91,-1229.12 1723.6,-1166.62 1747.09,-1050.87 1817.57,-928.032 1854.1,-869.879"
        },
        {
            "source": "N06-1014",
            "target": "D12-1091",
            "d": "M1806.09,-1346.08C1805.89,-1335 1805.69,-1321.92 1805.6,-1310.1 1804.75,-1204.62 1801.73,-1170.87 1849.6,-1076.88 1894.56,-988.597 1979.95,-907.512 2028.82,-865.535"
        },
        {
            "source": "N06-1015",
            "target": "P09-1104",
            "d": "M2491.05,-1345.86C2478.99,-1297.75 2453.23,-1195.03 2439.56,-1140.49"
        },
        {
            "source": "N06-1015",
            "target": "P10-1147",
            "d": "M2530.8,-1348.55C2619.46,-1286 2861.74,-1115.09 2960.67,-1045.3"
        },
        {
            "source": "N06-1041",
            "target": "N10-1061",
            "d": "M2136.58,-1363.35C2212.76,-1357.5 2317.94,-1350.12 2410.6,-1346.1 2838.05,-1327.57 3911.33,-1365.4 4335.6,-1310.1 4601.06,-1275.5 4659.84,-1227.24 4920.6,-1166.62 5139.05,-1115.84 5397.48,-1060.25 5527.31,-1032.63"
        },
        {
            "source": "N06-1041",
            "target": "N10-1082",
            "d": "M2055.65,-1346.68C2065.28,-1336.02 2075.81,-1323.08 2083.6,-1310.1 2140.82,-1214.68 2089.9,-1149.03 2174.6,-1076.88 2232.4,-1027.65 2267.53,-1057.57 2341.6,-1040.88 2350.15,-1038.95 2359.07,-1036.91 2367.96,-1034.85"
        },
        {
            "source": "N06-1041",
            "target": "N10-1083",
            "d": "M1953.17,-1353.37C1938.45,-1350.45 1923.11,-1347.83 1908.6,-1346.1 1839.56,-1337.86 717.21,-1345.89 657.597,-1310.1 606.445,-1279.39 566.395,-1119.45 524.597,-1076.88 510.999,-1063.03 493.67,-1051.12 477.074,-1041.55"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "P07-1003",
            "target": "D08-1092",
            "d": "M1959.06,-1256.38C1957.95,-1248.24 1956.7,-1239.1 1955.51,-1230.4"
        },
        {
            "source": "P07-1003",
            "target": "P09-1104",
            "d": "M2014.19,-1262.2C2074.08,-1239 2175.87,-1199.72 2263.6,-1166.62 2299.65,-1153.02 2340.17,-1138 2372.27,-1126.17"
        },
        {
            "source": "P07-1003",
            "target": "P10-1147",
            "d": "M2026.59,-1266.17C2076.03,-1253.73 2146.17,-1236.04 2207.6,-1220.36 2468.91,-1153.66 2778.15,-1073.54 2922.19,-1036.16"
        },
        {
            "source": "P07-1003",
            "target": "N10-1014",
            "d": "M1998.95,-1258.75C2011.4,-1248.6 2023.89,-1235.54 2030.6,-1220.36 2056.39,-1162.03 1989.78,-1128.24 2027.6,-1076.88 2040.91,-1058.8 2088.11,-1043.7 2133.2,-1032.95"
        },
        {
            "source": "P07-1003",
            "target": "N10-1015",
            "d": "M1913.76,-1261.38C1896.96,-1251.55 1880.01,-1238.03 1870.6,-1220.36 1859.37,-1199.28 1862.83,-1189.21 1870.6,-1166.62 1887.29,-1118.07 1927.29,-1073.9 1957.17,-1045.92"
        },
        {
            "source": "P07-1003",
            "target": "N10-1083",
            "d": "M1879.85,-1283.11C1718.48,-1283.4 1351.08,-1277.07 1048.6,-1220.36 917.24,-1195.74 886.113,-1179.17 761.597,-1130.62 709.41,-1110.27 700.169,-1096.22 647.597,-1076.88 590.123,-1055.74 573.675,-1057.01 514.597,-1040.88 507.581,-1038.97 500.272,-1036.97 492.966,-1034.96"
        },
        {
            "source": "P07-1003",
            "target": "D12-1079",
            "d": "M1908.35,-1262.81C1889.47,-1253.13 1870.33,-1239.33 1859.6,-1220.36 1795.27,-1106.68 1838.99,-942.461 1863.34,-870.991"
        },
        {
            "source": "P07-1107",
            "target": "D08-1033",
            "d": "M5556.85,-1278.65C5415.71,-1273.51 5146.65,-1263.85 4917.6,-1256.36 4389.19,-1239.09 4256.68,-1245.55 3728.6,-1220.36 3623.73,-1215.36 3503.44,-1207.18 3423,-1201.35"
        },
        {
            "source": "P07-1107",
            "target": "D09-1120",
            "d": "M5720.89,-1263.27C5818.96,-1235.09 6008.38,-1180.15 6168.6,-1130.62 6174.23,-1128.88 6180.07,-1127.05 6185.92,-1125.19"
        },
        {
            "source": "P07-1107",
            "target": "N10-1061",
            "d": "M5650.3,-1256.12C5642.4,-1208.13 5625.55,-1105.81 5616.55,-1051.17"
        },
        {
            "source": "N07-1051",
            "target": "W08-1005",
            "d": "M4325.83,-1279.59C4454.12,-1274.42 4705.7,-1259.8 4915.6,-1220.36 4923.87,-1218.81 4932.47,-1216.83 4940.93,-1214.65"
        },
        {
            "source": "N07-1051",
            "target": "D08-1012",
            "d": "M4178.62,-1265.18C4166.69,-1262.12 4154.28,-1259.05 4142.6,-1256.36 4062.66,-1237.99 4041.9,-1237.07 3961.6,-1220.36 3951.56,-1218.27 3941.05,-1216.05 3930.62,-1213.81"
        },
        {
            "source": "N07-1051",
            "target": "D08-1091",
            "d": "M4301.75,-1264.16C4350.27,-1249.58 4418.41,-1229.1 4468.26,-1214.12"
        },
        {
            "source": "N07-1051",
            "target": "D08-1092",
            "d": "M4157.55,-1279.27C3912.49,-1270.61 3177.49,-1244.42 2567.6,-1220.36 2375.28,-1212.78 2149.28,-1203.11 2031.41,-1198.01"
        },
        {
            "source": "N07-1051",
            "target": "P09-2036",
            "d": "M4184.63,-1263.18C4160.72,-1253.15 4133.83,-1239.01 4113.6,-1220.36 4089.31,-1197.97 4071.05,-1164.89 4059.61,-1139.85"
        },
        {
            "source": "N07-1051",
            "target": "N09-1026",
            "d": "M4157.78,-1278.41C3886.2,-1265.97 3046.3,-1227.06 3018.6,-1220.36 2962.19,-1206.72 2955.9,-1180.7 2899.6,-1166.62 2663.67,-1107.63 2593.1,-1172.81 2353.6,-1130.62 2346.22,-1129.32 2338.57,-1127.64 2331.01,-1125.75"
        },
        {
            "source": "N07-1051",
            "target": "N09-1063",
            "d": "M4320.28,-1272.96C4402.79,-1262.61 4536,-1244.16 4649.6,-1220.36 4739.68,-1201.49 4758.68,-1180.99 4849.6,-1166.62 5060.31,-1133.31 5696.37,-1113.97 5952.25,-1107.35"
        },
        {
            "source": "N07-1051",
            "target": "W10-2906",
            "d": "M4234.37,-1256.26C4231.67,-1245.28 4228.93,-1232.29 4227.6,-1220.36 4224.95,-1196.62 4226.6,-1190.49 4227.6,-1166.62 4228.26,-1150.58 4229.93,-1146.66 4230.6,-1130.62 4231.59,-1106.76 4238.78,-1099.32 4230.6,-1076.88 4226.74,-1066.28 4220.17,-1056.24 4212.98,-1047.45"
        },
        {
            "source": "N07-1051",
            "target": "P10-1112",
            "d": "M4308.27,-1266.35C4340.31,-1256.57 4378.07,-1241.73 4407.6,-1220.36 4432.38,-1202.43 4424.66,-1182.83 4450.6,-1166.62 4512.41,-1127.99 4549.96,-1171.07 4610.6,-1130.62 4634.93,-1114.39 4630.01,-1099.47 4648.6,-1076.88 4656.86,-1066.84 4666.37,-1056.43 4675.32,-1047.08"
        },
        {
            "source": "N07-1051",
            "target": "N10-1015",
            "d": "M4156.67,-1282.73C3941.07,-1283.05 3355.36,-1278.06 2871.6,-1220.36 2747.51,-1205.56 2719.42,-1183.53 2595.6,-1166.62 2495.32,-1152.93 2235.91,-1167.33 2141.6,-1130.62 2094.94,-1112.46 2051,-1074.49 2023.31,-1047.01"
        },
        {
            "source": "N07-1051",
            "target": "P11-2127",
            "d": "M4296.87,-1262.76C4320.88,-1252.56 4348.32,-1238.39 4369.6,-1220.36 4391.92,-1201.45 4386.9,-1187.3 4407.6,-1166.62 4427.22,-1147.01 4436.98,-1148.01 4458.6,-1130.62 4532.24,-1071.36 4535.24,-1036.93 4615.6,-987.141 4645.18,-968.811 4681.09,-954.324 4711.39,-943.991"
        },
        {
            "source": "N07-1051",
            "target": "P12-2021",
            "d": "M4176.94,-1265.66C4164.86,-1262.57 4152.35,-1259.37 4140.6,-1256.36 4077.94,-1240.34 4053.95,-1255.43 3999.6,-1220.36 3973.89,-1203.78 3981.36,-1184.58 3956.6,-1166.62 3919.71,-1139.87 3897.49,-1157.37 3860.6,-1130.62 3835.83,-1112.67 3843.73,-1092.78 3817.6,-1076.88 3747.9,-1034.47 3712.42,-1073.41 3637.6,-1040.88 3536.55,-996.958 3436.02,-913.086 3385.36,-867.295"
        },
        {
            "source": "N07-1051",
            "target": "D12-1079",
            "d": "M4156.9,-1281.53C3901.58,-1278.89 3142.05,-1267.22 2899.6,-1220.36 2828.42,-1206.61 2816.65,-1180.98 2745.6,-1166.62 2620.97,-1141.44 2285.55,-1192.71 2174.6,-1130.62 2106.55,-1092.54 2127.55,-1043.45 2073.6,-987.141 2027.36,-938.896 1964.99,-893.551 1922.92,-865.245"
        },
        {
            "source": "N07-1051",
            "target": "D12-1096",
            "d": "M4300.82,-1263.81C4311.02,-1261.04 4321.56,-1258.42 4331.6,-1256.36 4456.65,-1230.65 4509.31,-1289.57 4616.6,-1220.36 4640.15,-1205.17 4626.91,-1183.08 4649.6,-1166.62 4927.05,-965.39 5064.04,-1056.98 5399.6,-987.141 5489.31,-968.47 5535.57,-1013.62 5602.6,-951.141 5624.15,-931.052 5630.7,-897.547 5632.27,-871.763"
        },
        {
            "source": "N07-1051",
            "target": "D12-1105",
            "d": "M4193.32,-1260.87C4177.03,-1251.04 4160.69,-1237.65 4151.6,-1220.36 4121.86,-1163.84 4197.01,-1128.64 4159.6,-1076.88 4129.71,-1035.53 4093.72,-1068.15 4050.6,-1040.88 3977.22,-994.486 3912.03,-914.715 3878.49,-869.464"
        },
        {
            "source": "N07-1051",
            "target": "P13-2018",
            "d": "M4281.09,-1259.4C4297.71,-1248.74 4316.62,-1235.15 4331.6,-1220.36 4352.42,-1199.81 4350.88,-1189.11 4369.6,-1166.62 4440.87,-1080.99 4456.87,-1055.82 4544.6,-987.141 4606.67,-938.543 4628.19,-934.776 4697.6,-897.401 4785.32,-850.166 4889.81,-799.972 4951.77,-770.825"
        },
        {
            "source": "N07-1051",
            "target": "D13-1195",
            "d": "M4326.65,-1282.04C4477.47,-1280.39 4784.63,-1270.73 4877.6,-1220.36 4904.49,-1205.79 4893.19,-1180.21 4920.6,-1166.62 5064.08,-1095.45 6222.84,-1210.61 6361.6,-1130.62 6427.73,-1092.5 6509.21,-868.457 6538.99,-780.865"
        },
        {
            "source": "N07-1051",
            "target": "W14-1607",
            "d": "M4156.61,-1282.04C3913.03,-1280.83 3191.68,-1272.74 2595.6,-1220.36 2523.24,-1214 1360.1,-1082.53 1300.6,-1040.88 1241.48,-999.501 1273.33,-950.622 1224.6,-897.401 1155.21,-821.623 1102.36,-845.195 1030.6,-771.661 1011.02,-751.602 1015.92,-739.129 997.597,-717.921 988.083,-706.909 976.5,-696.243 965.35,-686.958"
        },
        {
            "source": "N07-1051",
            "target": "P14-2133",
            "d": "M4256.29,-1256.63C4281.57,-1210.11 4330.6,-1107.92 4330.6,-1015.01 4330.6,-1015.01 4330.6,-1015.01 4330.6,-833.531 4330.6,-776.86 4291.62,-721.434 4262.58,-688.001"
        },
        {
            "source": "N07-1051",
            "target": "P14-1020",
            "d": "M4326.41,-1280.52C4431.08,-1276.53 4613.59,-1263.37 4763.6,-1220.36 4817.44,-1204.92 4823.09,-1179.54 4877.6,-1166.62 4956.06,-1148.03 6272.29,-1188.34 6328.6,-1130.62 6380.17,-1077.75 6260.65,-1008.94 6242.6,-951.141 6211.7,-852.2 6184.91,-804.701 6241.6,-717.921 6250.47,-704.332 6282.66,-689.713 6314.04,-678.099"
        },
        {
            "source": "N07-1051",
            "target": "P14-1022",
            "d": "M4233.61,-1256.36C4230.2,-1245.28 4226.19,-1232.19 4222.6,-1220.36 4203.27,-1156.66 4225.05,-1125.52 4179.6,-1076.88 4148.48,-1043.59 4112.51,-1076.1 4083.6,-1040.88 4050.55,-1000.64 4064.6,-977.343 4064.6,-925.271 4064.6,-925.271 4064.6,-925.271 4064.6,-833.531 4064.6,-754.868 3977.58,-703.936 3915.57,-677.629"
        },
        {
            "source": "N07-1051",
            "target": "P15-1030",
            "d": "M4246.18,-1256.27C4253.72,-1212.18 4268.39,-1119.75 4273.6,-1040.88 4275.17,-1017.05 4278.11,-1010.6 4273.6,-987.141 4245.99,-843.685 4179.21,-825.377 4151.6,-681.921 4147.08,-658.467 4146.1,-651.424 4151.6,-628.181 4153.79,-618.908 4157.6,-609.442 4161.81,-600.793"
        },
        {
            "source": "N07-1051",
            "target": "P17-1076",
            "d": "M4173.75,-1266.96C4140.57,-1257.27 4101.27,-1242.33 4070.6,-1220.36 4045.73,-1202.55 4052.36,-1184.58 4027.6,-1166.62 3990.71,-1139.87 3968.49,-1157.37 3931.6,-1130.62 3906.83,-1112.67 3910.06,-1098.68 3888.6,-1076.88 3756.76,-942.986 3719.9,-910.452 3562.6,-807.661 3286.16,-627.022 2921.87,-467.512 2784.35,-409.793"
        },
        {
            "source": "N07-1051",
            "target": "N18-1091",
            "d": "M4178.49,-1265.14C4117.66,-1248.49 4033.88,-1225.02 4027.6,-1220.36 4004.69,-1203.4 4015.06,-1184.17 3992.6,-1166.62 3954.06,-1136.5 3927.14,-1160.74 3888.6,-1130.62 3866.14,-1113.07 3873.97,-1096.82 3853.6,-1076.88 3831.91,-1055.66 3820.54,-1058.16 3795.6,-1040.88 3688.15,-966.435 3653.12,-955.702 3562.6,-861.401 3542.34,-840.298 3548.13,-825.032 3524.6,-807.661 3483.21,-777.115 3463.08,-788.842 3414.6,-771.661 3098.46,-659.64 2996.3,-676.494 2709.6,-502.441 2658.27,-471.279 2644.66,-460.714 2608.6,-412.701 2590.18,-388.177 2575.64,-356.502 2566.14,-332.509"
        },
        {
            "source": "N07-1051",
            "target": "P19-1031",
            "d": "M4270.02,-1257.87C4281.06,-1247.24 4293.07,-1234.09 4301.6,-1220.36 4338.33,-1161.24 4406.6,-994.874 4406.6,-925.271 4406.6,-925.271 4406.6,-925.271 4406.6,-474.571 4406.6,-351.227 3513.52,-250.358 3193.22,-218.148"
        },
        {
            "source": "N07-1052",
            "target": "D08-1012",
            "d": "M6198.58,-1266.37C6176.76,-1262.38 6152.81,-1258.6 6130.6,-1256.36 5759.05,-1219.01 4823.83,-1232.3 4450.6,-1220.36 4277.79,-1214.83 4077.46,-1205.8 3955.41,-1200"
        },
        {
            "source": "N07-1052",
            "target": "P09-1108",
            "d": "M6334.86,-1262.39C6438.73,-1228.75 6648.5,-1160.81 6755.53,-1126.15"
        },
        {
            "source": "N07-1052",
            "target": "N09-1063",
            "d": "M6243.98,-1257.55C6205.86,-1225.79 6139.59,-1170.57 6098.02,-1135.93"
        },
        {
            "source": "D07-1072",
            "target": "D08-1091",
            "d": "M4785.24,-1266.65C4733.48,-1251.8 4655.45,-1229.43 4600.03,-1213.54"
        },
        {
            "source": "D07-1093",
            "target": "P10-1105",
            "d": "M751.036,-1256.37C766.517,-1215.86 799.896,-1135.97 842.597,-1076.88 850.32,-1066.19 859.965,-1055.71 869.368,-1046.49"
        },
        {
            "source": "D07-1093",
            "target": "P10-1131",
            "d": "M720.192,-1257.25C711.69,-1246.43 702.349,-1233.3 695.597,-1220.36 664.482,-1160.75 669.07,-1140.6 647.597,-1076.88 644.702,-1068.29 641.568,-1059.03 638.626,-1050.36"
        },
        {
            "source": "D07-1093",
            "target": "D11-1029",
            "d": "M811.06,-1272.82C890.364,-1257.94 1016.59,-1220.77 1074.6,-1130.62 1112,-1072.5 1067.91,-1031.13 1014.6,-987.141 992.343,-968.777 963.85,-955.112 937.885,-945.363"
        },
        {
            "source": "D07-1093",
            "target": "D11-1032",
            "d": "M747.949,-1256.3C759.608,-1203.07 779.669,-1078.78 738.597,-987.141 733.343,-975.419 724.672,-964.952 715.251,-956.11"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "P08-2007",
            "target": "D08-1033",
            "d": "M3153.67,-1193.49C3181.42,-1193.49 3209.18,-1193.49 3236.93,-1193.49"
        },
        {
            "source": "P08-2007",
            "target": "P09-1104",
            "d": "M3037.34,-1179.01C3018,-1174.52 2995.91,-1169.84 2975.6,-1166.62 2804.98,-1139.61 2759.94,-1152.59 2588.6,-1130.62 2560.12,-1126.97 2528.89,-1122.05 2501.78,-1117.49"
        },
        {
            "source": "P08-2007",
            "target": "P10-1147",
            "d": "M3078.24,-1166.73C3066.96,-1143.27 3049.78,-1107.69 3034.6,-1076.88 3030.31,-1068.18 3025.64,-1058.8 3021.26,-1050.05"
        },
        {
            "source": "P08-1088",
            "target": "D11-1029",
            "d": "M1140.13,-1166.32C1146,-1123.18 1150.68,-1036.97 1105.6,-987.141 1061.34,-938.222 1025.4,-967.895 961.597,-951.141 953.799,-949.094 945.649,-946.913 937.547,-944.718"
        },
        {
            "source": "P08-1100",
            "target": "N09-1069",
            "d": "M354.526,-1166.64C352.276,-1158.41 349.746,-1149.16 347.341,-1140.37"
        },
        {
            "source": "D08-1012",
            "target": "P09-1104",
            "d": "M3741.05,-1186.18C3472.19,-1169.47 2746.17,-1124.36 2507.67,-1109.54"
        },
        {
            "source": "D08-1012",
            "target": "N09-1026",
            "d": "M3741.71,-1185.45C3657.07,-1179.73 3532.99,-1171.8 3424.6,-1166.62 3186.73,-1155.27 2588.76,-1168.15 2353.6,-1130.62 2346,-1129.41 2338.14,-1127.75 2330.37,-1125.85"
        },
        {
            "source": "D08-1012",
            "target": "D09-1147",
            "d": "M3806.66,-1168.05C3791.44,-1157.7 3773.56,-1145.53 3757.56,-1134.65"
        },
        {
            "source": "D08-1012",
            "target": "P10-1112",
            "d": "M3766.94,-1174.73C3700.83,-1158.86 3614.6,-1137.12 3609.6,-1130.62 3595.04,-1111.69 3593.17,-1094.22 3609.6,-1076.88 3635,-1050.06 4236.71,-1042.98 4273.6,-1040.88 4392.12,-1034.14 4528.81,-1025.91 4616.62,-1020.58"
        },
        {
            "source": "D08-1012",
            "target": "P13-1021",
            "d": "M3753.02,-1179.16C3669.09,-1165.89 3554.26,-1145.42 3538.6,-1130.62 3430.72,-1028.68 3570.13,-899.585 3453.6,-807.661 3418.43,-779.922 1886.36,-773.104 1841.6,-771.661 1667.61,-766.053 1465.98,-757.085 1342.63,-751.318"
        },
        {
            "source": "D08-1012",
            "target": "P14-2020",
            "d": "M3752.59,-1179.38C3691.45,-1168.77 3617.67,-1152.08 3594.6,-1130.62 3483.96,-1027.75 3608.61,-899.417 3488.6,-807.661 3427.26,-760.763 2871.66,-776.529 2794.6,-771.661 2244.65,-736.919 1585.94,-684.158 1340.79,-664.138"
        },
        {
            "source": "D08-1033",
            "target": "P09-1011",
            "d": "M3259.46,-1179.54C3229.58,-1174.67 3194.52,-1169.6 3162.6,-1166.62 3077.05,-1158.64 1815.26,-1119.01 1453.11,-1107.74"
        },
        {
            "source": "D08-1033",
            "target": "P10-1147",
            "d": "M3269.66,-1175.27C3235.81,-1164.65 3193.74,-1149.46 3158.6,-1130.62 3114.6,-1107.03 3069.41,-1071.84 3039.27,-1046.48"
        },
        {
            "source": "D08-1033",
            "target": "N12-1004",
            "d": "M3321.85,-1166.79C3301.97,-1115 3254.54,-994.725 3206.6,-897.401 3202.1,-888.281 3196.88,-878.623 3191.86,-869.719"
        },
        {
            "source": "D08-1091",
            "target": "P10-1112",
            "d": "M4583.52,-1173.69C4605.52,-1163.51 4630.44,-1149.18 4648.6,-1130.62 4670.72,-1108.01 4686.32,-1075.44 4695.9,-1050.63"
        },
        {
            "source": "D08-1091",
            "target": "D12-1105",
            "d": "M4508.19,-1168.06C4463.73,-1126.34 4367.54,-1040.69 4273.6,-987.141 4177.19,-932.188 4145.05,-934.866 4040.6,-897.401 4001.8,-883.486 3958.08,-868.889 3922.78,-857.371"
        },
        {
            "source": "D08-1091",
            "target": "P14-1022",
            "d": "M4523.44,-1166.78C4506.42,-1125.78 4469.16,-1044.39 4420.6,-987.141 4295.46,-839.622 4251.23,-805.216 4078.6,-717.921 4031.57,-694.14 3974.22,-678.519 3929.32,-668.923"
        },
        {
            "source": "D08-1091",
            "target": "P15-1030",
            "d": "M4530.49,-1166.53C4518.7,-1079 4469.13,-791.082 4311.6,-628.181 4294.07,-610.058 4270.16,-596.58 4247.69,-586.912"
        },
        {
            "source": "D08-1092",
            "target": "W10-2906",
            "d": "M2020.03,-1186.96C2344.88,-1161.08 3714.48,-1051.98 4085.54,-1022.42"
        },
        {
            "source": "D08-1092",
            "target": "P10-1131",
            "d": "M1879.21,-1190.68C1649.78,-1184.54 940.47,-1163.14 842.597,-1130.62 801.599,-1117 799.35,-1097.89 761.597,-1076.88 738.529,-1064.05 712.318,-1051.58 689.167,-1041.24"
        },
        {
            "source": "D08-1092",
            "target": "N10-1015",
            "d": "M1946.61,-1166.57C1943.92,-1142.7 1942.33,-1106.58 1951.6,-1076.88 1954.73,-1066.84 1960.09,-1056.96 1965.94,-1048.15"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "P09-1011",
            "target": "D10-1040",
            "d": "M1359.6,-1076.43C1360.51,-1068.46 1361.52,-1059.57 1362.49,-1051.08"
        },
        {
            "source": "P09-1011",
            "target": "D10-1049",
            "d": "M1395.07,-1079.47C1414.37,-1067.79 1437.82,-1053.6 1457.8,-1041.51"
        },
        {
            "source": "P09-1011",
            "target": "P11-1060",
            "d": "M1321.01,-1079.05C1304.53,-1067.81 1284.82,-1053.99 1267.6,-1040.88 1231.68,-1013.54 1192.26,-980.437 1164.6,-956.671"
        },
        {
            "source": "P09-1011",
            "target": "J13-2005",
            "d": "M1296.31,-1084.33C1265.91,-1073.78 1229.12,-1059.01 1198.6,-1040.88 1166.66,-1021.92 1166.85,-1005.56 1134.6,-987.141 1092.85,-963.301 1075.3,-973.229 1032.6,-951.141 995.853,-932.134 990.799,-920.675 956.597,-897.401 932.444,-880.965 920.412,-883.752 901.597,-861.401 882.041,-838.169 868.989,-806.123 861.139,-781.661"
        },
        {
            "source": "P09-1104",
            "target": "P10-1147",
            "d": "M2493.28,-1093.15C2592.89,-1077.9 2788.07,-1048.01 2906.06,-1029.95"
        },
        {
            "source": "P09-1104",
            "target": "N10-1014",
            "d": "M2385.83,-1083.5C2357.24,-1071.2 2319.77,-1055.09 2288.37,-1041.58"
        },
        {
            "source": "P09-1104",
            "target": "N10-1015",
            "d": "M2381.92,-1084.73C2372.92,-1081.81 2363.54,-1079.05 2354.6,-1076.88 2248.49,-1051.19 2218.98,-1060.58 2111.6,-1040.88 2095.8,-1037.98 2078.89,-1034.5 2062.94,-1031.05"
        },
        {
            "source": "P09-1104",
            "target": "N10-1083",
            "d": "M2383.76,-1084.3C2374.24,-1081.27 2364.21,-1078.57 2354.6,-1076.88 1951.79,-1006.15 919.328,-1099.6 514.597,-1040.88 506.23,-1039.67 497.549,-1037.94 488.995,-1035.95"
        },
        {
            "source": "P09-1104",
            "target": "N12-1004",
            "d": "M2478.39,-1084.52C2539.02,-1061.48 2646.84,-1020.77 2739.6,-987.141 2871.86,-939.197 3027.73,-885.078 3111.35,-856.24"
        },
        {
            "source": "P09-1104",
            "target": "D12-1091",
            "d": "M2489.57,-1090.17C2516.39,-1081.18 2545.64,-1066.06 2561.6,-1040.88 2574.38,-1020.71 2575.81,-1006.34 2561.6,-987.141 2510.54,-918.148 2260.46,-867.834 2136.1,-846.641"
        },
        {
            "source": "P09-1108",
            "target": "P10-2037",
            "d": "M6842.71,-1077.37C6850.28,-1068.28 6858.92,-1057.89 6866.96,-1048.23"
        },
        {
            "source": "N09-1008",
            "target": "P10-1105",
            "d": "M942.982,-1076.9C937.753,-1068.24 931.839,-1058.45 926.283,-1049.26"
        },
        {
            "source": "N09-1008",
            "target": "D11-1029",
            "d": "M978.858,-1077.32C995.737,-1052.81 1014.38,-1015.39 996.597,-987.141 984.034,-967.182 963.045,-953.518 941.607,-944.224"
        },
        {
            "source": "N09-1008",
            "target": "D11-1032",
            "d": "M893.301,-1082.48C867.258,-1072.48 837.976,-1058.69 814.597,-1040.88 790.262,-1022.35 794.538,-1007.38 771.597,-987.141 757.414,-974.632 740.387,-963.125 724.389,-953.515"
        },
        {
            "source": "N09-1026",
            "target": "P09-2036",
            "d": "M2305.19,-1127.08C2322.32,-1135.66 2342.85,-1144.32 2362.6,-1148.62 2447,-1167.03 3832.43,-1163.12 3917.6,-1148.62 3939.34,-1144.92 3962.31,-1137.92 3982.69,-1130.53"
        },
        {
            "source": "N09-1026",
            "target": "P09-1104",
            "d": "M2345.33,-1103.75C2347.76,-1103.75 2350.2,-1103.75 2352.63,-1103.75"
        },
        {
            "source": "N09-1026",
            "target": "N10-1014",
            "d": "M2253.4,-1076.9C2249.8,-1068.59 2245.75,-1059.22 2241.9,-1050.35"
        },
        {
            "source": "N09-1026",
            "target": "N10-1082",
            "d": "M2309.86,-1081.51C2335.1,-1069.67 2366.79,-1054.8 2393.81,-1042.13"
        },
        {
            "source": "N09-1063",
            "target": "P09-1108",
            "d": "M6109.53,-1127.11C6130.06,-1135.69 6154.49,-1144.34 6177.6,-1148.62 6295.81,-1170.51 6600.9,-1173.14 6718.6,-1148.62 6736,-1145 6754.06,-1138.27 6770.11,-1131.1"
        },
        {
            "source": "N09-1063",
            "target": "P10-2064",
            "d": "M6062.1,-1076.43C6062.55,-1068.46 6063.06,-1059.57 6063.54,-1051.08"
        },
        {
            "source": "N09-1069",
            "target": "N10-1083",
            "d": "M361.16,-1077.84C370.097,-1068.4 380.408,-1057.5 389.907,-1047.45"
        },
        {
            "source": "N09-1069",
            "target": "P11-1060",
            "d": "M325.166,-1077.04C314.532,-1050.88 304.17,-1010.89 326.597,-987.141 374.741,-936.168 886.856,-958.363 956.597,-951.141 985.087,-948.191 1016.08,-943.876 1043.8,-939.612"
        },
        {
            "source": "N09-1069",
            "target": "J13-2005",
            "d": "M321.505,-1077.11C288.225,-1020.5 220.558,-883.854 290.597,-807.661 322.82,-772.608 670.434,-778.183 717.597,-771.661 735.059,-769.246 753.689,-766.05 771.333,-762.731"
        },
        {
            "source": "D09-1120",
            "target": "N10-1061",
            "d": "M6198.29,-1084.51C6188.54,-1081.54 6178.33,-1078.81 6168.6,-1076.88 5968.75,-1037.27 5912.58,-1074.28 5711.6,-1040.88 5702.97,-1039.45 5694,-1037.63 5685.13,-1035.63"
        },
        {
            "source": "D09-1120",
            "target": "P12-1041",
            "d": "M6246.77,-1076.65C6243.4,-1049.77 6233.82,-1008.84 6206.6,-987.141 6105.03,-906.192 6002.59,-1048.47 5916.6,-951.141 5896.55,-928.453 5910.45,-894.703 5926.45,-869.518"
        },
        {
            "source": "D09-1120",
            "target": "D13-1027",
            "d": "M6285.8,-1080.8C6339.59,-1048.17 6434.61,-986.919 6452.6,-951.141 6482.47,-891.727 6432.86,-819.789 6396.83,-778.751"
        },
        {
            "source": "D09-1120",
            "target": "D13-1203",
            "d": "M6250.2,-1076.53C6250.73,-1052.44 6249.13,-1016.19 6237.6,-987.141 6230.12,-968.302 6221.88,-967.267 6209.6,-951.141 6161.4,-887.858 6165.17,-856.774 6102.6,-807.661 6078.29,-788.58 6047.22,-774.336 6020,-764.356"
        },
        {
            "source": "D09-1120",
            "target": "Q14-1037",
            "d": "M6231.17,-1077.64C6223.51,-1066.46 6214.49,-1053.1 6206.6,-1040.88 6191.39,-1017.34 6197.1,-1002.42 6173.6,-987.141 6072.09,-921.169 6006.6,-1014.75 5903.6,-951.141 5804.41,-889.897 5862.26,-803.946 5783.6,-717.921 5773.22,-706.574 5741.88,-690.468 5713.97,-677.533"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "P10-1105",
            "target": "D11-1029",
            "d": "M894.402,-987.161C890.768,-978.771 886.674,-969.318 882.799,-960.371"
        },
        {
            "source": "P10-1105",
            "target": "D11-1032",
            "d": "M852.859,-993.321C818.573,-980.52 773.589,-963.725 736.891,-950.023"
        },
        {
            "source": "P10-1112",
            "target": "P11-2127",
            "d": "M4727.27,-987.633C4734.38,-978.46 4742.51,-967.966 4750.05,-958.237"
        },
        {
            "source": "P10-1131",
            "target": "D12-1001",
            "d": "M593.064,-988.52C549.094,-956.405 471.993,-900.093 424.6,-865.478"
        },
        {
            "source": "P10-1147",
            "target": "N12-1004",
            "d": "M3099.03,-1003.45C3132.25,-995.005 3166.26,-979.48 3186.6,-951.141 3203.26,-927.914 3196.97,-895.122 3187.98,-870.392"
        },
        {
            "source": "N10-1015",
            "target": "N10-1014",
            "d": "M2065.04,-1014.01C2080,-1014.01 2094.95,-1014.01 2109.91,-1014.01"
        },
        {
            "source": "N10-1015",
            "target": "D12-1079",
            "d": "M1976.83,-987.583C1956.39,-956.568 1921.75,-904.024 1899.02,-869.551"
        },
        {
            "source": "N10-1015",
            "target": "D12-1105",
            "d": "M2052.14,-998.613C2071.02,-994.39 2092.11,-990.113 2111.6,-987.141 2435.22,-937.802 3449.59,-864.064 3762.05,-841.994"
        },
        {
            "source": "N10-1061",
            "target": "P10-2054",
            "d": "M5703.23,-1014.01C5705.72,-1014.01 5708.21,-1014.01 5710.71,-1014.01"
        },
        {
            "source": "N10-1061",
            "target": "W11-1916",
            "d": "M5675.18,-994.727C5687.2,-991.819 5699.72,-989.122 5711.6,-987.141 5909.5,-954.129 5962.15,-972.971 6161.6,-951.141 6191.88,-947.827 6224.83,-943.486 6254.48,-939.309"
        },
        {
            "source": "N10-1061",
            "target": "P12-1041",
            "d": "M5679.37,-995.998C5715.67,-985.613 5760.29,-970.571 5797.6,-951.141 5842.01,-928.008 5887.29,-892.581 5917.35,-867.041"
        },
        {
            "source": "N10-1061",
            "target": "P13-1012",
            "d": "M5662.81,-991.806C5680.86,-981.991 5699.57,-968.572 5711.6,-951.141 5734.66,-917.734 5724.8,-901.817 5728.6,-861.401 5730.83,-837.621 5738.56,-829.367 5728.6,-807.661 5723.06,-795.606 5713.89,-785.016 5703.92,-776.161"
        },
        {
            "source": "N10-1061",
            "target": "D13-1203",
            "d": "M5675.46,-994.723C5687.4,-991.834 5699.82,-989.144 5711.6,-987.141 5757.45,-979.345 6097.16,-985.411 6128.6,-951.141 6178.1,-897.183 6063.06,-808.074 6062.6,-807.661 6046.68,-793.343 6026.92,-780.732 6008.8,-770.695"
        },
        {
            "source": "N10-1061",
            "target": "Q14-1037",
            "d": "M5674.63,-994.392C5696.78,-984.876 5719.75,-970.985 5734.6,-951.141 5783.13,-886.257 5759.46,-852.373 5766.6,-771.661 5768.7,-747.869 5778.32,-738.733 5766.6,-717.921 5760.2,-706.559 5735.42,-691.406 5711.89,-678.94"
        },
        {
            "source": "N10-1083",
            "target": "P10-1131",
            "d": "M505.492,-1014.01C507.953,-1014.01 510.415,-1014.01 512.876,-1014.01"
        },
        {
            "source": "N10-1083",
            "target": "P13-1021",
            "d": "M444.278,-987.804C470.64,-961.273 515.737,-920.254 562.597,-897.401 744.826,-808.534 980.761,-770.721 1117.05,-755.422"
        },
        {
            "source": "D10-1040",
            "target": "W14-1607",
            "d": "M1356.9,-987.494C1352.72,-976.361 1347.83,-963.136 1343.6,-951.141 1321.21,-887.734 1346.76,-851.297 1295.6,-807.661 1221.22,-744.22 1164.25,-818.795 1078.6,-771.661 1050.54,-756.221 1055.44,-738.133 1030.6,-717.921 1014.86,-705.12 996.162,-693.337 978.828,-683.566"
        },
        {
            "source": "D10-1040",
            "target": "D16-1125",
            "d": "M1409.35,-996.399C1440.79,-983.347 1479.84,-965.103 1490.6,-951.141 1522.62,-909.567 1514.6,-888.009 1514.6,-835.531 1514.6,-835.531 1514.6,-835.531 1514.6,-654.051 1514.6,-599.684 1547.2,-544.174 1571.78,-510.046"
        },
        {
            "source": "D10-1040",
            "target": "D17-1015",
            "d": "M1409.09,-996.058C1453.02,-978.464 1515.52,-953.212 1517.6,-951.141 1566.38,-902.417 1547.72,-870.272 1576.6,-807.661 1643.92,-661.713 1734.55,-492.96 1773.5,-421.572"
        },
        {
            "source": "D10-1040",
            "target": "N18-1177",
            "d": "M1370.02,-986.977C1374.16,-943.247 1375.39,-855.175 1326.6,-807.661 1257.19,-740.065 1188.76,-830.255 1111.6,-771.661 1073.93,-743.058 961.379,-437.994 923.329,-332.636"
        },
        {
            "source": "D10-1040",
            "target": "N19-1410",
            "d": "M1392.67,-990.004C1425.19,-958.613 1476.6,-898.97 1476.6,-835.531 1476.6,-835.531 1476.6,-835.531 1476.6,-384.831 1476.6,-329.049 1513.74,-273.994 1541.82,-240.344"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "W11-1916",
            "target": "D13-1027",
            "d": "M6350.11,-897.083C6353.03,-866.586 6357.87,-816.035 6361.14,-781.843"
        },
        {
            "source": "P11-1027",
            "target": "P12-1101",
            "d": "M5492.25,-898.831C5482.35,-889.283 5470.85,-878.196 5460.27,-867.994"
        },
        {
            "source": "P11-1049",
            "target": "D15-1032",
            "d": "M5028.13,-898.049C5048.08,-869.197 5078.57,-819.716 5090.6,-771.661 5105.52,-712.048 5080.61,-642.223 5061.72,-601.274"
        },
        {
            "source": "P11-1049",
            "target": "P16-1188",
            "d": "M5054.92,-902.107C5107.07,-874.256 5185.6,-819.995 5185.6,-745.791 5185.6,-745.791 5185.6,-745.791 5185.6,-654.051 5185.6,-638.161 5617.78,-539.435 5809.19,-496.489"
        },
        {
            "source": "P11-1060",
            "target": "J13-2005",
            "d": "M1064.01,-906.098C1032.48,-895.971 994.912,-881.21 964.597,-861.401 930.252,-838.959 898.286,-804.451 877.18,-778.974"
        },
        {
            "source": "P11-1060",
            "target": "W14-1607",
            "d": "M1106.44,-898.113C1080.67,-868.508 1037.23,-817.558 1002.6,-771.661 982.472,-744.991 961.186,-713.634 945.786,-690.311"
        },
        {
            "source": "P11-1060",
            "target": "D15-1138",
            "d": "M1063.1,-906.434C1001.51,-890.438 916.591,-867.893 901.597,-861.401 831.701,-831.138 792.769,-838.141 755.597,-771.661 725.917,-718.579 740.606,-645.217 753.759,-602.067"
        },
        {
            "source": "P11-1060",
            "target": "N16-1181",
            "d": "M1179.12,-902.203C1257.78,-867.783 1400.6,-798.089 1400.6,-745.791 1400.6,-745.791 1400.6,-745.791 1400.6,-654.051 1400.6,-600.666 1371.18,-545.012 1348.95,-510.575"
        },
        {
            "source": "P11-1060",
            "target": "P17-1105",
            "d": "M1154.96,-898.451C1182.26,-873.553 1226.82,-835.069 1269.6,-807.661 1300.39,-787.931 1324.2,-802.668 1343.6,-771.661 1377.42,-717.599 1361.64,-689.343 1343.6,-628.181 1330.16,-582.632 1319.22,-571.98 1285.6,-538.441 1264.65,-517.544 1243.78,-528.411 1229.6,-502.441 1218.15,-481.477 1220.45,-470.762 1229.6,-448.701 1234.33,-437.288 1242.31,-426.888 1250.99,-418.007"
        },
        {
            "source": "P11-1070",
            "target": "P12-1041",
            "d": "M6002.35,-897.893C5995.16,-888.878 5986.95,-878.588 5979.3,-869"
        },
        {
            "source": "P11-1070",
            "target": "D12-1001",
            "d": "M5930.08,-916.182C5845.02,-909.996 5715.4,-901.401 5602.6,-897.401 3896.48,-836.903 3468.61,-885.424 1761.6,-861.401 1278.55,-854.603 700.328,-842.437 477.5,-837.598"
        },
        {
            "source": "P11-1070",
            "target": "P14-1098",
            "d": "M6039.55,-897.522C6046.76,-886.464 6055.18,-873.359 6062.6,-861.401 6099.47,-801.931 6140.74,-731.643 6164.59,-690.634"
        },
        {
            "source": "D11-1029",
            "target": "P13-1021",
            "d": "M911.381,-901.686C976.568,-869.549 1099.16,-809.113 1171.05,-773.67"
        },
        {
            "source": "D11-1029",
            "target": "D13-1087",
            "d": "M832.823,-900.069C785.066,-868.205 699.133,-810.869 646.547,-775.782"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "P12-1041",
            "target": "P13-1012",
            "d": "M5886.91,-814.059C5841.09,-800.479 5779.86,-782.327 5732.04,-768.154"
        },
        {
            "source": "P12-1041",
            "target": "D13-1203",
            "d": "M5953.8,-807.206C5954.16,-799.239 5954.57,-790.348 5954.95,-781.86"
        },
        {
            "source": "P12-1041",
            "target": "Q14-1037",
            "d": "M5921.34,-808.84C5908.25,-797.96 5893.19,-784.703 5880.6,-771.661 5859.35,-749.656 5862.5,-735.687 5837.6,-717.921 5803.66,-693.705 5759.27,-678.088 5723.96,-668.603"
        },
        {
            "source": "N12-1004",
            "target": "D12-1105",
            "d": "M3207.33,-857.841C3222.49,-866.41 3240.76,-875.072 3258.6,-879.401 3342.82,-899.841 3562.43,-888.677 3648.6,-879.401 3692.69,-874.654 3741.2,-864.348 3780.06,-854.809"
        },
        {
            "source": "N12-1004",
            "target": "P13-1012",
            "d": "M3219.7,-815.08C3229.46,-812.049 3239.75,-809.35 3249.6,-807.661 3451.25,-773.067 4886.11,-778.332 5090.6,-771.661 5249.85,-766.465 5434.18,-757.487 5548.1,-751.593"
        },
        {
            "source": "N12-1004",
            "target": "Q14-1037",
            "d": "M3220.15,-815.107C3229.78,-812.115 3239.91,-809.423 3249.6,-807.661 3447.62,-771.627 3953.67,-783.345 4154.6,-771.661 4443.44,-754.865 5351.71,-681.499 5600.84,-661.246"
        },
        {
            "source": "D12-1091",
            "target": "P19-1340",
            "d": "M2074.7,-807.789C2108.46,-716.517 2235.9,-406.5 2454.6,-269.22 2502.5,-239.149 2648.56,-222.405 2749.99,-214.135"
        },
        {
            "source": "D12-1096",
            "target": "P13-2018",
            "d": "M5569.03,-815.429C5557.64,-812.539 5545.81,-809.801 5534.6,-807.661 5378.79,-777.934 5194.27,-760.266 5089.47,-751.844"
        },
        {
            "source": "D12-1096",
            "target": "D13-1027",
            "d": "M5712.5,-823.535C5751.77,-818.703 5799.64,-812.841 5842.6,-807.661 5984.57,-790.544 6148.31,-771.202 6253.98,-758.771"
        },
        {
            "source": "D12-1105",
            "target": "P14-1022",
            "d": "M3854.6,-807.343C3854.6,-776.846 3854.6,-726.294 3854.6,-692.103"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "P13-1012",
            "target": "D13-1203",
            "d": "M5757.96,-744.791C5798.26,-744.791 5838.55,-744.791 5878.85,-744.791"
        },
        {
            "source": "P13-1012",
            "target": "Q14-1037",
            "d": "M5659,-717.466C5659.72,-709.499 5660.53,-700.608 5661.31,-692.12"
        },
        {
            "source": "P13-1021",
            "target": "P14-2020",
            "d": "M1232.02,-717.941C1233.41,-709.802 1234.97,-700.662 1236.46,-691.956"
        },
        {
            "source": "P13-1021",
            "target": "N15-1109",
            "d": "M1179.15,-720.511C1164.35,-710.789 1149.82,-697.962 1141.6,-681.921 1128.81,-656.976 1135.04,-625.333 1143.5,-601.461"
        },
        {
            "source": "P13-1021",
            "target": "D19-1225",
            "d": "M1184.73,-720.03C1167.17,-709.405 1147.13,-696.086 1130.6,-681.921 1090.24,-647.35 1074.34,-640.203 1051.6,-592.181 1029.31,-545.121 1032.6,-528.642 1032.6,-476.571 1032.6,-476.571 1032.6,-476.571 1032.6,-384.831 1032.6,-331.773 1060,-275.625 1080.52,-241.028"
        },
        {
            "source": "J13-2005",
            "target": "W14-1607",
            "d": "M872.423,-718.413C879.886,-709.319 888.413,-698.927 896.339,-689.268"
        },
        {
            "source": "J13-2005",
            "target": "D15-1138",
            "d": "M818.478,-719.667C806.626,-709.35 794.306,-696.349 786.597,-681.921 773.554,-657.507 768.723,-626.436 767.06,-602.655"
        },
        {
            "source": "J13-2005",
            "target": "P17-1105",
            "d": "M833.384,-718.16C818.315,-693.507 802.002,-655.986 819.597,-628.181 839.584,-596.598 862.113,-608.786 895.597,-592.181 1020.1,-530.435 1164.65,-454.348 1239.95,-414.368"
        },
        {
            "source": "D13-1195",
            "target": "P14-1020",
            "d": "M6514.14,-724.753C6490.84,-712.58 6460.25,-696.604 6434.43,-683.121"
        },
        {
            "source": "D13-1203",
            "target": "D13-1027",
            "d": "M6023.93,-744.791C6096.06,-744.791 6168.19,-744.791 6240.31,-744.791"
        },
        {
            "source": "D13-1203",
            "target": "Q14-1037",
            "d": "M5905.24,-727.183C5895.41,-724.064 5885.2,-720.86 5875.6,-717.921 5823.28,-701.898 5763.4,-684.422 5720.83,-672.143"
        },
        {
            "source": "D13-1203",
            "target": "D15-1032",
            "d": "M5922.6,-721.428C5879.3,-694.207 5801.24,-649.267 5727.6,-628.181 5617.25,-596.587 5286.03,-577.521 5128.55,-570.009"
        },
        {
            "source": "D13-1203",
            "target": "P16-1188",
            "d": "M5950.93,-717.682C5940.51,-669.689 5918.31,-567.371 5906.45,-512.729"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "W14-1607",
            "target": "D15-1138",
            "d": "M881.937,-630.768C862.549,-619.934 839.284,-606.932 818.743,-595.452"
        },
        {
            "source": "Q14-1037",
            "target": "D15-1032",
            "d": "M5610.63,-650.697C5514.15,-644.051 5306.04,-626.768 5133.6,-592.181 5125.46,-590.549 5117,-588.54 5108.65,-586.369"
        },
        {
            "source": "Q14-1037",
            "target": "P16-1188",
            "d": "M5693.76,-631.932C5735.14,-600.549 5811.05,-542.971 5857.94,-507.41"
        },
        {
            "source": "Q14-1037",
            "target": "N16-1150",
            "d": "M5664.6,-627.863C5664.6,-597.366 5664.6,-546.814 5664.6,-512.622"
        },
        {
            "source": "Q14-1037",
            "target": "P17-2052",
            "d": "M5626.85,-635.473C5533.04,-589.309 5288.95,-469.187 5177.98,-414.578"
        },
        {
            "source": "P14-2020",
            "target": "N15-1109",
            "d": "M1218.59,-628.673C1209.82,-619.397 1199.76,-608.77 1190.47,-598.949"
        },
        {
            "source": "P14-2020",
            "target": "D19-1225",
            "d": "M1258.25,-628.351C1271.07,-603.647 1284.58,-566.088 1267.6,-538.441 1249.08,-508.292 1219.52,-529.388 1196.6,-502.441 1131.52,-425.95 1111.37,-303.985 1105.22,-243.625"
        },
        {
            "source": "P14-2133",
            "target": "P15-1030",
            "d": "M4217.42,-628.673C4212.59,-620.018 4207.1,-610.188 4201.93,-600.934"
        },
        {
            "source": "P14-1020",
            "target": "P16-1188",
            "d": "M6328.49,-634.208C6239.92,-601.731 6064.16,-537.283 5967.06,-501.675"
        },
        {
            "source": "P14-1022",
            "target": "P15-1030",
            "d": "M3911.73,-638.768C3968.24,-623.652 4054.72,-600.518 4114.7,-584.472"
        },
        {
            "source": "P14-1022",
            "target": "D15-1032",
            "d": "M3923.58,-647.695C3983.76,-642.234 4073.42,-634.296 4151.6,-628.181 4448.41,-604.962 4801.14,-581.808 4959.41,-571.653"
        },
        {
            "source": "P14-1022",
            "target": "P18-1249",
            "d": "M3790.85,-642.725C3628.23,-612.403 3205.08,-524.07 3124.6,-412.701 3108.16,-389.953 3110.86,-357.59 3116.7,-332.888"
        },
        {
            "source": "P14-1022",
            "target": "N18-1091",
            "d": "M3785.35,-647.94C3690.23,-638.944 3514.03,-620.121 3365.6,-592.181 3044.04,-531.651 2926.87,-589.579 2651.6,-412.701 2625.86,-396.165 2628.32,-382.342 2608.6,-358.96 2600.49,-349.348 2591.5,-339.079 2583.17,-329.725"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "P15-1030",
            "target": "D15-1032",
            "d": "M4256.17,-565.311C4490.04,-565.311 4723.91,-565.311 4957.78,-565.311"
        },
        {
            "source": "P15-1030",
            "target": "P18-1249",
            "d": "M4123.29,-549.275C3945.24,-504.132 3415.01,-369.704 3210.41,-317.831"
        },
        {
            "source": "P15-1030",
            "target": "2020.acl-main.557",
            "d": "M4229.53,-544.322C4284.63,-517.328 4368.6,-463.6 4368.6,-386.831 4368.6,-386.831 4368.6,-386.831 4368.6,-295.09 4368.6,-157.543 3353.17,-126.132 2999.97,-119.367"
        },
        {
            "source": "D15-1032",
            "target": "P16-1188",
            "d": "M5105.81,-549.918C5126.62,-545.59 5150.01,-541.25 5171.6,-538.441 5438.71,-503.674 5509.65,-538.479 5776.6,-502.441 5787.88,-500.917 5799.68,-498.928 5811.3,-496.728"
        },
        {
            "source": "D15-1032",
            "target": "Q17-1031",
            "d": "M5023.69,-539.259C4998.96,-508.121 4956.65,-454.871 4929.16,-420.271"
        },
        {
            "source": "D15-1032",
            "target": "P17-2052",
            "d": "M5054.94,-538.504C5068.46,-507.746 5091.11,-456.208 5106.2,-421.867"
        },
        {
            "source": "D15-1138",
            "target": "P19-1655",
            "d": "M769.563,-538.277C775.848,-486.389 792.216,-366.686 819.597,-269.22 822.065,-260.436 825.303,-251.166 828.598,-242.551"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "D16-1125",
            "target": "P17-1022",
            "d": "M1599.2,-448.246C1599.38,-440.279 1599.58,-431.388 1599.78,-422.9"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "P17-2025",
            "target": "D17-1178",
            "d": "M3077.63,-385.831C3092.67,-385.831 3107.7,-385.831 3122.73,-385.831"
        },
        {
            "source": "P17-2025",
            "target": "P18-2075",
            "d": "M2947.35,-360.855C2931.59,-350.14 2912.89,-337.415 2896.34,-326.162"
        },
        {
            "source": "P17-2025",
            "target": "P18-1249",
            "d": "M3021.72,-361.318C3039.66,-350.538 3061.11,-337.651 3080.06,-326.259"
        },
        {
            "source": "P17-2025",
            "target": "P19-1031",
            "d": "M2986.29,-358.942C2990.66,-334.438 2999.59,-297.322 3016.6,-269.22 3023.16,-258.37 3032.1,-248.119 3041.2,-239.197"
        },
        {
            "source": "P17-2025",
            "target": "P19-1340",
            "d": "M2977.83,-358.779C2972.24,-333.815 2961.03,-296.069 2940.6,-269.22 2931.02,-256.638 2918.17,-245.571 2905.28,-236.406"
        },
        {
            "source": "P17-1022",
            "target": "D17-1311",
            "d": "M1648.22,-409.188C1668.21,-417.764 1692.03,-426.419 1714.6,-430.701 1810.53,-448.9 5134.01,-450.617 5229.6,-430.701 5246.84,-427.109 5264.73,-420.47 5280.66,-413.38"
        },
        {
            "source": "P17-1076",
            "target": "D17-1178",
            "d": "M2779.04,-403.909C2809.93,-414.009 2850.52,-425.569 2887.6,-430.701 2985.77,-444.288 3013.09,-448.472 3110.6,-430.701 3130.85,-427.009 3152.14,-420.053 3171.02,-412.702"
        },
        {
            "source": "P17-1076",
            "target": "P18-2075",
            "d": "M2761.11,-362.007C2776.51,-351.197 2795.1,-338.153 2811.55,-326.602"
        },
        {
            "source": "P17-1076",
            "target": "P18-1249",
            "d": "M2787.19,-371.978C2854.06,-357.31 2964.11,-333.17 3041.14,-316.273"
        },
        {
            "source": "P17-1076",
            "target": "N18-1091",
            "d": "M2687.57,-364.259C2663.72,-352.305 2633.4,-337.099 2607.63,-324.179"
        },
        {
            "source": "P17-1076",
            "target": "P19-1031",
            "d": "M2729.79,-358.632C2732.46,-332.068 2740.84,-291.759 2766.6,-269.22 2832.27,-211.762 2874.04,-250.344 2959.6,-233.22 2969.31,-231.276 2979.46,-229.192 2989.56,-227.083"
        },
        {
            "source": "P17-1076",
            "target": "2020.emnlp-main.389",
            "d": "M2713.35,-359.6C2707.2,-348.63 2700.43,-335.45 2695.6,-322.96 2673.34,-265.41 2657.92,-195.089 2649.94,-153.458"
        },
        {
            "source": "P17-1076",
            "target": "2020.acl-main.557",
            "d": "M2722.98,-358.712C2715.47,-316.411 2707.38,-232.332 2747.6,-179.48 2760.51,-162.51 2779.21,-150.022 2798.52,-140.9"
        },
        {
            "source": "P17-1105",
            "target": "2020.acl-main.208",
            "d": "M1291.6,-358.722C1291.6,-310.828 1291.6,-208.836 1291.6,-154.11"
        },
        {
            "source": "D17-1178",
            "target": "P18-1249",
            "d": "M3201.19,-360.157C3189.99,-350.427 3176.99,-339.129 3165.11,-328.805"
        },
        {
            "source": "D17-1311",
            "target": "P19-1188",
            "d": "M5331.2,-358.839C5329.25,-334.912 5324.61,-298.762 5313.6,-269.22 5310.1,-259.84 5305.12,-250.326 5299.9,-241.664"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "P18-2075",
            "target": "P19-1340",
            "d": "M2853.6,-268.765C2853.6,-260.799 2853.6,-251.908 2853.6,-243.419"
        },
        {
            "source": "P18-1249",
            "target": "P19-1031",
            "d": "M3114.46,-269.24C3109.77,-260.672 3104.47,-250.994 3099.49,-241.88"
        },
        {
            "source": "P18-1249",
            "target": "P19-1340",
            "d": "M3065.16,-274.851C3023.43,-261.536 2968.72,-244.081 2925.39,-230.256"
        },
        {
            "source": "P18-1249",
            "target": "2020.emnlp-main.389",
            "d": "M3168.72,-271.256C3181.69,-261.287 3194.54,-248.445 3201.6,-233.22 3211.64,-211.549 3217.78,-197.044 3201.6,-179.48 3168.36,-143.409 2810.11,-150.749 2761.6,-143.48 2751.38,-141.949 2740.73,-140.05 2730.18,-137.98"
        },
        {
            "source": "N18-1091",
            "target": "P18-2075",
            "d": "M2643.54,-296.09C2683.97,-296.09 2724.4,-296.09 2764.83,-296.09"
        },
        {
            "source": "N18-1091",
            "target": "P18-1249",
            "d": "M2620.56,-314.004C2708.66,-335.012 2868.08,-364.197 3002.6,-340.96 3023.93,-337.275 3046.44,-330.322 3066.44,-322.97"
        },
        {
            "source": "N18-1177",
            "target": "P19-1655",
            "d": "M891.218,-269.712C884.343,-260.709 876.497,-250.434 869.184,-240.858"
        },
        {
            "source": "N18-1197",
            "target": "P19-1188",
            "d": "M5235.08,-269.712C5240.82,-260.877 5247.36,-250.818 5253.48,-241.396"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "P19-1340",
            "target": "P19-1031",
            "d": "M2950.69,-206.35C2953.32,-206.35 2955.95,-206.35 2958.57,-206.35"
        },
        {
            "source": "P19-1340",
            "target": "2020.acl-main.557",
            "d": "M2861.55,-179.5C2864.08,-171.273 2866.93,-162.023 2869.64,-153.231"
        },
        {
            "source": "P19-1655",
            "target": "2021.naacl-main.81",
            "d": "M843.597,-179.162C843.597,-148.665 843.597,-98.1136 843.597,-63.9219"
        },
        {
            "source": "2020",
            "target": "2021",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        }
    ],
    [
        {
            "id": "2001",
            "name": "2001",
            "x": "28.5975",
            "y": "-1817.97"
        },
        {
            "id": "2002",
            "name": "2002",
            "x": "28.5975",
            "y": "-1728.23"
        },
        {
            "id": "W01-1812",
            "name": "dan2001Parsing",
            "x": "6967.6",
            "y": "-1825.47"
        },
        {
            "id": "W01-1812",
            "name": "91",
            "x": "6967.6",
            "y": "-1810.47"
        },
        {
            "id": "N03-1016",
            "name": "dan2003{A}*",
            "x": "6846.6",
            "y": "-1645.99"
        },
        {
            "id": "N03-1016",
            "name": "192",
            "x": "6846.6",
            "y": "-1630.99"
        },
        {
            "id": "P09-1108",
            "name": "adam2009K-Best",
            "x": "6821.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1108",
            "name": "35",
            "x": "6821.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-2037",
            "name": "adam2010Top-Down",
            "x": "6894.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-2037",
            "name": "9",
            "x": "6894.6",
            "y": "-1002.81"
        },
        {
            "id": "W01-0714",
            "name": "dan2001Distributional",
            "x": "443.597",
            "y": "-1825.47"
        },
        {
            "id": "W01-0714",
            "name": "36",
            "x": "443.597",
            "y": "-1810.47"
        },
        {
            "id": "P02-1017",
            "name": "dan2002A",
            "x": "443.597",
            "y": "-1735.73"
        },
        {
            "id": "P02-1017",
            "name": "167",
            "x": "443.597",
            "y": "-1720.73"
        },
        {
            "id": "P01-1044",
            "name": "dan2001Parsing",
            "x": "6801.6",
            "y": "-1825.47"
        },
        {
            "id": "P01-1044",
            "name": "45",
            "x": "6801.6",
            "y": "-1810.47"
        },
        {
            "id": "P03-1054",
            "name": "dan2003Accurate",
            "x": "4888.6",
            "y": "-1645.99"
        },
        {
            "id": "P03-1054",
            "name": "2541",
            "x": "4888.6",
            "y": "-1630.99"
        },
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1638.49"
        },
        {
            "id": "W02-1002",
            "name": "dan2002Conditional",
            "x": "1547.6",
            "y": "-1735.73"
        },
        {
            "id": "W02-1002",
            "name": "90",
            "x": "1547.6",
            "y": "-1720.73"
        },
        {
            "id": "N03-1033",
            "name": "kristina2003Feature-Rich",
            "x": "1547.6",
            "y": "-1645.99"
        },
        {
            "id": "N03-1033",
            "name": "2214",
            "x": "1547.6",
            "y": "-1630.99"
        },
        {
            "id": "P04-1061",
            "name": "dan2004Corpus-Based",
            "x": "443.597",
            "y": "-1556.25"
        },
        {
            "id": "P04-1061",
            "name": "360",
            "x": "443.597",
            "y": "-1541.25"
        },
        {
            "id": "P06-1111",
            "name": "aria2006Prototype-Driven",
            "x": "264.597",
            "y": "-1376.77"
        },
        {
            "id": "P06-1111",
            "name": "39",
            "x": "264.597",
            "y": "-1361.77"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "steven2020Unsupervised",
            "x": "2643.6",
            "y": "-120.41"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "???",
            "x": "2643.6",
            "y": "-105.41"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "W04-3201",
            "name": "ben2004Max-Margin",
            "x": "3307.6",
            "y": "-1556.25"
        },
        {
            "id": "W04-3201",
            "name": "198",
            "x": "3307.6",
            "y": "-1541.25"
        },
        {
            "id": "W05-0104",
            "name": "dan2005A",
            "x": "5329.6",
            "y": "-1466.51"
        },
        {
            "id": "W05-0104",
            "name": "1",
            "x": "5329.6",
            "y": "-1451.51"
        },
        {
            "id": "W06-2903",
            "name": "slav2006Non-Local",
            "x": "5409.6",
            "y": "-1376.77"
        },
        {
            "id": "W06-2903",
            "name": "0",
            "x": "5409.6",
            "y": "-1361.77"
        },
        {
            "id": "P06-1055",
            "name": "slav2006Learning",
            "x": "5185.6",
            "y": "-1376.77"
        },
        {
            "id": "P06-1055",
            "name": "727",
            "x": "5185.6",
            "y": "-1361.77"
        },
        {
            "id": "N07-1051",
            "name": "slav2007Improved",
            "x": "4241.6",
            "y": "-1287.03"
        },
        {
            "id": "N07-1051",
            "name": "533",
            "x": "4241.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1072",
            "name": "percy2007The",
            "x": "4839.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1072",
            "name": "146",
            "x": "4839.6",
            "y": "-1272.03"
        },
        {
            "id": "W08-1005",
            "name": "slav2008Parsing",
            "x": "5005.6",
            "y": "-1197.29"
        },
        {
            "id": "W08-1005",
            "name": "23",
            "x": "5005.6",
            "y": "-1182.29"
        },
        {
            "id": "D08-1091",
            "name": "slav2008Sparse",
            "x": "4533.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1091",
            "name": "35",
            "x": "4533.6",
            "y": "-1182.29"
        },
        {
            "id": "N09-1026",
            "name": "john2009Efficient",
            "x": "2264.6",
            "y": "-1107.55"
        },
        {
            "id": "N09-1026",
            "name": "24",
            "x": "2264.6",
            "y": "-1092.55"
        },
        {
            "id": "N09-1063",
            "name": "adam2009Hierarchical",
            "x": "6060.6",
            "y": "-1107.55"
        },
        {
            "id": "N09-1063",
            "name": "17",
            "x": "6060.6",
            "y": "-1092.55"
        },
        {
            "id": "D09-1120",
            "name": "aria2009Simple",
            "x": "6248.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1120",
            "name": "158",
            "x": "6248.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-1112",
            "name": "mohit2010Simple,",
            "x": "4707.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1112",
            "name": "26",
            "x": "4707.6",
            "y": "-1002.81"
        },
        {
            "id": "P11-2127",
            "name": "mohit2011The",
            "x": "4775.6",
            "y": "-928.071"
        },
        {
            "id": "P11-2127",
            "name": "2",
            "x": "4775.6",
            "y": "-913.071"
        },
        {
            "id": "P12-2021",
            "name": "jonathan2012Robust",
            "x": "3351.6",
            "y": "-838.331"
        },
        {
            "id": "P12-2021",
            "name": "2",
            "x": "3351.6",
            "y": "-823.331"
        },
        {
            "id": "P12-1101",
            "name": "adam2012Large-Scale",
            "x": "5426.6",
            "y": "-838.331"
        },
        {
            "id": "P12-1101",
            "name": "27",
            "x": "5426.6",
            "y": "-823.331"
        },
        {
            "id": "D12-1091",
            "name": "taylor2012An",
            "x": "2065.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1091",
            "name": "45",
            "x": "2065.6",
            "y": "-823.331"
        },
        {
            "id": "D12-1096",
            "name": "jonathan2012Parser",
            "x": "5631.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1096",
            "name": "47",
            "x": "5631.6",
            "y": "-823.331"
        },
        {
            "id": "D12-1105",
            "name": "david2012Training",
            "x": "3854.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1105",
            "name": "8",
            "x": "3854.6",
            "y": "-823.331"
        },
        {
            "id": "P13-2018",
            "name": "jonathan2013An",
            "x": "5005.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2018",
            "name": "11",
            "x": "5005.6",
            "y": "-733.591"
        },
        {
            "id": "D13-1195",
            "name": "john2013A",
            "x": "6550.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1195",
            "name": "17",
            "x": "6550.6",
            "y": "-733.591"
        },
        {
            "id": "P14-1022",
            "name": "david2014Less",
            "x": "3854.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1022",
            "name": "43",
            "x": "3854.6",
            "y": "-643.851"
        },
        {
            "id": "P15-1030",
            "name": "greg2015Neural",
            "x": "4182.6",
            "y": "-569.111"
        },
        {
            "id": "P15-1030",
            "name": "33",
            "x": "4182.6",
            "y": "-554.111"
        },
        {
            "id": "P17-1076",
            "name": "mitchell2017A",
            "x": "2728.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1076",
            "name": "39",
            "x": "2728.6",
            "y": "-374.631"
        },
        {
            "id": "N18-1091",
            "name": "david2018What{'}s",
            "x": "2553.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1091",
            "name": "3",
            "x": "2553.6",
            "y": "-284.89"
        },
        {
            "id": "D08-1012",
            "name": "slav2008Coarse-to-Fine",
            "x": "3842.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1012",
            "name": "36",
            "x": "3842.6",
            "y": "-1182.29"
        },
        {
            "id": "P10-2064",
            "name": "adam2010Hierarchical",
            "x": "6065.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-2064",
            "name": "6",
            "x": "6065.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-1147",
            "name": "john2010Discriminative",
            "x": "3003.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1147",
            "name": "25",
            "x": "3003.6",
            "y": "-1002.81"
        },
        {
            "id": "P18-2075",
            "name": "daniel2018Policy",
            "x": "2853.6",
            "y": "-299.89"
        },
        {
            "id": "P18-2075",
            "name": "8",
            "x": "2853.6",
            "y": "-284.89"
        },
        {
            "id": "2005",
            "name": "2005",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "P11-1049",
            "name": "taylor2011Jointly",
            "x": "5009.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1049",
            "name": "165",
            "x": "5009.6",
            "y": "-913.071"
        },
        {
            "id": "D15-1032",
            "name": "jonathan2015An",
            "x": "5043.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1032",
            "name": "14",
            "x": "5043.6",
            "y": "-554.111"
        },
        {
            "id": "N06-1014",
            "name": "percy2006Alignment",
            "x": "1806.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1014",
            "name": "392",
            "x": "1806.6",
            "y": "-1361.77"
        },
        {
            "id": "P08-1100",
            "name": "percy2008Analyzing",
            "x": "361.597",
            "y": "-1197.29"
        },
        {
            "id": "P08-1100",
            "name": "21",
            "x": "361.597",
            "y": "-1182.29"
        },
        {
            "id": "P10-1131",
            "name": "taylor2010Phylogenetic",
            "x": "626.597",
            "y": "-1017.81"
        },
        {
            "id": "P10-1131",
            "name": "50",
            "x": "626.597",
            "y": "-1002.81"
        },
        {
            "id": "N10-1083",
            "name": "taylor2010Painless",
            "x": "420.597",
            "y": "-1017.81"
        },
        {
            "id": "N10-1083",
            "name": "168",
            "x": "420.597",
            "y": "-1002.81"
        },
        {
            "id": "D12-1001",
            "name": "greg2012Syntactic",
            "x": "383.597",
            "y": "-838.331"
        },
        {
            "id": "D12-1001",
            "name": "43",
            "x": "383.597",
            "y": "-823.331"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "P05-1046",
            "name": "trond2005Unsupervised",
            "x": "1299.6",
            "y": "-1466.51"
        },
        {
            "id": "P05-1046",
            "name": "72",
            "x": "1299.6",
            "y": "-1451.51"
        },
        {
            "id": "N06-1041",
            "name": "aria2006Prototype-Driven",
            "x": "2030.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1041",
            "name": "167",
            "x": "2030.6",
            "y": "-1361.77"
        },
        {
            "id": "P09-1011",
            "name": "percy2009Learning",
            "x": "1356.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1011",
            "name": "190",
            "x": "1356.6",
            "y": "-1092.55"
        },
        {
            "id": "H05-1010",
            "name": "ben2005A",
            "x": "2391.6",
            "y": "-1466.51"
        },
        {
            "id": "H05-1010",
            "name": "174",
            "x": "2391.6",
            "y": "-1451.51"
        },
        {
            "id": "N06-1015",
            "name": "simon2006Word",
            "x": "2497.6",
            "y": "-1376.77"
        },
        {
            "id": "N06-1015",
            "name": "75",
            "x": "2497.6",
            "y": "-1361.77"
        },
        {
            "id": "P07-1003",
            "name": "john2007Tailoring",
            "x": "1962.6",
            "y": "-1287.03"
        },
        {
            "id": "P07-1003",
            "name": "100",
            "x": "1962.6",
            "y": "-1272.03"
        },
        {
            "id": "P09-1104",
            "name": "aria2009Better",
            "x": "2430.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1104",
            "name": "91",
            "x": "2430.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1015",
            "name": "david2010Joint",
            "x": "1993.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1015",
            "name": "56",
            "x": "1993.6",
            "y": "-1002.81"
        },
        {
            "id": "N12-1004",
            "name": "david2012Fast",
            "x": "3171.6",
            "y": "-838.331"
        },
        {
            "id": "N12-1004",
            "name": "9",
            "x": "3171.6",
            "y": "-823.331"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "W06-3105",
            "name": "john2006Why",
            "x": "3312.6",
            "y": "-1376.77"
        },
        {
            "id": "W06-3105",
            "name": "71",
            "x": "3312.6",
            "y": "-1361.77"
        },
        {
            "id": "P08-2007",
            "name": "john2008The",
            "x": "3090.6",
            "y": "-1197.29"
        },
        {
            "id": "P08-2007",
            "name": "63",
            "x": "3090.6",
            "y": "-1182.29"
        },
        {
            "id": "D08-1033",
            "name": "john2008Sampling",
            "x": "3331.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1033",
            "name": "86",
            "x": "3331.6",
            "y": "-1182.29"
        },
        {
            "id": "N10-1014",
            "name": "adam2010Unsupervised",
            "x": "2226.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1014",
            "name": "18",
            "x": "2226.6",
            "y": "-1002.81"
        },
        {
            "id": "D07-1094",
            "name": "slav2007Learning",
            "x": "6041.6",
            "y": "-1287.03"
        },
        {
            "id": "D07-1094",
            "name": "13",
            "x": "6041.6",
            "y": "-1272.03"
        },
        {
            "id": "W10-2906",
            "name": "david2010Learning",
            "x": "4178.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-2906",
            "name": "33",
            "x": "4178.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-2054",
            "name": "aria2010An",
            "x": "5777.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-2054",
            "name": "8",
            "x": "5777.6",
            "y": "-1002.81"
        },
        {
            "id": "N10-1061",
            "name": "aria2010Coreference",
            "x": "5610.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1061",
            "name": "119",
            "x": "5610.6",
            "y": "-1002.81"
        },
        {
            "id": "N10-1082",
            "name": "percy2010Type-Based",
            "x": "2451.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1082",
            "name": "32",
            "x": "2451.6",
            "y": "-1002.81"
        },
        {
            "id": "W11-1916",
            "name": "jonathan2011Mention",
            "x": "6347.6",
            "y": "-928.071"
        },
        {
            "id": "W11-1916",
            "name": "10",
            "x": "6347.6",
            "y": "-913.071"
        },
        {
            "id": "P11-1060",
            "name": "percy2011Learning",
            "x": "1128.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1060",
            "name": "315",
            "x": "1128.6",
            "y": "-913.071"
        },
        {
            "id": "P11-1070",
            "name": "mohit2011Web-Scale",
            "x": "6022.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1070",
            "name": "41",
            "x": "6022.6",
            "y": "-913.071"
        },
        {
            "id": "P13-1012",
            "name": "greg2013Decentralized",
            "x": "5656.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1012",
            "name": "22",
            "x": "5656.6",
            "y": "-733.591"
        },
        {
            "id": "J13-2005",
            "name": "percy2013Learning",
            "x": "851.597",
            "y": "-748.591"
        },
        {
            "id": "J13-2005",
            "name": "54",
            "x": "851.597",
            "y": "-733.591"
        },
        {
            "id": "Q14-1037",
            "name": "greg2014A",
            "x": "5664.6",
            "y": "-658.851"
        },
        {
            "id": "Q14-1037",
            "name": "122",
            "x": "5664.6",
            "y": "-643.851"
        },
        {
            "id": "N15-1029",
            "name": "james2015Disfluency",
            "x": "6755.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1029",
            "name": "16",
            "x": "6755.6",
            "y": "-554.111"
        },
        {
            "id": "P16-1188",
            "name": "greg2016Learning-Based",
            "x": "5898.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1188",
            "name": "28",
            "x": "5898.6",
            "y": "-464.371"
        },
        {
            "id": "P06-1096",
            "name": "percy2006An",
            "x": "4419.6",
            "y": "-1376.77"
        },
        {
            "id": "P06-1096",
            "name": "244",
            "x": "4419.6",
            "y": "-1361.77"
        },
        {
            "id": "D08-1092",
            "name": "david2008Two",
            "x": "1950.6",
            "y": "-1197.29"
        },
        {
            "id": "D08-1092",
            "name": "90",
            "x": "1950.6",
            "y": "-1182.29"
        },
        {
            "id": "D12-1079",
            "name": "david2012Transforming",
            "x": "1876.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1079",
            "name": "13",
            "x": "1876.6",
            "y": "-823.331"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "P07-1107",
            "name": "aria2007Unsupervised",
            "x": "5654.6",
            "y": "-1287.03"
        },
        {
            "id": "P07-1107",
            "name": "127",
            "x": "5654.6",
            "y": "-1272.03"
        },
        {
            "id": "P09-2036",
            "name": "john2009Asynchronous",
            "x": "4045.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-2036",
            "name": "12",
            "x": "4045.6",
            "y": "-1092.55"
        },
        {
            "id": "W14-1607",
            "name": "jacob2014Grounding",
            "x": "923.597",
            "y": "-658.851"
        },
        {
            "id": "W14-1607",
            "name": "10",
            "x": "923.597",
            "y": "-643.851"
        },
        {
            "id": "P14-2133",
            "name": "jacob2014How",
            "x": "4231.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2133",
            "name": "36",
            "x": "4231.6",
            "y": "-643.851"
        },
        {
            "id": "P14-1020",
            "name": "david2014Sparser,",
            "x": "6382.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1020",
            "name": "16",
            "x": "6382.6",
            "y": "-643.851"
        },
        {
            "id": "P19-1031",
            "name": "daniel2019Cross-Domain",
            "x": "3080.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1031",
            "name": "0",
            "x": "3080.6",
            "y": "-195.15"
        },
        {
            "id": "N07-1052",
            "name": "aria2007Approximate",
            "x": "6273.6",
            "y": "-1287.03"
        },
        {
            "id": "N07-1052",
            "name": "6",
            "x": "6273.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1093",
            "name": "alexandre2007A",
            "x": "741.597",
            "y": "-1287.03"
        },
        {
            "id": "D07-1093",
            "name": "36",
            "x": "741.597",
            "y": "-1272.03"
        },
        {
            "id": "P10-1105",
            "name": "david2010Finding",
            "x": "905.597",
            "y": "-1017.81"
        },
        {
            "id": "P10-1105",
            "name": "23",
            "x": "905.597",
            "y": "-1002.81"
        },
        {
            "id": "D11-1029",
            "name": "taylor2011Simple",
            "x": "867.597",
            "y": "-928.071"
        },
        {
            "id": "D11-1029",
            "name": "11",
            "x": "867.597",
            "y": "-913.071"
        },
        {
            "id": "D11-1032",
            "name": "david2011Large-Scale",
            "x": "670.597",
            "y": "-928.071"
        },
        {
            "id": "D11-1032",
            "name": "6",
            "x": "670.597",
            "y": "-913.071"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "P08-1088",
            "name": "aria2008Learning",
            "x": "1135.6",
            "y": "-1197.29"
        },
        {
            "id": "P08-1088",
            "name": "259",
            "x": "1135.6",
            "y": "-1182.29"
        },
        {
            "id": "N09-1069",
            "name": "percy2009Online",
            "x": "337.597",
            "y": "-1107.55"
        },
        {
            "id": "N09-1069",
            "name": "157",
            "x": "337.597",
            "y": "-1092.55"
        },
        {
            "id": "D09-1147",
            "name": "adam2009Consensus",
            "x": "3713.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1147",
            "name": "25",
            "x": "3713.6",
            "y": "-1092.55"
        },
        {
            "id": "P13-1021",
            "name": "taylor2013Unsupervised",
            "x": "1227.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1021",
            "name": "22",
            "x": "1227.6",
            "y": "-733.591"
        },
        {
            "id": "P14-2020",
            "name": "taylor2014Improved",
            "x": "1242.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2020",
            "name": "13",
            "x": "1242.6",
            "y": "-643.851"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "D10-1040",
            "name": "dave2010A",
            "x": "1366.6",
            "y": "-1017.81"
        },
        {
            "id": "D10-1040",
            "name": "94",
            "x": "1366.6",
            "y": "-1002.81"
        },
        {
            "id": "D10-1049",
            "name": "gabor2010A",
            "x": "1501.6",
            "y": "-1017.81"
        },
        {
            "id": "D10-1049",
            "name": "121",
            "x": "1501.6",
            "y": "-1002.81"
        },
        {
            "id": "N09-1008",
            "name": "alexandre2009Improved",
            "x": "958.597",
            "y": "-1107.55"
        },
        {
            "id": "N09-1008",
            "name": "19",
            "x": "958.597",
            "y": "-1092.55"
        },
        {
            "id": "P12-1041",
            "name": "mohit2012Coreference",
            "x": "5952.6",
            "y": "-838.331"
        },
        {
            "id": "P12-1041",
            "name": "36",
            "x": "5952.6",
            "y": "-823.331"
        },
        {
            "id": "D13-1027",
            "name": "jonathan2013Error-Driven",
            "x": "6364.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1027",
            "name": "25",
            "x": "6364.6",
            "y": "-733.591"
        },
        {
            "id": "D13-1203",
            "name": "greg2013Easy",
            "x": "5956.6",
            "y": "-748.591"
        },
        {
            "id": "D13-1203",
            "name": "126",
            "x": "5956.6",
            "y": "-733.591"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "D16-1125",
            "name": "jacob2016Reasoning",
            "x": "1598.6",
            "y": "-479.371"
        },
        {
            "id": "D16-1125",
            "name": "36",
            "x": "1598.6",
            "y": "-464.371"
        },
        {
            "id": "D17-1015",
            "name": "nikita2017Where",
            "x": "1792.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1015",
            "name": "7",
            "x": "1792.6",
            "y": "-374.631"
        },
        {
            "id": "N18-1177",
            "name": "daniel2018Unified",
            "x": "910.597",
            "y": "-299.89"
        },
        {
            "id": "N18-1177",
            "name": "11",
            "x": "910.597",
            "y": "-284.89"
        },
        {
            "id": "N19-1410",
            "name": "sheng2019Pragmatically",
            "x": "1572.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1410",
            "name": "4",
            "x": "1572.6",
            "y": "-195.15"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "P11-1027",
            "name": "adam2011Faster",
            "x": "5517.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1027",
            "name": "103",
            "x": "5517.6",
            "y": "-913.071"
        },
        {
            "id": "D15-1138",
            "name": "jacob2015Alignment-Based",
            "x": "766.597",
            "y": "-569.111"
        },
        {
            "id": "D15-1138",
            "name": "17",
            "x": "766.597",
            "y": "-554.111"
        },
        {
            "id": "N16-1181",
            "name": "jacob2016Learning",
            "x": "1324.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1181",
            "name": "205",
            "x": "1324.6",
            "y": "-464.371"
        },
        {
            "id": "P17-1105",
            "name": "maxim2017Abstract",
            "x": "1291.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1105",
            "name": "58",
            "x": "1291.6",
            "y": "-374.631"
        },
        {
            "id": "P14-1098",
            "name": "mohit2014Structured",
            "x": "6184.6",
            "y": "-658.851"
        },
        {
            "id": "P14-1098",
            "name": "29",
            "x": "6184.6",
            "y": "-643.851"
        },
        {
            "id": "D13-1087",
            "name": "taylor2013Decipherment",
            "x": "601.597",
            "y": "-748.591"
        },
        {
            "id": "D13-1087",
            "name": "5",
            "x": "601.597",
            "y": "-733.591"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "P19-1340",
            "name": "nikita2019Multilingual",
            "x": "2853.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1340",
            "name": "9",
            "x": "2853.6",
            "y": "-195.15"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "N15-1109",
            "name": "dan2015Unsupervised",
            "x": "1159.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1109",
            "name": "7",
            "x": "1159.6",
            "y": "-554.111"
        },
        {
            "id": "D19-1225",
            "name": "nikita2019A",
            "x": "1102.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1225",
            "name": "1",
            "x": "1102.6",
            "y": "-195.15"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "N16-1150",
            "name": "matthew2016Capturing",
            "x": "5664.6",
            "y": "-479.371"
        },
        {
            "id": "N16-1150",
            "name": "4",
            "x": "5664.6",
            "y": "-464.371"
        },
        {
            "id": "P17-2052",
            "name": "maxim2017Fine-Grained",
            "x": "5121.6",
            "y": "-389.631"
        },
        {
            "id": "P17-2052",
            "name": "4",
            "x": "5121.6",
            "y": "-374.631"
        },
        {
            "id": "P18-1249",
            "name": "nikita2018Constituency",
            "x": "3128.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1249",
            "name": "22",
            "x": "3128.6",
            "y": "-284.89"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "2020.acl-main.557",
            "name": "nikita2020Tetra-Tagging:",
            "x": "2880.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.557",
            "name": "???",
            "x": "2880.6",
            "y": "-105.41"
        },
        {
            "id": "Q17-1031",
            "name": "jonathan2017Parsing",
            "x": "4902.6",
            "y": "-389.631"
        },
        {
            "id": "Q17-1031",
            "name": "4",
            "x": "4902.6",
            "y": "-374.631"
        },
        {
            "id": "P19-1655",
            "name": "ronghang2019Are",
            "x": "843.597",
            "y": "-210.15"
        },
        {
            "id": "P19-1655",
            "name": "4",
            "x": "843.597",
            "y": "-195.15"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "P17-1022",
            "name": "jacob2017Translating",
            "x": "1600.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1022",
            "name": "???",
            "x": "1600.6",
            "y": "-374.631"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "P17-2025",
            "name": "daniel2017Improving",
            "x": "2982.6",
            "y": "-389.631"
        },
        {
            "id": "P17-2025",
            "name": "9",
            "x": "2982.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1178",
            "name": "mitchell2017Effective",
            "x": "3229.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1178",
            "name": "9",
            "x": "3229.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1311",
            "name": "jacob2017Analogs",
            "x": "5332.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1311",
            "name": "3",
            "x": "5332.6",
            "y": "-374.631"
        },
        {
            "id": "2020.acl-main.208",
            "name": "ruiqi2020Semantic",
            "x": "1291.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.208",
            "name": "1",
            "x": "1291.6",
            "y": "-105.41"
        },
        {
            "id": "P19-1188",
            "name": "david2019Pre-Learning",
            "x": "5275.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1188",
            "name": "1",
            "x": "5275.6",
            "y": "-195.15"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "N18-1197",
            "name": "jacob2018Learning",
            "x": "5218.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1197",
            "name": "8",
            "x": "5218.6",
            "y": "-284.89"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "2021.naacl-main.81",
            "name": "rodolfo2021Modular",
            "x": "843.597",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.81",
            "name": "???",
            "x": "843.597",
            "y": "-15.6701"
        },
        {
            "id": "2021",
            "name": "2021",
            "x": "28.5975",
            "y": "-23.1701"
        }
    ],
    [
        "28.5975,-1749.93 28.5975,-1749.93 28.5975,-1749.93 28.5975,-1749.93",
        "6872.23,-1674.58 6863.71,-1668.29 6866.45,-1678.52 6872.23,-1674.58",
        "6847.34,-1137.01 6839.21,-1130.22 6841.33,-1140.6 6847.34,-1137.01",
        "6940.62,-1042.7 6930.66,-1039.09 6936.2,-1048.13 6940.62,-1042.7",
        "447.098,-1768.94 443.597,-1758.94 440.098,-1768.94 447.098,-1768.94",
        "6883.31,-1825.17 6893.31,-1821.67 6883.31,-1818.17 6883.31,-1825.17",
        "4975.96,-1647.78 4965.68,-1650.34 4975.31,-1654.75 4975.96,-1647.78",
        "6840.98,-1679.58 6840.05,-1669.03 6834.2,-1677.86 6840.98,-1679.58",
        "28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19 28.5975,-1660.19",
        "1551.1,-1679.2 1547.6,-1669.2 1544.1,-1679.2 1551.1,-1679.2",
        "447.098,-1589.46 443.597,-1579.46 440.098,-1589.46 447.098,-1589.46",
        "275.957,-1408.91 270.242,-1399.99 269.146,-1410.53 275.957,-1408.91",
        "2524.58,-123.33 2534.51,-119.627 2524.44,-116.331 2524.58,-123.33",
        "28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45",
        "3399.69,-1563.61 3389.31,-1565.72 3398.74,-1570.54 3399.69,-1563.61",
        "5322.12,-1499.98 5322.21,-1489.39 5315.53,-1497.61 5322.12,-1499.98",
        "5412.86,-1410.37 5409.84,-1400.21 5405.87,-1410.03 5412.86,-1410.37",
        "5169.38,-1410.06 5171.42,-1399.66 5163.34,-1406.52 5169.38,-1410.06",
        "4270.56,-1315.29 4261.52,-1309.75 4265.13,-1319.72 4270.56,-1315.29",
        "4847.98,-1319.52 4843.15,-1310.09 4841.04,-1320.48 4847.98,-1319.52",
        "5011.35,-1230.42 5007.27,-1220.64 5004.36,-1230.83 5011.35,-1230.42",
        "4564.59,-1224.87 4555.61,-1219.23 4559.12,-1229.23 4564.59,-1224.87",
        "6830.45,-1140.11 6825.68,-1130.65 6823.51,-1141.02 6830.45,-1140.11",
        "2257.14,-1141.29 2256.45,-1130.72 2250.4,-1139.42 2257.14,-1141.29",
        "6132.73,-1125.03 6122.14,-1124.92 6130.36,-1131.61 6132.73,-1125.03",
        "6324.09,-1113.32 6313.62,-1114.94 6322.81,-1120.2 6324.09,-1113.32",
        "4747.25,-1043.15 4737.39,-1039.27 4742.67,-1048.46 4747.25,-1043.15",
        "4845.89,-936.074 4835.41,-937.687 4844.61,-942.956 4845.89,-936.074",
        "3350.71,-871.717 3348.22,-861.419 3343.74,-871.022 3350.71,-871.717",
        "5431.3,-871.598 5427.41,-861.741 5424.3,-871.869 5431.3,-871.598",
        "2008.99,-859.674 2016.85,-852.571 2006.27,-853.221 2008.99,-859.674",
        "5654.15,-868.808 5646.72,-861.257 5647.82,-871.795 5654.15,-868.808",
        "3800.85,-864.885 3808.19,-857.246 3797.69,-858.639 3800.85,-864.885",
        "4921.3,-754.042 4931.09,-749.989 4920.91,-747.053 4921.3,-754.042",
        "6563.55,-780.536 6557.68,-771.718 6556.77,-782.274 6563.55,-780.536",
        "3773.8,-662.716 3783.65,-658.793 3773.51,-655.722 3773.8,-662.716",
        "4099.8,-572.833 4109.63,-568.874 4099.47,-565.84 4099.8,-572.833",
        "2719.85,-423.48 2719.62,-412.888 2713.2,-421.31 2719.85,-423.48",
        "2474.95,-318.901 2483.78,-313.042 2473.23,-312.118 2474.95,-318.901",
        "3895.04,-1219.88 3884.58,-1218.18 3891.7,-1226.03 3895.04,-1219.88",
        "6835.24,-1139.4 6829.26,-1130.65 6828.48,-1141.22 6835.24,-1139.4",
        "6141.81,-1121.43 6131.28,-1122.62 6140.25,-1128.25 6141.81,-1121.43",
        "6928.05,-1045.83 6919.02,-1040.29 6922.62,-1050.25 6928.05,-1045.83",
        "6174.62,-1012.1 6164.58,-1015.5 6174.55,-1019.1 6174.62,-1012.1",
        "3064.35,-1038.69 3053.81,-1037.63 3061.39,-1045.03 3064.35,-1038.69",
        "3206.2,-1562.01 3216.01,-1558.02 3205.85,-1555.02 3206.2,-1562.01",
        "3773.71,-662.481 3783.56,-658.584 3773.43,-655.487 3773.71,-662.481",
        "2685.13,-416.533 2692.02,-408.481 2681.62,-410.481 2685.13,-416.533",
        "2848.4,-333.504 2847.58,-322.941 2841.64,-331.711 2848.4,-333.504",
        "2454.05,-301.61 2463.98,-297.912 2453.91,-294.612 2454.05,-301.61",
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "4494.52,-1226.27 4500.83,-1217.77 4490.59,-1220.48 4494.52,-1226.27",
        "4988.86,-960.683 4991.9,-950.534 4983.19,-956.573 4988.86,-960.683",
        "3777,-667.836 3786.55,-663.248 3776.22,-660.88 3777,-667.836",
        "4967.66,-583.238 4976.91,-578.075 4966.46,-576.342 4967.66,-583.238",
        "299.59,-1403.74 290.025,-1399.18 294.662,-1408.71 299.59,-1403.74",
        "1742.82,-1401.3 1751.06,-1394.64 1740.47,-1394.7 1742.82,-1401.3",
        "373.188,-1229.32 367.538,-1220.35 366.366,-1230.88 373.188,-1229.32",
        "622.884,-1051.39 621.489,-1040.88 616.03,-1049.96 622.884,-1051.39",
        "443.102,-1047.76 435.364,-1040.52 436.9,-1051.01 443.102,-1047.76",
        "306.988,-856.747 315.625,-850.611 305.047,-850.021 306.988,-856.747",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "1941.96,-1397.5 1951.09,-1392.12 1940.6,-1390.63 1941.96,-1397.5",
        "402.286,-1222.2 392.198,-1218.96 398.066,-1227.79 402.286,-1222.2",
        "1354.35,-1141.04 1352.47,-1130.61 1347.43,-1139.94 1354.35,-1141.04",
        "1882.99,-1390.82 1872.45,-1391.91 1881.38,-1397.63 1882.99,-1390.82",
        "2463.15,-1407.03 2468.62,-1397.95 2458.69,-1401.63 2463.15,-1407.03",
        "2033.83,-1301.25 2023.25,-1301.68 2031.79,-1307.95 2033.83,-1301.25",
        "2430.17,-1140.93 2427.77,-1130.61 2423.21,-1140.17 2430.17,-1140.93",
        "2983.26,-1050.94 2986.15,-1040.75 2977.54,-1046.91 2983.26,-1050.94",
        "2011.02,-1048.51 2003.83,-1040.72 2004.6,-1051.29 2011.02,-1048.51",
        "3137.19,-868.137 3142.88,-859.201 3132.86,-862.637 3137.19,-868.137",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "3149.67,-1210.88 3139.07,-1210.97 3147.41,-1217.51 3149.67,-1210.88",
        "3331.23,-1230.81 3328.81,-1220.5 3324.27,-1230.07 3331.23,-1230.81",
        "3005.6,-1051.08 3002.42,-1040.97 2998.61,-1050.85 3005.6,-1051.08",
        "2314.43,-1031.26 2303.9,-1032.49 2312.9,-1038.09 2314.43,-1031.26",
        "3209.47,-863.358 3199.68,-859.305 3204.81,-868.578 3209.47,-863.358",
        "4332.72,-1289.19 4322.43,-1291.75 4332.07,-1296.16 4332.72,-1289.19",
        "4906.61,-1297.62 4896.06,-1298.55 4904.89,-1304.4 4906.61,-1297.62",
        "5975.52,-1307.9 5984.36,-1302.06 5973.81,-1301.11 5975.52,-1307.9",
        "5030.43,-1226.04 5021.89,-1219.76 5024.65,-1229.99 5030.43,-1226.04",
        "3931.3,-1210.41 3920.78,-1211.65 3929.78,-1217.24 3931.3,-1210.41",
        "4615.13,-1198.2 4604.83,-1200.64 4614.41,-1205.16 4615.13,-1198.2",
        "6806.42,-1140.73 6808.3,-1130.31 6800.33,-1137.29 6806.42,-1140.73",
        "2332.84,-1121.85 2322.27,-1122.59 2330.99,-1128.61 2332.84,-1121.85",
        "6039.73,-1140.57 6042.63,-1130.38 6034.01,-1136.54 6039.73,-1140.57",
        "4269.2,-1021.56 4258.87,-1023.9 4268.41,-1028.51 4269.2,-1021.56",
        "6809.01,-1036.67 6817.42,-1030.23 6806.83,-1030.02 6809.01,-1036.67",
        "5784.85,-1050.61 5780.41,-1040.99 5777.88,-1051.28 5784.85,-1050.61",
        "6001.4,-1043.73 6008.7,-1036.05 5998.2,-1037.5 6001.4,-1043.73",
        "4796.5,-1020.63 4786.09,-1022.63 4795.48,-1027.55 4796.5,-1020.63",
        "5602.04,-1051.55 5602.11,-1040.96 5595.45,-1049.19 5602.04,-1051.55",
        "2557.38,-1020.6 2547.07,-1023.04 2556.65,-1027.56 2557.38,-1020.6",
        "6372.88,-957.548 6364.62,-950.916 6366.94,-961.253 6372.88,-957.548",
        "4838.06,-942.067 4827.46,-941.94 4835.67,-948.646 4838.06,-942.067",
        "1225.55,-922.851 1215.51,-926.234 1225.47,-929.85 1225.55,-922.851",
        "6114.82,-937.604 6104.29,-938.77 6113.25,-944.426 6114.82,-937.604",
        "5413.08,-871.822 5413.95,-861.262 5406.69,-868.981 5413.08,-871.822",
        "476.591,-836.387 466.419,-839.352 476.22,-843.377 476.591,-836.387",
        "2139.63,-837.384 2129.39,-840.126 2139.1,-844.364 2139.63,-837.384",
        "5645.33,-870.55 5639.75,-861.539 5638.49,-872.058 5645.33,-870.55",
        "3949.27,-834.505 3939.18,-837.73 3949.08,-841.503 3949.27,-834.505",
        "5082.44,-757.41 5071.85,-757.869 5080.41,-764.111 5082.44,-757.41",
        "5547.87,-754.556 5557.58,-750.321 5547.35,-747.576 5547.87,-754.556",
        "941.73,-753.173 931.401,-755.535 940.948,-760.129 941.73,-753.173",
        "6553.77,-781.982 6550.36,-771.953 6546.77,-781.924 6553.77,-781.982",
        "5601.86,-665.421 5611.33,-660.656 5600.96,-658.48 5601.86,-665.421",
        "3931.84,-662.787 3921.46,-664.917 3930.9,-669.724 3931.84,-662.787",
        "6732.96,-601.944 6735.82,-591.743 6727.22,-597.932 6732.96,-601.944",
        "6016.67,-477.374 6006.51,-480.362 6016.32,-484.365 6016.67,-477.374",
        "3148.1,-1211.14 3137.51,-1211.44 3145.98,-1217.81 3148.1,-1211.14",
        "4972.37,-957.844 4978.23,-949.017 4968.15,-952.263 4972.37,-957.844",
        "436.401,-1049.73 430.359,-1041.03 429.655,-1051.6 436.401,-1049.73",
        "326.252,-863.74 333.677,-856.182 323.16,-857.459 326.252,-863.74",
        "2425.32,-1394.46 2434.31,-1388.85 2423.78,-1387.63 2425.32,-1394.46",
        "1915.36,-1314.82 1922.36,-1306.86 1911.93,-1308.72 1915.36,-1314.82",
        "1909.31,-1225.77 1915.29,-1217.03 1905.16,-1220.13 1909.31,-1225.77",
        "1402.78,-1131.14 1392.5,-1128.57 1398.93,-1136.99 1402.78,-1131.14",
        "2378.06,-1131.26 2385.72,-1123.94 2375.17,-1124.89 2378.06,-1131.26",
        "2936.39,-1042.98 2944.57,-1036.25 2933.97,-1036.41 2936.39,-1042.98",
        "2153.32,-1042.59 2160.84,-1035.12 2150.31,-1036.27 2153.32,-1042.59",
        "1936.45,-1040.9 1943.82,-1033.29 1933.31,-1034.64 1936.45,-1040.9",
        "464.883,-1042.16 454.873,-1038.69 460.532,-1047.64 464.883,-1042.16",
        "3121.5,-863.411 3129.01,-855.936 3118.48,-857.098 3121.5,-863.411",
        "351.224,-869.644 356.022,-860.198 346.38,-864.59 351.224,-869.644",
        "1857.26,-871.432 1859.66,-861.112 1851.35,-867.684 1857.26,-871.432",
        "2031.28,-868.038 2036.63,-858.894 2026.74,-862.706 2031.28,-868.038",
        "2442.9,-1139.42 2437.07,-1130.57 2436.11,-1141.12 2442.9,-1139.42",
        "2962.79,-1048.08 2968.94,-1039.46 2958.75,-1042.36 2962.79,-1048.08",
        "5528.14,-1036.03 5537.19,-1030.53 5526.68,-1029.19 5528.14,-1036.03",
        "2369.03,-1038.2 2377.97,-1032.53 2367.44,-1031.38 2369.03,-1038.2",
        "478.399,-1038.28 467.958,-1036.48 474.995,-1044.4 478.399,-1038.28",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "1958.97,-1229.89 1954.15,-1220.46 1952.04,-1230.84 1958.97,-1229.89",
        "2373.77,-1129.34 2381.94,-1122.6 2371.35,-1122.78 2373.77,-1129.34",
        "2923.48,-1039.44 2932.28,-1033.54 2921.72,-1032.67 2923.48,-1039.44",
        "2134.13,-1036.33 2143.07,-1030.66 2132.54,-1029.51 2134.13,-1036.33",
        "1959.91,-1048.16 1964.91,-1038.82 1955.18,-1043 1959.91,-1048.16",
        "493.603,-1031.51 483.033,-1032.23 491.749,-1038.26 493.603,-1031.51",
        "1866.65,-872.131 1866.63,-861.536 1860.04,-869.828 1866.65,-872.131",
        "3423.13,-1197.85 3412.91,-1200.62 3422.63,-1204.84 3423.13,-1197.85",
        "6187.27,-1128.44 6195.73,-1122.06 6185.14,-1121.77 6187.27,-1128.44",
        "5619.99,-1050.52 5614.91,-1041.23 5613.09,-1051.66 5619.99,-1050.52",
        "4942.02,-1217.98 4950.78,-1212.02 4940.22,-1211.22 4942.02,-1217.98",
        "3931.16,-1210.35 3920.65,-1211.67 3929.69,-1217.19 3931.16,-1210.35",
        "4469.41,-1217.43 4477.98,-1211.2 4467.39,-1210.73 4469.41,-1217.43",
        "2031.52,-1194.51 2021.38,-1197.57 2031.22,-1201.5 2031.52,-1194.51",
        "4062.8,-1138.41 4055.56,-1130.68 4056.4,-1141.24 4062.8,-1138.41",
        "2331.7,-1122.32 2321.14,-1123.17 2329.93,-1129.09 2331.7,-1122.32",
        "5952.4,-1110.85 5962.3,-1107.09 5952.22,-1103.85 5952.4,-1110.85",
        "4215.48,-1044.98 4206.26,-1039.75 4210.2,-1049.58 4215.48,-1044.98",
        "4677.89,-1049.45 4682.33,-1039.83 4672.86,-1044.58 4677.89,-1049.45",
        "2025.46,-1044.21 2015.94,-1039.56 2020.49,-1049.13 2025.46,-1044.21",
        "4712.64,-947.265 4721.02,-940.782 4710.43,-940.624 4712.64,-947.265",
        "3387.71,-864.701 3377.96,-860.554 3383,-869.876 3387.71,-864.701",
        "1924.78,-862.282 1914.52,-859.642 1920.9,-868.105 1924.78,-862.282",
        "5635.77,-871.728 5632.66,-861.601 5628.78,-871.459 5635.77,-871.728",
        "3881.15,-867.16 3872.41,-861.163 3875.5,-871.298 3881.15,-867.16",
        "4953.4,-773.925 4960.96,-766.506 4950.42,-767.588 4953.4,-773.925",
        "6542.32,-781.959 6542.2,-771.365 6535.69,-779.718 6542.32,-781.959",
        "967.312,-684.043 957.343,-680.455 962.899,-689.476 967.312,-684.043",
        "4265.08,-685.539 4255.82,-680.395 4259.85,-690.193 4265.08,-685.539",
        "6315.51,-681.289 6323.72,-674.596 6313.13,-674.706 6315.51,-681.289",
        "3916.82,-674.355 3906.24,-673.772 3914.14,-680.825 3916.82,-674.355",
        "4164.99,-602.265 4166.49,-591.776 4158.78,-599.044 4164.99,-602.265",
        "2785.42,-406.447 2774.84,-405.814 2782.72,-412.904 2785.42,-406.447",
        "2569.37,-331.144 2562.51,-323.069 2562.83,-333.658 2569.37,-331.144",
        "3193.29,-214.638 3182.99,-217.123 3192.59,-221.603 3193.29,-214.638",
        "3955.46,-1196.5 3945.31,-1199.52 3955.13,-1203.5 3955.46,-1196.5",
        "6756.82,-1129.41 6765.25,-1123 6754.66,-1122.75 6756.82,-1129.41",
        "6100.23,-1133.22 6090.31,-1129.51 6095.75,-1138.6 6100.23,-1133.22",
        "4600.87,-1210.14 4590.3,-1210.75 4598.94,-1216.87 4600.87,-1210.14",
        "871.993,-1048.82 876.817,-1039.39 867.163,-1043.75 871.993,-1048.82",
        "641.939,-1049.23 635.41,-1040.89 635.311,-1051.49 641.939,-1049.23",
        "939.043,-942.06 928.449,-941.947 936.66,-948.642 939.043,-942.06",
        "717.408,-953.346 707.586,-949.374 712.787,-958.604 717.408,-953.346",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "3237.15,-1196.99 3247.15,-1193.49 3237.15,-1189.99 3237.15,-1196.99",
        "2502.05,-1113.98 2491.6,-1115.76 2500.88,-1120.88 2502.05,-1113.98",
        "3024.26,-1048.23 3016.65,-1040.86 3018,-1051.37 3024.26,-1048.23",
        "938.284,-941.292 927.716,-942.043 936.446,-948.046 938.284,-941.292",
        "350.715,-1139.44 344.701,-1130.72 343.963,-1141.29 350.715,-1139.44",
        "2507.56,-1106.03 2497.36,-1108.9 2507.12,-1113.01 2507.56,-1106.03",
        "2331.17,-1122.44 2320.61,-1123.33 2329.42,-1129.22 2331.17,-1122.44",
        "3759.13,-1131.49 3748.89,-1128.76 3755.19,-1137.28 3759.13,-1131.49",
        "4617.05,-1024.06 4626.82,-1019.96 4616.62,-1017.07 4617.05,-1024.06",
        "1342.57,-747.811 1332.41,-750.839 1342.24,-754.803 1342.57,-747.811",
        "1340.74,-660.622 1330.49,-663.295 1340.17,-667.598 1340.74,-660.622",
        "1453.02,-1104.24 1442.91,-1107.43 1452.8,-1111.23 1453.02,-1104.24",
        "3041.45,-1043.74 3031.56,-1039.93 3036.92,-1049.07 3041.45,-1043.74",
        "3194.74,-867.709 3186.74,-860.766 3188.66,-871.184 3194.74,-867.709",
        "4699.2,-1051.79 4699.4,-1041.2 4692.64,-1049.36 4699.2,-1051.79",
        "3923.65,-853.973 3913.06,-854.207 3921.49,-860.63 3923.65,-853.973",
        "3929.86,-665.46 3919.35,-666.848 3928.43,-672.313 3929.86,-665.46",
        "4248.75,-583.564 4238.17,-583 4246.09,-590.038 4248.75,-583.564",
        "4086.01,-1025.9 4095.7,-1021.61 4085.46,-1018.92 4086.01,-1025.9",
        "690.35,-1037.94 679.789,-1037.09 687.519,-1044.34 690.35,-1037.94",
        "1968.99,-1049.9 1971.9,-1039.72 1963.27,-1045.86 1968.99,-1049.9",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "1365.98,-1051.35 1363.63,-1041.02 1359.02,-1050.55 1365.98,-1051.35",
        "1459.68,-1044.47 1466.42,-1036.3 1456.05,-1038.48 1459.68,-1044.47",
        "1166.62,-953.798 1156.76,-949.921 1162.05,-959.1 1166.62,-953.798",
        "864.462,-780.562 858.187,-772.025 857.769,-782.612 864.462,-780.562",
        "2906.82,-1033.37 2916.18,-1028.4 2905.76,-1026.45 2906.82,-1033.37",
        "2289.4,-1038.21 2278.83,-1037.48 2286.64,-1044.64 2289.4,-1038.21",
        "2063.68,-1027.63 2053.16,-1028.91 2062.18,-1034.46 2063.68,-1027.63",
        "489.585,-1032.49 479.039,-1033.51 487.916,-1039.29 489.585,-1032.49",
        "3112.62,-859.503 3120.93,-852.935 3110.34,-852.885 3112.62,-859.503",
        "2136.36,-843.135 2125.91,-844.922 2135.19,-850.037 2136.36,-843.135",
        "6869.82,-1050.26 6873.53,-1040.33 6864.44,-1045.78 6869.82,-1050.26",
        "929.267,-1047.43 921.101,-1040.68 923.276,-1051.05 929.267,-1047.43",
        "942.67,-940.878 932.087,-940.379 940.049,-947.369 942.67,-940.878",
        "726.028,-950.419 715.631,-948.378 722.485,-956.457 726.028,-950.419",
        "3984.02,-1133.77 3992.17,-1127 3981.58,-1127.21 3984.02,-1133.77",
        "2352.65,-1107.25 2362.65,-1103.75 2352.65,-1100.25 2352.65,-1107.25",
        "2245.03,-1048.77 2237.84,-1040.98 2238.61,-1051.55 2245.03,-1048.77",
        "2395.58,-1045.16 2403.15,-1037.74 2392.61,-1038.82 2395.58,-1045.16",
        "6771.69,-1134.23 6779.3,-1126.86 6768.76,-1127.87 6771.69,-1134.23",
        "6067.04,-1051.2 6064.12,-1041.02 6060.05,-1050.8 6067.04,-1051.2",
        "392.647,-1049.65 396.976,-1039.98 387.561,-1044.84 392.647,-1049.65",
        "1044.52,-943.043 1053.86,-938.046 1043.44,-936.126 1044.52,-943.043",
        "772.428,-766.085 781.591,-760.767 771.112,-759.21 772.428,-766.085",
        "5685.71,-1032.17 5675.17,-1033.31 5684.11,-1038.99 5685.71,-1032.17",
        "5929.5,-871.258 5932.15,-860.999 5923.68,-867.368 5929.5,-871.258",
        "6399.34,-776.313 6390.06,-771.213 6394.14,-780.991 6399.34,-776.313",
        "6021.14,-761.049 6010.55,-761.003 6018.8,-767.646 6021.14,-761.049",
        "5715.22,-674.256 5704.67,-673.279 5712.3,-680.622 5715.22,-674.256",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "885.899,-958.722 878.713,-950.937 879.476,-961.504 885.899,-958.722",
        "737.857,-946.647 727.264,-946.428 735.408,-953.205 737.857,-946.647",
        "4752.85,-960.337 4756.21,-950.289 4747.32,-956.049 4752.85,-960.337",
        "426.538,-862.559 416.398,-859.488 422.409,-868.212 426.538,-862.559",
        "3191.21,-869.045 3184.31,-861.011 3184.7,-871.599 3191.21,-869.045",
        "2110.26,-1017.51 2120.26,-1014.01 2110.26,-1010.51 2110.26,-1017.51",
        "1901.95,-867.623 1893.52,-861.2 1896.1,-871.476 1901.95,-867.623",
        "3762.69,-845.458 3772.42,-841.263 3762.2,-838.475 3762.69,-845.458",
        "5710.95,-1017.51 5720.95,-1014.01 5710.95,-1010.51 5710.95,-1017.51",
        "6254.99,-942.772 6264.4,-937.9 6254,-935.841 6254.99,-942.772",
        "5919.73,-869.616 5925.03,-860.447 5915.17,-864.304 5919.73,-869.616",
        "5706.11,-773.431 5696.18,-769.732 5701.63,-778.815 5706.11,-773.431",
        "6009.96,-767.346 5999.5,-765.686 6006.64,-773.509 6009.96,-767.346",
        "5713.46,-675.812 5702.97,-674.308 5710.23,-682.024 5713.46,-675.812",
        "512.996,-1017.51 522.996,-1014.01 512.995,-1010.51 512.996,-1017.51",
        "1117.54,-758.89 1127.09,-754.315 1116.77,-751.932 1117.54,-758.89",
        "980.357,-680.412 969.911,-678.644 976.975,-686.541 980.357,-680.412",
        "1574.71,-511.969 1577.83,-501.844 1569.08,-507.813 1574.71,-511.969",
        "1776.71,-423.01 1778.43,-412.556 1770.56,-419.653 1776.71,-423.01",
        "926.568,-331.3 919.884,-323.079 919.983,-333.674 926.568,-331.3",
        "1544.54,-242.55 1548.37,-232.674 1539.22,-238.002 1544.54,-242.55",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "6364.64,-782.085 6362.11,-771.797 6357.67,-781.418 6364.64,-782.085",
        "5462.39,-865.178 5452.76,-860.757 5457.53,-870.217 5462.39,-865.178",
        "5064.77,-599.534 5057.32,-592.002 5058.45,-602.536 5064.77,-599.534",
        "5810.23,-499.841 5819.23,-494.238 5808.7,-493.011 5810.23,-499.841",
        "879.75,-776.587 870.72,-771.046 874.324,-781.009 879.75,-776.587",
        "948.604,-688.226 940.186,-681.792 942.754,-692.071 948.604,-688.226",
        "757.17,-602.888 756.869,-592.298 750.5,-600.765 757.17,-602.888",
        "1351.63,-508.276 1343.19,-501.868 1345.79,-512.139 1351.63,-508.276",
        "1253.59,-420.36 1258.39,-410.915 1248.75,-415.305 1253.59,-420.36",
        "5982.01,-866.79 5973.04,-861.156 5976.54,-871.156 5982.01,-866.79",
        "477.554,-834.099 467.48,-837.38 477.401,-841.097 477.554,-834.099",
        "6167.69,-692.255 6169.69,-681.85 6161.64,-688.739 6167.69,-692.255",
        "1172.99,-776.613 1180.41,-769.052 1169.9,-770.335 1172.99,-776.613",
        "648.348,-772.776 638.087,-770.137 644.463,-778.599 648.348,-772.776",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "5732.94,-764.771 5722.36,-765.285 5730.95,-771.482 5732.94,-764.771",
        "5958.45,-781.944 5955.41,-771.795 5951.46,-781.625 5958.45,-781.944",
        "5724.82,-665.209 5714.26,-666.088 5723.06,-671.985 5724.82,-665.209",
        "3781.38,-858.088 3790.24,-852.275 3779.69,-851.295 3781.38,-858.088",
        "5548.41,-755.082 5558.21,-751.068 5548.05,-748.091 5548.41,-755.082",
        "5601.48,-664.706 5611.17,-660.406 5600.92,-657.729 5601.48,-664.706",
        "2750.28,-217.623 2759.97,-213.337 2749.73,-210.645 2750.28,-217.623",
        "5089.52,-748.336 5079.27,-751.033 5088.96,-755.314 5089.52,-748.336",
        "6254.5,-762.234 6264.03,-757.59 6253.68,-755.282 6254.5,-762.234",
        "3858.1,-692.057 3854.6,-682.057 3851.1,-692.057 3858.1,-692.057",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "5879.09,-748.291 5889.09,-744.791 5879.09,-741.291 5879.09,-748.291",
        "5664.8,-692.331 5662.23,-682.055 5657.83,-691.696 5664.8,-692.331",
        "1239.92,-692.466 1238.16,-682.02 1233.02,-691.287 1239.92,-692.466",
        "1146.79,-602.665 1147.08,-592.074 1140.25,-600.171 1146.79,-602.665",
        "1083.63,-242.651 1085.83,-232.287 1077.65,-239.016 1083.63,-242.651",
        "899.181,-691.323 902.819,-681.372 893.77,-686.882 899.181,-691.323",
        "770.553,-602.416 766.514,-592.621 763.563,-602.797 770.553,-602.416",
        "1241.6,-417.457 1248.79,-409.675 1238.31,-411.275 1241.6,-417.457",
        "6436,-679.993 6425.52,-678.466 6432.76,-686.198 6436,-679.993",
        "6240.49,-748.291 6250.49,-744.791 6240.49,-741.291 6240.49,-748.291",
        "5721.79,-668.776 5711.21,-669.371 5719.85,-675.503 5721.79,-668.776",
        "5128.59,-566.507 5118.44,-569.531 5128.26,-573.499 5128.59,-566.507",
        "5909.83,-511.815 5904.29,-502.785 5902.99,-513.3 5909.83,-511.815",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "820.261,-592.291 809.824,-590.468 816.846,-598.402 820.261,-592.291",
        "5109.5,-582.974 5098.94,-583.758 5107.69,-589.734 5109.5,-582.974",
        "5860.2,-510.085 5866.05,-501.253 5855.97,-504.507 5860.2,-510.085",
        "5668.1,-512.577 5664.6,-502.577 5661.1,-512.577 5668.1,-512.577",
        "5179.46,-411.406 5168.94,-410.13 5176.37,-417.686 5179.46,-411.406",
        "1192.97,-596.491 1183.55,-591.632 1187.88,-601.302 1192.97,-596.491",
        "1108.69,-243.069 1104.26,-233.442 1101.72,-243.727 1108.69,-243.069",
        "4204.84,-598.96 4196.91,-591.936 4198.73,-602.373 4204.84,-598.96",
        "5968.22,-498.373 5957.63,-498.216 5965.81,-504.945 5968.22,-498.373",
        "4115.71,-587.826 4124.47,-581.861 4113.9,-581.064 4115.71,-587.826",
        "4959.8,-575.136 4969.56,-571.003 4959.35,-568.15 4959.8,-575.136",
        "3120.1,-333.72 3119.25,-323.159 3113.33,-331.944 3120.1,-333.72",
        "2585.65,-327.24 2576.37,-322.117 2580.43,-331.905 2585.65,-327.24",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "4957.85,-568.811 4967.85,-565.311 4957.85,-561.811 4957.85,-568.811",
        "3211.17,-314.415 3200.62,-315.35 3209.45,-321.2 3211.17,-314.415",
        "2999.78,-115.862 2989.71,-119.173 2999.64,-122.861 2999.78,-115.862",
        "5812.31,-500.098 5821.45,-494.747 5810.97,-493.227 5812.31,-500.098",
        "4931.82,-417.983 4922.86,-412.33 4926.34,-422.337 4931.82,-417.983",
        "5109.42,-423.232 5110.24,-412.669 5103.01,-420.415 5109.42,-423.232",
        "831.888,-243.75 832.305,-233.163 825.377,-241.179 831.888,-243.75",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "1603.28,-422.912 1600,-412.835 1596.28,-422.752 1603.28,-422.912",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "3123.14,-389.331 3133.14,-385.831 3123.14,-382.331 3123.14,-389.331",
        "2898.15,-323.16 2887.91,-320.43 2894.21,-328.948 2898.15,-323.16",
        "3082.09,-329.124 3088.86,-320.973 3078.48,-323.125 3082.09,-329.124",
        "3043.61,-241.738 3048.5,-232.341 3038.82,-236.636 3043.61,-241.738",
        "2907.08,-233.396 2896.84,-230.662 2903.14,-239.182 2907.08,-233.396",
        "5282.18,-416.534 5289.8,-409.173 5279.26,-410.175 5282.18,-416.534",
        "3172.39,-415.925 3180.37,-408.955 3169.78,-409.427 3172.39,-415.925",
        "2813.76,-329.328 2819.94,-320.718 2809.74,-323.598 2813.76,-329.328",
        "3042.29,-319.604 3051.31,-314.043 3040.79,-312.767 3042.29,-319.604",
        "2608.87,-320.886 2598.36,-319.533 2605.73,-327.144 2608.87,-320.886",
        "2990.56,-230.449 2999.63,-224.969 2989.12,-223.598 2990.56,-230.449",
        "2653.36,-152.698 2648.07,-143.517 2646.48,-153.992 2653.36,-152.698",
        "2800,-144.071 2807.72,-136.814 2797.16,-137.673 2800,-144.071",
        "1295.1,-153.825 1291.6,-143.825 1288.1,-153.825 1295.1,-153.825",
        "3167.33,-326.104 3157.49,-322.188 3162.74,-331.389 3167.33,-326.104",
        "5302.73,-239.585 5294.44,-232.986 5296.81,-243.314 5302.73,-239.585",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "2857.1,-243.354 2853.6,-233.354 2850.1,-243.354 2857.1,-243.354",
        "3102.51,-240.109 3094.64,-233.016 3096.37,-243.469 3102.51,-240.109",
        "2926.36,-226.891 2915.77,-227.186 2924.23,-233.56 2926.36,-226.891",
        "2730.62,-134.498 2720.12,-135.95 2729.23,-141.36 2730.62,-134.498",
        "2764.83,-299.591 2774.83,-296.09 2764.83,-292.591 2764.83,-299.591",
        "3067.92,-326.151 3076.04,-319.341 3065.45,-319.603 3067.92,-326.151",
        "871.784,-238.495 862.933,-232.671 866.22,-242.743 871.784,-238.495",
        "5256.44,-243.268 5258.95,-232.975 5250.57,-239.454 5256.44,-243.268",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "2958.71,-209.85 2968.71,-206.35 2958.71,-202.85 2958.71,-209.85",
        "2873.01,-154.166 2872.61,-143.579 2866.32,-152.107 2873.01,-154.166",
        "847.098,-63.8762 843.597,-53.8762 840.098,-63.8762 847.098,-63.8762",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701"
    ]
]