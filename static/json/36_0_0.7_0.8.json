[
    [
        {
            "id": "2001",
            "citation_count": 136,
            "name": 136,
            "cx": 28.5975,
            "cy": -1642.19,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2003",
            "citation_count": 192,
            "name": 192,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W01-1812",
            "name": "Parsing and Hypergraphs",
            "publication_data": 2001,
            "citation": 91,
            "abstract": "None",
            "cx": 1290.6,
            "cy": -1642.19,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N03-1016",
            "name": "{A}* Parsing: Fast Exact {V}iterbi Parse Selection",
            "publication_data": 2003,
            "citation": 192,
            "abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of pre-computation, reduces the work to less than 5%. Un-like best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.",
            "cx": 1160.6,
            "cy": -1552.45,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1108",
            "name": "K-Best {A}* Parsing",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "A* parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing k-best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types.",
            "cx": 1111.6,
            "cy": -1014.01,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P01-1044",
            "name": "Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the {P}enn {T}reebank",
            "publication_data": 2001,
            "citation": 45,
            "abstract": "This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the Penn Treebank with the Treebank's own CFG grammar. We show how performance is dramatically affected by rule representation and tree transformations, but little by top-down vs. bottom-up strategies. We discuss grammatical saturation, including analysis of the strongly connected components of the phrasal nonterminals in the Treebank, and model how, as sentence length increases, the effective grammar rule size increases as regions of the grammar are unlocked, yielding super-cubic observed time behavior in some configurations.",
            "cx": 1124.6,
            "cy": -1642.19,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2004",
            "citation_count": 558,
            "name": 558,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D08-1012",
            "name": "Coarse-to-Fine Syntactic Machine Translation using Language Projections",
            "publication_data": 2008,
            "citation": 36,
            "abstract": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incrementally introduced. In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.",
            "cx": 1540.6,
            "cy": -1103.75,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2037",
            "name": "Top-Down K-Best {A}* Parsing",
            "publication_data": 2010,
            "citation": 9,
            "abstract": "We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA* is a variant of the k-best A* (KA*) algorithm of Pauls and Klein (2009). In contrast to KA*, which performs an inside and outside pass before performing k-best extraction bottom up, TKA* performs only the inside pass before extracting k-best lists top down. TKA* maintains the same optimality and efficiency guarantees of KA*, but is simpler to both specify and implement.",
            "cx": 1092.6,
            "cy": -924.271,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2005",
            "citation_count": 174,
            "name": 174,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W04-3201",
            "name": "Max-Margin Parsing",
            "publication_data": 2004,
            "citation": 198,
            "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the modelxe2x80x99s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.",
            "cx": 3248.6,
            "cy": -1462.71,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D08-1091",
            "name": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing",
            "publication_data": 2008,
            "citation": 35,
            "abstract": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.",
            "cx": 3248.6,
            "cy": -1103.75,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-1022",
            "name": "Less Grammar, More Features",
            "publication_data": 2014,
            "citation": 43,
            "abstract": "We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure. For example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as Xbar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual constituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bjorkelund et al. (2013) on a range of languages. In addition, despite being designed for syntactic analysis, our system also achieves stateof-the-art numbers on the structural sentiment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.",
            "cx": 3327.6,
            "cy": -565.311,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D15-1032",
            "name": "An Empirical Analysis of Optimization for Max-Margin {NLP}",
            "publication_data": 2015,
            "citation": 14,
            "abstract": "Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.",
            "cx": 1961.6,
            "cy": -475.571,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P04-1061",
            "name": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
            "publication_data": 2004,
            "citation": 360,
            "abstract": "We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.",
            "cx": 3932.6,
            "cy": -1462.71,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P08-1100",
            "name": "Analyzing the Errors of Unsupervised Learning",
            "publication_data": 2008,
            "citation": 21,
            "abstract": "We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",
            "cx": 3852.6,
            "cy": -1103.75,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1083",
            "name": "Painless Unsupervised Learning with Features",
            "publication_data": 2010,
            "citation": 168,
            "abstract": "We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.",
            "cx": 3919.6,
            "cy": -924.271,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 1190,
            "name": 1190,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "H05-1010",
            "name": "A Discriminative Matching Approach to Word Alignment",
            "publication_data": 2005,
            "citation": 174,
            "abstract": "We present a discriminative, large-margin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.",
            "cx": 2181.6,
            "cy": -1372.97,
            "rx": 52.1524,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P10-1147",
            "name": "Discriminative Modeling of Extraction Sets for Machine Translation",
            "publication_data": 2010,
            "citation": 25,
            "abstract": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.",
            "cx": 2432.6,
            "cy": -924.271,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2007",
            "citation_count": 779,
            "name": 779,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W06-3105",
            "name": "Why Generative Phrase Models Underperform Surface Heuristics",
            "publication_data": 2006,
            "citation": 71,
            "abstract": "We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased deter-minization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score.",
            "cx": 908.597,
            "cy": -1283.23,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P08-2007",
            "name": "The Complexity of Phrase Alignment Problems",
            "publication_data": 2008,
            "citation": 63,
            "abstract": "Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.",
            "cx": 912.597,
            "cy": -1103.75,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D08-1033",
            "name": "Sampling Alignment Structure under a {B}ayesian Translation Model",
            "publication_data": 2008,
            "citation": 86,
            "abstract": "We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase-based translation systems.",
            "cx": 747.597,
            "cy": -1103.75,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1014",
            "name": "Unsupervised Syntactic Alignment with Inversion Transduction Grammars",
            "publication_data": 2010,
            "citation": 18,
            "abstract": "Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.",
            "cx": 2159.6,
            "cy": -924.271,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P06-1055",
            "name": "Learning Accurate, Compact, and Interpretable Tree Annotation",
            "publication_data": 2006,
            "citation": 727,
            "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
            "cx": 3494.6,
            "cy": -1283.23,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D07-1094",
            "name": "Learning Structured Models for Phone Recognition",
            "publication_data": 2007,
            "citation": 13,
            "abstract": "We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",
            "cx": 3624.6,
            "cy": -1193.49,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-1112",
            "name": "Simple, Accurate Parsing with an All-Fragments Grammar",
            "publication_data": 2010,
            "citation": 26,
            "abstract": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.",
            "cx": 3464.6,
            "cy": -924.271,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-2127",
            "name": "The Surprising Variance in Shortest-Derivation Parsing",
            "publication_data": 2011,
            "citation": 2,
            "abstract": "We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.",
            "cx": 3464.6,
            "cy": -834.531,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D12-1105",
            "name": "Training Factored {PCFG}s with Expectation Propagation",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguistically-motivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.",
            "cx": 3164.6,
            "cy": -744.791,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N06-1014",
            "name": "Alignment by Agreement",
            "publication_data": 2006,
            "citation": 392,
            "abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.",
            "cx": 2414.6,
            "cy": -1283.23,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P07-1003",
            "name": "Tailoring Word Alignments to Syntactic Machine Translation",
            "publication_data": 2007,
            "citation": 100,
            "abstract": "Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our modelxe2x80x99s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",
            "cx": 2414.6,
            "cy": -1193.49,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 613,
            "name": 613,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D12-1079",
            "name": "Transforming Trees to Improve Syntactic Convergence",
            "publication_data": 2012,
            "citation": 13,
            "abstract": "We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.",
            "cx": 2140.6,
            "cy": -744.791,
            "rx": 106.132,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P07-1107",
            "name": "Unsupervised Coreference Resolution in a Nonparametric {B}ayesian Model",
            "publication_data": 2007,
            "citation": 127,
            "abstract": "We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",
            "cx": 384.597,
            "cy": -1193.49,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D09-1120",
            "name": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
            "publication_data": 2009,
            "citation": 158,
            "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).",
            "cx": 300.597,
            "cy": -1014.01,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N10-1061",
            "name": "Coreference Resolution in a Modular, Entity-Centered Model",
            "publication_data": 2010,
            "citation": 119,
            "abstract": "Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.",
            "cx": 440.597,
            "cy": -924.271,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N07-1051",
            "name": "Improved Inference for Unlexicalized Parsing",
            "publication_data": 2007,
            "citation": 533,
            "abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammarxe2x80x99s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.",
            "cx": 2926.6,
            "cy": -1193.49,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "W08-1005",
            "name": "Parsing {G}erman with Latent Variable Grammars",
            "publication_data": 2008,
            "citation": 23,
            "abstract": "We describe experiments on learning latent variable grammars for various German tree-banks, using a language-agnostic statistical approach. In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks.",
            "cx": 2993.6,
            "cy": -1103.75,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1026",
            "name": "Efficient Parsing for Transducer Grammars",
            "publication_data": 2009,
            "citation": 24,
            "abstract": "The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized. We address the problem of parsing efficiently with such grammars in three ways. First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form. Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices. Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar. In all, parsing time reduces by 81%. We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU.",
            "cx": 1844.6,
            "cy": -1014.01,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1063",
            "name": "Hierarchical Search for Parsing",
            "publication_data": 2009,
            "citation": 17,
            "abstract": "Both coarse-to-fine and A* parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarse-to-fine case because of accumulated slack in A* heuristics.",
            "cx": 876.597,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P12-2021",
            "name": "Robust Conversion of {CCG} Derivations to Phrase Structure Trees",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.",
            "cx": 2721.6,
            "cy": -744.791,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-1020",
            "name": "Sparser, Better, Faster {GPU} Parsing",
            "publication_data": 2014,
            "citation": 16,
            "abstract": "Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per secondxe2x80x94more than a 2x speedupxe2x80x94on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruningxe2x80x94nearly a 6x speedup.",
            "cx": 1044.6,
            "cy": -565.311,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P15-1030",
            "name": "Neural {CRF} Parsing",
            "publication_data": 2015,
            "citation": 33,
            "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.",
            "cx": 2990.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N07-1052",
            "name": "Approximate Factoring for {A}* Search",
            "publication_data": 2007,
            "citation": 6,
            "abstract": "We present a novel method for creating Axe2x88x97 estimates for structured search problems. In our approach, we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains.",
            "cx": 1414.6,
            "cy": -1193.49,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2009",
            "citation_count": 728,
            "name": 728,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P08-1088",
            "name": "Learning Bilingual Lexicons from Monolingual Corpora",
            "publication_data": 2008,
            "citation": 259,
            "abstract": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
            "cx": 2669.6,
            "cy": -1103.75,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D11-1029",
            "name": "Simple Effective Decipherment via Combinatorial Optimization",
            "publication_data": 2011,
            "citation": 11,
            "abstract": "We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.",
            "cx": 1452.6,
            "cy": -834.531,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1069",
            "name": "Online {EM} for Unsupervised Models",
            "publication_data": 2009,
            "citation": 157,
            "abstract": "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.",
            "cx": 3859.6,
            "cy": -1014.01,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P09-1104",
            "name": "Better Word Alignments with Supervised {ITG} Models",
            "publication_data": 2009,
            "citation": 91,
            "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA alignments.",
            "cx": 2353.6,
            "cy": -1014.01,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D09-1147",
            "name": "Consensus Training for Consensus Decoding in Machine Translation",
            "publication_data": 2009,
            "citation": 25,
            "abstract": "We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forest-based consensus and minimum Bayes risk decoding methods. Our continuous objective can be optimized using simple gradient ascent. However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here. Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning. First, it specifically optimizes model weights for downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.",
            "cx": 1612.6,
            "cy": -1014.01,
            "rx": 95.4188,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1021",
            "name": "Unsupervised Transcription of Historical Documents",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Googlexe2x80x99s open source OCR system.",
            "cx": 1219.6,
            "cy": -655.051,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-2020",
            "name": "Improved Typesetting Models for Historical {OCR}",
            "publication_data": 2014,
            "citation": 13,
            "abstract": "We present richer typesetting models that extend the unsupervised historical document recognition system of BergKirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers.",
            "cx": 1500.6,
            "cy": -565.311,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N12-1004",
            "name": "Fast Inference in Phrase Extraction Models with Belief Propagation",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up.",
            "cx": 1000.6,
            "cy": -744.791,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D08-1092",
            "name": "Two Languages are Better than One (for Syntactic Parsing)",
            "publication_data": 2008,
            "citation": 90,
            "abstract": "We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",
            "cx": 1735.6,
            "cy": -1103.75,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1015",
            "name": "Joint Parsing and Alignment with Weakly Synchronized Grammars",
            "publication_data": 2010,
            "citation": 56,
            "abstract": "Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English parsing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",
            "cx": 1743.6,
            "cy": -924.271,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 490,
            "name": 490,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P09-2036",
            "name": "Asynchronous Binarization for Synchronous Grammars",
            "publication_data": 2009,
            "citation": 12,
            "abstract": "Binarization of n-ary rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.",
            "cx": 2048.6,
            "cy": -1014.01,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P09-1011",
            "name": "Learning Semantic Correspondences with Less Supervision",
            "publication_data": 2009,
            "citation": 190,
            "abstract": "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.",
            "cx": 4087.6,
            "cy": -1014.01,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "J13-2005",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2013,
            "citation": 54,
            "abstract": "This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.",
            "cx": 4087.6,
            "cy": -655.051,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N09-1008",
            "name": "Improved Reconstruction of Protolanguage Word Forms",
            "publication_data": 2009,
            "citation": 19,
            "abstract": "We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous work. First, previous work focused on faithfulness features, which model changes between successive languages. We add markedness features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of Proto-Oceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date.",
            "cx": 1354.6,
            "cy": -1014.01,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N10-1082",
            "name": "Type-Based {MCMC}",
            "publication_data": 2010,
            "citation": 32,
            "abstract": "Most existing algorithms for learning latent-variable models---such as EM and existing Gibbs samplers---are token-based, meaning that they update the variables associated with one sentence at a time. The incremental nature of these methods makes them susceptible to local optima/slow mixing. In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences. We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars.",
            "cx": 1934.6,
            "cy": -924.271,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P10-2064",
            "name": "Hierarchical {A}* Parsing with Bridge Outside Scores",
            "publication_data": 2010,
            "citation": 6,
            "abstract": "Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.",
            "cx": 876.597,
            "cy": -924.271,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P12-1041",
            "name": "Coreference Semantics from Web Features",
            "publication_data": 2012,
            "citation": 36,
            "abstract": "To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.",
            "cx": 590.597,
            "cy": -744.791,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2011",
            "citation_count": 647,
            "name": 647,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-2054",
            "name": "An Entity-Level Approach to Information Extraction",
            "publication_data": 2010,
            "citation": 8,
            "abstract": "We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.",
            "cx": 607.597,
            "cy": -924.271,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1105",
            "name": "Finding Cognate Groups Using Phylogenies",
            "publication_data": 2010,
            "citation": 23,
            "abstract": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.",
            "cx": 1464.6,
            "cy": -924.271,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-1916",
            "name": "Mention Detection: Heuristics for the {O}nto{N}otes annotations",
            "publication_data": 2011,
            "citation": 10,
            "abstract": "Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3, Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.",
            "cx": 383.597,
            "cy": -834.531,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-1012",
            "name": "Decentralized Entity-Level Modeling for Coreference Resolution",
            "publication_data": 2013,
            "citation": 22,
            "abstract": "Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.",
            "cx": 719.597,
            "cy": -655.051,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1203",
            "name": "Easy Victories and Uphill Battles in Coreference Resolution",
            "publication_data": 2013,
            "citation": 126,
            "abstract": "Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win xe2x80x9ceasy victoriesxe2x80x9d without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an xe2x80x9cuphill battle.xe2x80x9d Nonetheless, our final system 1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bjxc2xa8 orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.",
            "cx": 905.597,
            "cy": -655.051,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 187,
            "name": 187,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D13-1027",
            "name": "Error-Driven Analysis of Challenges in Coreference Resolution",
            "publication_data": 2013,
            "citation": 25,
            "abstract": "Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.",
            "cx": 383.597,
            "cy": -655.051,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-1027",
            "name": "Faster and Smaller N-Gram Language Models",
            "publication_data": 2011,
            "citation": 103,
            "abstract": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.",
            "cx": 4214.6,
            "cy": -834.531,
            "rx": 76.2353,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P12-1101",
            "name": "Large-Scale Syntactic Language Modeling with Treelets",
            "publication_data": 2012,
            "citation": 27,
            "abstract": "We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.",
            "cx": 4214.6,
            "cy": -744.791,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-1049",
            "name": "Jointly Learning to Extract and Compress",
            "publication_data": 2011,
            "citation": 165,
            "abstract": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.",
            "cx": 153.597,
            "cy": -834.531,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P16-1188",
            "name": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
            "publication_data": 2016,
            "citation": 28,
            "abstract": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.",
            "cx": 806.597,
            "cy": -385.831,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P11-1060",
            "name": "Learning Dependency-Based Compositional Semantics",
            "publication_data": 2011,
            "citation": 315,
            "abstract": "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (Geo and Jobs), our system obtains the highest published accuracies, despite requiring no annotated logical forms.",
            "cx": 4395.6,
            "cy": -834.531,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "N16-1181",
            "name": "Learning to Compose Neural Networks for Question Answering",
            "publication_data": 2016,
            "citation": 205,
            "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.",
            "cx": 4395.6,
            "cy": -385.831,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "P11-1070",
            "name": "Web-Scale Features for Full-Scale Parsing",
            "publication_data": 2011,
            "citation": 41,
            "abstract": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.",
            "cx": 594.597,
            "cy": -834.531,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D13-1087",
            "name": "Decipherment with a Million Random Restarts",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.",
            "cx": 1452.6,
            "cy": -655.051,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 265,
            "name": 265,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "Q14-1037",
            "name": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking",
            "publication_data": 2014,
            "citation": 122,
            "abstract": "We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",
            "cx": 806.597,
            "cy": -565.311,
            "rx": 54.3945,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D12-1091",
            "name": "An Empirical Investigation of Statistical Significance in {NLP}",
            "publication_data": 2012,
            "citation": 45,
            "abstract": "We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.",
            "cx": 2897.6,
            "cy": -744.791,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P19-1340",
            "name": "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
            "publication_data": 2019,
            "citation": 9,
            "abstract": "We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2{\\%} relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",
            "cx": 2855.6,
            "cy": -116.61,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D12-1096",
            "name": "Parser Showdown at the {W}all {S}treet Corral: An Empirical Investigation of Error Types in Parser Output",
            "publication_data": 2012,
            "citation": 47,
            "abstract": "Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.",
            "cx": 4511.6,
            "cy": -744.791,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-2018",
            "name": "An Empirical Examination of Challenges in {C}hinese Parsing",
            "publication_data": 2013,
            "citation": 11,
            "abstract": "Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.",
            "cx": 4511.6,
            "cy": -655.051,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 240,
            "name": 240,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 64,
            "name": 64,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-1607",
            "name": "Grounding Language with Points and Paths in Continuous Spaces",
            "publication_data": 2014,
            "citation": 10,
            "abstract": "We present a model for generating pathvalued interpretations of natural language text. Our model encodes a map from natural language descriptions to paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010).",
            "cx": 4543.6,
            "cy": -565.311,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D15-1138",
            "name": "Alignment-Based Compositional Semantics for Instruction Following",
            "publication_data": 2015,
            "citation": 17,
            "abstract": "This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the modelxe2x80x99s flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",
            "cx": 4543.6,
            "cy": -475.571,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P14-2133",
            "name": "How much do word embeddings encode about syntax?",
            "publication_data": 2014,
            "citation": 36,
            "abstract": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",
            "cx": 2891.6,
            "cy": -565.311,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-1249",
            "name": "Constituency Parsing with a Self-Attentive Encoder",
            "publication_data": 2018,
            "citation": 22,
            "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
            "cx": 3223.6,
            "cy": -206.35,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N18-1091",
            "name": "What{'}s Going On in Neural Constituency Parsers? An Analysis",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity. The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods. To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments. We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.",
            "cx": 3580.6,
            "cy": -206.35,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 269,
            "name": 269,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.acl-main.557",
            "name": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word{'}s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.",
            "cx": 2980.6,
            "cy": -26.8701,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "Q17-1031",
            "name": "Parsing with Traces: An {O}(n4) Algorithm and a Structural Representation",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons. We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3{\\%} of the Penn English Treebank. We also implement a proof-of-concept parser that recovers a range of null elements and trace types.",
            "cx": 1961.6,
            "cy": -296.09,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 64,
            "name": 64,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D16-1125",
            "name": "Reasoning about Pragmatics with Neural Listeners and Speakers",
            "publication_data": 2016,
            "citation": 36,
            "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural listener and speaker models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.",
            "cx": 4593.6,
            "cy": -385.831,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P17-1022",
            "name": "Translating Neuralese",
            "publication_data": 2017,
            "citation": "???",
            "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents{'} messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",
            "cx": 4593.6,
            "cy": -296.09,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018",
            "citation_count": 41,
            "name": 41,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-2025",
            "name": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.",
            "cx": 3018.6,
            "cy": -296.09,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1031",
            "name": "Cross-Domain Generalization of Neural Constituency Parsers",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing{---}but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",
            "cx": 3264.6,
            "cy": -116.61,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1076",
            "name": "A Minimal Span-Based Neural Constituency Parser",
            "publication_data": 2017,
            "citation": 39,
            "abstract": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",
            "cx": 3485.6,
            "cy": -296.09,
            "rx": 67.7647,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "Unsupervised Parsing via Constituency Tests",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.",
            "cx": 3339.6,
            "cy": -26.8701,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D17-1178",
            "name": "Effective Inference for Generative Neural Parsing",
            "publication_data": 2017,
            "citation": 9,
            "abstract": "Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.",
            "cx": 3265.6,
            "cy": -296.09,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1311",
            "name": "Analogs of Linguistic Structure in Deep Representations",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a {``}syntax{''} with functional analogues to qualitative properties of natural language.",
            "cx": 4792.6,
            "cy": -296.09,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1188",
            "name": "Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.",
            "cx": 4849.6,
            "cy": -116.61,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2019",
            "citation_count": 10,
            "name": 10,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-2075",
            "name": "Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser{'}s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",
            "cx": 2874.6,
            "cy": -206.35,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1197",
            "name": "Learning with Latent Language",
            "publication_data": 2018,
            "citation": 8,
            "abstract": "The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter{'}s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without.",
            "cx": 4906.6,
            "cy": -206.35,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        }
    ],
    [
        {
            "source": "2001",
            "target": "2003",
            "d": "M28.5975,-1623.86C28.5975,-1608.5 28.5975,-1586.18 28.5975,-1570.81"
        },
        {
            "source": "W01-1812",
            "target": "N03-1016",
            "d": "M1256.44,-1618.14C1239.89,-1606.97 1219.87,-1593.46 1202.41,-1581.67"
        },
        {
            "source": "W01-1812",
            "target": "P09-1108",
            "d": "M1290.6,-1615.03C1290.6,-1580.41 1290.6,-1517.48 1290.6,-1463.71 1290.6,-1463.71 1290.6,-1463.71 1290.6,-1192.49 1290.6,-1121.75 1219.23,-1069.58 1166.62,-1040.74"
        },
        {
            "source": "P01-1044",
            "target": "W01-1812",
            "d": "M1199.17,-1642.19C1201.49,-1642.19 1203.81,-1642.19 1206.13,-1642.19"
        },
        {
            "source": "P01-1044",
            "target": "N03-1016",
            "d": "M1135.2,-1615.34C1138.65,-1606.95 1142.53,-1597.5 1146.2,-1588.55"
        },
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "N03-1016",
            "target": "D08-1012",
            "d": "M1196.75,-1529.77C1270.77,-1483.29 1439.32,-1366.35 1519.6,-1220.36 1533.04,-1195.91 1538.11,-1164.59 1539.92,-1140.72"
        },
        {
            "source": "N03-1016",
            "target": "P10-2037",
            "d": "M1122.19,-1530.27C1075.71,-1501.5 1003.6,-1445.35 1003.6,-1373.97 1003.6,-1373.97 1003.6,-1373.97 1003.6,-1102.75 1003.6,-1050.68 997.408,-1032.71 1022.6,-987.141 1029.1,-975.377 1038.73,-964.745 1048.78,-955.746"
        },
        {
            "source": "2004",
            "target": "2005",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "W04-3201",
            "target": "D08-1091",
            "d": "M3248.6,-1435.75C3248.6,-1373.19 3248.6,-1213.9 3248.6,-1141.53"
        },
        {
            "source": "W04-3201",
            "target": "P14-1022",
            "d": "M3329.34,-1449.22C3467.04,-1425.61 3732.6,-1369.09 3732.6,-1284.23 3732.6,-1284.23 3732.6,-1284.23 3732.6,-743.791 3732.6,-603.928 3527.47,-573.249 3409.19,-567.184"
        },
        {
            "source": "W04-3201",
            "target": "D15-1032",
            "d": "M3172.33,-1446.92C3047.57,-1420.52 2813.6,-1360.8 2813.6,-1284.23 2813.6,-1284.23 2813.6,-1284.23 2813.6,-923.271 2813.6,-833.4 2211.3,-578.872 2019.51,-500.101"
        },
        {
            "source": "P04-1061",
            "target": "P08-1100",
            "d": "M3926.78,-1435.75C3912.69,-1372.9 3876.74,-1212.46 3860.62,-1140.56"
        },
        {
            "source": "P04-1061",
            "target": "N10-1083",
            "d": "M3942.28,-1435.67C3954.12,-1401.43 3972.6,-1339.19 3972.6,-1284.23 3972.6,-1284.23 3972.6,-1284.23 3972.6,-1102.75 3972.6,-1051.61 3952.14,-995.507 3936.65,-960.351"
        },
        {
            "source": "2005",
            "target": "2006",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "H05-1010",
            "target": "P10-1147",
            "d": "M2197.48,-1347.03C2222.75,-1307.76 2274.21,-1229.52 2322.6,-1166.62 2367.51,-1108.23 2404.41,-1109.74 2430.6,-1040.88 2440.19,-1015.66 2440.26,-985.054 2438.16,-961.668"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "W06-3105",
            "target": "P08-2007",
            "d": "M909.188,-1256.04C909.875,-1225.55 911.014,-1175 911.785,-1140.8"
        },
        {
            "source": "W06-3105",
            "target": "D08-1033",
            "d": "M886.211,-1257.55C857.775,-1226.21 808.623,-1172.02 777.137,-1137.31"
        },
        {
            "source": "W06-3105",
            "target": "N10-1014",
            "d": "M975.133,-1279.23C1087.97,-1273.31 1323.68,-1257.34 1519.6,-1220.36 1811.15,-1165.33 1983.2,-1277.21 2162.6,-1040.88 2179.67,-1018.39 2177.08,-985.725 2171.28,-960.848"
        },
        {
            "source": "P06-1055",
            "target": "D07-1094",
            "d": "M3529.09,-1258.95C3545.11,-1248.14 3564.32,-1235.18 3581.29,-1223.72"
        },
        {
            "source": "P06-1055",
            "target": "P10-1112",
            "d": "M3492.42,-1256.27C3487.14,-1193.55 3473.71,-1033.64 3467.64,-961.521"
        },
        {
            "source": "P06-1055",
            "target": "P11-2127",
            "d": "M3502.87,-1256.31C3526.43,-1180.46 3590.12,-960.341 3556.6,-897.401 3548.28,-881.779 3534.06,-869.398 3519.27,-859.944"
        },
        {
            "source": "P06-1055",
            "target": "D12-1105",
            "d": "M3478.9,-1256.72C3425,-1169.09 3247.34,-880.3 3185.8,-780.265"
        },
        {
            "source": "N06-1014",
            "target": "P07-1003",
            "d": "M2414.6,-1255.91C2414.6,-1247.94 2414.6,-1239.05 2414.6,-1230.56"
        },
        {
            "source": "N06-1014",
            "target": "P10-1147",
            "d": "M2464.89,-1260.52C2481.21,-1250.77 2497.46,-1237.52 2506.6,-1220.36 2530.45,-1175.58 2522.99,-1086.89 2480.6,-987.141 2476.33,-977.091 2470.01,-967.256 2463.35,-958.491"
        },
        {
            "source": "N06-1014",
            "target": "N10-1014",
            "d": "M2371.47,-1259.23C2354.81,-1248.9 2336.47,-1235.59 2322.6,-1220.36 2243.13,-1133.1 2253.5,-1089.98 2195.6,-987.141 2190.62,-978.301 2185.16,-968.806 2180.02,-959.974"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "P07-1003",
            "target": "P10-1147",
            "d": "M2428.52,-1166.89C2447.88,-1128.04 2478.91,-1051.88 2463.6,-987.141 2461.42,-977.954 2457.66,-968.578 2453.5,-959.994"
        },
        {
            "source": "P07-1003",
            "target": "N10-1014",
            "d": "M2391.15,-1167.71C2363.28,-1138.05 2315.65,-1086.63 2276.6,-1040.88 2256.74,-1017.62 2254.99,-1009 2233.6,-987.141 2223.32,-976.644 2211.4,-966.077 2200.17,-956.72"
        },
        {
            "source": "P07-1003",
            "target": "D12-1079",
            "d": "M2495.85,-1188.36C2585.27,-1182.06 2721.1,-1166.69 2756.6,-1130.62 2793.12,-1093.51 2775.6,-1067.08 2775.6,-1015.01 2775.6,-1015.01 2775.6,-1015.01 2775.6,-923.271 2775.6,-815.772 2433.78,-770.477 2251.72,-753.976"
        },
        {
            "source": "P07-1107",
            "target": "D09-1120",
            "d": "M372.383,-1166.68C357.784,-1135.84 333.294,-1084.09 317.041,-1049.75"
        },
        {
            "source": "P07-1107",
            "target": "N10-1061",
            "d": "M390.069,-1166.38C400.127,-1118.39 421.569,-1016.07 433.02,-961.429"
        },
        {
            "source": "N07-1051",
            "target": "W08-1005",
            "d": "M2945.98,-1167.11C2952.91,-1158.03 2960.83,-1147.66 2968.2,-1138.01"
        },
        {
            "source": "N07-1051",
            "target": "N09-1026",
            "d": "M2847.2,-1183.86C2678.12,-1164.72 2270.49,-1114.51 1934.6,-1040.88 1927.31,-1039.28 1919.74,-1037.43 1912.23,-1035.46"
        },
        {
            "source": "N07-1051",
            "target": "N09-1063",
            "d": "M2844.15,-1186.72C2759.49,-1180.96 2623.9,-1172.19 2506.6,-1166.62 2026.87,-1143.86 1903.4,-1188.24 1426.6,-1130.62 1256.75,-1110.1 1061.54,-1063.45 955.401,-1036.1"
        },
        {
            "source": "N07-1051",
            "target": "P11-2127",
            "d": "M3011.47,-1190.81C3116.64,-1186.88 3287.42,-1173.82 3331.6,-1130.62 3406.85,-1057.04 3315.24,-985.64 3372.6,-897.401 3382.33,-882.427 3397.1,-870.179 3411.96,-860.649"
        },
        {
            "source": "N07-1051",
            "target": "P12-2021",
            "d": "M2901.42,-1167.66C2892,-1157.06 2882.04,-1144.05 2875.6,-1130.62 2852.91,-1083.3 2851.6,-1067.49 2851.6,-1015.01 2851.6,-1015.01 2851.6,-1015.01 2851.6,-923.271 2851.6,-870.793 2858.24,-850.262 2827.6,-807.661 2816.78,-792.628 2801.1,-780.672 2785.09,-771.435"
        },
        {
            "source": "N07-1051",
            "target": "D12-1105",
            "d": "M3005.57,-1183.41C3042.41,-1175.27 3084.09,-1159.85 3111.6,-1130.62 3148.05,-1091.88 3142.6,-1068.21 3142.6,-1015.01 3142.6,-1015.01 3142.6,-1015.01 3142.6,-923.271 3142.6,-873.959 3151.07,-817.522 3157.49,-781.781"
        },
        {
            "source": "N07-1051",
            "target": "P14-1020",
            "d": "M2843.25,-1188.33C2753.29,-1182.02 2618.08,-1166.69 2582.6,-1130.62 2508.95,-1055.75 2623.56,-966.64 2544.6,-897.401 2495.24,-854.124 1425.96,-881.884 1363.6,-861.401 1230.19,-817.587 1196.1,-787.555 1103.6,-681.921 1082.98,-658.38 1067.37,-626.36 1057.4,-601.987"
        },
        {
            "source": "N07-1051",
            "target": "P14-1022",
            "d": "M3001.06,-1180.57C3030.3,-1171.84 3061.03,-1156.74 3078.6,-1130.62 3180.99,-978.396 2980.58,-877.775 3070.6,-717.921 3110.65,-646.796 3198.76,-605.789 3260.6,-584.771"
        },
        {
            "source": "N07-1051",
            "target": "P15-1030",
            "d": "M2917.31,-1166.66C2913.84,-1155.7 2910.32,-1142.69 2908.6,-1130.62 2905.23,-1106.98 2906.17,-1100.64 2908.6,-1076.88 2922.67,-939.085 2953.1,-908.934 2971.6,-771.661 2984.09,-678.935 2988.43,-568.673 2989.89,-512.842"
        },
        {
            "source": "N07-1052",
            "target": "D08-1012",
            "d": "M1449.36,-1168.28C1464.11,-1158.01 1481.47,-1145.93 1497.06,-1135.07"
        },
        {
            "source": "N07-1052",
            "target": "P09-1108",
            "d": "M1396.37,-1166.82C1376.57,-1140.69 1342.62,-1100.73 1304.6,-1076.88 1293.61,-1069.99 1231.21,-1050.5 1180.3,-1035.22"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "P08-1088",
            "target": "D11-1029",
            "d": "M2666.55,-1076.87C2659.7,-1031.71 2638.6,-939.34 2577.6,-897.401 2493.28,-839.438 1795.34,-834.893 1543.1,-835.168"
        },
        {
            "source": "P08-1100",
            "target": "N09-1069",
            "d": "M3854.7,-1076.43C3855.33,-1068.46 3856.04,-1059.57 3856.72,-1051.08"
        },
        {
            "source": "D08-1012",
            "target": "P09-1104",
            "d": "M1614.22,-1084.47C1627.9,-1081.56 1642.13,-1078.86 1655.6,-1076.88 1879.09,-1044 1937.89,-1064.05 2162.6,-1040.88 2201.68,-1036.85 2245.09,-1031.08 2280.63,-1026"
        },
        {
            "source": "D08-1012",
            "target": "N09-1026",
            "d": "M1608.44,-1083.17C1658.07,-1068.85 1725.17,-1049.48 1775.17,-1035.05"
        },
        {
            "source": "D08-1012",
            "target": "D09-1147",
            "d": "M1561.42,-1077.37C1568.89,-1068.28 1577.41,-1057.89 1585.34,-1048.23"
        },
        {
            "source": "D08-1012",
            "target": "P13-1021",
            "d": "M1450.91,-1089.62C1367.34,-1076.56 1253.41,-1056.29 1238.6,-1040.88 1202.51,-1003.35 1219.6,-977.343 1219.6,-925.271 1219.6,-925.271 1219.6,-925.271 1219.6,-833.531 1219.6,-784.804 1219.6,-728.552 1219.6,-692.666"
        },
        {
            "source": "D08-1012",
            "target": "P14-2020",
            "d": "M1523.18,-1076.95C1509.24,-1052.83 1494.22,-1016.27 1508.6,-987.141 1520.25,-963.547 1543.38,-974.447 1555.6,-951.141 1611.2,-845.036 1565.71,-801.679 1568.6,-681.921 1569.17,-658.043 1578.26,-650.026 1568.6,-628.181 1563.35,-616.317 1554.58,-605.75 1545.06,-596.852"
        },
        {
            "source": "D08-1033",
            "target": "N12-1004",
            "d": "M743.55,-1076.86C738.481,-1035.61 734.21,-953.875 768.597,-897.401 807.865,-832.912 886.164,-790.517 940.89,-767.362"
        },
        {
            "source": "D08-1091",
            "target": "P14-1022",
            "d": "M3259.51,-1077.12C3278.5,-1030.13 3315.6,-926.644 3315.6,-835.531 3315.6,-835.531 3315.6,-835.531 3315.6,-743.791 3315.6,-694.742 3320.22,-638.242 3323.72,-602.414"
        },
        {
            "source": "D08-1091",
            "target": "P15-1030",
            "d": "M3252.14,-1076.69C3262.01,-1000.96 3287.21,-782.183 3258.6,-717.921 3213.85,-617.397 3104.25,-541.153 3039.92,-503.225"
        },
        {
            "source": "D08-1092",
            "target": "N10-1015",
            "d": "M1736.78,-1076.56C1738.15,-1046.07 1740.43,-995.515 1741.97,-961.323"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "P09-1011",
            "target": "J13-2005",
            "d": "M4087.6,-987.052C4087.6,-924.486 4087.6,-765.198 4087.6,-692.828"
        },
        {
            "source": "P09-1104",
            "target": "P10-1147",
            "d": "M2375.6,-988.571C2384.12,-979.116 2393.99,-968.152 2403.1,-958.032"
        },
        {
            "source": "P09-1104",
            "target": "N10-1014",
            "d": "M2310.06,-993.321C2283.25,-981.194 2248.51,-965.482 2219.16,-952.211"
        },
        {
            "source": "P09-1104",
            "target": "N12-1004",
            "d": "M2339.88,-987.635C2322.62,-956.782 2293.01,-907.554 2274.6,-897.401 2178.5,-844.414 1394.93,-884.245 1287.6,-861.401 1200.25,-842.812 1105.32,-799.612 1049.49,-771.604"
        },
        {
            "source": "P09-1108",
            "target": "P10-2037",
            "d": "M1106,-987.161C1104.24,-979.023 1102.26,-969.883 1100.37,-961.176"
        },
        {
            "source": "N09-1008",
            "target": "D11-1029",
            "d": "M1353.62,-986.843C1353.87,-962.136 1357.33,-924.879 1373.6,-897.401 1381.24,-884.491 1392.68,-873.265 1404.5,-864.051"
        },
        {
            "source": "N09-1026",
            "target": "P09-2036",
            "d": "M1925.48,-1014.01C1927.91,-1014.01 1930.35,-1014.01 1932.78,-1014.01"
        },
        {
            "source": "N09-1026",
            "target": "P09-1104",
            "d": "M1885.63,-1037.35C1902.94,-1045.92 1923.68,-1054.58 1943.6,-1058.88 2012.89,-1073.85 2193.56,-1074.98 2262.6,-1058.88 2278.08,-1055.27 2293.95,-1048.63 2308.02,-1041.55"
        },
        {
            "source": "N09-1026",
            "target": "N10-1014",
            "d": "M1904.12,-995.798C1914.28,-992.889 1924.74,-989.914 1934.6,-987.141 1982.54,-973.66 2036.32,-958.861 2079.27,-947.117"
        },
        {
            "source": "N09-1026",
            "target": "N10-1082",
            "d": "M1870.15,-988.103C1879.85,-978.645 1891.04,-967.731 1901.35,-957.679"
        },
        {
            "source": "N09-1063",
            "target": "P09-1108",
            "d": "M975.738,-1014.01C990.975,-1014.01 1006.21,-1014.01 1021.45,-1014.01"
        },
        {
            "source": "N09-1063",
            "target": "P10-2064",
            "d": "M876.597,-986.686C876.597,-978.72 876.597,-969.829 876.597,-961.34"
        },
        {
            "source": "N09-1069",
            "target": "N10-1083",
            "d": "M3876.95,-987.633C3883.05,-978.721 3889.99,-968.562 3896.49,-959.069"
        },
        {
            "source": "D09-1120",
            "target": "P12-1041",
            "d": "M287.391,-987.592C267.399,-945.165 236.047,-859.478 278.597,-807.661 304.307,-776.353 403.036,-760.609 482.638,-752.873"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "P10-1105",
            "target": "D11-1029",
            "d": "M1461.06,-897.421C1459.95,-889.282 1458.7,-880.143 1457.51,-871.436"
        },
        {
            "source": "P10-1112",
            "target": "P11-2127",
            "d": "M3464.6,-896.946C3464.6,-888.98 3464.6,-880.089 3464.6,-871.6"
        },
        {
            "source": "N10-1015",
            "target": "N10-1014",
            "d": "M1780.22,-947.35C1796.01,-955.998 1815.07,-964.78 1833.6,-969.141 1920.99,-989.717 1947.17,-984.648 2035.6,-969.141 2056.43,-965.488 2078.37,-958.628 2097.92,-951.352"
        },
        {
            "source": "N10-1015",
            "target": "D12-1079",
            "d": "M1788.37,-903.257C1859.88,-871.286 1999.99,-808.65 2080.27,-772.761"
        },
        {
            "source": "N10-1015",
            "target": "D12-1105",
            "d": "M1794.6,-905.28C1804.46,-902.288 1814.78,-899.486 1824.6,-897.401 2326.25,-790.914 2462.53,-833.674 2971.6,-771.661 3006.42,-767.419 3044.72,-762.366 3077.96,-757.855"
        },
        {
            "source": "N10-1061",
            "target": "P10-2054",
            "d": "M533.23,-924.271C535.722,-924.271 538.214,-924.271 540.706,-924.271"
        },
        {
            "source": "N10-1061",
            "target": "W11-1916",
            "d": "M423.804,-897.421C418.18,-888.764 411.819,-878.973 405.845,-869.776"
        },
        {
            "source": "N10-1061",
            "target": "P13-1012",
            "d": "M507.899,-905.752C519.139,-902.907 530.688,-900.033 541.597,-897.401 612.032,-880.41 652.811,-915.864 700.597,-861.401 741.454,-814.837 735.763,-737.411 727.78,-692.112"
        },
        {
            "source": "N10-1061",
            "target": "D13-1203",
            "d": "M507.284,-905.582C518.703,-902.728 530.47,-899.895 541.597,-897.401 624.186,-878.892 655.35,-903.808 728.597,-861.401 747.776,-850.298 834.531,-744.393 879.055,-689.182"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "W11-1916",
            "target": "D13-1027",
            "d": "M383.597,-807.343C383.597,-776.846 383.597,-726.294 383.597,-692.103"
        },
        {
            "source": "P11-1027",
            "target": "P12-1101",
            "d": "M4214.6,-807.206C4214.6,-799.239 4214.6,-790.348 4214.6,-781.86"
        },
        {
            "source": "P11-1049",
            "target": "P16-1188",
            "d": "M153.597,-807.369C153.597,-772.749 153.597,-709.819 153.597,-656.051 153.597,-656.051 153.597,-656.051 153.597,-564.311 153.597,-454.144 504.078,-410.139 691.505,-394.482"
        },
        {
            "source": "P11-1060",
            "target": "N16-1181",
            "d": "M4395.6,-807.369C4395.6,-772.749 4395.6,-709.819 4395.6,-656.051 4395.6,-656.051 4395.6,-656.051 4395.6,-564.311 4395.6,-515.584 4395.6,-459.331 4395.6,-423.445"
        },
        {
            "source": "P11-1070",
            "target": "P12-1041",
            "d": "M593.397,-807.206C593.034,-799.239 592.629,-790.348 592.242,-781.86"
        },
        {
            "source": "D11-1029",
            "target": "P13-1021",
            "d": "M1421.17,-809.595C1379.54,-777.885 1306.05,-721.9 1260.24,-687.006"
        },
        {
            "source": "D11-1029",
            "target": "D13-1087",
            "d": "M1452.6,-807.343C1452.6,-776.846 1452.6,-726.294 1452.6,-692.103"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "P12-1041",
            "target": "P13-1012",
            "d": "M626.189,-719.583C641.291,-709.312 659.058,-697.227 675.021,-686.37"
        },
        {
            "source": "P12-1041",
            "target": "Q14-1037",
            "d": "M587.611,-717.581C586.179,-691.777 588.268,-652.762 609.597,-628.181 642.51,-590.25 698.393,-575.305 742.252,-569.548"
        },
        {
            "source": "N12-1004",
            "target": "D12-1105",
            "d": "M1069.23,-748.8C1235.03,-758.259 1670.6,-781.767 2034.6,-789.661 2148.13,-792.123 2945.24,-811.874 3056.6,-789.661 3074.99,-785.992 3094.18,-779.126 3111.19,-771.85"
        },
        {
            "source": "N12-1004",
            "target": "P13-1012",
            "d": "M947.744,-727.288C904.09,-713.657 841.641,-694.158 793.112,-679.005"
        },
        {
            "source": "N12-1004",
            "target": "Q14-1037",
            "d": "M1003.4,-717.739C1004.69,-692.058 1002.49,-653.137 981.597,-628.181 954.066,-595.29 907.854,-579.876 869.831,-572.656"
        },
        {
            "source": "D12-1091",
            "target": "P19-1340",
            "d": "M2866.04,-720.966C2827.85,-690.428 2768.6,-632.4 2768.6,-566.311 2768.6,-566.311 2768.6,-566.311 2768.6,-295.09 2768.6,-243.019 2762.74,-225.234 2787.6,-179.48 2793.85,-167.972 2803.08,-157.484 2812.74,-148.545"
        },
        {
            "source": "D12-1096",
            "target": "P13-2018",
            "d": "M4511.6,-717.466C4511.6,-709.499 4511.6,-700.608 4511.6,-692.12"
        },
        {
            "source": "D12-1105",
            "target": "P14-1022",
            "d": "M3187.61,-718.739C3216.57,-687.203 3266.36,-632.987 3298.1,-598.434"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "P13-1012",
            "target": "Q14-1037",
            "d": "M744.761,-628.673C754.534,-618.817 765.818,-607.437 776.05,-597.118"
        },
        {
            "source": "P13-1021",
            "target": "P14-2020",
            "d": "M1285.12,-633.59C1328.49,-620.051 1385.32,-602.304 1429.74,-588.437"
        },
        {
            "source": "D13-1203",
            "target": "Q14-1037",
            "d": "M878.807,-630.307C866.856,-619.716 852.651,-607.127 840.015,-595.928"
        },
        {
            "source": "D13-1203",
            "target": "D15-1032",
            "d": "M959.985,-639.055C1050.51,-614.608 1238.12,-566.121 1399.6,-538.441 1568.24,-509.533 1767.78,-491.307 1877.95,-482.619"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "W14-1607",
            "target": "D15-1138",
            "d": "M4543.6,-537.986C4543.6,-530.019 4543.6,-521.128 4543.6,-512.64"
        },
        {
            "source": "Q14-1037",
            "target": "P16-1188",
            "d": "M806.597,-538.123C806.597,-507.626 806.597,-457.074 806.597,-422.882"
        },
        {
            "source": "P14-2133",
            "target": "P15-1030",
            "d": "M2918.65,-540.335C2930.01,-530.263 2943.38,-518.415 2955.49,-507.683"
        },
        {
            "source": "P14-1020",
            "target": "P16-1188",
            "d": "M1012.5,-540.375C969.888,-508.599 894.589,-452.447 847.817,-417.568"
        },
        {
            "source": "P14-1022",
            "target": "P18-1249",
            "d": "M3341.95,-538.883C3370.57,-484.439 3427.31,-354.615 3370.6,-269.22 3357.4,-249.34 3336.42,-235.833 3314.4,-226.662"
        },
        {
            "source": "P14-1022",
            "target": "N18-1091",
            "d": "M3360.64,-541.25C3413.05,-502.689 3514.25,-419.75 3562.6,-322.96 3574.93,-298.266 3579.24,-267.213 3580.57,-243.509"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "P15-1030",
            "target": "P18-1249",
            "d": "M3013.35,-450.011C3040.31,-420.556 3086.16,-369.339 3122.6,-322.96 3140.67,-299.958 3141.64,-291.5 3160.6,-269.22 3169.14,-259.178 3179.06,-248.9 3188.45,-239.678"
        },
        {
            "source": "P15-1030",
            "target": "2020.acl-main.557",
            "d": "M2973.36,-449.428C2954.64,-420.449 2925.89,-370.666 2914.6,-322.96 2909.1,-299.718 2904.02,-290.637 2914.6,-269.22 2926.25,-245.626 2948.87,-256.252 2961.6,-233.22 2991.03,-179.949 2989.75,-106.883 2985.57,-63.7979"
        },
        {
            "source": "D15-1032",
            "target": "Q17-1031",
            "d": "M1961.6,-448.383C1961.6,-417.886 1961.6,-367.334 1961.6,-333.142"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "D16-1125",
            "target": "P17-1022",
            "d": "M4593.6,-358.506C4593.6,-350.539 4593.6,-341.648 4593.6,-333.159"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "P17-2025",
            "target": "P18-1249",
            "d": "M3069.54,-273.289C3097.57,-261.29 3132.56,-246.314 3162.11,-233.667"
        },
        {
            "source": "P17-2025",
            "target": "P19-1031",
            "d": "M3033.76,-269.36C3050.1,-243.559 3078.3,-204.208 3111.6,-179.48 3133.32,-163.347 3159.84,-150.697 3184.65,-141.157"
        },
        {
            "source": "P17-2025",
            "target": "P19-1340",
            "d": "M3010.7,-269.085C3001.97,-243.802 2985.86,-205.508 2961.6,-179.48 2948.66,-165.605 2931.91,-153.899 2915.55,-144.531"
        },
        {
            "source": "P17-1076",
            "target": "N18-1091",
            "d": "M3511.56,-271.115C3522.18,-261.303 3534.63,-249.807 3546.01,-239.299"
        },
        {
            "source": "P17-1076",
            "target": "P19-1031",
            "d": "M3454.61,-271.86C3439.43,-260.368 3420.95,-246.208 3404.6,-233.22 3369.55,-205.383 3330.32,-172.861 3302.32,-149.414"
        },
        {
            "source": "P17-1076",
            "target": "2020.emnlp-main.389",
            "d": "M3484.72,-268.938C3482.12,-227.326 3471.97,-145.1 3432.6,-89.7401 3423.38,-76.7762 3410.52,-65.7678 3397.3,-56.7927"
        },
        {
            "source": "P17-1076",
            "target": "2020.acl-main.557",
            "d": "M3476.43,-269.189C3458.24,-219.262 3416.65,-112.603 3385.6,-89.7401 3362.07,-72.42 3201.94,-51.8489 3090.23,-39.333"
        },
        {
            "source": "D17-1178",
            "target": "P18-1249",
            "d": "M3253.22,-269.24C3249.2,-260.836 3244.67,-251.365 3240.38,-242.405"
        },
        {
            "source": "D17-1311",
            "target": "P19-1188",
            "d": "M4793.99,-269.099C4795.94,-245.172 4800.58,-209.022 4811.6,-179.48 4815.09,-170.1 4820.08,-160.586 4825.29,-151.923"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "P18-2075",
            "target": "P19-1340",
            "d": "M2869,-179.5C2867.24,-171.362 2865.26,-162.222 2863.37,-153.515"
        },
        {
            "source": "P18-1249",
            "target": "P19-1031",
            "d": "M3235.68,-179.5C3239.6,-171.096 3244.03,-161.625 3248.22,-152.665"
        },
        {
            "source": "P18-1249",
            "target": "2020.emnlp-main.389",
            "d": "M3179.3,-181.986C3165.23,-172.131 3151.33,-159.242 3143.6,-143.48 3133.07,-122.039 3129.48,-109.004 3143.6,-89.7401 3163.24,-62.9412 3194.53,-47.4522 3226.38,-38.5909"
        },
        {
            "source": "N18-1197",
            "target": "P19-1188",
            "d": "M4890.11,-179.972C4884.37,-171.137 4877.84,-161.078 4871.71,-151.656"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        },
        {
            "source": "P19-1340",
            "target": "P19-1031",
            "d": "M2952.66,-116.61C3015.98,-116.61 3079.31,-116.61 3142.63,-116.61"
        },
        {
            "source": "P19-1340",
            "target": "2020.acl-main.557",
            "d": "M2890.09,-91.4025C2904.6,-81.2166 2921.65,-69.2475 2937.02,-58.461"
        }
    ],
    [
        {
            "id": "2001",
            "name": "2001",
            "x": "28.5975",
            "y": "-1638.49"
        },
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "W01-1812",
            "name": "dan2001Parsing",
            "x": "1290.6",
            "y": "-1645.99"
        },
        {
            "id": "W01-1812",
            "name": "91",
            "x": "1290.6",
            "y": "-1630.99"
        },
        {
            "id": "N03-1016",
            "name": "dan2003{A}*",
            "x": "1160.6",
            "y": "-1556.25"
        },
        {
            "id": "N03-1016",
            "name": "192",
            "x": "1160.6",
            "y": "-1541.25"
        },
        {
            "id": "P09-1108",
            "name": "adam2009K-Best",
            "x": "1111.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1108",
            "name": "35",
            "x": "1111.6",
            "y": "-1002.81"
        },
        {
            "id": "P01-1044",
            "name": "dan2001Parsing",
            "x": "1124.6",
            "y": "-1645.99"
        },
        {
            "id": "P01-1044",
            "name": "45",
            "x": "1124.6",
            "y": "-1630.99"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "D08-1012",
            "name": "slav2008Coarse-to-Fine",
            "x": "1540.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1012",
            "name": "36",
            "x": "1540.6",
            "y": "-1092.55"
        },
        {
            "id": "P10-2037",
            "name": "adam2010Top-Down",
            "x": "1092.6",
            "y": "-928.071"
        },
        {
            "id": "P10-2037",
            "name": "9",
            "x": "1092.6",
            "y": "-913.071"
        },
        {
            "id": "2005",
            "name": "2005",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "W04-3201",
            "name": "ben2004Max-Margin",
            "x": "3248.6",
            "y": "-1466.51"
        },
        {
            "id": "W04-3201",
            "name": "198",
            "x": "3248.6",
            "y": "-1451.51"
        },
        {
            "id": "D08-1091",
            "name": "slav2008Sparse",
            "x": "3248.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1091",
            "name": "35",
            "x": "3248.6",
            "y": "-1092.55"
        },
        {
            "id": "P14-1022",
            "name": "david2014Less",
            "x": "3327.6",
            "y": "-569.111"
        },
        {
            "id": "P14-1022",
            "name": "43",
            "x": "3327.6",
            "y": "-554.111"
        },
        {
            "id": "D15-1032",
            "name": "jonathan2015An",
            "x": "1961.6",
            "y": "-479.371"
        },
        {
            "id": "D15-1032",
            "name": "14",
            "x": "1961.6",
            "y": "-464.371"
        },
        {
            "id": "P04-1061",
            "name": "dan2004Corpus-Based",
            "x": "3932.6",
            "y": "-1466.51"
        },
        {
            "id": "P04-1061",
            "name": "360",
            "x": "3932.6",
            "y": "-1451.51"
        },
        {
            "id": "P08-1100",
            "name": "percy2008Analyzing",
            "x": "3852.6",
            "y": "-1107.55"
        },
        {
            "id": "P08-1100",
            "name": "21",
            "x": "3852.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1083",
            "name": "taylor2010Painless",
            "x": "3919.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1083",
            "name": "168",
            "x": "3919.6",
            "y": "-913.071"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "H05-1010",
            "name": "ben2005A",
            "x": "2181.6",
            "y": "-1376.77"
        },
        {
            "id": "H05-1010",
            "name": "174",
            "x": "2181.6",
            "y": "-1361.77"
        },
        {
            "id": "P10-1147",
            "name": "john2010Discriminative",
            "x": "2432.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1147",
            "name": "25",
            "x": "2432.6",
            "y": "-913.071"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "W06-3105",
            "name": "john2006Why",
            "x": "908.597",
            "y": "-1287.03"
        },
        {
            "id": "W06-3105",
            "name": "71",
            "x": "908.597",
            "y": "-1272.03"
        },
        {
            "id": "P08-2007",
            "name": "john2008The",
            "x": "912.597",
            "y": "-1107.55"
        },
        {
            "id": "P08-2007",
            "name": "63",
            "x": "912.597",
            "y": "-1092.55"
        },
        {
            "id": "D08-1033",
            "name": "john2008Sampling",
            "x": "747.597",
            "y": "-1107.55"
        },
        {
            "id": "D08-1033",
            "name": "86",
            "x": "747.597",
            "y": "-1092.55"
        },
        {
            "id": "N10-1014",
            "name": "adam2010Unsupervised",
            "x": "2159.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1014",
            "name": "18",
            "x": "2159.6",
            "y": "-913.071"
        },
        {
            "id": "P06-1055",
            "name": "slav2006Learning",
            "x": "3494.6",
            "y": "-1287.03"
        },
        {
            "id": "P06-1055",
            "name": "727",
            "x": "3494.6",
            "y": "-1272.03"
        },
        {
            "id": "D07-1094",
            "name": "slav2007Learning",
            "x": "3624.6",
            "y": "-1197.29"
        },
        {
            "id": "D07-1094",
            "name": "13",
            "x": "3624.6",
            "y": "-1182.29"
        },
        {
            "id": "P10-1112",
            "name": "mohit2010Simple,",
            "x": "3464.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1112",
            "name": "26",
            "x": "3464.6",
            "y": "-913.071"
        },
        {
            "id": "P11-2127",
            "name": "mohit2011The",
            "x": "3464.6",
            "y": "-838.331"
        },
        {
            "id": "P11-2127",
            "name": "2",
            "x": "3464.6",
            "y": "-823.331"
        },
        {
            "id": "D12-1105",
            "name": "david2012Training",
            "x": "3164.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1105",
            "name": "8",
            "x": "3164.6",
            "y": "-733.591"
        },
        {
            "id": "N06-1014",
            "name": "percy2006Alignment",
            "x": "2414.6",
            "y": "-1287.03"
        },
        {
            "id": "N06-1014",
            "name": "392",
            "x": "2414.6",
            "y": "-1272.03"
        },
        {
            "id": "P07-1003",
            "name": "john2007Tailoring",
            "x": "2414.6",
            "y": "-1197.29"
        },
        {
            "id": "P07-1003",
            "name": "100",
            "x": "2414.6",
            "y": "-1182.29"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "D12-1079",
            "name": "david2012Transforming",
            "x": "2140.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1079",
            "name": "13",
            "x": "2140.6",
            "y": "-733.591"
        },
        {
            "id": "P07-1107",
            "name": "aria2007Unsupervised",
            "x": "384.597",
            "y": "-1197.29"
        },
        {
            "id": "P07-1107",
            "name": "127",
            "x": "384.597",
            "y": "-1182.29"
        },
        {
            "id": "D09-1120",
            "name": "aria2009Simple",
            "x": "300.597",
            "y": "-1017.81"
        },
        {
            "id": "D09-1120",
            "name": "158",
            "x": "300.597",
            "y": "-1002.81"
        },
        {
            "id": "N10-1061",
            "name": "aria2010Coreference",
            "x": "440.597",
            "y": "-928.071"
        },
        {
            "id": "N10-1061",
            "name": "119",
            "x": "440.597",
            "y": "-913.071"
        },
        {
            "id": "N07-1051",
            "name": "slav2007Improved",
            "x": "2926.6",
            "y": "-1197.29"
        },
        {
            "id": "N07-1051",
            "name": "533",
            "x": "2926.6",
            "y": "-1182.29"
        },
        {
            "id": "W08-1005",
            "name": "slav2008Parsing",
            "x": "2993.6",
            "y": "-1107.55"
        },
        {
            "id": "W08-1005",
            "name": "23",
            "x": "2993.6",
            "y": "-1092.55"
        },
        {
            "id": "N09-1026",
            "name": "john2009Efficient",
            "x": "1844.6",
            "y": "-1017.81"
        },
        {
            "id": "N09-1026",
            "name": "24",
            "x": "1844.6",
            "y": "-1002.81"
        },
        {
            "id": "N09-1063",
            "name": "adam2009Hierarchical",
            "x": "876.597",
            "y": "-1017.81"
        },
        {
            "id": "N09-1063",
            "name": "17",
            "x": "876.597",
            "y": "-1002.81"
        },
        {
            "id": "P12-2021",
            "name": "jonathan2012Robust",
            "x": "2721.6",
            "y": "-748.591"
        },
        {
            "id": "P12-2021",
            "name": "2",
            "x": "2721.6",
            "y": "-733.591"
        },
        {
            "id": "P14-1020",
            "name": "david2014Sparser,",
            "x": "1044.6",
            "y": "-569.111"
        },
        {
            "id": "P14-1020",
            "name": "16",
            "x": "1044.6",
            "y": "-554.111"
        },
        {
            "id": "P15-1030",
            "name": "greg2015Neural",
            "x": "2990.6",
            "y": "-479.371"
        },
        {
            "id": "P15-1030",
            "name": "33",
            "x": "2990.6",
            "y": "-464.371"
        },
        {
            "id": "N07-1052",
            "name": "aria2007Approximate",
            "x": "1414.6",
            "y": "-1197.29"
        },
        {
            "id": "N07-1052",
            "name": "6",
            "x": "1414.6",
            "y": "-1182.29"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "P08-1088",
            "name": "aria2008Learning",
            "x": "2669.6",
            "y": "-1107.55"
        },
        {
            "id": "P08-1088",
            "name": "259",
            "x": "2669.6",
            "y": "-1092.55"
        },
        {
            "id": "D11-1029",
            "name": "taylor2011Simple",
            "x": "1452.6",
            "y": "-838.331"
        },
        {
            "id": "D11-1029",
            "name": "11",
            "x": "1452.6",
            "y": "-823.331"
        },
        {
            "id": "N09-1069",
            "name": "percy2009Online",
            "x": "3859.6",
            "y": "-1017.81"
        },
        {
            "id": "N09-1069",
            "name": "157",
            "x": "3859.6",
            "y": "-1002.81"
        },
        {
            "id": "P09-1104",
            "name": "aria2009Better",
            "x": "2353.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1104",
            "name": "91",
            "x": "2353.6",
            "y": "-1002.81"
        },
        {
            "id": "D09-1147",
            "name": "adam2009Consensus",
            "x": "1612.6",
            "y": "-1017.81"
        },
        {
            "id": "D09-1147",
            "name": "25",
            "x": "1612.6",
            "y": "-1002.81"
        },
        {
            "id": "P13-1021",
            "name": "taylor2013Unsupervised",
            "x": "1219.6",
            "y": "-658.851"
        },
        {
            "id": "P13-1021",
            "name": "22",
            "x": "1219.6",
            "y": "-643.851"
        },
        {
            "id": "P14-2020",
            "name": "taylor2014Improved",
            "x": "1500.6",
            "y": "-569.111"
        },
        {
            "id": "P14-2020",
            "name": "13",
            "x": "1500.6",
            "y": "-554.111"
        },
        {
            "id": "N12-1004",
            "name": "david2012Fast",
            "x": "1000.6",
            "y": "-748.591"
        },
        {
            "id": "N12-1004",
            "name": "9",
            "x": "1000.6",
            "y": "-733.591"
        },
        {
            "id": "D08-1092",
            "name": "david2008Two",
            "x": "1735.6",
            "y": "-1107.55"
        },
        {
            "id": "D08-1092",
            "name": "90",
            "x": "1735.6",
            "y": "-1092.55"
        },
        {
            "id": "N10-1015",
            "name": "david2010Joint",
            "x": "1743.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1015",
            "name": "56",
            "x": "1743.6",
            "y": "-913.071"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "P09-2036",
            "name": "john2009Asynchronous",
            "x": "2048.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-2036",
            "name": "12",
            "x": "2048.6",
            "y": "-1002.81"
        },
        {
            "id": "P09-1011",
            "name": "percy2009Learning",
            "x": "4087.6",
            "y": "-1017.81"
        },
        {
            "id": "P09-1011",
            "name": "190",
            "x": "4087.6",
            "y": "-1002.81"
        },
        {
            "id": "J13-2005",
            "name": "percy2013Learning",
            "x": "4087.6",
            "y": "-658.851"
        },
        {
            "id": "J13-2005",
            "name": "54",
            "x": "4087.6",
            "y": "-643.851"
        },
        {
            "id": "N09-1008",
            "name": "alexandre2009Improved",
            "x": "1354.6",
            "y": "-1017.81"
        },
        {
            "id": "N09-1008",
            "name": "19",
            "x": "1354.6",
            "y": "-1002.81"
        },
        {
            "id": "N10-1082",
            "name": "percy2010Type-Based",
            "x": "1934.6",
            "y": "-928.071"
        },
        {
            "id": "N10-1082",
            "name": "32",
            "x": "1934.6",
            "y": "-913.071"
        },
        {
            "id": "P10-2064",
            "name": "adam2010Hierarchical",
            "x": "876.597",
            "y": "-928.071"
        },
        {
            "id": "P10-2064",
            "name": "6",
            "x": "876.597",
            "y": "-913.071"
        },
        {
            "id": "P12-1041",
            "name": "mohit2012Coreference",
            "x": "590.597",
            "y": "-748.591"
        },
        {
            "id": "P12-1041",
            "name": "36",
            "x": "590.597",
            "y": "-733.591"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "P10-2054",
            "name": "aria2010An",
            "x": "607.597",
            "y": "-928.071"
        },
        {
            "id": "P10-2054",
            "name": "8",
            "x": "607.597",
            "y": "-913.071"
        },
        {
            "id": "P10-1105",
            "name": "david2010Finding",
            "x": "1464.6",
            "y": "-928.071"
        },
        {
            "id": "P10-1105",
            "name": "23",
            "x": "1464.6",
            "y": "-913.071"
        },
        {
            "id": "W11-1916",
            "name": "jonathan2011Mention",
            "x": "383.597",
            "y": "-838.331"
        },
        {
            "id": "W11-1916",
            "name": "10",
            "x": "383.597",
            "y": "-823.331"
        },
        {
            "id": "P13-1012",
            "name": "greg2013Decentralized",
            "x": "719.597",
            "y": "-658.851"
        },
        {
            "id": "P13-1012",
            "name": "22",
            "x": "719.597",
            "y": "-643.851"
        },
        {
            "id": "D13-1203",
            "name": "greg2013Easy",
            "x": "905.597",
            "y": "-658.851"
        },
        {
            "id": "D13-1203",
            "name": "126",
            "x": "905.597",
            "y": "-643.851"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "D13-1027",
            "name": "jonathan2013Error-Driven",
            "x": "383.597",
            "y": "-658.851"
        },
        {
            "id": "D13-1027",
            "name": "25",
            "x": "383.597",
            "y": "-643.851"
        },
        {
            "id": "P11-1027",
            "name": "adam2011Faster",
            "x": "4214.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1027",
            "name": "103",
            "x": "4214.6",
            "y": "-823.331"
        },
        {
            "id": "P12-1101",
            "name": "adam2012Large-Scale",
            "x": "4214.6",
            "y": "-748.591"
        },
        {
            "id": "P12-1101",
            "name": "27",
            "x": "4214.6",
            "y": "-733.591"
        },
        {
            "id": "P11-1049",
            "name": "taylor2011Jointly",
            "x": "153.597",
            "y": "-838.331"
        },
        {
            "id": "P11-1049",
            "name": "165",
            "x": "153.597",
            "y": "-823.331"
        },
        {
            "id": "P16-1188",
            "name": "greg2016Learning-Based",
            "x": "806.597",
            "y": "-389.631"
        },
        {
            "id": "P16-1188",
            "name": "28",
            "x": "806.597",
            "y": "-374.631"
        },
        {
            "id": "P11-1060",
            "name": "percy2011Learning",
            "x": "4395.6",
            "y": "-838.331"
        },
        {
            "id": "P11-1060",
            "name": "315",
            "x": "4395.6",
            "y": "-823.331"
        },
        {
            "id": "N16-1181",
            "name": "jacob2016Learning",
            "x": "4395.6",
            "y": "-389.631"
        },
        {
            "id": "N16-1181",
            "name": "205",
            "x": "4395.6",
            "y": "-374.631"
        },
        {
            "id": "P11-1070",
            "name": "mohit2011Web-Scale",
            "x": "594.597",
            "y": "-838.331"
        },
        {
            "id": "P11-1070",
            "name": "41",
            "x": "594.597",
            "y": "-823.331"
        },
        {
            "id": "D13-1087",
            "name": "taylor2013Decipherment",
            "x": "1452.6",
            "y": "-658.851"
        },
        {
            "id": "D13-1087",
            "name": "5",
            "x": "1452.6",
            "y": "-643.851"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "Q14-1037",
            "name": "greg2014A",
            "x": "806.597",
            "y": "-569.111"
        },
        {
            "id": "Q14-1037",
            "name": "122",
            "x": "806.597",
            "y": "-554.111"
        },
        {
            "id": "D12-1091",
            "name": "taylor2012An",
            "x": "2897.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1091",
            "name": "45",
            "x": "2897.6",
            "y": "-733.591"
        },
        {
            "id": "P19-1340",
            "name": "nikita2019Multilingual",
            "x": "2855.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1340",
            "name": "9",
            "x": "2855.6",
            "y": "-105.41"
        },
        {
            "id": "D12-1096",
            "name": "jonathan2012Parser",
            "x": "4511.6",
            "y": "-748.591"
        },
        {
            "id": "D12-1096",
            "name": "47",
            "x": "4511.6",
            "y": "-733.591"
        },
        {
            "id": "P13-2018",
            "name": "jonathan2013An",
            "x": "4511.6",
            "y": "-658.851"
        },
        {
            "id": "P13-2018",
            "name": "11",
            "x": "4511.6",
            "y": "-643.851"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "W14-1607",
            "name": "jacob2014Grounding",
            "x": "4543.6",
            "y": "-569.111"
        },
        {
            "id": "W14-1607",
            "name": "10",
            "x": "4543.6",
            "y": "-554.111"
        },
        {
            "id": "D15-1138",
            "name": "jacob2015Alignment-Based",
            "x": "4543.6",
            "y": "-479.371"
        },
        {
            "id": "D15-1138",
            "name": "17",
            "x": "4543.6",
            "y": "-464.371"
        },
        {
            "id": "P14-2133",
            "name": "jacob2014How",
            "x": "2891.6",
            "y": "-569.111"
        },
        {
            "id": "P14-2133",
            "name": "36",
            "x": "2891.6",
            "y": "-554.111"
        },
        {
            "id": "P18-1249",
            "name": "nikita2018Constituency",
            "x": "3223.6",
            "y": "-210.15"
        },
        {
            "id": "P18-1249",
            "name": "22",
            "x": "3223.6",
            "y": "-195.15"
        },
        {
            "id": "N18-1091",
            "name": "david2018What{'}s",
            "x": "3580.6",
            "y": "-210.15"
        },
        {
            "id": "N18-1091",
            "name": "3",
            "x": "3580.6",
            "y": "-195.15"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "2020.acl-main.557",
            "name": "nikita2020Tetra-Tagging:",
            "x": "2980.6",
            "y": "-30.6701"
        },
        {
            "id": "2020.acl-main.557",
            "name": "???",
            "x": "2980.6",
            "y": "-15.6701"
        },
        {
            "id": "Q17-1031",
            "name": "jonathan2017Parsing",
            "x": "1961.6",
            "y": "-299.89"
        },
        {
            "id": "Q17-1031",
            "name": "4",
            "x": "1961.6",
            "y": "-284.89"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "D16-1125",
            "name": "jacob2016Reasoning",
            "x": "4593.6",
            "y": "-389.631"
        },
        {
            "id": "D16-1125",
            "name": "36",
            "x": "4593.6",
            "y": "-374.631"
        },
        {
            "id": "P17-1022",
            "name": "jacob2017Translating",
            "x": "4593.6",
            "y": "-299.89"
        },
        {
            "id": "P17-1022",
            "name": "???",
            "x": "4593.6",
            "y": "-284.89"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "P17-2025",
            "name": "daniel2017Improving",
            "x": "3018.6",
            "y": "-299.89"
        },
        {
            "id": "P17-2025",
            "name": "9",
            "x": "3018.6",
            "y": "-284.89"
        },
        {
            "id": "P19-1031",
            "name": "daniel2019Cross-Domain",
            "x": "3264.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1031",
            "name": "0",
            "x": "3264.6",
            "y": "-105.41"
        },
        {
            "id": "P17-1076",
            "name": "mitchell2017A",
            "x": "3485.6",
            "y": "-299.89"
        },
        {
            "id": "P17-1076",
            "name": "39",
            "x": "3485.6",
            "y": "-284.89"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "steven2020Unsupervised",
            "x": "3339.6",
            "y": "-30.6701"
        },
        {
            "id": "2020.emnlp-main.389",
            "name": "???",
            "x": "3339.6",
            "y": "-15.6701"
        },
        {
            "id": "D17-1178",
            "name": "mitchell2017Effective",
            "x": "3265.6",
            "y": "-299.89"
        },
        {
            "id": "D17-1178",
            "name": "9",
            "x": "3265.6",
            "y": "-284.89"
        },
        {
            "id": "D17-1311",
            "name": "jacob2017Analogs",
            "x": "4792.6",
            "y": "-299.89"
        },
        {
            "id": "D17-1311",
            "name": "3",
            "x": "4792.6",
            "y": "-284.89"
        },
        {
            "id": "P19-1188",
            "name": "david2019Pre-Learning",
            "x": "4849.6",
            "y": "-120.41"
        },
        {
            "id": "P19-1188",
            "name": "1",
            "x": "4849.6",
            "y": "-105.41"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "P18-2075",
            "name": "daniel2018Policy",
            "x": "2874.6",
            "y": "-210.15"
        },
        {
            "id": "P18-2075",
            "name": "8",
            "x": "2874.6",
            "y": "-195.15"
        },
        {
            "id": "N18-1197",
            "name": "jacob2018Learning",
            "x": "4906.6",
            "y": "-210.15"
        },
        {
            "id": "N18-1197",
            "name": "8",
            "x": "4906.6",
            "y": "-195.15"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-23.1701"
        }
    ],
    [
        "28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45",
        "1204.32,-1578.74 1194.08,-1576.05 1200.41,-1584.54 1204.32,-1578.74",
        "1168.16,-1037.59 1157.69,-1035.97 1164.86,-1043.77 1168.16,-1037.59",
        "1206.31,-1645.69 1216.31,-1642.19 1206.31,-1638.69 1206.31,-1645.69",
        "1149.51,-1589.7 1150.07,-1579.12 1143.03,-1587.04 1149.51,-1589.7",
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "1543.42,-1140.85 1540.53,-1130.66 1536.43,-1140.43 1543.42,-1140.85",
        "1051.14,-958.331 1056.5,-949.191 1046.61,-952.995 1051.14,-958.331",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "3252.1,-1141.1 3248.6,-1131.1 3245.1,-1141.1 3252.1,-1141.1",
        "3409.22,-563.682 3399.07,-566.715 3408.9,-570.675 3409.22,-563.682",
        "2020.74,-496.819 2010.15,-496.263 2018.08,-503.296 2020.74,-496.819",
        "3864,-1139.61 3858.39,-1130.61 3857.17,-1141.14 3864,-1139.61",
        "3939.73,-958.685 3932.43,-951.012 3933.35,-961.566 3939.73,-958.685",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "2441.62,-961.049 2437.07,-951.477 2434.66,-961.793 2441.62,-961.049",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "915.285,-1140.83 912.011,-1130.76 908.287,-1140.68 915.285,-1140.83",
        "779.624,-1134.85 770.313,-1129.79 774.439,-1139.55 779.624,-1134.85",
        "2174.64,-959.858 2168.74,-951.056 2167.86,-961.614 2174.64,-959.858",
        "3583.61,-1226.38 3589.94,-1217.88 3579.69,-1220.58 3583.61,-1226.38",
        "3471.1,-960.805 3466.77,-951.133 3464.12,-961.391 3471.1,-960.805",
        "3520.9,-856.845 3510.52,-854.714 3517.31,-862.852 3520.9,-856.845",
        "3188.61,-778.143 3180.39,-771.46 3182.65,-781.811 3188.61,-778.143",
        "2418.1,-1230.5 2414.6,-1220.5 2411.1,-1230.5 2418.1,-1230.5",
        "2465.9,-956.069 2456.92,-950.443 2460.43,-960.44 2465.9,-956.069",
        "2182.89,-957.94 2174.82,-951.066 2176.84,-961.467 2182.89,-957.94",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "2456.57,-958.316 2448.87,-951.037 2450.35,-961.528 2456.57,-958.316",
        "2202.1,-953.775 2192.15,-950.14 2197.66,-959.187 2202.1,-953.775",
        "2251.84,-750.473 2241.57,-753.071 2251.22,-757.446 2251.84,-750.473",
        "320.132,-1048.1 312.69,-1040.56 313.805,-1051.1 320.132,-1048.1",
        "436.478,-961.991 435.104,-951.486 429.627,-960.555 436.478,-961.991",
        "2971.21,-1139.84 2974.49,-1129.77 2965.64,-1135.59 2971.21,-1139.84",
        "1912.98,-1032.03 1902.42,-1032.81 1911.16,-1038.79 1912.98,-1032.03",
        "955.969,-1032.63 945.411,-1033.51 954.215,-1039.4 955.969,-1032.63",
        "3413.96,-863.532 3420.69,-855.352 3410.33,-857.547 3413.96,-863.532",
        "2786.65,-768.299 2776.19,-766.582 2783.3,-774.444 2786.65,-768.299",
        "3160.96,-782.297 3159.33,-771.828 3154.07,-781.028 3160.96,-782.297",
        "1060.54,-600.401 1053.6,-592.39 1054.03,-602.977 1060.54,-600.401",
        "3261.89,-588.03 3270.28,-581.567 3259.69,-581.384 3261.89,-588.03",
        "2993.39,-512.827 2990.13,-502.744 2986.39,-512.655 2993.39,-512.827",
        "1499.3,-1137.77 1505.51,-1129.18 1495.3,-1132.03 1499.3,-1137.77",
        "1181.2,-1031.83 1170.61,-1032.32 1179.19,-1038.54 1181.2,-1031.83",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "1542.87,-831.668 1532.87,-835.182 1542.88,-838.668 1542.87,-831.668",
        "3860.22,-1051.26 3857.52,-1041.02 3853.24,-1050.71 3860.22,-1051.26",
        "2281.52,-1029.41 2290.92,-1024.53 2280.52,-1022.48 2281.52,-1029.41",
        "1776.29,-1038.37 1784.93,-1032.23 1774.35,-1031.64 1776.29,-1038.37",
        "1588.18,-1050.28 1591.82,-1040.33 1582.77,-1045.84 1588.18,-1050.28",
        "1223.1,-692.213 1219.6,-682.213 1216.1,-692.213 1223.1,-692.213",
        "1547.14,-594.026 1537.31,-590.082 1542.54,-599.297 1547.14,-594.026",
        "942.236,-770.592 950.138,-763.536 939.56,-764.124 942.236,-770.592",
        "3327.21,-602.735 3324.72,-592.436 3320.24,-602.037 3327.21,-602.735",
        "3041.56,-500.129 3031.15,-498.129 3038.04,-506.181 3041.56,-500.129",
        "1745.47,-961.425 1742.43,-951.277 1738.48,-961.11 1745.47,-961.425",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "4091.1,-692.395 4087.6,-682.395 4084.1,-692.395 4091.1,-692.395",
        "2405.79,-960.27 2409.88,-950.497 2400.59,-955.586 2405.79,-960.27",
        "2220.4,-948.928 2209.84,-947.996 2217.51,-955.306 2220.4,-948.928",
        "1051.04,-768.466 1040.53,-767.075 1047.88,-774.712 1051.04,-768.466",
        "1103.76,-960.273 1098.22,-951.24 1096.92,-961.754 1103.76,-960.273",
        "1406.74,-866.753 1412.71,-857.998 1402.58,-861.121 1406.74,-866.753",
        "1932.79,-1017.51 1942.79,-1014.01 1932.79,-1010.51 1932.79,-1017.51",
        "2309.74,-1044.6 2316.98,-1036.85 2306.49,-1038.39 2309.74,-1044.6",
        "2080.38,-950.442 2089.11,-944.43 2078.54,-943.69 2080.38,-950.442",
        "1903.95,-960.033 1908.67,-950.546 1899.07,-955.021 1903.95,-960.033",
        "1021.46,-1017.51 1031.46,-1014.01 1021.46,-1010.51 1021.46,-1017.51",
        "880.098,-961.275 876.597,-951.275 873.098,-961.275 880.098,-961.275",
        "3899.53,-960.822 3902.28,-950.592 3893.75,-956.871 3899.53,-960.822",
        "483.11,-756.344 492.739,-751.923 482.455,-749.374 483.11,-756.344",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "1460.97,-870.933 1456.15,-861.5 1454.04,-871.882 1460.97,-870.933",
        "3468.1,-871.535 3464.6,-861.535 3461.1,-871.535 3468.1,-871.535",
        "2099.21,-954.603 2107.3,-947.759 2096.71,-948.066 2099.21,-954.603",
        "2082.05,-775.797 2089.75,-768.521 2079.2,-769.407 2082.05,-775.797",
        "3078.67,-761.292 3088.11,-756.475 3077.72,-754.356 3078.67,-761.292",
        "540.951,-927.771 550.951,-924.271 540.951,-920.771 540.951,-927.771",
        "408.654,-867.676 400.271,-861.197 402.784,-871.489 408.654,-867.676",
        "731.192,-691.316 725.894,-682.14 724.314,-692.617 731.192,-691.316",
        "882.017,-691.083 885.563,-681.099 876.565,-686.693 882.017,-691.083",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "387.098,-692.057 383.597,-682.057 380.098,-692.057 387.098,-692.057",
        "4218.1,-781.795 4214.6,-771.795 4211.1,-781.795 4218.1,-781.795",
        "692.002,-397.954 701.682,-393.647 691.429,-390.977 692.002,-397.954",
        "4399.1,-422.993 4395.6,-412.993 4392.1,-422.993 4399.1,-422.993",
        "595.735,-781.625 591.783,-771.795 588.742,-781.944 595.735,-781.625",
        "1262.09,-684.021 1252.02,-680.746 1257.85,-689.59 1262.09,-684.021",
        "1456.1,-692.057 1452.6,-682.057 1449.1,-692.057 1456.1,-692.057",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "677.374,-689.002 683.674,-680.484 673.437,-683.214 677.374,-689.002",
        "742.746,-573.014 752.265,-568.363 741.923,-566.063 742.746,-573.014",
        "3112.66,-775.026 3120.39,-767.783 3109.83,-768.623 3112.66,-775.026",
        "793.879,-675.578 783.291,-675.939 791.793,-682.26 793.879,-675.578",
        "870.306,-569.186 859.853,-570.911 869.1,-576.082 870.306,-569.186",
        "2815.32,-150.942 2820.53,-141.718 2810.7,-145.678 2815.32,-150.942",
        "4515.1,-692.055 4511.6,-682.055 4508.1,-692.055 4515.1,-692.055",
        "3300.78,-600.683 3304.97,-590.95 3295.63,-595.948 3300.78,-600.683",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "778.722,-599.394 783.278,-589.829 773.751,-594.466 778.722,-599.394",
        "1430.8,-591.772 1439.3,-585.45 1428.71,-585.09 1430.8,-591.772",
        "842.19,-593.178 832.384,-589.165 837.547,-598.417 842.19,-593.178",
        "1878.31,-486.102 1888.01,-481.833 1877.77,-479.123 1878.31,-486.102",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "4547.1,-512.575 4543.6,-502.575 4540.1,-512.575 4547.1,-512.575",
        "810.098,-422.837 806.597,-412.837 803.098,-422.837 810.098,-422.837",
        "2958.07,-510.074 2963.23,-500.822 2953.43,-504.835 2958.07,-510.074",
        "849.823,-414.698 839.714,-411.526 845.638,-420.31 849.823,-414.698",
        "3315.51,-223.338 3304.92,-222.991 3312.98,-229.866 3315.51,-223.338",
        "3584.07,-243.649 3580.99,-233.514 3577.08,-243.361 3584.07,-243.649",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "3191.09,-242 3195.85,-232.534 3186.22,-236.967 3191.09,-242",
        "2989.04,-63.348 2984.49,-53.7797 2982.08,-64.0967 2989.04,-63.348",
        "1965.1,-333.097 1961.6,-323.097 1958.1,-333.097 1965.1,-333.097",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "4597.1,-333.094 4593.6,-323.094 4590.1,-333.094 4597.1,-333.094",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "3163.68,-236.802 3171.5,-229.65 3160.93,-230.367 3163.68,-236.802",
        "3185.92,-144.421 3194.07,-137.651 3183.47,-137.86 3185.92,-144.421",
        "2916.98,-141.323 2906.54,-139.568 2913.61,-147.455 2916.98,-141.323",
        "3548.7,-241.57 3553.68,-232.214 3543.96,-236.428 3548.7,-241.57",
        "3304.28,-146.493 3294.37,-142.75 3299.78,-151.857 3304.28,-146.493",
        "3398.89,-53.6585 3388.59,-51.1883 3395.11,-59.5443 3398.89,-53.6585",
        "3090.57,-35.8503 3080.25,-38.2221 3089.8,-42.8073 3090.57,-35.8503",
        "3243.5,-240.828 3236.03,-233.319 3237.19,-243.85 3243.5,-240.828",
        "4828.39,-153.573 4830.75,-143.246 4822.46,-149.845 4828.39,-153.573",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "2866.76,-152.612 2861.22,-143.579 2859.92,-154.093 2866.76,-152.612",
        "3251.4,-154.12 3252.46,-143.579 3245.06,-151.157 3251.4,-154.12",
        "3227.46,-41.9274 3236.28,-36.0485 3225.72,-35.1476 3227.46,-41.9274",
        "4874.63,-149.714 4866.24,-143.235 4868.76,-153.528 4874.63,-149.714",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701",
        "3142.8,-120.11 3152.8,-116.61 3142.8,-113.11 3142.8,-120.11",
        "2939.18,-61.2189 2945.35,-52.6089 2935.16,-55.4894 2939.18,-61.2189"
    ]
]