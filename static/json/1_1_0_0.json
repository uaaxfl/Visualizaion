[
    [
        {
            "id": "2003",
            "citation_count": 45,
            "name": 45,
            "cx": 28.5975,
            "cy": -1642.19,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2004",
            "citation_count": 32,
            "name": 32,
            "cx": 28.5975,
            "cy": -1552.45,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W03-1201",
            "name": "Question Answering via {B}ayesian Inference on Lexical Relations",
            "publication_data": 2003,
            "citation": 45,
            "abstract": "Many researchers have used lexical networks and ontologies to mitigate synonymy and polysemy problems in Question Answering (QA), systems coupled with taggers, query classifiers, and answer extractors in complex and ad-hoc ways. We seek to make QA systems reproducible with shared and modest human effort, carefully separating knowledge from algorithms. To this end, we propose an aesthetically clean Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA. The factors which contribute to the efficacy of Bayesian Inferencing on lexical relations are soft word sense disambiguation, parameter smoothing which ameliorates the data sparsity problem and estimation of joint probability over words which overcomes the deficiency of naive-bayes-like approaches. Our system is superior to vector-space ranking techniques from IR, and its accuracy approaches that of the top contenders at the TREC QA tasks in recent years.",
            "cx": 168.597,
            "cy": -1642.19,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2005",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -1462.71,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W04-0853",
            "name": "A gloss-centered algorithm for disambiguation",
            "publication_data": 2004,
            "citation": 7,
            "abstract": "The task of word sense disambiguation is to assign a sense label to a word in a passage. We report our algorithms and experiments for the two tasks that we participated in viz. the task of WSD of WordNet glosses and the task of WSD of English lexical sample. For both the tasks, we explore a method of sense disambiguation through a process of xe2x80x9ccomparingxe2x80x9d the current context for a word against a repository of contextual clues or glosses for each sense of each word. We compile these glosses in two different ways for the two tasks. For the first task, these glosses are all compiled using WordNet and are of various types viz. hypernymy glosses, holonymy mixture, descriptive glosses and some hybrid mixtures of these glosses. The xe2x80x9ccomparisonxe2x80x9d could be done in a variety of ways that could include/exclude stemming, expansion of one gloss type with another gloss type, etc. The results show that the system does best when stemming is used and glosses are expanded. However, it appears that the evidence for word-senses ,accumulated through WordNet, in the form of glosses, are quite sparse. Generating dense glosses for all WordNet senses requires a massive sense tagged corpus - which is currently unavailable. Hence, as part of the English lexical sample task, we try the same approach on densely populated glosses accumulated from the training data for this task.",
            "cx": 140.597,
            "cy": -1552.45,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "bellare-etal-2004-generic",
            "name": "Generic Text Summarization Using {W}ord{N}et",
            "publication_data": 2004,
            "citation": 25,
            "abstract": "This paper presents a WordNet based approach to text summarization. The document to be summarized is used to extract a xe2x80x9crelevantxe2x80x9d sub-graph from the WordNet graph. Weights are assigned to each node of this sub-graph using a strategy similar to the Google Pageranking algorithm. These weights capture the relevance of the respective synsets with respect to the whole document. A matrix in which each row repesents a sentence and each column a node of the sub-graph (i.e., a synset) is created. Principal Component Analysis is performed on this matrix to help extract the sentences for the summary. Our approach is generic unlike most previous approaches which address specific genres of documents like news articles and biographies. Testing our system on the standard DUC2002 extracts shows that our results are promising and comparable to existing summarizers.",
            "cx": 306.597,
            "cy": -1552.45,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2006",
            "citation_count": 60,
            "name": 60,
            "cx": 28.5975,
            "cy": -1372.97,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2005.mtsummit-papers.14",
            "name": "Semantically Relatable Sets: Building Blocks for Representing Semantics",
            "publication_data": 2005,
            "citation": "???",
            "abstract": "None",
            "cx": 171.597,
            "cy": -1462.71,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2007",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -1283.23,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P06-2100",
            "name": "Morphological Richness Offsets Resource Demand {--} Experiences in Constructing a {POS} Tagger for {H}indi",
            "publication_data": 2006,
            "citation": 60,
            "abstract": "In this paper we report our work on building a POS tagger for a morphologically rich language- Hindi. The theme of the research is to vindicate the stand that- if morphology is strong and harnessable, then lack of training corpora is not debilitating. We establish a methodology of POS tagging which the resource disadvantaged (lacking annotated corpora) languages can make use of. The methodology makes use of locally annotated modestly-sized corpora (15,562 words), exhaustive morpohological analysis backed by high-coverage lexicon and a decision tree based learning algorithm (CN2). The evaluation of the system was done with 4-fold cross validation of the corpora in the news domain (www.bbc.co.uk/hindi). The current accuracy of POS tagging is 93.45% and can be further improved.",
            "cx": 4586.6,
            "cy": -1372.97,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I08-1067",
            "name": "Simple Syntactic and Morphological Processing Can Help {E}nglish-{H}indi Statistical Machine Translation",
            "publication_data": 2008,
            "citation": 61,
            "abstract": "In this paper, we report our work on incorporating syntactic and morphological information for English to Hindi statistical machine translation. Two simple and computationally inexpensive ideas have proven to be surprisingly effective: (i) reordering the English source sentence as per Hindi syntax, and (ii) using the suffixes of Hindi words. The former is done by applying simple transformation rules on the English parse tree. The latter, by using a simple suffix separation program. With only a small amount of bilingual training data and limited tools for Hindi, we achieve reasonable performance and substantial improvements over the baseline phrase-based system. Our approach eschews the use of parsing or other sophisticated linguistic tools for the target language (Hindi) making it a useful framework for statistical machine translation from English to Indian languages in general, since such tools are not widely available for Indian languages currently.",
            "cx": 2231.6,
            "cy": -1193.49,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C10-2040",
            "name": "Verbs are where all the action lies: Experiences of Shallow Parsing of a Morphologically Rich Language",
            "publication_data": 2010,
            "citation": 13,
            "abstract": "Verb suffixes and verb complexes of morphologically rich languages carry a lot of information. We show that this information if harnessed for the task of shallow parsing can lead to dramatic improvements in accuracy for a morphologically rich language- Marathi. The crux of the approach is to use a powerful morphological analyzer backed by a high coverage lexicon to generate rich features for a CRF based sequence classifier. Accuracy figures of 94% for Part of Speech Tagging and 97% for Chunking using a modestly sized corpus (20K words) vindicate our claim that for morphologically rich languages linguistic insight can obviate the need for large amount of annotated corpora.",
            "cx": 6493.6,
            "cy": -1014.01,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2008",
            "citation_count": 150,
            "name": 150,
            "cx": 28.5975,
            "cy": -1193.49,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2007.mtsummit-papers.56",
            "name": "{H}indi generation from interlingua",
            "publication_data": 2007,
            "citation": "???",
            "abstract": "None",
            "cx": 5482.6,
            "cy": -1283.23,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2009",
            "citation_count": 77,
            "name": 77,
            "cx": 28.5975,
            "cy": -1103.75,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "sankaran-etal-2008-common",
            "name": "A Common Parts-of-Speech Tagset Framework for {I}ndian Languages",
            "publication_data": 2008,
            "citation": 29,
            "abstract": "We present a universal Parts-of-Speech (POS) tagset framework covering most of the Indian languages (ILs) following the hierarchical and decomposable tagset schema. In spite of significant number of speakers, there is no workable POS tagset and tagger for most ILs, which serve as fundamental building blocks for NLP research. Existing IL POS tagsets are often designed for a specific language; the few that have been designed for multiple languages cover only shallow linguistic features ignoring linguistic richness and the idiosyncrasies. The new framework that is proposed here addresses these deficiencies in an efficient and principled manner. We follow a hierarchical schema similar to that of EAGLES and this enables the framework to be flexible enough to capture rich features of a language/ language family, even while capturing the shared linguistic structures in a methodical way. The proposed common framework further facilitates the sharing and reusability of scarce resources in these languages and ensures cross-linguistic compatibility.",
            "cx": 7299.6,
            "cy": -1193.49,
            "rx": 72.6644,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "mohanty-bhattacharyya-2008-lexical",
            "name": "Lexical Resources for Semantics Extraction",
            "publication_data": 2008,
            "citation": 1,
            "abstract": "In this paper, we report our work on the creation of a number of lexical resources that are crucial for an interlingua based MT from English to other languages. These lexical resources are in the form of sub-categorization frames, verb knowledge bases and rule templates for establishing semantic relations and speech act like attributes. We have created these resources over a long period of time from Oxford Advanced Learners\u00c2\u0092 Dictionary (OALD) [1], VerbNet [2], Princeton WordNet 2.1 [3], LCS database [4], Penn Tree Bank [5], and XTAG lexicon [6]. On the challenging problem of generating interlingua from domain and structure unrestricted English sentences, we are able to demonstrate that the use of these lexical resources makes a difference in terms of accuracy figures.",
            "cx": 7866.6,
            "cy": -1193.49,
            "rx": 75.8212,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I08-7013",
            "name": "Designing a Common {POS}-Tagset Framework for {I}ndian Languages",
            "publication_data": 2008,
            "citation": 8,
            "abstract": "Research in Parts-of-Speech (POS) tagset design for European and East Asian languages started with a mere listing of important morphosyntactic features in one language and has matured in later years towards hierarchical tagsets, decomposable tags, common framework for multiple languages (EAGLES) etc. Several tagsets have been developed in these languages along with large amount of annotated data for furthering research. Indian Languages (ILs) present a contrasting picture with very little research in tagset design issues. We present our work in designing a common POS-tagset framework for ILs, which is the result of in-depth analysis of eight languages from two major families, viz. Indo-Aryan and Dravidian. Our framework follows hierarchical tagset layout similar to the EAGLES guidelines, but with significant changes as needed for the ILs.",
            "cx": 7104.6,
            "cy": -1193.49,
            "rx": 103.889,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I08-6009",
            "name": "{H}indi and {M}arathi to {E}nglish Cross Language Information Retrieval",
            "publication_data": 2008,
            "citation": 2,
            "abstract": "None",
            "cx": 8845.6,
            "cy": -1193.49,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P09-1090",
            "name": "Case markers and Morphology: Addressing the crux of the fluency problem in {E}nglish-{H}indi {SMT}",
            "publication_data": 2009,
            "citation": 35,
            "abstract": "We report in this paper our work on accurately generating case markers and suffixes in English-to-Hindi SMT. Hindi is a relatively free word-order language, and makes use of a comparatively richer set of case markers and morphological suffixes for correct meaning representation. From our experience of large-scale English-Hindi MT, we are convinced that fluency and fidelity in the Hindi output get an order of magnitude facelift if accurate case markers and suffixes are produced. Now, the moot question is: what entity on the English side encodes the information contained in case markers and suffixes on the Hindi side? Our studies of correspondences in the two languages show that case markers and suffixes in Hindi are predominantly determined by the combination of suffixes and semantic relations on the English side. We, therefore, augment the aligned corpus of the two languages, with the correspondence of English suffixes and semantic relations with Hindi suffixes and case markers. Our results on 400 test sentences, translated using an SMT system trained on around 13000 parallel sentences, show that suffix  semantic relation xe2x86x92 case marker/suffix is a very useful translation factor, in the sense of making a significant difference to output quality as indicated by subjective evaluation as well as BLEU scores.",
            "cx": 2161.6,
            "cy": -1103.75,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W12-5906",
            "name": "Partially modelling word reordering as a sequence labelling problem",
            "publication_data": 2012,
            "citation": 3,
            "abstract": "Source side reordering has been shown to improve the performance of phrase based machine translation systems. In this work, we explore the learning of source side reordering given a training corpus of word aligned data. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space.",
            "cx": 919.597,
            "cy": -834.531,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5105",
            "name": "Supertag Based Pre-ordering in Machine Translation",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "None",
            "cx": 2088.6,
            "cy": -655.051,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5148",
            "name": "Merging Verb Senses of {H}indi {W}ord{N}et using Word Embeddings",
            "publication_data": 2014,
            "citation": 2,
            "abstract": "In this paper, we present an approach for merging fine-grained verb senses of Hindi WordNet. Senses are merged based on gloss similarity score. We explore the use of word embeddings for gloss similarity computation and compare with various WordNet based gloss similarity measures. Our results indicate that word embeddings show significant improvement over WordNet based measures. Consequently, we observe an increase in accuracy on merging fine-grained senses. Gold standard data constructed for our experiments is made available.",
            "cx": 3652.6,
            "cy": -655.051,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-3308",
            "name": "The {IIT} {B}ombay {H}indi-{E}nglish Translation System at {WMT} 2014",
            "publication_data": 2014,
            "citation": 9,
            "abstract": "In this paper, we describe our EnglishHindi and Hindi-English statistical systems submitted to the WMT14 shared task. The core components of our translation systems are phrase based (Hindi-English) and factored (English-Hindi) SMT systems. We show that the use of number, case and Tree Adjoining Grammar information as factors helps to improve English-Hindi translation, primarily by generating morphological inflections correctly. We show improvements to the translation systems using pre-procesing and post-processing components. To overcome the structural divergence between English and Hindi, we preorder the source side sentence to conform to the target language word order. Since parallel corpus is limited, many words are not translated. We translate out-of-vocabulary words and transliterate named entities in a post-processing stage. We also investigate ranking of translations from multiple systems to select the best translation.",
            "cx": 1062.6,
            "cy": -655.051,
            "rx": 72.25,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "kunchukuttan-etal-2014-shata",
            "name": "Shata-Anuvadak: Tackling Multiway Translation of {I}ndian Languages",
            "publication_data": 2014,
            "citation": 15,
            "abstract": "We present a compendium of 110 Statistical Machine Translation systems built from parallel corpora of 11 Indian languages belonging to both Indo-Aryan and Dravidian families. We analyze the relationship between translation accuracy and the language families involved. We feel that insights obtained from this analysis will provide guidelines for creating machine translation systems of specific Indian language pairs. We build phrase based systems and some extensions. Across multiple languages, we show improvements on the baseline phrase based systems using these extensions: (1) source side reordering for English-Indian language translation, and (2) transliteration of untranslated words for Indian language-Indian language translation. These enhancements harness shared characteristics of Indian languages. To stimulate similar innovation widely in the NLP community, we have made the trained models for these language pairs publicly available.",
            "cx": 606.597,
            "cy": -655.051,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W15-5950",
            "name": "Investigating the potential of post-ordering {SMT} output to improve translation quality",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Post-ordering of Statistical Machine Translation (SMT) output to correct word order errors could be a promising area of research to overcome structural divergence between language pairs. This is especially true when it is difficult to incorporate rich linguistic features into the baseline decoder. In this paper, we propose an algorithm for generating oracle reorderings of MT output. We use the oracle reorderings to empirically quantify an upper bound on improvement in translation quality through post-ordering techniques. In our study encompassing multiple language pairs, we show that significant improvement in translation quality can be obtained by applying reordering transformations on the output of the SMT system. This presents a strong case for investing effort in exploring the post-ordering problem.",
            "cx": 3425.6,
            "cy": -565.311,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6303",
            "name": "Can {SMT} and {RBMT} Improve each other{'}s Performance?- An Experiment with {E}nglish-{H}indi Translation",
            "publication_data": 2016,
            "citation": 3,
            "abstract": "None",
            "cx": 2128.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4622",
            "name": "{IITP} {E}nglish-{H}indi Machine Translation System at {WAT} 2016",
            "publication_data": 2016,
            "citation": 3,
            "abstract": "In this paper we describe the system that we develop as part of our participation in WAT 2016. We develop a system based on hierarchical phrase-based SMT for English to Hindi language pair. We perform re-ordering and augment bilingual dictionary to improve the performance. As a baseline we use a phrase-based SMT model. The MT models are fine-tuned on the development set, and the best configurations are used to report the evaluation on the test set. Experiments show the BLEU of 13.71 on the benchmark test data. This is better compared to the official baseline BLEU score of 10.79.",
            "cx": 2320.6,
            "cy": -475.571,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1485",
            "name": "Synset Ranking of {H}indi {W}ord{N}et",
            "publication_data": 2016,
            "citation": 1,
            "abstract": "Word Sense Disambiguation (WSD) is one of the open problems in the area of natural language processing. Various supervised, unsupervised and knowledge based approaches have been proposed for automatically determining the sense of a word in a particular context. It has been observed that such approaches often find it difficult to beat the WordNet First Sense (WFS) baseline which assigns the sense irrespective of context. In this paper, we present our work on creating the WFS baseline for Hindi language by manually ranking the synsets of Hindi WordNet. A ranking tool is developed where human experts can see the frequency of the word senses in the sense-tagged corpora and have been asked to rank the senses of a word by using this information and also his/her intuition. The accuracy of WFS baseline is tested on several standard datasets. F-score is found to be 60{\\%}, 65{\\%} and 55{\\%} on Health, Tourism and News datasets respectively. The created rankings can also be used in other NLP applications viz., Machine Translation, Information Retrieval, Text Summarization, etc.",
            "cx": 3886.6,
            "cy": -475.571,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I17-2048",
            "name": "Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based {SMT}",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "We investigate pivot-based translation between related languages in a low resource, phrase-based SMT setting. We show that a subword-level pivot-based SMT model using a related pivot language is substantially better than word and morpheme-level pivot models. It is also highly competitive with the best direct translation model, which is encouraging as no direct source-target training corpus is used. We also show that combining multiple related language pivot models can rival a direct translation model. Thus, the use of subwords as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus.",
            "cx": 1056.6,
            "cy": -385.831,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1548",
            "name": "The {IIT} {B}ombay {E}nglish-{H}indi Parallel Corpus",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "None",
            "cx": 3150.6,
            "cy": -296.09,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N19-1387",
            "name": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages",
            "publication_data": 2019,
            "citation": 6,
            "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality in extremely low-resource scenarios.",
            "cx": 1631.6,
            "cy": -206.35,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C08-2007",
            "name": "{H}indi Compound Verbs and their Automatic Extraction",
            "publication_data": 2008,
            "citation": 19,
            "abstract": "We analyse Hindi complex predicates and propose linguistic tests for their detection. This analysis enables us to identify a category of VV complex predicates called lexical compound verbs (LCpdVs) which need to be stored in the dictionary. Based on the linguistic analysis, a simple automatic method has been devised for extracting LCpdVs from corpora. We achieve an accuracy of around 98% in this task. The LCpdVs thus extracted may be used to automatically augment lexical resources like wordnets, an otherwise time consuming and labourintensive process",
            "cx": 9439.6,
            "cy": -1193.49,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L16-1369",
            "name": "Multiword Expressions Dataset for {I}ndian Languages",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "Multiword Expressions (MWEs) are used frequently in natural languages, but understanding the diversity in MWEs is one of the open problem in the area of Natural Language Processing. In the context of Indian languages, MWEs play an important role. In this paper, we present MWEs annotation dataset created for Indian languages viz., Hindi and Marathi. We extract possible MWE candidates using two repositories: 1) the POS-tagged corpus and 2) the IndoWordNet synsets. Annotation is done for two types of MWEs: compound nouns and light verb constructions. In the process of annotation, human annotators tag valid MWEs from these candidates based on the standard guidelines provided to them. We obtained 3178 compound nouns and 2556 light verb constructions in Hindi and 1003 compound nouns and 2416 light verb constructions in Marathi using two repositories mentioned before. This created resource is made available publicly and can be used as a gold standard for Hindi and Marathi MWE systems.",
            "cx": 9439.6,
            "cy": -475.571,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C08-1068",
            "name": "{H}indi {U}rdu Machine Transliteration using Finite-State Transducers",
            "publication_data": 2008,
            "citation": 30,
            "abstract": "Finite-state Transducers (FST) can be very efficient to implement inter-dialectal transliteration. We illustrate this on the Hindi and Urdu language pair. FSTs can also be used for translation between surface-close languages. We introduce UIT (universal intermediate transcription) for the same pair on the basis of their common phonetic repository in such a way that it can be extended to other languages like Arabic, Chinese, English, French, etc. We describe a transliteration model based on FST and UIT, and evaluate it on Hindi and Urdu corpora.",
            "cx": 9761.6,
            "cy": -1193.49,
            "rx": 84.7059,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W09-3536",
            "name": "A Hybrid Model for {U}rdu {H}indi Transliteration",
            "publication_data": 2009,
            "citation": 20,
            "abstract": "We report in this paper a novel hybrid approach for Urdu to Hindi transliteration that combines finite-state machine (FSM) based techniques with statistical word language model based approach. The output from the FSM is filtered with the word language model to produce the correct Hindi output. The main problem handled is the case of omission of diacritical marks from the input Urdu text. Our system produces the correct Hindi output even when the crucial information in the form of diacritic marks is absent. The approach improves the accuracy of the transducer-only approach from 50.7% to 79.1%. The results reported show that performance can be improved using a word language model to disambiguate the output produced by the transducer-only approach, especially when diacritic marks are not present in the Urdu input.",
            "cx": 9698.6,
            "cy": -1103.75,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C10-2091",
            "name": "Finite-state Scriptural Translation",
            "publication_data": 2010,
            "citation": 2,
            "abstract": "We use robust and fast Finite-State Machines (FSMs) to solve scriptural translation problems. We describe a phonetico-morphotactic pivot UIT (universal intermediate transcription), based on the common phonetic repository of Indo-Pak languages. It is also extendable to other language groups. We describe a finite-state scriptural translation model based on finite-state transducers and UIT. We report its performance on Hindi, Urdu, Punjabi and Seraiki corpora. For evaluation, we design two classification scales based on the word and sentence accuracies for translation system classifications. We also show that subjective evaluations are vital for real life usage of a translation system in addition to objective evaluations.",
            "cx": 9564.6,
            "cy": -1014.01,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2010.jeptalnrecital-court.7",
            "name": "Weak Translation Problems {--} a case study of Scriptural Translation",
            "publication_data": 2010,
            "citation": 0,
            "abstract": "General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.",
            "cx": 9816.6,
            "cy": -1014.01,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W13-4706",
            "name": "{U}rdu {H}indi Machine Transliteration using {SMT}",
            "publication_data": 2013,
            "citation": 3,
            "abstract": "Transliteration is a process of transcribing a word of the source language into the target language such that when the native speaker of the target language pronounces it, it sounds as the native pronunciation of the source word. Statistical techniques have brought significant advances and have made real progress in various fields of Natural Language Processing (NLP). In this paper, we have analysed the application of Statistical Machine Translation (SMT) for solving the problem of Urdu Hindi transliteration using a parallel lexicon. We have designed total 24 Statistical Transliteration (ST) systems by combining different types of alignments, translation models and target language models. We have performed total 576 experiments and have reported significant results. From Hindixe2x80x93toxe2x80x93Urdu transliteration, we have achieved the maximum word-level accuracy of 71.5%. From Urduxe2x80x93toxe2x80x93Hindi transliteration, the maximum word-level accuracy is 77.8% when the input Urdu text contains all necessary diacritical marks and 77% when the input Urdu text does not contain all necessary diacritical marks. At character-level, transliteration accuracy is more than 90% in both directions.",
            "cx": 9816.6,
            "cy": -744.791,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2010",
            "citation_count": 115,
            "name": 115,
            "cx": 28.5975,
            "cy": -1014.01,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W09-3518",
            "name": "Improving Transliteration Accuracy Using Word-Origin Detection and Lexicon Lookup",
            "publication_data": 2009,
            "citation": 8,
            "abstract": "We propose a framework for transliteration which uses (i) a word-origin detection engine (pre-processing) (ii) a CRF based transliteration engine and (iii) a re-ranking model based on lexicon-lookup (post-processing). The results obtained for English-Hindi and English-Kannada transliteration show that the preprocessing and post-processing modules improve the top-1 accuracy by 7.1%.",
            "cx": 10068.6,
            "cy": -1103.75,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5914",
            "name": "Solving Data Sparsity by Morphology Injection in Factored {SMT}",
            "publication_data": 2015,
            "citation": 3,
            "abstract": "None",
            "cx": 2139.6,
            "cy": -565.311,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D09-1048",
            "name": "Projecting Parameters for Multilingual Word Sense Disambiguation",
            "publication_data": 2009,
            "citation": 14,
            "abstract": "We report in this paper a way of doing Word Sense Disambiguation (WSD) that has its origin in multilingual MT and that is cognizant of the fact that parallel corpora, wordnets and sense annotated corpora are scarce resources. With respect to these resources, languages show different levels of readiness; however a more resource fortunate language can help a less resource fortunate language. Our WSD method can be applied to a language even when no sense tagged corpora for that language is available. This is achieved by projecting wordnet and corpus parameters from another language to the language in question. The approach is centered around a novel synset based multilingual dictionary and the empirical observation that within a domain the distribution of senses remains more or less invariant across languages. The effectiveness of our approach is verified by doing parameter projection and then running two different WSD algorithms. The accuracy values of approximately 75% (F1-score) for three languages in two different domains establish the fact that within a domain it is possible to circumvent the problem of scarcity of resources by projecting parameters like sense distributions, corpus-co-occurrences, conceptual distance, etc. from one language to another.",
            "cx": 4073.6,
            "cy": -1103.75,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C10-1063",
            "name": "Value for Money: Balancing Annotation Effort, Lexicon Building and Accuracy for Multilingual {WSD}",
            "publication_data": 2010,
            "citation": 7,
            "abstract": "Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on multilingual WSD has shown that it is possible to leverage the annotation work done for WSD of one language (SL) for another (TL), by projecting Wordnet and sense marked corpus parameters of SL to TL. However, this work does not take into account the cost of manually cross-linking the words within aligned synsets. Further, it does not answer the question of Can better accuracy be achieved if a user is willing to pay additional money? We propose a measure for cost-benefit analysis which measures the value for money earned in terms of accuracy by investing in annotation effort and lexicon building. Two key ideas explored in this paper are (i) the use of probabilistic cross-linking model to reduce manual cross-linking effort and (ii) the use of selective sampling to inject a few training examples for hard-to-disambiguate words from the target language to boost the accuracy.",
            "cx": 4020.6,
            "cy": -1014.01,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-1057",
            "name": "Together We Can: Bilingual Bootstrapping for {WSD}",
            "publication_data": 2011,
            "citation": 9,
            "abstract": "Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.",
            "cx": 4055.6,
            "cy": -924.271,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I11-1078",
            "name": "It Takes Two to Tango: A Bilingual Unsupervised Approach for Estimating Sense Distributions using Expectation Maximization",
            "publication_data": 2011,
            "citation": 4,
            "abstract": "Several bilingual WSD algorithms which exploit translation correspondences between parallel corpora have been proposed. However, the availability of such parallel corpora itself is a tall task for some of the resource constrained languages of the world. We propose an unsupervised bilingual EM based algorithm which relies on the counts of translations to estimate sense distributions. No parallel or sense annotated corpora are needed. The algorithm relies on a synset-aligned bilingual dictionary and in-domain corpora from the two languages. A symmetric generalized Expectation Maximization formulation is used wherein the sense distributions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised ap",
            "cx": 3883.6,
            "cy": -924.271,
            "rx": 62.4516,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.57",
            "name": "Mapping it differently: A solution to the linking challenges",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "This paper reports the work of creating bilingual mappings in English for certain synsets of Hindi wordnet, the need for doing this, the methods adopted and the tools created for the task. Hindi wordnet, which forms the foundation for other Indian language wordnets, has been linked to the English WordNet. To maximize linkages, an important strategy of using direct and hypernymy linkages has been followed. However, the hypernymy linkages were found to be inadequate in certain cases and posed a challenge due to sense granularity of language. Thus, the idea of creating bilingual mappings was adopted as a solution. A bilingual mapping means a linkage between a concept in two different languages, with the help of translation and/or transliteration. Such mappings retain meaningful representations, while capturing semantic similarity at the same time. This has also proven to be a great enhancement of Hindi wordnet and can be a crucial resource for multilingual applications in natural language processing, including machine translation and cross language information retrieval.",
            "cx": 4357.6,
            "cy": -475.571,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2011",
            "citation_count": 108,
            "name": 108,
            "cx": 28.5975,
            "cy": -924.271,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-4001",
            "name": "Word Sense Disambiguation and {IR}",
            "publication_data": 2010,
            "citation": 1,
            "abstract": "None",
            "cx": 10067.6,
            "cy": -1014.01,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-4011",
            "name": "More Languages, More {MAP}?: A Study of Multiple Assisting Languages in Multilingual {PRF}",
            "publication_data": 2010,
            "citation": 0,
            "abstract": "Multilingual Pseudo-Relevance Feedback (MultiPRF) is a framework to improve the PRF of a source language by taking the help of another language called assisting language. In this paper, we extend the MultiPRF framework to include multiple assisting languages. We consider three different configurations to incorporate multiple assisting languages - a) Parallel - all assisting languages combined simultaneously b) Serial - assisting languages combined in sequence one after another and c) Selective - dynamically selecting the best feedback model for each query. We study their effect on MultiPRF performance. Results using multiple assisting languages are mixed and it helps in boosting MultiPRF accuracy only in some cases. We also observe that MultiPRF becomes more robust with increase in number of assisting languages.",
            "cx": 1392.6,
            "cy": -1014.01,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-3604",
            "name": "A Paradigm-Based Finite State Morphological Analyzer for {M}arathi",
            "publication_data": 2010,
            "citation": 12,
            "abstract": "A morphological analyzer forms the foundation for many NLP applications of Indian Languages. In this paper, we propose and evaluate the morphological analyzer for Marathi, an inflectional language. The morphological analyzer exploits the efficiency and flexibility offered by finite state machines in modeling the morphotactics while using the well devised system of paradigms to handle the stem alternations intelligently by exploiting the regularity in inflectional forms. We plug the morphological analyzer with statistical pos tagger and chunker to see its impact on their performance so as to confirm its usability as a foundation for NLP applications.",
            "cx": 10250.6,
            "cy": -1014.01,
            "rx": 69.0935,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W12-5020",
            "name": "Error tracking in search engine development",
            "publication_data": 2012,
            "citation": 1,
            "abstract": "In this paper, we describe a tool that allows one to track the output of every module of a search engine. The tool provides the ability to perform pseudo error-correction by allowing the user to modify these outputs or tune parameters of the modules to check for improvement of results. Often it is important to see if certain surface level changes can help in the improvement of the result quality. This is crucial since it saves the immediate need to make changes in the system in terms of resource updation or development efforts. We describe query processing pipeline in sufficient detail and then show the efficacy of our tool for an example in Marathi along with giving a thorough error analysis for the example considered. We believe this paper will establish that such a tool is of significant importance for instant detection and correction of errors along with giving the readers an idea on how to develop the same.",
            "cx": 10052.6,
            "cy": -834.531,
            "rx": 81.135,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-2023",
            "name": "Morphological Analyzer for Affix Stacking Languages: A Case Study of {M}arathi",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "In this paper we describe and evaluate a Finite State Machine (FSM) based Morphological Analyzer (MA) for Marathi, a highly inflectional language with agglutinative su ffixes. Marathi belongs to the Indo-European family and is considerably influenced by Dravidian languages. Adroit handling of participial constructions and other derived forms ( Krudantas and Taddhitas) in addition to inflected forms is crucial to NLP and MT of Marathi. We firs t describe Marathi morphological phenomena, detailing the complexities of inflectional and derivational morphology, and then go into the construction and working of the MA. The MA produces the root word and the features. A thorough evaluation against gold standard data establish es the efficacy of this MA. To the best of our knowledge, this work is t he first of its kind on a systematic and exhaustive study of the Morphotactics of a suffix-stac king language, leading to high quality morph analyzer. The system forms part of a Marathi -Hindi transfer based machine translation system. The methodology delineated in the paper can be replicated fo r other languages showing similar suffix stacking behaviour as Marathi.",
            "cx": 10250.6,
            "cy": -834.531,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5103",
            "name": "Tackling Close Cousins: Experiences In Developing Statistical Machine Translation Systems For {M}arathi And {H}indi",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "None",
            "cx": 10376.6,
            "cy": -655.051,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W10-3607",
            "name": "Hybrid Stemmer for {G}ujarati",
            "publication_data": 2010,
            "citation": 20,
            "abstract": "In this paper we present a lightweight stemmer for Gujarati using a hybrid approach. Instead of using a completely unsupervised approach, we have harnessed linguistic knowledge in the form of a hand-crafted Gujarati suffix list in order to improve the quality of the stems and suffixes learnt during the training phase. We used the EMILLE corpus for training and evaluating the stemmerxe2x80x99s performance. The use of hand-crafted suffixes boosted the accuracy of our stemmer by about 17% and helped us achieve an accuracy of 67.86 %.",
            "cx": 10484.6,
            "cy": -1014.01,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W11-3001",
            "name": "Hybrid Inflectional Stemmer and Rule-based Derivational Stemmer for {G}ujarati",
            "publication_data": 2011,
            "citation": 24,
            "abstract": "In this paper we present two stemmers for Gujaratia lightweight inflectional stemmer based on a hybrid approach and a heavyweight derivational stemmer based on a rule-based approach. Besides using a module for unsupervised learning of stems and suffixes for lightweight stemming, we have also included a module performing POS (Part Of Speech) based stemming and a module using a set of substitution rules, in order to improve the quality of these stems and suffixes. The inclusion of these modules boosted the accuracy of the inflectional stemmer by 9.6% and 12.7% respectively, helping us achieve an accuracy of 90.7%. The maximum index compression obtained for the inflectional stemmer is about 95%. On the other hand, the derivational stemmer is completely rule-based, for which, we attained an accuracy of 70.7% with the help of suffix-stripping, substitution and orthographic rules. Both these systems were developed to be useful in applications such as Information Retrieval, corpus compression, dictionary search and as pre-processing modules in other NLP problems such as WSD.",
            "cx": 10484.6,
            "cy": -924.271,
            "rx": 79.8062,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W10-2418",
            "name": "Think Globally, Apply Locally: Using Distributional Characteristics for {H}indi Named Entity Identification",
            "publication_data": 2010,
            "citation": 5,
            "abstract": "In this paper, we present a novel approach for Hindi Named Entity Identification (NEI) in a large corpus. The key idea is to harness the global distributional characteristics of the words in the corpus. We show that combining the global distributional characteristics along with the local context information improves the NEI performance over statistical baseline systems that employ only local context. The improvement is very significant (about 10%) in scenarios where the test and train corpus belong to different genres. We also propose a novel measure for NEI based on term informativeness and show that it is competitive with the best measure and better than other well known information measures.",
            "cx": 10685.6,
            "cy": -1014.01,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S10-1028",
            "name": "{OWNS}: Cross-lingual Word Sense Disambiguation Using Weighted Overlap Counts and {W}ordnet Based Similarity Measures",
            "publication_data": 2010,
            "citation": 7,
            "abstract": "We report here our work on English French Cross-lingual Word Sense Disambiguation where the task is to find the best French translation for a target English word depending on the context in which it is used. Our approach relies on identifying the nearest neighbors of the test sentence from the training data using a pairwise similarity measure. The proposed measure finds the affinity between two sentences by calculating a weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over the French translations of the nearest neighbors.",
            "cx": 10869.6,
            "cy": -1014.01,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S10-1094",
            "name": "{CFILT}: Resource Conscious Approaches for All-Words Domain Specific {WSD}",
            "publication_data": 2010,
            "citation": 4,
            "abstract": "We describe two approaches for All-words Word Sense Disambiguation on a Specific Domain. The first approach is a knowledge based approach which extracts domain-specific largest connected components from the Wordnet graph by exploiting the semantic relations between all candidate synsets appearing in a domain-specific untagged corpus. Given a test word, disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components.n n The second approach is a weakly supervised approach which relies on the One Sense Per Domain heuristic and uses a few hand labeled examples for the most frequently appearing words in the target domain. Once the most frequent words have been disambiguated they can provide strong clues for disambiguating other words in the sentence using an iterative disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain.",
            "cx": 11065.6,
            "cy": -1014.01,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1137",
            "name": "Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages",
            "publication_data": 2010,
            "citation": 8,
            "abstract": "In a previous work of ours Chinnakotla et al. (2010) we introduced a novel framework for Pseudo-Relevance Feedback (PRF) called MultiPRF. Given a query in one language called Source, we used English as the Assisting Language to improve the performance of PRF for the source language. MulitiPRF showed remarkable improvement over plain Model Based Feedback (MBF) uniformly for 4 languages, viz., French, German, Hungarian and Finnish with English as the assisting language. This fact inspired us to study the effect of any source-assistant pair on MultiPRF performance from out of a set of languages with widely different characteristics, viz., Dutch, English, Finnish, French, German and Spanish. Carrying this further, we looked into the effect of using two assisting languages together on PRF.n n The present paper is a report of these investigations, their results and conclusions drawn therefrom. While performance improvement on MultiPRF is observed whatever the assisting language and whatever the source, observations are mixed when two assisting languages are used simultaneously. Interestingly, the performance improvement is more pronounced when the source and assisting languages are closely related, e.g., French and Spanish.",
            "cx": 1162.6,
            "cy": -1014.01,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N15-1125",
            "name": "Leveraging Small Multilingual Corpora for {SMT} Using Many Pivot Languages",
            "publication_data": 2015,
            "citation": 10,
            "abstract": "We present our work on leveraging multilingual parallel corpora of small sizes for Statistical Machine Translation between Japanese and Hindi using multiple pivot languages. In our setting, the source and target part of the corpus remains the same, but we show that using several different pivot to extract phrase pairs from these source and target parts lead to large BLEU improvements. We focus on a variety of ways to exploit phrase tables generated using multiple pivots to support a direct source-target phrase table. Our main method uses the Multiple Decoding Paths (MDP) feature of Moses, which we empirically verify as the best compared to the other methods we used. We compare and contrast our various results to show that one can overcome the limitations of small corpora by using as many pivot languages as possible in a multilingual setting. Most importantly, we show that such pivoting aids in learning of additional phrase pairs which are not learned when the direct sourcetarget corpus is small. We obtained improvements of up to 3 BLEU points using multiple pivots for Japanese to Hindi translation compared to when only one pivot is used. To the best of our knowledge, this work is also the first of its kind to attempt the simultaneous utilization of 7 pivot languages at decoding time.",
            "cx": 1137.6,
            "cy": -565.311,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P10-1155",
            "name": "All Words Domain Adapted {WSD}: Finding a Middle Ground between Supervision and Unsupervision",
            "publication_data": 2010,
            "citation": 19,
            "abstract": "In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Accuracy figures close to self domain training lend credence to the viability of our approach. Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.",
            "cx": 2689.6,
            "cy": -1014.01,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-2096",
            "name": "Neighbors Help: Bilingual Unsupervised {WSD} Using Context",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modification to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.",
            "cx": 3410.6,
            "cy": -744.791,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0124",
            "name": "Graph Based Algorithm for Automatic Domain Segmentation of {W}ord{N}et",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "We present a graph based algorithm for automatic domain segmentation of Wordnet. We pose the problem as a Markov Random Field Classification problem and show how existing graph based algorithms for Image Processing can be used to solve the problem. Our approach is unsupervised and can be easily adopted for any language. We conduct our experiments for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology ex-",
            "cx": 2560.6,
            "cy": -655.051,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5908",
            "name": "Using Word Embeddings for Bilingual Unsupervised {WSD}",
            "publication_data": 2015,
            "citation": 1,
            "abstract": "None",
            "cx": 3623.6,
            "cy": -565.311,
            "rx": 77.1494,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5945",
            "name": "Using Multilingual Topic Models for Improved Alignment in {E}nglish-{H}indi {MT}",
            "publication_data": 2015,
            "citation": 1,
            "abstract": "None",
            "cx": 2450.6,
            "cy": -565.311,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1349",
            "name": "That{'}ll Do Fine!: A Coarse Lexical Resource for {E}nglish-{H}indi {MT}, Using Polylingual Topic Models",
            "publication_data": 2016,
            "citation": 1,
            "abstract": "Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel {`}sentential{'} approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its benefit to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages.",
            "cx": 2520.6,
            "cy": -475.571,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.54",
            "name": "High, Medium or Low? Detecting Intensity Variation Among polar synonyms in {W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "For fine-grained sentiment analysis, we need to go beyond zero-one polarity and find a way to compare adjectives (synonyms) that share the same sense. Choice of a word from a set of synonyms, provides a way to select the exact polarity-intensity. For example, choosing to describe a person as benevolent rather than kind1 changes the intensity of the expression. In this paper, we present a sense based lexical resource, where synonyms are assigned intensity levels, viz., high, medium and low. We show that the measure P (s|w) (probability of a sense s given the word w) can derive the intensity of a word within the sense. We observe a statistically significant positive correlation between P(s|w) and intensity of synonyms for three languages, viz., English, Marathi and Hindi. The average correlation scores are 0.47 for English, 0.56 for Marathi and 0.58 for Hindi.",
            "cx": 2709.6,
            "cy": -475.571,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N10-1065",
            "name": "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages",
            "publication_data": 2010,
            "citation": 17,
            "abstract": "Most state of the art approaches for machine transliteration are data driven and require significant parallel names corpora between languages. As a result, developing transliteration functionality among n languages could be a resource intensive task requiring parallel names corpora in the order of nC2. In this paper, we explore ways of reducing this high resource requirement by leveraging the available parallel data between subsets of the n languages, transitively. We propose, and show empirically, that reasonable quality transliteration engines may be developed between two languages, X and Y, even when no direct parallel names data exists between them, but only transitively through language Z. Such systems alleviate the need for O(nC2) corpora, significantly. In addition we show that the performance of such transitive transliteration systems is in par with direct transliteration systems, in practical applications, such as CLIR systems.",
            "cx": 1892.6,
            "cy": -1014.01,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "khapra-etal-2014-transliteration",
            "name": "When Transliteration Met Crowdsourcing : An Empirical Study of Transliteration via Crowdsourcing using Efficient, Non-redundant and Fair Quality Control",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "Sufficient parallel transliteration pairs are needed for training state of the art transliteration engines. Given the cost involved, it is often infeasible to collect such data using experts. Crowdsourcing could be a cheaper alternative, provided that a good quality control (QC) mechanism can be devised for this task. Most QC mechanisms employed in crowdsourcing are aggressive (unfair to workers) and expensive (unfair to requesters). In contrast, we propose a low-cost QC mechanism which is fair to both workers and requesters. At the heart of our approach, lies a rule based Transliteration Equivalence approach which takes as input a list of vowels in the two languages and a mapping of the consonants in the two languages. We empirically show that our approach outperforms other popular QC mechanisms ({\\textbackslash}textit{viz.}, consensus and sampling) on two vital parameters : (i) fairness to requesters (lower cost per correct transliteration) and (ii) fairness to workers (lower rate of rejecting correct answers). Further, as an extrinsic evaluation we use the standard NEWS 2010 test set and show that such quality controlled crowdsourced data compares well to expert data when used for training a transliteration engine.",
            "cx": 1830.6,
            "cy": -655.051,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "K16-1027",
            "name": "Substring-based unsupervised transliteration with phonetic and contextual knowledge",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 1828.6,
            "cy": -475.571,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "Q18-1022",
            "name": "Leveraging Orthographic Similarity for Multilingual Neural Transliteration",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "We address the task of joint training of transliteration models for multiple language pairs (multilingual transliteration). This is an instance of multitask learning, where individual tasks (language pairs) benefit from sharing knowledge with related tasks. We focus on transliteration involving related tasks i.e., languages sharing writing systems and phonetic properties (orthographically similar languages). We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity. We show that multilingual transliteration significantly outperforms bilingual transliteration in different scenarios (average increase of 58{\\%} across a variety of languages we experimented with). We also show that multilingual transliteration models can generalize well to languages/language pairs not encountered during training and hence perform well on the zeroshot transliteration task. We show that further improvements can be achieved by using phonetic feature input.",
            "cx": 1727.6,
            "cy": -296.09,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "bhattacharyya-2010-indowordnet",
            "name": "{I}ndo{W}ord{N}et",
            "publication_data": 2010,
            "citation": "???",
            "abstract": "India is a multilingual country where machine translation and cross lingual search are highly relevant problems. These problems require large resources- like wordnets and lexicons- of high quality and coverage. Wordnets are lexical structures composed of synsets and semantic relations. Synsets are sets of synonyms. They are linked by semantic relations like hypernymy (is-a), meronymy (part-of), troponymy (manner-of) etc. IndoWordnet is a linked structure of wordnets of major Indian languages from Indo-Aryan, Dravidian and Sino-Tibetan families. These wordnets have been created by following the expansion approach from Hindi wordnet which was made available free for research in 2006. Since then a number of Indian languages have been creating their wordnets. In this paper we discuss the methodology, coverage, important considerations and multifarious benefits of IndoWordnet. Case studies are provided for Marathi, Sanskrit, Bodo and Telugu, to bring out the basic methodology of and challenges involved in the expansion approach. The guidelines the lexicographers follow for wordnet construction are enumerated. The difference between IndoWordnet and EuroWordnet also is discussed.",
            "cx": 4838.6,
            "cy": -1014.01,
            "rx": 148.485,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W12-5209",
            "name": "Domain Specific Ontology Extractor For {I}ndian Languages",
            "publication_data": 2012,
            "citation": 8,
            "abstract": "We present a k-partite graph learning algorithm for ontology extraction from unstructured text. The algorithm divides the initial set of terms into different partitions based on information content of the terms and then constructs ontology by detecting subsumption relation between terms in different partitions. This approach not only reduces the amount of computation required for ontology construction but also provides an additional level of term filtering. The experiments are conducted for Hindi and English and the performance is evaluated by comparing resulting ontology with manually constructed ontology for Health domain. We observe that our approach significantly improves the precision. The proposed approach does not require sophisticated NLP tools such as NER and parser and can be easily adopted for any language.",
            "cx": 4329.6,
            "cy": -834.531,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-2008",
            "name": "Cross-Lingual Sentiment Analysis for {I}ndian Languages using Linked {W}ord{N}ets",
            "publication_data": 2012,
            "citation": 38,
            "abstract": "Cross-Lingual Sentiment Analysis (CLSA) is the task of predicting the polarity of the opinion expressed in a text in a language Ltest using a classifier trained on the corpus of another language Lt rain. Popular approaches use Machine Translation (MT) to convert the test document in Ltest to Lt rain and use the classifier of Lt rain. However, MT systems do not exist for most pairs of languages and even if they do, their translation accuracy is low. So we present an alternative approach to CLSA using WordNet senses as features for supervised sentiment classification. A document in Ltest is tested for polarity through a classifier trained on sense marked and polarity labeled corpora of Lt rain. The crux of the idea is to use the linked WordNets of two languages to bridge the language gap. We report our results on two widely spoken Indian languages, Hindi (450 million speakers) and Marathi (72 million speakers), which do not have an MT system between them. The sense-based approach gives a CLSA accuracy of 72% and 84% for Hindi and Marathi sentiment classification respectively. This is an improvement of 14%-15% over an approach that uses a bilingual dictionary.",
            "cx": 4993.6,
            "cy": -834.531,
            "rx": 126.644,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L18-1728",
            "name": "{I}ndian Language Wordnets and their Linkages with {P}rinceton {W}ord{N}et",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 4333.6,
            "cy": -296.09,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018.gwc-1.31",
            "name": "Semi-automatic {W}ord{N}et Linking using Word Embeddings",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of wordnets, which link similar concepts in wordnets of different languages. Such resources are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these resources are considered as gold standard/oracle. Thus, it is crucial that these resources hold correct information. Thereby, they are created by human experts. However, manual maintenance of such resources is a tedious and costly affair. Thus techniques that can aid the experts are desirable. In this paper, we propose an approach to link wordnets. Given a synset of the source language, the approach returns a ranked list of potential candidate synsets in the target language from which the human expert can choose the correct one(s). Our technique is able to retrieve a winner synset in the top 10 ranked list for 60{\\%} of all synsets and 70{\\%} of noun synsets.",
            "cx": 4557.6,
            "cy": -296.09,
            "rx": 112.36,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W19-7509",
            "name": "Introduction to {S}anskrit Shabdamitra: An Educational Application of {S}anskrit {W}ordnet",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "None",
            "cx": 8037.6,
            "cy": -206.35,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W15-5905",
            "name": "Noun Phrase Chunking for {M}arathi using Distant Supervision",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Information Extraction from Indian languages requires effective shallow parsing, especially identification of xe2x80x9cmeaningfulxe2x80x9d noun phrases. Particularly, for an agglutinative and free word order language like Marathi, this problem is quite challenging. We model this task of extracting noun phrases as a sequence labelling problem. A Distant Supervision framework is used to automatically create a large labelled data for training the sequence labelling model. The framework exploits a set of heuristic rules based on corpus statistics for the automatic labelling. Our approach puts together the benefits of heuristic rules, a large unlabelled corpus as well as supervised learning to model complex underlying characteristics of noun phrase occurrences. In comparison to a simple English-like chunking baseline and a publicly available Marathi Shallow Parser, our method demonstrates a better performance.",
            "cx": 7871.6,
            "cy": -565.311,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "ar-etal-2012-cost",
            "name": "Cost and Benefit of Using {W}ord{N}et Senses for Sentiment Analysis",
            "publication_data": 2012,
            "citation": 4,
            "abstract": "Typically, accuracy is used to represent the performance of an NLP system. However, accuracy attainment is a function of investment in annotation. Typically, the more the amount and sophistication of annotation, higher is the accuracy. However, a moot question is ''''''``is the accuracy improvement commensurate with the cost incurred in annotation''''''''? We present an economic model to assess the marginal benefit accruing from increase in cost of annotation. In particular, as a case in point we have chosen the sentiment analysis (SA) problem. In SA, documents normally are polarity classified by running them through classifiers trained on document vectors constructed from lexeme features, i.e., words. If, however, instead of words, one uses word senses (synset ids in wordnets) as features, the accuracy improves dramatically. But is this improvement significant enough to justify the cost of annotation? This question, to the best of our knowledge, has not been investigated with the seriousness it deserves. We perform a cost benefit study based on a vendor-machine model. By setting up a cost price, selling price and profit scenario, we show that although extra cost is incurred in sense annotation, the profit margin is high, justifying the cost.",
            "cx": 4524.6,
            "cy": -834.531,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2012",
            "citation_count": 147,
            "name": 147,
            "cx": 28.5975,
            "cy": -834.531,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W11-1717",
            "name": "Robust Sense-based Sentiment Classification",
            "publication_data": 2011,
            "citation": 9,
            "abstract": "The new trend in sentiment classification is to use semantic features for representation of documents. We propose a semantic space based on WordNet senses for a supervised document-level sentiment classifier. Not only does this show a better performance for sentiment classification, it also opens opportunities for building a robust sentiment classifier. We examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. Using three popular similarity metrics, we replace unknown synsets in the test set with a similar synset from the training set. An improvement of 6.2% is seen with respect to baseline using this approach.",
            "cx": 10682.6,
            "cy": -924.271,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P11-4022",
            "name": "{C}-Feel-It: A Sentiment Analyzer for Micro-blogs",
            "publication_data": 2011,
            "citation": 25,
            "abstract": "Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.",
            "cx": 5850.6,
            "cy": -924.271,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C12-1113",
            "name": "Sentiment Analysis in {T}witter with Lightweight Discourse Analysis",
            "publication_data": 2012,
            "citation": 51,
            "abstract": "We propose a lightweight method for using discourse relations for polarity detection of tweets . This method is targeted towards the web-based appli cations that deal with noisy, unstructured text, like the tweets, and cannot afford to use heavy linguistic resource s like parsing due to frequent failure of the parsers to handle noisy dat a. Most of the works in micro-blogs, like Twitter, use a bag-of-words model that ignores the discours e particles like but, since, although etc. In this work, we show how the discourse relations like the connectives and conditionals can be used to incorporate discourse information in any bag-of-words model, to improve sentiment classification accuracy. We also probe the influenc e of the semantic operators like modals and negations on the discourse relations that affect the sentime nt of a sentence. Discourse relations and corresponding rules are identified with minimal processing - just a list look up. We first give a linguistic description of the various discourse r elations which leads to conditions in rules and features in SVM. We show that our discourse-based bag-of-words model performs well in a noisy medium ( Twitter ), where it performs better than an existing Twitte r-based application. Furthermore, we show that our approach is beneficia l to structured reviews as well, where we achieve a better accuracy than a state-of-the-art s ystem in the travel review domain. Our system compares favorably with the state-of-the-art system s and has the additional attractiveness of being less resource intensive.",
            "cx": 5920.6,
            "cy": -834.531,
            "rx": 112.36,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "S13-2082",
            "name": "{IITB}-Sentiment-Analysts: Participation in Sentiment Analysis in {T}witter {S}em{E}val 2013 Task",
            "publication_data": 2013,
            "citation": 13,
            "abstract": "We propose a method for using discourse relations for polarity detection of tweets. We have focused on unstructured and noisy text like tweets on which linguistic tools like parsers and POS-taggers donxe2x80x99t work properly. We have showed how conjunctions, connectives, modals and conditionals affect the sentiments in tweets. We have also handled the commonly used abbreviations, slangs and collocations which are usually used in short text messages like tweets. This work focuses on a Web based application which produces results in real time. This approach is an extension of the previous work (Mukherjee et al. 2012).",
            "cx": 5812.6,
            "cy": -744.791,
            "rx": 161.855,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "N13-1088",
            "name": "More than meets the eye: Study of Human Cognition in Sense Annotation",
            "publication_data": 2013,
            "citation": 12,
            "abstract": "Word Sense Disambiguation (WSD) approaches have reported good accuracies in recent years. However, these approaches can be classified as weak AI systems. According to the classical definition, a strong AI based WSD system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings. In order to accomplish this, a detailed understanding of the human techniques employed for sense disambiguation is necessary. Instead of building yet another WSD system that uses contextual evidence for sense disambiguation, as has been done before, we have taken a step back - we have endeavored to discover the cognitive faculties that lie at the very core of the human sense disambiguation technique. In this paper, we present a hypothesis regarding the cognitive sub-processes involved in the task of WSD. We support our hypothesis using the experiments conducted through the means of an eye-tracking device. We also strive to find the levels of difficulties in annotating various classes of words, with senses. We believe, once such an in-depth analysis is performed, numerous insights can be gained to develop a robust WSD system that conforms to the principle of strong AI.",
            "cx": 5489.6,
            "cy": -744.791,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "I11-1152",
            "name": "Clause-Based Reordering Constraints to Improve Statistical Machine Translation",
            "publication_data": 2011,
            "citation": 11,
            "abstract": "We demonstrate that statistical machine translation (SMT) can be improved substantially by imposing clause-based reordering constraints during decoding. Our analysis of clause-wise translation of different types of clauses shows that it is beneficial to apply these constraints for finite clauses, but not for non-finite clauses. In our experiments in English-Hindi translation with an SMT system (DTM2), on a test corpus containing around 850 sentences with manually annotated clause boundaries, BLEU improves to 20.4 from the baseline score of 19.4. This statistically significant improvement is also confirmed by subjective (human) evaluation. We also report preliminary work on automatically identifying the kind of clause boundaries appropriate for enforcing reordering constraints.",
            "cx": 10947.6,
            "cy": -924.271,
            "rx": 147.571,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D11-1100",
            "name": "Harnessing {W}ord{N}et Senses for Supervised Sentiment Classification",
            "publication_data": 2011,
            "citation": 26,
            "abstract": "Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-of-speech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline.",
            "cx": 5186.6,
            "cy": -924.271,
            "rx": 113.689,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P13-1041",
            "name": "The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis",
            "publication_data": 2013,
            "citation": 20,
            "abstract": "Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.",
            "cx": 4375.6,
            "cy": -744.791,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W14-2619",
            "name": "Dive deeper: Deep Semantics for Sentiment Analysis",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "This paper illustrates the use of deep semantic processing for sentiment analysis. Existing methods for sentiment analysis use supervised approaches which take into account all the subjective words and or phrases. Due to this, the fact that not all of these words and phrases actually contribute to the overall sentiment of the text is ignored. We propose an unsupervised rule-based approach using deep semantic processing to identify only relevant subjective terms. We generate a UNL (Universal Networking Language) graph for the input text. Rules are applied on the graph to extract relevant terms. The sentiment expressed in these terms is used to figure out the overall sentiment of the text. Results on binary sentiment classification have shown promising results.",
            "cx": 5760.6,
            "cy": -655.051,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5920",
            "name": "Domain Sentiment Matters: A Two Stage Sentiment Analyzer",
            "publication_data": 2015,
            "citation": 2,
            "abstract": "There are words that change its polarity from domain to domain. For example, the word deadly is of positive polarity in the cricket domain as in xe2x80x9cShane Warne is a xe2x80x98deadlyxe2x80x99 leg spinnerxe2x80x9d. However, xe2x80x98I witnessed a deadly accidentxe2x80x99 carries negative polarity and going by the sentiment in cricket domain will be misleading. In addition to this, there exist domainspecific words, which have the same polarity across domains, but are used very frequently in a particular domain. For example, blockbuster, is specific to the movie domain. We combine such words as Domain Dedicated Polar Words (DDPW). A concise feature set made up of principal polarity clues makes the classifier less expensive in terms of time complexity and enhances the accuracy of classification. In this paper, we show that DDPW make such a concise feature set for sentiment analysis in a domain. Use of domaindedicated polar words as features beats the state of art accuracies achieved independently with unigrams, adjectives or Universal Sentiment Lexicon (USL).",
            "cx": 2937.6,
            "cy": -565.311,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6315",
            "name": "Meaning Matters: Senses of Words are More Informative than Words for Cross-domain Sentiment Analysis",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 5344.6,
            "cy": -475.571,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1429",
            "name": "Aspect based Sentiment Analysis in {H}indi: Resource Creation and Evaluation",
            "publication_data": 2016,
            "citation": 15,
            "abstract": "Due to the phenomenal growth of online product reviews, sentiment analysis (SA) has gained huge attention, for example, by online service providers. A number of benchmark datasets for a wide range of domains have been made available for sentiment analysis, especially in resource-rich languages. In this paper we assess the challenges of SA in Hindi by providing a benchmark setup, where we create an annotated dataset of high quality, build machine learning models for sentiment analysis in order to show the effective usage of the dataset, and finally make the resource available to the community for further advancement of research. The dataset comprises of Hindi product reviews crawled from various online sources. Each sentence of the review is annotated with aspect term and its associated sentiment. As classification algorithms we use Conditional Random Filed (CRF) and Support Vector Machine (SVM) for aspect term extraction and sentiment analysis, respectively. Evaluation results show the average F-measure of 41.07{\\%} for aspect term extraction and accuracy of 54.05{\\%} for sentiment classification.",
            "cx": 4894.6,
            "cy": -475.571,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "K16-1016",
            "name": "Leveraging Cognitive Features for Sentiment Analysis",
            "publication_data": 2016,
            "citation": 9,
            "abstract": "None",
            "cx": 5582.6,
            "cy": -475.571,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P17-1035",
            "name": "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
            "publication_data": 2017,
            "citation": 14,
            "abstract": "Cognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
            "cx": 5651.6,
            "cy": -385.831,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2013",
            "citation_count": 107,
            "name": 107,
            "cx": 28.5975,
            "cy": -744.791,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W12-5806",
            "name": "Textbook Construction from Lecture Transcripts",
            "publication_data": 2012,
            "citation": 0,
            "abstract": "None",
            "cx": 10505.6,
            "cy": -834.531,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W12-5018",
            "name": "Building Multilingual Search Index using open source framework",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "This paper presents a comparison of open source search engine development frameworks in the context of their malleability for constructing multilingual search index. The comparison study reveals that none of these frameworks are designed for this task. This paper elicits the challenges involved in building a multilingual index. We also discuss policy decisions and the implementation changes made to an open source framework for building such an index. As a main contribution of this work, we propose an architecture that can be used for building multilingual index. It also lists some of the open research challenges involved.",
            "cx": 10706.6,
            "cy": -834.531,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W12-4906",
            "name": "A heuristic-based approach for systematic error correction of gaze data for reading",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "In eye-tracking research, temporally constant deviations between usersxe2x80x99 intended gaze location and location captured by eye-samplers are referred to as systematic error. Systematic errors are frequent and add a lot of noise to the data. It also takes a lot of time and effort to manually correct such disparities. In this paper, we propose and validate a heuristic-based technique to reduce such errors associated with gaze fixations by shifting them to their true locations. This technique is exclusively applicable for reading tasks where the visual objects (characters) are placed on a grid in a sequential manner; which is often the case in psycholinguistic studies.",
            "cx": 6138.6,
            "cy": -834.531,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2062",
            "name": "Automatically Predicting Sentence Translation Difficulty",
            "publication_data": 2013,
            "citation": 14,
            "abstract": "In this paper we introduce Translation Difficulty Index (TDI), a measure of difficulty in text translation. We first define and quantify translation difficulty in terms of TDI. We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism. We, rather, rely on cognitive evidences from eye tracking. TDI is measured as the sum of fixation (gaze) and saccade (rapid eye movement) times of the eye. We then establish that TDI is correlated with three properties of the input sentence, viz. length (L), degree of polysemy (DP) and structural complexity (SC). We train a Support Vector Regression (SVR) system to predict TDIs for new sentences using these features as input. The prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of and TDI pairs for a set of sentences. The primary use of our work is a way of xe2x80x9cbinningxe2x80x9d sentences (to be translated) in xe2x80x9ceasyxe2x80x9d, xe2x80x9cmediumxe2x80x9d and xe2x80x9chardxe2x80x9d categories as per their predicted TDI. This can decide pricing of any translation task, especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing. This can also provide a way of monitoring progress of second language learners.",
            "cx": 6138.6,
            "cy": -744.791,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "S12-1098",
            "name": "janardhan: Semantic Textual Similarity using Universal Networking Language graph matching",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "Sentences that are syntactically quite different can often have similar or same meaning. The SemEval 2012 task of Semantic Textual Similarity aims at finding the semantic similarity between two sentences. The semantic representation of Universal Networking Language (UNL), represents only the inherent meaning in a sentence without any syntactic details. Thus, comparing the UNL graphs of two sentences can give an insight into how semantically similar the two sentences are. This paper presents the UNL graph matching method for the Semantic Textual Similarity(STS) task.",
            "cx": 10914.6,
            "cy": -834.531,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "kunchukuttan-etal-2012-experiences",
            "name": "Experiences in Resource Generation for Machine Translation through Crowdsourcing",
            "publication_data": 2012,
            "citation": 9,
            "abstract": "The logistics of collecting resources for Machine Translation (MT) has always been a cause of concern for some of the resource deprived languages of the world. The recent advent of crowdsourcing platforms provides an opportunity to explore the large scale generation of resources for MT. However, before venturing into this mode of resource collection, it is important to understand the various factors such as, task design, crowd motivation, quality control, etc. which can influence the success of such a crowd sourcing venture. In this paper, we present our experiences based on a series of experiments performed. This is an attempt to provide a holistic view of the different facets of translation crowd sourcing and identifying key challenges which need to be addressed for building a practical crowdsourcing solution for MT.",
            "cx": 3721.6,
            "cy": -834.531,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-4030",
            "name": "{T}rans{D}oop: A Map-Reduce based Crowdsourced Translation for Complex Domain",
            "publication_data": 2013,
            "citation": 2,
            "abstract": "Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowdxe2x80x99s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.",
            "cx": 3721.6,
            "cy": -744.791,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D12-1012",
            "name": "Towards Efficient Named-Entity Rule Induction for Customizability",
            "publication_data": 2012,
            "citation": 6,
            "abstract": "Generic rule-based systems for Information Extraction (IE) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization. However, it is generally recognized that manually building and customizing rules is a complex and labor intensive process. In this paper, we discuss an approach that facilitates the process of building customizable rules for Named-Entity Recognition (NER) tasks via rule induction, in the Annotation Query Language (AQL). Given a set of basic features and an annotated document collection, our goal is to generate an initial set of rules with reasonable accuracy, that are interpretable and thus can be easily refined by a human developer. We present an efficient rule induction process, modeled on a four-stage manual rule development process and present initial promising results with our system. We also propose a simple notion of extractor complexity as a first step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure.",
            "cx": 11122.6,
            "cy": -834.531,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3013",
            "name": "Automated Paradigm Selection for {FSA} based {K}onkani Verb Morphological Analyzer",
            "publication_data": 2012,
            "citation": 5,
            "abstract": "A Morphological Analyzer is a crucial tool for any language. In popular tools used to build morphological analyzers like XFST, HFST and Apertiumxe2x80x99s lttoolbox, the finite state approach is used to sequence input characters. We have used the finite state approach to sequence morphemes instead of characters. In this paper we present the architecture and implementation details of a Corpus assisted FSA approach for build ing a Verb Morphological Analyzer. Our main contribution in this paper is the paradigm def inition methodology used for the verbs in a morphologically rich Indian Language Konkani. The mapping of citation form of the verbs to paradigms was carried out using an untagged corpus for Konkani. Besides a reduction in human effort required an F-Score of 0.95 was obtained whe n the mapping was tested on a tagged corpus.",
            "cx": 11319.6,
            "cy": -834.531,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5136",
            "name": "{A}uto{P}ar{S}e: An Automatic Paradigm Selector For Nouns in {K}onkani",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "None",
            "cx": 11319.6,
            "cy": -655.051,
            "rx": 127.059,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3030",
            "name": "Eating Your Own Cooking: Automatically Linking {W}ordnet Synsets of Two Languages",
            "publication_data": 2012,
            "citation": 1,
            "abstract": "Linked wordnets are invaluable linked lexical resources. Wordnet linking involves matching a particular synset (concept) in one wordnet to a synset in another wordnet. We have developed an automatic wordnet linking system that is divided into a number of stages. Starting with a synset in the first language (also referred to as the source language), our algorithm generates a list of candidate synsets in the second language (also referred to as the target language). In consecutive stages, a heuristic is used to prune and rank this list. The winner synset is then chosen as the linkage for the source synset. The candidate synsets are generated using a bilingual dictionary (BiDict). Further, the earlier heuristics which we developed used BiDict to rank these candidate synsets. However, development of a BiDict is cumbersome and requires human labor. Furthermore, in several cases sparsity of the BiDict handicaps the ranking algorithm to a great extent. We have thus devised heuristics to eliminate the requirement of BiDict during the ranking process by using the already linked synsets. Once sufficient number of linked synsets are available, these heuristics outperform our heuristics which use a BiDict. These heuristics are based on observations made from linking techniques applied by lexicographers. Our wordnet linking system can be used for any pair of languages, given either a BiDict or sufficient number of already linked synsets. The interface of the system is easy to comprehend and use. In this paper, we present this interface along with the developed heuristics.",
            "cx": 4740.6,
            "cy": -834.531,
            "rx": 70.0071,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3031",
            "name": "{I} Can Sense It: a Comprehensive Online System for {WSD}",
            "publication_data": 2012,
            "citation": 2,
            "abstract": "We have developed an online interface for running all the current state-of-the-art algorithms for WSD. This is motivated by the fact that exhaustive comparison of a new Word Sense Disambiguation (WSD) algorithm with existing state-of-the-art algorithms is a tedious task. This impediment is due to one of the following reasons: (1) the source code of the earlier approach is not available and there is a considerable overhead in implementing it or (2) the source code/binary is available but there is some overhead in using it due to system requirements, portability issues, customization issues and software dependencies. A simple tool which has no overhead for the user and has minimal system requirements would greatly benefit the researchers. Our system currently supports 3 languages, viz., English, Hindi and Marathi, and requires only a web-browser to run. To demonstrate the usability of our system, we compare the performance of current state-of-the-art algorithms on 3 publicly available datasets.",
            "cx": 11493.6,
            "cy": -834.531,
            "rx": 58.8803,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-3033",
            "name": "Discrimination-Net for {H}indi",
            "publication_data": 2012,
            "citation": 1,
            "abstract": "Current state-of-the-art Word Sense Disambiguation (WSD) algorithms are mostly supervised and use the P (Sense|Word) statistic for annotation. This P (Sense|Word) statistic is obtained after training the model on an annotated corpus. The performance of WSD algorithms do not match the efficiency and quality of human annotation. It is therefore important to know the role of the contextual clues in WSD. Human beings in turn, actuate the task of disambiguating the sense of a word, by gathering hints from the context words in the neighbourhood of the word. Contextual clues thus form the basic building block for the human sense disambiguation task. The need was thus felt for a tool, which could help us get a deeper insight into the human mind, while disambiguating polysemous words. As mentioned earlier, in the human mind, sense disambiguation highly depends on finding clues in corpus text, which finally lead to a winner sense. In order to makeWSD algorithms more efficient, it is highly desirable to assimilate knowledge regarding contextual clues of words. In order to make WSD algorithms more efficient, it is highly desirable to assimilate knowledge regarding contextual clues of words, which aid in finding correct senses of words in that context. Hence, we developed a tool which could help a lexicographer mark the clues for disambiguating a word in a context. In the current phase, this tool lets the lexicographer select the clues from the gloss and example fields in the synset, and adds them to a database.",
            "cx": 5385.6,
            "cy": -834.531,
            "rx": 132.872,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0126",
            "name": "Do not do processing, when you can look up: Towards a Discrimination Net for {WSD}",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "The task of Word Sense Disambiguation (WSD) incorporates in its definition the role of xe2x80x98contextxe2x80x99. We present our work on the development of a tool which allows for automatic acquisition and ranking of xe2x80x98context cluesxe2x80x99 for WSD. These clue words are extracted from the contexts of words appearing in a large monolingual corpus. These mined collection of contextual clues form a discrimination net in the sense that for targeted WSD, navigation of the net leads to the correct sense of a word given its context. Utilizing this resource we intend to develop efficient and light weight WSD based on look up and navigation of memory-resident knowledge base, thereby avoiding heavy computation which often prevents incorporation of any serious WSD in MT and search. The need for large quantities of sense marked data too can be reduced.",
            "cx": 5323.6,
            "cy": -655.051,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C16-1047",
            "name": "A Hybrid Deep Learning Architecture for Sentiment Analysis",
            "publication_data": 2016,
            "citation": 20,
            "abstract": "In this paper, we propose a novel hybrid deep learning archtecture which is highly efficient for sentiment analysis in resource-poor languages. We learn sentiment embedded vectors from the Convolutional Neural Network (CNN). These are augmented to a set of optimized features selected through a multi-objective optimization (MOO) framework. The sentiment augmented optimized vector obtained at the end is used for the training of SVM for sentiment classification. We evaluate our proposed approach for coarse-grained (i.e. sentence level) as well as fine-grained (i.e. aspect level) sentiment analysis on four Hindi datasets covering varying domains. In order to show that our proposed method is generic in nature we also evaluate it on two benchmark English datasets. Evaluation shows that the results of the proposed method are consistent across all the datasets and often outperforms the state-of-art systems. To the best of our knowledge, this is the very first attempt where such a deep learning model is used for less-resourced languages such as Hindi.",
            "cx": 5034.6,
            "cy": -475.571,
            "rx": 50.41,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "C16-1287",
            "name": "Borrow a Little from your Rich Cousin: Using Embeddings and Polarities of {E}nglish Words for Multilingual Sentiment Classification",
            "publication_data": 2016,
            "citation": 5,
            "abstract": "In this paper, we provide a solution to multilingual sentiment classification using deep learning. Given input text in a language, we use word translation into English and then the embeddings of these English words to train a classifier. This projection into the English space plus word embeddings gives a simple and uniform framework for multilingual sentiment analysis. A novel idea is augmentation of the training data with polar words, appearing in these sentences, along with their polarities. This approach leads to a performance gain of 7-10{\\%} over traditional classifiers on many languages, irrespective of text genre, despite the scarcity of resources in most languages.",
            "cx": 4714.6,
            "cy": -475.571,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1053",
            "name": "Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality and Multi-Linguality",
            "publication_data": 2018,
            "citation": 4,
            "abstract": "Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the hand-crafted features for the prediction. We show the efficacy of the proposed model against state-of-the-art methods in two experimental setups i.e. multi-lingual and cross-lingual.",
            "cx": 5331.6,
            "cy": -296.09,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-0413",
            "name": "Language-Agnostic Model for Aspect-Based Sentiment Analysis",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In this paper, we propose a language-agnostic deep neural network architecture for aspect-based sentiment analysis. The proposed approach is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network, which is further assisted with extra hand-crafted features. We define three different architectures for the successful combination of word embeddings and hand-crafted features. We evaluate the proposed approach for six languages (i.e. English, Spanish, French, Dutch, German and Hindi) and two problems (i.e. aspect term extraction and aspect sentiment classification). Experiments show that the proposed model attains state-of-the-art performance in most of the settings.",
            "cx": 5231.6,
            "cy": -206.35,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C12-1114",
            "name": "{Y}ou{C}at: Weakly Supervised {Y}outube Video Categorization System from Meta Data {\\\\&} User Comments using {W}ord{N}et {\\\\&} {W}ikipedia",
            "publication_data": 2012,
            "citation": 3,
            "abstract": "In this paper, we propose a weakly supervised system, YouCat , for categorizing Youtube videos into different genres like Comedy, Horror, Romance, Sports and Technology The system takes a Youtube video url as input and gives it a belongingness score for ea ch genre. The key aspects of this work can be summarized as: (1) Unlike other ge nre identification works, which are mostly supervised, this system is mostly unsupervised, requiring no labeled data for training. (2) The system can easily incorporate new genres without re quiring labeled data for the genres. (3) YouCat extracts information from the video title , meta description and user comments (which together form the video descriptor ). (4) It uses Wikipedia and WordNet for concept expansion. (5) The proposed algorithm with a time complexity of O(|W|) (where (|W|) is the number of words in the video descriptor) is efficient to be deployed i n web for real-time video categorization. Experimentations have been performed on real world Youtube videos where YouCat achieves an F-score of 80.9% , without using any labeled training set, compared to the supervised, multiclass SVM F-score of 84.36% for single genre prediction . YouCat performs better for multi-genre prediction with an F-Score of 90.48% . Weak supervision in the system arises out of the usage of manually constructed WordNet and genre description by a few root words.",
            "cx": 11696.6,
            "cy": -834.531,
            "rx": 125.73,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2014",
            "citation_count": 101,
            "name": 101,
            "cx": 28.5975,
            "cy": -655.051,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W13-3611",
            "name": "{IITB} System for {C}o{NLL} 2013 Shared Task: A Hybrid Approach to Grammatical Error Correction",
            "publication_data": 2013,
            "citation": 5,
            "abstract": "We describe our grammar correction system for the CoNLL-2013 shared task. Our system corrects three of the five error types specified for the shared task noun-number, determiner and subject-verb agreement errors. For noun-number and determiner correction, we apply a classification approach using rich lexical and syntactic features. For subject-verb agreement correction, we propose a new rulebased system which utilizes dependency parse information and a set of conditional rules to ensure agreement of the verb group with its subject. Our system obtained an F-score of 11.03 on the official test set using the M 2 evaluation method (the official evaluation method).",
            "cx": 11603.6,
            "cy": -744.791,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-1708",
            "name": "Tuning a Grammar Correction System for Increased Precision",
            "publication_data": 2014,
            "citation": 6,
            "abstract": "In this paper, we propose two enhancements to a statistical machine translation based approach to grammar correction for correcting all error categories. First, we propose tuning the SMT systems to optimize a metric more suited to the grammar correction task (F- score) rather than the traditional BLEU metric used for tuning language translation tasks. Since the F- score favours higher precision, tuning to this score can potentially improve precision. While the results do not indicate improvement due to tuning with the new metric, we believe this could be due to the small number of grammatical errors in the tuning corpus and further investigation is required to answer the question conclusively. We also explore the combination of custom-engineered grammar correction techniques, which are targeted to specific error categories, with the SMT based method. Our simple ensemble methods yield improvements in recall but decrease the precision. Tuning the custom-built techniques can help in increasing the overall accuracy also.",
            "cx": 11548.6,
            "cy": -655.051,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5902",
            "name": "Addressing Class Imbalance in Grammatical Error Detection with Evaluation Metric Optimization",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "We address the problem of class imbalance in supervised grammatical error detection (GED) for non-native speaker text, which is the result of the low proportion of erroneous examples compared to a large number of error-free examples. Most learning algorithms maximize accuracy which is not a suitable objective for such imbalanced data. For GED, most systems address this issue by tuning hyperparameters to maximize metrics like Fxcexb2 . Instead, we show that learning classifiers that directly learn model parameters by optimizing evaluation metrics like F1 and F2 score deliver better performance on these metrics as compared to traditional sampling and cost-sensitive learning solutions for addressing class imbalance. Optimizing these metrics is useful in recall-oriented grammar error detection scenarios. We also show that there are inherent difficulties in optimizing precision-oriented evaluation metrics like F0.5. We establish this through a systematic evaluation on multiple datasets and different GED tasks.",
            "cx": 11604.6,
            "cy": -565.311,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S13-1031",
            "name": "{CFILT}-{CORE}: Semantic Textual Similarity using Universal Networking Language",
            "publication_data": 2013,
            "citation": 1,
            "abstract": "This paper describes the system that was submitted in the *SEM 2013 Semantic Textual Similarity shared task. The task aims to find the similarity score between a pair of sentences. We describe a Universal Networking Language (UNL) based semantic extraction system for measuring the semantic similarity. Our approach combines syntactic and word level similarity measures along with the UNL based semantic similarity measures for finding similarity scores between sentences.",
            "cx": 11844.6,
            "cy": -744.791,
            "rx": 139.1,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2048",
            "name": "{I}ndo{N}et: A Multilingual Lexical Knowledge Network for {I}ndian Languages",
            "publication_data": 2013,
            "citation": 3,
            "abstract": "We present IndoNet, a multilingual lexical knowledge base for Indian languages. It is a linked structure of wordnets of 18 different Indian languages, Universal Word dictionary and the Suggested Upper Merged Ontology (SUMO). We discuss various benefits of the network and challenges involved in the development. The system is encoded in Lexical Markup Framework (LMF) and we propose modifications in LMF to accommodate Universal Word Dictionary and SUMO. This standardized version of lexical knowledge base of Indian Languages can now easily be linked to similar global resources.",
            "cx": 12112.6,
            "cy": -744.791,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P14-2007",
            "name": "Measuring Sentiment Annotation Complexity of Text",
            "publication_data": 2014,
            "citation": 13,
            "abstract": "The effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise. We aim to predict a score that quantifies this effort, using linguistic properties of the text. Our proposed metric is called Sentiment Annotation Complexity (SAC). As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking. The sentences in our dataset are labeled with SAC scores derived from eye-fixation duration. Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation. We also study the correlation between a human annotatorxe2x80x99s perception of complexity and a machinexe2x80x99s confidence in polarity determination. The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction.",
            "cx": 6043.6,
            "cy": -655.051,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "P18-1219",
            "name": "Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "Predicting a reader{'}s rating of text quality is a challenging task that involves estimating different subjective aspects of the text, like structure, clarity, etc. Such subjective aspects are better handled using cognitive information. One such source of cognitive information is gaze behaviour. In this paper, we show that gaze behaviour does indeed help in effectively predicting the rating of text quality. To do this, we first we model text quality as a function of three properties - organization, coherence and cohesion. Then, we demonstrate how capturing gaze behaviour helps in predicting each of these properties, and hence the overall quality, by reporting improvements obtained by adding gaze features to traditional textual features for score prediction. We also hypothesize that if a reader has fully understood the text, the corresponding gaze behaviour would give a better indication of the assigned rating, as opposed to partial understanding. Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text.",
            "cx": 6186.6,
            "cy": -296.09,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.aacl-main.86",
            "name": "Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "The gaze behaviour of a reader is helpful in solving several NLP tasks such as automatic essay grading. However, collecting gaze behaviour from readers is costly in terms of time and money. In this paper, we propose a way to improve automatic essay grading using gaze behaviour, which is learnt at run time using a multi-task learning framework. To demonstrate the efficacy of this multi-task learning based approach to automatic essay grading, we collect gaze behaviour for 48 essays across 4 essay sets, and learn gaze behaviour for the rest of the essays, numbering over 7000 essays. Using the learnt gaze behaviour, we can achieve a statistically significant improvement in performance over the state-of-the-art system for the essay sets where we have gaze data. We also achieve a statistically significant improvement for 4 other essay sets, numbering about 6000 essays, where we have no gaze behaviour data available. Our approach establishes that learning gaze behaviour improves automatic essay grading.",
            "cx": 6181.6,
            "cy": -116.61,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P13-2149",
            "name": "Detecting Turnarounds in Sentiment Analysis: Thwarting",
            "publication_data": 2013,
            "citation": 10,
            "abstract": "Thwarting and sarcasm are two uncharted territories in sentiment analysis, the former because of the lack of training corpora and the latter because of the enormous amount of world knowledge it demands. In this paper, we propose a working definition of thwarting amenable to machine learning and create a system that detects if the document is thwarted or not. We focus on identifying thwarting in product reviews, especially in the camera domain. An ontology of the camera domain is created. Thwarting is looked upon as the phenomenon of polarity reversal at a higher level of ontology compared to the polarity expressed at the lower level. This notion of thwarting defined with respect to an ontology is novel, to the best of our knowledge. A rule based implementation building upon this idea forms our baseline. We show that machine learning with annotated corpora (thwarted/nonthwarted) is more effective than the rule based system. Because of the skewed distribution of thwarting, we adopt the Areaunder-the-Curve measure of performance. To the best of our knowledge, this is the first attempt at the difficult problem of thwarting detection, which we hope will at least provide a baseline system to compare against.",
            "cx": 6350.6,
            "cy": -744.791,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P15-2124",
            "name": "Harnessing Context Incongruity for Sarcasm Detection",
            "publication_data": 2015,
            "citation": 101,
            "abstract": "The relationship between context incongruity and sarcasm has been studied in linguistics. We present a computational system that harnesses context incongruity as a basis for sarcasm detection. Our statistical sarcasm classifiers incorporate two kinds of incongruity features: explicit and implicit. We show the benefit of our incongruity features for two text forms tweets and discussion forum posts. Our system also outperforms two past works (with Fscore improvement of 10-20%). We also show how our features can capture intersentential incongruity.",
            "cx": 6553.6,
            "cy": -565.311,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "10",
            "dasharray": null
        },
        {
            "id": "D15-1300",
            "name": "Adjective Intensity and Sentiment Analysis",
            "publication_data": 2015,
            "citation": 8,
            "abstract": "For fine-grained sentiment analysis, we need to go beyond zero-one polarity and find a way to compare adjectives that share a common semantic property. In this paper, we present a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, where adjectives are compared when they belong to the same semantic category. For example, in the semantic category of EXPERTISE, expert, experienced and familiar are respectively of level high, medium and low. We obtain an overall accuracy of 77% for intensity assignment. We show the significance of considering intensity information of adjectives in predicting star-rating of reviews. Our intensity based prediction system results in an accuracy of 59% for a 5-star rated movie review corpus.",
            "cx": 3173.6,
            "cy": -565.311,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-2623",
            "name": "A cognitive study of subjectivity extraction in sentiment annotation",
            "publication_data": 2014,
            "citation": 8,
            "abstract": "Existing sentiment analysers are weak AI systems: they try to capture the functionality of human sentiment detection faculty, without worrying about how such faculty is realized in the hardware of the human. These analysers are agnostic of the actual cognitive processes involved. This, however, does not deliver when applications demand order of magnitude facelift in accuracy, as well as insight into characteristics of sentiment detection process. In this paper, we present a cognitive study of sentiment detection from the perspective of strong AI. We study the sentiment detection process of a set of human xe2x80x9csentiment readersxe2x80x9d. Using eye-tracking, we show that on the way to sentiment detection, humans first extract subjectivity. They focus attention on a subset of sentences before arriving at the overall sentiment. This they do either through xe2x80x9danticipationxe2x80x9d where sentences are skipped during the first pass of reading, or through xe2x80x9dhomingxe2x80x9d where a subset of the sentences are read over multiple passes, or through both. xe2x80x9dHomingxe2x80x9d behaviour is also observed at the sub-sentence level in complex sentiment phenomena like sarcasm.",
            "cx": 5473.6,
            "cy": -655.051,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-1904",
            "name": "Leveraging Annotators{'} Gaze Behaviour for Coreference Resolution",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "None",
            "cx": 5859.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-2006",
            "name": "Making Headlines in {H}indi: Automatic {E}nglish to {H}indi News Headline Translation",
            "publication_data": 2013,
            "citation": 1,
            "abstract": "News headlines exhibit stylistic peculiarities. The goal of our translation engine xe2x80x98Making Headlines in Hindixe2x80x99 is to achieve automatic translation of English news headlines to Hindi while retaining the Hindi news headline styles. There are two central modules of our engine: the modified translation unit based on Moses and a co-occurrencebased post-processing unit. The modified translation unit provides two machine translation (MT) models: phrase-based and factor-based (both using in-domain data). In addition, a co-occurrence-based post-processing option may be turned on by a user. Our evaluation shows that this engine handles some linguistic phenomena observed in Hindi news headlines.",
            "cx": 2489.6,
            "cy": -744.791,
            "rx": 83.3772,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-1076",
            "name": "Detecting Domain Dedicated Polar Words",
            "publication_data": 2013,
            "citation": 13,
            "abstract": "There are many examples in which a word changes its polarity from domain to domain. For example, unpredictable is positive in the movie domain, but negative in the product domain. Such words cannot be entered in a xe2x80x9cuniversal sentiment lexiconxe2x80x9d which is supposed to be a repository of words with polarity invariant across domains. Rather, we need to maintain separate domain specific sentiment lexicons. The main contribution of this paper is to present an effective method of generating a domain specific sentiment lexicon. For a word whose domain specific polarity needs to be determined, the approach uses the Chi-Square test to detect if the difference is significant between the counts of the word in positive and negative polarity documents. We extract 274 words that are polar in the movie domain, but are not present in the universal sentiment lexicon. Our overall accuracy is around 60% in detecting movie domain specific polar words.",
            "cx": 3052.6,
            "cy": -744.791,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D17-1058",
            "name": "Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings",
            "publication_data": 2017,
            "citation": 4,
            "abstract": "Identification of intensity ordering among polar (positive or negative) words which have the same semantics can lead to a fine-grained sentiment analysis. For example, {`}master{'}, {`}seasoned{'} and {`}familiar{'} point to different intensity levels, though they all convey the same meaning (semantics), i.e., expertise: having a good knowledge of. In this paper, we propose a semi-supervised technique that uses sentiment bearing word embeddings to produce a continuous ranking among adjectives that share common semantics. Our system demonstrates a strong Spearman{'}s rank correlation of 0.83 with the gold standard ranking. We show that sentiment bearing word embeddings facilitate a more accurate intensity ranking system than other standard word embeddings (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering task.",
            "cx": 3056.6,
            "cy": -385.831,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-1089",
            "name": "Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification",
            "publication_data": 2018,
            "citation": 5,
            "abstract": "Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on \u00cf\u00872 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.",
            "cx": 2941.6,
            "cy": -296.09,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.613",
            "name": "Recommendation Chart of Domains for Cross-Domain Sentiment Analysis: Findings of A 20 Domain Study",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "Cross-domain sentiment analysis (CDSA) helps to address the problem of data scarcity in scenarios where labelled data for a domain (known as the target domain) is unavailable or insufficient. However, the decision to choose a domain (known as the source domain) to leverage from is, at best, intuitive. In this paper, we investigate text similarity metrics to facilitate source domain selection for CDSA. We report results on 20 domains (all possible pairs) using 11 similarity metrics. Specifically, we compare CDSA performance with these metrics for different domain-pairs to enable the selection of a suitable source domain, given a target domain. These metrics include two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The goal of our experiments is a recommendation chart that gives the K best source domains for CDSA for a given target domain. We show that the best K source domains returned by our similarity metrics have a precision of over 50{\\%}, for varying values of K.",
            "cx": 2787.6,
            "cy": -116.61,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-1093",
            "name": "Little by Little: Semi Supervised Stemming through Stem Set Minimization",
            "publication_data": 2013,
            "citation": 1,
            "abstract": "In this paper we take an important step towards completely unsupervised stemming by giving a scheme for semi supervised stemming. The input to the system is a list of word forms and suffixes. The motivation of the work comes from the need to create a root or stem identifier for a language that has electronic corpora and some elementary linguistic work in the form of, say, suffix list. The scope of our work is suffix based morphology, (i.e., no prefix or infix morphology). We give two greedy algorithms for stemming. We have performed extensive experimentation with four languages: English, Hindi, Malayalam and Marathi. Accuracy figures ranges from 80% to 88% are reported for all languages.",
            "cx": 12332.6,
            "cy": -744.791,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-1122",
            "name": "Automated Grammar Correction Using Hierarchical Phrase-Based Statistical Machine Translation",
            "publication_data": 2013,
            "citation": 2,
            "abstract": "We introduce a novel technique that uses hierarchical phrase-based statistical machine translation (SMT) for grammar correction. SMT systems provide a uniform platform for any sequence transformation task. Thus grammar correction can be considered a translation problem from incorrect text to correct text. Over the years, grammar correction data in the electronic form (i.e., parallel corpora of incorrect and correct sentences) has increased manifolds in quality and quantity, making SMT systems feasible for grammar correction. Firstly, sophisticated translation models like hierarchical phrase-based SMT can handle errors as complicated as reordering or insertion, which were difficult to deal with previously throuh the mediation of rule based systems. Secondly, this SMT based correction technique is similar in spirit to human correction, because the system extracts grammar rules from the corpus and later uses these rules to translate incorrect sentences to correct sentences. We describe how to use Joshua, a hierarchical phrase-based SMT system for grammar correction. An accuracy of 0.77 (BLEU score) establishes the efficacy of our approach.",
            "cx": 12538.6,
            "cy": -744.791,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I13-1131",
            "name": "Structure Cognizant Pseudo Relevance Feedback",
            "publication_data": 2013,
            "citation": 2,
            "abstract": "We propose a structure cognizant framework for pseudo relevance feedback (PRF). This has an application, for example, in selecting expansion terms for general search from subsets such as Wikipedia, wherein documents typically have a minimally fixed set of fields, viz., Title, Body, Infobox and Categories. In existing approaches to PRF based expansion, weights of expansion terms do not depend on their field(s) of origin. This, we feel, is a weakness of current PRF approaches. We propose a per field EM formulation for finding the importance of the expansion terms, in line with traditional PRF. However, the final weight of an expansion term is found by weighting these importance based on whether the term belongs to the title, the body, the infobox or the category field(s). In our experiments with four languages, viz., English, Spanish, Finnish and Hindi, we find that this structure-aware PRF yields a 2% to 30% improvement in performance (MAP) over the vanilla PRF. We conduct ablation tests to evaluate the importance of various fields. As expected, results from these tests emphasize the importance of fields in the order of title, body, categories and infobox.",
            "cx": 3985.6,
            "cy": -744.791,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2015",
            "citation_count": 233,
            "name": 233,
            "cx": 28.5975,
            "cy": -565.311,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5504",
            "name": "A Framework for Learning Morphology using Suffix Association Matrix",
            "publication_data": 2014,
            "citation": 3,
            "abstract": "Unsupervised learning of morphology is used for aut omatic affix identification, morphological segmenta tion of words and generating paradigms which give a list of all affixes that can be combined with a list of st ems. Various unsupervised approaches are used to segment words into stem and suffix. Most unsupervised methods used to learn morphology assume that suffixes occur frequently in a corpus. We have observed that for morphologically rich Indian Languages like Konkani, 31 percent of suffixes are not frequent. In this p aper we report our framework for Unsupervised Morphology Learner which works for less frequent suffixes. Less frequent suffixes can be identified using p-similar technique which has been used for suffix identific ation, but cannot be used for segmentation of short stem words . Using proposed Suffix Association Matrix, our Unsupervised Morphology Learner can also do segmentation of short stem words correctly. We tested our framework to learn derivational morphology for English and two Indian languages, namely Hindi and Konkani. Compared to other similar techniques used for segme ntation, there was an improvement in the precision and recall.",
            "cx": 11748.6,
            "cy": -655.051,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5111",
            "name": "{H}in{MA}: Distributed Morphology based {H}indi Morphological Analyzer",
            "publication_data": 2014,
            "citation": 2,
            "abstract": "Morphology plays a crucial role in the working of various NLP applications. Whenever we run a spell checker, provide a query term to a web search engine, explore translation or transliteration tools, use online dictionaries or thesauri, or try using text-to-speech or speech recognition applications, morphology works at the back of these applications. We present here a novel computational tool HinMA, or the Hindi Morphological Analyzer, based on the framework of Distributed Morphology (DM). We discuss the implementation of linguistically motivated analysis and later, we evaluate the accuracy of this tool. We find, that this rule based system exhibits extremely high accuracy and has a good overall coverage. The design of the tool is language independent and by changing few configuration files, one can use this framework for developing such a tool for other languages as well. The analysis of Hindi inflectional morphology based on the Distributed morphology framework, its implementation in the development of this tool and integration with NLP resources like Hindi Wordnet or Sense Marker Tool and possible development of a word generator are interesting aspects of this work.",
            "cx": 11927.6,
            "cy": -655.051,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5113",
            "name": "Anou Tradir: Experiences In Building Statistical Machine Translation Systems For Mauritian Languages {--} Creole, {E}nglish, {F}rench",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "We present, in this paper, our experiences in developing Statistical Machine Translation (SMT) systems involving English, French and Mauritian Creole, the languages most spoken in Mauritius. We give a brief overview of the peculiarities of the language phenomena in Mauritian Creole and indicate the differences between it and English and French. We then give descriptions of the developed corpora used for the various MT systems where we also explore the possibility of using French as a bridge language when translating from English to Creole. We evaluate these systems using the standard objective evaluation measure, BLEU. We postulate and through an error analysis, indicated by examples, verify that when English to French translations are perfect, the subsequent translation of French to Creole results in better quality translations than direct English to Creole translation.",
            "cx": 12107.6,
            "cy": -655.051,
            "rx": 62.4516,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5115",
            "name": "Introduction to Synskarta: An Online Interface for Synset Creation with Special Reference to {S}anskrit",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "None",
            "cx": 8377.6,
            "cy": -655.051,
            "rx": 117.26,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.46",
            "name": "Sam{\\\\=a}sa-Kart{\\\\=a}: An Online Tool for Producing Compound Words using {I}ndo{W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "Sam{\\=a}sa or compounds are a regular feature of Indian Languages. They are also found in other languages like German, Italian, French, Russian, Spanish, etc. Compound word is constructed from two or more words to form a single word. The meaning of this word is derived from each of the individual words of the compound. To develop a system to generate, identify and interpret compounds, is an important task in Natural Language Processing. This paper introduces a web based tool - Sam{\\=a}sa-Kart{\\=a} for producing compound words. Here, the focus is on Sanskrit language due to its richness in usage of compounds; however, this approach can be applied to any Indian language as well as other languages. IndoWordNet is used as a resource for words to be compounded. The motivation behind creating compound words is to create, to improve the vocabulary, to reduce sense ambiguity, etc. in order to enrich the WordNet. The Sam{\\=a}sa-Kart{\\=a} can be used for various applications viz., compound categorization, sandhi creation, morphological analysis, paraphrasing, synset creation, etc.",
            "cx": 8225.6,
            "cy": -475.571,
            "rx": 161.441,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W14-5124",
            "name": "A Sentiment Analyzer for {H}indi Using {H}indi Senti Lexicon",
            "publication_data": 2014,
            "citation": 4,
            "abstract": "None",
            "cx": 12250.6,
            "cy": -655.051,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-5126",
            "name": "{P}a{CM}an : Parallel Corpus Management Workbench",
            "publication_data": 2014,
            "citation": 0,
            "abstract": "We present a Parallel Corpora Management tool that aides parallel corpora generation for the task of Machine Translation (MT). It takes source and target text of a corpus for any language pair in text file format, or zip archives containing multiple corresponding text files. Then, it provides with a helpful interface to lexicographers for manual translation / validation, and gives out the corrected text files as output. It provides various dictionary references as help within the interface which increase the productivity and efficiency of a lexicographer. It also provides automatic translation of the source sentence using an integrated MT system. The tool interface includes a corpora management system which facilitates maintenance of parallel corpora by assigning roles such as manager, lexicographer etc. We have designed a novel tool that provides aides like references to various dictionary sources such as Wordnets, Shabdkosh, Wikitionary etc. We also provide manual word alignment correction which is visualized in the tool and can lead to its gamification in the future, thus, providing a valuable source of word / phrase alignments.",
            "cx": 859.597,
            "cy": -655.051,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5916",
            "name": "{T}rans{C}hat: Cross-Lingual Instant Messaging for {I}ndian Languages",
            "publication_data": 2015,
            "citation": 1,
            "abstract": "None",
            "cx": 913.597,
            "cy": -565.311,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-3350",
            "name": "{LAYERED}: Metric for Machine Translation Evaluation",
            "publication_data": 2014,
            "citation": 15,
            "abstract": "This paper describes the LAYERED metric which is used for the shared WMTxe2x80x9914 metrics task. Various metrics exist for MT evaluation: BLEU (Papineni, 2002), METEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example, in case of free word order languages. In this paper, we propose an MT evaluation scheme that is based on the NLP layers: lexical, syntactic and semantic. We contend that higher layer metrics are after all needed. Results are presented on the corpora of ACL-WMT, 2013 and 2014. We end with a metric which is composed of weighted metrics at individual layers, which correlates very well with human judgment.",
            "cx": 12454.6,
            "cy": -655.051,
            "rx": 123.073,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W14-0130",
            "name": "Facilitating Multi-Lingual Sense Annotation: Human Mediated Lemmatizer",
            "publication_data": 2014,
            "citation": 9,
            "abstract": "Sense marked corpora is essential for supervised word sense disambiguation (WSD). The marked sense ids come from wordnets. However, words in corpora appear in morphed forms, while wordnets store lemma. This situation calls for accurate lemmatizers. The lemma is the gateway to the wordnet. However, the problem is that for many languages, lemmatizers do not exist, and this problem is not easy to solve, since rule based lemmatizers take time and require highly skilled linguists.Satistical stemmers on the other hand do not return legitimate lemma. We present here a novel scheme for creating accurate lemmatizers quickly. These lemmatizers are human mediated. The key idea is that a trie is created out of the vocabulary of the language. The lemmatizing process consists in navigating the trie, trying to find a match between the input word and an entry in the trie. At the point of first mismatch, the yield of the subtree rooted at the partially matched node is output as the list of possible lemma. If the correct lemma does not appear in the listas noted by a human lexicographerbacktracking is initiated. This can output more possibilities. A ranking function filters and orders the output list of lemma. We have evaluated the performance of this human mediated lemmatizer for eighteen Indian Languages and five European languages. We have compared accuracy figures against well known lemmatizers/stemmers like Morpha, Morfessor and Snowball stemmers, and observed superior performance in all cases. Our work shows a way of speedily creating human assisted accurate lemmatizers, thereby removing a difficult roadblock in many NLP tasks, e.g., sense annotation.",
            "cx": 8706.6,
            "cy": -655.051,
            "rx": 106.547,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5910",
            "name": "{I}ndo{W}ord{N}et Dictionary: An Online Multilingual Dictionary using {I}ndo{W}ord{N}et",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "None",
            "cx": 9121.6,
            "cy": -565.311,
            "rx": 152.97,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016.gwc-1.22",
            "name": "Sophisticated Lexical Databases - Simplified Usage: Mobile Applications and Browser Plugins For Wordnets",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "India is a country with 22 officially recognized languages and 17 of these have WordNets, a crucial resource. Web browser based interfaces are available for these WordNets, but are not suited for mobile devices which deters people from effectively using this resource. We present our initial work on developing mobile applications and browser extensions to access WordNets for Indian Languages. Our contribution is two fold: (1) We develop mobile applications for the Android, iOS and Windows Phone OS platforms for Hindi, Marathi and Sanskrit WordNets which allow users to search for words and obtain more information along with their translations in English and other Indian languages. (2) We also develop browser extensions for English, Hindi, Marathi, and Sanskrit WordNets, for both Mozilla Firefox, and Google Chrome. We believe that such applications can be quite helpful in a classroom scenario, where students would be able to access the WordNets as dictionaries as well as lexical knowledge bases. This can help in overcoming the language barrier along with furthering language understanding.",
            "cx": 8706.6,
            "cy": -475.571,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.naacl-main.322",
            "name": "How low is too low? A monolingual take on lemmatisation in {I}ndian languages",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Most prior work on ML based lemmatization has focused on high resource languages, where data sets (word forms) are readily available. For languages which have no linguistic work available, especially on morphology or in languages where the computational realization of linguistic rules is complex and cumbersome, machine learning based lemmatizers are the way togo. In this paper, we devote our attention to lemmatisation for low resource, morphologically rich scheduled Indian languages using neural methods. Here, low resource means only a small number of word forms are available. We perform tests to analyse the variance in monolingual models{'} performance on varying the corpus size and contextual morphological tag data for training. We show that monolingual approaches with data augmentation can give competitive accuracy even in the low resource setting, which augurs well for NLP in low resource setting.",
            "cx": 9302.6,
            "cy": -26.8701,
            "rx": 74.4932,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W14-0145",
            "name": "Semi-Automatic Extension of {S}anskrit {W}ordnet using Bilingual Dictionary",
            "publication_data": 2014,
            "citation": 1,
            "abstract": "In this paper, we report our methods and results of using, for the first time, semi-automatic approach to enhance an Indian language Wordnet. We apply our methods to enhancing an already existing Sanskrit Wordnet created from Hindi Wordnet (which is created from Princeton Wordnet) using expansion approach. We base our experiment on an existing bilingual Sanskrit English Dictionary and show how lemma in this dictionary can be mapped to Princeton Wordnet through which corresponding Sanskrit synsets can be populated by Sanskrit lexemes. This our method will also show how absence of resources of a pair of languages need not be an obstacle, if another resource of one of them is available. Sanskrit being historically related to languages of Indo-European family, we believe that this semi-automatic approach will help enhance Wordnets of other Indian languages of the same family.",
            "cx": 8036.6,
            "cy": -655.051,
            "rx": 118.174,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W14-0147",
            "name": "{I}ndo{W}ordnet Visualizer: A Graphical User Interface for Browsing and Exploring Wordnets of {I}ndian Languages",
            "publication_data": 2014,
            "citation": 3,
            "abstract": "In this paper, we are presenting a graphical user interface to browse and explore the IndoWordnet lexical database for various Indian languages. IndoWordnet visualizer extracts the related concepts for a given word and displays a sub graph containing those concepts. The interface is enhanced with different features in order to provide flexibility to the user. IndoWordnet visualizer is made publically available. Though it was initially constructed for making the wordnet validation process easier, it is proving to be very useful in analyzing various Natural Language Processing tasks, viz., Semantic relatedness, Word Sense Disambiguation, Information Retrieval, Textual Entailment, etc.",
            "cx": 12734.6,
            "cy": -655.051,
            "rx": 139.1,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-7512",
            "name": "An Introduction to the Textual History Tool",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "None",
            "cx": 12734.6,
            "cy": -206.35,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N15-3017",
            "name": "Brahmi-Net: A transliteration and script conversion system for languages of the {I}ndian subcontinent",
            "publication_data": 2015,
            "citation": 11,
            "abstract": "We present Brahmi-Net - an online system for transliteration and script conversion for all major Indian language pairs (306 pairs). The system covers 13 Indo-Aryan languages, 4 Dravidian languages and English. For training the transliteration systems, we mined parallel transliteration corpora from parallel translation corpora using an unsupervised method and trained statistical transliteration systems using the mined corpora. Languages which do not have parallel corpora are supported by transliteration through a bridge language. Our script conversion system supports conversion between all Brahmi-derived scripts as well as ITRANS romanization scheme. For this, we leverage co-ordinated Unicode ranges between Indic scripts and use an extended ITRANS encoding for transliterating between English and Indic scripts. The system also provides top-k transliterations and simultaneous transliteration into multiple output languages. We provide a Python as well as REST API to access these services. The API and the mined transliteration corpus are made available for research use under an open source license.",
            "cx": 632.597,
            "cy": -565.311,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L16-1098",
            "name": "Lexical Resources to Enrich {E}nglish {M}alayalam Machine Translation",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "In this paper we present our work on the usage of lexical resources for the Machine Translation English and Malayalam. We describe a comparative performance between different Statistical Machine Translation (SMT) systems on top of phrase based SMT system as baseline. We explore different ways of utilizing lexical resources to improve the quality of English Malayalam statistical machine translation. In order to enrich the training corpus we have augmented the lexical resources in two ways (a) additional vocabulary and (b) inflected verbal forms. Lexical resources include IndoWordnet semantic relation set, lexical words and verb phrases etc. We have described case studies, evaluations and have given detailed error analysis for both Malayalam to English and English to Malayalam machine translation systems. We observed significant improvement in evaluations of translation quality. Lexical resources do help uplift performance when parallel corpora are scanty.",
            "cx": 263.597,
            "cy": -475.571,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W18-1207",
            "name": "Meaningless yet meaningful: Morphology grounded subword-level {NMT}",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "We explore the use of two independent subsystems Byte Pair Encoding (BPE) and Morfessor as basic units for subword-level neural machine translation (NMT). We show that, for linguistically distant language-pairs Morfessor-based segmentation algorithm produces significantly better quality translation than BPE. However, for close language-pairs BPE-based subword-NMT may translate better than Morfessor-based subword-NMT. We propose a combined approach of these two segmentation algorithms Morfessor-BPE (M-BPE) which outperforms these two baseline systems in terms of BLEU score. Our results are supported by experiments on three language-pairs: English-Hindi, Bengali-Hindi and English-Bengali.",
            "cx": 516.597,
            "cy": -296.09,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1413",
            "name": "Morphology Injection for {E}nglish-{M}alayalam Statistical Machine Translation",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 191.597,
            "cy": -296.09,
            "rx": 115.931,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2016",
            "citation_count": 179,
            "name": 179,
            "cx": 28.5975,
            "cy": -475.571,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5911",
            "name": "Let Sense Bags Do Talking: Cross Lingual Word Semantic Similarity for {E}nglish and {H}indi",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "None",
            "cx": 12832.6,
            "cy": -565.311,
            "rx": 70.0071,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5912",
            "name": "A temporal expression recognition system for medical documents by",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "None",
            "cx": 12983.6,
            "cy": -565.311,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5925",
            "name": "Judge a Book by its Cover: Conservative Focused Crawling under Resource Constraints",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "None",
            "cx": 13154.6,
            "cy": -565.311,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5931",
            "name": "Logistic Regression for Automatic Lexical Level Morphological Paradigm Selection for {K}onkani Nouns",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Automatic selection of morphological paradigm for a noun lemma is necessary to automate the task of building morphological analyzer for nouns with minimal human interventions. Morphological paradigms can be of two types namely surface level morphological paradigms and lexical level morphological paradigms. In this paper we present a method to automatically select lexical level morphological paradigms for Konkani nouns. Using the proposed concept of paradigm differentiating measure to generate a training data set we found that logistic regression can be used to automatically select lexical level morphological paradigms with an F-Score of 0.957.",
            "cx": 13347.6,
            "cy": -565.311,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5937",
            "name": "Automated Analysis of {B}angla Poetry for Classification and Poet Identification",
            "publication_data": 2015,
            "citation": 10,
            "abstract": "Computational analysis of poetry is a challenging and interesting task in NLP. Human expertise on stylistics and aesthetics of poetry is generally expensive and scarce. In this work, we delve into the data to automatically extract stylistic and linguistic information which are useful for analysis and comparison of poems. We make use of semantic (word) features to perform subject-based classification of Bangla poems, and various stylistic as well as semantic features for poet identification. We have used a Multiclass SVM classifier to classify Tagorexe2x80x99s collection of poetry into four categories: devotional, love, nature and nationalism. We identified the most useful word features for each category of poems. The overall accuracy of the classifier was 56.8%, and the analysis led us to conclude that for poetry classification, word features alone do not suffice, due to allusions often being used as a poetic device. We, next, used these features along with stylistic features (syntactic, orthographic and phonemic), for poet identification on a dataset of poems from four poets and achieved a performance of 92.3% using a Multiclass SVM classifier. While contentbased and stylometric analysis of prose in Bangla has been done in the past, this is a first such attempt for poetry.",
            "cx": 13560.6,
            "cy": -565.311,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5943",
            "name": "Detection of Multiword Expressions for {H}indi Language using Word Embeddings and {W}ord{N}et-based Features",
            "publication_data": 2015,
            "citation": 6,
            "abstract": "None",
            "cx": 13793.6,
            "cy": -565.311,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5944",
            "name": "Augmenting Pivot based {SMT} with word segmentation",
            "publication_data": 2015,
            "citation": 2,
            "abstract": "None",
            "cx": 1610.6,
            "cy": -565.311,
            "rx": 96.3333,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5946",
            "name": "Triangulation of Reordering Tables: An Advancement Over Phrase Table Triangulation in Pivot-Based {SMT}",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Triangulation in Pivot-Based Statistical Machine Translation(SMT) is a very effective method for building Machine Translation(MT) systems in case of scarcity of the parallel corpus. Phrase Table Triangulation helps in such a resource constrained setting by inducing new phrase pairs with the help of a pivot. However, it does not explore the possibility of extracting reordering information through the use of pivot. This paper presents a novel method for triangulation of reordering tables in Pivot Based SMT. We show that the use of a pivot can help in extracting better reordering information and can assist in improving the quality of the translation. With a detailed example, we show that triangulation of reordering tables also improves the lexical choices a system makes during translation. We observe a BLEU score improvement of 1.06 for Marathi to English MT system with Hindi as a pivot, and also significant improvements in 8 other translation systems by using this method.",
            "cx": 1386.6,
            "cy": -565.311,
            "rx": 110.118,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-5947",
            "name": "Post-editing a chapter of a specialized textbook into 7 languages: importance of terminological proximity with {E}nglish for productivity",
            "publication_data": 2015,
            "citation": 0,
            "abstract": "Access to textbooks in onexe2x80x99s own language, in parallel with the original version in the instructional language, is known to be quite helpful for foreign students studying abroad. Cooperative post-editing (PE) of specialized textbook pretranslations by the foreign students themselves is a good way to produce the xe2x80x9dtargetxe2x80x9d versions, if the students find it rewarding, and not too time-consuming, that is, no longer than about 15-20 minutes per standard page (of 1400 characters or 250 words). In the experiment reported here, PE has been performed on a whole chapter of 4420 words (in English), or about 18 standard pages, into 7 languages (Portuguese, Japanese, Russian, Spanish, Bengali, Hindi, Marathi), native tongues of the participants. Average PE time has been measured, and when possible correlated with primary PE time (the time spent in editing a MT pre-translation in the PE text area). When terms are cognates of English terms (as in French, Spanish, Portuguese, and even Russian or Japanese), because neologisms are directly borrowed from English, or built using similar roots (often Latin or Greek) and similar word formation mechanisms (composition, affixation of special prefixes and suffixes), target terms can be xe2x80x9dguessedxe2x80x9d and PE time is in the order of 15 minutes per page, even if the target language is considered xe2x80x9ddistantxe2x80x9d from English. On the other hand, PE times increase by a factor of 3 to 5 when the target language is terminologically distant from English. We found that xe2x88x97Ritesh.Shah@imag.fr xe2x80xa0Christian.Boitet@imag.fr xe2x80xa1pb@cse.iitb.ac.in is the case of Hindi, Bengali and Marathi, and probably of all Indic languages. Previous experiments seem to have missed that important point, because they were performed on too short texts (often, only a few paragraphs), and on xe2x80x9deasyxe2x80x9d pairs like English-French. A consequence is that, for terminologically distant language pairs, one should begin by separately collecting, or if necessary coining, the terms in the target languages.",
            "cx": 14015.6,
            "cy": -565.311,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-3912",
            "name": "Data representation methods and use of mined corpora for {I}ndian language transliteration",
            "publication_data": 2015,
            "citation": 7,
            "abstract": "Our NEWS 2015 shared task submission is a PBSMT based transliteration system with the following corpus preprocessing enhancements: (i) addition of wordboundary markers, and (ii) languageindependent, overlapping character segmentation. We show that the addition of word-boundary markers improves transliteration accuracy substantially, whereas our overlapping segmentation shows promise in our preliminary analysis. We also compare transliteration systems trained using manually created corpora with the ones mined from parallel translation corpus for English to Indian language pairs. We identify the major errors in English to Indian language transliterations by analyzing heat maps of confusion matrices.",
            "cx": 1836.6,
            "cy": -565.311,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W15-2905",
            "name": "Your Sentiment Precedes You: Using an author{'}s historical tweets to predict sarcasm",
            "publication_data": 2015,
            "citation": 41,
            "abstract": "Sarcasm understanding may require information beyond the text itself, as in the case of xe2x80x98I absolutely love this restaurant!xe2x80x99 which may be sarcastic, depending on the contextual situation. We present the first quantitative evidence to show that historical tweets by an author can provide additional context for sarcasm detection. Our sarcasm detection approach uses two components: a contrast-based predictor (that identifies if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identifies if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past).",
            "cx": 7217.6,
            "cy": -565.311,
            "rx": 82.0488,
            "ry": 26.7407,
            "stroke": "black",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W16-5001",
            "name": "{`}Who would have thought of that!{'}: A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports the first topic model for sarcasm detection, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as {`}work{'}, {`}gun laws{'}, {`}weather{'} are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25{\\%}.",
            "cx": 7419.6,
            "cy": -475.571,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "K16-1015",
            "name": "Harnessing Sequence Labeling for Sarcasm Detection in Dialogue from {TV} Series {`}{F}riends{'}",
            "publication_data": 2016,
            "citation": 18,
            "abstract": "This paper is a novel study that views sarcasm detection in dialogue as a sequence labeling task, where a dialogue is made up of a sequence of utterances. We create a manuallylabeled dataset of dialogue from TV series xe2x80x98Friendsxe2x80x99 annotated with sarcasm. Our goal is to predict sarcasm in each utterance, using sequential nature of a scene. We show performance gain using sequence labeling as compared to classification-based approaches. Our experiments are based on three sets of features, one is derived from information in our dataset, the other two are from past works. Two sequence labeling algorithms (SVM-HMM and SEARN) outperform three classification algorithms (SVM, Naive Bayes) for all these feature sets, with an increase in F-score of around 4%. Our observations highlight the viability of sequence labeling techniques for sarcasm detection of dialogue.",
            "cx": 7217.6,
            "cy": -475.571,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "D16-1104",
            "name": "Are Word Embedding-based Features Useful for Sarcasm Detection?",
            "publication_data": 2016,
            "citation": 27,
            "abstract": "None",
            "cx": 6601.6,
            "cy": -475.571,
            "rx": 68.6788,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L18-1424",
            "name": "Sarcasm Target Identification: Dataset and An Introductory Approach",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "None",
            "cx": 7091.6,
            "cy": -296.09,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "R15-1013",
            "name": "Coreference Resolution to Support {IE} from {I}ndian Classical Music Forums",
            "publication_data": 2015,
            "citation": 2,
            "abstract": "Efficient music information retrieval (MIR) require to have meta information about music along with content based information in the knowledge base. Discussion forums on music are rich sources of information gathered from a wider audience. Taking into consideration the nature of text in these web resources, the yield of relation extraction is quite dependent on resolving the entity references in the document. Among the few music forums dealing with Indian classical music, rasikas.org (rasikas, 2015) having rich information about artistes, raga and other music concepts is taken for our study. The forum posts generally contain anaphoric references to the main topic of the thread or any other entity in the discourse. In this paper we focus on coreference resolution for short discourse noisy text like that of forum posts. Since grammatical roles capture relation between mentions in a discourse, those features extracted from dependency parsing are widely explored along with semantic compatibility feature. On investigation of issues, the need for integrating known dependencies between features emerged. A Bayesian network with predefined network structure is evaluated, since a Bayesian belief network enacts a probabilistic rule based system. To the extent possible the superior behaviour of Bayesian network over SVM is analysed.",
            "cx": 14223.6,
            "cy": -565.311,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P15-2100",
            "name": "A Computational Approach to Automatic Prediction of Drunk-Texting",
            "publication_data": 2015,
            "citation": 5,
            "abstract": "Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting.",
            "cx": 14392.6,
            "cy": -565.311,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-2111",
            "name": "How Do Cultural Differences Impact the Quality of Sarcasm Annotation?: A Case Study of {I}ndian Annotators and {A}merican Text",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "None",
            "cx": 6402.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "U16-1013",
            "name": "How Challenging is Sarcasm versus Irony Classification?: A Study With a Dataset from {E}nglish Literature",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "None",
            "cx": 6761.6,
            "cy": -475.571,
            "rx": 73.5782,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P16-1104",
            "name": "Harnessing Cognitive Features for Sarcasm Detection",
            "publication_data": 2016,
            "citation": 13,
            "abstract": "In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by this observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of Fscore), over the performance of the best reported system.",
            "cx": 6060.6,
            "cy": -475.571,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W17-7518",
            "name": "Is your Statement Purposeless? Predicting Computer Science Graduation Admission Acceptance based on Statement Of Purpose",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "None",
            "cx": 6393.6,
            "cy": -385.831,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-1309",
            "name": "{``}When Numbers Matter!{''}: Detecting Sarcasm in Numerical Portions of Text",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "Research in sarcasm detection spans almost a decade. However a particular form of sarcasm remains unexplored: sarcasm expressed through numbers, which we estimate, forms about 11{\\%} of the sarcastic tweets in our dataset. The sentence {`}Love waking up at 3 am{'} is sarcastic because of the number. In this paper, we focus on detecting sarcasm in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such sarcasm. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an F-score of 0.93 and 0.91 on our dataset of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of sarcasm arising out of numbers, culminating in a detector thereof.",
            "cx": 6874.6,
            "cy": -206.35,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.aacl-main.31",
            "name": "All-in-One: A Deep Attentive Multi-task Learning Framework for Humour, Sarcasm, Offensive, Motivation, and Sentiment on Memes",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and sentiment analysis on a somewhat complicated form of information, i.e., memes. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For multi-tasking, we propose two attention-like mechanisms viz., Inter-task Relationship Module (iTRM) and Inter-class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, iCRM develops relations between the different classes of tasks. Finally, representations from both the attentions are concatenated and shared across the five tasks (i.e., humour, sarcasm, offensive, motivational, and sentiment) for multi-tasking. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of memes annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-the-art systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.",
            "cx": 5964.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D16-1196",
            "name": "Orthographic Syllable as basic unit for {SMT} between Related Languages",
            "publication_data": 2016,
            "citation": 8,
            "abstract": "None",
            "cx": 858.597,
            "cy": -475.571,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-4102",
            "name": "Learning variable length units for {SMT} between related languages via Byte Pair Encoding",
            "publication_data": 2017,
            "citation": 7,
            "abstract": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11{\\%} increase in BLEU score. While orthographic syllables can be used only for languages whose writing systems use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and writing systems.",
            "cx": 560.597,
            "cy": -385.831,
            "rx": 89.6056,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P18-2064",
            "name": "Judicious Selection of Training Data in Assisting Language for Multilingual Neural {NER}",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages. Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages. We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning. To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.",
            "cx": 858.597,
            "cy": -296.09,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.emnlp-main.675",
            "name": "Role of {L}anguage {R}elatedness in {M}ultilingual {F}ine-tuning of {L}anguage {M}odels: {A} {C}ase {S}tudy in {I}ndo-{A}ryan {L}anguages",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the sense of best boost of performance) manner; which reveals that careful selection of subset of related languages can significantly improve performance than utilizing all related languages. The Indo-Aryan (IA) language family is chosen for the study, the exact languages being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script barrier is crossed by simple rule-based transliteration of the text of all languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource languages, such as Oriya and Punjabi, are found to be the largest beneficiaries of multilingual fine-tuning. Textual Entailment, Entity Classification, Section Title Prediction, tasks of IndicGLUE and POS tagging form our test bed. Compared to monolingual fine tuning we get relative performance improvement of up to 150{\\%} in the downstream tasks. The surprise take-away is that for any language there is a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental.",
            "cx": 630.597,
            "cy": -26.8701,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N15-1132",
            "name": "Unsupervised Most Frequent Sense Detection using Word Embeddings",
            "publication_data": 2015,
            "citation": 20,
            "abstract": "An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The field of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent.",
            "cx": 8842.6,
            "cy": -565.311,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "L18-1049",
            "name": "Sentence Level Temporality Detection using an Implicit Time-sensed Resource",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "None",
            "cx": 8847.6,
            "cy": -296.09,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018.gwc-1.34",
            "name": "An Iterative Approach for Unsupervised Most Frequent Sense Detection using {W}ord{N}et and Word Embeddings",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "Given a word, what is the most frequent sense in which it occurs in a given corpus? Most Frequent Sense (MFS) is a strong baseline for unsupervised word sense disambiguation. If we have large amounts of sense-annotated corpora, MFS can be trivially created. However, sense-annotated corpora are a rarity. In this paper, we propose a method which can compute MFS from raw corpora. Our approach iteratively exploits the semantic congruity among related words in corpus. Our method performs better compared to another similar work.",
            "cx": 9038.6,
            "cy": -296.09,
            "rx": 62.8651,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D15-1017",
            "name": "Monotone Submodularity in Opinion Summaries",
            "publication_data": 2015,
            "citation": 2,
            "abstract": "Opinion summarization is the task of producing the summary of a text, such that the summary also preserves the sentiment of the text. Opinion Summarization is thus a trade-off between summarization and sentiment analysis. The demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sentences. We harness the power of submodularity to strike a balance between two conflicting requirements. We investigate an incipient class of submodular functions for the problem, and a partial enumeration based greedy algorithm that has performance guarantee of 63%. Our functions generate summaries such that there is good correlation between document sentiment and summary sentiment along with good ROUGE score, which outperforms thestate-of-the-art algorithms.",
            "cx": 14570.6,
            "cy": -565.311,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W18-3705",
            "name": "Thank {``}Goodness{''}! A Way to Measure Style in Student Essays",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Essays have two major components for scoring - content and style. In this paper, we describe a property of the essay, called goodness, and use it to predict the score given for the style of student essays. We compare our approach to solve this problem with baseline approaches, like language modeling and also a state-of-the-art deep learning system. We show that, despite being quite intuitive, our approach is very powerful in predicting the style of the essays.",
            "cx": 3328.6,
            "cy": -296.09,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2017",
            "citation_count": 87,
            "name": 87,
            "cx": 28.5975,
            "cy": -385.831,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6311",
            "name": "Improving Document Ranking using Query Expansion and Classification Techniques for Mixed Script Information Retrieval",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 12865.6,
            "cy": -475.571,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6325",
            "name": "A Recurrent Neural Network Architecture for De-identifying Clinical Records",
            "publication_data": 2016,
            "citation": 7,
            "abstract": "None",
            "cx": 13226.6,
            "cy": -475.571,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6331",
            "name": "Opinion Mining in a Code-Mixed Environment: A Case Study with Government Portals",
            "publication_data": 2016,
            "citation": 2,
            "abstract": "None",
            "cx": 4114.6,
            "cy": -475.571,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "K18-1012",
            "name": "Uncovering Code-Mixed Challenges: A Framework for Linguistically Driven Question Generation and Neural Based Question Answering",
            "publication_data": 2018,
            "citation": 4,
            "abstract": "Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like English. In recent times, the rapid growth of multi-lingual web content has posed several challenges to the existing QA systems. Code-mixing is one such challenge that makes the task more complex. In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neural network based architecture for code-mixed question answering (CMQA). For evaluation, we manually create the code-mixed questions for Hindi-English language pair. In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, SQuAD and MMQA. Experiments show that our proposed model achieves encouraging performance on CMQG and CMQA.",
            "cx": 4117.6,
            "cy": -296.09,
            "rx": 104.804,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.findings-emnlp.206",
            "name": "A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based models to be deployed for a variety of natural language processing (NLP) applications. A potential solution is to either manually create or crowd-source the code-mixed labelled data for the task at hand, but that requires much human efforts and often not feasible because of the language specific diversity in the code-mixed text. To circumvent the data scarcity issue, we propose an effective deep learning approach for automatically generating the code-mixed text from English to multiple languages without any parallel data. In order to train the neural network, we create synthetic code-mixed texts from the available parallel corpus by modelling various linguistic properties of code-mixing. Our codemixed text generator is built upon the encoder-decoder framework, where the encoder is augmented with the linguistic and task-agnostic features obtained from the transformer based language model. We also transfer the knowledge from a neural machine translation (NMT) to warm-start the training of code-mixed generator. Experimental results and in-depth analysis show the effectiveness of our proposed code-mixed text generation on eight diverse language pairs.",
            "cx": 3491.6,
            "cy": -116.61,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W16-6336",
            "name": "On Why Coarse Class Classification is Bottleneck in Noun Compound Interpretation",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 7649.6,
            "cy": -475.571,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1489",
            "name": "Towards a Standardized Dataset for Noun Compound Interpretation",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 7361.6,
            "cy": -296.09,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-6337",
            "name": "{V}erbframator:Semi-Automatic Verb Frame Annotator Tool with Special Reference to {M}arathi",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "None",
            "cx": 13506.6,
            "cy": -475.571,
            "rx": 196.651,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4811",
            "name": "Faster Decoding for Subword Level Phrase-based {SMT} between Related Languages",
            "publication_data": 2016,
            "citation": 1,
            "abstract": "A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.",
            "cx": 1064.6,
            "cy": -475.571,
            "rx": 79.8063,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4604",
            "name": "{IIT} {B}ombay{'}s {E}nglish-{I}ndonesian submission at {WAT}: Integrating Neural Language Models with {SMT}",
            "publication_data": 2016,
            "citation": 0,
            "abstract": "This paper describes the IIT Bombay{'}s submission as a part of the shared task in WAT 2016 for English{--}Indonesian language pair. The results reported here are for both the direction of the language pair. Among the various approaches experimented, Operation Sequence Model (OSM) and Neural Language Model have been submitted for WAT. The OSM approach integrates translation and reordering process resulting in relatively improved translation. Similarly the neural experiment integrates Neural Language Model with Statistical Machine Translation (SMT) as a feature for translation. The Neural Probabilistic Language Model (NPLM) gave relatively high BLEU points for Indonesian to English translation system while the Neural Network Joint Model (NNJM) performed better for English to Indonesian direction of translation system. The results indicate improvement over the baseline Phrase-based SMT by 0.61 BLEU points for English-Indonesian system and 0.55 BLEU points for Indonesian-English translation system.",
            "cx": 13806.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W16-4206",
            "name": "Deep Learning Architecture for Patient Data De-identification in Clinical Records",
            "publication_data": 2016,
            "citation": 14,
            "abstract": "Rapid growth in Electronic Medical Records (EMR) has emerged to an expansion of data in the clinical domain. The majority of the available health care information is sealed in the form of narrative documents which form the rich source of clinical information. Text mining of such clinical records has gained huge attention in various medical applications like treatment and decision making. However, medical records enclose patient Private Health Information (PHI) which can reveal the identities of the patients. In order to retain the privacy of patients, it is mandatory to remove all the PHI information prior to making it publicly available. The aim is to de-identify or encrypt the PHI from the patient medical records. In this paper, we propose an algorithm based on deep learning architecture to solve this problem. We perform de-identification of seven PHI terms from the clinical records. Experiments on benchmark datasets show that our proposed approach achieves encouraging performance, which is better than the baseline model developed with Conditional Random Field.",
            "cx": 13064.6,
            "cy": -475.571,
            "rx": 78.4777,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "W16-0415",
            "name": "Political Issue Extraction Model: A Novel Hierarchical Topic Model That Uses Tweets By Political And Non-Political Authors",
            "publication_data": 2016,
            "citation": 9,
            "abstract": "People often use social media to discuss opinions, including political ones. We refer to relevant topics in these discussions as political issues, and the alternate stands towards these topics as political positions. We present a Political Issue Extraction (PIE) model that is capable of discovering political issues and positions from an unlabeled dataset of tweets. A strength of this model is that it uses twitter timelines of political and non-political authors, and affiliation information of only political authors. The model estimates word-specific distributions (that denote political issues and positions) and hierarchical author/group-specific distributions (that show how these issues divide people). Our experiments using a dataset of 2.4 million tweets from the US show that this model effectively captures the desired properties (with respect to words and groups) of political discussions. We also evaluate the two components of the model by experimenting with: (a) Use to alternate strategies to classify words, and (b) Value addition due to incorporation of group membership information. Estimated distributions are then used to predict political affiliation with 68% accuracy.",
            "cx": 6977.6,
            "cy": -475.571,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N16-4006",
            "name": "Statistical Machine Translation between Related Languages",
            "publication_data": 2016,
            "citation": 4,
            "abstract": "None",
            "cx": 554.597,
            "cy": -475.571,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L16-1686",
            "name": "{S}lang{N}et: A {W}ord{N}et like resource for {E}nglish Slang",
            "publication_data": 2016,
            "citation": 7,
            "abstract": "We present a WordNet like structured resource for slang words and neologisms on the internet. The dynamism of language is often an indication that current language technology tools trained on today{'}s data, may not be able to process the language in the future. Our resource could be (1) used to augment the WordNet, (2) used in several Natural Language Processing (NLP) applications which make use of noisy data on the internet like Information Retrieval and Web Mining. Such a resource can also be used to distinguish slang word senses from conventional word senses. To stimulate similar innovations widely in the NLP community, we test the efficacy of our resource for detecting slang using standard bag of words Word Sense Disambiguation (WSD) algorithms (Lesk and Extended Lesk) for English data on the internet.",
            "cx": 14033.6,
            "cy": -475.571,
            "rx": 123.988,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-2338",
            "name": "Adapting Pre-trained Word Embeddings For Use In Medical Coding",
            "publication_data": 2017,
            "citation": 5,
            "abstract": "Word embeddings are a crucial component in modern NLP. Pre-trained embeddings released by different groups have been a major reason for their popularity. However, they are trained on generic corpora, which limits their direct use for domain specific tasks. In this paper, we propose a method to add task specific information to pre-trained word embeddings. Such information can improve their utility. We add information from medical coding data, as well as the first level from the hierarchy of ICD-10 medical code set to different pre-trained word embeddings. We adapt CBOW algorithm from the word2vec package for our purpose. We evaluated our approach on five different pre-trained word embeddings. Both the original word embeddings, and their modified versions (the ones with added information) were used for automated review of medical coding. The modified word embeddings give an improvement in f-score by 1{\\%} on the 5-fold evaluation on a private medical claims dataset. Our results show that adding extra information is possible and beneficial for the task at hand.",
            "cx": 6566.6,
            "cy": -385.831,
            "rx": 87.8629,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I17-2006",
            "name": "Towards Lower Bounds on Number of Dimensions for Word Embeddings",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "Word embeddings are a relatively new addition to the modern NLP researcher{'}s toolkit. However, unlike other tools, word embeddings are used in a black box manner. There are very few studies regarding various hyperparameters. One such hyperparameter is the dimension of word embeddings. They are rather decided based on a rule of thumb: in the range 50 to 300. In this paper, we show that the dimension should instead be chosen based on corpus statistics. More specifically, we show that the number of pairwise equidistant words of the corpus vocabulary (as defined by some distance/similarity metric) gives a lower bound on the the number of dimensions , and going below this bound results in degradation of quality of learned word embeddings. Through our evaluations on standard word embedding evaluation tasks, we show that for dimensions higher than or equal to the bound, we get better results as compared to the ones below it.",
            "cx": 6759.6,
            "cy": -385.831,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C18-1155",
            "name": "Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using {LSTM}",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Interpreting noun compounds is a challenging task. It involves uncovering the underlying predicate which is dropped in the formation of the compound. In most cases, this predicate is of the form VERB+PREP. It has been observed that uncovering the preposition is a significant step towards uncovering the predicate. In this paper, we attempt to paraphrase noun compounds using prepositions. We consider noun compounds and their corresponding prepositional paraphrases as parallelly aligned sequences of words. This enables us to adapt different architectures from cross-lingual embedding literature. We choose the architecture where we create representations of both noun compound (source sequence) and its corresponding prepositional paraphrase (target sequence), such that their sim- ilarity is high. We use LSTMs to learn these representations. We use these representations to decide the correct preposition. Our experiments show that this approach performs considerably well on different datasets of noun compounds that are manually annotated with prepositions.",
            "cx": 7588.6,
            "cy": -296.09,
            "rx": 96.7474,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-5229",
            "name": "{IITP} at {E}mo{I}nt-2017: Measuring Intensity of Emotions using Sentence Embeddings and Optimized Features",
            "publication_data": 2017,
            "citation": 3,
            "abstract": "This paper describes the system that we submitted as part of our participation in the shared task on Emotion Intensity (EmoInt-2017). We propose a Long short term memory (LSTM) based architecture cascaded with Support Vector Regressor (SVR) for intensity prediction. We also employ Particle Swarm Optimization (PSO) based feature selection algorithm for obtaining an optimized feature set for training and evaluation. System evaluation shows interesting results on the four emotion datasets i.e. anger, fear, joy and sadness. In comparison to the other participating teams our system was ranked 5th in the competition.",
            "cx": 5220.6,
            "cy": -385.831,
            "rx": 71.3357,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-1057",
            "name": "A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis",
            "publication_data": 2017,
            "citation": 11,
            "abstract": "In this paper, we propose a novel method for combining deep learning and classical feature based models using a Multi-Layer Perceptron (MLP) network for financial sentiment analysis. We develop various deep learning models based on Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). These are trained on top of pre-trained, autoencoder-based, financial word embeddings and lexicon features. An ensemble is constructed by combining these deep learning models and a classical supervised model based on Support Vector Regression (SVR). We evaluate our proposed technique on a benchmark dataset of SemEval-2017 shared task on financial sentiment analysis. The propose model shows impressive results on two datasets, i.e. microblogs and news headlines datasets. Comparisons show that our proposed model performs better than the existing state-of-the-art systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.",
            "cx": 4749.6,
            "cy": -385.831,
            "rx": 50.41,
            "ry": 26.7407,
            "stroke": "red",
            "width": "3",
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.621",
            "name": "Multi-domain Tweet Corpora for Sentiment Analysis: Resource Creation and Evaluation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Due to the phenomenal growth of online content in recent time, sentiment analysis has attracted attention of the researchers and developers. A number of benchmark annotated corpora are available for domains like movie reviews, product reviews, hotel reviews, etc.The pervasiveness of social media has also lead to a huge amount of content posted by users who are misusing the power of social media to spread false beliefs and to negatively influence others. This type of content is coming from the domains like terrorism, cybersecurity, technology, social issues, etc. Mining of opinions from these domains is important to create a socially intelligent system to provide security to the public and to maintain the law and order situations. To the best of our knowledge, there is no publicly available tweet corpora for such pervasive domains. Hence, we firstly create a multi-domain tweet sentiment corpora and then establish a deep neural network based baseline framework to address the above mentioned issues. Annotated corpus has Cohen{'}s Kappa measurement for annotation quality of 0.770, which shows that the data is of acceptable quality. We are able to achieve 84.65{\\%} accuracy for sentiment analysis by using an ensemble of Convolutional Neural Network (CNN), Long Short Term Memory (LSTM), and Gated Recurrent Unit(GRU).",
            "cx": 5217.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.lrec-1.201",
            "name": "{CEASE}, a Corpus of Emotion Annotated Suicide notes in {E}nglish",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "A suicide note is usually written shortly before the suicide and it provides a chance to comprehend the self-destructive state of mind of the deceased. From a psychological point of view, suicide notes have been utilized for recognizing the motive behind the suicide. To the best of our knowledge, there is no openly accessible suicide note corpus at present, making it challenging for the researchers and developers to deep dive into the area of mental health assessment and suicide prevention. In this paper, we create a fine-grained emotion annotated corpus (CEASE) of suicide notes in English and develop various deep learning models to perform emotion detection on the curated dataset. The corpus consists of 2393 sentences from around 205 suicide notes collected from various sources. Each sentence is annotated with a particular emotion class from a set of 15 fine-grained emotion labels, namely (forgiveness, happiness{\\_}peacefulness, love, pride, hopefulness, thankfulness, blame, anger, fear, abuse, sorrow, hopelessness, guilt, information, instructions). For the evaluation, we develop an ensemble architecture, where the base models correspond to three supervised deep learning models, namely Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM). We obtain the highest test accuracy of 60.17{\\%} and cross-validation accuracy of 60.32{\\%}",
            "cx": 4600.6,
            "cy": -116.61,
            "rx": 108.375,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.icon-main.62",
            "name": "Annotated Corpus of Tweets in {E}nglish from Various Domains for Emotion Detection",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Emotion recognition is a very well-attended problem in Natural Language Processing (NLP). Most of the existing works on emotion recognition focus on the general domain and in some cases to specific domains like fairy tales, blogs, weather, Twitter etc. But emotion analysis systems in the domains of security, social issues, technology, politics, sports, etc. are very rare. In this paper, we create a benchmark setup for emotion recognition in these specialised domains. First, we construct a corpus of 18,921 tweets in English annotated with Paul Ekman{'}s six basic emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and a non-emotive class Others. Thereafter, we propose a deep neural framework to perform emotion recognition in an end-to-end setting. We build various models based on Convolutional Neural Network (CNN), Bi-directional Long Short Term Memory (Bi-LSTM), Bi-directional Gated Recurrent Unit (Bi-GRU). We propose a Hierarchical Attention-based deep neural network for Emotion Detection (HAtED). We also develop multiple systems by considering different sets of emotion classes for each system and report the detailed comparative analysis of the results. Experiments show the hierarchical attention-based model achieves best results among the considered baselines with accuracy of 69{\\%}.",
            "cx": 4832.6,
            "cy": -116.61,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2016.gwc-1.4",
            "name": "Detecting Most Frequent Sense using Word Embeddings and {B}abel{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "Since the inception of the SENSEVAL evaluation exercises there has been a great deal of recent research into Word Sense Disambiguation (WSD). Over the years, various supervised, unsupervised and knowledge based WSD systems have been proposed. Beating the first sense heuristics is a challenging task for these systems. In this paper, we present our work on Most Frequent Sense (MFS) detection using Word Embeddings and BabelNet features. The semantic features from BabelNet viz., synsets, gloss, relations, etc. are used for generating sense embeddings. We compare word embedding of a word with its sense embeddings to obtain the MFS with the highest similarity. The MFS is detected for six languages viz., English, Spanish, Russian, German, French and Italian. However, this approach can be applied to any language provided that word embeddings are available for that language.",
            "cx": 14274.6,
            "cy": -475.571,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2016.gwc-1.7",
            "name": "{I}ndo{W}ord{N}et::{S}imilarity- Computing Semantic Similarity and Relatedness using {I}ndo{W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "Semantic similarity and relatedness measures play an important role in natural language processing applications. In this paper, we present the IndoWordNet::Similarity tool and interface, designed for computing the semantic similarity and relatedness between two words in IndoWordNet. A java based tool and a web interface have been developed to compute this semantic similarity and relatedness. Also, Java API has been developed for this purpose. This tool, web interface and the API are made available for the research purpose.",
            "cx": 14584.6,
            "cy": -475.571,
            "rx": 192.166,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2016.gwc-1.23",
            "name": "A picture is worth a thousand words: Using {O}pen{C}lip{A}rt library for enriching {I}ndo{W}ord{N}et",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "WordNet has proved to be immensely useful for Word Sense Disambiguation, and thence Machine translation, Information Retrieval and Question Answering. It can also be used as a dictionary for educational purposes. The semantic nature of concepts in a WordNet motivates one to try to express this meaning in a more visual way. In this paper, we describe our work of enriching IndoWordNet with image acquisitions from the OpenClipArt library. We describe an approach used to enrich WordNets for eighteen Indian languages. Our contribution is three fold: (1) We develop a system, which, given a synset in English, finds an appropriate image for the synset. The system uses the OpenclipArt library (OCAL) to retrieve images and ranks them. (2) After retrieving the images, we map the results along with the linkages between Princeton WordNet and Hindi WordNet, to link several synsets to corresponding images. We choose and sort top three images based on our ranking heuristic per synset. (3) We develop a tool that allows a lexicographer to manually evaluate these images. The top images are shown to a lexicographer by the evaluation tool for the task of choosing the best image representation. The lexicographer also selects the number of relevant images. Using our system, we obtain an Average Precision (P @ 3) score of 0.30.",
            "cx": 8471.6,
            "cy": -475.571,
            "rx": 66.4361,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018.gwc-1.37",
            "name": "{H}indi {W}ordnet for Language Teaching: Experiences and Lessons Learnt",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "This paper reports the work related to making Hindi Wordnet1 available as a digital resource for language learning and teaching, and the experiences and lessons that were learnt during the process. The language data of the Hindi Wordnet has been suitably modified and enhanced to make it into a language learning aid. This aid is based on modern pedagogical axioms and is aligned to the learning objectives of the syllabi of the school education in India. To make it into a comprehensive language tool, grammatical information has also been encoded, as far as these can be marked on the lexical items. The delivery of information is multi-layered, multi-sensory and is available across multiple digital platforms. The front end has been designed to offer an eye-catching user-friendly interface which is suitable for learners starting from age six onward. Preliminary testing of the tool has been done and it has been modified as per the feedbacks that were received. Above all, the entire exercise has offered gainful insights into learning based on associative networks and how knowledge based on such networks can be made available to modern learners.",
            "cx": 8202.6,
            "cy": -296.09,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018.gwc-1.49",
            "name": "Synthesizing Audio for {H}indi {W}ord{N}et",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "In this paper, we describe our work on the creation of a voice model using a speech synthesis system for the Hindi Language. We use pre-existing {``}voices{''}, use publicly available speech corpora to create a {``}voice{''} using the Festival Speech Synthesis System (Black, 1997). Our contribution is two-fold: (1) We scrutinize multiple speech synthesis systems and provide an extensive report on the currently available state-of-the-art systems. We also develop voices using the existing implementations of the aforementioned systems, and (2) We use these voices to generate sample audios for randomly chosen words; manually evaluate the audio generated, and produce audio for all WordNet words using the winner voice model. We also produce audios for the Hindi WordNet Glosses and Example sentences. We describe our efforts to use pre-existing implementations for WaveNet - a model to generate raw audio using neural nets (Oord et al., 2016) and generate speech for Hindi. Our lexicographers perform a manual evaluation of the audio generated using multiple voices. A qualitative and quantitative analysis reveals that the voice model generated by us performs the best with an accuracy of 0.44.",
            "cx": 8428.6,
            "cy": -296.09,
            "rx": 108.789,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2016.gwc-1.37",
            "name": "{I}ndo{W}ord{N}et Conversion to Web Ontology Language ({OWL})",
            "publication_data": 2016,
            "citation": "???",
            "abstract": "WordNet plays a significant role in Linked Open Data (LOD) cloud. It has numerous application ranging from ontology annotation to ontology mapping. IndoWordNet is a linked WordNet connecting 18 Indian language WordNets with Hindi as a source WordNet. The Hindi WordNet was initially developed by linking it to English WordNet. In this paper, we present a data representation of IndoWordNet in Web Ontology Language (OWL). The schema of Princeton WordNet has been enhanced to support the representation of IndoWordNet. This IndoWordNet representation in OWL format is now available to link other web resources. This representation is implemented for eight Indian languages.",
            "cx": 14936.6,
            "cy": -475.571,
            "rx": 141.343,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2018",
            "citation_count": 60,
            "name": 60,
            "cx": 28.5975,
            "cy": -296.09,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-7517",
            "name": "Document Level Novelty Detection: Textual Entailment Lends a Helping Hand",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "None",
            "cx": 12853.6,
            "cy": -385.831,
            "rx": 90.5193,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W18-0522",
            "name": "The Whole is Greater than the Sum of its Parts: Towards the Effectiveness of Voting Ensemble Classifiers for Complex Word Identification",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "In this paper, we present an effective system using voting ensemble classifiers to detect contextually complex words for non-native English speakers. To make the final decision, we channel a set of eight calibrated classifiers based on lexical, size and vocabulary features and train our model with annotated datasets collected from a mixture of native and non-native speakers. Thereafter, we test our system on three datasets namely News, WikiNews, and Wikipedia and report competitive results with an F1-Score ranging between 0.777 to 0.855 for each of the datasets. Our system outperforms multiple other models and falls within 0.042 to 0.026 percent of the best-performing model{'}s score in the shared task.",
            "cx": 6393.6,
            "cy": -296.09,
            "rx": 67.3507,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-7531",
            "name": "{H}indi Shabdamitra: A {W}ordnet based {E}-Learning Tool for Language Learning and Teaching",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "None",
            "cx": 7892.6,
            "cy": -385.831,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-5904",
            "name": "{H}indi Shabdamitra: A {W}ordnet based {E}-Learning Tool for Language Learning and Teaching",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "In today{'}s technology driven digital era, education domain is undergoing a transformation from traditional approaches to more learner controlled and flexible methods of learning. This transformation has opened the new avenues for interdisciplinary research in the field of educational technology and natural language processing in developing quality digital aids for learning and teaching. The tool presented here - Hindi Shabhadamitra, developed using Hindi Wordnet for Hindi language learning, is one such e-learning tool. It has been developed as a teaching and learning aid suitable for formal school based curriculum and informal setup for self learning users. Besides vocabulary, it also provides word based grammar along with images and pronunciation for better learning and retention. This aid demonstrates that how a rich lexical resource like wordnet can be systematically remodeled for practical usage in the educational domain.",
            "cx": 13061.6,
            "cy": -385.831,
            "rx": 98.9899,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-5717",
            "name": "Comparing Recurrent and Convolutional Architectures for {E}nglish-{H}indi Neural Machine Translation",
            "publication_data": 2017,
            "citation": 1,
            "abstract": "In this paper, we empirically compare the two encoder-decoder neural machine translation architectures: convolutional sequence to sequence model (ConvS2S) and recurrent sequence to sequence model (RNNS2S) for English-Hindi language pair as part of IIT Bombay{'}s submission to WAT2017 shared task. We report the results for both English-Hindi and Hindi-English direction of language pair.",
            "cx": 13285.6,
            "cy": -385.831,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W17-2605",
            "name": "Towards Harnessing Memory Networks for Coreference Resolution",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "Coreference resolution task demands comprehending a discourse, especially for anaphoric mentions which require semantic information for resolving antecedents. We investigate into how memory networks can be helpful for coreference resolution when posed as question answering problem. The comprehension capability of memory networks assists coreference resolution, particularly for the mentions that require semantic and context information. We experiment memory networks for coreference resolution, with 4 synthetic datasets generated for coreference resolution with varying difficulty levels. Our system{'}s performance is compared with a traditional coreference resolution system to show why memory network can be promising for coreference resolution.",
            "cx": 13488.6,
            "cy": -385.831,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S17-2009",
            "name": "{IIT}-{UHH} at {S}em{E}val-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification",
            "publication_data": 2017,
            "citation": 6,
            "abstract": "In this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3. We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features. In addition, we propose a novel method for dialogue chain identification in comment threads. Our primary submission won subtask C, outperforming other systems in all the primary evaluation metrics. We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B. We also developed open source toolkits for all the three English subtasks by the name cQARank [\\url{https://github.com/TitasNandi/cQARank}].",
            "cx": 13686.6,
            "cy": -385.831,
            "rx": 102.561,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S17-2087",
            "name": "{IITP} at {S}em{E}val-2017 Task 8 : A Supervised Approach for Rumour Evaluation",
            "publication_data": 2017,
            "citation": 10,
            "abstract": "This paper describes our system participation in the SemEval-2017 Task 8 {`}RumourEval: Determining rumour veracity and support for rumours{'}. The objective of this task was to predict the stance and veracity of the underlying rumour. We propose a supervised classification approach employing several lexical, content and twitter specific features for learning. Evaluation shows promising results for both the problems.",
            "cx": 13892.6,
            "cy": -385.831,
            "rx": 85.6199,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S17-2153",
            "name": "{IITPB} at {S}em{E}val-2017 Task 5: Sentiment Prediction in Financial Text",
            "publication_data": 2017,
            "citation": 5,
            "abstract": "This paper reports team IITPB{'}s participation in the SemEval 2017 Task 5 on {`}Fine-grained sentiment analysis on financial microblogs and news{'}. We developed 2 systems for the two tracks. One system was based on an ensemble of Support Vector Classifier and Logistic Regression. This system relied on Distributional Thesaurus (DT), word embeddings and lexicon features to predict a floating sentiment value between -1 and +1. The other system was based on Support Vector Regression using word embeddings, lexicon features, and PMI scores as features. The system was ranked 5th in track 1 and 8th in track 2.",
            "cx": 14095.6,
            "cy": -385.831,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "S17-2154",
            "name": "{IITP} at {S}em{E}val-2017 Task 5: An Ensemble of Deep Learning and Feature Based Models for Financial Sentiment Analysis",
            "publication_data": 2017,
            "citation": 6,
            "abstract": "In this paper we propose an ensemble based model which combines state of the art deep learning sentiment analysis algorithms like Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) along with feature based models to identify optimistic or pessimistic sentiments associated with companies and stocks in financial texts. We build our system to participate in a competition organized by Semantic Evaluation 2017 International Workshop. We combined predictions from various models using an artificial neural network to determine the opinion towards an entity in (a) Microblog Messages and (b) News Headlines data. Our models achieved a cosine similarity score of 0.751 and 0.697 for the above two tracks giving us the rank of 2nd and 7th best team respectively.",
            "cx": 14316.6,
            "cy": -385.831,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "I17-4031",
            "name": "{IITP} at {IJCNLP}-2017 Task 4: Auto Analysis of Customer Feedback using {CNN} and {GRU} Network",
            "publication_data": 2017,
            "citation": 0,
            "abstract": "Analyzing customer feedback is the best way to channelize the data into new marketing strategies that benefit entrepreneurs as well as customers. Therefore an automated system which can analyze the customer behavior is in great demand. Users may write feedbacks in any language, and hence mining appropriate information often becomes intractable. Especially in a traditional feature-based supervised model, it is difficult to build a generic system as one has to understand the concerned language for finding the relevant features. In order to overcome this, we propose deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) based approaches that do not require handcrafting of features. We evaluate these techniques for analyzing customer feedback sentences on four languages, namely English, French, Japanese and Spanish. Our empirical analysis shows that our models perform well in all the four languages on the setups of IJCNLP Shared Task on Customer Feedback Analysis. Our model achieved the second rank in French, with an accuracy of 71.75{\\%} and third ranks for all the other languages.",
            "cx": 14524.6,
            "cy": -385.831,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "E17-1077",
            "name": "End-to-end Relation Extraction using Neural Networks and {M}arkov {L}ogic {N}etworks",
            "publication_data": 2017,
            "citation": 5,
            "abstract": "End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions. Traditionally, separate predictive models were trained for each of these tasks and were used in a {``}pipeline{''} fashion where output of one model is fed as input to another. But it was observed that addressing some of these tasks jointly results in better performance. We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, entity type classification and relation type classification. This model is referred to as {``}All Word Pairs{''} model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.",
            "cx": 14729.6,
            "cy": -385.831,
            "rx": 100.318,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "E17-1109",
            "name": "Entity Extraction in Biomedical Corpora: An Approach to Evaluate Word Embedding Features with {PSO} based Feature Selection",
            "publication_data": 2017,
            "citation": 7,
            "abstract": "Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for biomedical text mining. In this paper, we propose a novel approach of feature selection for entity extraction that exploits the concept of deep learning and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other features extracted by studying the properties of the datasets. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86{\\%}, 5.27{\\%} and 7.25{\\%} F-measure points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.",
            "cx": 4936.6,
            "cy": -385.831,
            "rx": 80.7205,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D17-3002",
            "name": "Computational Sarcasm",
            "publication_data": 2017,
            "citation": "???",
            "abstract": "Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Motivated by challenges posed by sarcastic text to sentiment analysis, computational approaches to sarcasm have witnessed a growing interest at NLP forums in the past decade. Computational sarcasm refers to automatic approaches pertaining to sarcasm. The tutorial will provide a bird{'}s-eye view of the research in computational sarcasm for text, while focusing on significant milestones.The tutorial begins with linguistic theories of sarcasm, with a focus on incongruity: a useful notion that underlies sarcasm and other forms of figurative language. Since the most significant work in computational sarcasm is sarcasm detection: predicting whether a given piece of text is sarcastic or not, sarcasm detection forms the focus hereafter. We begin our discussion on sarcasm detection with datasets, touching on strategies, challenges and nature of datasets. Then, we describe algorithms for sarcasm detection: rule-based (where a specific evidence of sarcasm is utilised as a rule), statistical classifier-based (where features are designed for a statistical classifier), a topic model-based technique, and deep learning-based algorithms for sarcasm detection. In case of each of these algorithms, we refer to our work on sarcasm detection and share our learnings. Since information beyond the text to be classified, contextual information is useful for sarcasm detection, we then describe approaches that use such information through conversational context or author-specific context.We then follow it by novel areas in computational sarcasm such as sarcasm generation, sarcasm v/s irony classification, etc. We then summarise the tutorial and describe future directions based on errors reported in past work. The tutorial will end with a demonstration of our work on sarcasm detection.This tutorial will be of interest to researchers investigating computational sarcasm and related areas such as computational humour, figurative language understanding, emotion and sentiment sentiment analysis, etc. The tutorial is motivated by our continually evolving survey paper of sarcasm detection, that is available on arXiv at: Joshi, Aditya, Pushpak Bhattacharyya, and Mark James Carman. {``}Automatic Sarcasm Detection: A Survey.{''} arXiv preprint arXiv:1602.03426 (2016).",
            "cx": 14970.6,
            "cy": -385.831,
            "rx": 122.159,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019",
            "citation_count": 14,
            "name": 14,
            "cx": 28.5975,
            "cy": -206.35,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "Y18-3012",
            "name": "{IITP}-{MT} at {WAT}2018: Transformer-based Multilingual Indic-{E}nglish Neural Machine Translation System",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "None",
            "cx": 12878.6,
            "cy": -296.09,
            "rx": 115.931,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.wat-1.29",
            "name": "{IITP}-{MT} at {WAT}2021: Indic-{E}nglish Multilingual Neural Machine Translation using {R}omanized Vocabulary",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "This paper describes the systems submitted to WAT 2021 MultiIndicMT shared task by IITP-MT team. We submit two multilingual Neural Machine Translation (NMT) systems (Indic-to-English and English-to-Indic). We romanize all Indic data and create subword vocabulary which is shared between all Indic languages. We use back-translation approach to generate synthetic data which is appended to parallel corpus and used to train our models. The models are evaluated using BLEU, RIBES and AMFM scores with Indic-to-English model achieving 40.08 BLEU for Hindi-English pair and English-to-Indic model achieving 34.48 BLEU for English-Hindi pair. However, we observe that the shared romanized subword vocabulary is not helping English-to-Indic model at the time of generation, leading it to produce poor quality translations for Tamil, Telugu and Malayalam to English pairs with BLEU score of 8.51, 6.25 and 3.79 respectively.",
            "cx": 12878.6,
            "cy": -26.8701,
            "rx": 133.787,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.bea-1.8",
            "name": "Can Neural Networks Automatically Score Essay Traits?",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include Content, Organization, Language, Sentence Fluency, Word Choice, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring - where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing - which is why trait-scoring is important. In this paper, we show how a deep-learning based system can outperform feature-based machine learning systems, as well as a string kernel system in scoring essay traits.",
            "cx": 3328.6,
            "cy": -116.61,
            "rx": 79.3924,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P18-2011",
            "name": "Identification of Alias Links among Participants in Narratives",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Identification of distinct and independent participants (entities of interest) in a narrative is an important task for many NLP applications. This task becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns, pronouns or noun phrases with common noun headword. We use Markov Logic Network (MLN) to encode the linguistic knowledge for identification of aliases. We evaluate on four diverse history narratives of varying complexity. Our approach performs better than the state-of-the-art approach as well as a combination of standard named entity recognition and coreference resolution techniques.",
            "cx": 13214.6,
            "cy": -296.09,
            "rx": 139.1,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-2404",
            "name": "Extraction of Message Sequence Charts from Narrative History Text",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "In this paper, we advocate the use of Message Sequence Chart (MSC) as a knowledge representation to capture and visualize multi-actor interactions and their temporal ordering. We propose algorithms to automatically extract an MSC from a history narrative. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extraction, we employ a state-of-the art algorithm to temporally re-order these interactions. Our evaluation on multiple publicly available narratives shows improvements over four baselines.",
            "cx": 13075.6,
            "cy": -206.35,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N19-2017",
            "name": "Extraction of Message Sequence Charts from Software Use-Case Descriptions",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Software Requirement Specification documents provide natural language descriptions of the core functional requirements as a set of use-cases. Essentially, each use-case contains a set of actors and sequences of steps describing the interactions among them. Goals of use-case reviews and analyses include their correctness, completeness, detection of ambiguities, prototyping, verification, test case generation and traceability. Message Sequence Chart (MSC) have been proposed as a expressive, rigorous yet intuitive visual representation of use-cases. In this paper, we describe a linguistic knowledge-based approach to extract MSCs from use-cases. Compared to existing techniques, we extract richer constructs of the MSC notation such as timers, conditions and alt-boxes. We apply this tool to extract MSCs from several real-life software use-case descriptions and show that it performs better than the existing techniques. We also discuss the benefits and limitations of the extracted MSCs to meet the above goals.",
            "cx": 13278.6,
            "cy": -206.35,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.icon-main.23",
            "name": "Cognitively Aided Zero-Shot Automatic Essay Grading",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Automatic essay grading (AEG) is a process in which machines assign a grade to an essay written in response to a topic, called the prompt. Zero-shot AEG is when we train a system to grade essays written to a new prompt which was not present in our training data. In this paper, we describe a solution to the problem of zero-shot automatic essay grading, using cognitive information, in the form of gaze behaviour. Our experiments show that using gaze behaviour helps in improving the performance of AEG systems, especially when we provide a new essay written in response to a new prompt for scoring, by an average of almost 5 percentage points of QWK.",
            "cx": 6396.6,
            "cy": -116.61,
            "rx": 107.46,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "N18-2044",
            "name": "Multi-Task Learning Framework for Mining Crowd Intelligence towards Clinical Treatment",
            "publication_data": 2018,
            "citation": 6,
            "abstract": "In recent past, social media has emerged as an active platform in the context of healthcare and medicine. In this paper, we present a study where medical user{'}s opinions on health-related issues are analyzed to capture the medical sentiment at a blog level. The medical sentiments can be studied in various facets such as medical condition, treatment, and medication that characterize the overall health status of the user. Considering these facets, we treat analysis of this information as a multi-task classification problem. In this paper, we adopt a novel adversarial learning approach for our multi-task learning framework to learn the sentiment{'}s strengths expressed in a medical blog. Our evaluation shows promising results for our target tasks.",
            "cx": 13472.6,
            "cy": -296.09,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N18-1061",
            "name": "Fine-Grained Temporal Orientation and its Relationship with Psycho-Demographic Correlates",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "Temporal orientation refers to an individual{'}s tendency to connect to the psychological concepts of past, present or future, and it affects personality, motivation, emotion, decision making and stress coping processes. The study of the social media users{'} psycho-demographic attributes from the perspective of human temporal orientation can be of utmost interest and importance to the business and administrative decision makers as it can provide an extra precious information for them to make informed decisions. In this paper, we propose a very first study to demonstrate the association between the sentiment view of the temporal orientation of the users and their different psycho-demographic attributes by analyzing their tweets. We first create a temporal orientation classifier in a minimally supervised way which classifies each tweet of the users in one of the three temporal categories, namely past, present, and future. A deep Bi-directional Long Short Term Memory (BLSTM) is used for the tweet classification task. Our tweet classifier achieves an accuracy of 78.27{\\%} when tested on a manually created test set. We then determine the users{'} overall temporal orientation based on their tweets on the social media. The sentiment is added to the tweets at the fine-grained level where each temporal tweet is given a sentiment with either of the positive, negative or neutral. Our experiment reveals that depending upon the sentiment view of temporal orientation, a user{'}s attributes vary. We finally measure the correlation between the users{'} sentiment view of temporal orientation and their different psycho-demographic factors using regression.",
            "cx": 13716.6,
            "cy": -296.09,
            "rx": 125.316,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1187",
            "name": "{ASAP}++: Enriching the {ASAP} Automated Essay Grading Dataset with Essay Attribute Scores",
            "publication_data": 2018,
            "citation": 0,
            "abstract": "None",
            "cx": 13971.6,
            "cy": -296.09,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1278",
            "name": "A Deep Neural Network based Approach for Entity Extraction in Code-Mixed {I}ndian Social Media Text",
            "publication_data": 2018,
            "citation": 5,
            "abstract": "None",
            "cx": 3760.6,
            "cy": -296.09,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.aacl-main.90",
            "name": "A Unified Framework for Multilingual and Code-Mixed Visual Question Answering",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we propose an effective deep learning framework for multilingual and code- mixed visual question answering. The pro- posed model is capable of predicting answers from the questions in Hindi, English or Code- mixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Vi- sual Question Answering (VQA) focus on En- glish questions only. However, many applica- tions such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust tech- nique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the dif- ferent languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image fea- tures. We perform extensive evaluation and ablation studies for English, Hindi and Code- mixed VQA. The evaluation shows that the proposed multilingual model achieves state-of- the-art performance in all these settings.",
            "cx": 3984.6,
            "cy": -116.61,
            "rx": 65.5227,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "L18-1440",
            "name": "{MMQA}: A Multi-domain Multi-lingual Question-Answering Framework for {E}nglish and {H}indi",
            "publication_data": 2018,
            "citation": 6,
            "abstract": "None",
            "cx": 3574.6,
            "cy": -296.09,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.coling-main.249",
            "name": "Reinforced Multi-task Approach for Multi-hop Question Generation",
            "publication_data": 2020,
            "citation": 0,
            "abstract": "Question generation (QG) attempts to solve the inverse of question answering (QA) problem by generating a natural language question given a document and an answer. While sequence to sequence neural models surpass rule-based systems for QG, they are limited in their capacity to focus on more than one supporting fact. For QG, we often require multiple supporting facts to generate high-quality questions. Inspired by recent works on multi-hop reasoning in QA, we take up Multi-hop question generation, which aims at generating relevant questions based on supporting facts in the context. We employ multitask learning with the auxiliary task of answer-aware supporting fact prediction to guide the question generator. In addition, we also proposed a question-aware reward function in a Reinforcement Learning (RL) framework to maximize the utilization of the supporting facts. We demonstrate the effectiveness of our approach through experiments on the multi-hop question answering dataset, HotPotQA. Empirical evaluation shows our model to outperform the single-hop neural question generation models on both automatic evaluation metrics such as BLEU, METEOR, and ROUGE and human evaluation metrics for quality and coverage of the generated questions.",
            "cx": 3718.6,
            "cy": -116.61,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "L18-1442",
            "name": "Medical Sentiment Analysis using Social Media: Towards building a Patient Assisted System",
            "publication_data": 2018,
            "citation": 5,
            "abstract": "None",
            "cx": 5550.6,
            "cy": -296.09,
            "rx": 89.191,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.findings-emnlp.386",
            "name": "Looking inside Noun Compounds: Unsupervised Prepositional and Free Paraphrasing",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "A noun compound is a sequence of contiguous nouns that acts as a single noun, although the predicate denoting the semantic relation between its components is dropped. Noun Compound Interpretation is the task of uncovering the relation, in the form of a preposition or a free paraphrase. Prepositional paraphrasing refers to the use of preposition to explain the semantic relation, whereas free paraphrasing refers to invoking an appropriate predicate denoting the semantic relation. In this paper, we propose an unsupervised methodology for these two types of paraphrasing. We use pre-trained contextualized language models to uncover the {`}missing{'} words (preposition or predicate). These language models are usually trained to uncover the missing word/words in a given input sentence. Our approach uses templates to prepare the input sequence for the language model. The template uses a special token to indicate the missing predicate. As the model has already been pre-trained to uncover a missing word (or a sequence of words), we exploit it to predict missing words for the input sequence. Our experiments using four datasets show that our unsupervised approach (a) performs comparably to supervised approaches for prepositional paraphrasing, and (b) outperforms supervised approaches for free paraphrasing. Paraphrasing (prepositional or free) using our unsupervised approach is potentially helpful for NLP tasks like machine translation and information extraction.",
            "cx": 7475.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.findings-acl.256",
            "name": "{F}rame{N}et-assisted Noun Compound Interpretation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "None",
            "cx": 7475.6,
            "cy": -26.8701,
            "rx": 168.997,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W19-5426",
            "name": "Utilizing Monolingual Data in {NMT} for Similar Languages: Submission to Similar Language Translation Task",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi -{\\textgreater} Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.",
            "cx": 3060.6,
            "cy": -206.35,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.mtsummit-research.2",
            "name": "Investigating Active Learning in Interactive Neural Machine Translation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Interactive-predictive translation is a collaborative iterative process and where human translators produce translations with the help of machine translation (MT) systems interactively. Various sampling techniques in active learning (AL) exist to update the neural MT (NMT) model in the interactive-predictive scenario. In this paper and we explore term based (named entity count (NEC)) and quality based (quality estimation (QE) and sentence similarity (Sim)) sampling techniques {--} which are used to find the ideal candidates from the incoming data {--} for human supervision and MT model{'}s weight updation. We carried out experiments with three language pairs and viz. German-English and Spanish-English and Hindi-English. Our proposed sampling technique yields 1.82 and 0.77 and 0.81 BLEU points improvements for German-English and Spanish-English and Hindi-English and respectively and over random sampling based baseline. It also improves the present state-of-the-art by 0.35 and 0.12 BLEU points for German-English and Spanish-English and respectively. Human editing effort in terms of number-of-words-changed also improves by 5 and 4 points for German-English and Spanish-English and respectively and compared to the state-of-the-art.",
            "cx": 3183.6,
            "cy": -26.8701,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.calcs-1.5",
            "name": "{IITP}-{MT} at {CALCS}2021: {E}nglish to {H}inglish Neural Machine Translation using Unsupervised Synthetic Code-Mixed Parallel Corpus",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "This paper describes the system submitted by IITP-MT team to Computational Approaches to Linguistic Code-Switching (CALCS 2021) shared task on MT for English\u00e2\u0086\u0092Hinglish. We submit a neural machine translation (NMT) system which is trained on the synthetic code-mixed (cm) English-Hinglish parallel corpus. We propose an approach to create code-mixed parallel corpus from a clean parallel corpus in an unsupervised manner. It is an alignment based approach and we do not use any linguistic resources for explicitly marking any token for code-switching. We also train NMT model on the gold corpus provided by the workshop organizers augmented with the generated synthetic code-mixed parallel corpus. The model trained over the generated synthetic cm data achieves 10.09 BLEU points over the given test set.",
            "cx": 3473.6,
            "cy": -26.8701,
            "rx": 133.787,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "L18-1559",
            "name": "{TAP}-{DLND} 1.0 : A Corpus for Document Level Novelty Detection",
            "publication_data": 2018,
            "citation": 2,
            "abstract": "Detecting novelty of an entire document is an Artificial Intelligence (AI) frontier problem that has widespread NLP applications, such as extractive document summarization, tracking development of news events, predicting impact of scholarly articles, etc. Important though the problem is, we are unaware of any benchmark document level data that correctly addresses the evaluation of automatic novelty detection techniques in a classification framework. To bridge this gap, we present here a resource for benchmarking the techniques for document level novelty detection. We create the resource via event-specific crawling of news documents across several domains in a periodic manner. We release the annotated corpus with necessary statistics and show its use with a developed system for the problem in concern.",
            "cx": 14236.6,
            "cy": -296.09,
            "rx": 135.115,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "C18-1237",
            "name": "Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection",
            "publication_data": 2018,
            "citation": 1,
            "abstract": "The rapid growth of documents across the web has necessitated finding means of discarding redundant documents and retaining novel ones. Capturing redundancy is challenging as it may involve investigating at a deep semantic level. Techniques for detecting such semantic redundancy at the document level are scarce. In this work we propose a deep Convolutional Neural Networks (CNN) based model to classify a document as novel or redundant with respect to a set of relevant documents already seen by the system. The system is simple and do not require any manual feature engineering. Our novel scheme encodes relevant and relative information from both source and target texts to generate an intermediate representation which we coin as the Relative Document Vector (RDV). The proposed method outperforms the existing state-of-the-art on a document-level novelty detection dataset by a margin of \u00e2\u0088\u00bc5{\\%} in terms of accuracy. We further demonstrate the effectiveness of our approach on a standard paraphrase detection dataset where paraphrased passages closely resemble to semantically redundant documents.",
            "cx": 14487.6,
            "cy": -296.09,
            "rx": 98.0761,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2021.ltedi-1.29",
            "name": "{CFILT} {IIT} {B}ombay@{LT}-{EDI}-{EACL}2021: Hope Speech Detection for Equality, Diversity, and Inclusion using Multilingual Representation from{T}ransformers",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "With the internet becoming part and parcel of our lives, engagement in social media has increased a lot. Identifying and eliminating offensive content from social media has become of utmost priority to prevent any kind of violence. However, detecting encouraging, supportive and positive content is equally important to prevent misuse of censorship targeted to attack freedom of speech. This paper presents our system for the shared task Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI, EACL 2021. The data for this shared task is provided in English, Tamil, and Malayalam which was collected from YouTube comments. It is a multiclass classification problem where each data instance is categorized into one of the three classes: {`}Hope speech{'}, {`}Not hope speech{'}, and {`}Not in intended language{'}. We propose a system that employs multilingual transformer models to obtain the representation of text and classifies it into one of the three classes. We explored the use of multilingual models trained specifically for Indian languages along with generic multilingual models. Our system was ranked 2nd for English, 2nd for Malayalam, and 7th for the Tamil language in the final leader board published by organizers and obtained a weighted F1-score of 0.92, 0.84, 0.55 respectively on the hidden test dataset used for the competition. We have made our system publicly available at GitHub.",
            "cx": 4117.6,
            "cy": -26.8701,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "D18-1382",
            "name": "Contextual Inter-modal Attention for Multi-modal Sentiment Analysis",
            "publication_data": 2018,
            "citation": 7,
            "abstract": "Multi-modal sentiment analysis offers various challenges, one being the effective combination of different input modalities, namely text, visual and acoustic. In this paper, we propose a recurrent neural network based multi-modal attention framework that leverages the contextual information for utterance-level sentiment prediction. The proposed approach applies attention on multi-modal multi-utterance representations and tries to learn the contributing features amongst them. We evaluate our proposed approach on two multi-modal sentiment analysis benchmark datasets, viz. CMU Multi-modal Opinion-level Sentiment Intensity (CMU-MOSI) corpus and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Evaluation results show the effectiveness of our proposed approach with the accuracies of 82.31{\\%} and 79.80{\\%} for the MOSI and MOSEI datasets, respectively. These are approximately 2 and 1 points performance improvement over the state-of-the-art models for the datasets.",
            "cx": 5774.6,
            "cy": -296.09,
            "rx": 117.26,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N19-1034",
            "name": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis",
            "publication_data": 2019,
            "citation": 4,
            "abstract": "Related tasks often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both. The multi-modal inputs (i.e. text, acoustic and visual frames) of a video convey diverse and distinctive information, and usually do not have equal contribution in the decision making. We propose a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance. We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment and emotion analysis. Evaluation results suggest that multi-task learning framework offers improvement over the single-task framework. The proposed approach reports new state-of-the-art performance for both sentiment analysis and emotion analysis.",
            "cx": 5528.6,
            "cy": -206.35,
            "rx": 82.9636,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "D19-1566",
            "name": "Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis",
            "publication_data": 2019,
            "citation": 2,
            "abstract": "In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information, (e.g., textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. We employ a context-aware attention module to exploit the correspondence among the neighboring utterances. We evaluate our proposed approach for five standard multi-modal affect analysis datasets. Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.",
            "cx": 5752.6,
            "cy": -206.35,
            "rx": 123.988,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.coling-main.393",
            "name": "{MEISD}: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Emotion and sentiment classification in dialogues is a challenging task that has gained popularity in recent times. Humans tend to have multiple emotions with varying intensities while expressing their thoughts and feelings. Emotions in an utterance of dialogue can either be independent or dependent on the previous utterances, thus making the task complex and interesting. Multi-label emotion detection in conversations is a significant task that provides the ability to the system to understand the various emotions of the users interacting. Sentiment analysis in dialogue/conversation, on the other hand, helps in understanding the perspective of the user with respect to the ongoing conversation. Along with text, additional information in the form of audio and video assist in identifying the correct emotions with the appropriate intensity and sentiments in an utterance of a dialogue. Lately, quite a few datasets have been made available for dialogue emotion and sentiment classification, but these datasets are imbalanced in representing different emotions and consist of an only single emotion. Hence, we present at first a large-scale balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD), collected from different TV series that has textual, audio and visual features, and then establish a baseline setup for further research.",
            "cx": 5496.6,
            "cy": -116.61,
            "rx": 111.946,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.acl-main.401",
            "name": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit. For multi-tasking, we propose two attention mechanisms, viz. Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention). The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities. In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities. Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking. Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems. The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.",
            "cx": 5731.6,
            "cy": -116.61,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "C18-1042",
            "name": "Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy",
            "publication_data": 2018,
            "citation": 3,
            "abstract": "In this paper, we propose a hybrid technique for semantic question matching. It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art deep learning models with question classes obtained from a deep learning based question classifier. Experiments performed on three open-domain datasets demonstrate the effectiveness of our proposed approach. We achieve state-of-the-art results on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis shows that coupling standard distributional features (provided by the question encoder) with knowledge from taxonomy is more effective than either deep learning or taxonomy-based knowledge alone.",
            "cx": 3919.6,
            "cy": -296.09,
            "rx": 74.9067,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2018.gwc-1.47",
            "name": "pyiwn: A Python based {API} to access {I}ndian Language {W}ord{N}ets",
            "publication_data": 2018,
            "citation": "???",
            "abstract": "Indian language WordNets have their individual web-based browsing interfaces along with a common interface for IndoWordNet. These interfaces prove to be useful for language learners and in an educational domain, however, they do not provide the functionality of connecting to them and browsing their data through a lucid application programming interface or an API. In this paper, we present our work on creating such an easy-to-use framework which is bundled with the data for Indian language WordNets and provides NLTK WordNet interface like core functionalities in Python. Additionally, we use a pre-built speech synthesis system for Hindi language and augment Hindi data with audios for words, glosses, and example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development.",
            "cx": 14681.6,
            "cy": -296.09,
            "rx": 77.5641,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.nuse-1.11",
            "name": "Extracting Message Sequence Charts from {H}indi Narrative Text",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we propose the use of Message Sequence Charts (MSC) as a representation for visualizing narrative text in Hindi. An MSC is a formal representation allowing the depiction of actors and interactions among these actors in a scenario, apart from supporting a rich framework for formal inference. We propose an approach to extract MSC actors and interactions from a Hindi narrative. As a part of the approach, we enrich an existing event annotation scheme where we provide guidelines for annotation of the mood of events (realis vs irrealis) and guidelines for annotation of event arguments. We report performance on multiple evaluation criteria by experimenting with Hindi narratives from Indian History. Though Hindi is the fourth most-spoken first language in the world, from the NLP perspective it has comparatively lesser resources than English. Moreover, there is relatively less work in the context of event processing in Hindi. Hence, we believe that this work is among the initial works for Hindi event processing.",
            "cx": 14681.6,
            "cy": -116.61,
            "rx": 100.318,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -116.61,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-7511",
            "name": "Utilizing Word Embeddings based Features for Phylogenetic Tree Generation of {S}anskrit Texts",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "None",
            "cx": 14800.6,
            "cy": -206.35,
            "rx": 91.4341,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "W19-5440",
            "name": "Parallel Corpus Filtering Based on Fuzzy String Matching",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In this paper, we describe the IIT Patna{'}s submission to WMT 2019 shared task on parallel corpus filtering. This shared task asks the participants to develop methods for scoring each parallel sentence from a given noisy parallel corpus. Quality of the scoring method is judged based on the quality of SMT and NMT systems trained on smaller set of high-quality parallel sentences sub-sampled from the original noisy corpus. This task has two language pairs. We submit for both the Nepali-English and Sinhala-English language pairs. We define fuzzy string matching score between English and the translated (into English) source based on Levenshtein distance. Based on the scores, we sub-sample two sets (having 1 million and 5 millions English tokens) of parallel sentences from each parallel corpus, and train SMT systems for development purpose only. The organizers publish the official evaluation using both SMT and NMT on the final official test set. Total 10 teams participated in the shared task and according the official evaluation, our scoring method obtains 2nd position in the team ranking for 1-million NepaliEnglish NMT and 5-million Sinhala-English NMT categories.",
            "cx": 14998.6,
            "cy": -206.35,
            "rx": 88.2768,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "W19-5346",
            "name": "{IITP}-{MT} System for {G}ujarati-{E}nglish News Translation Task at {WMT} 2019",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati\u00e2\u0086\u0092English and English\u00e2\u0086\u0092Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.",
            "cx": 15220.6,
            "cy": -206.35,
            "rx": 115.931,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "P19-1106",
            "name": "{D}eep{S}enti{P}eer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Automatically validating a research artefact is one of the frontiers in Artificial Intelligence (AI) that directly brings it close to competing with human intellect and intuition. Although criticised sometimes, the existing peer review system still stands as the benchmark of research validation. The present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of AI. However, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. Here in this work, we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome. Our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and review{'}s polarity to predict the overall recommendation score as well as the final decision. We achieve significant performance improvement over the baselines (\u00e2\u0088\u00bc 29{\\%} error reduction) proposed in a recently released dataset of peer reviews. An AI of this kind could assist the editors/program chairs as an additional layer of confidence, especially when non-responding/missing reviewers are frequent in present day peer review.",
            "cx": 15510.6,
            "cy": -206.35,
            "rx": 156.042,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1297",
            "name": "Multilingual Unsupervised {NMT} using Shared Encoder and Language-Specific Decoders",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder{'}s ability to generate interlingual representation.",
            "cx": 15790.6,
            "cy": -206.35,
            "rx": 105.218,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1516",
            "name": "A Unified Multi-task Adversarial Learning Framework for Pharmacovigilance Mining",
            "publication_data": 2019,
            "citation": 1,
            "abstract": "The mining of adverse drug reaction (ADR) has a crucial role in the pharmacovigilance. The traditional ways of identifying ADR are reliable but time-consuming, non-scalable and offer a very limited amount of ADR relevant information. With the unprecedented growth of information sources in the forms of social media texts (Twitter, Blogs, Reviews etc.), biomedical literature, and Electronic Medical Records (EMR), it has become crucial to extract the most pertinent ADR related information from these free-form texts. In this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract ADRs from various sources. We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources. Unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as {`}Indications{'}, {`}Symptoms{'}, {`}Finding{'}, {`}Disease{'}, {`}Drug{'}) which provide important cues in pharmacovigilance. We evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a Twitter dataset from PSB 2016 Social Me- dia Shared Task, CADEC corpus and Medline ADR corpus. Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. This establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets.",
            "cx": 15978.6,
            "cy": -206.35,
            "rx": 65.1077,
            "ry": 26.7407,
            "stroke": "red",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "P19-1540",
            "name": "Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.",
            "cx": 16145.6,
            "cy": -206.35,
            "rx": 84.2917,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "N19-1091",
            "name": "Courteously Yours: Inducing courteous behavior in Customer Care responses using Reinforced Pointer Generator Network",
            "publication_data": 2019,
            "citation": 0,
            "abstract": "In this paper, we propose an effective deep learning framework for inducing courteous behavior in customer care responses. The interaction between a customer and the customer care representative contributes substantially to the overall customer experience. Thus it is imperative for customer care agents and chatbots engaging with humans to be personal, cordial and emphatic to ensure customer satisfaction and retention. Our system aims at automatically transforming neutral customer care responses into courteous replies. Along with stylistic transfer (of courtesy), our system ensures that responses are coherent with the conversation history, and generates courteous expressions consistent with the emotional state of the customer. Our technique is based on a reinforced pointer-generator model for the sequence to sequence task. The model is also conditioned on a hierarchically encoded and emotionally aware conversational context. We use real interactions on Twitter between customer care professionals and aggrieved customers to create a large conversational dataset having both forms of agent responses: {`}generic{'} and {`}courteous{'}. We perform quantitative and qualitative analyses on established and task-specific metrics, both automatic and human evaluation based. Our evaluation shows that the proposed models can generate emotionally-appropriate courteous expressions while preserving the content. Experimental results also prove that our proposed approach performs better than the baseline models.",
            "cx": 16349.6,
            "cy": -206.35,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.lrec-1.514",
            "name": "Incorporating Politeness across Languages in Customer Care Responses: Towards building a Multi-lingual Empathetic Dialogue Agent",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Customer satisfaction is an essential aspect of customer care systems. It is imperative for such systems to be polite while handling customer requests/demands. In this paper, we present a large multi-lingual conversational dataset for English and Hindi. We choose data from Twitter having both generic and courteous responses between customer care agents and aggrieved users. We also propose strong baselines that can induce courteous behaviour in generic customer care response in a multi-lingual scenario. We build a deep learning framework that can simultaneously handle different languages and incorporate polite behaviour in the customer care agent{'}s responses. Our system is competent in generating responses in different languages (here, English and Hindi) depending on the customer{'}s preference and also is able to converse with humans in an empathetic manner to ensure customer satisfaction and retention. Experimental results show that our proposed models can converse in both the languages and the information shared between the languages helps in improving the performance of the overall system. Qualitative and quantitative analysis shows that the proposed method can converse in an empathetic manner by incorporating courteousness in the responses and hence increasing customer satisfaction.",
            "cx": 16349.6,
            "cy": -116.61,
            "rx": 123.073,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.icon-1.2",
            "name": "A Deep Ensemble Framework for Fake News Detection and Multi-Class Classification of Short Political Statements",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "Fake news, rumor, incorrect information, and misinformation detection are nowadays crucial issues as these might have serious consequences for our social fabrics. Such information is increasing rapidly due to the availability of enormous web information sources including social media feeds, news blogs, online newspapers etc. In this paper, we develop various deep learning models for detecting fake news and classifying them into the pre-defined fine-grained categories. At first, we develop individual models based on Convolutional Neural Network (CNN), and Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations obtained from these two models are fed into a Multi-layer Perceptron Model (MLP) for the final classification. Our experiments on a benchmark dataset show promising results with an overall accuracy of 44.87{\\%}, which outperforms the current state of the arts.",
            "cx": 16525.6,
            "cy": -206.35,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.icon-1.16",
            "name": "Multi-linguality helps: Event-Argument Extraction for Disaster Domain in Cross-lingual and Multi-lingual setting",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "Automatic extraction of disaster-related events and their arguments from natural language text is vital for building a decision support system for crisis management. Event extraction from various news sources is a well-explored area for this objective. However, extracting events alone, without any context, provides only partial help for this purpose. Extracting related arguments like Time, Place, Casualties, etc., provides a complete picture of the disaster event. In this paper, we create a disaster domain dataset in Hindi by annotating disaster-related event and arguments. We also obtain equivalent datasets for Bengali and English from a collaboration. We build a multi-lingual deep learning model for argument extraction in all the three languages. We also compare our multi-lingual system with a similar baseline mono-lingual system trained for each language separately. It is observed that a single multi-lingual system is able to compensate for lack of training data, by using joint training of dataset from different languages in shared space, thus giving a better overall result.",
            "cx": 16713.6,
            "cy": -206.35,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.icon-1.19",
            "name": "A Multi-task Model for Multilingual Trigger Detection and Classification",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "In this paper we present a deep multi-task learning framework for multilingual event and argument trigger detection and classification. In our current work, we identify detection and classification of both event and argument triggers as related tasks and follow a multi-tasking approach to solve them simultaneously in contrast to the previous works where these tasks were solved separately or learning some of the above mentioned tasks jointly. We evaluate the proposed approach with multiple low-resource Indian languages. As there were no datasets available for the Indian languages, we have annotated disaster related news data crawled from the online news portal for different low-resource Indian languages for our experiments. Our empirical evaluation shows that multi-task model performs better than the single task model, and classification helps in trigger detection and vice-versa.",
            "cx": 16905.6,
            "cy": -206.35,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.icon-1.20",
            "name": "Converting Sentiment Annotated Data to Emotion Annotated Data",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "Existing supervised solutions for emotion classification demand large amount of emotion annotated data. Such resources may not be available for many languages. However, it is common to have sentiment annotated data available in these languages. The sentiment information (+1 or -1) is useful to segregate between positive emotions or negative emotions. In this paper, we propose an unsupervised approach for emotion recognition by taking advantage of the sentiment information. Given a sentence and its sentiment information, recognize the best possible emotion for it. For every sentence, the semantic relatedness between the words from sentence and a set of emotion-specific words is calculated using cosine similarity. An emotion vector representing the emotion score for each emotion category of Ekman{'}s model, is created. It is further improved with the dependency relations and the best possible emotion is predicted. The results show the significant improvement in f-score values for text with sentiment information as input over our baseline as text without sentiment information. We report the weighted f-score on three different datasets with the Ekman{'}s emotion model. This supports that by leveraging the sentiment value, better emotion annotated data can be created.",
            "cx": 17085.6,
            "cy": -206.35,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.icon-1.27",
            "name": "A Deep Learning Approach for Automatic Detection of Fake News",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "Fake news detection is a very prominent and essential task in the field of journalism. This challenging problem is seen so far in the field of politics, but it could be even more challenging when it is to be determined in the multi-domain platform. In this paper, we propose two effective models based on deep learning for solving fake news detection problem in online news contents of multiple domains. We evaluate our techniques on the two recently released datasets, namely Fake News AMT and Celebrity for fake news detection. The proposed systems yield encouraging performance, outperforming the current hand-crafted feature engineering based state-of-the-art system with a significant margin of 3.08{\\%} and 9.3{\\%} by the two models, respectively. In order to exploit the datasets, available for the related tasks, we perform cross-domain analysis (model trained on FakeNews AMT and tested on Celebrity and vice versa) to explore the applicability of our systems across the domains.",
            "cx": 17261.6,
            "cy": -206.35,
            "rx": 56.6372,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2019.gwc-1.51",
            "name": "Utilizing Wordnets for Cognate Detection among {I}ndian Languages",
            "publication_data": 2019,
            "citation": "???",
            "abstract": "Automatic Cognate Detection (ACD) is a challenging task which has been utilized to help NLP applications like Machine Translation, Information Retrieval and Computational Phylogenetics. Unidentified cognate pairs can pose a challenge to these applications and result in a degradation of performance. In this paper, we detect cognate word pairs among ten Indian languages with Hindi and use deep learning methodologies to predict whether a word pair is cognate or not. We identify IndoWordnet as a potential resource to detect cognate word pairs based on orthographic similarity-based methods and train neural network models using the data obtained from it. We identify parallel corpora as another potential resource and perform the same experiments for them. We also validate the contribution of Wordnets through further experimentation and report improved performance of up to 26{\\%}. We discuss the nuances of cognate detection among closely related Indian languages and release the lists of detected cognates as a dataset. We also observe the behaviour of, to an extent, unrelated Indian language pairs and release the lists of detected cognates among them as well.",
            "cx": 17427.6,
            "cy": -206.35,
            "rx": 91.4341,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.lrec-1.378",
            "name": "Challenge Dataset of Cognates and False Friend Pairs from {I}ndian Languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Cognates are present in multiple variants of the same text across different languages (e.g., {``}hund{''} in German and {``}hound{''} in the English language mean {``}dog{''}). They pose a challenge to various Natural Language Processing (NLP) applications such as Machine Translation, Cross-lingual Sense Disambiguation, Computational Phylogenetics, and Information Retrieval. A possible solution to address this challenge is to identify cognates across language pairs. In this paper, we describe the creation of two cognate datasets for twelve Indian languages namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. We digitize the cognate data from an Indian language cognate dictionary and utilize linked Indian language Wordnets to generate cognate sets. Additionally, we use the Wordnet data to create a False Friends{'} dataset for eleven language pairs. We also evaluate the efficacy of our dataset using previously available baseline cognate detection approaches. We also perform a manual evaluation with the help of lexicographers and release the curated gold-standard dataset with this paper.",
            "cx": 17317.6,
            "cy": -116.61,
            "rx": 97.6615,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.coling-main.119",
            "name": "Harnessing Cross-lingual Features to Improve Cognate Detection for Low-resource Languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Cognates are variants of the same lexical form across different languages; for example {``}fonema{''} in Spanish and {``}phoneme{''} in English are cognates, both of which mean {``}a unit of sound{''}. The task of automatic detection of cognates among any two languages can help downstream NLP tasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine Translation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen Indian Languages. Our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. We, then, evaluate the impact of our cognate detection mechanism on neural machine translation (NMT), as a downstream task. We evaluate our methods to detect cognates on a challenging dataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets for two more Indian languages, Konkani and Nepali. We observe an improvement of up to 18{\\%} points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates extracted using our method help improve NMT quality by up to 2.76 BLEU. We also release our code, newly constructed datasets and cross-lingual models publicly.",
            "cx": 17536.6,
            "cy": -116.61,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021",
            "citation_count": 0,
            "name": 0,
            "cx": 28.5975,
            "cy": -26.8701,
            "rx": 28.6953,
            "ry": 18.0,
            "stroke": "black",
            "width": 1,
            "dasharray": null
        },
        {
            "id": "2020.wildre-1.1",
            "name": "Part-of-Speech Annotation Challenges in {M}arathi",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Part of Speech (POS) annotation is a significant challenge in natural language processing. The paper discusses issues and challenges faced in the process of POS annotation of the Marathi data from four domains viz., tourism, health, entertainment and agriculture. During POS annotation, a lot of issues were encountered. Some of the major ones are discussed in detail in this paper. Also, the two approaches viz., the lexical (L approach) and the functional (F approach) of POS tagging have been discussed and presented with examples. Further, some ambiguous cases in POS annotation are presented in the paper.",
            "cx": 17777.6,
            "cy": -116.61,
            "rx": 120.417,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.sltu-1.49",
            "name": "{``}A Passage to {I}ndia{''}: Pre-trained Word Embeddings for {I}ndian Languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Dense word vectors or {`}word embeddings{'} which encode semantic properties of words, have now become integral to NLP tasks like Machine Translation (MT), Question Answering (QA), Word Sense Disambiguation (WSD), and Information Retrieval (IR). In this paper, we use various existing approaches to create multiple word embeddings for 14 Indian languages. We place these embeddings for all these languages, \\textit{viz.}, Assamese, Bengali, Gujarati, Hindi, Kannada, Konkani, Malayalam, Marathi, Nepali, Odiya, Punjabi, Sanskrit, Tamil, and Telugu in a single repository. Relatively newer approaches that emphasize catering to context (BERT, ELMo, \\textit{etc.}) have shown significant improvements, but require a large amount of resources to generate usable models. We release pre-trained embeddings generated using both contextual and non-contextual approaches. We also use MUSE and XLM to train cross-lingual embeddings for all pairs of the aforementioned languages. To show the efficacy of our embeddings, we evaluate our embedding models on XPOS, UPOS and NER tasks for all these languages. We release a total of 436 models using 8 different approaches. We hope they are useful for the resource-constrained Indian language NLP. The title of this paper refers to the famous novel {``}A Passage to India{''} by E.M. Forster, published initially in 1924.",
            "cx": 577.597,
            "cy": -116.61,
            "rx": 79.3924,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.semeval-1.214",
            "name": "{EL}-{BERT} at {S}em{E}val-2020 Task 10: A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In visual media, text emphasis is the strengthening of words in a text to convey the intent of the author. Text emphasis in visual media is generally done by using different colors, backgrounds, or font for the text; it helps in conveying the actual meaning of the message to the readers. Emphasis selection is the task of choosing candidate words for emphasis, it helps in automatically designing posters and other media contents with written text. If we consider only the text and do not know the intent, then there can be multiple valid emphasis selections. We propose the use of ensembles for emphasis selection to improve over single emphasis selection models. We show that the use of multi-embedding helps in enhancing the results for base models. To show the efficacy of proposed approach we have also done a comparison of our results with state-of-the-art models.",
            "cx": 18046.6,
            "cy": -116.61,
            "rx": 130.63,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.semeval-1.261",
            "name": "{IITP}-{AINLPML} at {S}em{E}val-2020 Task 12: Offensive Tweet Identification and Target Categorization in a Multitask Environment",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we describe the participation of IITP-AINLPML team in the SemEval-2020 SharedTask 12 on Offensive Language Identification and Target Categorization in English Twitter data. Our proposed model learns to extract textual features using a BiGRU-based deep neural network supported by a Hierarchical Attention architecture to focus on the most relevant areas in the text. We leverage the effectiveness of multitask learning while building our models for sub-task A and B. We do necessary undersampling of the over-represented classes in the sub-tasks A and C.During training, we consider a threshold of 0.5 as the separation margin between the instances belonging to classes OFF and NOT in sub-task A and UNT and TIN in sub-task B. For sub-task C, the class corresponding to the maximum score among the given confidence scores of the classes(IND, GRP and OTH) is considered as the final label for an instance. Our proposed model obtains the macro F1-scores of 90.95{\\%}, 55.69{\\%} and 63.88{\\%} in sub-task A, B and C, respectively.",
            "cx": 18343.6,
            "cy": -116.61,
            "rx": 148.485,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.sdp-1.27",
            "name": "{IIITBH}-{IITP}@{CL}-{S}ci{S}umm20, {CL}-{L}ay{S}umm20, {L}ong{S}umm20",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In this paper, we present the IIIT Bhagalpur and IIT Patna team{'}s effort to solve the three shared tasks namely, CL-SciSumm 2020, CL-LaySumm 2020, LongSumm 2020 at SDP 2020. The theme of these tasks is to generate medium-scale, lay and long summaries, respectively, for scientific articles. For the first two tasks, unsupervised systems are developed, while for the third one, we develop a supervised system.The performances of all the systems were evaluated on the associated datasets with the shared tasks in term of well-known ROUGE metric.",
            "cx": 18755.6,
            "cy": -116.61,
            "rx": 245.232,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.sdp-1.30",
            "name": "{IITP}-{AI}-{NLP}-{ML}@ {CL}-{S}ci{S}umm 2020, {CL}-{L}ay{S}umm 2020, {L}ong{S}umm 2020",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "The publication rate of scientific literature increases rapidly, which poses a challenge for researchers to keep themselves updated with new state-of-the-art. Scientific document summarization solves this problem by summarizing the essential fact and findings of the document. In the current paper, we present the participation of IITP-AI-NLP-ML team in three shared tasks, namely, CL-SciSumm 2020, LaySumm 2020, LongSumm 2020, which aims to generate medium, lay, and long summaries of the scientific articles, respectively. To solve CL-SciSumm 2020 and LongSumm 2020 tasks, three well-known clustering techniques are used, and then various sentence scoring functions, including textual entailment, are used to extract the sentences from each cluster for a summary generation. For LaySumm 2020, an encoder-decoder based deep learning model has been utilized. Performances of our developed systems are evaluated in terms of ROUGE measures on the associated datasets with the shared task.",
            "cx": 19198.6,
            "cy": -116.61,
            "rx": 179.71,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.lrec-1.273",
            "name": "A Platform for Event Extraction in {H}indi",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Event Extraction is an important task in the widespread field of Natural Language Processing (NLP). Though this task is adequately addressed in English with sufficient resources, we are unaware of any benchmark setup in Indian languages. Hindi is one of the most widely spoken languages in the world. In this paper, we present an Event Extraction framework for Hindi language by creating an annotated resource for benchmarking, and then developing deep learning based models to set as the baselines. We crawl more than seventeen hundred disaster related Hindi news articles from the various news sources. We also develop deep learning based models for Event Trigger Detection and Classification, Argument Detection and Classification and Event-Argument Linking.",
            "cx": 19456.6,
            "cy": -116.61,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.lrec-1.675",
            "name": "{S}cholarly{R}ead: A New Dataset for Scientific Article Reading Comprehension",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present ScholarlyRead, span-of-word-based scholarly articles{'} Reading Comprehension (RC) dataset with approximately 10K manually checked passage-question-answer instances. ScholarlyRead was constructed in semi-automatic way. We consider the articles from two popular journals of a reputed publishing house. Firstly, we generate questions from these articles in an automatic way. Generated questions are then manually checked by the human annotators. We propose a baseline model based on Bi-Directional Attention Flow (BiDAF) network that yields the F1 score of 37.31{\\%}. The framework would be useful for building Question-Answering (QA) systems on scientific articles.",
            "cx": 19664.6,
            "cy": -116.61,
            "rx": 128.887,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.iwslt-1.22",
            "name": "Generating Fluent Translations from Disfluent Text Without Access to Fluent References: {IIT} {B}ombay@{IWSLT}2020",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Machine translation systems perform reasonably well when the input is well-formed speech or text. Conversational speech is spontaneous and inherently consists of many disfluencies. Producing fluent translations of disfluent source text would typically require parallel disfluent to fluent training data. However, fluent translations of spontaneous speech are an additional resource that is tedious to obtain. This work describes the submission of IIT Bombay to the Conversational Speech Translation challenge at IWSLT 2020. We specifically tackle the problem of disfluency removal in disfluent-to-fluent text-to-text translation assuming no access to fluent references during training. Common patterns of disfluency are extracted from disfluent references and a noise induction model is used to simulate them starting from a clean monolingual corpus. This synthetically constructed dataset is then considered as a proxy for labeled data during training. We also make use of additional fluent text in the target language to help generate fluent translations. This work uses no fluent references during training and beats a baseline model by a margin of 4.21 and 3.11 BLEU points where the baseline uses disfluent and fluent references, respectively. Index Terms- disfluency removal, machine translation, noise induction, leveraging monolingual data, denoising for disfluency removal.",
            "cx": 19905.6,
            "cy": -116.61,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.eacl-main.299",
            "name": "Disfluency Correction using Unsupervised and Semi-supervised Learning",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Spoken language is different from the written language in its style and structure. Disfluencies that appear in transcriptions from speech recognition systems generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semi-supervised way. Our unsupervised approach achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semi-supervision. Both are comparable to two competitive fully-supervised models.",
            "cx": 19905.6,
            "cy": -26.8701,
            "rx": 93.1765,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.icon-main.25",
            "name": "Semantic Extractor-Paraphraser based Abstractive Summarization",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "The anthology of spoken languages today is inundated with textual information, necessitating the development of automatic summarization models. In this manuscript, we propose an extractor-paraphraser based abstractive summarization system that exploits semantic overlap as opposed to its predecessors that focus more on syntactic information overlap. Our model outperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word mover similarity (WMS), establishing the superiority of the proposed system via extensive ablation experiments. We have also challenged the summarization capabilities of the state of the art Pointer Generator Network (PGN), and through thorough experimentation, shown that PGN is more of a paraphraser, contrary to the prevailing notion of a summarizer; illustrating it{'}s incapability to accumulate information across multiple sentences.",
            "cx": 20117.6,
            "cy": -116.61,
            "rx": 99.9045,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.icon-main.42",
            "name": "A Multi-modal Personality Prediction System",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Automatic prediction of personality traits has many real-life applications, e.g., in forensics, recommender systems, personalized services etc.. In this work, we have proposed a solution framework for solving the problem of predicting the personality traits of a user from videos. Ambient, facial and the audio features are extracted from the video of the user. These features are used for the final output prediction. The visual and audio modalities are combined in two different ways: averaging of predictions obtained from the individual modalities, and concatenation of features in multi-modal setting. The dataset released in Chalearn-16 is used for evaluating the performance of the system. Experimental results illustrate that it is possible to obtain better performance with a hand full of images, rather than using all the images present in the video",
            "cx": 20307.6,
            "cy": -116.61,
            "rx": 72.25,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.icon-main.43",
            "name": "{D}-Coref: A Fast and Lightweight Coreference Resolution Model using {D}istil{BERT}",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Smart devices are often deployed in some edge-devices, which require quality solutions in limited amount of memory usage. In most of the user-interaction based smart devices, coreference resolution is often required. Keeping this in view, we have developed a fast and lightweight coreference resolution model which meets the minimum memory requirement and converges faster. In order to generate the embeddings for solving the task of coreference resolution, DistilBERT, a light weight BERT module is utilized. DistilBERT consumes less memory (only 60{\\%} of memory in comparison to BERT-based heavy model) and it is suitable for deployment in edge devices. DistilBERT embedding helps in 60{\\%} faster convergence with an accuracy compromise of 2.59{\\%}, and 6.49{\\%} with respect to its base model and current state-of-the-art, respectively.",
            "cx": 20507.6,
            "cy": -116.61,
            "rx": 109.703,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.fnp-1.22",
            "name": "Knowledge Graph and Deep Neural Network for Extractive Text Summarization by Utilizing Triples",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In our research work, we represent the content of the sentence in graphical form after extracting triples from the sentences. In this paper, we will discuss novel methods to generate an extractive summary by scoring the triples. Our work has also touched upon sequence-to-sequence encoding of the content of the sentence, to classify it as a summary or a non-summary sentence. Our findings help to decide the nature of the sentences forming the summary and the length of the system generated summary as compared to the length of the reference summary.",
            "cx": 20727.6,
            "cy": -116.61,
            "rx": 91.8478,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.eamt-1.21",
            "name": "Modelling Source- and Target- Language Syntactic Information as Conditional Context in Interactive Neural Machine Translation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "In interactive machine translation (MT), human translators correct errors in automatic translations in collaboration with the MT systems, which is seen as an effective way to improve the productivity gain in translation. In this study, we model source-language syntactic constituency parse and target-language syntactic descriptions in the form of supertags as conditional context for interactive prediction in neural MT (NMT). We found that the supertags significantly improve productivity gain in translation in interactive-predictive NMT (INMT), while syntactic parsing somewhat found to be effective in reducing human effort in translation. Furthermore, when we model this source- and target-language syntactic information together as the conditional context, both types complement each other and our fully syntax-informed INMT model statistically significantly reduces human efforts in a French{--}to{--}English translation task, achieving 4.30 points absolute (corresponding to 9.18{\\%} relative) improvement in terms of word prediction accuracy (WPA) and 4.84 points absolute (corresponding to 9.01{\\%} relative) reduction in terms of word stroke ratio (WSR) over the baseline.",
            "cx": 20930.6,
            "cy": -116.61,
            "rx": 92.7622,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.coling-main.111",
            "name": "A Retrofitting Model for Incorporating Semantic Relations into Word Embeddings",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "We present a novel retrofitting model that can leverage relational knowledge available in a knowledge resource to improve word embeddings. The knowledge is captured in terms of relation inequality constraints that compare similarity of related and unrelated entities in the context of an anchor entity. These constraints are used as training data to learn a non-linear transformation function that maps original word vectors to a vector space respecting these constraints. The transformation function is learned in a similarity metric learning setting using Triplet network architecture. We applied our model to synonymy, antonymy and hypernymy relations in WordNet and observed large gains in performance over original distributional models as well as other retrofitting approaches on word similarity task and significant overall improvement on lexical entailment detection task.",
            "cx": 21101.6,
            "cy": -116.61,
            "rx": 60.623,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.coling-main.383",
            "name": "Filtering Back-Translated Data in Unsupervised Neural Machine Translation",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Unsupervised neural machine translation (NMT) utilizes only monolingual data for training. The quality of back-translated data plays an important role in the performance of NMT systems. In back-translation, all generated pseudo parallel sentence pairs are not of the same quality. Taking inspiration from domain adaptation where in-domain sentences are given more weight in training, in this paper we propose an approach to filter back-translated data as part of the training process of unsupervised NMT. Our approach gives more weight to good pseudo parallel sentence pairs in the back-translation phase. We calculate the weight of each pseudo parallel sentence pair using sentence-wise round-trip BLEU score which is normalized batch-wise. We compare our approach with the current state of the art approaches for unsupervised NMT.",
            "cx": 21275.6,
            "cy": -116.61,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.coling-main.534",
            "name": "Analysing cross-lingual transfer in lemmatisation for {I}ndian languages",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. However, most of the prior work on this topic has focused on high resource languages. In this paper, we evaluate cross-lingual approaches for low resource languages, especially in the context of morphologically rich Indian languages. We test our model on six languages from two different families and develop linguistic insights into each model{'}s performance.",
            "cx": 21482.6,
            "cy": -116.61,
            "rx": 94.0904,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.acl-main.402",
            "name": "Towards Emotion-aided Multi-modal Dialogue Act Classification",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of \\textit{both} multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.",
            "cx": 21681.6,
            "cy": -116.61,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.naacl-main.456",
            "name": "Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in {T}witter",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ({`}TA{'} means tweet act, i.e., speech act in Twitter) dataset called \\textit{EmoTA} collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.",
            "cx": 21681.6,
            "cy": -26.8701,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2020.aacl-main.33",
            "name": "Unsupervised Aspect-Level Sentiment Controllable Style Transfer",
            "publication_data": 2020,
            "citation": "???",
            "abstract": "Unsupervised style transfer in text has previously been explored through the sentiment transfer task. The task entails inverting the overall sentiment polarity in a given input sentence, while preserving its content. From the Aspect-Based Sentiment Analysis (ABSA) task, we know that multiple sentiment polarities can often be present together in a sentence with multiple aspects. In this paper, the task of aspect-level sentiment controllable style transfer is introduced, where each of the aspect-level sentiments can individually be controlled at the output. To achieve this goal, a BERT-based encoder-decoder architecture with saliency weighted polarity injection is proposed, with unsupervised training strategies, such as ABSA masked-language-modelling. Through both automatic and manual evaluation, we show that the system is successful in controlling aspect-level sentiments.",
            "cx": 21909.6,
            "cy": -116.61,
            "rx": 123.073,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.wat-1.26",
            "name": "Language Relatedness and Lexical Closeness can help Improve Multilingual {NMT}: {IITB}ombay@{M}ulti{I}ndic{NMT} {WAT}2021",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages. This paper describes our submission (Team ID: CFILT-IITB) for the MultiIndicMT: An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for Indic languages in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., related languages.",
            "cx": 21887.6,
            "cy": -26.8701,
            "rx": 101.233,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.wat-1.28",
            "name": "Multilingual Machine Translation Systems at {WAT} 2021: One-to-Many and Many-to-One Transformer based {NMT}",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models: one for English to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.",
            "cx": 22109.6,
            "cy": -26.8701,
            "rx": 103.476,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.smm4h-1.15",
            "name": "{BERT} based Adverse Drug Effect Tweet Classification",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "This paper describes models developed for the Social Media Mining for Health (SMM4H) 2021 shared tasks. Our team participated in the first subtask that classifies tweets with Adverse Drug Effect (ADE) mentions. Our best performing model utilizes BERTweet followed by a single layer of BiLSTM. The system achieves an F-score of 0.45 on the test set without the use of any auxiliary resources such as Part-of-Speech tags, dependency tags, or knowledge from medical dictionaries.",
            "cx": 22317.6,
            "cy": -26.8701,
            "rx": 86.0347,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.mtsummit-loresmt.17",
            "name": "Evaluating the Performance of Back-translation for Low Resource {E}nglish-{M}arathi Language Pair: {CFILT}-{IITB}ombay @ {L}o{R}es{MT} 2021",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "In this paper, we discuss the details of the various Machine Translation (MT) systems that we have submitted for the English-Marathi LoResMT task. As a part of this task, we have submitted three different Neural Machine Translation (NMT) systems; a Baseline English-Marathi system, a Baseline Marathi-English system, and an English-Marathi system that is based on the back-translation technique. We explore the performance of these NMT systems between English and Marathi languages, which forms a low resource language pair due to unavailability of sufficient parallel data. We also explore the performance of the back-translation technique when the back-translated data is obtained from NMT systems that are trained on a very less amount of data. From our experiments, we observe that the back-translation technique can help improve the MT quality over the baseline for the English-Marathi language pair.",
            "cx": 22516.6,
            "cy": -26.8701,
            "rx": 94.5053,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.inlg-1.39",
            "name": "{SEPRG}: Sentiment aware Emotion controlled Personalized Response Generation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Social chatbots have gained immense popularity, and their appeal lies not just in their capacity to respond to the diverse requests from users, but also in the ability to develop an emotional connection with users. To further develop and promote social chatbots, we need to concentrate on increasing user interaction and take into account both the intellectual and emotional quotient in the conversational agents. Therefore, in this work, we propose the task of sentiment aware emotion controlled personalized dialogue generation giving the machine the capability to respond emotionally and in accordance with the persona of the user. As sentiment and emotions are highly co-related, we use the sentiment knowledge of the previous utterance to generate the correct emotional response in accordance with the user persona. We design a Transformer based Dialogue Generation framework, that generates responses that are sensitive to the emotion of the user and corresponds to the persona and sentiment as well. Moreover, the persona information is encoded by a different Transformer encoder, along with the dialogue history, is fed to the decoder for generating responses. We annotate the PersonaChat dataset with sentiment information to improve the response quality. Experimental results on the PersonaChat dataset show that the proposed framework significantly outperforms the existing baselines, thereby generating personalized emotional responses in accordance with the sentiment that provides better emotional connection and user satisfaction as desired in a social chatbot.",
            "cx": 22742.6,
            "cy": -26.8701,
            "rx": 113.274,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.emnlp-main.789",
            "name": "{``}So You Think You{'}re Funny?{''}: Rating the Humour Quotient in Standup Comedy",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour-annotated dataset ({\\textasciitilde}40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience{'}s laughter. The normalized duration (laughter duration divided by the clip duration) of laughter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by comparing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a {`}funniness{'} score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our {`}Open Mic{'} dataset is released for further research along with the code.",
            "cx": 22960.6,
            "cy": -26.8701,
            "rx": 86.9483,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.eacl-main.255",
            "name": "Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "A recent topic of research in natural language generation has been the development of automatic response generation modules that can automatically respond to a user{'}s utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting emotion classes with the input sequences. However, the outputs by these models may be inconsistent. We employ multi-task learning to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the decoders. Human evaluation reveals that our model produces more emotionally pertinent responses. In addition, our model outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses.",
            "cx": 23167.6,
            "cy": -26.8701,
            "rx": 101.647,
            "ry": 26.7407,
            "stroke": "black",
            "width": null,
            "dasharray": "5,2"
        },
        {
            "id": "2021.eacl-main.288",
            "name": "Cognition-aware Cognate Detection",
            "publication_data": 2021,
            "citation": "???",
            "abstract": "Automatic detection of cognates helps downstream NLP tasks of Machine Translation, Cross-lingual Information Retrieval, Computational Phylogenetics and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with cognitive features extracted from human readers{'} gaze behaviour. We collect gaze behaviour data for a small sample of cognates and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10{\\%} with the collected gaze features, and 12{\\%} using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models.",
            "cx": 23411.6,
            "cy": -26.8701,
            "rx": 124.402,
            "ry": 26.7407,
            "stroke": "red",
            "width": null,
            "dasharray": "5,2"
        }
    ],
    [
        {
            "source": "2003",
            "target": "2004",
            "d": "M28.5975,-1623.86C28.5975,-1608.5 28.5975,-1586.18 28.5975,-1570.81"
        },
        {
            "source": "2004",
            "target": "2005",
            "d": "M28.5975,-1534.12C28.5975,-1518.76 28.5975,-1496.44 28.5975,-1481.07"
        },
        {
            "source": "2005",
            "target": "2006",
            "d": "M28.5975,-1444.38C28.5975,-1429.02 28.5975,-1406.7 28.5975,-1391.33"
        },
        {
            "source": "2006",
            "target": "2007",
            "d": "M28.5975,-1354.64C28.5975,-1339.28 28.5975,-1316.96 28.5975,-1301.59"
        },
        {
            "source": "P06-2100",
            "target": "I08-1067",
            "d": "M4482.37,-1364.12C4098.8,-1335.21 2766.69,-1234.82 2354.27,-1203.74"
        },
        {
            "source": "P06-2100",
            "target": "C10-2040",
            "d": "M4672.24,-1355.94C4989.23,-1296.61 6093.76,-1089.85 6409,-1030.85"
        },
        {
            "source": "2007",
            "target": "2008",
            "d": "M28.5975,-1264.9C28.5975,-1249.54 28.5975,-1227.22 28.5975,-1211.85"
        },
        {
            "source": "2008",
            "target": "2009",
            "d": "M28.5975,-1175.16C28.5975,-1159.8 28.5975,-1137.48 28.5975,-1122.11"
        },
        {
            "source": "I08-7013",
            "target": "sankaran-etal-2008-common",
            "d": "M7208.57,-1193.49C7211.22,-1193.49 7213.86,-1193.49 7216.51,-1193.49"
        },
        {
            "source": "I08-1067",
            "target": "P09-1090",
            "d": "M2210.97,-1166.64C2203.92,-1157.81 2195.93,-1147.79 2188.47,-1138.43"
        },
        {
            "source": "I08-1067",
            "target": "W12-5906",
            "d": "M2112.23,-1189.98C1815.23,-1183.26 1063.7,-1163.12 1020.6,-1130.62 938.275,-1068.55 922.538,-935.578 919.878,-871.504"
        },
        {
            "source": "I08-1067",
            "target": "W14-5105",
            "d": "M2126.81,-1180.26C2094.53,-1171.4 2061.71,-1156.3 2039.6,-1130.62 2005.62,-1091.17 2020.6,-1067.08 2020.6,-1015.01 2020.6,-1015.01 2020.6,-1015.01 2020.6,-833.531 2020.6,-780.978 2046.92,-725.121 2066.81,-690.411"
        },
        {
            "source": "I08-1067",
            "target": "W14-5148",
            "d": "M2350.72,-1189.5C2577.2,-1183.07 3061.9,-1165.62 3225.6,-1130.62 3368.58,-1100.05 3533.6,-1161.23 3533.6,-1015.01 3533.6,-1015.01 3533.6,-1015.01 3533.6,-833.531 3533.6,-778.394 3546.43,-762.706 3578.6,-717.921 3587.01,-706.21 3598.13,-695.377 3609.19,-686.158"
        },
        {
            "source": "I08-1067",
            "target": "W14-3308",
            "d": "M2112.4,-1189.48C1892.75,-1183.16 1432.33,-1166.04 1276.6,-1130.62 1160.37,-1104.19 1034.6,-1134.21 1034.6,-1015.01 1034.6,-1015.01 1034.6,-1015.01 1034.6,-833.531 1034.6,-783.989 1045.37,-727.607 1053.56,-691.941"
        },
        {
            "source": "I08-1067",
            "target": "kunchukuttan-etal-2014-shata",
            "d": "M2111.54,-1192.08C1840.04,-1190.27 1185.87,-1180.68 971.597,-1130.62 846.073,-1101.29 705.597,-1143.92 705.597,-1015.01 705.597,-1015.01 705.597,-1015.01 705.597,-833.531 705.597,-777.513 667.517,-722.712 638.591,-689.166"
        },
        {
            "source": "I08-1067",
            "target": "W15-5950",
            "d": "M2351.53,-1190.83C2585.14,-1186.65 3084.12,-1172.86 3149.6,-1130.62 3198.59,-1099.02 3211.6,-1073.32 3211.6,-1015.01 3211.6,-1015.01 3211.6,-1015.01 3211.6,-743.791 3211.6,-667.579 3293.01,-617.863 3355.4,-590.951"
        },
        {
            "source": "I08-1067",
            "target": "W16-6303",
            "d": "M2258.11,-1167.28C2267.59,-1156.82 2277.44,-1144.01 2283.6,-1130.62 2305.35,-1083.31 2302.6,-1067.08 2302.6,-1015.01 2302.6,-1015.01 2302.6,-1015.01 2302.6,-654.051 2302.6,-600.737 2305.55,-578.699 2270.6,-538.441 2253.56,-518.811 2229.14,-505.184 2205.34,-495.826"
        },
        {
            "source": "I08-1067",
            "target": "W16-4622",
            "d": "M2277.24,-1168.35C2291.87,-1158.5 2306.8,-1145.8 2316.6,-1130.62 2345.05,-1086.53 2340.6,-1067.49 2340.6,-1015.01 2340.6,-1015.01 2340.6,-1015.01 2340.6,-654.051 2340.6,-604.804 2332.9,-548.351 2327.06,-512.589"
        },
        {
            "source": "I08-1067",
            "target": "L16-1485",
            "d": "M2351.98,-1192.8C2606.93,-1192.37 3193.54,-1185.24 3384.6,-1130.62 3478.26,-1103.85 3822,-878.017 3833.6,-861.401 3909.03,-753.355 3900.19,-586.06 3891.78,-512.9"
        },
        {
            "source": "I08-1067",
            "target": "I17-2048",
            "d": "M2131.95,-1178.46C1983.76,-1154.76 1722.6,-1100.82 1722.6,-1015.01 1722.6,-1015.01 1722.6,-1015.01 1722.6,-743.791 1722.6,-652.471 1773.22,-609.285 1715.6,-538.441 1645.12,-451.797 1315.38,-410.198 1150.69,-394.547"
        },
        {
            "source": "I08-1067",
            "target": "L18-1548",
            "d": "M2341.09,-1182.34C2581.77,-1158.56 3135.6,-1095.13 3135.6,-1015.01 3135.6,-1015.01 3135.6,-1015.01 3135.6,-923.271 3135.6,-854.381 3190.33,-684.08 3230.6,-628.181 3245.57,-607.398 3264.5,-615.265 3275.6,-592.181 3285.94,-570.654 3281.05,-561.695 3275.6,-538.441 3256.83,-458.328 3205.63,-375.374 3174.89,-330.62"
        },
        {
            "source": "I08-1067",
            "target": "N19-1387",
            "d": "M2112.12,-1190.44C1887.61,-1183.88 1415.74,-1156.36 1309.6,-1040.88 1224.24,-948.018 1248.6,-602.704 1248.6,-476.571 1248.6,-476.571 1248.6,-476.571 1248.6,-384.831 1248.6,-262.117 1412.46,-224.003 1525.82,-212.318"
        },
        {
            "source": "C08-2007",
            "target": "L16-1369",
            "d": "M9439.6,-1166.33C9439.6,-1131.71 9439.6,-1068.78 9439.6,-1015.01 9439.6,-1015.01 9439.6,-1015.01 9439.6,-654.051 9439.6,-605.324 9439.6,-549.071 9439.6,-513.185"
        },
        {
            "source": "C08-1068",
            "target": "W09-3536",
            "d": "M9743.38,-1167.11C9736.8,-1157.95 9729.28,-1147.48 9722.3,-1137.76"
        },
        {
            "source": "C08-1068",
            "target": "C10-2091",
            "d": "M9702.16,-1174.11C9677.28,-1164.22 9649.37,-1149.98 9628.6,-1130.62 9604.89,-1108.53 9587.86,-1075.62 9577.36,-1050.55"
        },
        {
            "source": "C08-1068",
            "target": "2010.jeptalnrecital-court.7",
            "d": "M9769.59,-1166.68C9779.09,-1136.06 9794.96,-1084.83 9805.6,-1050.49"
        },
        {
            "source": "C08-1068",
            "target": "W13-4706",
            "d": "M9809.42,-1171.26C9863.34,-1143.69 9943.6,-1090.04 9943.6,-1015.01 9943.6,-1015.01 9943.6,-1015.01 9943.6,-923.271 9943.6,-862.607 9893.92,-808.401 9856.7,-776.305"
        },
        {
            "source": "2009",
            "target": "2010",
            "d": "M28.5975,-1085.42C28.5975,-1070.06 28.5975,-1047.74 28.5975,-1032.37"
        },
        {
            "source": "W09-3536",
            "target": "C10-2091",
            "d": "M9665.47,-1081.06C9648.69,-1070.07 9628.06,-1056.56 9609.83,-1044.63"
        },
        {
            "source": "W09-3536",
            "target": "2010.jeptalnrecital-court.7",
            "d": "M9728.68,-1080.38C9742.85,-1069.85 9759.98,-1057.11 9775.34,-1045.69"
        },
        {
            "source": "W09-3536",
            "target": "W13-4706",
            "d": "M9698.39,-1076.62C9698.78,-1052.91 9700.82,-1017.17 9708.6,-987.141 9728.63,-909.815 9771.33,-826.098 9796.63,-780.406"
        },
        {
            "source": "P09-1090",
            "target": "W15-5914",
            "d": "M2171.04,-1076.68C2182.58,-1042.43 2200.6,-980.164 2200.6,-925.271 2200.6,-925.271 2200.6,-925.271 2200.6,-743.791 2200.6,-691.719 2200.89,-676.545 2181.6,-628.181 2177.74,-618.504 2172.17,-608.815 2166.34,-600.071"
        },
        {
            "source": "P09-1090",
            "target": "W16-6303",
            "d": "M2187.86,-1077.48C2218.37,-1045.26 2264.6,-986.369 2264.6,-925.271 2264.6,-925.271 2264.6,-925.271 2264.6,-654.051 2264.6,-601.979 2276.01,-580.711 2245.6,-538.441 2233.43,-521.53 2215.21,-508.837 2196.74,-499.495"
        },
        {
            "source": "D09-1048",
            "target": "C10-1063",
            "d": "M4057.98,-1076.9C4052.71,-1068.17 4046.73,-1058.28 4041.14,-1049.01"
        },
        {
            "source": "D09-1048",
            "target": "P11-1057",
            "d": "M4091.43,-1077.18C4097.86,-1066.48 4104.34,-1053.58 4107.6,-1040.88 4113.54,-1017.75 4115.78,-1009.58 4107.6,-987.141 4103.74,-976.543 4097.17,-966.5 4089.98,-957.709"
        },
        {
            "source": "D09-1048",
            "target": "I11-1078",
            "d": "M4006.16,-1084.52C3980.85,-1074.92 3953.44,-1060.88 3933.6,-1040.88 3911.94,-1019.05 3898.99,-986.319 3891.71,-961.249"
        },
        {
            "source": "D09-1048",
            "target": "2016.gwc-1.57",
            "d": "M4097.27,-1077.55C4106.58,-1066.8 4116.86,-1053.77 4124.6,-1040.88 4182.44,-944.485 4304.41,-620.376 4344.51,-512.118"
        },
        {
            "source": "2010",
            "target": "2011",
            "d": "M28.5975,-995.677C28.5975,-980.316 28.5975,-958.002 28.5975,-942.633"
        },
        {
            "source": "W10-3604",
            "target": "W12-5020",
            "d": "M10223.9,-989.076C10188.6,-957.408 10126.2,-901.531 10087.3,-866.628"
        },
        {
            "source": "W10-3604",
            "target": "C12-2023",
            "d": "M10250.6,-986.824C10250.6,-956.326 10250.6,-905.775 10250.6,-871.583"
        },
        {
            "source": "W10-3604",
            "target": "W14-5103",
            "d": "M10274.1,-988.575C10299.6,-960.427 10339.1,-911.575 10357.6,-861.401 10378.4,-804.888 10380.1,-733.963 10378.8,-691.978"
        },
        {
            "source": "W10-3607",
            "target": "W11-3001",
            "d": "M10484.6,-986.686C10484.6,-978.72 10484.6,-969.829 10484.6,-961.34"
        },
        {
            "source": "P10-1137",
            "target": "W10-4011",
            "d": "M1262.32,-1014.01C1277.63,-1014.01 1292.94,-1014.01 1308.24,-1014.01"
        },
        {
            "source": "P10-1137",
            "target": "N15-1125",
            "d": "M1162.6,-986.849C1162.6,-952.229 1162.6,-889.299 1162.6,-835.531 1162.6,-835.531 1162.6,-835.531 1162.6,-743.791 1162.6,-694.371 1152.98,-637.96 1145.67,-602.254"
        },
        {
            "source": "P10-1155",
            "target": "P13-2096",
            "d": "M2756.67,-1012.45C2841.98,-1009.69 2992.51,-997.838 3111.6,-951.141 3217.83,-909.486 3323.47,-823.92 3376.14,-777.473"
        },
        {
            "source": "P10-1155",
            "target": "W14-0124",
            "d": "M2674.74,-987.626C2668.65,-976.63 2661.82,-963.472 2656.6,-951.141 2614.14,-850.879 2615.76,-821.303 2581.6,-717.921 2578.75,-709.313 2575.64,-700.05 2572.7,-691.379"
        },
        {
            "source": "P10-1155",
            "target": "W15-5908",
            "d": "M2733.27,-993.406C2817.29,-955.029 3006.25,-865.509 3154.6,-771.661 3243.72,-715.279 3247.15,-670.838 3343.6,-628.181 3423.8,-592.711 3452.37,-612.842 3537.6,-592.181 3544.21,-590.577 3551.08,-588.767 3557.91,-586.868"
        },
        {
            "source": "P10-1155",
            "target": "W15-5945",
            "d": "M2625.91,-1005.33C2534.92,-990.415 2378.6,-948.363 2378.6,-835.531 2378.6,-835.531 2378.6,-835.531 2378.6,-743.791 2378.6,-690.832 2406.47,-635.074 2427.53,-600.498"
        },
        {
            "source": "P10-1155",
            "target": "L16-1349",
            "d": "M2685.05,-986.942C2679.43,-952.425 2670.6,-889.606 2670.6,-835.531 2670.6,-835.531 2670.6,-835.531 2670.6,-654.051 2670.6,-589.62 2612.22,-536.506 2568.3,-505.664"
        },
        {
            "source": "P10-1155",
            "target": "2016.gwc-1.54",
            "d": "M2694.15,-986.942C2699.77,-952.425 2708.6,-889.606 2708.6,-835.531 2708.6,-835.531 2708.6,-835.531 2708.6,-654.051 2708.6,-605.323 2708.98,-549.071 2709.27,-513.185"
        },
        {
            "source": "N10-1065",
            "target": "khapra-etal-2014-transliteration",
            "d": "M1888.09,-987.052C1877.17,-924.204 1849.31,-763.763 1836.82,-691.857"
        },
        {
            "source": "N10-1065",
            "target": "K16-1027",
            "d": "M1903.73,-987.096C1917.35,-953.004 1938.6,-890.916 1938.6,-835.531 1938.6,-835.531 1938.6,-835.531 1938.6,-654.051 1938.6,-601.979 1947.62,-582.328 1919.6,-538.441 1911.33,-525.497 1899.28,-514.581 1886.64,-505.696"
        },
        {
            "source": "N10-1065",
            "target": "Q18-1022",
            "d": "M1914.27,-987.752C1922.72,-976.99 1931.94,-963.974 1938.6,-951.141 1963.5,-903.128 1976.6,-889.618 1976.6,-835.531 1976.6,-835.531 1976.6,-835.531 1976.6,-474.571 1976.6,-390.361 1880.99,-342.366 1808.26,-318.039"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "W12-5209",
            "d": "M4772.32,-989.9C4674.61,-955.831 4492.98,-892.501 4395.49,-858.508"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "C12-2008",
            "d": "M4861.14,-987.204C4888.42,-955.964 4934.43,-903.284 4964.4,-868.962"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "L18-1728",
            "d": "M4711.35,-1000.13C4550.62,-980.83 4288.2,-938.213 4232.6,-861.401 4111.04,-693.486 4254.14,-425.26 4311.71,-331.1"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "2018.gwc-1.31",
            "d": "M4840.23,-986.845C4841.83,-946.944 4841.44,-869.053 4819.6,-807.661 4764.82,-653.706 4681.51,-651.522 4614.6,-502.441 4587.63,-442.36 4595.96,-422.205 4577.6,-358.96 4575.13,-350.467 4572.3,-341.378 4569.56,-332.853"
        },
        {
            "source": "bhattacharyya-2010-indowordnet",
            "target": "W19-7509",
            "d": "M4986.76,-1011.86C5581.77,-1006.29 7765.6,-976.248 7765.6,-835.531 7765.6,-835.531 7765.6,-835.531 7765.6,-384.831 7765.6,-296.151 7868.26,-249.446 7947.44,-226.612"
        },
        {
            "source": "C10-2040",
            "target": "W15-5905",
            "d": "M6583.7,-1012.95C6858.55,-1012.16 7673.97,-1005.24 7779.6,-951.141 7838.04,-921.206 7871.6,-901.197 7871.6,-835.531 7871.6,-835.531 7871.6,-835.531 7871.6,-743.791 7871.6,-695.064 7871.6,-638.812 7871.6,-602.925"
        },
        {
            "source": "C10-1063",
            "target": "P11-1057",
            "d": "M4030.91,-987.161C4034.23,-978.846 4037.96,-969.485 4041.5,-960.609"
        },
        {
            "source": "C10-1063",
            "target": "I11-1078",
            "d": "M3984.6,-989.959C3966.67,-978.475 3944.87,-964.513 3926.12,-952.504"
        },
        {
            "source": "C10-1063",
            "target": "ar-etal-2012-cost",
            "d": "M4074.06,-994.186C4166.83,-961.517 4357.36,-894.423 4458.36,-858.856"
        },
        {
            "source": "2011",
            "target": "2012",
            "d": "M28.5975,-905.937C28.5975,-890.576 28.5975,-868.262 28.5975,-852.893"
        },
        {
            "source": "P11-4022",
            "target": "C12-1113",
            "d": "M5870.84,-897.893C5878.04,-888.878 5886.25,-878.588 5893.9,-869"
        },
        {
            "source": "P11-4022",
            "target": "S13-2082",
            "d": "M5823.18,-898.148C5813.82,-887.792 5804.49,-875.027 5799.6,-861.401 5790.38,-835.734 5794.5,-804.982 5800.46,-781.649"
        },
        {
            "source": "P11-1057",
            "target": "L16-1485",
            "d": "M4075.87,-897.917C4100.15,-864.919 4137.6,-804.511 4137.6,-745.791 4137.6,-745.791 4137.6,-745.791 4137.6,-654.051 4137.6,-580.63 4079.97,-580.217 4019.6,-538.441 3997.79,-523.349 3971.71,-510.296 3948.45,-500.122"
        },
        {
            "source": "I11-1078",
            "target": "P13-2096",
            "d": "M3824.87,-915.31C3768.33,-906.424 3681.09,-889.476 3609.6,-861.401 3552.55,-838.998 3492.42,-801.933 3453.38,-775.821"
        },
        {
            "source": "I11-1078",
            "target": "N13-1088",
            "d": "M3926.9,-904.968C3935.91,-901.883 3945.44,-899.125 3954.6,-897.401 4040.5,-881.223 5466.5,-923.914 5527.6,-861.401 5549.22,-839.274 5533.67,-804.63 5516.4,-779.031"
        },
        {
            "source": "I11-1078",
            "target": "W15-5908",
            "d": "M3823.15,-917.464C3686.76,-902.887 3362.61,-859.475 3306.6,-771.661 3293.75,-751.524 3294.55,-738.547 3306.6,-717.921 3333.77,-671.379 3474.45,-616.86 3559,-587.547"
        },
        {
            "source": "D11-1100",
            "target": "ar-etal-2012-cost",
            "d": "M5078.75,-915.604C4965.91,-906.581 4783.91,-889.2 4628.6,-861.401 4619.22,-859.722 4609.43,-857.68 4599.77,-855.487"
        },
        {
            "source": "D11-1100",
            "target": "C12-2008",
            "d": "M5135.64,-900.104C5110.87,-888.844 5080.91,-875.225 5054.9,-863.401"
        },
        {
            "source": "D11-1100",
            "target": "C12-1113",
            "d": "M5286.73,-911.301C5424.08,-894.884 5669.96,-865.491 5811.74,-848.544"
        },
        {
            "source": "D11-1100",
            "target": "S13-2082",
            "d": "M5300,-921.465C5393.09,-916.767 5526.95,-902.536 5636.6,-861.401 5688.5,-841.928 5740.67,-804.957 5774.58,-778.052"
        },
        {
            "source": "D11-1100",
            "target": "P13-1041",
            "d": "M5073.71,-920.599C4940.64,-915.632 4729.05,-901.496 4661.6,-861.401 4636.45,-846.454 4647.89,-823.962 4623.6,-807.661 4564.51,-768.01 4536.53,-789.297 4467.6,-771.661 4460.03,-769.724 4452.12,-767.623 4444.27,-765.486"
        },
        {
            "source": "D11-1100",
            "target": "W14-2619",
            "d": "M5299.01,-919.661C5405.34,-913.954 5554.95,-899.25 5598.6,-861.401 5648.89,-817.789 5599.16,-769.21 5641.6,-717.921 5654.86,-701.887 5673.37,-689.508 5691.84,-680.185"
        },
        {
            "source": "D11-1100",
            "target": "W15-5920",
            "d": "M5072.57,-922.551C4812.59,-920.089 4156.18,-909.228 3609.6,-861.401 3315.13,-835.635 3151.94,-988.076 2950.6,-771.661 2908.3,-726.192 2917.45,-647.775 2927.69,-602.13"
        },
        {
            "source": "D11-1100",
            "target": "W16-6315",
            "d": "M5195.7,-897.479C5206.94,-863.265 5224.6,-800.777 5224.6,-745.791 5224.6,-745.791 5224.6,-745.791 5224.6,-654.051 5224.6,-594.747 5271.15,-540.469 5306.26,-507.961"
        },
        {
            "source": "D11-1100",
            "target": "L16-1429",
            "d": "M5174.72,-897.134C5163.71,-873.421 5146.52,-837.682 5129.6,-807.661 5059.87,-683.97 5047.04,-648.714 4957.6,-538.441 4948.98,-527.815 4938.71,-517.078 4928.99,-507.599"
        },
        {
            "source": "D11-1100",
            "target": "K16-1016",
            "d": "M5297.5,-917.785C5390.19,-910.837 5512.66,-895.365 5547.6,-861.401 5642.81,-768.838 5611.71,-589.347 5592.71,-512.692"
        },
        {
            "source": "D11-1100",
            "target": "P17-1035",
            "d": "M5186.6,-897.109C5186.6,-862.489 5186.6,-799.559 5186.6,-745.791 5186.6,-745.791 5186.6,-745.791 5186.6,-654.051 5186.6,-606.571 5209.94,-480.048 5245.6,-448.701 5257.37,-438.352 5444.59,-413.021 5560.63,-398.178"
        },
        {
            "source": "2012",
            "target": "2013",
            "d": "M28.5975,-816.196C28.5975,-800.835 28.5975,-778.522 28.5975,-763.153"
        },
        {
            "source": "W12-4906",
            "target": "P13-2062",
            "d": "M6138.6,-807.206C6138.6,-799.239 6138.6,-790.348 6138.6,-781.86"
        },
        {
            "source": "kunchukuttan-etal-2012-experiences",
            "target": "P13-4030",
            "d": "M3721.6,-807.206C3721.6,-799.239 3721.6,-790.348 3721.6,-781.86"
        },
        {
            "source": "kunchukuttan-etal-2012-experiences",
            "target": "W14-5148",
            "d": "M3648.22,-815.571C3625.85,-806.286 3603.72,-792.376 3590.6,-771.661 3577.81,-751.486 3581.45,-739.983 3590.6,-717.921 3595.33,-706.509 3603.31,-696.108 3611.99,-687.227"
        },
        {
            "source": "kunchukuttan-etal-2012-experiences",
            "target": "L16-1485",
            "d": "M3823.66,-830.151C3917.97,-824.623 4048.25,-810.055 4080.6,-771.661 4156.63,-681.404 4008.65,-560.045 3930.54,-505.382"
        },
        {
            "source": "C12-3013",
            "target": "W14-5136",
            "d": "M11319.6,-807.343C11319.6,-776.846 11319.6,-726.294 11319.6,-692.103"
        },
        {
            "source": "C12-3030",
            "target": "L18-1728",
            "d": "M4696.07,-813.736C4622.02,-779.369 4481.6,-707.426 4481.6,-656.051 4481.6,-656.051 4481.6,-656.051 4481.6,-474.571 4481.6,-410.629 4424.24,-357.515 4380.93,-326.529"
        },
        {
            "source": "C12-3030",
            "target": "2018.gwc-1.31",
            "d": "M4724.27,-808.231C4690.55,-754.484 4613.7,-623.887 4581.6,-502.441 4566.28,-444.501 4560.73,-374.787 4558.73,-333.304"
        },
        {
            "source": "C12-3033",
            "target": "W14-0126",
            "d": "M5376.45,-807.343C5365.72,-776.627 5347.88,-725.567 5335.93,-691.368"
        },
        {
            "source": "C12-2008",
            "target": "P13-1041",
            "d": "M4892.41,-818.327C4868.54,-814.785 4843.18,-811.057 4819.6,-807.661 4693.31,-789.475 4546.7,-769.21 4457.57,-756.986"
        },
        {
            "source": "C12-2008",
            "target": "L16-1429",
            "d": "M4991.67,-807.331C4987.09,-754.642 4972.92,-632.837 4934.6,-538.441 4930.66,-528.752 4925.16,-518.942 4919.5,-510.076"
        },
        {
            "source": "C12-2008",
            "target": "L16-1485",
            "d": "M4941.74,-809.969C4817.37,-754.38 4491.21,-614.063 4205.6,-538.441 4108.12,-512.632 4080.16,-523.718 3981.6,-502.441 3973.22,-500.632 3964.48,-498.54 3955.84,-496.345"
        },
        {
            "source": "C12-2008",
            "target": "C16-1047",
            "d": "M4996.58,-807.572C5003.8,-744.724 5022.23,-584.283 5030.48,-512.377"
        },
        {
            "source": "C12-2008",
            "target": "C16-1287",
            "d": "M4973.53,-807.859C4924.07,-744.57 4796.28,-581.073 4740.87,-510.183"
        },
        {
            "source": "C12-2008",
            "target": "N18-1053",
            "d": "M5004.22,-807.531C5038.15,-725.012 5143.52,-473.007 5174.6,-448.701 5239.2,-398.17 5303.56,-476.103 5355.6,-412.701 5374.16,-390.079 5364.56,-356.734 5352.43,-331.661"
        },
        {
            "source": "C12-2008",
            "target": "W19-0413",
            "d": "M5017.21,-808.047C5026.62,-797.251 5037.19,-784.269 5045.6,-771.661 5068.01,-738.05 5180.57,-473.324 5212.6,-448.701 5271.41,-403.479 5318.34,-459.874 5375.6,-412.701 5409.16,-385.049 5405.75,-365.538 5414.6,-322.96 5419.46,-299.576 5428.52,-288.626 5414.6,-269.22 5403.22,-253.363 5364.36,-239.136 5325.36,-228.335"
        },
        {
            "source": "C12-1113",
            "target": "S13-2082",
            "d": "M5889.94,-808.623C5878.17,-799.064 5864.58,-788.018 5852.09,-777.879"
        },
        {
            "source": "C12-1113",
            "target": "W14-2619",
            "d": "M5955.33,-808.655C5966.54,-798.581 5977.61,-785.939 5983.6,-771.661 5992.84,-749.636 5998.05,-736.934 5983.6,-717.921 5979.01,-711.887 5902.32,-691.463 5840.14,-675.714"
        },
        {
            "source": "2013",
            "target": "2014",
            "d": "M28.5975,-726.456C28.5975,-711.095 28.5975,-688.782 28.5975,-673.413"
        },
        {
            "source": "W13-3611",
            "target": "W14-1708",
            "d": "M11587.4,-717.941C11582,-709.283 11575.8,-699.493 11570.1,-690.296"
        },
        {
            "source": "W13-3611",
            "target": "W15-5902",
            "d": "M11623,-718.43C11630,-707.756 11637,-694.829 11640.6,-681.921 11647,-658.898 11646.8,-651.241 11640.6,-628.181 11638,-618.652 11633.6,-609.082 11628.7,-600.412"
        },
        {
            "source": "P13-4030",
            "target": "L18-1548",
            "d": "M3734.13,-717.789C3738.94,-706.914 3744.12,-694.013 3747.6,-681.921 3762.01,-631.882 3766.6,-618.382 3766.6,-566.311 3766.6,-566.311 3766.6,-566.311 3766.6,-474.571 3766.6,-227.43 3468.23,-394.257 3231.6,-322.96 3225.66,-321.172 3219.49,-319.274 3213.33,-317.35"
        },
        {
            "source": "P13-2062",
            "target": "P14-2007",
            "d": "M6111.37,-718.648C6100.95,-709.02 6088.92,-697.911 6077.92,-687.749"
        },
        {
            "source": "P13-2062",
            "target": "K16-1016",
            "d": "M6055.7,-727.406C5999.57,-715.243 5931.29,-698.041 5906.6,-681.921 5880.98,-665.201 5889.1,-645.074 5863.6,-628.181 5814.33,-595.545 5788.53,-618.45 5735.6,-592.181 5690.84,-569.969 5645.97,-534.064 5616.51,-508.156"
        },
        {
            "source": "P13-2062",
            "target": "P18-1219",
            "d": "M6160.08,-718.314C6185.55,-685.424 6224.6,-625.374 6224.6,-566.311 6224.6,-566.311 6224.6,-566.311 6224.6,-474.571 6224.6,-424.524 6209.97,-368.264 6198.87,-332.763"
        },
        {
            "source": "P13-2062",
            "target": "2020.aacl-main.86",
            "d": "M6050.44,-728.942C6013.76,-719.712 5972.22,-705.026 5939.6,-681.921 5915.3,-664.713 5919.46,-650.36 5899.6,-628.181 5844.89,-567.101 5797.31,-577.625 5764.6,-502.441 5755.07,-480.54 5754.21,-470.209 5764.6,-448.701 5785.12,-406.193 6038.36,-220.74 6139.89,-147.51"
        },
        {
            "source": "P13-2096",
            "target": "W14-5148",
            "d": "M3467.34,-723.22C3503.5,-710.108 3550.45,-693.084 3588.04,-679.456"
        },
        {
            "source": "P13-2096",
            "target": "W15-5908",
            "d": "M3432.64,-718.179C3454.56,-693.54 3489.84,-656.029 3524.6,-628.181 3540.08,-615.777 3558.19,-603.814 3574.66,-593.768"
        },
        {
            "source": "P13-2096",
            "target": "L16-1485",
            "d": "M3454.64,-720.842C3473.13,-710.21 3494.31,-696.667 3511.6,-681.921 3535.52,-661.518 3530.76,-644.552 3557.6,-628.181 3616.87,-592.03 3645.81,-619.572 3709.6,-592.181 3761.05,-570.087 3814.04,-533.264 3848.46,-507.117"
        },
        {
            "source": "P13-2149",
            "target": "P14-2007",
            "d": "M6288.47,-726.035C6239.42,-712.017 6170.8,-692.405 6118.67,-677.507"
        },
        {
            "source": "P13-2149",
            "target": "P15-2124",
            "d": "M6378.82,-719.113C6415.08,-687.414 6478.05,-632.361 6517.68,-597.71"
        },
        {
            "source": "P13-2149",
            "target": "K16-1016",
            "d": "M6315.34,-719.995C6275.93,-694.292 6209.49,-653.619 6147.6,-628.181 5985.52,-561.57 5785.26,-515.878 5671.91,-493.166"
        },
        {
            "source": "P13-1041",
            "target": "W14-5148",
            "d": "M4302.29,-734.895C4168.9,-718.707 3886.92,-684.488 3742.29,-666.936"
        },
        {
            "source": "P13-1041",
            "target": "W14-2619",
            "d": "M4453.63,-741.43C4641.54,-735.36 5133.91,-717.154 5543.6,-681.921 5584.02,-678.444 5628.54,-673.358 5666.68,-668.613"
        },
        {
            "source": "P13-1041",
            "target": "D15-1300",
            "d": "M4310.39,-729.574C4196.68,-705.279 3954.46,-655.733 3747.6,-628.181 3555.74,-602.628 3505.45,-617.798 3313.6,-592.181 3295.61,-589.779 3276.43,-586.628 3258.22,-583.354"
        },
        {
            "source": "P13-1041",
            "target": "L16-1485",
            "d": "M4356.1,-718.682C4320.52,-674.732 4240.49,-584.193 4151.6,-538.441 4082.93,-503.097 4056.66,-520.594 3981.6,-502.441 3973.48,-500.477 3964.99,-498.32 3956.58,-496.115"
        },
        {
            "source": "P13-1041",
            "target": "K16-1016",
            "d": "M4441.85,-730.099C4567.67,-704.002 4849.78,-645.025 5086.6,-592.181 5232.81,-559.555 5402.75,-519.474 5500.96,-496.098"
        },
        {
            "source": "N13-1088",
            "target": "W14-2623",
            "d": "M5484.88,-717.941C5483.4,-709.802 5481.73,-700.662 5480.14,-691.956"
        },
        {
            "source": "N13-1088",
            "target": "W14-0126",
            "d": "M5450.26,-722.998C5427.03,-710.722 5397.43,-695.075 5372.68,-681.996"
        },
        {
            "source": "N13-1088",
            "target": "P14-2007",
            "d": "M5550.58,-733.185C5578.33,-728.416 5611.61,-722.781 5641.6,-717.921 5747.55,-700.753 5869.4,-682.16 5950.92,-669.887"
        },
        {
            "source": "N13-1088",
            "target": "W16-1904",
            "d": "M5521.14,-721.008C5587.95,-672.761 5743.66,-560.3 5818.29,-506.406"
        },
        {
            "source": "N13-1088",
            "target": "K16-1016",
            "d": "M5515.84,-719.843C5526,-709.187 5536.8,-695.896 5543.6,-681.921 5570.37,-626.866 5578.79,-555.194 5581.43,-512.716"
        },
        {
            "source": "I13-2006",
            "target": "W15-5945",
            "d": "M2480.25,-717.848C2476.55,-706.871 2472.5,-693.879 2469.6,-681.921 2463.2,-655.521 2458.31,-625.203 2455.09,-602.24"
        },
        {
            "source": "I13-1076",
            "target": "W15-5920",
            "d": "M3038.25,-718.075C3024.77,-694.338 3003.72,-658.312 2983.6,-628.181 2977.34,-618.806 2970.22,-608.903 2963.48,-599.835"
        },
        {
            "source": "I13-1076",
            "target": "D15-1300",
            "d": "M3069.93,-718.362C3091.08,-687.348 3126.9,-634.804 3150.4,-600.331"
        },
        {
            "source": "I13-1076",
            "target": "K16-1016",
            "d": "M3137.72,-733.688C3186.99,-728.201 3250.21,-721.769 3306.6,-717.921 3765.81,-686.584 3884.23,-733.6 4341.6,-681.921 4676.66,-644.061 4752.96,-587.29 5086.6,-538.441 5244.39,-515.338 5285.74,-525.05 5443.6,-502.441 5460.56,-500.012 5478.6,-496.984 5495.88,-493.865"
        },
        {
            "source": "I13-1076",
            "target": "2016.gwc-1.54",
            "d": "M2966.02,-734.483C2893.92,-725.03 2798.68,-708.03 2770.6,-681.921 2758,-670.21 2731.76,-568.199 2718.17,-512.505"
        },
        {
            "source": "I13-1076",
            "target": "P17-1035",
            "d": "M3138.96,-734.561C3188.09,-729.42 3250.76,-723.043 3306.6,-717.921 3502.43,-699.957 3552.09,-703.093 3747.6,-681.921 4221.12,-630.643 4337.34,-600.273 4809.6,-538.441 4935.75,-521.923 4971.15,-537.007 5093.6,-502.441 5149.45,-486.675 5156.86,-464.855 5212.6,-448.701 5327.56,-415.382 5464.84,-399.712 5554.91,-392.544"
        },
        {
            "source": "I13-1076",
            "target": "D17-1058",
            "d": "M3052.89,-717.832C3053.59,-655.265 3055.37,-495.977 3056.19,-423.608"
        },
        {
            "source": "I13-1076",
            "target": "P18-1089",
            "d": "M2999.25,-722.694C2948.26,-699.298 2874.43,-656.2 2841.6,-592.181 2817.61,-545.406 2887.69,-399.564 2922.82,-332.042"
        },
        {
            "source": "I13-1076",
            "target": "2020.lrec-1.613",
            "d": "M2967.38,-733.531C2930.91,-725.113 2890.41,-709.794 2862.6,-681.921 2808.41,-627.615 2815.6,-404.659 2815.6,-386.831 2815.6,-386.831 2815.6,-386.831 2815.6,-295.09 2815.6,-245.548 2804.82,-189.166 2796.64,-153.5"
        },
        {
            "source": "I13-1131",
            "target": "L16-1485",
            "d": "M3975.92,-717.682C3958.06,-669.466 3919.88,-566.417 3899.71,-511.969"
        },
        {
            "source": "2014",
            "target": "2015",
            "d": "M28.5975,-636.716C28.5975,-621.355 28.5975,-599.042 28.5975,-583.673"
        },
        {
            "source": "W14-5105",
            "target": "N19-1387",
            "d": "M2060.83,-629.383C2050.71,-618.904 2040.15,-605.933 2033.6,-592.181 2011.19,-545.176 2014.6,-528.642 2014.6,-476.571 2014.6,-476.571 2014.6,-476.571 2014.6,-384.831 2014.6,-262.117 1850.74,-224.003 1737.38,-212.318"
        },
        {
            "source": "W14-5115",
            "target": "2016.gwc-1.46",
            "d": "M8355.82,-628.622C8329.14,-597.476 8283.88,-544.618 8254.36,-510.154"
        },
        {
            "source": "W14-5115",
            "target": "W19-7509",
            "d": "M8431.47,-631.043C8487.02,-603.269 8565.6,-550.856 8565.6,-476.571 8565.6,-476.571 8565.6,-476.571 8565.6,-384.831 8565.6,-332.759 8584.18,-305.261 8546.6,-269.22 8518.48,-242.258 8290.83,-223.24 8149.66,-213.909"
        },
        {
            "source": "W14-3308",
            "target": "W15-5916",
            "d": "M1025,-631.91C1006.45,-620.987 983.817,-607.66 963.809,-595.878"
        },
        {
            "source": "W14-2623",
            "target": "K16-1016",
            "d": "M5488.98,-628.999C5507.89,-598.211 5540.08,-545.807 5561.34,-511.186"
        },
        {
            "source": "W14-1708",
            "target": "W15-5902",
            "d": "M11564.8,-628.673C11570.4,-619.928 11576.7,-609.984 11582.7,-600.645"
        },
        {
            "source": "W14-0130",
            "target": "kunchukuttan-etal-2014-shata",
            "d": "M8632.64,-674.664C8592.18,-684.386 8540.99,-695.065 8494.6,-699.921 8387.57,-711.123 852.917,-716.542 746.597,-699.921 723.093,-696.246 698.146,-689.3 675.922,-681.95"
        },
        {
            "source": "W14-0130",
            "target": "W15-5910",
            "d": "M8786.61,-637.134C8852.08,-623.294 8945.18,-603.61 9016.62,-588.506"
        },
        {
            "source": "W14-0130",
            "target": "2016.gwc-1.22",
            "d": "M8706.6,-627.863C8706.6,-597.366 8706.6,-546.814 8706.6,-512.622"
        },
        {
            "source": "W14-0130",
            "target": "L18-1548",
            "d": "M8616.87,-640.473C8581.64,-635.704 8540.8,-630.869 8503.6,-628.181 7591.61,-562.287 7360.62,-617.04 6446.6,-592.181 6311.44,-588.505 4133.5,-566.797 4014.6,-502.441 3988.87,-488.517 3999.11,-467.374 3976.6,-448.701 3958.16,-433.411 3803.82,-364.826 3780.6,-358.96 3543.52,-299.075 3471.84,-368.495 3231.6,-322.96 3224.87,-321.686 3217.93,-320.025 3211.06,-318.159"
        },
        {
            "source": "W14-0130",
            "target": "2021.naacl-main.322",
            "d": "M8811.46,-649.599C8968.5,-641.93 9250.04,-623.911 9283.6,-592.181 9321.44,-556.407 9302.6,-528.642 9302.6,-476.571 9302.6,-476.571 9302.6,-476.571 9302.6,-205.35 9302.6,-156.623 9302.6,-100.371 9302.6,-64.4848"
        },
        {
            "source": "W14-0145",
            "target": "W19-7509",
            "d": "M8036.66,-628.104C8036.83,-552.314 8037.32,-331.12 8037.52,-243.602"
        },
        {
            "source": "W14-0147",
            "target": "W19-7512",
            "d": "M12734.6,-627.888C12734.6,-593.269 12734.6,-530.339 12734.6,-476.571 12734.6,-476.571 12734.6,-476.571 12734.6,-384.831 12734.6,-336.103 12734.6,-279.851 12734.6,-243.965"
        },
        {
            "source": "P14-2007",
            "target": "W16-1904",
            "d": "M6030.59,-628.171C6016.98,-602.984 5993.69,-564.761 5965.6,-538.441 5950.9,-524.67 5932.6,-512.634 5915.37,-502.929"
        },
        {
            "source": "P14-2007",
            "target": "K16-1016",
            "d": "M5982.1,-634.428C5946.64,-622.792 5901.34,-607.386 5861.6,-592.181 5804.74,-570.426 5791.82,-561.796 5735.6,-538.441 5706.25,-526.249 5673.61,-513.007 5646.04,-501.913"
        },
        {
            "source": "P14-2007",
            "target": "P17-1035",
            "d": "M5993.03,-632.185C5934.97,-605.845 5838.26,-558.123 5764.6,-502.441 5732.27,-478.006 5700.47,-444.361 5678.88,-419.666"
        },
        {
            "source": "P14-2007",
            "target": "P18-1219",
            "d": "M6071.94,-629.376C6101.51,-601.75 6146.45,-554.042 6167.6,-502.441 6190.44,-446.703 6191.46,-375.534 6189.48,-333.29"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W14-5126",
            "d": "M729.144,-655.051C731.456,-655.051 733.768,-655.051 736.079,-655.051"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W14-3308",
            "d": "M666.248,-678.671C690.731,-687.172 719.615,-695.703 746.597,-699.921 845.837,-715.435 874.827,-722.942 972.597,-699.921 987.51,-696.41 1002.77,-690.034 1016.4,-683.169"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W15-5916",
            "d": "M678.963,-633.369C724.737,-620.287 784.017,-603.345 831.54,-589.763"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "N15-3017",
            "d": "M614.258,-628.2C616.696,-619.974 619.437,-610.723 622.042,-601.932"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "N15-1125",
            "d": "M693.1,-635.975C707.909,-633.166 723.17,-630.448 737.597,-628.181 873.312,-606.857 909.796,-618.669 1044.6,-592.181 1052.43,-590.642 1060.57,-588.78 1068.63,-586.768"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "L16-1098",
            "d": "M505.007,-639.941C462.001,-630.827 412.684,-616.028 372.597,-592.181 337.728,-571.437 306.946,-536.367 287.065,-510.28"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "W18-1207",
            "d": "M559.295,-630.043C515.954,-604.972 454.947,-561.107 430.597,-502.441 405.587,-442.184 428.636,-415.262 461.597,-358.96 467.607,-348.696 475.478,-338.603 483.38,-329.642"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "L18-1413",
            "d": "M501.334,-641.24C442.871,-631.926 369.635,-616.569 307.597,-592.181 236.006,-564.037 197.342,-569.469 159.597,-502.441 129.312,-448.66 153.789,-375.062 173.365,-332.145"
        },
        {
            "source": "kunchukuttan-etal-2014-shata",
            "target": "N19-1387",
            "d": "M673.527,-632.435C697.55,-622.603 723.814,-609.278 744.597,-592.181 767.188,-573.597 757.3,-553.128 782.597,-538.441 854.231,-496.851 1088.5,-553.659 1153.6,-502.441 1239.83,-434.592 1150.36,-339.535 1234.6,-269.22 1257.12,-250.418 1421.52,-229.607 1531.57,-217.548"
        },
        {
            "source": "2015",
            "target": "2016",
            "d": "M28.5975,-546.976C28.5975,-531.615 28.5975,-509.302 28.5975,-493.932"
        },
        {
            "source": "W15-5920",
            "target": "P18-1089",
            "d": "M2937.99,-538.202C2938.71,-490.309 2940.23,-388.316 2941.05,-333.59"
        },
        {
            "source": "W15-5944",
            "target": "I17-2048",
            "d": "M1548.93,-544.554C1444.62,-511.137 1234.12,-443.701 1125.26,-408.828"
        },
        {
            "source": "W15-3912",
            "target": "Q18-1022",
            "d": "M1782.7,-546.769C1748.7,-534.488 1709.4,-517.624 1699.6,-502.441 1666.03,-450.451 1690.21,-375.629 1709.69,-332.121"
        },
        {
            "source": "W15-2905",
            "target": "W16-5001",
            "d": "M7265.72,-543.406C7294.49,-530.912 7331.22,-514.956 7361.59,-501.769"
        },
        {
            "source": "W15-2905",
            "target": "K16-1015",
            "d": "M7217.6,-537.986C7217.6,-530.019 7217.6,-521.128 7217.6,-512.64"
        },
        {
            "source": "W15-2905",
            "target": "D16-1104",
            "d": "M7136.82,-560.331C7030.63,-553.9 6838.8,-538.131 6678.6,-502.441 6672.55,-501.092 6666.3,-499.444 6660.11,-497.639"
        },
        {
            "source": "W15-2905",
            "target": "L18-1424",
            "d": "M7298.85,-561.115C7405.09,-553.74 7571.37,-529.353 7514.6,-448.701 7474.39,-391.575 7283.41,-340.177 7172.3,-314.451"
        },
        {
            "source": "P15-2124",
            "target": "W15-2905",
            "d": "M6651.19,-565.311C6809.14,-565.311 6967.1,-565.311 7125.05,-565.311"
        },
        {
            "source": "P15-2124",
            "target": "W16-5001",
            "d": "M6650.79,-562.29C6797.7,-557.937 7084.67,-544.36 7324.6,-502.441 7332.71,-501.024 7341.13,-499.225 7349.45,-497.237"
        },
        {
            "source": "P15-2124",
            "target": "W16-2111",
            "d": "M6513.13,-540.798C6493.5,-529.39 6469.8,-515.622 6449.4,-503.764"
        },
        {
            "source": "P15-2124",
            "target": "U16-1013",
            "d": "M6605.28,-542.509C6635.7,-529.677 6674.2,-513.438 6705.38,-500.287"
        },
        {
            "source": "P15-2124",
            "target": "P16-1104",
            "d": "M6473.29,-550.019C6384.84,-534.276 6242.94,-509.023 6150.74,-492.614"
        },
        {
            "source": "P15-2124",
            "target": "K16-1015",
            "d": "M6646.84,-557.164C6756.96,-547.961 6945.43,-529.821 7105.6,-502.441 7115.81,-500.695 7126.48,-498.594 7137.02,-496.353"
        },
        {
            "source": "P15-2124",
            "target": "K16-1016",
            "d": "M6458.68,-558.804C6310.32,-549.76 6014.53,-529.975 5764.6,-502.441 5735.65,-499.252 5704.22,-495.05 5675.75,-490.962"
        },
        {
            "source": "P15-2124",
            "target": "D16-1104",
            "d": "M6567.74,-538.46C6572.47,-529.816 6577.82,-520.043 6582.84,-510.858"
        },
        {
            "source": "P15-2124",
            "target": "W17-7518",
            "d": "M6540.19,-538.554C6524.55,-509.093 6499.12,-463.07 6485.6,-448.701 6472.91,-435.228 6456.78,-423.149 6441.58,-413.317"
        },
        {
            "source": "P15-2124",
            "target": "P17-1035",
            "d": "M6473.3,-549.989C6419.69,-539.387 6348.02,-523.2 6286.6,-502.441 6231.62,-483.859 6223.5,-464.278 6167.6,-448.701 6025.66,-409.149 5854.82,-394.881 5749.72,-389.735"
        },
        {
            "source": "P15-2124",
            "target": "L18-1424",
            "d": "M6648.78,-559.359C6790.52,-551.043 7043.61,-532.201 7072.6,-502.441 7116.14,-457.731 7109.2,-378.697 7100.33,-332.813"
        },
        {
            "source": "P15-2124",
            "target": "W19-1309",
            "d": "M6645.9,-556.263C6720.79,-547.716 6818.13,-531.338 6844.6,-502.441 6910.03,-431.011 6895.5,-305.217 6883.19,-243.526"
        },
        {
            "source": "P15-2124",
            "target": "2020.aacl-main.31",
            "d": "M6469.29,-551.75C6410.85,-541.297 6340.01,-524.56 6319.6,-502.441 6248.17,-425.038 6341.25,-353.131 6277.6,-269.22 6223.49,-197.892 6127.06,-158.329 6055.48,-137.663"
        },
        {
            "source": "N15-3017",
            "target": "W15-3912",
            "d": "M694.053,-586.918C723.212,-596.062 758.782,-605.639 791.597,-610.181 892.304,-624.118 1606.33,-626.998 1706.6,-610.181 1731.11,-606.07 1757.18,-597.883 1779.49,-589.615"
        },
        {
            "source": "N15-3017",
            "target": "K16-1027",
            "d": "M713.702,-548.552C735.96,-544.712 760.147,-540.982 782.597,-538.441 1073.62,-505.496 1148.25,-520.183 1440.6,-502.441 1528.95,-497.078 1628.58,-490.423 1704.75,-485.205"
        },
        {
            "source": "N15-3017",
            "target": "D16-1196",
            "d": "M688.463,-542.622C719.871,-530.429 759.27,-515.133 792.242,-502.332"
        },
        {
            "source": "N15-3017",
            "target": "W17-4102",
            "d": "M535.602,-555.882C500.831,-547.624 464.975,-531.933 443.597,-502.441 429.58,-483.102 431.273,-469.16 443.597,-448.701 453.947,-431.521 470.84,-418.864 488.612,-409.642"
        },
        {
            "source": "N15-3017",
            "target": "Q18-1022",
            "d": "M714.953,-548.978C736.886,-545.204 760.598,-541.401 782.597,-538.441 946.781,-516.348 991.932,-538.617 1153.6,-502.441 1347.9,-458.96 1566.61,-368.488 1668.92,-323.571"
        },
        {
            "source": "N15-3017",
            "target": "P18-2064",
            "d": "M661.148,-539.345C673.127,-528.419 686.951,-515.201 698.597,-502.441 705.77,-494.582 788.983,-387.148 832.474,-330.901"
        },
        {
            "source": "N15-3017",
            "target": "2021.emnlp-main.675",
            "d": "M648.706,-538.671C654.849,-527.851 661.381,-514.894 665.597,-502.441 682.295,-453.119 684.597,-438.902 684.597,-386.831 684.597,-386.831 684.597,-386.831 684.597,-205.35 684.597,-153.279 682.906,-138.851 665.597,-89.7401 662.279,-80.3242 657.562,-70.6809 652.663,-61.8941"
        },
        {
            "source": "N15-1125",
            "target": "W15-5946",
            "d": "M1221.25,-565.311C1236.12,-565.311 1250.98,-565.311 1265.85,-565.311"
        },
        {
            "source": "N15-1125",
            "target": "I17-2048",
            "d": "M1149.79,-538.329C1159.41,-513.758 1168.89,-476.595 1153.6,-448.701 1145.45,-433.834 1132.04,-422.024 1117.75,-412.878"
        },
        {
            "source": "N15-1132",
            "target": "L16-1485",
            "d": "M8736.14,-560.503C8570.08,-554.721 8239.33,-543.829 7958.6,-538.441 7520.44,-530.031 4450.46,-548.056 4014.6,-502.441 3997.07,-500.606 3978.38,-497.5 3960.86,-494.064"
        },
        {
            "source": "N15-1132",
            "target": "L18-1049",
            "d": "M8843.09,-538.202C8843.98,-490.309 8845.89,-388.316 8846.91,-333.59"
        },
        {
            "source": "N15-1132",
            "target": "2018.gwc-1.34",
            "d": "M8861.39,-538.689C8897.13,-489.958 8974.86,-383.983 9014.42,-330.05"
        },
        {
            "source": "D15-1300",
            "target": "D17-1058",
            "d": "M3156.58,-538.504C3136.16,-507.527 3101.85,-455.471 3079.21,-421.134"
        },
        {
            "source": "D15-1300",
            "target": "W18-3705",
            "d": "M3188.46,-538.689C3216.47,-490.405 3277.07,-385.925 3308.61,-331.549"
        },
        {
            "source": "D15-1300",
            "target": "P18-1089",
            "d": "M3180.58,-538.241C3190.29,-495.633 3202.34,-410.653 3160.6,-358.96 3151.7,-347.937 3084,-329.841 3026.2,-316.059"
        },
        {
            "source": "2016",
            "target": "2017",
            "d": "M28.5975,-457.236C28.5975,-441.875 28.5975,-419.562 28.5975,-404.192"
        },
        {
            "source": "W16-6331",
            "target": "K18-1012",
            "d": "M4115.04,-448.383C4115.56,-417.886 4116.41,-367.334 4116.99,-333.142"
        },
        {
            "source": "W16-6331",
            "target": "2020.findings-emnlp.206",
            "d": "M4083.09,-450.327C4045.35,-422.782 3979.12,-379.053 3914.6,-358.96 3818.6,-329.067 3531.8,-396.828 3463.6,-322.96 3420.76,-276.562 3449.82,-197.26 3472.33,-151.981"
        },
        {
            "source": "W16-6336",
            "target": "L18-1489",
            "d": "M7633.07,-448.933C7615,-422.827 7583.76,-382.886 7547.6,-358.96 7519.3,-340.242 7484.84,-326.719 7453.31,-317.185"
        },
        {
            "source": "W16-4811",
            "target": "W17-4102",
            "d": "M1007.44,-456.724C996.913,-453.79 985.972,-450.971 975.597,-448.701 866.188,-424.758 738.506,-407.258 653.964,-397.098"
        },
        {
            "source": "W16-4811",
            "target": "I17-2048",
            "d": "M1062.2,-448.246C1061.47,-440.279 1060.66,-431.388 1059.89,-422.9"
        },
        {
            "source": "W16-4206",
            "target": "W16-6325",
            "d": "M13143.4,-475.571C13146,-475.571 13148.6,-475.571 13151.2,-475.571"
        },
        {
            "source": "W16-1904",
            "target": "P18-1219",
            "d": "M5901.34,-451.912C5961.33,-419.357 6071.74,-359.429 6136.19,-324.448"
        },
        {
            "source": "W16-0415",
            "target": "W16-5001",
            "d": "M7030.95,-496.694C7057.2,-505.977 7089.57,-515.81 7119.6,-520.441 7205.69,-533.719 7230.29,-538.059 7315.6,-520.441 7332.98,-516.85 7351.04,-510.211 7367.13,-503.121"
        },
        {
            "source": "P16-1104",
            "target": "P17-1035",
            "d": "M5989.98,-456.941C5978.18,-454.11 5966.06,-451.268 5954.6,-448.701 5880.53,-432.106 5795.86,-414.971 5734.88,-402.953"
        },
        {
            "source": "N16-4006",
            "target": "W16-4811",
            "d": "M621.3,-495.891C703.362,-517.911 846.562,-546.614 966.597,-520.441 982.949,-516.875 999.837,-510.324 1014.89,-503.309"
        },
        {
            "source": "N16-4006",
            "target": "D16-1196",
            "d": "M656.426,-475.571C684.325,-475.571 712.225,-475.571 740.125,-475.571"
        },
        {
            "source": "N16-4006",
            "target": "W17-4102",
            "d": "M556.398,-448.246C556.942,-440.279 557.55,-431.388 558.131,-422.9"
        },
        {
            "source": "N16-4006",
            "target": "I17-2048",
            "d": "M637.774,-460.033C730.257,-443.869 878.839,-417.899 971.922,-401.63"
        },
        {
            "source": "N16-4006",
            "target": "2021.emnlp-main.675",
            "d": "M496.392,-453.362C435.476,-426.969 348.597,-375.571 348.597,-297.09 348.597,-297.09 348.597,-297.09 348.597,-205.35 348.597,-105.848 475.908,-59.9334 559.448,-40.5156"
        },
        {
            "source": "L16-1098",
            "target": "L18-1413",
            "d": "M253.128,-448.763C240.704,-418.138 219.921,-366.908 205.99,-332.568"
        },
        {
            "source": "L16-1429",
            "target": "C16-1047",
            "d": "M4966.24,-475.571C4968.85,-475.571 4971.46,-475.571 4974.07,-475.571"
        },
        {
            "source": "L16-1429",
            "target": "W19-0413",
            "d": "M4944.15,-456.18C4969.91,-445.452 5001.23,-430.519 5026.6,-412.701 5098.78,-362.005 5168.13,-284.588 5204.75,-240.722"
        },
        {
            "source": "K16-1015",
            "target": "W16-5001",
            "d": "M7315.44,-475.571C7317.9,-475.571 7320.36,-475.571 7322.82,-475.571"
        },
        {
            "source": "K16-1016",
            "target": "P17-1035",
            "d": "M5602.56,-449.193C5609.71,-440.098 5617.88,-429.706 5625.47,-420.048"
        },
        {
            "source": "K16-1027",
            "target": "Q18-1022",
            "d": "M1813.91,-448.763C1796.33,-417.875 1766.83,-366.027 1747.29,-331.688"
        },
        {
            "source": "D16-1104",
            "target": "U16-1013",
            "d": "M6670.35,-475.571C6672.8,-475.571 6675.26,-475.571 6677.72,-475.571"
        },
        {
            "source": "D16-1104",
            "target": "W17-2338",
            "d": "M6591.29,-448.72C6587.97,-440.405 6584.23,-431.044 6580.69,-422.168"
        },
        {
            "source": "D16-1104",
            "target": "I17-2006",
            "d": "M6640.25,-453.107C6661.13,-441.514 6687.14,-427.067 6709.55,-414.623"
        },
        {
            "source": "D16-1104",
            "target": "L18-1424",
            "d": "M6651.44,-456.828C6660.42,-453.916 6669.74,-451.078 6678.6,-448.701 6756.13,-427.878 6779.6,-438.568 6855.6,-412.701 6923.22,-389.684 6996.4,-351.579 7043.05,-325.45"
        },
        {
            "source": "D16-1104",
            "target": "C18-1155",
            "d": "M6650.5,-456.648C6659.75,-453.687 6669.4,-450.877 6678.6,-448.701 6789.55,-422.434 6819.88,-429.882 6932.6,-412.701 7177.45,-375.378 7239.89,-372.302 7482.6,-322.96 7491.32,-321.187 7500.42,-319.192 7509.45,-317.118"
        },
        {
            "source": "D16-1104",
            "target": "2018.gwc-1.31",
            "d": "M6545.93,-459.817C6526.93,-455.408 6505.47,-451.11 6485.6,-448.701 6395.18,-437.735 4927.02,-455.455 4846.6,-412.701 4820.77,-398.969 4832.77,-375.441 4808.6,-358.96 4768.53,-331.634 4717.56,-316.353 4672.03,-307.818"
        },
        {
            "source": "D16-1196",
            "target": "W16-4811",
            "d": "M966.828,-475.571C969.298,-475.571 971.769,-475.571 974.239,-475.571"
        },
        {
            "source": "D16-1196",
            "target": "W17-4102",
            "d": "M790.604,-454.551C743.436,-440.664 680.707,-422.194 632.674,-408.052"
        },
        {
            "source": "D16-1196",
            "target": "I17-2048",
            "d": "M909.845,-451.861C937.023,-439.818 970.514,-424.977 998.637,-412.515"
        },
        {
            "source": "D16-1196",
            "target": "P18-2064",
            "d": "M858.597,-448.383C858.597,-417.886 858.597,-367.334 858.597,-333.142"
        },
        {
            "source": "C16-1047",
            "target": "W17-5229",
            "d": "M5071.35,-457.236C5098.88,-444.246 5136.96,-426.285 5167.76,-411.756"
        },
        {
            "source": "C16-1047",
            "target": "D17-1057",
            "d": "M4997.56,-457.324C4990.06,-454.194 4982.15,-451.15 4974.6,-448.701 4904.85,-426.086 4883.6,-434.513 4813.6,-412.701 4808.31,-411.052 4802.84,-409.154 4797.43,-407.152"
        },
        {
            "source": "C16-1047",
            "target": "N18-1053",
            "d": "M5072.34,-457.631C5080.85,-454.288 5089.93,-451.081 5098.6,-448.701 5149.54,-434.707 5299.18,-452.787 5333.6,-412.701 5352.1,-391.147 5349.54,-358.099 5343.54,-332.848"
        },
        {
            "source": "C16-1047",
            "target": "W19-0413",
            "d": "M5072.38,-457.789C5080.89,-454.437 5089.96,-451.187 5098.6,-448.701 5186.23,-423.477 5242.04,-482.612 5300.6,-412.701 5315.93,-394.39 5311.68,-380.117 5300.6,-358.96 5287.55,-334.062 5263.42,-346.841 5248.6,-322.96 5233.87,-299.239 5230.06,-267.608 5229.69,-243.428"
        },
        {
            "source": "C16-1047",
            "target": "2020.lrec-1.621",
            "d": "M5033.76,-448.316C5033.34,-392.839 5039.61,-262.024 5102.6,-179.48 5114.03,-164.502 5130.16,-152.728 5146.73,-143.656"
        },
        {
            "source": "C16-1287",
            "target": "D17-1057",
            "d": "M4724.91,-448.72C4728.29,-440.256 4732.09,-430.71 4735.69,-421.693"
        },
        {
            "source": "C16-1287",
            "target": "N18-1053",
            "d": "M4779.76,-456.86C4791.33,-453.962 4803.29,-451.119 4814.6,-448.701 4908.06,-428.714 4935.42,-441.341 5026.6,-412.701 5080.04,-395.914 5088.51,-379.575 5140.6,-358.96 5181.36,-342.828 5228.16,-327.614 5265.24,-316.31"
        },
        {
            "source": "C16-1287",
            "target": "2020.lrec-1.201",
            "d": "M4684.02,-450.001C4673.56,-439.713 4663.12,-426.85 4657.6,-412.701 4648.91,-390.452 4651.36,-382.017 4657.6,-358.96 4662.43,-341.079 4673.76,-340.842 4678.6,-322.96 4684.83,-299.904 4684.22,-292.434 4678.6,-269.22 4668.15,-226.071 4643.17,-181.442 4624.27,-151.878"
        },
        {
            "source": "C16-1287",
            "target": "2020.icon-main.62",
            "d": "M4702.1,-448.708C4697.52,-437.854 4692.89,-424.931 4690.6,-412.701 4686.2,-389.224 4684.32,-382.006 4690.6,-358.96 4712.8,-277.437 4770.31,-195.366 4804.95,-151.066"
        },
        {
            "source": "2016.gwc-1.23",
            "target": "2018.gwc-1.37",
            "d": "M8437.8,-452.272C8389.68,-420.524 8301.32,-362.226 8247.67,-326.829"
        },
        {
            "source": "2016.gwc-1.23",
            "target": "2018.gwc-1.49",
            "d": "M8465.34,-448.763C8457.96,-418.269 8445.62,-367.347 8437.3,-333.011"
        },
        {
            "source": "2016.gwc-1.46",
            "target": "2018.gwc-1.37",
            "d": "M8222.2,-448.383C8218.25,-417.886 8211.7,-367.334 8207.27,-333.142"
        },
        {
            "source": "2016.gwc-1.46",
            "target": "W19-7509",
            "d": "M8199.73,-448.854C8171.56,-419.955 8126.31,-370.802 8094.6,-322.96 8077.65,-297.386 8062.5,-266.043 8052.09,-242.447"
        },
        {
            "source": "2016.gwc-1.57",
            "target": "L18-1728",
            "d": "M4354.06,-448.383C4349.93,-417.886 4343.1,-367.334 4338.47,-333.142"
        },
        {
            "source": "2016.gwc-1.57",
            "target": "2018.gwc-1.31",
            "d": "M4385.83,-449.518C4421.31,-418.029 4482.28,-363.924 4521.23,-329.365"
        },
        {
            "source": "2017",
            "target": "2018",
            "d": "M28.5975,-367.496C28.5975,-352.135 28.5975,-329.821 28.5975,-314.452"
        },
        {
            "source": "W17-7518",
            "target": "W18-0522",
            "d": "M6393.6,-358.506C6393.6,-350.539 6393.6,-341.648 6393.6,-333.159"
        },
        {
            "source": "W17-7531",
            "target": "W17-7531",
            "d": "M7971.06,-402.24C7992.88,-401.484 8009.59,-396.014 8009.59,-385.831 8009.59,-377.318 7997.91,-372.099 7981.34,-370.174"
        },
        {
            "source": "W17-7531",
            "target": "W19-7509",
            "d": "M7913.37,-359.402C7938.89,-328.167 7982.25,-275.096 8010.4,-240.639"
        },
        {
            "source": "W17-4102",
            "target": "I17-2048",
            "d": "M650.691,-385.831C753.245,-385.831 855.799,-385.831 958.353,-385.831"
        },
        {
            "source": "W17-4102",
            "target": "W18-1207",
            "d": "M547.634,-358.98C543.382,-350.501 538.585,-340.936 534.056,-331.905"
        },
        {
            "source": "E17-1109",
            "target": "W17-5229",
            "d": "M5017.3,-385.831C5057.9,-385.831 5098.49,-385.831 5139.09,-385.831"
        },
        {
            "source": "2018",
            "target": "2019",
            "d": "M28.5975,-277.756C28.5975,-262.395 28.5975,-240.081 28.5975,-224.712"
        },
        {
            "source": "Y18-3012",
            "target": "2021.wat-1.29",
            "d": "M12878.6,-268.982C12878.6,-221.088 12878.6,-119.096 12878.6,-64.3697"
        },
        {
            "source": "W18-3705",
            "target": "2020.bea-1.8",
            "d": "M3328.6,-268.903C3328.6,-238.405 3328.6,-187.854 3328.6,-153.662"
        },
        {
            "source": "P18-2011",
            "target": "W19-2404",
            "d": "M13175.1,-270.182C13158.6,-259.742 13139.2,-247.527 13122,-236.657"
        },
        {
            "source": "P18-2011",
            "target": "N19-2017",
            "d": "M13233.5,-269.24C13239.9,-260.416 13247.2,-250.415 13254,-241.065"
        },
        {
            "source": "P18-2064",
            "target": "2020.lrec-1.613",
            "d": "M929.264,-279.898C1050.58,-254.48 1306.58,-203.738 1525.6,-179.48 1935.93,-134.034 2425.01,-121.97 2655.28,-118.768"
        },
        {
            "source": "P18-1089",
            "target": "2020.lrec-1.613",
            "d": "M2919.53,-269.662C2892.51,-238.516 2846.64,-185.658 2816.74,-151.193"
        },
        {
            "source": "P18-1219",
            "target": "2020.icon-main.23",
            "d": "M6215.36,-270.785C6252.82,-239.123 6318.38,-183.718 6359.53,-148.935"
        },
        {
            "source": "P18-1219",
            "target": "2020.aacl-main.86",
            "d": "M6185.86,-268.903C6185,-238.405 6183.58,-187.854 6182.61,-153.662"
        },
        {
            "source": "L18-1278",
            "target": "K18-1012",
            "d": "M3794.59,-319.155C3809.29,-327.8 3827.12,-336.584 3844.6,-340.96 3909.27,-357.15 3928.95,-352.561 3994.6,-340.96 4015.18,-337.323 4036.85,-330.509 4056.17,-323.269"
        },
        {
            "source": "L18-1278",
            "target": "2020.aacl-main.90",
            "d": "M3789.88,-271.888C3830.3,-239.867 3903.17,-182.123 3947.4,-147.085"
        },
        {
            "source": "L18-1440",
            "target": "K18-1012",
            "d": "M3625.14,-319.574C3646.14,-328.112 3671.07,-336.705 3694.6,-340.96 3825.8,-364.688 3863.3,-364.161 3994.6,-340.96 4015.18,-337.323 4036.85,-330.509 4056.17,-323.269"
        },
        {
            "source": "L18-1440",
            "target": "2020.coling-main.249",
            "d": "M3595.23,-269.662C3620.57,-238.427 3663.63,-185.356 3691.59,-150.899"
        },
        {
            "source": "L18-1442",
            "target": "2020.lrec-1.621",
            "d": "M5499.86,-273.942C5475.95,-263.093 5447.5,-248.874 5423.6,-233.22 5392.81,-213.058 5391.83,-198.945 5360.6,-179.48 5337.9,-165.335 5311.4,-152.886 5287.43,-142.935"
        },
        {
            "source": "L18-1489",
            "target": "C18-1155",
            "d": "M7473.32,-296.09C7475.95,-296.09 7478.58,-296.09 7481.2,-296.09"
        },
        {
            "source": "L18-1489",
            "target": "2020.findings-emnlp.386",
            "d": "M7378.17,-269.283C7398.01,-238.395 7431.32,-186.547 7453.37,-152.207"
        },
        {
            "source": "L18-1489",
            "target": "2021.findings-acl.256",
            "d": "M7352.65,-269.144C7340.12,-227.818 7322.45,-145.981 7356.6,-89.7401 7365.1,-75.7298 7377.93,-64.7464 7392.06,-56.1847"
        },
        {
            "source": "L18-1548",
            "target": "W19-5426",
            "d": "M3125.53,-270.65C3115.55,-260.927 3103.94,-249.608 3093.32,-239.253"
        },
        {
            "source": "L18-1548",
            "target": "N19-1387",
            "d": "M3095.43,-278.907C3080.37,-275.091 3063.96,-271.48 3048.6,-269.22 2796.07,-232.084 2023.27,-214.52 1738.46,-209.183"
        },
        {
            "source": "L18-1548",
            "target": "2021.mtsummit-research.2",
            "d": "M3157.28,-269.045C3159.9,-258.052 3162.72,-245.076 3164.6,-233.22 3173.94,-174.343 3179.17,-105.184 3181.68,-64.0251"
        },
        {
            "source": "L18-1548",
            "target": "2021.emnlp-main.675",
            "d": "M3093.58,-279.906C3039.92,-265.14 2966.36,-243.481 2956.6,-233.22 2911.13,-185.427 2970.33,-130.677 2918.6,-89.7401 2874.3,-54.6803 1102.08,-33.0754 707.797,-28.6971"
        },
        {
            "source": "L18-1548",
            "target": "2021.calcs-1.5",
            "d": "M3164.53,-269.467C3170.35,-258.43 3177.04,-245.308 3182.6,-233.22 3211.32,-170.721 3189.03,-135.254 3240.6,-89.7401 3259.76,-72.8285 3313.67,-57.9717 3364.81,-47.0581"
        },
        {
            "source": "L18-1559",
            "target": "C18-1237",
            "d": "M14371.9,-296.09C14374.2,-296.09 14376.6,-296.09 14378.9,-296.09"
        },
        {
            "source": "K18-1012",
            "target": "2020.findings-emnlp.206",
            "d": "M4040.91,-277.657C3976.63,-262.92 3890,-242.732 3855.6,-233.22 3746.95,-203.185 3622.08,-162.057 3550.45,-137.815"
        },
        {
            "source": "K18-1012",
            "target": "2020.coling-main.249",
            "d": "M4066.66,-272.432C3992.97,-239.655 3856.9,-179.13 3778.5,-144.256"
        },
        {
            "source": "K18-1012",
            "target": "2020.aacl-main.90",
            "d": "M4098.54,-269.662C4075.1,-238.382 4035.25,-185.204 4009.43,-150.75"
        },
        {
            "source": "K18-1012",
            "target": "2021.ltedi-1.29",
            "d": "M4117.6,-268.982C4117.6,-221.088 4117.6,-119.096 4117.6,-64.3697"
        },
        {
            "source": "D18-1382",
            "target": "W19-0413",
            "d": "M5690.68,-277.172C5676.65,-274.375 5662.23,-271.62 5648.6,-269.22 5540.2,-250.145 5512.46,-249.405 5403.6,-233.22 5382.84,-230.134 5360.68,-226.817 5339.4,-223.622"
        },
        {
            "source": "D18-1382",
            "target": "N19-1034",
            "d": "M5713.16,-273.176C5676.45,-260.083 5629.88,-243.475 5592.71,-230.217"
        },
        {
            "source": "D18-1382",
            "target": "D19-1566",
            "d": "M5768.12,-269.24C5766.08,-261.102 5763.78,-251.962 5761.6,-243.255"
        },
        {
            "source": "D18-1382",
            "target": "2020.coling-main.393",
            "d": "M5690.29,-277.365C5676.38,-274.559 5662.1,-271.751 5648.6,-269.22 5601.63,-260.414 5467.43,-269.729 5436.6,-233.22 5421.19,-214.972 5427.63,-201.616 5436.6,-179.48 5441.03,-168.54 5448.43,-158.467 5456.55,-149.766"
        },
        {
            "source": "D18-1382",
            "target": "2020.acl-main.401",
            "d": "M5839.07,-273.601C5857.34,-264.193 5875.09,-251.102 5885.6,-233.22 5897.7,-212.627 5898.97,-199.268 5885.6,-179.48 5871.7,-158.919 5849.72,-145.19 5826.67,-136.023"
        },
        {
            "source": "D18-1382",
            "target": "2020.aacl-main.31",
            "d": "M5840.22,-273.658C5862.61,-263.959 5886.53,-250.674 5904.6,-233.22 5927.57,-211.028 5943.46,-178.129 5953.09,-153.095"
        },
        {
            "source": "C18-1042",
            "target": "K18-1012",
            "d": "M3994.62,-296.09C3997.28,-296.09 3999.95,-296.09 4002.61,-296.09"
        },
        {
            "source": "C18-1155",
            "target": "2020.findings-emnlp.386",
            "d": "M7572.17,-269.283C7552.5,-238.395 7519.49,-186.547 7497.63,-152.207"
        },
        {
            "source": "C18-1155",
            "target": "2021.findings-acl.256",
            "d": "M7597.73,-269.145C7610.56,-227.822 7628.75,-145.988 7594.6,-89.7401 7586.09,-75.7298 7573.27,-64.7464 7559.14,-56.1847"
        },
        {
            "source": "2018.gwc-1.47",
            "target": "2020.nuse-1.11",
            "d": "M14681.6,-268.903C14681.6,-238.405 14681.6,-187.854 14681.6,-153.662"
        },
        {
            "source": "2019",
            "target": "2020",
            "d": "M28.5975,-188.016C28.5975,-172.655 28.5975,-150.341 28.5975,-134.972"
        },
        {
            "source": "N19-1034",
            "target": "2020.coling-main.393",
            "d": "M5519.17,-179.5C5516.17,-171.273 5512.8,-162.023 5509.59,-153.231"
        },
        {
            "source": "N19-1034",
            "target": "2020.acl-main.401",
            "d": "M5576.96,-184.446C5604.9,-172.371 5640.31,-157.064 5670.2,-144.146"
        },
        {
            "source": "N19-1091",
            "target": "2020.lrec-1.514",
            "d": "M16349.6,-179.025C16349.6,-171.059 16349.6,-162.168 16349.6,-153.679"
        },
        {
            "source": "D19-1566",
            "target": "2020.coling-main.393",
            "d": "M5688.33,-183.323C5652.03,-170.881 5606.54,-155.293 5568.99,-142.421"
        },
        {
            "source": "D19-1566",
            "target": "2020.acl-main.401",
            "d": "M5746.41,-179.5C5744.46,-171.362 5742.27,-162.222 5740.19,-153.515"
        },
        {
            "source": "D19-1566",
            "target": "2020.aacl-main.31",
            "d": "M5808.3,-182.298C5836.75,-170.523 5871.49,-156.144 5900.99,-143.936"
        },
        {
            "source": "2019.gwc-1.51",
            "target": "2020.lrec-1.378",
            "d": "M17397,-180.91C17384.4,-170.916 17369.8,-159.236 17356.5,-148.65"
        },
        {
            "source": "2019.gwc-1.51",
            "target": "2020.coling-main.119",
            "d": "M17458,-180.91C17470.3,-171 17484.6,-159.432 17497.7,-148.917"
        },
        {
            "source": "2020",
            "target": "2021",
            "d": "M28.5975,-98.2755C28.5975,-82.9146 28.5975,-60.601 28.5975,-45.2319"
        },
        {
            "source": "2020.sltu-1.49",
            "target": "2021.emnlp-main.675",
            "d": "M592.927,-90.2321C598.256,-81.4107 604.322,-71.3683 610.006,-61.9585"
        },
        {
            "source": "2020.lrec-1.378",
            "target": "2020.coling-main.119",
            "d": "M17415.5,-116.61C17418,-116.61 17420.5,-116.61 17422.9,-116.61"
        },
        {
            "source": "2020.iwslt-1.22",
            "target": "2021.eacl-main.299",
            "d": "M19905.6,-89.2852C19905.6,-81.3185 19905.6,-72.4275 19905.6,-63.9391"
        },
        {
            "source": "2020.findings-emnlp.206",
            "target": "2021.calcs-1.5",
            "d": "M3486.29,-89.7598C3484.62,-81.6214 3482.75,-72.4816 3480.96,-63.7749"
        },
        {
            "source": "2020.findings-emnlp.386",
            "target": "2021.findings-acl.256",
            "d": "M7475.6,-89.2852C7475.6,-81.3185 7475.6,-72.4275 7475.6,-63.9391"
        },
        {
            "source": "2020.acl-main.401",
            "target": "2020.aacl-main.31",
            "d": "M5837.18,-116.61C5839.63,-116.61 5842.08,-116.61 5844.53,-116.61"
        },
        {
            "source": "2020.acl-main.402",
            "target": "2021.naacl-main.456",
            "d": "M21681.6,-89.2852C21681.6,-81.3185 21681.6,-72.4275 21681.6,-63.9391"
        },
        {
            "source": "2020.aacl-main.86",
            "target": "2020.icon-main.23",
            "d": "M6271.46,-116.61C6273.94,-116.61 6276.41,-116.61 6278.88,-116.61"
        },
        {
            "source": "2021.mtsummit-research.2",
            "target": "2021.mtsummit-research.2",
            "d": "M3265.48,-43.259C3287.81,-42.4307 3304.84,-36.9677 3304.84,-26.8701 3304.84,-18.3502 3292.71,-13.1297 3275.51,-11.2088"
        }
    ],
    [
        {
            "id": "2003",
            "name": "2003",
            "x": "28.5975",
            "y": "-1638.49"
        },
        {
            "id": "2004",
            "name": "2004",
            "x": "28.5975",
            "y": "-1548.75"
        },
        {
            "id": "W03-1201",
            "name": "ganesh2003Question",
            "x": "168.597",
            "y": "-1645.99"
        },
        {
            "id": "W03-1201",
            "name": "45",
            "x": "168.597",
            "y": "-1630.99"
        },
        {
            "id": "2005",
            "name": "2005",
            "x": "28.5975",
            "y": "-1459.01"
        },
        {
            "id": "W04-0853",
            "name": "ganesh2004A",
            "x": "140.597",
            "y": "-1556.25"
        },
        {
            "id": "W04-0853",
            "name": "7",
            "x": "140.597",
            "y": "-1541.25"
        },
        {
            "id": "bellare-etal-2004-generic",
            "name": "kedar2004Generic",
            "x": "306.597",
            "y": "-1556.25"
        },
        {
            "id": "bellare-etal-2004-generic",
            "name": "25",
            "x": "306.597",
            "y": "-1541.25"
        },
        {
            "id": "2006",
            "name": "2006",
            "x": "28.5975",
            "y": "-1369.27"
        },
        {
            "id": "2005.mtsummit-papers.14",
            "name": "rajat2005Semantically",
            "x": "171.597",
            "y": "-1466.51"
        },
        {
            "id": "2005.mtsummit-papers.14",
            "name": "???",
            "x": "171.597",
            "y": "-1451.51"
        },
        {
            "id": "2007",
            "name": "2007",
            "x": "28.5975",
            "y": "-1279.53"
        },
        {
            "id": "P06-2100",
            "name": "smriti2006Morphological",
            "x": "4586.6",
            "y": "-1376.77"
        },
        {
            "id": "P06-2100",
            "name": "60",
            "x": "4586.6",
            "y": "-1361.77"
        },
        {
            "id": "I08-1067",
            "name": "ananthakrishnan2008Simple",
            "x": "2231.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-1067",
            "name": "61",
            "x": "2231.6",
            "y": "-1182.29"
        },
        {
            "id": "C10-2040",
            "name": "harshada2010Verbs",
            "x": "6493.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-2040",
            "name": "13",
            "x": "6493.6",
            "y": "-1002.81"
        },
        {
            "id": "2008",
            "name": "2008",
            "x": "28.5975",
            "y": "-1189.79"
        },
        {
            "id": "2007.mtsummit-papers.56",
            "name": "smriti2007{H}indi",
            "x": "5482.6",
            "y": "-1287.03"
        },
        {
            "id": "2007.mtsummit-papers.56",
            "name": "???",
            "x": "5482.6",
            "y": "-1272.03"
        },
        {
            "id": "2009",
            "name": "2009",
            "x": "28.5975",
            "y": "-1100.05"
        },
        {
            "id": "sankaran-etal-2008-common",
            "name": "baskaran2008A",
            "x": "7299.6",
            "y": "-1197.29"
        },
        {
            "id": "sankaran-etal-2008-common",
            "name": "29",
            "x": "7299.6",
            "y": "-1182.29"
        },
        {
            "id": "mohanty-bhattacharyya-2008-lexical",
            "name": "rajat2008Lexical",
            "x": "7866.6",
            "y": "-1197.29"
        },
        {
            "id": "mohanty-bhattacharyya-2008-lexical",
            "name": "1",
            "x": "7866.6",
            "y": "-1182.29"
        },
        {
            "id": "I08-7013",
            "name": "sankaran2008Designing",
            "x": "7104.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-7013",
            "name": "8",
            "x": "7104.6",
            "y": "-1182.29"
        },
        {
            "id": "I08-6009",
            "name": "manoj2008{H}indi",
            "x": "8845.6",
            "y": "-1197.29"
        },
        {
            "id": "I08-6009",
            "name": "2",
            "x": "8845.6",
            "y": "-1182.29"
        },
        {
            "id": "P09-1090",
            "name": "ananthakrishnan2009Case",
            "x": "2161.6",
            "y": "-1107.55"
        },
        {
            "id": "P09-1090",
            "name": "35",
            "x": "2161.6",
            "y": "-1092.55"
        },
        {
            "id": "W12-5906",
            "name": "anoop2012Partially",
            "x": "919.597",
            "y": "-838.331"
        },
        {
            "id": "W12-5906",
            "name": "3",
            "x": "919.597",
            "y": "-823.331"
        },
        {
            "id": "W14-5105",
            "name": "rajen2014Supertag",
            "x": "2088.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5105",
            "name": "1",
            "x": "2088.6",
            "y": "-643.851"
        },
        {
            "id": "W14-5148",
            "name": "sudha2014Merging",
            "x": "3652.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5148",
            "name": "2",
            "x": "3652.6",
            "y": "-643.851"
        },
        {
            "id": "W14-3308",
            "name": "piyush2014The",
            "x": "1062.6",
            "y": "-658.851"
        },
        {
            "id": "W14-3308",
            "name": "9",
            "x": "1062.6",
            "y": "-643.851"
        },
        {
            "id": "kunchukuttan-etal-2014-shata",
            "name": "anoop2014Shata-Anuvadak:",
            "x": "606.597",
            "y": "-658.851"
        },
        {
            "id": "kunchukuttan-etal-2014-shata",
            "name": "15",
            "x": "606.597",
            "y": "-643.851"
        },
        {
            "id": "W15-5950",
            "name": "pratik2015Investigating",
            "x": "3425.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5950",
            "name": "0",
            "x": "3425.6",
            "y": "-554.111"
        },
        {
            "id": "W16-6303",
            "name": "debajyoty2016Can",
            "x": "2128.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6303",
            "name": "3",
            "x": "2128.6",
            "y": "-464.371"
        },
        {
            "id": "W16-4622",
            "name": "sukanta2016{IITP}",
            "x": "2320.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4622",
            "name": "3",
            "x": "2320.6",
            "y": "-464.371"
        },
        {
            "id": "L16-1485",
            "name": "sudha2016Synset",
            "x": "3886.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1485",
            "name": "1",
            "x": "3886.6",
            "y": "-464.371"
        },
        {
            "id": "I17-2048",
            "name": "anoop2017Utilizing",
            "x": "1056.6",
            "y": "-389.631"
        },
        {
            "id": "I17-2048",
            "name": "1",
            "x": "1056.6",
            "y": "-374.631"
        },
        {
            "id": "L18-1548",
            "name": "anoop2018The",
            "x": "3150.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1548",
            "name": "???",
            "x": "3150.6",
            "y": "-284.89"
        },
        {
            "id": "N19-1387",
            "name": "rudra2019Addressing",
            "x": "1631.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1387",
            "name": "6",
            "x": "1631.6",
            "y": "-195.15"
        },
        {
            "id": "C08-2007",
            "name": "debasri2008{H}indi",
            "x": "9439.6",
            "y": "-1197.29"
        },
        {
            "id": "C08-2007",
            "name": "19",
            "x": "9439.6",
            "y": "-1182.29"
        },
        {
            "id": "L16-1369",
            "name": "dhirendra2016Multiword",
            "x": "9439.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1369",
            "name": "0",
            "x": "9439.6",
            "y": "-464.371"
        },
        {
            "id": "C08-1068",
            "name": "abbas2008{H}indi",
            "x": "9761.6",
            "y": "-1197.29"
        },
        {
            "id": "C08-1068",
            "name": "30",
            "x": "9761.6",
            "y": "-1182.29"
        },
        {
            "id": "W09-3536",
            "name": "abbas2009A",
            "x": "9698.6",
            "y": "-1107.55"
        },
        {
            "id": "W09-3536",
            "name": "20",
            "x": "9698.6",
            "y": "-1092.55"
        },
        {
            "id": "C10-2091",
            "name": "abbas2010Finite-state",
            "x": "9564.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-2091",
            "name": "2",
            "x": "9564.6",
            "y": "-1002.81"
        },
        {
            "id": "2010.jeptalnrecital-court.7",
            "name": "muhammad2010Weak",
            "x": "9816.6",
            "y": "-1017.81"
        },
        {
            "id": "2010.jeptalnrecital-court.7",
            "name": "0",
            "x": "9816.6",
            "y": "-1002.81"
        },
        {
            "id": "W13-4706",
            "name": "abbas2013{U}rdu",
            "x": "9816.6",
            "y": "-748.591"
        },
        {
            "id": "W13-4706",
            "name": "3",
            "x": "9816.6",
            "y": "-733.591"
        },
        {
            "id": "2010",
            "name": "2010",
            "x": "28.5975",
            "y": "-1010.31"
        },
        {
            "id": "W09-3518",
            "name": "mitesh2009Improving",
            "x": "10068.6",
            "y": "-1107.55"
        },
        {
            "id": "W09-3518",
            "name": "8",
            "x": "10068.6",
            "y": "-1092.55"
        },
        {
            "id": "W15-5914",
            "name": "sreelekha2015Solving",
            "x": "2139.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5914",
            "name": "3",
            "x": "2139.6",
            "y": "-554.111"
        },
        {
            "id": "D09-1048",
            "name": "mitesh2009Projecting",
            "x": "4073.6",
            "y": "-1107.55"
        },
        {
            "id": "D09-1048",
            "name": "14",
            "x": "4073.6",
            "y": "-1092.55"
        },
        {
            "id": "C10-1063",
            "name": "mitesh2010Value",
            "x": "4020.6",
            "y": "-1017.81"
        },
        {
            "id": "C10-1063",
            "name": "7",
            "x": "4020.6",
            "y": "-1002.81"
        },
        {
            "id": "P11-1057",
            "name": "mitesh2011Together",
            "x": "4055.6",
            "y": "-928.071"
        },
        {
            "id": "P11-1057",
            "name": "9",
            "x": "4055.6",
            "y": "-913.071"
        },
        {
            "id": "I11-1078",
            "name": "mitesh2011It",
            "x": "3883.6",
            "y": "-928.071"
        },
        {
            "id": "I11-1078",
            "name": "4",
            "x": "3883.6",
            "y": "-913.071"
        },
        {
            "id": "2016.gwc-1.57",
            "name": "meghna2016Mapping",
            "x": "4357.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.57",
            "name": "???",
            "x": "4357.6",
            "y": "-464.371"
        },
        {
            "id": "2011",
            "name": "2011",
            "x": "28.5975",
            "y": "-920.571"
        },
        {
            "id": "W10-4001",
            "name": "pushpak2010Word",
            "x": "10067.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-4001",
            "name": "1",
            "x": "10067.6",
            "y": "-1002.81"
        },
        {
            "id": "W10-4011",
            "name": "vishal2010More",
            "x": "1392.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-4011",
            "name": "0",
            "x": "1392.6",
            "y": "-1002.81"
        },
        {
            "id": "W10-3604",
            "name": "mugdha2010A",
            "x": "10250.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-3604",
            "name": "12",
            "x": "10250.6",
            "y": "-1002.81"
        },
        {
            "id": "W12-5020",
            "name": "swapnil2012Error",
            "x": "10052.6",
            "y": "-838.331"
        },
        {
            "id": "W12-5020",
            "name": "1",
            "x": "10052.6",
            "y": "-823.331"
        },
        {
            "id": "C12-2023",
            "name": "raj2012Morphological",
            "x": "10250.6",
            "y": "-838.331"
        },
        {
            "id": "C12-2023",
            "name": "2",
            "x": "10250.6",
            "y": "-823.331"
        },
        {
            "id": "W14-5103",
            "name": "raj2014Tackling",
            "x": "10376.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5103",
            "name": "0",
            "x": "10376.6",
            "y": "-643.851"
        },
        {
            "id": "W10-3607",
            "name": "pratikkumar2010Hybrid",
            "x": "10484.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-3607",
            "name": "20",
            "x": "10484.6",
            "y": "-1002.81"
        },
        {
            "id": "W11-3001",
            "name": "kartik2011Hybrid",
            "x": "10484.6",
            "y": "-928.071"
        },
        {
            "id": "W11-3001",
            "name": "24",
            "x": "10484.6",
            "y": "-913.071"
        },
        {
            "id": "W10-2418",
            "name": "shalini2010Think",
            "x": "10685.6",
            "y": "-1017.81"
        },
        {
            "id": "W10-2418",
            "name": "5",
            "x": "10685.6",
            "y": "-1002.81"
        },
        {
            "id": "S10-1028",
            "name": "lipta2010{OWNS}:",
            "x": "10869.6",
            "y": "-1017.81"
        },
        {
            "id": "S10-1028",
            "name": "7",
            "x": "10869.6",
            "y": "-1002.81"
        },
        {
            "id": "S10-1094",
            "name": "anup2010{CFILT}:",
            "x": "11065.6",
            "y": "-1017.81"
        },
        {
            "id": "S10-1094",
            "name": "4",
            "x": "11065.6",
            "y": "-1002.81"
        },
        {
            "id": "P10-1137",
            "name": "manoj2010Multilingual",
            "x": "1162.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1137",
            "name": "8",
            "x": "1162.6",
            "y": "-1002.81"
        },
        {
            "id": "N15-1125",
            "name": "raj2015Leveraging",
            "x": "1137.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1125",
            "name": "10",
            "x": "1137.6",
            "y": "-554.111"
        },
        {
            "id": "P10-1155",
            "name": "mitesh2010All",
            "x": "2689.6",
            "y": "-1017.81"
        },
        {
            "id": "P10-1155",
            "name": "19",
            "x": "2689.6",
            "y": "-1002.81"
        },
        {
            "id": "P13-2096",
            "name": "sudha2013Neighbors",
            "x": "3410.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2096",
            "name": "5",
            "x": "3410.6",
            "y": "-733.591"
        },
        {
            "id": "W14-0124",
            "name": "brijesh2014Graph",
            "x": "2560.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0124",
            "name": "0",
            "x": "2560.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5908",
            "name": "sudha2015Using",
            "x": "3623.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5908",
            "name": "1",
            "x": "3623.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5945",
            "name": "diptesh2015Using",
            "x": "2450.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5945",
            "name": "1",
            "x": "2450.6",
            "y": "-554.111"
        },
        {
            "id": "L16-1349",
            "name": "diptesh2016That{'}ll",
            "x": "2520.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1349",
            "name": "1",
            "x": "2520.6",
            "y": "-464.371"
        },
        {
            "id": "2016.gwc-1.54",
            "name": "raksha2016High,",
            "x": "2709.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.54",
            "name": "???",
            "x": "2709.6",
            "y": "-464.371"
        },
        {
            "id": "N10-1065",
            "name": "mitesh2010Everybody",
            "x": "1892.6",
            "y": "-1017.81"
        },
        {
            "id": "N10-1065",
            "name": "17",
            "x": "1892.6",
            "y": "-1002.81"
        },
        {
            "id": "khapra-etal-2014-transliteration",
            "name": "mitesh2014When",
            "x": "1830.6",
            "y": "-658.851"
        },
        {
            "id": "khapra-etal-2014-transliteration",
            "name": "4",
            "x": "1830.6",
            "y": "-643.851"
        },
        {
            "id": "K16-1027",
            "name": "anoop2016Substring-based",
            "x": "1828.6",
            "y": "-479.371"
        },
        {
            "id": "K16-1027",
            "name": "0",
            "x": "1828.6",
            "y": "-464.371"
        },
        {
            "id": "Q18-1022",
            "name": "anoop2018Leveraging",
            "x": "1727.6",
            "y": "-299.89"
        },
        {
            "id": "Q18-1022",
            "name": "1",
            "x": "1727.6",
            "y": "-284.89"
        },
        {
            "id": "bhattacharyya-2010-indowordnet",
            "name": "pushpak2010{I}ndo{W}ord{N}et",
            "x": "4838.6",
            "y": "-1017.81"
        },
        {
            "id": "bhattacharyya-2010-indowordnet",
            "name": "???",
            "x": "4838.6",
            "y": "-1002.81"
        },
        {
            "id": "W12-5209",
            "name": "brijesh2012Domain",
            "x": "4329.6",
            "y": "-838.331"
        },
        {
            "id": "W12-5209",
            "name": "8",
            "x": "4329.6",
            "y": "-823.331"
        },
        {
            "id": "C12-2008",
            "name": "balamurali2012Cross-Lingual",
            "x": "4993.6",
            "y": "-838.331"
        },
        {
            "id": "C12-2008",
            "name": "38",
            "x": "4993.6",
            "y": "-823.331"
        },
        {
            "id": "L18-1728",
            "name": "diptesh2018{I}ndian",
            "x": "4333.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1728",
            "name": "0",
            "x": "4333.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.31",
            "name": "kevin2018Semi-automatic",
            "x": "4557.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.31",
            "name": "???",
            "x": "4557.6",
            "y": "-284.89"
        },
        {
            "id": "W19-7509",
            "name": "malhar2019Introduction",
            "x": "8037.6",
            "y": "-210.15"
        },
        {
            "id": "W19-7509",
            "name": "???",
            "x": "8037.6",
            "y": "-195.15"
        },
        {
            "id": "W15-5905",
            "name": "sachin2015Noun",
            "x": "7871.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5905",
            "name": "0",
            "x": "7871.6",
            "y": "-554.111"
        },
        {
            "id": "ar-etal-2012-cost",
            "name": "balamurali2012Cost",
            "x": "4524.6",
            "y": "-838.331"
        },
        {
            "id": "ar-etal-2012-cost",
            "name": "4",
            "x": "4524.6",
            "y": "-823.331"
        },
        {
            "id": "2012",
            "name": "2012",
            "x": "28.5975",
            "y": "-830.831"
        },
        {
            "id": "W11-1717",
            "name": "balamurali2011Robust",
            "x": "10682.6",
            "y": "-928.071"
        },
        {
            "id": "W11-1717",
            "name": "9",
            "x": "10682.6",
            "y": "-913.071"
        },
        {
            "id": "P11-4022",
            "name": "aditya2011{C}-Feel-It:",
            "x": "5850.6",
            "y": "-928.071"
        },
        {
            "id": "P11-4022",
            "name": "25",
            "x": "5850.6",
            "y": "-913.071"
        },
        {
            "id": "C12-1113",
            "name": "subhabrata2012Sentiment",
            "x": "5920.6",
            "y": "-838.331"
        },
        {
            "id": "C12-1113",
            "name": "51",
            "x": "5920.6",
            "y": "-823.331"
        },
        {
            "id": "S13-2082",
            "name": "karan2013{IITB}-Sentiment-Analysts:",
            "x": "5812.6",
            "y": "-748.591"
        },
        {
            "id": "S13-2082",
            "name": "13",
            "x": "5812.6",
            "y": "-733.591"
        },
        {
            "id": "N13-1088",
            "name": "salil2013More",
            "x": "5489.6",
            "y": "-748.591"
        },
        {
            "id": "N13-1088",
            "name": "12",
            "x": "5489.6",
            "y": "-733.591"
        },
        {
            "id": "I11-1152",
            "name": "ananthakrishnan2011Clause-Based",
            "x": "10947.6",
            "y": "-928.071"
        },
        {
            "id": "I11-1152",
            "name": "11",
            "x": "10947.6",
            "y": "-913.071"
        },
        {
            "id": "D11-1100",
            "name": "balamurali2011Harnessing",
            "x": "5186.6",
            "y": "-928.071"
        },
        {
            "id": "D11-1100",
            "name": "26",
            "x": "5186.6",
            "y": "-913.071"
        },
        {
            "id": "P13-1041",
            "name": "kashyap2013The",
            "x": "4375.6",
            "y": "-748.591"
        },
        {
            "id": "P13-1041",
            "name": "20",
            "x": "4375.6",
            "y": "-733.591"
        },
        {
            "id": "W14-2619",
            "name": "nikhilkumar2014Dive",
            "x": "5760.6",
            "y": "-658.851"
        },
        {
            "id": "W14-2619",
            "name": "4",
            "x": "5760.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5920",
            "name": "raksha2015Domain",
            "x": "2937.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5920",
            "name": "2",
            "x": "2937.6",
            "y": "-554.111"
        },
        {
            "id": "W16-6315",
            "name": "raksha2016Meaning",
            "x": "5344.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6315",
            "name": "0",
            "x": "5344.6",
            "y": "-464.371"
        },
        {
            "id": "L16-1429",
            "name": "md2016Aspect",
            "x": "4894.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1429",
            "name": "15",
            "x": "4894.6",
            "y": "-464.371"
        },
        {
            "id": "K16-1016",
            "name": "abhijit2016Leveraging",
            "x": "5582.6",
            "y": "-479.371"
        },
        {
            "id": "K16-1016",
            "name": "9",
            "x": "5582.6",
            "y": "-464.371"
        },
        {
            "id": "P17-1035",
            "name": "abhijit2017Learning",
            "x": "5651.6",
            "y": "-389.631"
        },
        {
            "id": "P17-1035",
            "name": "14",
            "x": "5651.6",
            "y": "-374.631"
        },
        {
            "id": "2013",
            "name": "2013",
            "x": "28.5975",
            "y": "-741.091"
        },
        {
            "id": "W12-5806",
            "name": "aliabbas2012Textbook",
            "x": "10505.6",
            "y": "-838.331"
        },
        {
            "id": "W12-5806",
            "name": "0",
            "x": "10505.6",
            "y": "-823.331"
        },
        {
            "id": "W12-5018",
            "name": "arjun2012Building",
            "x": "10706.6",
            "y": "-838.331"
        },
        {
            "id": "W12-5018",
            "name": "2",
            "x": "10706.6",
            "y": "-823.331"
        },
        {
            "id": "W12-4906",
            "name": "abhijit2012A",
            "x": "6138.6",
            "y": "-838.331"
        },
        {
            "id": "W12-4906",
            "name": "9",
            "x": "6138.6",
            "y": "-823.331"
        },
        {
            "id": "P13-2062",
            "name": "abhijit2013Automatically",
            "x": "6138.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2062",
            "name": "14",
            "x": "6138.6",
            "y": "-733.591"
        },
        {
            "id": "S12-1098",
            "name": "janardhan2012janardhan:",
            "x": "10914.6",
            "y": "-838.331"
        },
        {
            "id": "S12-1098",
            "name": "2",
            "x": "10914.6",
            "y": "-823.331"
        },
        {
            "id": "kunchukuttan-etal-2012-experiences",
            "name": "anoop2012Experiences",
            "x": "3721.6",
            "y": "-838.331"
        },
        {
            "id": "kunchukuttan-etal-2012-experiences",
            "name": "9",
            "x": "3721.6",
            "y": "-823.331"
        },
        {
            "id": "P13-4030",
            "name": "anoop2013{T}rans{D}oop:",
            "x": "3721.6",
            "y": "-748.591"
        },
        {
            "id": "P13-4030",
            "name": "2",
            "x": "3721.6",
            "y": "-733.591"
        },
        {
            "id": "D12-1012",
            "name": "ajay2012Towards",
            "x": "11122.6",
            "y": "-838.331"
        },
        {
            "id": "D12-1012",
            "name": "6",
            "x": "11122.6",
            "y": "-823.331"
        },
        {
            "id": "C12-3013",
            "name": "shilpa2012Automated",
            "x": "11319.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3013",
            "name": "5",
            "x": "11319.6",
            "y": "-823.331"
        },
        {
            "id": "W14-5136",
            "name": "shilpa2014{A}uto{P}ar{S}e:",
            "x": "11319.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5136",
            "name": "1",
            "x": "11319.6",
            "y": "-643.851"
        },
        {
            "id": "C12-3030",
            "name": "salil2012Eating",
            "x": "4740.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3030",
            "name": "1",
            "x": "4740.6",
            "y": "-823.331"
        },
        {
            "id": "C12-3031",
            "name": "salil2012{I}",
            "x": "11493.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3031",
            "name": "2",
            "x": "11493.6",
            "y": "-823.331"
        },
        {
            "id": "C12-3033",
            "name": "diptesh2012Discrimination-Net",
            "x": "5385.6",
            "y": "-838.331"
        },
        {
            "id": "C12-3033",
            "name": "1",
            "x": "5385.6",
            "y": "-823.331"
        },
        {
            "id": "W14-0126",
            "name": "diptesh2014Do",
            "x": "5323.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0126",
            "name": "0",
            "x": "5323.6",
            "y": "-643.851"
        },
        {
            "id": "C16-1047",
            "name": "md2016A",
            "x": "5034.6",
            "y": "-479.371"
        },
        {
            "id": "C16-1047",
            "name": "20",
            "x": "5034.6",
            "y": "-464.371"
        },
        {
            "id": "C16-1287",
            "name": "prerana2016Borrow",
            "x": "4714.6",
            "y": "-479.371"
        },
        {
            "id": "C16-1287",
            "name": "5",
            "x": "4714.6",
            "y": "-464.371"
        },
        {
            "id": "N18-1053",
            "name": "md2018Solving",
            "x": "5331.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1053",
            "name": "4",
            "x": "5331.6",
            "y": "-284.89"
        },
        {
            "id": "W19-0413",
            "name": "md2019Language-Agnostic",
            "x": "5231.6",
            "y": "-210.15"
        },
        {
            "id": "W19-0413",
            "name": "0",
            "x": "5231.6",
            "y": "-195.15"
        },
        {
            "id": "C12-1114",
            "name": "subhabrata2012{Y}ou{C}at:",
            "x": "11696.6",
            "y": "-838.331"
        },
        {
            "id": "C12-1114",
            "name": "3",
            "x": "11696.6",
            "y": "-823.331"
        },
        {
            "id": "2014",
            "name": "2014",
            "x": "28.5975",
            "y": "-651.351"
        },
        {
            "id": "W13-3611",
            "name": "anoop2013{IITB}",
            "x": "11603.6",
            "y": "-748.591"
        },
        {
            "id": "W13-3611",
            "name": "5",
            "x": "11603.6",
            "y": "-733.591"
        },
        {
            "id": "W14-1708",
            "name": "anoop2014Tuning",
            "x": "11548.6",
            "y": "-658.851"
        },
        {
            "id": "W14-1708",
            "name": "6",
            "x": "11548.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5902",
            "name": "anoop2015Addressing",
            "x": "11604.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5902",
            "name": "0",
            "x": "11604.6",
            "y": "-554.111"
        },
        {
            "id": "S13-1031",
            "name": "avishek2013{CFILT}-{CORE}:",
            "x": "11844.6",
            "y": "-748.591"
        },
        {
            "id": "S13-1031",
            "name": "1",
            "x": "11844.6",
            "y": "-733.591"
        },
        {
            "id": "P13-2048",
            "name": "brijesh2013{I}ndo{N}et:",
            "x": "12112.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2048",
            "name": "3",
            "x": "12112.6",
            "y": "-733.591"
        },
        {
            "id": "P14-2007",
            "name": "aditya2014Measuring",
            "x": "6043.6",
            "y": "-658.851"
        },
        {
            "id": "P14-2007",
            "name": "13",
            "x": "6043.6",
            "y": "-643.851"
        },
        {
            "id": "P18-1219",
            "name": "sandeep2018Eyes",
            "x": "6186.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1219",
            "name": "2",
            "x": "6186.6",
            "y": "-284.89"
        },
        {
            "id": "2020.aacl-main.86",
            "name": "sandeep2020Happy",
            "x": "6181.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.86",
            "name": "0",
            "x": "6181.6",
            "y": "-105.41"
        },
        {
            "id": "P13-2149",
            "name": "ankit2013Detecting",
            "x": "6350.6",
            "y": "-748.591"
        },
        {
            "id": "P13-2149",
            "name": "10",
            "x": "6350.6",
            "y": "-733.591"
        },
        {
            "id": "P15-2124",
            "name": "aditya2015Harnessing",
            "x": "6553.6",
            "y": "-569.111"
        },
        {
            "id": "P15-2124",
            "name": "101",
            "x": "6553.6",
            "y": "-554.111"
        },
        {
            "id": "D15-1300",
            "name": "raksha2015Adjective",
            "x": "3173.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1300",
            "name": "8",
            "x": "3173.6",
            "y": "-554.111"
        },
        {
            "id": "W14-2623",
            "name": "abhijit2014A",
            "x": "5473.6",
            "y": "-658.851"
        },
        {
            "id": "W14-2623",
            "name": "8",
            "x": "5473.6",
            "y": "-643.851"
        },
        {
            "id": "W16-1904",
            "name": "joe2016Leveraging",
            "x": "5859.6",
            "y": "-479.371"
        },
        {
            "id": "W16-1904",
            "name": "4",
            "x": "5859.6",
            "y": "-464.371"
        },
        {
            "id": "I13-2006",
            "name": "aditya2013Making",
            "x": "2489.6",
            "y": "-748.591"
        },
        {
            "id": "I13-2006",
            "name": "1",
            "x": "2489.6",
            "y": "-733.591"
        },
        {
            "id": "I13-1076",
            "name": "raksha2013Detecting",
            "x": "3052.6",
            "y": "-748.591"
        },
        {
            "id": "I13-1076",
            "name": "13",
            "x": "3052.6",
            "y": "-733.591"
        },
        {
            "id": "D17-1058",
            "name": "raksha2017Sentiment",
            "x": "3056.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1058",
            "name": "4",
            "x": "3056.6",
            "y": "-374.631"
        },
        {
            "id": "P18-1089",
            "name": "raksha2018Identifying",
            "x": "2941.6",
            "y": "-299.89"
        },
        {
            "id": "P18-1089",
            "name": "5",
            "x": "2941.6",
            "y": "-284.89"
        },
        {
            "id": "2020.lrec-1.613",
            "name": "akash2020Recommendation",
            "x": "2787.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.613",
            "name": "0",
            "x": "2787.6",
            "y": "-105.41"
        },
        {
            "id": "I13-1093",
            "name": "vasudevan2013Little",
            "x": "12332.6",
            "y": "-748.591"
        },
        {
            "id": "I13-1093",
            "name": "1",
            "x": "12332.6",
            "y": "-733.591"
        },
        {
            "id": "I13-1122",
            "name": "bibek2013Automated",
            "x": "12538.6",
            "y": "-748.591"
        },
        {
            "id": "I13-1122",
            "name": "2",
            "x": "12538.6",
            "y": "-733.591"
        },
        {
            "id": "I13-1131",
            "name": "arjun2013Structure",
            "x": "3985.6",
            "y": "-748.591"
        },
        {
            "id": "I13-1131",
            "name": "2",
            "x": "3985.6",
            "y": "-733.591"
        },
        {
            "id": "2015",
            "name": "2015",
            "x": "28.5975",
            "y": "-561.611"
        },
        {
            "id": "W14-5504",
            "name": "shilpa2014A",
            "x": "11748.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5504",
            "name": "3",
            "x": "11748.6",
            "y": "-643.851"
        },
        {
            "id": "W14-5111",
            "name": "ankit2014{H}in{MA}:",
            "x": "11927.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5111",
            "name": "2",
            "x": "11927.6",
            "y": "-643.851"
        },
        {
            "id": "W14-5113",
            "name": "raj2014Anou",
            "x": "12107.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5113",
            "name": "0",
            "x": "12107.6",
            "y": "-643.851"
        },
        {
            "id": "W14-5115",
            "name": "hanumant2014Introduction",
            "x": "8377.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5115",
            "name": "1",
            "x": "8377.6",
            "y": "-643.851"
        },
        {
            "id": "2016.gwc-1.46",
            "name": "hanumant2016Sam{\\=a}sa-Kart{\\=a}:",
            "x": "8225.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.46",
            "name": "???",
            "x": "8225.6",
            "y": "-464.371"
        },
        {
            "id": "W14-5124",
            "name": "raksha2014A",
            "x": "12250.6",
            "y": "-658.851"
        },
        {
            "id": "W14-5124",
            "name": "4",
            "x": "12250.6",
            "y": "-643.851"
        },
        {
            "id": "W14-5126",
            "name": "diptesh2014{P}a{CM}an",
            "x": "859.597",
            "y": "-658.851"
        },
        {
            "id": "W14-5126",
            "name": "0",
            "x": "859.597",
            "y": "-643.851"
        },
        {
            "id": "W15-5916",
            "name": "diptesh2015{T}rans{C}hat:",
            "x": "913.597",
            "y": "-569.111"
        },
        {
            "id": "W15-5916",
            "name": "1",
            "x": "913.597",
            "y": "-554.111"
        },
        {
            "id": "W14-3350",
            "name": "shubham2014{LAYERED}:",
            "x": "12454.6",
            "y": "-658.851"
        },
        {
            "id": "W14-3350",
            "name": "15",
            "x": "12454.6",
            "y": "-643.851"
        },
        {
            "id": "W14-0130",
            "name": "pushpak2014Facilitating",
            "x": "8706.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0130",
            "name": "9",
            "x": "8706.6",
            "y": "-643.851"
        },
        {
            "id": "W15-5910",
            "name": "hanumant2015{I}ndo{W}ord{N}et",
            "x": "9121.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5910",
            "name": "0",
            "x": "9121.6",
            "y": "-554.111"
        },
        {
            "id": "2016.gwc-1.22",
            "name": "diptesh2016Sophisticated",
            "x": "8706.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.22",
            "name": "???",
            "x": "8706.6",
            "y": "-464.371"
        },
        {
            "id": "2021.naacl-main.322",
            "name": "kumar2021How",
            "x": "9302.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.322",
            "name": "???",
            "x": "9302.6",
            "y": "-15.6701"
        },
        {
            "id": "W14-0145",
            "name": "sudha2014Semi-Automatic",
            "x": "8036.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0145",
            "name": "1",
            "x": "8036.6",
            "y": "-643.851"
        },
        {
            "id": "W14-0147",
            "name": "devendra2014{I}ndo{W}ordnet",
            "x": "12734.6",
            "y": "-658.851"
        },
        {
            "id": "W14-0147",
            "name": "3",
            "x": "12734.6",
            "y": "-643.851"
        },
        {
            "id": "W19-7512",
            "name": "diptesh2019An",
            "x": "12734.6",
            "y": "-210.15"
        },
        {
            "id": "W19-7512",
            "name": "???",
            "x": "12734.6",
            "y": "-195.15"
        },
        {
            "id": "N15-3017",
            "name": "anoop2015Brahmi-Net:",
            "x": "632.597",
            "y": "-569.111"
        },
        {
            "id": "N15-3017",
            "name": "11",
            "x": "632.597",
            "y": "-554.111"
        },
        {
            "id": "L16-1098",
            "name": "sreelekha2016Lexical",
            "x": "263.597",
            "y": "-479.371"
        },
        {
            "id": "L16-1098",
            "name": "4",
            "x": "263.597",
            "y": "-464.371"
        },
        {
            "id": "W18-1207",
            "name": "tamali2018Meaningless",
            "x": "516.597",
            "y": "-299.89"
        },
        {
            "id": "W18-1207",
            "name": "2",
            "x": "516.597",
            "y": "-284.89"
        },
        {
            "id": "L18-1413",
            "name": "sreelekha2018Morphology",
            "x": "191.597",
            "y": "-299.89"
        },
        {
            "id": "L18-1413",
            "name": "0",
            "x": "191.597",
            "y": "-284.89"
        },
        {
            "id": "2016",
            "name": "2016",
            "x": "28.5975",
            "y": "-471.871"
        },
        {
            "id": "W15-5911",
            "name": "apurva2015Let",
            "x": "12832.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5911",
            "name": "0",
            "x": "12832.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5912",
            "name": "naman2015A",
            "x": "12983.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5912",
            "name": "0",
            "x": "12983.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5925",
            "name": "shehzaad2015Judge",
            "x": "13154.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5925",
            "name": "0",
            "x": "13154.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5931",
            "name": "shilpa2015Logistic",
            "x": "13347.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5931",
            "name": "0",
            "x": "13347.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5937",
            "name": "geetanjali2015Automated",
            "x": "13560.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5937",
            "name": "10",
            "x": "13560.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5943",
            "name": "dhirendra2015Detection",
            "x": "13793.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5943",
            "name": "6",
            "x": "13793.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5944",
            "name": "rohit2015Augmenting",
            "x": "1610.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5944",
            "name": "2",
            "x": "1610.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5946",
            "name": "deepak2015Triangulation",
            "x": "1386.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5946",
            "name": "0",
            "x": "1386.6",
            "y": "-554.111"
        },
        {
            "id": "W15-5947",
            "name": "ritesh2015Post-editing",
            "x": "14015.6",
            "y": "-569.111"
        },
        {
            "id": "W15-5947",
            "name": "0",
            "x": "14015.6",
            "y": "-554.111"
        },
        {
            "id": "W15-3912",
            "name": "anoop2015Data",
            "x": "1836.6",
            "y": "-569.111"
        },
        {
            "id": "W15-3912",
            "name": "7",
            "x": "1836.6",
            "y": "-554.111"
        },
        {
            "id": "W15-2905",
            "name": "anupam2015Your",
            "x": "7217.6",
            "y": "-569.111"
        },
        {
            "id": "W15-2905",
            "name": "41",
            "x": "7217.6",
            "y": "-554.111"
        },
        {
            "id": "W16-5001",
            "name": "aditya2016{`}Who",
            "x": "7419.6",
            "y": "-479.371"
        },
        {
            "id": "W16-5001",
            "name": "4",
            "x": "7419.6",
            "y": "-464.371"
        },
        {
            "id": "K16-1015",
            "name": "aditya2016Harnessing",
            "x": "7217.6",
            "y": "-479.371"
        },
        {
            "id": "K16-1015",
            "name": "18",
            "x": "7217.6",
            "y": "-464.371"
        },
        {
            "id": "D16-1104",
            "name": "aditya2016Are",
            "x": "6601.6",
            "y": "-479.371"
        },
        {
            "id": "D16-1104",
            "name": "27",
            "x": "6601.6",
            "y": "-464.371"
        },
        {
            "id": "L18-1424",
            "name": "aditya2018Sarcasm",
            "x": "7091.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1424",
            "name": "1",
            "x": "7091.6",
            "y": "-284.89"
        },
        {
            "id": "R15-1013",
            "name": "joe2015Coreference",
            "x": "14223.6",
            "y": "-569.111"
        },
        {
            "id": "R15-1013",
            "name": "2",
            "x": "14223.6",
            "y": "-554.111"
        },
        {
            "id": "P15-2100",
            "name": "aditya2015A",
            "x": "14392.6",
            "y": "-569.111"
        },
        {
            "id": "P15-2100",
            "name": "5",
            "x": "14392.6",
            "y": "-554.111"
        },
        {
            "id": "W16-2111",
            "name": "aditya2016How",
            "x": "6402.6",
            "y": "-479.371"
        },
        {
            "id": "W16-2111",
            "name": "???",
            "x": "6402.6",
            "y": "-464.371"
        },
        {
            "id": "U16-1013",
            "name": "aditya2016How",
            "x": "6761.6",
            "y": "-479.371"
        },
        {
            "id": "U16-1013",
            "name": "???",
            "x": "6761.6",
            "y": "-464.371"
        },
        {
            "id": "P16-1104",
            "name": "abhijit2016Harnessing",
            "x": "6060.6",
            "y": "-479.371"
        },
        {
            "id": "P16-1104",
            "name": "13",
            "x": "6060.6",
            "y": "-464.371"
        },
        {
            "id": "W17-7518",
            "name": "diptesh2017Is",
            "x": "6393.6",
            "y": "-389.631"
        },
        {
            "id": "W17-7518",
            "name": "0",
            "x": "6393.6",
            "y": "-374.631"
        },
        {
            "id": "W19-1309",
            "name": "abhijeet2019{``}When",
            "x": "6874.6",
            "y": "-210.15"
        },
        {
            "id": "W19-1309",
            "name": "1",
            "x": "6874.6",
            "y": "-195.15"
        },
        {
            "id": "2020.aacl-main.31",
            "name": "dushyant2020All-in-One:",
            "x": "5964.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.31",
            "name": "???",
            "x": "5964.6",
            "y": "-105.41"
        },
        {
            "id": "D16-1196",
            "name": "anoop2016Orthographic",
            "x": "858.597",
            "y": "-479.371"
        },
        {
            "id": "D16-1196",
            "name": "8",
            "x": "858.597",
            "y": "-464.371"
        },
        {
            "id": "W17-4102",
            "name": "anoop2017Learning",
            "x": "560.597",
            "y": "-389.631"
        },
        {
            "id": "W17-4102",
            "name": "7",
            "x": "560.597",
            "y": "-374.631"
        },
        {
            "id": "P18-2064",
            "name": "rudra2018Judicious",
            "x": "858.597",
            "y": "-299.89"
        },
        {
            "id": "P18-2064",
            "name": "3",
            "x": "858.597",
            "y": "-284.89"
        },
        {
            "id": "2021.emnlp-main.675",
            "name": "tejas2021Role",
            "x": "630.597",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.675",
            "name": "???",
            "x": "630.597",
            "y": "-15.6701"
        },
        {
            "id": "N15-1132",
            "name": "sudha2015Unsupervised",
            "x": "8842.6",
            "y": "-569.111"
        },
        {
            "id": "N15-1132",
            "name": "20",
            "x": "8842.6",
            "y": "-554.111"
        },
        {
            "id": "L18-1049",
            "name": "sabyasachi2018Sentence",
            "x": "8847.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1049",
            "name": "1",
            "x": "8847.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.34",
            "name": "kevin2018An",
            "x": "9038.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.34",
            "name": "???",
            "x": "9038.6",
            "y": "-284.89"
        },
        {
            "id": "D15-1017",
            "name": "jayanth2015Monotone",
            "x": "14570.6",
            "y": "-569.111"
        },
        {
            "id": "D15-1017",
            "name": "2",
            "x": "14570.6",
            "y": "-554.111"
        },
        {
            "id": "W18-3705",
            "name": "sandeep2018Thank",
            "x": "3328.6",
            "y": "-299.89"
        },
        {
            "id": "W18-3705",
            "name": "0",
            "x": "3328.6",
            "y": "-284.89"
        },
        {
            "id": "2017",
            "name": "2017",
            "x": "28.5975",
            "y": "-382.131"
        },
        {
            "id": "W16-6311",
            "name": "subham2016Improving",
            "x": "12865.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6311",
            "name": "0",
            "x": "12865.6",
            "y": "-464.371"
        },
        {
            "id": "W16-6325",
            "name": "shweta2016A",
            "x": "13226.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6325",
            "name": "7",
            "x": "13226.6",
            "y": "-464.371"
        },
        {
            "id": "W16-6331",
            "name": "deepak2016Opinion",
            "x": "4114.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6331",
            "name": "2",
            "x": "4114.6",
            "y": "-464.371"
        },
        {
            "id": "K18-1012",
            "name": "deepak2018Uncovering",
            "x": "4117.6",
            "y": "-299.89"
        },
        {
            "id": "K18-1012",
            "name": "4",
            "x": "4117.6",
            "y": "-284.89"
        },
        {
            "id": "2020.findings-emnlp.206",
            "name": "deepak2020A",
            "x": "3491.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.206",
            "name": "???",
            "x": "3491.6",
            "y": "-105.41"
        },
        {
            "id": "W16-6336",
            "name": "girishkumar2016On",
            "x": "7649.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6336",
            "name": "0",
            "x": "7649.6",
            "y": "-464.371"
        },
        {
            "id": "L18-1489",
            "name": "girishkumar2018Towards",
            "x": "7361.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1489",
            "name": "0",
            "x": "7361.6",
            "y": "-284.89"
        },
        {
            "id": "W16-6337",
            "name": "hanumant2016{V}erbframator:Semi-Automatic",
            "x": "13506.6",
            "y": "-479.371"
        },
        {
            "id": "W16-6337",
            "name": "0",
            "x": "13506.6",
            "y": "-464.371"
        },
        {
            "id": "W16-4811",
            "name": "anoop2016Faster",
            "x": "1064.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4811",
            "name": "1",
            "x": "1064.6",
            "y": "-464.371"
        },
        {
            "id": "W16-4604",
            "name": "sandhya2016{IIT}",
            "x": "13806.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4604",
            "name": "0",
            "x": "13806.6",
            "y": "-464.371"
        },
        {
            "id": "W16-4206",
            "name": "shweta2016Deep",
            "x": "13064.6",
            "y": "-479.371"
        },
        {
            "id": "W16-4206",
            "name": "14",
            "x": "13064.6",
            "y": "-464.371"
        },
        {
            "id": "W16-0415",
            "name": "aditya2016Political",
            "x": "6977.6",
            "y": "-479.371"
        },
        {
            "id": "W16-0415",
            "name": "9",
            "x": "6977.6",
            "y": "-464.371"
        },
        {
            "id": "N16-4006",
            "name": "pushpak2016Statistical",
            "x": "554.597",
            "y": "-479.371"
        },
        {
            "id": "N16-4006",
            "name": "4",
            "x": "554.597",
            "y": "-464.371"
        },
        {
            "id": "L16-1686",
            "name": "shehzaad2016{S}lang{N}et:",
            "x": "14033.6",
            "y": "-479.371"
        },
        {
            "id": "L16-1686",
            "name": "7",
            "x": "14033.6",
            "y": "-464.371"
        },
        {
            "id": "W17-2338",
            "name": "kevin2017Adapting",
            "x": "6566.6",
            "y": "-389.631"
        },
        {
            "id": "W17-2338",
            "name": "5",
            "x": "6566.6",
            "y": "-374.631"
        },
        {
            "id": "I17-2006",
            "name": "kevin2017Towards",
            "x": "6759.6",
            "y": "-389.631"
        },
        {
            "id": "I17-2006",
            "name": "1",
            "x": "6759.6",
            "y": "-374.631"
        },
        {
            "id": "C18-1155",
            "name": "girishkumar2018Treat",
            "x": "7588.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1155",
            "name": "0",
            "x": "7588.6",
            "y": "-284.89"
        },
        {
            "id": "W17-5229",
            "name": "md2017{IITP}",
            "x": "5220.6",
            "y": "-389.631"
        },
        {
            "id": "W17-5229",
            "name": "3",
            "x": "5220.6",
            "y": "-374.631"
        },
        {
            "id": "D17-1057",
            "name": "md2017A",
            "x": "4749.6",
            "y": "-389.631"
        },
        {
            "id": "D17-1057",
            "name": "11",
            "x": "4749.6",
            "y": "-374.631"
        },
        {
            "id": "2020.lrec-1.621",
            "name": "mamta2020Multi-domain",
            "x": "5217.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.621",
            "name": "???",
            "x": "5217.6",
            "y": "-105.41"
        },
        {
            "id": "2020.lrec-1.201",
            "name": "soumitra2020{CEASE},",
            "x": "4600.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.201",
            "name": "???",
            "x": "4600.6",
            "y": "-105.41"
        },
        {
            "id": "2020.icon-main.62",
            "name": "soumitra2020Annotated",
            "x": "4832.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.62",
            "name": "???",
            "x": "4832.6",
            "y": "-105.41"
        },
        {
            "id": "2016.gwc-1.4",
            "name": "harpreet2016Detecting",
            "x": "14274.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.4",
            "name": "???",
            "x": "14274.6",
            "y": "-464.371"
        },
        {
            "id": "2016.gwc-1.7",
            "name": "sudha2016{I}ndo{W}ord{N}et::{S}imilarity-",
            "x": "14584.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.7",
            "name": "???",
            "x": "14584.6",
            "y": "-464.371"
        },
        {
            "id": "2016.gwc-1.23",
            "name": "diptesh2016A",
            "x": "8471.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.23",
            "name": "???",
            "x": "8471.6",
            "y": "-464.371"
        },
        {
            "id": "2018.gwc-1.37",
            "name": "hanumant2018{H}indi",
            "x": "8202.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.37",
            "name": "???",
            "x": "8202.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.49",
            "name": "diptesh2018Synthesizing",
            "x": "8428.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.49",
            "name": "???",
            "x": "8428.6",
            "y": "-284.89"
        },
        {
            "id": "2016.gwc-1.37",
            "name": "apurva2016{I}ndo{W}ord{N}et",
            "x": "14936.6",
            "y": "-479.371"
        },
        {
            "id": "2016.gwc-1.37",
            "name": "???",
            "x": "14936.6",
            "y": "-464.371"
        },
        {
            "id": "2018",
            "name": "2018",
            "x": "28.5975",
            "y": "-292.39"
        },
        {
            "id": "W17-7517",
            "name": "tanik2017Document",
            "x": "12853.6",
            "y": "-389.631"
        },
        {
            "id": "W17-7517",
            "name": "1",
            "x": "12853.6",
            "y": "-374.631"
        },
        {
            "id": "W18-0522",
            "name": "nikhil2018The",
            "x": "6393.6",
            "y": "-299.89"
        },
        {
            "id": "W18-0522",
            "name": "1",
            "x": "6393.6",
            "y": "-284.89"
        },
        {
            "id": "W17-7531",
            "name": "hanumant2017{H}indi",
            "x": "7892.6",
            "y": "-389.631"
        },
        {
            "id": "W17-7531",
            "name": "0",
            "x": "7892.6",
            "y": "-374.631"
        },
        {
            "id": "W17-5904",
            "name": "hanumant2017{H}indi",
            "x": "13061.6",
            "y": "-389.631"
        },
        {
            "id": "W17-5904",
            "name": "0",
            "x": "13061.6",
            "y": "-374.631"
        },
        {
            "id": "W17-5717",
            "name": "sandhya2017Comparing",
            "x": "13285.6",
            "y": "-389.631"
        },
        {
            "id": "W17-5717",
            "name": "1",
            "x": "13285.6",
            "y": "-374.631"
        },
        {
            "id": "W17-2605",
            "name": "joe2017Towards",
            "x": "13488.6",
            "y": "-389.631"
        },
        {
            "id": "W17-2605",
            "name": "0",
            "x": "13488.6",
            "y": "-374.631"
        },
        {
            "id": "S17-2009",
            "name": "titas2017{IIT}-{UHH}",
            "x": "13686.6",
            "y": "-389.631"
        },
        {
            "id": "S17-2009",
            "name": "6",
            "x": "13686.6",
            "y": "-374.631"
        },
        {
            "id": "S17-2087",
            "name": "vikram2017{IITP}",
            "x": "13892.6",
            "y": "-389.631"
        },
        {
            "id": "S17-2087",
            "name": "10",
            "x": "13892.6",
            "y": "-374.631"
        },
        {
            "id": "S17-2153",
            "name": "abhishek2017{IITPB}",
            "x": "14095.6",
            "y": "-389.631"
        },
        {
            "id": "S17-2153",
            "name": "5",
            "x": "14095.6",
            "y": "-374.631"
        },
        {
            "id": "S17-2154",
            "name": "deepanway2017{IITP}",
            "x": "14316.6",
            "y": "-389.631"
        },
        {
            "id": "S17-2154",
            "name": "6",
            "x": "14316.6",
            "y": "-374.631"
        },
        {
            "id": "I17-4031",
            "name": "deepak2017{IITP}",
            "x": "14524.6",
            "y": "-389.631"
        },
        {
            "id": "I17-4031",
            "name": "0",
            "x": "14524.6",
            "y": "-374.631"
        },
        {
            "id": "E17-1077",
            "name": "sachin2017End-to-end",
            "x": "14729.6",
            "y": "-389.631"
        },
        {
            "id": "E17-1077",
            "name": "5",
            "x": "14729.6",
            "y": "-374.631"
        },
        {
            "id": "E17-1109",
            "name": "shweta2017Entity",
            "x": "4936.6",
            "y": "-389.631"
        },
        {
            "id": "E17-1109",
            "name": "7",
            "x": "4936.6",
            "y": "-374.631"
        },
        {
            "id": "D17-3002",
            "name": "pushpak2017Computational",
            "x": "14970.6",
            "y": "-389.631"
        },
        {
            "id": "D17-3002",
            "name": "???",
            "x": "14970.6",
            "y": "-374.631"
        },
        {
            "id": "2019",
            "name": "2019",
            "x": "28.5975",
            "y": "-202.65"
        },
        {
            "id": "Y18-3012",
            "name": "sukanta2018{IITP}-{MT}",
            "x": "12878.6",
            "y": "-299.89"
        },
        {
            "id": "Y18-3012",
            "name": "1",
            "x": "12878.6",
            "y": "-284.89"
        },
        {
            "id": "2021.wat-1.29",
            "name": "ramakrishna2021{IITP}-{MT}",
            "x": "12878.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.wat-1.29",
            "name": "???",
            "x": "12878.6",
            "y": "-15.6701"
        },
        {
            "id": "2020.bea-1.8",
            "name": "sandeep2020Can",
            "x": "3328.6",
            "y": "-120.41"
        },
        {
            "id": "2020.bea-1.8",
            "name": "???",
            "x": "3328.6",
            "y": "-105.41"
        },
        {
            "id": "P18-2011",
            "name": "sangameshwar2018Identification",
            "x": "13214.6",
            "y": "-299.89"
        },
        {
            "id": "P18-2011",
            "name": "0",
            "x": "13214.6",
            "y": "-284.89"
        },
        {
            "id": "W19-2404",
            "name": "girish2019Extraction",
            "x": "13075.6",
            "y": "-210.15"
        },
        {
            "id": "W19-2404",
            "name": "???",
            "x": "13075.6",
            "y": "-195.15"
        },
        {
            "id": "N19-2017",
            "name": "girish2019Extraction",
            "x": "13278.6",
            "y": "-210.15"
        },
        {
            "id": "N19-2017",
            "name": "0",
            "x": "13278.6",
            "y": "-195.15"
        },
        {
            "id": "2020.icon-main.23",
            "name": "sandeep2020Cognitively",
            "x": "6396.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.23",
            "name": "???",
            "x": "6396.6",
            "y": "-105.41"
        },
        {
            "id": "N18-2044",
            "name": "shweta2018Multi-Task",
            "x": "13472.6",
            "y": "-299.89"
        },
        {
            "id": "N18-2044",
            "name": "6",
            "x": "13472.6",
            "y": "-284.89"
        },
        {
            "id": "N18-1061",
            "name": "sabyasachi2018Fine-Grained",
            "x": "13716.6",
            "y": "-299.89"
        },
        {
            "id": "N18-1061",
            "name": "0",
            "x": "13716.6",
            "y": "-284.89"
        },
        {
            "id": "L18-1187",
            "name": "sandeep2018{ASAP}++:",
            "x": "13971.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1187",
            "name": "0",
            "x": "13971.6",
            "y": "-284.89"
        },
        {
            "id": "L18-1278",
            "name": "deepak2018A",
            "x": "3760.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1278",
            "name": "5",
            "x": "3760.6",
            "y": "-284.89"
        },
        {
            "id": "2020.aacl-main.90",
            "name": "deepak2020A",
            "x": "3984.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.90",
            "name": "???",
            "x": "3984.6",
            "y": "-105.41"
        },
        {
            "id": "L18-1440",
            "name": "deepak2018{MMQA}:",
            "x": "3574.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1440",
            "name": "6",
            "x": "3574.6",
            "y": "-284.89"
        },
        {
            "id": "2020.coling-main.249",
            "name": "deepak2020Reinforced",
            "x": "3718.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.249",
            "name": "0",
            "x": "3718.6",
            "y": "-105.41"
        },
        {
            "id": "L18-1442",
            "name": "shweta2018Medical",
            "x": "5550.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1442",
            "name": "5",
            "x": "5550.6",
            "y": "-284.89"
        },
        {
            "id": "2020.findings-emnlp.386",
            "name": "girishkumar2020Looking",
            "x": "7475.6",
            "y": "-120.41"
        },
        {
            "id": "2020.findings-emnlp.386",
            "name": "???",
            "x": "7475.6",
            "y": "-105.41"
        },
        {
            "id": "2021.findings-acl.256",
            "name": "girishkumar2021{F}rame{N}et-assisted",
            "x": "7475.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.findings-acl.256",
            "name": "???",
            "x": "7475.6",
            "y": "-15.6701"
        },
        {
            "id": "W19-5426",
            "name": "jyotsana2019Utilizing",
            "x": "3060.6",
            "y": "-210.15"
        },
        {
            "id": "W19-5426",
            "name": "0",
            "x": "3060.6",
            "y": "-195.15"
        },
        {
            "id": "2021.mtsummit-research.2",
            "name": "kamal2021Investigating",
            "x": "3183.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.mtsummit-research.2",
            "name": "???",
            "x": "3183.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.calcs-1.5",
            "name": "ramakrishna2021{IITP}-{MT}",
            "x": "3473.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.calcs-1.5",
            "name": "???",
            "x": "3473.6",
            "y": "-15.6701"
        },
        {
            "id": "L18-1559",
            "name": "tirthankar2018{TAP}-{DLND}",
            "x": "14236.6",
            "y": "-299.89"
        },
        {
            "id": "L18-1559",
            "name": "2",
            "x": "14236.6",
            "y": "-284.89"
        },
        {
            "id": "C18-1237",
            "name": "tirthankar2018Novelty",
            "x": "14487.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1237",
            "name": "1",
            "x": "14487.6",
            "y": "-284.89"
        },
        {
            "id": "2021.ltedi-1.29",
            "name": "pankaj2021{CFILT}",
            "x": "4117.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.ltedi-1.29",
            "name": "???",
            "x": "4117.6",
            "y": "-15.6701"
        },
        {
            "id": "D18-1382",
            "name": "deepanway2018Contextual",
            "x": "5774.6",
            "y": "-299.89"
        },
        {
            "id": "D18-1382",
            "name": "7",
            "x": "5774.6",
            "y": "-284.89"
        },
        {
            "id": "N19-1034",
            "name": "md2019Multi-task",
            "x": "5528.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1034",
            "name": "4",
            "x": "5528.6",
            "y": "-195.15"
        },
        {
            "id": "D19-1566",
            "name": "dushyant2019Context-aware",
            "x": "5752.6",
            "y": "-210.15"
        },
        {
            "id": "D19-1566",
            "name": "2",
            "x": "5752.6",
            "y": "-195.15"
        },
        {
            "id": "2020.coling-main.393",
            "name": "mauajama2020{MEISD}:",
            "x": "5496.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.393",
            "name": "???",
            "x": "5496.6",
            "y": "-105.41"
        },
        {
            "id": "2020.acl-main.401",
            "name": "dushyant2020Sentiment",
            "x": "5731.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.401",
            "name": "???",
            "x": "5731.6",
            "y": "-105.41"
        },
        {
            "id": "C18-1042",
            "name": "deepak2018Can",
            "x": "3919.6",
            "y": "-299.89"
        },
        {
            "id": "C18-1042",
            "name": "3",
            "x": "3919.6",
            "y": "-284.89"
        },
        {
            "id": "2018.gwc-1.47",
            "name": "ritesh2018pyiwn:",
            "x": "14681.6",
            "y": "-299.89"
        },
        {
            "id": "2018.gwc-1.47",
            "name": "???",
            "x": "14681.6",
            "y": "-284.89"
        },
        {
            "id": "2020.nuse-1.11",
            "name": "swapnil2020Extracting",
            "x": "14681.6",
            "y": "-120.41"
        },
        {
            "id": "2020.nuse-1.11",
            "name": "???",
            "x": "14681.6",
            "y": "-105.41"
        },
        {
            "id": "2020",
            "name": "2020",
            "x": "28.5975",
            "y": "-112.91"
        },
        {
            "id": "W19-7511",
            "name": "diptesh2019Utilizing",
            "x": "14800.6",
            "y": "-210.15"
        },
        {
            "id": "W19-7511",
            "name": "???",
            "x": "14800.6",
            "y": "-195.15"
        },
        {
            "id": "W19-5440",
            "name": "sukanta2019Parallel",
            "x": "14998.6",
            "y": "-210.15"
        },
        {
            "id": "W19-5440",
            "name": "0",
            "x": "14998.6",
            "y": "-195.15"
        },
        {
            "id": "W19-5346",
            "name": "sukanta2019{IITP}-{MT}",
            "x": "15220.6",
            "y": "-210.15"
        },
        {
            "id": "W19-5346",
            "name": "???",
            "x": "15220.6",
            "y": "-195.15"
        },
        {
            "id": "P19-1106",
            "name": "tirthankar2019{D}eep{S}enti{P}eer:",
            "x": "15510.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1106",
            "name": "0",
            "x": "15510.6",
            "y": "-195.15"
        },
        {
            "id": "P19-1297",
            "name": "sukanta2019Multilingual",
            "x": "15790.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1297",
            "name": "0",
            "x": "15790.6",
            "y": "-195.15"
        },
        {
            "id": "P19-1516",
            "name": "shweta2019A",
            "x": "15978.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1516",
            "name": "1",
            "x": "15978.6",
            "y": "-195.15"
        },
        {
            "id": "P19-1540",
            "name": "hardik2019Ordinal",
            "x": "16145.6",
            "y": "-210.15"
        },
        {
            "id": "P19-1540",
            "name": "0",
            "x": "16145.6",
            "y": "-195.15"
        },
        {
            "id": "N19-1091",
            "name": "hitesh2019Courteously",
            "x": "16349.6",
            "y": "-210.15"
        },
        {
            "id": "N19-1091",
            "name": "0",
            "x": "16349.6",
            "y": "-195.15"
        },
        {
            "id": "2020.lrec-1.514",
            "name": "mauajama2020Incorporating",
            "x": "16349.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.514",
            "name": "???",
            "x": "16349.6",
            "y": "-105.41"
        },
        {
            "id": "2019.icon-1.2",
            "name": "arjun2019A",
            "x": "16525.6",
            "y": "-210.15"
        },
        {
            "id": "2019.icon-1.2",
            "name": "???",
            "x": "16525.6",
            "y": "-195.15"
        },
        {
            "id": "2019.icon-1.16",
            "name": "zishan2019Multi-linguality",
            "x": "16713.6",
            "y": "-210.15"
        },
        {
            "id": "2019.icon-1.16",
            "name": "???",
            "x": "16713.6",
            "y": "-195.15"
        },
        {
            "id": "2019.icon-1.19",
            "name": "sovan2019A",
            "x": "16905.6",
            "y": "-210.15"
        },
        {
            "id": "2019.icon-1.19",
            "name": "???",
            "x": "16905.6",
            "y": "-195.15"
        },
        {
            "id": "2019.icon-1.20",
            "name": "manasi2019Converting",
            "x": "17085.6",
            "y": "-210.15"
        },
        {
            "id": "2019.icon-1.20",
            "name": "???",
            "x": "17085.6",
            "y": "-195.15"
        },
        {
            "id": "2019.icon-1.27",
            "name": "tanik2019A",
            "x": "17261.6",
            "y": "-210.15"
        },
        {
            "id": "2019.icon-1.27",
            "name": "???",
            "x": "17261.6",
            "y": "-195.15"
        },
        {
            "id": "2019.gwc-1.51",
            "name": "diptesh2019Utilizing",
            "x": "17427.6",
            "y": "-210.15"
        },
        {
            "id": "2019.gwc-1.51",
            "name": "???",
            "x": "17427.6",
            "y": "-195.15"
        },
        {
            "id": "2020.lrec-1.378",
            "name": "diptesh2020Challenge",
            "x": "17317.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.378",
            "name": "???",
            "x": "17317.6",
            "y": "-105.41"
        },
        {
            "id": "2020.coling-main.119",
            "name": "diptesh2020Harnessing",
            "x": "17536.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.119",
            "name": "???",
            "x": "17536.6",
            "y": "-105.41"
        },
        {
            "id": "2021",
            "name": "2021",
            "x": "28.5975",
            "y": "-23.1701"
        },
        {
            "id": "2020.wildre-1.1",
            "name": "gajanan2020Part-of-Speech",
            "x": "17777.6",
            "y": "-120.41"
        },
        {
            "id": "2020.wildre-1.1",
            "name": "???",
            "x": "17777.6",
            "y": "-105.41"
        },
        {
            "id": "2020.sltu-1.49",
            "name": "saurav2020{``}A",
            "x": "577.597",
            "y": "-120.41"
        },
        {
            "id": "2020.sltu-1.49",
            "name": "???",
            "x": "577.597",
            "y": "-105.41"
        },
        {
            "id": "2020.semeval-1.214",
            "name": "chandresh2020{EL}-{BERT}",
            "x": "18046.6",
            "y": "-120.41"
        },
        {
            "id": "2020.semeval-1.214",
            "name": "???",
            "x": "18046.6",
            "y": "-105.41"
        },
        {
            "id": "2020.semeval-1.261",
            "name": "soumitra2020{IITP}-{AINLPML}",
            "x": "18343.6",
            "y": "-120.41"
        },
        {
            "id": "2020.semeval-1.261",
            "name": "???",
            "x": "18343.6",
            "y": "-105.41"
        },
        {
            "id": "2020.sdp-1.27",
            "name": "saichethan2020{IIITBH}-{IITP}@{CL}-{S}ci{S}umm20,",
            "x": "18755.6",
            "y": "-120.41"
        },
        {
            "id": "2020.sdp-1.27",
            "name": "???",
            "x": "18755.6",
            "y": "-105.41"
        },
        {
            "id": "2020.sdp-1.30",
            "name": "santosh2020{IITP}-{AI}-{NLP}-{ML}@",
            "x": "19198.6",
            "y": "-120.41"
        },
        {
            "id": "2020.sdp-1.30",
            "name": "???",
            "x": "19198.6",
            "y": "-105.41"
        },
        {
            "id": "2020.lrec-1.273",
            "name": "sovan2020A",
            "x": "19456.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.273",
            "name": "???",
            "x": "19456.6",
            "y": "-105.41"
        },
        {
            "id": "2020.lrec-1.675",
            "name": "tanik2020{S}cholarly{R}ead:",
            "x": "19664.6",
            "y": "-120.41"
        },
        {
            "id": "2020.lrec-1.675",
            "name": "???",
            "x": "19664.6",
            "y": "-105.41"
        },
        {
            "id": "2020.iwslt-1.22",
            "name": "nikhil2020Generating",
            "x": "19905.6",
            "y": "-120.41"
        },
        {
            "id": "2020.iwslt-1.22",
            "name": "???",
            "x": "19905.6",
            "y": "-105.41"
        },
        {
            "id": "2021.eacl-main.299",
            "name": "nikhil2021Disfluency",
            "x": "19905.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.eacl-main.299",
            "name": "???",
            "x": "19905.6",
            "y": "-15.6701"
        },
        {
            "id": "2020.icon-main.25",
            "name": "anubhav2020Semantic",
            "x": "20117.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.25",
            "name": "???",
            "x": "20117.6",
            "y": "-105.41"
        },
        {
            "id": "2020.icon-main.42",
            "name": "chanchal2020A",
            "x": "20307.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.42",
            "name": "???",
            "x": "20307.6",
            "y": "-105.41"
        },
        {
            "id": "2020.icon-main.43",
            "name": "chanchal2020{D}-Coref:",
            "x": "20507.6",
            "y": "-120.41"
        },
        {
            "id": "2020.icon-main.43",
            "name": "???",
            "x": "20507.6",
            "y": "-105.41"
        },
        {
            "id": "2020.fnp-1.22",
            "name": "amit2020Knowledge",
            "x": "20727.6",
            "y": "-120.41"
        },
        {
            "id": "2020.fnp-1.22",
            "name": "???",
            "x": "20727.6",
            "y": "-105.41"
        },
        {
            "id": "2020.eamt-1.21",
            "name": "kamal2020Modelling",
            "x": "20930.6",
            "y": "-120.41"
        },
        {
            "id": "2020.eamt-1.21",
            "name": "???",
            "x": "20930.6",
            "y": "-105.41"
        },
        {
            "id": "2020.coling-main.111",
            "name": "sapan2020A",
            "x": "21101.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.111",
            "name": "???",
            "x": "21101.6",
            "y": "-105.41"
        },
        {
            "id": "2020.coling-main.383",
            "name": "jyotsana2020Filtering",
            "x": "21275.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.383",
            "name": "???",
            "x": "21275.6",
            "y": "-105.41"
        },
        {
            "id": "2020.coling-main.534",
            "name": "kumar2020Analysing",
            "x": "21482.6",
            "y": "-120.41"
        },
        {
            "id": "2020.coling-main.534",
            "name": "???",
            "x": "21482.6",
            "y": "-105.41"
        },
        {
            "id": "2020.acl-main.402",
            "name": "tulika2020Towards",
            "x": "21681.6",
            "y": "-120.41"
        },
        {
            "id": "2020.acl-main.402",
            "name": "???",
            "x": "21681.6",
            "y": "-105.41"
        },
        {
            "id": "2021.naacl-main.456",
            "name": "tulika2021Towards",
            "x": "21681.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.naacl-main.456",
            "name": "???",
            "x": "21681.6",
            "y": "-15.6701"
        },
        {
            "id": "2020.aacl-main.33",
            "name": "mukuntha2020Unsupervised",
            "x": "21909.6",
            "y": "-120.41"
        },
        {
            "id": "2020.aacl-main.33",
            "name": "???",
            "x": "21909.6",
            "y": "-105.41"
        },
        {
            "id": "2021.wat-1.26",
            "name": "jyotsana2021Language",
            "x": "21887.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.wat-1.26",
            "name": "???",
            "x": "21887.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.wat-1.28",
            "name": "shivam2021Multilingual",
            "x": "22109.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.wat-1.28",
            "name": "???",
            "x": "22109.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.smm4h-1.15",
            "name": "tanay2021{BERT}",
            "x": "22317.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.smm4h-1.15",
            "name": "???",
            "x": "22317.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.mtsummit-loresmt.17",
            "name": "aditya2021Evaluating",
            "x": "22516.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.mtsummit-loresmt.17",
            "name": "???",
            "x": "22516.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.inlg-1.39",
            "name": "mauajama2021{SEPRG}:",
            "x": "22742.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.inlg-1.39",
            "name": "???",
            "x": "22742.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.emnlp-main.789",
            "name": "anirudh2021{``}So",
            "x": "22960.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.emnlp-main.789",
            "name": "???",
            "x": "22960.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.eacl-main.255",
            "name": "deeksha2021Modelling",
            "x": "23167.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.eacl-main.255",
            "name": "???",
            "x": "23167.6",
            "y": "-15.6701"
        },
        {
            "id": "2021.eacl-main.288",
            "name": "diptesh2021Cognition-aware",
            "x": "23411.6",
            "y": "-30.6701"
        },
        {
            "id": "2021.eacl-main.288",
            "name": "???",
            "x": "23411.6",
            "y": "-15.6701"
        }
    ],
    [
        "28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45 28.5975,-1570.45",
        "28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71 28.5975,-1480.71",
        "28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97 28.5975,-1390.97",
        "28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23 28.5975,-1301.23",
        "2354.34,-1200.23 2344.11,-1202.97 2353.82,-1207.21 2354.34,-1200.23",
        "6409.81,-1034.26 6418.99,-1028.98 6408.52,-1027.38 6409.81,-1034.26",
        "28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49 28.5975,-1211.49",
        "28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75 28.5975,-1121.75",
        "7216.71,-1196.99 7226.71,-1193.49 7216.71,-1189.99 7216.71,-1196.99",
        "2191.05,-1136.05 2182.07,-1130.42 2185.57,-1140.42 2191.05,-1136.05",
        "923.375,-871.346 919.546,-861.467 916.379,-871.578 923.375,-871.346",
        "2069.92,-692.026 2071.96,-681.631 2063.88,-688.482 2069.92,-692.026",
        "3611.58,-688.726 3617.2,-679.74 3607.2,-683.265 3611.58,-688.726",
        "1057.01,-692.547 1055.89,-682.011 1050.2,-690.945 1057.01,-692.547",
        "641.079,-686.696 631.835,-681.518 635.833,-691.33 641.079,-686.696",
        "3356.98,-594.084 3364.85,-586.986 3354.27,-587.629 3356.98,-594.084",
        "2206.51,-492.526 2195.92,-492.32 2204.07,-499.087 2206.51,-492.526",
        "2330.49,-511.914 2325.39,-502.63 2323.59,-513.07 2330.49,-511.914",
        "3895.23,-512.312 3890.55,-502.808 3888.28,-513.157 3895.23,-512.312",
        "1150.85,-391.047 1140.57,-393.598 1150.2,-398.016 1150.85,-391.047",
        "3177.61,-328.404 3169.03,-322.19 3171.87,-332.399 3177.61,-328.404",
        "1526.33,-215.785 1535.94,-211.33 1525.65,-208.818 1526.33,-215.785",
        "9443.1,-512.733 9439.6,-502.733 9436.1,-512.733 9443.1,-512.733",
        "9725.02,-1135.55 9716.34,-1129.47 9719.33,-1139.63 9725.02,-1135.55",
        "9580.51,-1049 9573.52,-1041.03 9574.01,-1051.62 9580.51,-1049",
        "9808.97,-1051.44 9808.59,-1040.85 9802.29,-1049.37 9808.97,-1051.44",
        "9858.88,-773.56 9848.97,-769.797 9854.37,-778.915 9858.88,-773.56",
        "28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01 28.5975,-1032.01",
        "9611.39,-1041.47 9601.11,-1038.92 9607.56,-1047.32 9611.39,-1041.47",
        "9777.73,-1048.27 9783.67,-1039.5 9773.55,-1042.66 9777.73,-1048.27",
        "9799.77,-781.955 9801.59,-771.519 9793.66,-778.541 9799.77,-781.955",
        "2169.08,-597.876 2160.49,-591.678 2163.34,-601.882 2169.08,-597.876",
        "2198.22,-496.321 2187.69,-495.175 2195.2,-502.639 2198.22,-496.321",
        "4044.08,-1047.12 4035.92,-1040.37 4038.09,-1050.74 4044.08,-1047.12",
        "4092.48,-955.242 4083.26,-950.007 4087.2,-959.843 4092.48,-955.242",
        "3895.03,-960.1 3889.02,-951.372 3888.27,-961.941 3895.03,-960.1",
        "4347.84,-513.216 4348.02,-502.623 4341.27,-510.788 4347.84,-513.216",
        "28.5975,-942.271 28.5975,-942.271 28.5975,-942.271 28.5975,-942.271",
        "10089.6,-864.008 10079.8,-859.94 10084.9,-869.221 10089.6,-864.008",
        "10254.1,-871.537 10250.6,-861.537 10247.1,-871.537 10254.1,-871.537",
        "10382.3,-691.81 10378.4,-681.953 10375.3,-692.081 10382.3,-691.81",
        "10488.1,-961.275 10484.6,-951.275 10481.1,-961.275 10488.1,-961.275",
        "1308.3,-1017.51 1318.3,-1014.01 1308.3,-1010.51 1308.3,-1017.51",
        "1149.06,-601.381 1143.59,-592.312 1142.21,-602.817 1149.06,-601.381",
        "3378.68,-779.901 3383.83,-770.64 3374.03,-774.669 3378.68,-779.901",
        "2576.01,-690.249 2569.48,-681.908 2569.38,-692.502 2576.01,-690.249",
        "3559.2,-590.139 3567.86,-584.037 3557.29,-583.406 3559.2,-590.139",
        "2430.66,-602.089 2432.98,-591.753 2424.72,-598.383 2430.66,-602.089",
        "2570.24,-502.748 2560.01,-499.982 2566.28,-508.522 2570.24,-502.748",
        "2712.77,-512.762 2709.36,-502.733 2705.77,-512.703 2712.77,-512.762",
        "1840.25,-691.166 1835.09,-681.913 1833.35,-692.364 1840.25,-691.166",
        "1888.55,-502.763 1878.29,-500.15 1884.68,-508.596 1888.55,-502.763",
        "1809.21,-314.666 1798.62,-314.904 1807.05,-321.323 1809.21,-314.666",
        "4396.64,-855.2 4386.04,-855.212 4394.33,-861.81 4396.64,-855.2",
        "4967.34,-870.916 4971.28,-861.081 4962.07,-866.311 4967.34,-870.916",
        "4314.72,-332.886 4317,-322.54 4308.76,-329.205 4314.72,-332.886",
        "4572.84,-331.601 4566.41,-323.174 4566.18,-333.766 4572.84,-331.601",
        "7948.77,-229.874 7957.46,-223.812 7946.89,-223.132 7948.77,-229.874",
        "7875.1,-602.473 7871.6,-592.473 7868.1,-602.473 7875.1,-602.473",
        "4044.78,-961.825 4045.24,-951.24 4038.28,-959.231 4044.78,-961.825",
        "3927.78,-949.409 3917.47,-946.963 3924,-955.304 3927.78,-949.409",
        "4459.58,-862.138 4467.85,-855.515 4457.25,-855.535 4459.58,-862.138",
        "28.5975,-852.531 28.5975,-852.531 28.5975,-852.531 28.5975,-852.531",
        "5896.65,-871.156 5900.15,-861.156 5891.18,-866.79 5896.65,-871.156",
        "5803.88,-782.39 5803.18,-771.818 5797.14,-780.519 5803.88,-782.39",
        "3949.58,-496.8 3939.01,-496.082 3946.83,-503.235 3949.58,-496.8",
        "3455.14,-772.788 3444.89,-770.096 3451.23,-778.59 3455.14,-772.788",
        "5519.16,-776.871 5510.53,-770.727 5513.45,-780.913 5519.16,-776.871",
        "3560.2,-590.836 3568.51,-584.272 3557.92,-584.217 3560.2,-590.836",
        "4600.31,-852.018 4589.78,-853.155 4598.72,-858.835 4600.31,-852.018",
        "5056.07,-860.087 5045.52,-859.134 5053.17,-866.459 5056.07,-860.087",
        "5812.25,-852.008 5821.76,-847.346 5811.42,-845.057 5812.25,-852.008",
        "5777.2,-780.439 5782.8,-771.445 5772.81,-774.984 5777.2,-780.439",
        "4444.95,-762.043 4434.38,-762.767 4443.1,-768.792 4444.95,-762.043",
        "5693.38,-683.33 5700.89,-675.852 5690.36,-677.017 5693.38,-683.33",
        "2931.1,-602.902 2930,-592.364 2924.29,-601.285 2931.1,-602.902",
        "5308.93,-510.266 5314.01,-500.966 5304.24,-505.071 5308.93,-510.266",
        "4931.35,-505.008 4921.71,-500.618 4926.51,-510.063 4931.35,-505.008",
        "5595.99,-511.399 5590.12,-502.578 5589.21,-513.133 5595.99,-511.399",
        "5561.12,-401.645 5570.59,-396.908 5560.23,-394.701 5561.12,-401.645",
        "28.5975,-762.791 28.5975,-762.791 28.5975,-762.791 28.5975,-762.791",
        "6142.1,-781.795 6138.6,-771.795 6135.1,-781.795 6142.1,-781.795",
        "3725.1,-781.795 3721.6,-771.795 3718.1,-781.795 3725.1,-781.795",
        "3614.59,-689.58 3619.39,-680.135 3609.75,-684.526 3614.59,-689.58",
        "3932.51,-502.489 3922.29,-499.678 3928.52,-508.245 3932.51,-502.489",
        "11323.1,-692.057 11319.6,-682.057 11316.1,-692.057 11323.1,-692.057",
        "4382.69,-323.483 4372.48,-320.63 4378.68,-329.223 4382.69,-323.483",
        "4562.22,-332.976 4558.28,-323.139 4555.22,-333.284 4562.22,-332.976",
        "5339.18,-690.055 5332.58,-681.769 5332.58,-692.364 5339.18,-690.055",
        "4457.91,-753.499 4447.53,-755.609 4456.96,-760.434 4457.91,-753.499",
        "4922.29,-507.948 4913.83,-501.565 4916.46,-511.828 4922.29,-507.948",
        "3956.72,-492.955 3946.16,-493.832 3954.96,-499.731 3956.72,-492.955",
        "5033.96,-512.767 5031.63,-502.433 5027.01,-511.968 5033.96,-512.767",
        "4743.4,-507.733 4734.48,-502.01 4737.88,-512.044 4743.4,-507.733",
        "5355.4,-329.785 5347.71,-322.497 5349.18,-332.99 5355.4,-329.785",
        "5326.03,-224.889 5315.46,-225.664 5324.21,-231.648 5326.03,-224.889",
        "5854.05,-774.961 5844.08,-771.372 5849.64,-780.394 5854.05,-774.961",
        "5840.85,-672.284 5830.3,-673.23 5839.14,-679.071 5840.85,-672.284",
        "28.5975,-673.051 28.5975,-673.051 28.5975,-673.051 28.5975,-673.051",
        "11573,-688.33 11564.7,-681.716 11567,-692.048 11573,-688.33",
        "11631.6,-598.506 11623.5,-591.743 11625.6,-602.116 11631.6,-598.506",
        "3214.27,-313.976 3203.68,-314.319 3212.17,-320.655 3214.27,-313.976",
        "6080.22,-685.108 6070.5,-680.894 6075.47,-690.25 6080.22,-685.108",
        "5618.79,-505.499 5608.99,-501.467 5614.13,-510.729 5618.79,-505.499",
        "6202.09,-331.336 6195.7,-322.882 6195.42,-333.473 6202.09,-331.336",
        "6142.02,-150.291 6148.09,-141.606 6137.93,-144.612 6142.02,-150.291",
        "3589.67,-682.588 3597.88,-675.889 3587.29,-676.008 3589.67,-682.588",
        "3576.85,-596.541 3583.62,-588.394 3573.24,-590.539 3576.85,-596.541",
        "3851,-509.58 3856.8,-500.714 3846.74,-504.028 3851,-509.58",
        "6119.39,-674.071 6108.81,-674.688 6117.46,-680.802 6119.39,-674.071",
        "6520.06,-600.281 6525.28,-591.064 6515.45,-595.012 6520.06,-600.281",
        "5672.38,-489.691 5661.89,-491.172 5671.02,-496.556 5672.38,-489.691",
        "3742.47,-663.432 3732.12,-665.702 3741.63,-670.381 3742.47,-663.432",
        "5667.26,-672.068 5676.74,-667.351 5666.39,-665.123 5667.26,-672.068",
        "3258.68,-579.881 3248.21,-581.527 3257.42,-586.767 3258.68,-579.881",
        "3957.33,-492.693 3946.77,-493.514 3955.54,-499.459 3957.33,-492.693",
        "5501.91,-499.471 5510.82,-493.749 5500.28,-492.661 5501.91,-499.471",
        "5483.57,-691.23 5478.33,-682.02 5476.68,-692.485 5483.57,-691.23",
        "5374.28,-678.883 5363.81,-677.304 5371.01,-685.072 5374.28,-678.883",
        "5951.79,-673.296 5961.16,-668.348 5950.75,-666.374 5951.79,-673.296",
        "5820.49,-509.135 5826.54,-500.443 5816.39,-503.46 5820.49,-509.135",
        "5584.93,-512.748 5581.98,-502.573 5577.94,-512.368 5584.93,-512.748",
        "2458.55,-601.684 2453.73,-592.247 2451.61,-602.627 2458.55,-601.684",
        "2966.24,-597.679 2957.44,-591.78 2960.64,-601.88 2966.24,-597.679",
        "3153.36,-602.214 3156.1,-591.98 3147.57,-598.271 3153.36,-602.214",
        "5496.74,-497.265 5505.95,-492.021 5495.48,-490.379 5496.74,-497.265",
        "2721.54,-511.57 2715.78,-502.677 2714.74,-513.22 2721.54,-511.57",
        "5555.41,-396.016 5565.11,-391.753 5554.87,-389.037 5555.41,-396.016",
        "3059.69,-423.214 3056.3,-413.175 3052.69,-423.135 3059.69,-423.214",
        "2925.99,-333.535 2927.54,-323.053 2919.79,-330.284 2925.99,-333.535",
        "2800,-152.504 2794.3,-143.57 2793.19,-154.106 2800,-152.504",
        "3902.92,-510.553 3896.16,-502.392 3896.36,-512.985 3902.92,-510.553",
        "28.5975,-583.311 28.5975,-583.311 28.5975,-583.311 28.5975,-583.311",
        "1737.55,-208.818 1727.26,-211.33 1736.87,-215.785 1737.55,-208.818",
        "8256.74,-507.559 8247.58,-502.24 8251.43,-512.112 8256.74,-507.559",
        "8149.86,-210.415 8139.65,-213.254 8149.4,-217.4 8149.86,-210.415",
        "965.514,-592.82 955.121,-590.762 961.962,-598.852 965.514,-592.82",
        "5564.51,-512.712 5566.76,-502.359 5558.54,-509.049 5564.51,-512.712",
        "11585.8,-602.248 11588.2,-591.936 11579.9,-598.482 11585.8,-602.248",
        "676.842,-678.567 666.248,-678.671 674.595,-685.196 676.842,-678.567",
        "9017.49,-591.899 9026.55,-586.406 9016.04,-585.05 9017.49,-591.899",
        "8710.1,-512.577 8706.6,-502.577 8703.1,-512.577 8710.1,-512.577",
        "3211.67,-314.693 3201.1,-315.299 3209.74,-321.421 3211.67,-314.693",
        "9306.1,-64.0324 9302.6,-54.0324 9299.1,-64.0325 9306.1,-64.0324",
        "8041.02,-243.377 8037.54,-233.369 8034.02,-243.361 8041.02,-243.377",
        "12738.1,-243.513 12734.6,-233.513 12731.1,-243.513 12738.1,-243.513",
        "5916.93,-499.795 5906.48,-498.069 5913.58,-505.937 5916.93,-499.795",
        "5647.1,-498.566 5636.51,-498.084 5644.49,-505.061 5647.1,-498.566",
        "5681.43,-417.263 5672.24,-411.986 5676.13,-421.84 5681.43,-417.263",
        "6192.97,-332.993 6188.93,-323.201 6185.98,-333.378 6192.97,-332.993",
        "736.215,-658.551 746.215,-655.051 736.215,-651.551 736.215,-658.551",
        "1018.31,-686.116 1025.54,-678.368 1015.06,-679.918 1018.31,-686.116",
        "832.533,-593.119 841.186,-587.006 830.609,-586.389 832.533,-593.119",
        "625.417,-602.862 624.902,-592.28 618.705,-600.873 625.417,-602.862",
        "1069.58,-590.137 1078.39,-584.252 1067.83,-583.358 1069.58,-590.137",
        "289.787,-508.077 281.001,-502.156 284.177,-512.264 289.787,-508.077",
        "486.125,-331.826 490.286,-322.083 480.957,-327.105 486.125,-331.826",
        "176.599,-333.494 177.697,-322.956 170.267,-330.509 176.599,-333.494",
        "1532.15,-221.006 1541.72,-216.445 1531.4,-214.047 1532.15,-221.006",
        "28.5975,-493.571 28.5975,-493.571 28.5975,-493.571 28.5975,-493.571",
        "2944.55,-333.356 2941.21,-323.305 2937.56,-333.252 2944.55,-333.356",
        "1126.28,-405.48 1115.69,-405.762 1124.15,-412.146 1126.28,-405.48",
        "1712.97,-333.356 1714,-322.811 1706.62,-330.414 1712.97,-333.356",
        "7363.01,-504.966 7370.79,-497.772 7360.22,-498.545 7363.01,-504.966",
        "7221.1,-512.575 7217.6,-502.575 7214.1,-512.575 7221.1,-512.575",
        "6661.04,-494.264 6650.46,-494.692 6659,-500.959 6661.04,-494.264",
        "7172.99,-311.019 7162.46,-312.192 7171.43,-317.841 7172.99,-311.019",
        "7125.46,-568.811 7135.46,-565.311 7125.46,-561.811 7125.46,-568.811",
        "7350.3,-500.634 7359.16,-494.829 7348.61,-493.84 7350.3,-500.634",
        "6450.98,-500.636 6440.58,-498.639 6447.46,-506.689 6450.98,-500.636",
        "6706.95,-503.423 6714.8,-496.311 6704.23,-496.973 6706.95,-503.423",
        "6151.33,-489.163 6140.87,-490.857 6150.1,-496.055 6151.33,-489.163",
        "7138.02,-499.716 7147.05,-494.168 7136.53,-492.876 7138.02,-499.716",
        "5676.21,-487.492 5665.81,-489.523 5675.21,-494.42 5676.21,-487.492",
        "6585.99,-512.387 6587.72,-501.934 6579.85,-509.027 6585.99,-512.387",
        "6443.1,-410.141 6432.77,-407.794 6439.39,-416.071 6443.1,-410.141",
        "5749.6,-386.225 5739.44,-389.25 5749.27,-393.217 5749.6,-386.225",
        "7103.75,-332.087 7098.31,-323.001 7096.9,-333.501 7103.75,-332.087",
        "6886.56,-242.558 6881.09,-233.486 6879.71,-243.99 6886.56,-242.558",
        "6056.13,-134.209 6045.55,-134.872 6054.23,-140.948 6056.13,-134.209",
        "1781.03,-592.776 1789.13,-585.952 1778.54,-586.232 1781.03,-592.776",
        "1705.13,-488.688 1714.87,-484.512 1704.65,-481.704 1705.13,-488.688",
        "793.75,-505.501 801.805,-498.619 791.216,-498.976 793.75,-505.501",
        "490.474,-412.63 497.956,-405.128 487.429,-406.327 490.474,-412.63",
        "1670.42,-326.736 1678.16,-319.501 1667.6,-320.33 1670.42,-326.736",
        "835.494,-332.716 838.841,-322.664 829.956,-328.435 835.494,-332.716",
        "655.597,-59.9761 647.542,-53.0929 649.546,-63.4965 655.597,-59.9761",
        "1266.15,-568.811 1276.15,-565.311 1266.15,-561.811 1266.15,-568.811",
        "1119.16,-409.645 1108.78,-407.524 1115.58,-415.655 1119.16,-409.645",
        "3961.18,-490.56 3950.69,-492.01 3959.8,-497.421 3961.18,-490.56",
        "8850.42,-333.369 8847.11,-323.305 8843.42,-333.238 8850.42,-333.369",
        "9017.34,-331.991 9020.43,-321.857 9011.69,-327.851 9017.34,-331.991",
        "3081.87,-418.804 3073.44,-412.381 3076.02,-422.656 3081.87,-418.804",
        "3311.79,-333.041 3313.78,-322.635 3305.74,-329.529 3311.79,-333.041",
        "3026.82,-312.609 3016.29,-313.713 3025.21,-319.421 3026.82,-312.609",
        "28.5975,-403.831 28.5975,-403.831 28.5975,-403.831 28.5975,-403.831",
        "4120.49,-333.154 4117.16,-323.097 4113.49,-333.036 4120.49,-333.154",
        "3475.56,-153.355 3477.01,-142.86 3469.33,-150.162 3475.56,-153.355",
        "7454.2,-313.801 7443.62,-314.353 7452.24,-320.519 7454.2,-313.801",
        "654.216,-393.603 643.872,-395.895 653.388,-400.554 654.216,-393.603",
        "1063.36,-422.475 1058.97,-412.835 1056.39,-423.111 1063.36,-422.475",
        "13151.3,-479.071 13161.3,-475.571 13151.3,-472.071 13151.3,-479.071",
        "6138.24,-327.32 6145.36,-319.474 6134.9,-321.168 6138.24,-327.32",
        "7368.72,-506.247 7376.36,-498.914 7365.81,-499.878 7368.72,-506.247",
        "5735.42,-399.49 5724.93,-400.994 5734.06,-406.359 5735.42,-399.49",
        "1016.52,-506.407 1024.01,-498.904 1013.48,-500.105 1016.52,-506.407",
        "740.404,-479.071 750.404,-475.571 740.404,-472.071 740.404,-479.071",
        "561.629,-423.05 558.819,-412.835 554.645,-422.572 561.629,-423.05",
        "972.623,-405.061 981.871,-399.891 971.418,-398.165 972.623,-405.061",
        "560.419,-43.8849 569.408,-38.2771 558.884,-37.0553 560.419,-43.8849",
        "209.082,-330.879 202.08,-322.929 202.596,-333.511 209.082,-330.879",
        "4974.13,-479.071 4984.13,-475.571 4974.13,-472.071 4974.13,-479.071",
        "5207.51,-242.869 5211.2,-232.936 5202.12,-238.404 5207.51,-242.869",
        "7322.94,-479.071 7332.94,-475.571 7322.94,-472.071 7322.94,-479.071",
        "5628.25,-422.176 5631.68,-412.152 5622.75,-417.849 5628.25,-422.176",
        "1750.29,-329.889 1742.3,-322.929 1744.21,-333.351 1750.29,-329.889",
        "6677.81,-479.071 6687.81,-475.571 6677.81,-472.071 6677.81,-479.071",
        "6583.91,-420.791 6576.96,-412.799 6577.41,-423.384 6583.91,-420.791",
        "6711.54,-417.521 6718.58,-409.606 6708.14,-411.401 6711.54,-417.521",
        "7045.02,-328.354 7052.01,-320.392 7041.58,-322.257 7045.02,-328.354",
        "7510.26,-320.522 7519.21,-314.843 7508.67,-313.705 7510.26,-320.522",
        "4672.56,-304.358 4662.1,-306.046 4671.33,-311.249 4672.56,-304.358",
        "974.396,-479.071 984.396,-475.571 974.396,-472.071 974.396,-479.071",
        "633.547,-404.661 622.966,-405.194 631.57,-411.376 633.547,-404.661",
        "1000.23,-415.639 1007.95,-408.387 997.391,-409.239 1000.23,-415.639",
        "862.098,-333.097 858.597,-323.097 855.098,-333.097 862.098,-333.097",
        "5169.51,-414.798 5177.06,-407.366 5166.53,-408.467 5169.51,-414.798",
        "4798.4,-403.775 4787.81,-403.465 4795.89,-410.312 4798.4,-403.775",
        "5346.85,-331.68 5340.92,-322.905 5340.09,-333.468 5346.85,-331.68",
        "5233.19,-243.246 5229.73,-233.233 5226.19,-243.221 5233.19,-243.246",
        "5148.68,-146.59 5155.96,-138.891 5145.46,-140.371 5148.68,-146.59",
        "4739.03,-422.779 4739.48,-412.194 4732.52,-420.185 4739.03,-422.779",
        "5266.38,-319.623 5274.94,-313.376 5264.35,-312.923 5266.38,-319.623",
        "4627.02,-149.676 4618.63,-143.205 4621.15,-153.495 4627.02,-149.676",
        "4807.88,-152.995 4811.34,-142.982 4802.39,-148.652 4807.88,-152.995",
        "8249.32,-323.724 8239.05,-321.139 8245.47,-329.567 8249.32,-323.724",
        "8440.61,-331.823 8434.86,-322.929 8433.81,-333.472 8440.61,-331.823",
        "8210.72,-332.564 8205.97,-323.097 8203.78,-333.463 8210.72,-332.564",
        "8055.26,-240.951 8048.07,-233.17 8048.84,-243.737 8055.26,-240.951",
        "4341.92,-332.537 4337.11,-323.097 4334.99,-333.475 4341.92,-332.537",
        "4524.03,-331.557 4529.19,-322.302 4519.38,-326.321 4524.03,-331.557",
        "28.5975,-314.09 28.5975,-314.09 28.5975,-314.09 28.5975,-314.09",
        "6397.1,-333.094 6393.6,-323.094 6390.1,-333.094 6397.1,-333.094",
        "7981.29,-366.661 7971.06,-369.421 7980.78,-373.643 7981.29,-366.661",
        "8013.24,-242.69 8016.86,-232.732 8007.82,-238.261 8013.24,-242.69",
        "958.598,-389.331 968.598,-385.831 958.598,-382.331 958.598,-389.331",
        "537.08,-330.126 529.468,-322.756 530.822,-333.264 537.08,-330.126",
        "5139.13,-389.331 5149.13,-385.831 5139.13,-382.331 5139.13,-389.331",
        "28.5975,-224.35 28.5975,-224.35 28.5975,-224.35 28.5975,-224.35",
        "12882.1,-64.0848 12878.6,-54.0848 12875.1,-64.0848 12882.1,-64.0848",
        "3332.1,-153.616 3328.6,-143.616 3325.1,-153.616 3332.1,-153.616",
        "13123.6,-233.49 13113.2,-231.112 13119.8,-239.41 13123.6,-233.49",
        "13257,-242.855 13260.1,-232.714 13251.4,-238.73 13257,-242.855",
        "2655.42,-122.266 2665.38,-118.631 2655.33,-115.267 2655.42,-122.266",
        "2819.07,-148.539 2809.87,-143.28 2813.78,-153.126 2819.07,-148.539",
        "6361.79,-151.607 6367.17,-142.479 6357.28,-146.26 6361.79,-151.607",
        "6186.11,-153.514 6182.33,-143.616 6179.11,-153.711 6186.11,-153.514",
        "4057.67,-326.441 4065.74,-319.577 4055.15,-319.91 4057.67,-326.441",
        "3949.64,-149.772 3955.31,-140.818 3945.29,-144.285 3949.64,-149.772",
        "4057.67,-326.441 4065.74,-319.577 4055.15,-319.91 4057.67,-326.441",
        "3694.42,-152.962 3698,-142.992 3688.99,-148.552 3694.42,-152.962",
        "5288.61,-139.637 5278.03,-139.106 5285.97,-146.12 5288.61,-139.637",
        "7481.34,-299.591 7491.34,-296.09 7481.34,-292.591 7481.34,-299.591",
        "7456.54,-153.754 7459,-143.448 7450.65,-149.971 7456.54,-153.754",
        "7393.99,-59.1124 7400.99,-51.1613 7390.56,-53.0103 7393.99,-59.1124",
        "3095.76,-236.744 3086.16,-232.269 3090.88,-241.756 3095.76,-236.744",
        "1738.19,-205.677 1728.12,-208.991 1738.06,-212.676 1738.19,-205.677",
        "3185.18,-64.1274 3182.28,-53.9385 3178.19,-63.7149 3185.18,-64.1274",
        "707.674,-25.1956 697.636,-28.5847 707.597,-32.1952 707.674,-25.1956",
        "3365.58,-50.4723 3374.65,-44.9976 3364.15,-43.6208 3365.58,-50.4723",
        "14379.1,-299.591 14389.1,-296.09 14379.1,-292.591 14379.1,-299.591",
        "3551.29,-134.407 3540.7,-134.509 3549.05,-141.036 3551.29,-134.407",
        "3779.79,-141 3769.23,-140.134 3776.95,-147.396 3779.79,-141",
        "4011.99,-148.323 4003.19,-142.419 4006.38,-152.521 4011.99,-148.323",
        "4121.1,-64.0848 4117.6,-54.0848 4114.1,-64.0848 4121.1,-64.0848",
        "5339.78,-220.14 5329.37,-222.115 5338.74,-227.062 5339.78,-220.14",
        "5593.58,-226.81 5582.98,-226.747 5591.22,-233.403 5593.58,-226.81",
        "5764.94,-242.168 5759.11,-233.319 5758.15,-243.87 5764.94,-242.168",
        "5459.22,-152.038 5463.81,-142.485 5454.27,-147.095 5459.22,-152.038",
        "5827.7,-132.67 5817.1,-132.488 5825.27,-139.236 5827.7,-132.67",
        "5956.42,-154.178 5956.59,-143.585 5949.85,-151.757 5956.42,-154.178",
        "4002.88,-299.591 4012.88,-296.09 4002.88,-292.591 4002.88,-299.591",
        "7500.37,-150.004 7492.05,-143.448 7494.47,-153.764 7500.37,-150.004",
        "7560.63,-53.0103 7550.2,-51.1613 7557.2,-59.1124 7560.63,-53.0103",
        "14685.1,-153.616 14681.6,-143.616 14678.1,-153.616 14685.1,-153.616",
        "28.5975,-134.61 28.5975,-134.61 28.5975,-134.61 28.5975,-134.61",
        "5512.78,-151.774 5506.07,-143.579 5506.21,-154.173 5512.78,-151.774",
        "5671.9,-147.224 5679.69,-140.043 5669.13,-140.798 5671.9,-147.224",
        "16353.1,-153.614 16349.6,-143.614 16346.1,-153.614 16353.1,-153.614",
        "5569.72,-138.971 5559.12,-139.04 5567.45,-145.593 5569.72,-138.971",
        "5743.54,-152.49 5737.81,-143.579 5736.74,-154.119 5743.54,-152.49",
        "5902.47,-147.112 5910.37,-140.054 5899.79,-140.644 5902.47,-147.112",
        "17358.5,-145.723 17348.5,-142.224 17354.1,-151.196 17358.5,-145.723",
        "17500,-151.526 17505.6,-142.529 17495.7,-146.073 17500,-151.526",
        "28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701 28.5975,-44.8701",
        "613.127,-63.5604 615.302,-53.1911 607.136,-59.941 613.127,-63.5604",
        "17423,-120.11 17433,-116.61 17423,-113.11 17423,-120.11",
        "19909.1,-63.874 19905.6,-53.874 19902.1,-63.8741 19909.1,-63.874",
        "3484.36,-62.9314 3478.92,-53.8389 3477.51,-64.3383 3484.36,-62.9314",
        "7479.1,-63.874 7475.6,-53.874 7472.1,-63.8741 7479.1,-63.874",
        "5844.6,-120.11 5854.6,-116.61 5844.6,-113.11 5844.6,-120.11",
        "21685.1,-63.874 21681.6,-53.874 21678.1,-63.8741 21685.1,-63.874",
        "6279.06,-120.11 6289.06,-116.61 6279.06,-113.11 6279.06,-120.11",
        "3275.7,-7.71366 3265.48,-10.4811 3275.2,-14.6953 3275.7,-7.71366"
    ]
]